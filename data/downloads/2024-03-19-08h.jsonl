{"created":"2024-03-18 12:21:00","title":"Replica Field Theory for a Generalized Franz--Parisi Potential of Inhomogeneous Glassy Systems: New Closure and the Associated Self-Consistent Equation","abstract":"On approaching the dynamical transition temperature, supercooled liquids show heterogeneity over space and time. Static replica theory investigates the dynamical crossover in terms of the free energy landscape (FEL). Two kinds of static approaches have provided a self-consistent equation for determining this crossover, similar to the mode coupling theory for glassy dynamics. One uses the Morita-Hiroike formalism of the liquid state theory, whereas the other relies on the density functional theory (DFT). Each of the two approaches has advantages in terms of perturbative field theory. Here, we develop a replica field theory that has the benefits from both formulations. We introduce the generalized Franz-Parisi potential to formulate a correlation functional. Considering fluctuations around an inhomogeneous density determined by the Ramakrishnan--Yussouf DFT, we find a new closure as the stability condition of the correlation functional. The closure leads to the self-consistent equation involving the triplet direct correlation function. The present field theory further helps us study the FEL beyond the mean-field approximation.","sentences":["On approaching the dynamical transition temperature, supercooled liquids show heterogeneity over space and time.","Static replica theory investigates the dynamical crossover in terms of the free energy landscape (FEL).","Two kinds of static approaches have provided a self-consistent equation for determining this crossover, similar to the mode coupling theory for glassy dynamics.","One uses the Morita-Hiroike formalism of the liquid state theory, whereas the other relies on the density functional theory (DFT).","Each of the two approaches has advantages in terms of perturbative field theory.","Here, we develop a replica field theory that has the benefits from both formulations.","We introduce the generalized Franz-Parisi potential to formulate a correlation functional.","Considering fluctuations around an inhomogeneous density determined by the Ramakrishnan--Yussouf DFT, we find a new closure as the stability condition of the correlation functional.","The closure leads to the self-consistent equation involving the triplet direct correlation function.","The present field theory further helps us study the FEL beyond the mean-field approximation."],"url":"http://arxiv.org/abs/2403.11720v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-18 12:16:16","title":"Adelic approximation on spheres","abstract":"We establish an adelic version of Dirichlet's approximation theorem on spheres. Let $K$ be a number field, $E$ be a rigid adelic space over $K$ and $q\\colon E\\to K$ be a quadratic form. Let $v$ be a place of $K$ and $\\alpha\\in E\\otimes_{K}K_{v}$ such that $q(\\alpha)=1$. We produce an explicit constant $c$ having the following property. If there exists $x\\in E$ such that $q(x)=1$ then, for any $T>c$, there exists $(\\upupsilon,\\upphi)\\in E\\times K$, with $\\max{(\\Vert\\upupsilon\\Vert_{E,v},\\vert\\upphi\\vert_{v})}\\le T$ and $\\max{(\\Vert\\upupsilon\\Vert_{E,w},\\vert\\upphi\\vert_{w})}$ controlled for any place $w$, satisfying $q(\\upupsilon)=\\upphi^{2}\\ne 0$ and $\\vert q(\\alpha\\upphi-\\upupsilon)\\vert_{v}\\le c\\vert\\upphi\\vert_{v}/T$. This remains true for some infinite algebraic extensions as well as for a compact set of places of $K$. Our statements generalize and improve on earlier results by Kleinbock \\\\& Merrill (2015) and Moshchevitin (2017). The proofs rely on the quadratic Siegel's lemma in a rigid adelic space obtained by the author and R{\\'e}mond (2017).","sentences":["We establish an adelic version of Dirichlet's approximation theorem on spheres.","Let $K$ be a number field, $E$ be a rigid adelic space over $K$ and $q\\colon E\\to K$ be a quadratic form.","Let $v$ be a place of $K$ and $\\alpha\\in E\\otimes_{K}K_{v}$ such that $q(\\alpha)=1$. We produce an explicit constant $c$ having the following property.","If there exists $x\\in E$ such that $q(x)=1$ then, for any $T>c$, there exists $(\\upupsilon,\\upphi)\\in E\\times K$, with $\\max{(\\Vert\\upupsilon\\Vert_{E,v},\\vert\\upphi\\vert_{v})}\\le T$ and $\\max{(\\Vert\\upupsilon\\Vert_{E,w},\\vert\\upphi\\vert_{w})}$ controlled for any place $w$, satisfying $q(\\upupsilon)=\\upphi^{2}\\ne 0$ and $\\vert q(\\alpha\\upphi-\\upupsilon)\\vert_{v}\\le","c\\vert\\upphi\\vert_{v}/T$.","This remains true for some infinite algebraic extensions as well as for a compact set of places of $K$. Our statements generalize and improve on earlier results by Kleinbock \\\\& Merrill (2015) and Moshchevitin (2017).","The proofs rely on the quadratic Siegel's lemma in a rigid adelic space obtained by the author and R{\\'e}mond (2017)."],"url":"http://arxiv.org/abs/2403.11714v1","category":"math.NT"}
{"created":"2024-03-18 12:13:17","title":"Matter and cosmogenesis in Kant's Theory of the Heavens","abstract":"In 1755, Kant published his General Natural History and Theory of the Heavens, in which he presented his hypothesis on the formation of the solar system, known as the primitive nebula hypothesis. This original theory of the heavens was written in dialogue with the conceptions of celestial matter of his time. On the one hand, Kant recognized Descartes' cosmological enterprise as a decisive mechanistic requirement for the intelligence of physics, but his vortices of matter fell into disrepute: Newton invalidated them mathematically. On the other hand, attraction at a distance provides an incomparable explanation of phenomena, but has left a gaping hole in our understanding of the law's material anchorage in bodies, and ultimately sends the question of how the system works back to God. He then set out to overcome the dichotomy established by these two authors, drawing on the Cartesian adage ''give me matter and I'll make a world'', but applying Newton's laws to cosmogenesis. In concrete terms, interplanetary space currently contains no matter capable of explaining the motion of the stars, we need to look further back into the system's past, to find an earlier state of material dispersion whose effects are still being felt. He must assume the existence of a cloud of dust, made up of the simplest elements and moved by the forces of attraction and repulsion alone. From this, Kant can form the hypothesis of the primitive nebula breaking down first into a phase of stellogenesis, and then into the creation of an accretion disk gradually forming the planets and their satellites.","sentences":["In 1755, Kant published his General Natural History and Theory of the Heavens, in which he presented his hypothesis on the formation of the solar system, known as the primitive nebula hypothesis.","This original theory of the heavens was written in dialogue with the conceptions of celestial matter of his time.","On the one hand, Kant recognized Descartes' cosmological enterprise as a decisive mechanistic requirement for the intelligence of physics, but his vortices of matter fell into disrepute:","Newton invalidated them mathematically.","On the other hand, attraction at a distance provides an incomparable explanation of phenomena, but has left a gaping hole in our understanding of the law's material anchorage in bodies, and ultimately sends the question of how the system works back to God.","He then set out to overcome the dichotomy established by these two authors, drawing on the Cartesian adage ''give me matter and I'll make a world'', but applying Newton's laws to cosmogenesis.","In concrete terms, interplanetary space currently contains no matter capable of explaining the motion of the stars, we need to look further back into the system's past, to find an earlier state of material dispersion whose effects are still being felt.","He must assume the existence of a cloud of dust, made up of the simplest elements and moved by the forces of attraction and repulsion alone.","From this, Kant can form the hypothesis of the primitive nebula breaking down first into a phase of stellogenesis, and then into the creation of an accretion disk gradually forming the planets and their satellites."],"url":"http://arxiv.org/abs/2403.11710v1","category":"physics.hist-ph"}
{"created":"2024-03-18 12:13:01","title":"Significant impact of light-matter strong coupling on chiral nonlinear optical effect","abstract":"Light-matter strong coupling (LMSC) is intriguing state in which light and matter are coherently hybridized inside cavity. It has been gaining widespread recognition as an excellent way for controlling material properties without any chemical modification. Here we show the LMSC is a powerful state to manipulate and improve a chiral nonlinear optical (NLO) effect through the investigation of second harmonic generation circular dichroism. At the upper polaritonic band in LMSC, in addition to an enhancement of SHG intensity by more than one order of magnitude, the responsivity to the handedness of circular polarized light is largely modified, where sign inversion and increase of dissymmetric factor is achieved. Quarter wave plate rotation analysis reveals that the LMSC clearly influence the coefficients associated with chirality in NLO process and it also contributes to the enhancement of nonlinear magnetic dipole interactions. This study demonstrates that LMSC serves as a novel platform for control of chiral optoelectronics and magneto-optics.","sentences":["Light-matter strong coupling (LMSC) is intriguing state in which light and matter are coherently hybridized inside cavity.","It has been gaining widespread recognition as an excellent way for controlling material properties without any chemical modification.","Here we show the LMSC is a powerful state to manipulate and improve a chiral nonlinear optical (NLO) effect through the investigation of second harmonic generation circular dichroism.","At the upper polaritonic band in LMSC, in addition to an enhancement of SHG intensity by more than one order of magnitude, the responsivity to the handedness of circular polarized light is largely modified, where sign inversion and increase of dissymmetric factor is achieved.","Quarter wave plate rotation analysis reveals that the LMSC clearly influence the coefficients associated with chirality in NLO process and it also contributes to the enhancement of nonlinear magnetic dipole interactions.","This study demonstrates that LMSC serves as a novel platform for control of chiral optoelectronics and magneto-optics."],"url":"http://arxiv.org/abs/2403.11709v1","category":"physics.optics"}
{"created":"2024-03-18 12:08:01","title":"Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models","abstract":"Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.","sentences":["Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation.","Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time.","This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings.","These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control.","We propose an inference procedure enabling the coherent generation of sources and accompaniments.","Additionally, we adapt the Dirac separator of MSDM to perform source separation.","We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting."],"url":"http://arxiv.org/abs/2403.11706v1","category":"cs.SD"}
{"created":"2024-03-18 12:04:11","title":"LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images","abstract":"Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.","sentences":["Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world.","Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness.","In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy.","To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.","LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.","Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks.","Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA.","Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5).","We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD."],"url":"http://arxiv.org/abs/2403.11703v1","category":"cs.CV"}
{"created":"2024-03-18 11:56:35","title":"Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing","abstract":"With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.","sentences":["With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information.","However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth.","Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages.","These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money.","This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo.","With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos.","Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects.","This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects.","Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly.","This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges."],"url":"http://arxiv.org/abs/2403.11700v1","category":"cs.MM"}
{"created":"2024-03-18 11:54:35","title":"Urban Scene Diffusion through Semantic Occupancy Map","abstract":"Generating unbounded 3D scenes is crucial for large-scale scene understanding and simulation. Urban scenes, unlike natural landscapes, consist of various complex man-made objects and structures such as roads, traffic signs, vehicles, and buildings. To create a realistic and detailed urban scene, it is crucial to accurately represent the geometry and semantics of the underlying objects, going beyond their visual appearance. In this work, we propose UrbanDiffusion, a 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and generates an urban scene with geometry and semantics in the form of semantic occupancy map. Our model introduces a novel paradigm that learns the data distribution of scene-level structures within a latent space and further enables the expansion of the synthesized scene into an arbitrary scale. After training on real-world driving datasets, our model can generate a wide range of diverse urban scenes given the BEV maps from the held-out set and also generalize to the synthesized maps from a driving simulator. We further demonstrate its application to scene image synthesis with a pretrained image generator as a prior.","sentences":["Generating unbounded 3D scenes is crucial for large-scale scene understanding and simulation.","Urban scenes, unlike natural landscapes, consist of various complex man-made objects and structures such as roads, traffic signs, vehicles, and buildings.","To create a realistic and detailed urban scene, it is crucial to accurately represent the geometry and semantics of the underlying objects, going beyond their visual appearance.","In this work, we propose UrbanDiffusion, a 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and generates an urban scene with geometry and semantics in the form of semantic occupancy map.","Our model introduces a novel paradigm that learns the data distribution of scene-level structures within a latent space and further enables the expansion of the synthesized scene into an arbitrary scale.","After training on real-world driving datasets, our model can generate a wide range of diverse urban scenes given the BEV maps from the held-out set and also generalize to the synthesized maps from a driving simulator.","We further demonstrate its application to scene image synthesis with a pretrained image generator as a prior."],"url":"http://arxiv.org/abs/2403.11697v1","category":"cs.CV"}
{"created":"2024-03-18 11:52:33","title":"Generalization error of spectral algorithms","abstract":"The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of $\\textit{spectral algorithms}$ specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomenon (iii) conjecture, and demonstrate for the considered data models, the universality of the loss w.r.t. non-spectral details of the problem, but only in case of noisy observation.","sentences":["The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels.","However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD).","In the present work, we consider the training of kernels with a family of $\\textit{spectral algorithms}$ specified by profile $h(\\lambda)$, and including KRR and GD as special cases.","Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model.","Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomenon (iii) conjecture, and demonstrate for the considered data models, the universality of the loss w.r.t.","non-spectral details of the problem, but only in case of noisy observation."],"url":"http://arxiv.org/abs/2403.11696v1","category":"cs.LG"}
{"created":"2024-03-18 11:48:02","title":"Beamforming Design for Semantic-Bit Coexisting Communication System","abstract":"Semantic communication (SemCom) is emerging as a key technology for future sixth-generation (6G) systems. Unlike traditional bit-level communication (BitCom), SemCom directly optimizes performance at the semantic level, leading to supe- rior communication efficiency. Nevertheless, the task-oriented nature of SemCom renders it challenging to completely replace BitCom. Consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (BS) serves SemCom users (sem-users) and BitCom users (bit-users) simultaneously. Such a system faces severe and heterogeneous inter-user interference. In this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users. Specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (QoS) requirements for bit-users. Due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting. With the resulting complex transcendental function, majorization minimization (MM) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (FP) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (QCQP) problem. Solving the problem leads to a semi-closed form solution with undetermined Lagrangian factors that can be updated by a fixed point algorithm. Extensive simulation results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and weighted minimum mean-square error (WMMSE).","sentences":["Semantic communication (SemCom) is emerging as a key technology for future sixth-generation (6G) systems.","Unlike traditional bit-level communication (BitCom), SemCom directly optimizes performance at the semantic level, leading to supe- rior communication efficiency.","Nevertheless, the task-oriented nature of SemCom renders it challenging to completely replace BitCom.","Consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (BS) serves SemCom users (sem-users) and BitCom users (bit-users) simultaneously.","Such a system faces severe and heterogeneous inter-user interference.","In this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users.","Specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (QoS) requirements for bit-users.","Due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting.","With the resulting complex transcendental function, majorization minimization (MM) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (FP) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (QCQP) problem.","Solving the problem leads to a semi-closed form solution with undetermined Lagrangian factors that can be updated by a fixed point algorithm.","Extensive simulation results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and weighted minimum mean-square error (WMMSE)."],"url":"http://arxiv.org/abs/2403.11693v1","category":"cs.IT"}
{"created":"2024-03-18 11:38:47","title":"MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation","abstract":"The task of single-source domain generalization (SDG) in medical image segmentation is crucial due to frequent domain shifts in clinical image datasets. To address the challenge of poor generalization across different domains, we introduce a Plug-and-Play module for data augmentation called MoreStyle. MoreStyle diversifies image styles by relaxing low-frequency constraints in Fourier space, guiding the image reconstruction network. With the help of adversarial learning, MoreStyle further expands the style range and pinpoints the most intricate style combinations within latent features. To handle significant style variations, we introduce an uncertainty-weighted loss. This loss emphasizes hard-to-classify pixels resulting only from style shifts while mitigating true hard-to-classify pixels in both MoreStyle-generated and original images. Extensive experiments on two widely used benchmarks demonstrate that the proposed MoreStyle effectively helps to achieve good domain generalization ability, and has the potential to further boost the performance of some state-of-the-art SDG methods.","sentences":["The task of single-source domain generalization (SDG) in medical image segmentation is crucial due to frequent domain shifts in clinical image datasets.","To address the challenge of poor generalization across different domains, we introduce a Plug-and-Play module for data augmentation called MoreStyle.","MoreStyle diversifies image styles by relaxing low-frequency constraints in Fourier space, guiding the image reconstruction network.","With the help of adversarial learning, MoreStyle further expands the style range and pinpoints the most intricate style combinations within latent features.","To handle significant style variations, we introduce an uncertainty-weighted loss.","This loss emphasizes hard-to-classify pixels resulting only from style shifts while mitigating true hard-to-classify pixels in both MoreStyle-generated and original images.","Extensive experiments on two widely used benchmarks demonstrate that the proposed MoreStyle effectively helps to achieve good domain generalization ability, and has the potential to further boost the performance of some state-of-the-art SDG methods."],"url":"http://arxiv.org/abs/2403.11689v1","category":"eess.IV"}
{"created":"2024-03-18 11:36:49","title":"Primal-dual interior-point algorithm for linearly constrained convex optimization based on a parametric algebraic transformation","abstract":"In this paper, we present an interior point algorithm with a full-Newton step for solving a linearly constrained convex optimization problem, in which we propose a generalization of the work of Kheirfam and Nasrollahi \\cite{kheirfam2018full}, that consists in determining the descent directions through a parametric algebraic transformation. The work concludes with a complete study of the convergence of the algorithm and its complexity, where we show that the obtained algorithm achieves a polynomial complexity bounds.","sentences":["In this paper, we present an interior point algorithm with a full-Newton step for solving a linearly constrained convex optimization problem, in which we propose a generalization of the work of Kheirfam and Nasrollahi \\cite{kheirfam2018full}, that consists in determining the descent directions through a parametric algebraic transformation.","The work concludes with a complete study of the convergence of the algorithm and its complexity, where we show that the obtained algorithm achieves a polynomial complexity bounds."],"url":"http://arxiv.org/abs/2403.11684v1","category":"math.NA"}
{"created":"2024-03-18 11:36:45","title":"Hubble tension in a nonminimally coupled curvature-matter gravity model","abstract":"The presently open problem of the Hubble tension is shown to be removed in the context of a modified theory of gravity with a non-minimal coupling between curvature and matter. By evolving the cosmological parameters that match the cosmic microwave background data until their values from direct late-time measurements, we obtain an agreement between different experimental methods without disrupting their individual validity. These modified gravity models are shown to provide adequate fits for other observational data from recent astrophysical surveys and to reproduce the late-time accelerated expansion of the Universe without the inclusion of a cosmological constant. This compatibility with observations presents further evidence of the versatility of these models in mimicking diverse cosmological phenomena in a unified manner.","sentences":["The presently open problem of the Hubble tension is shown to be removed in the context of a modified theory of gravity with a non-minimal coupling between curvature and matter.","By evolving the cosmological parameters that match the cosmic microwave background data until their values from direct late-time measurements, we obtain an agreement between different experimental methods without disrupting their individual validity.","These modified gravity models are shown to provide adequate fits for other observational data from recent astrophysical surveys and to reproduce the late-time accelerated expansion of the Universe without the inclusion of a cosmological constant.","This compatibility with observations presents further evidence of the versatility of these models in mimicking diverse cosmological phenomena in a unified manner."],"url":"http://arxiv.org/abs/2403.11683v1","category":"gr-qc"}
{"created":"2024-03-18 11:35:18","title":"MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion","abstract":"Surface prediction and completion have been widely studied in various applications. Recently, research in surface completion has evolved from small objects to complex large-scale scenes. As a result, researchers have begun increasing the volume of data and leveraging a greater variety of data modalities including rendered RGB images, descriptive texts, depth images, etc, to enhance algorithm performance. However, existing datasets suffer from a deficiency in the amounts of scene-level models along with the corresponding multi-modal information. Therefore, a method to scale the datasets and generate multi-modal information in them efficiently is essential. To bridge this research gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with a verSatile Toolchain for surfAce pRediction and completion. We develop a versatile and efficient toolchain for processing the raw 3D data from the environments. It screens out a set of fine-grained scene models and generates the corresponding multi-modal data. Utilizing the toolchain, we then generate an example dataset composed of over a thousand scene-level models with partial real-world data added. We compare MASSTAR with the existing datasets, which validates its superiority: the ability to efficiently extract high-quality models from complex scenarios to expand the dataset. Additionally, several representative surface completion algorithms are benchmarked on MASSTAR, which reveals that existing algorithms can hardly deal with scene-level completion. We will release the source code of our toolchain and the dataset. For more details, please see our project page at https://sysu-star.github.io/MASSTAR.","sentences":["Surface prediction and completion have been widely studied in various applications.","Recently, research in surface completion has evolved from small objects to complex large-scale scenes.","As a result, researchers have begun increasing the volume of data and leveraging a greater variety of data modalities including rendered RGB images, descriptive texts, depth images, etc, to enhance algorithm performance.","However, existing datasets suffer from a deficiency in the amounts of scene-level models along with the corresponding multi-modal information.","Therefore, a method to scale the datasets and generate multi-modal information in them efficiently is essential.","To bridge this research gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with a verSatile Toolchain for surfAce pRediction and completion.","We develop a versatile and efficient toolchain for processing the raw 3D data from the environments.","It screens out a set of fine-grained scene models and generates the corresponding multi-modal data.","Utilizing the toolchain, we then generate an example dataset composed of over a thousand scene-level models with partial real-world data added.","We compare MASSTAR with the existing datasets, which validates its superiority: the ability to efficiently extract high-quality models from complex scenarios to expand the dataset.","Additionally, several representative surface completion algorithms are benchmarked on MASSTAR, which reveals that existing algorithms can hardly deal with scene-level completion.","We will release the source code of our toolchain and the dataset.","For more details, please see our project page at https://sysu-star.github.io/MASSTAR."],"url":"http://arxiv.org/abs/2403.11681v1","category":"cs.RO"}
{"created":"2024-03-18 11:32:16","title":"Multiscale Orientation Values for Biodiversity, Climate and Water: A Scientific Input for Science- Based Targets","abstract":"In this study, we explore a range of options and outcomes associated with using different allocation approaches to operationalise the Planetary Boundaries (PB) framework at the country, sector, and city scales. We demonstrate: (i) how to translate the PB framework into various sub-global scales (countries, cities, industries); and (ii) how to take global/local aspects (e.g., water use at the watershed level) into account. Finally, we apply the proposed methodology to derive country, city, and sector-specific budgets consistent with the PB concept for Switzerland. We then benchmark the translated PBs for climate, biodiversity, and freshwater use against actual environmental pressures in Switzerland from both production- and consumption-based perspectives. This effectively enables us to provide a comprehensive assessment of whether Switzerland is living within its safe operating space.","sentences":["In this study, we explore a range of options and outcomes associated with using different allocation approaches to operationalise the Planetary Boundaries (PB) framework at the country, sector, and city scales.","We demonstrate: (i) how to translate the PB framework into various sub-global scales (countries, cities, industries); and (ii) how to take global/local aspects (e.g., water use at the watershed level) into account.","Finally, we apply the proposed methodology to derive country, city, and sector-specific budgets consistent with the PB concept for Switzerland.","We then benchmark the translated PBs for climate, biodiversity, and freshwater use against actual environmental pressures in Switzerland from both production- and consumption-based perspectives.","This effectively enables us to provide a comprehensive assessment of whether Switzerland is living within its safe operating space."],"url":"http://arxiv.org/abs/2403.11680v1","category":"econ.GN"}
{"created":"2024-03-18 11:21:52","title":"Towards Generalizing to Unseen Domains with Few Labels","abstract":"We approach the challenge of addressing semi-supervised domain generalization (SSDG). Specifically, our aim is to obtain a model that learns domain-generalizable features by leveraging a limited subset of labelled data alongside a substantially larger pool of unlabeled data. Existing domain generalization (DG) methods which are unable to exploit unlabeled data perform poorly compared to semi-supervised learning (SSL) methods under SSDG setting. Nevertheless, SSL methods have considerable room for performance improvement when compared to fully-supervised DG training. To tackle this underexplored, yet highly practical problem of SSDG, we make the following core contributions. First, we propose a feature-based conformity technique that matches the posterior distributions from the feature space with the pseudo-label from the model's output space. Second, we develop a semantics alignment loss to learn semantically-compatible representations by regularizing the semantic structure in the feature space. Our method is plug-and-play and can be readily integrated with different SSL-based SSDG baselines without introducing any additional parameters. Extensive experimental results across five challenging DG benchmarks with four strong SSL baselines suggest that our method provides consistent and notable gains in two different SSDG settings.","sentences":["We approach the challenge of addressing semi-supervised domain generalization (SSDG).","Specifically, our aim is to obtain a model that learns domain-generalizable features by leveraging a limited subset of labelled data alongside a substantially larger pool of unlabeled data.","Existing domain generalization (DG) methods which are unable to exploit unlabeled data perform poorly compared to semi-supervised learning (SSL) methods under SSDG setting.","Nevertheless, SSL methods have considerable room for performance improvement when compared to fully-supervised DG training.","To tackle this underexplored, yet highly practical problem of SSDG, we make the following core contributions.","First, we propose a feature-based conformity technique that matches the posterior distributions from the feature space with the pseudo-label from the model's output space.","Second, we develop a semantics alignment loss to learn semantically-compatible representations by regularizing the semantic structure in the feature space.","Our method is plug-and-play and can be readily integrated with different SSL-based SSDG baselines without introducing any additional parameters.","Extensive experimental results across five challenging DG benchmarks with four strong SSL baselines suggest that our method provides consistent and notable gains in two different SSDG settings."],"url":"http://arxiv.org/abs/2403.11674v1","category":"cs.CV"}
{"created":"2024-03-18 11:19:37","title":"HDLdebugger: Streamlining HDL debugging with Large Language Models","abstract":"In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.","sentences":["In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role.","However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers.","Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers.","Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results.","In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach.","Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design.","Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging."],"url":"http://arxiv.org/abs/2403.11671v1","category":"cs.AR"}
{"created":"2024-03-18 11:17:27","title":"Semantic Data Representation for Explainable Windows Malware Detection Models","abstract":"Ontologies are a standard tool for creating semantic schemata in many knowledge intensive domains of human interest. They are becoming increasingly important also in the areas that have been until very recently dominated by subsymbolic knowledge representation and machine-learning (ML) based data processing. One such area is information security, and specifically, malware detection. We thus propose PE Malware Ontology that offers a reusable semantic schema for Portable Executable (PE - the Windows binary format) malware files. This ontology is inspired by the structure of the EMBER dataset, which focuses on the static malware analysis of PE files. With this proposal, we hope to provide a unified semantic representation for the existing and future PE-malware datasets and facilitate the application of symbolic, neuro-symbolic, or otherwise explainable approaches in the PE-malware-detection domain, which may produce interpretable results described by the terms defined in our ontology. In addition, we also publish semantically treated EMBER data, including fractional datasets, to support the reproducibility of experiments on EMBER. We supplement our work with a preliminary case study, conducted using concept learning, to show the general feasibility of our approach. While we were not able to match the precision of the state-of-the-art ML tools, the learned malware discriminators were interesting and highly interpretable.","sentences":["Ontologies are a standard tool for creating semantic schemata in many knowledge intensive domains of human interest.","They are becoming increasingly important also in the areas that have been until very recently dominated by subsymbolic knowledge representation and machine-learning (ML) based data processing.","One such area is information security, and specifically, malware detection.","We thus propose PE Malware Ontology that offers a reusable semantic schema for Portable Executable (PE - the Windows binary format) malware files.","This ontology is inspired by the structure of the EMBER dataset, which focuses on the static malware analysis of PE files.","With this proposal, we hope to provide a unified semantic representation for the existing and future PE-malware datasets and facilitate the application of symbolic, neuro-symbolic, or otherwise explainable approaches in the PE-malware-detection domain, which may produce interpretable results described by the terms defined in our ontology.","In addition, we also publish semantically treated EMBER data, including fractional datasets, to support the reproducibility of experiments on EMBER.","We supplement our work with a preliminary case study, conducted using concept learning, to show the general feasibility of our approach.","While we were not able to match the precision of the state-of-the-art ML tools, the learned malware discriminators were interesting and highly interpretable."],"url":"http://arxiv.org/abs/2403.11669v1","category":"cs.CR"}
{"created":"2024-03-18 11:15:03","title":"Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection","abstract":"The high performance of denoising diffusion models for image generation has paved the way for their application in unsupervised medical anomaly detection. As diffusion-based methods require a lot of GPU memory and have long sampling times, we present a novel and fast unsupervised anomaly detection approach based on latent Bernoulli diffusion models. We first apply an autoencoder to compress the input images into a binary latent representation. Next, a diffusion model that follows a Bernoulli noise schedule is employed to this latent space and trained to restore binary latent representations from perturbed ones. The binary nature of this diffusion model allows us to identify entries in the latent space that have a high probability of flipping their binary code during the denoising process, which indicates out-of-distribution data. We propose a masking algorithm based on these probabilities, which improves the anomaly detection scores. We achieve state-of-the-art performance compared to other diffusion-based unsupervised anomaly detection algorithms while significantly reducing sampling time and memory consumption. The code is available at https://github.com/JuliaWolleb/Anomaly_berdiff.","sentences":["The high performance of denoising diffusion models for image generation has paved the way for their application in unsupervised medical anomaly detection.","As diffusion-based methods require a lot of GPU memory and have long sampling times, we present a novel and fast unsupervised anomaly detection approach based on latent Bernoulli diffusion models.","We first apply an autoencoder to compress the input images into a binary latent representation.","Next, a diffusion model that follows a Bernoulli noise schedule is employed to this latent space and trained to restore binary latent representations from perturbed ones.","The binary nature of this diffusion model allows us to identify entries in the latent space that have a high probability of flipping their binary code during the denoising process, which indicates out-of-distribution data.","We propose a masking algorithm based on these probabilities, which improves the anomaly detection scores.","We achieve state-of-the-art performance compared to other diffusion-based unsupervised anomaly detection algorithms while significantly reducing sampling time and memory consumption.","The code is available at https://github.com/JuliaWolleb/Anomaly_berdiff."],"url":"http://arxiv.org/abs/2403.11667v1","category":"cs.CV"}
{"created":"2024-03-18 11:14:56","title":"Skyrmion on Magnetic Tunnel Junction: Interweaving Quantum Transport with Micro-magnetism","abstract":"Over the last two decades, non-trivial magnetic textures, especially the magnetic skyrmion family, have been extensively explored out of fundamental interest, and diverse possible applications. Given the possible technological and scientific ramifications of skyrmion-texture on magnetic tunnel junction (ST-MTJ), in this work, we present non-equilibrium Green's function (NEGF) based description of ST-MTJs both for N\\'eel and Bloch textures, to capture the spin/charge current across different voltages, temperatures, and sizes. We predict the emergence of a textured spin current from the uniform layer of the ST-MTJs, along with a radially varying, asymmetrical voltage dependence of spin torque. We delineate the voltage-induced rotation of the spin current texture, coupled with the appearance of helicity in spin current, particularly in the case of N\\'eel skyrmions on MTJs. We describe the TMR roll-off in ST-MTJ with lower cross-sectional area and higher temperature based on transmission spectra analysis. We also introduce a computationally efficient coupled spatio-eigen framework of NEGF to address the 3D-NEGF requirement of the ST-MTJs. With analytical underpinning, we establish the generic nature of the spatio-eigen framework of NEGF, alleviating the sine-qua-non of the 3D-NEGF for systems that lack transnational invariance and simultaneous eigen-basis in the transverse directions.","sentences":["Over the last two decades, non-trivial magnetic textures, especially the magnetic skyrmion family, have been extensively explored out of fundamental interest, and diverse possible applications.","Given the possible technological and scientific ramifications of skyrmion-texture on magnetic tunnel junction (ST-MTJ), in this work, we present non-equilibrium Green's function (NEGF) based description of ST-MTJs both for N\\'eel and Bloch textures, to capture the spin/charge current across different voltages, temperatures, and sizes.","We predict the emergence of a textured spin current from the uniform layer of the ST-MTJs, along with a radially varying, asymmetrical voltage dependence of spin torque.","We delineate the voltage-induced rotation of the spin current texture, coupled with the appearance of helicity in spin current, particularly in the case of N\\'eel skyrmions on MTJs.","We describe the TMR roll-off in ST-MTJ with lower cross-sectional area and higher temperature based on transmission spectra analysis.","We also introduce a computationally efficient coupled spatio-eigen framework of NEGF to address the 3D-NEGF requirement of the ST-MTJs.","With analytical underpinning, we establish the generic nature of the spatio-eigen framework of NEGF, alleviating the sine-qua-non of the 3D-NEGF for systems that lack transnational invariance and simultaneous eigen-basis in the transverse directions."],"url":"http://arxiv.org/abs/2403.11666v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 11:07:58","title":"Calibration-Based ALE Model Order Reduction for Hyperbolic Problems with Self-Similar Travelling Discontinuities","abstract":"We propose a novel Model Order Reduction framework that is able to handle solutions of hyperbolic problems characterized by multiple travelling discontinuities. By means of an optimization based approach, we introduce suitable calibration maps that allow us to transform the original solution manifold into a lower dimensional one. The optimization process does not require the knowledge of the discontinuities location. In the online phase, the coefficients of the projection of the reduced order solution onto the reduced space are recovered by means of an Artificial Neural Network. To validate the methodology, we present numerical results for the 1D Sod shock tube problem and for the 2D double Mach reflection problem, also in the parametric case.","sentences":["We propose a novel Model Order Reduction framework that is able to handle solutions of hyperbolic problems characterized by multiple travelling discontinuities.","By means of an optimization based approach, we introduce suitable calibration maps that allow us to transform the original solution manifold into a lower dimensional one.","The optimization process does not require the knowledge of the discontinuities location.","In the online phase, the coefficients of the projection of the reduced order solution onto the reduced space are recovered by means of an Artificial Neural Network.","To validate the methodology, we present numerical results for the 1D Sod shock tube problem and for the 2D double Mach reflection problem, also in the parametric case."],"url":"http://arxiv.org/abs/2403.11664v1","category":"math.NA"}
{"created":"2024-03-18 11:07:38","title":"Finite nuclear mass correction to the hyperfine splitting in hydrogenic systems","abstract":"A general quantum electrodynamic method for derivation of nuclear recoil corrections in hydrogenic systems, which are exact in the nuclear charge parameter $Z\\,\\alpha$, is introduced. The exemplary derivation is presented for the $O(m/M)$ nuclear pure recoil correction to the hyperfine splitting. The obtained result is verified by comparison to the known $(Z\\,\\alpha)^5$ contribution","sentences":["A general quantum electrodynamic method for derivation of nuclear recoil corrections in hydrogenic systems, which are exact in the nuclear charge parameter $Z\\,\\alpha$, is introduced.","The exemplary derivation is presented for the $O(m/M)$ nuclear pure recoil correction to the hyperfine splitting.","The obtained result is verified by comparison to the known $(Z\\,\\alpha)^5$ contribution"],"url":"http://arxiv.org/abs/2403.11663v1","category":"physics.atom-ph"}
{"created":"2024-03-18 10:58:02","title":"Effective Computation of the Heegaard Genus of 3-Manifolds","abstract":"The Heegaard genus is a fundamental invariant of 3-manifolds. However, computing the Heegaard genus of a triangulated 3-manifold is NP-hard, and while algorithms exist, little work has been done in making such an algorithm efficient and practical for implementation. Current algorithms use almost normal surfaces, which are an extension of the algorithm-friendly normal surface theory but which add considerable complexity for both running time and implementation.   Here we take a different approach: instead of working with almost normal surfaces, we give a general method of modifying the input triangulation that allows us to avoid almost normal surfaces entirely. The cost is just four new tetrahedra, and the benefit is that important surfaces that were once almost normal can be moved to the simpler setting of normal surfaces in the new triangulation. We apply this technique to the computation of Heegaard genus, where we develop algorithms and heuristics that prove successful in practice when applied to a data set of 3,000 closed hyperbolic 3-manifolds; we precisely determine the genus for at least 2,705 of these.","sentences":["The Heegaard genus is a fundamental invariant of 3-manifolds.","However, computing the Heegaard genus of a triangulated 3-manifold is NP-hard, and while algorithms exist, little work has been done in making such an algorithm efficient and practical for implementation.","Current algorithms use almost normal surfaces, which are an extension of the algorithm-friendly normal surface theory but which add considerable complexity for both running time and implementation.   ","Here we take a different approach: instead of working with almost normal surfaces, we give a general method of modifying the input triangulation that allows us to avoid almost normal surfaces entirely.","The cost is just four new tetrahedra, and the benefit is that important surfaces that were once almost normal can be moved to the simpler setting of normal surfaces in the new triangulation.","We apply this technique to the computation of Heegaard genus, where we develop algorithms and heuristics that prove successful in practice when applied to a data set of 3,000 closed hyperbolic 3-manifolds; we precisely determine the genus for at least 2,705 of these."],"url":"http://arxiv.org/abs/2403.11659v1","category":"math.GT"}
{"created":"2024-03-18 10:55:39","title":"Negative Capacitance for Stabilizing Logic State in Tunnel Field-Effect Transistor","abstract":"The study investigates the influence of negative capacitance on the transfer characteristics of vdW FETs on the heterophase of CIPS ferroelectric. Notably, a less pronounced NC resulting from the spatial distribution of the ferroelectric and paraelectric phases plays crucial role in stabilizing of n-channel-conductance. This results into the emergence of a non-volatile logic state, between the two binary states of TFETs. Concerned study proposed NC-TFETs based on ferroionic crystals as promising devices for generating a stable logic state below Vth.","sentences":["The study investigates the influence of negative capacitance on the transfer characteristics of vdW FETs on the heterophase of CIPS ferroelectric.","Notably, a less pronounced NC resulting from the spatial distribution of the ferroelectric and paraelectric phases plays crucial role in stabilizing of n-channel-conductance.","This results into the emergence of a non-volatile logic state, between the two binary states of TFETs.","Concerned study proposed NC-TFETs based on ferroionic crystals as promising devices for generating a stable logic state below Vth."],"url":"http://arxiv.org/abs/2403.11658v1","category":"physics.app-ph"}
{"created":"2024-03-18 10:49:16","title":"SRB measures for partially hyperbolic systems with one-dimensional center subbundles","abstract":"For a partially hyperbolic attractor with a center bundle splitting in a dominatedway into one-dimensional subbundles we show that for Lebesgue almost every point there is anempirical measure from $x$ with a SRB component. Moreover if the center exponents are nonzero, then $x$ lies in the basin of an ergodic hyperbolic SRB measure and there are only finitely many such measures. This gives another proof of the existence of SRB measures in this context, which was established firstly in [11] by using random perturbations. Moreover this generalizes results of [15,18] which deal with a single one-dimensional center subbundle.","sentences":["For a partially hyperbolic attractor with a center bundle splitting in a dominatedway into one-dimensional subbundles we show that for Lebesgue almost every point there is anempirical measure from $x$ with a SRB component.","Moreover if the center exponents are nonzero, then $x$ lies in the basin of an ergodic hyperbolic SRB measure and there are only finitely many such measures.","This gives another proof of the existence of SRB measures in this context, which was established firstly in [11] by using random perturbations.","Moreover this generalizes results of [15,18] which deal with a single one-dimensional center subbundle."],"url":"http://arxiv.org/abs/2403.11654v1","category":"math.DS"}
{"created":"2024-03-18 10:44:26","title":"Lattice QCD estimates of thermal photon production from the QGP","abstract":"Thermal photons produced in heavy-ion collision experiments are an important observable for understanding quark-gluon plasma (QGP). The thermal photon rate from the QGP at a given temperature can be calculated from the spectral function of the vector current correlator. Extraction of the spectral function from the lattice correlator is known to be an ill-conditioned problem, as there is no unique solution for a spectral function for a given lattice correlator with statistical errors. The vector current correlator, on the other hand, receives a large ultraviolet contribution from the vacuum, which makes the extraction of the thermal photon rate difficult from this channel. We therefore consider the difference between the transverse and longitudinal part of the spectral function, only capturing the thermal contribution to the current correlator, simplifying the reconstruction significantly. The lattice correlator is calculated for light quarks in quenched QCD at $T=470~$MeV ($\\sim 1.5\\, T_c$), as well as in 2+1 flavor QCD at $T=220~$MeV ($\\sim 1.2 \\, T_{pc}$) with $m_{\\pi}=320$ MeV. In order to quantify the non-perturbative effects, the lattice correlator is compared with the corresponding $\\text{NLO}+\\text{LPM}^{\\text{LO}}$ estimate of correlator. The reconstruction of the spectral function is performed in several different frameworks, ranging from physics-informed models of the spectral function to more general models in the Backus-Gilbert method and Gaussian Process regression. We find that the resulting photon rates agree within errors.","sentences":["Thermal photons produced in heavy-ion collision experiments are an important observable for understanding quark-gluon plasma (QGP).","The thermal photon rate from the QGP at a given temperature can be calculated from the spectral function of the vector current correlator.","Extraction of the spectral function from the lattice correlator is known to be an ill-conditioned problem, as there is no unique solution for a spectral function for a given lattice correlator with statistical errors.","The vector current correlator, on the other hand, receives a large ultraviolet contribution from the vacuum, which makes the extraction of the thermal photon rate difficult from this channel.","We therefore consider the difference between the transverse and longitudinal part of the spectral function, only capturing the thermal contribution to the current correlator, simplifying the reconstruction significantly.","The lattice correlator is calculated for light quarks in quenched QCD at $T=470~$MeV ($\\sim 1.5\\, T_c$), as well as in 2+1 flavor QCD at $T=220~$MeV ($\\sim 1.2 \\, T_{pc}$) with $m_{\\pi}=320$ MeV.","In order to quantify the non-perturbative effects, the lattice correlator is compared with the corresponding $\\text{NLO}+\\text{LPM}^{\\text{LO}}$ estimate of correlator.","The reconstruction of the spectral function is performed in several different frameworks, ranging from physics-informed models of the spectral function to more general models in the Backus-Gilbert method and Gaussian Process regression.","We find that the resulting photon rates agree within errors."],"url":"http://arxiv.org/abs/2403.11647v1","category":"hep-lat"}
{"created":"2024-03-18 10:41:38","title":"Many-body quantum heat engines based on free-fermion systems","abstract":"We study the performances of an imperfect quantum many-body Otto engine based on free-fermion systems. Starting from the thermodynamic definitions of heat and work along ideal isothermal, adiabatic, and isochoric transformations, we generalize these expressions in the case when the hypotheses of ideality are relaxed (i.e., nonperfect thermalization with the external baths, as well as nonperfect quantum adiabaticity in the unitary dynamic protocols). These results are used to evaluate the work and the power delivered by an imperfect quantum many-body heat engine in a finite time, whose working substance is constituted by a quantum Ising chain in a transverse field: We discuss the emerging optimal working points as functions of the various model parameters.","sentences":["We study the performances of an imperfect quantum many-body Otto engine based on free-fermion systems.","Starting from the thermodynamic definitions of heat and work along ideal isothermal, adiabatic, and isochoric transformations, we generalize these expressions in the case when the hypotheses of ideality are relaxed (i.e., nonperfect thermalization with the external baths, as well as nonperfect quantum adiabaticity in the unitary dynamic protocols).","These results are used to evaluate the work and the power delivered by an imperfect quantum many-body heat engine in a finite time, whose working substance is constituted by a quantum Ising chain in a transverse field: We discuss the emerging optimal working points as functions of the various model parameters."],"url":"http://arxiv.org/abs/2403.11645v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 10:35:15","title":"Diffusion-Based Environment-Aware Trajectory Prediction","abstract":"The ability to predict the future trajectories of traffic participants is crucial for the safe and efficient operation of autonomous vehicles. In this paper, a diffusion-based generative model for multi-agent trajectory prediction is proposed. The model is capable of capturing the complex interactions between traffic participants and the environment, accurately learning the multimodal nature of the data. The effectiveness of the approach is assessed on large-scale datasets of real-world traffic scenarios, showing that our model outperforms several well-established methods in terms of prediction accuracy. By the incorporation of differential motion constraints on the model output, we illustrate that our model is capable of generating a diverse set of realistic future trajectories. Through the use of an interaction-aware guidance signal, we further demonstrate that the model can be adapted to predict the behavior of less cooperative agents, emphasizing its practical applicability under uncertain traffic conditions.","sentences":["The ability to predict the future trajectories of traffic participants is crucial for the safe and efficient operation of autonomous vehicles.","In this paper, a diffusion-based generative model for multi-agent trajectory prediction is proposed.","The model is capable of capturing the complex interactions between traffic participants and the environment, accurately learning the multimodal nature of the data.","The effectiveness of the approach is assessed on large-scale datasets of real-world traffic scenarios, showing that our model outperforms several well-established methods in terms of prediction accuracy.","By the incorporation of differential motion constraints on the model output, we illustrate that our model is capable of generating a diverse set of realistic future trajectories.","Through the use of an interaction-aware guidance signal, we further demonstrate that the model can be adapted to predict the behavior of less cooperative agents, emphasizing its practical applicability under uncertain traffic conditions."],"url":"http://arxiv.org/abs/2403.11643v1","category":"cs.CV"}
{"created":"2024-03-18 10:34:40","title":"Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring","abstract":"Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowledge is given, and we adapt the fitness function, as well as the crossover and mutation operators, to maintain the satisfaction of the constraints. The proposed methods are evaluated with respect to state-of-the-art genetic algorithms for counterfactual generation and the results are presented. We showcase that the inclusion of temporal background knowledge allows the generation of counterfactuals more conformant to the temporal background knowledge, without however losing in terms of the counterfactual traditional quality metrics.","sentences":["Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system.","When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered.","A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege).","Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions.","However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals.","In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime.","We assume that this temporal background knowledge is given, and we adapt the fitness function, as well as the crossover and mutation operators, to maintain the satisfaction of the constraints.","The proposed methods are evaluated with respect to state-of-the-art genetic algorithms for counterfactual generation and the results are presented.","We showcase that the inclusion of temporal background knowledge allows the generation of counterfactuals more conformant to the temporal background knowledge, without however losing in terms of the counterfactual traditional quality metrics."],"url":"http://arxiv.org/abs/2403.11642v1","category":"cs.AI"}
{"created":"2024-03-18 10:32:51","title":"Arc2Face: A Foundation Model of Human Faces","abstract":"This paper presents Arc2Face, an identity-conditioned face foundation model, which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of face similarity than existing models. Despite previous attempts to decode face recognition features into detailed images, we find that common high-resolution datasets (e.g. FFHQ) lack sufficient identities to reconstruct any subject. To that end, we meticulously upsample a significant portion of the WebFace42M database, the largest public dataset for face recognition (FR). Arc2Face builds upon a pretrained Stable Diffusion model, yet adapts it to the task of ID-to-face generation, conditioned solely on ID vectors. Deviating from recent works that combine ID with text embeddings for zero-shot personalization of text-to-image models, we emphasize on the compactness of FR features, which can fully capture the essence of the human face, as opposed to hand-crafted prompts. Crucially, text-augmented models struggle to decouple identity and text, usually necessitating some description of the given face to achieve satisfactory similarity. Arc2Face, however, only needs the discriminative features of ArcFace to guide the generation, offering a robust prior for a plethora of tasks where ID consistency is of paramount importance. As an example, we train a FR model on synthetic images from our model and achieve superior performance to existing synthetic datasets.","sentences":["This paper presents Arc2Face, an identity-conditioned face foundation model, which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of face similarity than existing models.","Despite previous attempts to decode face recognition features into detailed images, we find that common high-resolution datasets (e.g. FFHQ) lack sufficient identities to reconstruct any subject.","To that end, we meticulously upsample a significant portion of the WebFace42M database, the largest public dataset for face recognition (FR). Arc2Face","builds upon a pretrained Stable Diffusion model, yet adapts it to the task of ID-to-face generation, conditioned solely on ID vectors.","Deviating from recent works that combine ID with text embeddings for zero-shot personalization of text-to-image models, we emphasize on the compactness of FR features, which can fully capture the essence of the human face, as opposed to hand-crafted prompts.","Crucially, text-augmented models struggle to decouple identity and text, usually necessitating some description of the given face to achieve satisfactory similarity.","Arc2Face, however, only needs the discriminative features of ArcFace to guide the generation, offering a robust prior for a plethora of tasks where ID consistency is of paramount importance.","As an example, we train a FR model on synthetic images from our model and achieve superior performance to existing synthetic datasets."],"url":"http://arxiv.org/abs/2403.11641v1","category":"cs.CV"}
{"created":"2024-03-18 10:23:59","title":"Quasinormal Modes of Near-Extremal Electric and Magnetic Black Branes","abstract":"Gauge-gravity duality provides a robust mathematical framework for studying the behavior of strongly coupled non-abelian plasmas both near and far away from thermodynamic equilibrium. In particular, their near-equilibrium transport coefficients such as viscosity, conductivity, diffusion constants, etc. can be determined from poles of the retarded Green's function which are the dissipative eigenmodes i.e., the quasinormal modes (QNMs) of the dual gravitational field equations. The AdS5/CFT4 correspondence admits the description of a strongly coupled $\\mathcal{N}$= 4 Supersymmetric Yang Mills (SYM) plasma at non-zero temperature as a dual AdS5 black brane geometry. We demonstrate the application of pseudospectral methods to solving the dual Einstein field equations using the example of homogenous isotropization in $\\mathcal{N}$= 4 SYM plasma far from equilibrium. Using this framework, we also compute the quasinormal modes of electrically (Reissner-Nordstrom) and magnetically charged AdS5 black branes for the case of vanishing spatial momenta. The near-extremal behavior of these QNMs is analyzed for both types of black branes.","sentences":["Gauge-gravity duality provides a robust mathematical framework for studying the behavior of strongly coupled non-abelian plasmas both near and far away from thermodynamic equilibrium.","In particular, their near-equilibrium transport coefficients such as viscosity, conductivity, diffusion constants, etc. can be determined from poles of the retarded Green's function which are the dissipative eigenmodes i.e., the quasinormal modes (QNMs) of the dual gravitational field equations.","The AdS5/CFT4 correspondence admits the description of a strongly coupled $\\mathcal{N}$= 4 Supersymmetric Yang Mills (SYM) plasma at non-zero temperature as a dual AdS5 black brane geometry.","We demonstrate the application of pseudospectral methods to solving the dual Einstein field equations using the example of homogenous isotropization in $\\mathcal{N}$= 4 SYM plasma far from equilibrium.","Using this framework, we also compute the quasinormal modes of electrically (Reissner-Nordstrom) and magnetically charged AdS5 black branes for the case of vanishing spatial momenta.","The near-extremal behavior of these QNMs is analyzed for both types of black branes."],"url":"http://arxiv.org/abs/2403.11640v1","category":"hep-th"}
{"created":"2024-03-18 10:20:33","title":"On systems of fractional nonlinear partial differential equations","abstract":"The work considers a system of fractional order partial differential equations. The existence and uniqueness theorems for the classical solution of initial-boundary value problems are proved in two cases: 1) the right-hand side of the equation does not depend on the solution of the problem and 2) it depends on the solution, but at the same time satisfies the classical Lipschitz condition with respect to this variable and an additional condition which guarantees a global existence of the solution. Sufficient conditions are found (in some cases they are necessary) on the initial function and on the right-hand side of the equation, which ensure the existence of a classical solution. In previously known works, linear but more general systems of fractional pseudodifferential equations were considered and the existence of a weak solution was proven in the special classes of distributions.","sentences":["The work considers a system of fractional order partial differential equations.","The existence and uniqueness theorems for the classical solution of initial-boundary value problems are proved in two cases: 1) the right-hand side of the equation does not depend on the solution of the problem and 2) it depends on the solution, but at the same time satisfies the classical Lipschitz condition with respect to this variable and an additional condition which guarantees a global existence of the solution.","Sufficient conditions are found (in some cases they are necessary) on the initial function and on the right-hand side of the equation, which ensure the existence of a classical solution.","In previously known works, linear but more general systems of fractional pseudodifferential equations were considered and the existence of a weak solution was proven in the special classes of distributions."],"url":"http://arxiv.org/abs/2403.11638v1","category":"math.AP"}
{"created":"2024-03-18 10:12:17","title":"Data-driven Stabilization of Nitsche's Method","abstract":"The weak imposition of essential boundary conditions is an integral aspect of unfitted finite element methods, where the physical boundary does not in general coincide with the computational domain. In this regard, the symmetric Nitsche's method is a powerful technique that preserves the symmetry and variational consistency of the unmodified weak formulation. The stabilization parameter in Nitsche's method plays a crucial role in the stability of the resultant formulation, whose estimation is computationally intensive and dependent on the particular cut configuration using the conventional eigenvalue-based approach. In this work, we employ as model problem the finite cell method in which the need for the generation of a boundary-conforming mesh is circumvented by embedding the physical domain in a, typically regular, background mesh. We propose a data-driven estimate based on machine learning methods for the estimation of the stabilization parameter in Nitsche's method that offers an efficient constant-complexity alternative to the eigenvalue-based approach independent of the cut configuration. It is shown, using numerical benchmarks, that the proposed method can estimate the stabilization parameter accurately and is by far more computationally efficient. The data-driven estimate can be integrated into existing numerical codes with minimal modifications and thanks to the wide adoption of accelerators such as GPUs by machine learning frameworks, can be used with virtually no extra implementation cost on GPU devices, further increasing the potential for computational gains over the conventional eigenvalue-based estimate.","sentences":["The weak imposition of essential boundary conditions is an integral aspect of unfitted finite element methods, where the physical boundary does not in general coincide with the computational domain.","In this regard, the symmetric Nitsche's method is a powerful technique that preserves the symmetry and variational consistency of the unmodified weak formulation.","The stabilization parameter in Nitsche's method plays a crucial role in the stability of the resultant formulation, whose estimation is computationally intensive and dependent on the particular cut configuration using the conventional eigenvalue-based approach.","In this work, we employ as model problem the finite cell method in which the need for the generation of a boundary-conforming mesh is circumvented by embedding the physical domain in a, typically regular, background mesh.","We propose a data-driven estimate based on machine learning methods for the estimation of the stabilization parameter in Nitsche's method that offers an efficient constant-complexity alternative to the eigenvalue-based approach independent of the cut configuration.","It is shown, using numerical benchmarks, that the proposed method can estimate the stabilization parameter accurately and is by far more computationally efficient.","The data-driven estimate can be integrated into existing numerical codes with minimal modifications and thanks to the wide adoption of accelerators such as GPUs by machine learning frameworks, can be used with virtually no extra implementation cost on GPU devices, further increasing the potential for computational gains over the conventional eigenvalue-based estimate."],"url":"http://arxiv.org/abs/2403.11632v1","category":"math.NA"}
{"created":"2024-03-18 10:09:28","title":"Compositional Kronecker Context Optimization for Vision-Language Models","abstract":"Context Optimization (CoOp) has emerged as a simple yet effective technique for adapting CLIP-like vision-language models to downstream image recognition tasks. Nevertheless, learning compact context with satisfactory base-to-new, domain and cross-task generalization ability while adapting to new tasks is still a challenge. To tackle such a challenge, we propose a lightweight yet generalizable approach termed Compositional Kronecker Context Optimization (CK-CoOp). Technically, the prompt's context words in CK-CoOp are learnable vectors, which are crafted by linearly combining base vectors sourced from a dictionary. These base vectors consist of a non-learnable component obtained by quantizing the weights in the token embedding layer, and a learnable component constructed by applying Kronecker product on several learnable tiny matrices. Intuitively, the compositional structure mitigates the risk of overfitting on training data by remembering more pre-trained knowledge. Meantime, the Kronecker product breaks the non-learnable restrictions of the dictionary, thereby enhancing representation ability with minimal additional parameters. Extensive experiments confirm that CK-CoOp achieves state-of-the-art performance under base-to-new, domain and cross-task generalization evaluation, but also has the metrics of fewer learnable parameters and efficient training and inference speed.","sentences":["Context Optimization (CoOp) has emerged as a simple yet effective technique for adapting CLIP-like vision-language models to downstream image recognition tasks.","Nevertheless, learning compact context with satisfactory base-to-new, domain and cross-task generalization ability while adapting to new tasks is still a challenge.","To tackle such a challenge, we propose a lightweight yet generalizable approach termed Compositional Kronecker Context Optimization (CK-CoOp).","Technically, the prompt's context words in CK-CoOp are learnable vectors, which are crafted by linearly combining base vectors sourced from a dictionary.","These base vectors consist of a non-learnable component obtained by quantizing the weights in the token embedding layer, and a learnable component constructed by applying Kronecker product on several learnable tiny matrices.","Intuitively, the compositional structure mitigates the risk of overfitting on training data by remembering more pre-trained knowledge.","Meantime, the Kronecker product breaks the non-learnable restrictions of the dictionary, thereby enhancing representation ability with minimal additional parameters.","Extensive experiments confirm that CK-CoOp achieves state-of-the-art performance under base-to-new, domain and cross-task generalization evaluation, but also has the metrics of fewer learnable parameters and efficient training and inference speed."],"url":"http://arxiv.org/abs/2403.11631v1","category":"cs.CV"}
{"created":"2024-03-18 10:00:26","title":"GLACE survey: OSIRIS/GTC tuneable imaging of the galaxy cluster ZwCl 0024.0+1652 II. The mass--metallicity relationship and the effect of the environment","abstract":"In this paper, we revisit the data for the galaxy cluster ZwCl 0024.0+1652 provided by the GLACE survey and study the mass--metallicity function and its relationship with the environment. Here we describe an alternative way to reduce the data from OSIRIS tunable filters. This method gives us better uncertainties in the fluxes of the emission lines and the derived quantities. We present an updated catalogue of cluster galaxies with emission in H$\\alpha$ and [N\\,{\\sc{ii}}] $\\lambda\\lambda$6548,6583. We also discuss the biases of these new fluxes and describe the way in which we calculated the mass--metallicity relationship and its uncertainties. We generated a new catalogue of 84 emission-line galaxies with reliable fluxes in [N\\,{\\sc{ii}}] and H$\\alpha$ lines from a list of 174 galaxies. We find a relationship between the clustercentric radius and the density of galaxies. We derived the mass--metallicity relationship for ZwCl 0024.0+1652 and compared it with clusters and field galaxies from the literature. We find a difference in the mass--metallicity relationship when compared to more massive clusters, with the latter showing on average higher values of abundance. This could be an effect of the quenching of the star formation, which seems to be more prevalent in low-mass galaxies in more massive clusters. We find little to no difference between ZwCl 0024.0+1652 galaxies and field galaxies located at the same redshift.","sentences":["In this paper, we revisit the data for the galaxy cluster ZwCl 0024.0+1652 provided by the GLACE survey and study the mass--metallicity function and its relationship with the environment.","Here we describe an alternative way to reduce the data from OSIRIS tunable filters.","This method gives us better uncertainties in the fluxes of the emission lines and the derived quantities.","We present an updated catalogue of cluster galaxies with emission in H$\\alpha$ and [N\\,{\\sc{ii}}] $\\lambda\\lambda$6548,6583.","We also discuss the biases of these new fluxes and describe the way in which we calculated the mass--metallicity relationship and its uncertainties.","We generated a new catalogue of 84 emission-line galaxies with reliable fluxes in [N\\,{\\sc{ii}}] and H$\\alpha$ lines from a list of 174 galaxies.","We find a relationship between the clustercentric radius and the density of galaxies.","We derived the mass--metallicity relationship for ZwCl 0024.0+1652 and compared it with clusters and field galaxies from the literature.","We find a difference in the mass--metallicity relationship when compared to more massive clusters, with the latter showing on average higher values of abundance.","This could be an effect of the quenching of the star formation, which seems to be more prevalent in low-mass galaxies in more massive clusters.","We find little to no difference between ZwCl 0024.0+1652 galaxies and field galaxies located at the same redshift."],"url":"http://arxiv.org/abs/2403.11629v1","category":"astro-ph.GA"}
{"created":"2024-03-18 09:58:52","title":"LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models","abstract":"Customization generation techniques have significantly advanced the synthesis of specific concepts across varied contexts. Multi-concept customization emerges as the challenging task within this domain. Existing approaches often rely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRA to merge various concepts into a single image. However, we identify this straightforward method faces two major challenges: 1) concept confusion, which occurs when the model cannot preserve distinct individual characteristics, and 2) concept vanishing, where the model fails to generate the intended subjects. To address these issues, we introduce LoRA-Composer, a training-free framework designed for seamlessly integrating multiple LoRAs, thereby enhancing the harmony among different concepts within generated images. LoRA-Composer addresses concept vanishing through Concept Injection Constraints, enhancing concept visibility via an expanded cross-attention mechanism. To combat concept confusion, Concept Isolation Constraints are introduced, refining the self-attention computation. Furthermore, Latent Re-initialization is proposed to effectively stimulate concept-specific latent within designated regions. Our extensive testing showcases a notable enhancement in LoRA-Composer's performance compared to standard baselines, especially when eliminating the image-based conditions like canny edge or pose estimations. Code is released at https://github.com/Young98CN/LoRA\\_Composer.","sentences":["Customization generation techniques have significantly advanced the synthesis of specific concepts across varied contexts.","Multi-concept customization emerges as the challenging task within this domain.","Existing approaches often rely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRA to merge various concepts into a single image.","However, we identify this straightforward method faces two major challenges: 1) concept confusion, which occurs when the model cannot preserve distinct individual characteristics, and 2) concept vanishing, where the model fails to generate the intended subjects.","To address these issues, we introduce LoRA-Composer, a training-free framework designed for seamlessly integrating multiple LoRAs, thereby enhancing the harmony among different concepts within generated images.","LoRA-Composer addresses concept vanishing through Concept Injection Constraints, enhancing concept visibility via an expanded cross-attention mechanism.","To combat concept confusion, Concept Isolation Constraints are introduced, refining the self-attention computation.","Furthermore, Latent Re-initialization is proposed to effectively stimulate concept-specific latent within designated regions.","Our extensive testing showcases a notable enhancement in LoRA-Composer's performance compared to standard baselines, especially when eliminating the image-based conditions like canny edge or pose estimations.","Code is released at https://github.com/Young98CN/LoRA\\_Composer."],"url":"http://arxiv.org/abs/2403.11627v1","category":"cs.CV"}
{"created":"2024-03-18 09:58:43","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","abstract":"The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and improved understanding of the connection between music and dance. Second, QRA represents and fuses 3D motion features and audio features in the form of a series of quaternions, enabling the model to better learn the temporal coordination of music and dance under the complex temporal cycle conditions of dance generation. Finally, we conducted experiments on the dataset AIST++, and the results show that our approach achieves better and more robust performance in generating accurate, high-quality dance movements. Our source code and dataset can be available from https://github.com/MarasyZZ/QEAN and https://google.github.io/aistplusplus_dataset respectively.","sentences":["The study of music-generated dance is a novel and challenging Image generation task.","It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music.","Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects.","This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music.","In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module.","First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and improved understanding of the connection between music and dance.","Second, QRA represents and fuses 3D motion features and audio features in the form of a series of quaternions, enabling the model to better learn the temporal coordination of music and dance under the complex temporal cycle conditions of dance generation.","Finally, we conducted experiments on the dataset AIST++, and the results show that our approach achieves better and more robust performance in generating accurate, high-quality dance movements.","Our source code and dataset can be available from https://github.com/MarasyZZ/QEAN and https://google.github.io/aistplusplus_dataset respectively."],"url":"http://arxiv.org/abs/2403.11626v1","category":"cs.GR"}
{"created":"2024-03-18 09:44:44","title":"CRS-Diff: Controllable Generative Remote Sensing Foundation Model","abstract":"The emergence of diffusion models has revolutionized the field of image generation, providing new methods for creating high-quality, high-resolution images across various applications. However, the potential of these models for generating domain-specific images, particularly remote sensing (RS) images, remains largely untapped. RS images that are notable for their high resolution, extensive coverage, and rich information content, bring new challenges that general diffusion models may not adequately address. This paper proposes CRS-Diff, a pioneering diffusion modeling framework specifically tailored for generating remote sensing imagery, leveraging the inherent advantages of diffusion models while integrating advanced control mechanisms to ensure that the imagery is not only visually clear but also enriched with geographic and temporal information. The model integrates global and local control inputs, enabling precise combinations of generation conditions to refine the generation process. A comprehensive evaluation of CRS-Diff has demonstrated its superior capability to generate RS imagery both in a single condition and multiple conditions compared with previous methods in terms of image quality and diversity.","sentences":["The emergence of diffusion models has revolutionized the field of image generation, providing new methods for creating high-quality, high-resolution images across various applications.","However, the potential of these models for generating domain-specific images, particularly remote sensing (RS) images, remains largely untapped.","RS images that are notable for their high resolution, extensive coverage, and rich information content, bring new challenges that general diffusion models may not adequately address.","This paper proposes CRS-Diff, a pioneering diffusion modeling framework specifically tailored for generating remote sensing imagery, leveraging the inherent advantages of diffusion models while integrating advanced control mechanisms to ensure that the imagery is not only visually clear but also enriched with geographic and temporal information.","The model integrates global and local control inputs, enabling precise combinations of generation conditions to refine the generation process.","A comprehensive evaluation of CRS-Diff has demonstrated its superior capability to generate RS imagery both in a single condition and multiple conditions compared with previous methods in terms of image quality and diversity."],"url":"http://arxiv.org/abs/2403.11614v1","category":"cs.CV"}
{"created":"2024-03-18 09:42:46","title":"Scattering Singularity in Topological Dielectric Photonic Crystals","abstract":"The exploration of topology in natural materials and metamaterials has garnered significant attention. Notably, the one-dimensional (1D) and two-dimensional (2D) Su-Schrieffer-Heeger (SSH) model, assessed through tight-binding approximations, has been extensively investigated in both quantum and classical systems, encompassing general and higher-order topology. Despite these advancements, a comprehensive examination of these models from the perspective of wave physics, particularly the scattering view, remains underexplored. In this study, we systematically unveil the origin of the 1D and 2D Zak phases stemming from the zero-scattering point, termed the scattering singularity in k-space. Employing an expanded plane wave expansion, we accurately compute the reflective spectrum of an infinite 2D photonic crystal (2D-PhC). Analyzing the reflective spectrum reveals the presence of a zero-scattering line in the 2D-PhC, considered the topological origin of the non-trivial Zak phase. Two distinct models, representing omnidirectional non-trivial cases and directional non-trivial cases, are employed to substantiate these findings. Our work introduces a novel perspective for characterizing the nature of non-trivial topological phases. The identification of the zero-scattering line not only enhances our understanding of the underlying physics but also provides valuable insights for the design of innovative devices.","sentences":["The exploration of topology in natural materials and metamaterials has garnered significant attention.","Notably, the one-dimensional (1D) and two-dimensional (2D) Su-Schrieffer-Heeger (SSH) model, assessed through tight-binding approximations, has been extensively investigated in both quantum and classical systems, encompassing general and higher-order topology.","Despite these advancements, a comprehensive examination of these models from the perspective of wave physics, particularly the scattering view, remains underexplored.","In this study, we systematically unveil the origin of the 1D and 2D Zak phases stemming from the zero-scattering point, termed the scattering singularity in k-space.","Employing an expanded plane wave expansion, we accurately compute the reflective spectrum of an infinite 2D photonic crystal (2D-PhC).","Analyzing the reflective spectrum reveals the presence of a zero-scattering line in the 2D-PhC, considered the topological origin of the non-trivial Zak phase.","Two distinct models, representing omnidirectional non-trivial cases and directional non-trivial cases, are employed to substantiate these findings.","Our work introduces a novel perspective for characterizing the nature of non-trivial topological phases.","The identification of the zero-scattering line not only enhances our understanding of the underlying physics but also provides valuable insights for the design of innovative devices."],"url":"http://arxiv.org/abs/2403.11613v1","category":"physics.optics"}
{"created":"2024-03-18 09:40:53","title":"Shadow Hamiltonians of structure-preserving integrators for Nambu mechanics","abstract":"Symplectic integrators are widely implemented numerical integrators for Hamiltonian mechanics, which preserve the Hamiltonian structure (symplecticity) of the system. Although the symplectic integrator does not conserve the energy of the system, it is well known that there exists a conserving modified Hamiltonian, called the shadow Hamiltonian. For the Nambu mechanics, which is one of the generalized Hamiltonian mechanics, we can also construct structure-preserving integrators by the same procedure used to construct the symplectic integrators. In the structure-preserving integrator, however, the existence of shadow Hamiltonians is non-trivial. This is because the Nambu mechanics is driven by multiple Hamiltonians and it is non-trivial whether the time evolution by the integrator can be cast into the Nambu mechanical time evolution driven by multiple shadow Hamiltonians. In the present paper we construct structure-preserving integrators for a simple Nambu mechanical system, and derive the shadow Hamiltonians in two ways. This is the first attempt to derive shadow Hamiltonians of structure-preserving integrators for Nambu mechanics. We show that the fundamental identity, which corresponds to the Jacobi identity in Hamiltonian mechanics, plays an important role to calculate the shadow Hamiltonians using the Baker-Campbell-Hausdorff formula. It turns out that the resulting shadow Hamiltonians have indefinite forms depending on how the fundamental identities are used. This is not a technical artifact, because the exact shadow Hamiltonians obtained independently have the same indefiniteness.","sentences":["Symplectic integrators are widely implemented numerical integrators for Hamiltonian mechanics, which preserve the Hamiltonian structure (symplecticity) of the system.","Although the symplectic integrator does not conserve the energy of the system, it is well known that there exists a conserving modified Hamiltonian, called the shadow Hamiltonian.","For the Nambu mechanics, which is one of the generalized Hamiltonian mechanics, we can also construct structure-preserving integrators by the same procedure used to construct the symplectic integrators.","In the structure-preserving integrator, however, the existence of shadow Hamiltonians is non-trivial.","This is because the Nambu mechanics is driven by multiple Hamiltonians and it is non-trivial whether the time evolution by the integrator can be cast into the Nambu mechanical time evolution driven by multiple shadow Hamiltonians.","In the present paper we construct structure-preserving integrators for a simple Nambu mechanical system, and derive the shadow Hamiltonians in two ways.","This is the first attempt to derive shadow Hamiltonians of structure-preserving integrators for Nambu mechanics.","We show that the fundamental identity, which corresponds to the Jacobi identity in Hamiltonian mechanics, plays an important role to calculate the shadow Hamiltonians using the Baker-Campbell-Hausdorff formula.","It turns out that the resulting shadow Hamiltonians have indefinite forms depending on how the fundamental identities are used.","This is not a technical artifact, because the exact shadow Hamiltonians obtained independently have the same indefiniteness."],"url":"http://arxiv.org/abs/2403.11612v1","category":"math-ph"}
{"created":"2024-03-18 09:34:13","title":"On the weak and strong field effects in antiscalar background","abstract":"The triumph of general relativity under the banner \"gravity is geometry\" began with confirming the crucial effects within the Solar system and proceeded recently to the strong-field shadow effect for the compact object in the center of the Milky Way. Here, we examine some of those phenomena for the Einstein-scalar equations in the antiscalar regime to reveal the difference from vacuum both in weak and strong fields. As a result, we find that for week-field perihelion shift the difference between vacuum and antiscalar cases proves to be observationally imperceptible in practice, even for S-cluster stars with high eccentricities, and even if accumulated over a century. In strong-field case, we reconsider the shadow effect (this time without involving complex-valued scalar field) as the most perspective from an observational viewpoint. Even though the resulting difference is quite appreciable (about 5%), no conclusion can be made until the mass of the central object is known with the accuracy an order of magnitude higher than the currently available.","sentences":["The triumph of general relativity under the banner \"gravity is geometry\" began with confirming the crucial effects within the Solar system and proceeded recently to the strong-field shadow effect for the compact object in the center of the Milky Way.","Here, we examine some of those phenomena for the Einstein-scalar equations in the antiscalar regime to reveal the difference from vacuum both in weak and strong fields.","As a result, we find that for week-field perihelion shift the difference between vacuum and antiscalar cases proves to be observationally imperceptible in practice, even for S-cluster stars with high eccentricities, and even if accumulated over a century.","In strong-field case, we reconsider the shadow effect (this time without involving complex-valued scalar field) as the most perspective from an observational viewpoint.","Even though the resulting difference is quite appreciable (about 5%), no conclusion can be made until the mass of the central object is known with the accuracy an order of magnitude higher than the currently available."],"url":"http://arxiv.org/abs/2403.11610v1","category":"gr-qc"}
{"created":"2024-03-18 09:28:45","title":"FLRW Cosmology in Myrzakulov $F(R,Q)$ Gravity","abstract":"In the present work, we investigate some exact cosmological models in the Myrzakulov $F(R,Q)$ gravity or the Myrzakulov gravity-II (MG-II) which was proposed in [arXiv:1205.5266]. Here $R$ and $Q$ are the curvature and nonmetricity scalars using non-special connection, respectively. We have solved field equations in two different contexts using a flat FLRW metric. We have obtained two exact solutions in the form of scale factor $a(t)$, and using this scale factor, we have derived other cosmological parameters. After that using recent observational datasets $H(z)$ and Pantheon SNe Ia, we have obtained best fit constrained values of model parameters by applying the MCMC analysis. Using these best fit values of model parameters, we have discussed the results behaviour of the derived models. We have found that both models are transit phase model and approaches to $\\Lambda$CDM model at late-time universe. We have found that the geometrical sector dark equation of state $\\omega_{(geom)}$ behaves just like dark energy candidate. Also, we have estimated the transition redshift $z_{t}$ and present age of the universe $t_{0}$ which are consistent with recent observations.","sentences":["In the present work, we investigate some exact cosmological models in the Myrzakulov $F(R,Q)$ gravity or the Myrzakulov gravity-II (MG-II) which was proposed in [arXiv:1205.5266].","Here $R$ and $Q$ are the curvature and nonmetricity scalars using non-special connection, respectively.","We have solved field equations in two different contexts using a flat FLRW metric.","We have obtained two exact solutions in the form of scale factor $a(t)$, and using this scale factor, we have derived other cosmological parameters.","After that using recent observational datasets $H(z)$ and Pantheon SNe Ia, we have obtained best fit constrained values of model parameters by applying the MCMC analysis.","Using these best fit values of model parameters, we have discussed the results behaviour of the derived models.","We have found that both models are transit phase model and approaches to $\\Lambda$CDM model at late-time universe.","We have found that the geometrical sector dark equation of state $\\omega_{(geom)}$ behaves just like dark energy candidate.","Also, we have estimated the transition redshift $z_{t}$ and present age of the universe $t_{0}$ which are consistent with recent observations."],"url":"http://arxiv.org/abs/2403.11604v1","category":"gr-qc"}
{"created":"2024-03-18 09:25:59","title":"Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)","abstract":"In intelligent Internet of Things (IoT) systems, edge servers within a network exchange information with their neighbors and collect data from sensors to complete delivered tasks. In this paper, we propose a multiplayer multi-armed bandit model for intelligent IoT systems to facilitate data collection and incorporate fairness considerations. In our model, we establish an effective communication protocol that helps servers cooperate with their neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB, enabling servers to collaboratively select sensors to maximize data rates while maintaining fairness in their choices. We conduct an analysis of the reward regret and fairness regret of DC-ULCB, and prove that both regrets have logarithmic instance-dependent upper bounds. Additionally, through extensive simulations, we validate that DC-ULCB outperforms existing algorithms in maximizing reward and ensuring fairness.","sentences":["In intelligent Internet of Things (IoT) systems, edge servers within a network exchange information with their neighbors and collect data from sensors to complete delivered tasks.","In this paper, we propose a multiplayer multi-armed bandit model for intelligent IoT systems to facilitate data collection and incorporate fairness considerations.","In our model, we establish an effective communication protocol that helps servers cooperate with their neighbors.","Then we design a distributed cooperative bandit algorithm, DC-ULCB, enabling servers to collaboratively select sensors to maximize data rates while maintaining fairness in their choices.","We conduct an analysis of the reward regret and fairness regret of DC-ULCB, and prove that both regrets have logarithmic instance-dependent upper bounds.","Additionally, through extensive simulations, we validate that DC-ULCB outperforms existing algorithms in maximizing reward and ensuring fairness."],"url":"http://arxiv.org/abs/2403.11603v1","category":"cs.DC"}
{"created":"2024-03-18 09:23:38","title":"Layer potential operators for transmission problems on extension domains","abstract":"We use the well-posedness of transmission problems on classes of two-sided Sobolevextension domains to give variational definitions for (boundary) layer potential oper-ators and Neumann-Poincar{\\'e} operators. These classes of domains contain Lipschitzdomains, but also domains with fractal boundaries. Our formulation does not involveany measures on the boundary. We discuss basic properties of these operators and usethem to generalize basic results in imaging beyond the Lipschitz case.","sentences":["We use the well-posedness of transmission problems on classes of two-sided Sobolevextension domains to give variational definitions for (boundary) layer potential oper-ators and Neumann-Poincar{\\'e} operators.","These classes of domains contain Lipschitzdomains, but also domains with fractal boundaries.","Our formulation does not involveany measures on the boundary.","We discuss basic properties of these operators and usethem to generalize basic results in imaging beyond the Lipschitz case."],"url":"http://arxiv.org/abs/2403.11601v1","category":"math.AP"}
{"created":"2024-03-18 09:19:01","title":"Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits","abstract":"Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP gate insertions are needed for scheduling 2-qubit gates only on connected physical qubits. With the ever-increasing number of qubits in NISQ processors, scalable layout synthesis is of utmost importance. With large optimality gaps observed in heuristic approaches, scalable exact methods are needed. While recent exact and near-optimal approaches scale to moderate circuits, large deep circuits are still out of scope.   In this work, we propose a SAT encoding based on parallel plans that apply 1 SWAP and a group of CNOTs at each time step. Using domain-specific information, we maintain optimality in parallel plans while scaling to large and deep circuits. From our results, we show the scalability of our approach which significantly outperforms leading exact and near-optimal approaches (up to 100x). For the first time, we can optimally map several 8, 14, and 16 qubit circuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding optimal SWAPs, we also report near-optimal depth in our mapped circuits.","sentences":["Layout synthesis is mapping a quantum circuit to a quantum processor.","SWAP gate insertions are needed for scheduling 2-qubit gates only on connected physical qubits.","With the ever-increasing number of qubits in NISQ processors, scalable layout synthesis is of utmost importance.","With large optimality gaps observed in heuristic approaches, scalable exact methods are needed.","While recent exact and near-optimal approaches scale to moderate circuits, large deep circuits are still out of scope.   ","In this work, we propose a SAT encoding based on parallel plans that apply 1 SWAP and a group of CNOTs at each time step.","Using domain-specific information, we maintain optimality in parallel plans while scaling to large and deep circuits.","From our results, we show the scalability of our approach which significantly outperforms leading exact and near-optimal approaches (up to 100x).","For the first time, we can optimally map several 8, 14, and 16 qubit circuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs.","While adding optimal SWAPs, we also report near-optimal depth in our mapped circuits."],"url":"http://arxiv.org/abs/2403.11598v1","category":"quant-ph"}
{"created":"2024-03-18 09:10:39","title":"A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs","abstract":"We present a physics-informed neural network (PINN) approach for the discovery of slow invariant manifolds (SIMs), for the most general class of fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML) approaches that construct reduced order black box surrogate models using simple regression, and/or require a priori knowledge of the fast and slow variables, our approach, simultaneously decomposes the vector field into fast and slow components and provides a functional of the underlying SIM in a closed form. The decomposition is achieved by finding a transformation of the state variables to the fast and slow ones, which enables the derivation of an explicit, in terms of fast variables, SIM functional. The latter is obtained by solving a PDE corresponding to the invariance equation within the Geometric Singular Perturbation Theory (GSPT) using a single-layer feedforward neural network with symbolic differentiation. The performance of the proposed physics-informed ML framework is assessed via three benchmark problems: the Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model and a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a comparison with other GPST methods, namely the quasi steady state approximation (QSSA), the partial equilibrium approximation (PEA) and CSP with one and two iterations. We show that the proposed PINN scheme provides SIM approximations, of equivalent or even higher accuracy, than those provided by QSSA, PEA and CSP, especially close to the boundaries of the underlying SIMs.","sentences":["We present a physics-informed neural network (PINN) approach for the discovery of slow invariant manifolds (SIMs), for the most general class of fast/slow dynamical systems of ODEs.","In contrast to other machine learning (ML) approaches that construct reduced order black box surrogate models using simple regression, and/or require a priori knowledge of the fast and slow variables, our approach, simultaneously decomposes the vector field into fast and slow components and provides a functional of the underlying SIM in a closed form.","The decomposition is achieved by finding a transformation of the state variables to the fast and slow ones, which enables the derivation of an explicit, in terms of fast variables, SIM functional.","The latter is obtained by solving a PDE corresponding to the invariance equation within the Geometric Singular Perturbation Theory (GSPT) using a single-layer feedforward neural network with symbolic differentiation.","The performance of the proposed physics-informed ML framework is assessed via three benchmark problems: the Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model and a fully competitive substrate-inhibitor(fCSI) mechanism.","We also provide a comparison with other GPST methods, namely the quasi steady state approximation (QSSA), the partial equilibrium approximation (PEA) and CSP with one and two iterations.","We show that the proposed PINN scheme provides SIM approximations, of equivalent or even higher accuracy, than those provided by QSSA, PEA and CSP, especially close to the boundaries of the underlying SIMs."],"url":"http://arxiv.org/abs/2403.11591v1","category":"math.NA"}
{"created":"2024-03-18 08:59:17","title":"One Axis Twisting (OAT) spin squeezing for metrology","abstract":"In this work we study One Axis Twisting (OAT) spin squeezing for metrology in the presence of decoherence. We study Linbladian evolution in the presence of both T_1 and T_2 (longitudinal and transverse relaxation processes). We show that spin squeezing can be an effective way to improve metrological accuracy even in the presence of decoherence for OAT squeezing. We show our results are not sensitive to inhomogeneity of the squeezing strength of the many spin OAT Hamiltonian and that very general squeezed states do not have entanglement enhanced decoherence. We also extend the Kitagawa-Ueda OAT squeezing formula to finite polarization.","sentences":["In this work we study One Axis Twisting (OAT) spin squeezing for metrology in the presence of decoherence.","We study Linbladian evolution in the presence of both T_1 and T_2 (longitudinal and transverse relaxation processes).","We show that spin squeezing can be an effective way to improve metrological accuracy even in the presence of decoherence for OAT squeezing.","We show our results are not sensitive to inhomogeneity of the squeezing strength of the many spin OAT Hamiltonian and that very general squeezed states do not have entanglement enhanced decoherence.","We also extend the Kitagawa-Ueda OAT squeezing formula to finite polarization."],"url":"http://arxiv.org/abs/2403.11587v1","category":"quant-ph"}
{"created":"2024-03-18 08:58:47","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines","abstract":"In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.","sentences":["In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge.","This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions.","The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task.","This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code.","Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code.","It holds great promise for advancing machine learning applications across diverse domains.","Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction.","In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus.","The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields."],"url":"http://arxiv.org/abs/2403.11585v1","category":"cs.LG"}
{"created":"2024-03-18 08:53:04","title":"AdaMER-CTC: Connectionist Temporal Classification with Adaptive Maximum Entropy Regularization for Automatic Speech Recognition","abstract":"In Automatic Speech Recognition (ASR) systems, a recurring obstacle is the generation of narrowly focused output distributions. This phenomenon emerges as a side effect of Connectionist Temporal Classification (CTC), a robust sequence learning tool that utilizes dynamic programming for sequence mapping. While earlier efforts have tried to combine the CTC loss with an entropy maximization regularization term to mitigate this issue, they employed a constant weighting term on the regularization during the training, which we find may not be optimal. In this work, we introduce Adaptive Maximum Entropy Regularization (AdaMER), a technique that can modulate the impact of entropy regularization throughout the training process. This approach not only refines ASR model training but ensures that as training proceeds, predictions display the desired model confidence.","sentences":["In Automatic Speech Recognition (ASR) systems, a recurring obstacle is the generation of narrowly focused output distributions.","This phenomenon emerges as a side effect of Connectionist Temporal Classification (CTC), a robust sequence learning tool that utilizes dynamic programming for sequence mapping.","While earlier efforts have tried to combine the CTC loss with an entropy maximization regularization term to mitigate this issue, they employed a constant weighting term on the regularization during the training, which we find may not be optimal.","In this work, we introduce Adaptive Maximum Entropy Regularization (AdaMER), a technique that can modulate the impact of entropy regularization throughout the training process.","This approach not only refines ASR model training but ensures that as training proceeds, predictions display the desired model confidence."],"url":"http://arxiv.org/abs/2403.11578v1","category":"eess.AS"}
{"created":"2024-03-18 08:52:23","title":"MISS: Memory-efficient Instance Segmentation Framework By Visual Inductive Priors Flow Propagation","abstract":"Instance segmentation, a cornerstone task in computer vision, has wide-ranging applications in diverse industries. The advent of deep learning and artificial intelligence has underscored the criticality of training effective models, particularly in data-scarce scenarios - a concern that resonates in both academic and industrial circles. A significant impediment in this domain is the resource-intensive nature of procuring high-quality, annotated data for instance segmentation, a hurdle that amplifies the challenge of developing robust models under resource constraints. In this context, the strategic integration of a visual prior into the training dataset emerges as a potential solution to enhance congruity with the testing data distribution, consequently reducing the dependency on computational resources and the need for highly complex models. However, effectively embedding a visual prior into the learning process remains a complex endeavor. Addressing this challenge, we introduce the MISS (Memory-efficient Instance Segmentation System) framework. MISS leverages visual inductive prior flow propagation, integrating intrinsic prior knowledge from the Synergy-basketball dataset at various stages: data preprocessing, augmentation, training, and inference. Our empirical evaluations underscore the efficacy of MISS, demonstrating commendable performance in scenarios characterized by limited data availability and memory constraints.","sentences":["Instance segmentation, a cornerstone task in computer vision, has wide-ranging applications in diverse industries.","The advent of deep learning and artificial intelligence has underscored the criticality of training effective models, particularly in data-scarce scenarios - a concern that resonates in both academic and industrial circles.","A significant impediment in this domain is the resource-intensive nature of procuring high-quality, annotated data for instance segmentation, a hurdle that amplifies the challenge of developing robust models under resource constraints.","In this context, the strategic integration of a visual prior into the training dataset emerges as a potential solution to enhance congruity with the testing data distribution, consequently reducing the dependency on computational resources and the need for highly complex models.","However, effectively embedding a visual prior into the learning process remains a complex endeavor.","Addressing this challenge, we introduce the MISS (Memory-efficient Instance Segmentation System) framework.","MISS leverages visual inductive prior flow propagation, integrating intrinsic prior knowledge from the Synergy-basketball dataset at various stages: data preprocessing, augmentation, training, and inference.","Our empirical evaluations underscore the efficacy of MISS, demonstrating commendable performance in scenarios characterized by limited data availability and memory constraints."],"url":"http://arxiv.org/abs/2403.11576v1","category":"cs.CV"}
{"created":"2024-03-18 08:50:04","title":"Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem","abstract":"Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication.","sentences":["Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed).","To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity.","In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes.","Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information.","We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations.","Our code and data will be publicly available upon publication."],"url":"http://arxiv.org/abs/2403.11573v1","category":"cs.CV"}
{"created":"2024-03-18 08:44:40","title":"Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes","abstract":"Instance segmentation is a fundamental task in computer vision with broad applications across various industries. In recent years, with the proliferation of deep learning and artificial intelligence applications, how to train effective models with limited data has become a pressing issue for both academia and industry. In the Visual Inductive Priors challenge (VIPriors2023), participants must train a model capable of precisely locating individuals on a basketball court, all while working with limited data and without the use of transfer learning or pre-trained models. We propose Memory effIciency inStance Segmentation framework based on visual inductive prior flow propagation that effectively incorporates inherent prior information from the dataset into both the data preprocessing and data augmentation stages, as well as the inference phase. Our team (ACVLAB) experiments demonstrate that our model achieves promising performance (0.509 AP@0.50:0.95) even under limited data and memory constraints.","sentences":["Instance segmentation is a fundamental task in computer vision with broad applications across various industries.","In recent years, with the proliferation of deep learning and artificial intelligence applications, how to train effective models with limited data has become a pressing issue for both academia and industry.","In the Visual Inductive Priors challenge (VIPriors2023), participants must train a model capable of precisely locating individuals on a basketball court, all while working with limited data and without the use of transfer learning or pre-trained models.","We propose Memory effIciency inStance Segmentation framework based on visual inductive prior flow propagation that effectively incorporates inherent prior information from the dataset into both the data preprocessing and data augmentation stages, as well as the inference phase.","Our team (ACVLAB) experiments demonstrate that our model achieves promising performance (0.509 AP@0.50:0.95) even under limited data and memory constraints."],"url":"http://arxiv.org/abs/2403.11572v1","category":"cs.CV"}
{"created":"2024-03-18 08:43:42","title":"LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge","abstract":"Large text-to-image models have achieved astonishing performance in synthesizing diverse and high-quality images guided by texts. With detail-oriented conditioning control, even finer-grained spatial control can be achieved. However, some generated images still appear unreasonable, even with plentiful object features and a harmonious style. In this paper, we delve into the underlying causes and find that deep-level logical information, serving as common-sense knowledge, plays a significant role in understanding and processing images. Nonetheless, almost all models have neglected the importance of logical relations in images, resulting in poor performance in this aspect. Following this observation, we propose LogicalDefender, which combines images with the logical knowledge already summarized by humans in text. This encourages models to learn logical knowledge faster and better, and concurrently, extracts the widely applicable logical knowledge from both images and human knowledge. Experiments show that our model has achieved better logical performance, and the extracted logical knowledge can be effectively applied to other scenarios.","sentences":["Large text-to-image models have achieved astonishing performance in synthesizing diverse and high-quality images guided by texts.","With detail-oriented conditioning control, even finer-grained spatial control can be achieved.","However, some generated images still appear unreasonable, even with plentiful object features and a harmonious style.","In this paper, we delve into the underlying causes and find that deep-level logical information, serving as common-sense knowledge, plays a significant role in understanding and processing images.","Nonetheless, almost all models have neglected the importance of logical relations in images, resulting in poor performance in this aspect.","Following this observation, we propose LogicalDefender, which combines images with the logical knowledge already summarized by humans in text.","This encourages models to learn logical knowledge faster and better, and concurrently, extracts the widely applicable logical knowledge from both images and human knowledge.","Experiments show that our model has achieved better logical performance, and the extracted logical knowledge can be effectively applied to other scenarios."],"url":"http://arxiv.org/abs/2403.11570v1","category":"cs.CV"}
{"created":"2024-03-18 08:42:53","title":"Sensitivity Assessment of Multi-Criteria Decision-Making Methods in Chemical Engineering Optimization Applications","abstract":"This chapter assesses the sensitivity of multi-criteria decision-making (MCDM) methods to modifications within the decision or objective matrix (DOM) in the context of chemical engineering optimization applications. Employing eight common or recent MCDM methods and three weighting methods, this study evaluates the impact of three specific DOM alterations: linear transformation of an objective (LTO), reciprocal objective reformulation (ROR), and the removal of alternatives (RA). Our comprehensive analysis reveals that the weights generated by entropy method are more sensitive to the examined modifications compared to the criteria importance through intercriteria correlation (CRITIC) and standard deviation (StDev) methods. ROR is found to have the largest effect on the ranking of alternatives. Moreover, certain methods, gray relational analysis (GRA) without any weights, multi-attributive border approximation area comparison (MABAC), combinative distance-based assessment (CODAS), and simple additive weighting (SAW) with entropy or CRITIC weights, and CODAS, SAW, and technique for order of preference by similarity to ideal solution (TOPSIS) with StDev weight are more robust to DOM modifications. This investigation not only corroborates the findings from the previous study, but also offers insights into the stability and reliability of MCDM methods in the context of chemical engineering.","sentences":["This chapter assesses the sensitivity of multi-criteria decision-making (MCDM) methods to modifications within the decision or objective matrix (DOM) in the context of chemical engineering optimization applications.","Employing eight common or recent MCDM methods and three weighting methods, this study evaluates the impact of three specific DOM alterations: linear transformation of an objective (LTO), reciprocal objective reformulation (ROR), and the removal of alternatives (RA).","Our comprehensive analysis reveals that the weights generated by entropy method are more sensitive to the examined modifications compared to the criteria importance through intercriteria correlation (CRITIC) and standard deviation (StDev) methods.","ROR is found to have the largest effect on the ranking of alternatives.","Moreover, certain methods, gray relational analysis (GRA) without any weights, multi-attributive border approximation area comparison (MABAC), combinative distance-based assessment (CODAS), and simple additive weighting (SAW) with entropy or CRITIC weights, and CODAS, SAW, and technique for order of preference by similarity to ideal solution (TOPSIS) with StDev weight are more robust to DOM modifications.","This investigation not only corroborates the findings from the previous study, but also offers insights into the stability and reliability of MCDM methods in the context of chemical engineering."],"url":"http://arxiv.org/abs/2403.11569v1","category":"physics.chem-ph"}
{"created":"2024-03-18 08:42:08","title":"EffiVED:Efficient Video Editing via Text-instruction Diffusion Models","abstract":"Large-scale text-to-video models have shown remarkable abilities, but their direct application in video editing remains challenging due to limited available datasets. Current video editing methods commonly require per-video fine-tuning of diffusion models or specific inversion optimization to ensure high-fidelity edits. In this paper, we introduce EffiVED, an efficient diffusion-based model that directly supports instruction-guided video editing. To achieve this, we present two efficient workflows to gather video editing pairs, utilizing augmentation and fundamental vision-language techniques. These workflows transform vast image editing datasets and open-world videos into a high-quality dataset for training EffiVED. Experimental results reveal that EffiVED not only generates high-quality editing videos but also executes rapidly. Finally, we demonstrate that our data collection method significantly improves editing performance and can potentially tackle the scarcity of video editing data. The datasets will be made publicly available upon publication.","sentences":["Large-scale text-to-video models have shown remarkable abilities, but their direct application in video editing remains challenging due to limited available datasets.","Current video editing methods commonly require per-video fine-tuning of diffusion models or specific inversion optimization to ensure high-fidelity edits.","In this paper, we introduce EffiVED, an efficient diffusion-based model that directly supports instruction-guided video editing.","To achieve this, we present two efficient workflows to gather video editing pairs, utilizing augmentation and fundamental vision-language techniques.","These workflows transform vast image editing datasets and open-world videos into a high-quality dataset for training EffiVED.","Experimental results reveal that EffiVED not only generates high-quality editing videos but also executes rapidly.","Finally, we demonstrate that our data collection method significantly improves editing performance and can potentially tackle the scarcity of video editing data.","The datasets will be made publicly available upon publication."],"url":"http://arxiv.org/abs/2403.11568v1","category":"cs.CV"}
{"created":"2024-03-18 08:35:17","title":"Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization","abstract":"In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our proposed framework. Consequently, our convergence results establish, for the first time, global convergence of these methods when applied to nonsmooth nonconvex objectives. Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized subgradient methods with convergence guarantees in the training of nonsmooth neural networks.","sentences":["In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks.","We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods.","We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion.","Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm).","In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our proposed framework.","Consequently, our convergence results establish, for the first time, global convergence of these methods when applied to nonsmooth nonconvex objectives.","Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized subgradient methods with convergence guarantees in the training of nonsmooth neural networks."],"url":"http://arxiv.org/abs/2403.11565v1","category":"math.OC"}
{"created":"2024-03-18 08:31:02","title":"A Comparison of Joint Species Distribution Models for Percent Cover Data","abstract":"1. Joint species distribution models (JSDMs) have gained considerable traction among ecologists over the past decade, due to their capacity to answer a wide range of questions at both the species- and the community-level. The family of generalized linear latent variable models in particular has proven popular for building JSDMs, being able to handle many response types including presence-absence data, biomass, overdispersed and/or zero-inflated counts.   2. We extend latent variable models to handle percent cover data, with vegetation, sessile invertebrate, and macroalgal cover data representing the prime examples of such data arising in community ecology.   3. Sparsity is a commonly encountered challenge with percent cover data. Responses are typically recorded as percentages covered per plot, though some species may be completely absent or present, i.e., have 0% or 100% cover respectively, rendering the use of beta distribution inadequate.   4. We propose two JSDMs suitable for percent cover data, namely a hurdle beta model and an ordered beta model. We compare the two proposed approaches to a beta distribution for shifted responses, transformed presence-absence data, and an ordinal model for percent cover classes. Results demonstrate the hurdle beta JSDM was generally the most accurate at retrieving the latent variables and predicting ecological percent cover data.","sentences":["1.","Joint species distribution models (JSDMs) have gained considerable traction among ecologists over the past decade, due to their capacity to answer a wide range of questions at both the species- and the community-level.","The family of generalized linear latent variable models in particular has proven popular for building JSDMs, being able to handle many response types including presence-absence data, biomass, overdispersed and/or zero-inflated counts.   ","2.","We extend latent variable models to handle percent cover data, with vegetation, sessile invertebrate, and macroalgal cover data representing the prime examples of such data arising in community ecology.   ","3.","Sparsity is a commonly encountered challenge with percent cover data.","Responses are typically recorded as percentages covered per plot, though some species may be completely absent or present, i.e., have 0% or 100% cover respectively, rendering the use of beta distribution inadequate.   ","4.","We propose two JSDMs suitable for percent cover data, namely a hurdle beta model and an ordered beta model.","We compare the two proposed approaches to a beta distribution for shifted responses, transformed presence-absence data, and an ordinal model for percent cover classes.","Results demonstrate the hurdle beta JSDM was generally the most accurate at retrieving the latent variables and predicting ecological percent cover data."],"url":"http://arxiv.org/abs/2403.11562v1","category":"stat.ME"}
{"created":"2024-03-18 08:29:47","title":"Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection","abstract":"In the field of multi-class anomaly detection, reconstruction-based methods derived from single-class anomaly detection face the well-known challenge of ``learning shortcuts'', wherein the model fails to learn the patterns of normal samples as it should, opting instead for shortcuts such as identity mapping or artificial noise elimination. Consequently, the model becomes unable to reconstruct genuine anomalies as normal instances, resulting in a failure of anomaly detection. To counter this issue, we present a novel unified feature reconstruction-based anomaly detection framework termed RLR (Reconstruct features from a Learnable Reference representation). Unlike previous methods, RLR utilizes learnable reference representations to compel the model to learn normal feature patterns explicitly, thereby prevents the model from succumbing to the ``learning shortcuts'' issue. Additionally, RLR incorporates locality constraints into the learnable reference to facilitate more effective normal pattern capture and utilizes a masked learnable key attention mechanism to enhance robustness. Evaluation of RLR on the 15-category MVTec-AD dataset and the 12-category VisA dataset shows superior performance compared to state-of-the-art methods under the unified setting. The code of RLR will be publicly available.","sentences":["In the field of multi-class anomaly detection, reconstruction-based methods derived from single-class anomaly detection face the well-known challenge of ``learning shortcuts'', wherein the model fails to learn the patterns of normal samples as it should, opting instead for shortcuts such as identity mapping or artificial noise elimination.","Consequently, the model becomes unable to reconstruct genuine anomalies as normal instances, resulting in a failure of anomaly detection.","To counter this issue, we present a novel unified feature reconstruction-based anomaly detection framework termed RLR (Reconstruct features from a Learnable Reference representation).","Unlike previous methods, RLR utilizes learnable reference representations to compel the model to learn normal feature patterns explicitly, thereby prevents the model from succumbing to the ``learning shortcuts'' issue.","Additionally, RLR incorporates locality constraints into the learnable reference to facilitate more effective normal pattern capture and utilizes a masked learnable key attention mechanism to enhance robustness.","Evaluation of RLR on the 15-category MVTec-AD dataset and the 12-category VisA dataset shows superior performance compared to state-of-the-art methods under the unified setting.","The code of RLR will be publicly available."],"url":"http://arxiv.org/abs/2403.11561v1","category":"cs.CV"}
{"created":"2024-03-18 08:25:56","title":"Variable Hyperparameterized Gaussian Kernel using Displaced Squeezed Vacuum State","abstract":"There are schemes for realizing different types of kernels by quantum states of light. It is particularly interesting to realize the Gaussian kernel due to its wider applicability. A multimode coherent state can generate the Gaussian kernel with a constant value of hyperparameter. This constant hyperparameter has limited the application of the Gaussian kernel when it is applied to complex learning problems. We realize the variable hyperparameterized Gaussian kernel with a multimode-displaced squeezed vacuum state. The learning capacity of this kernel is tested with the support vector machines over some synthesized data sets as well as public benchmark data sets. We establish that the proposed variable hyperparameterized Gaussian kernel offers better accuracy over the constant Gaussian kernel.","sentences":["There are schemes for realizing different types of kernels by quantum states of light.","It is particularly interesting to realize the Gaussian kernel due to its wider applicability.","A multimode coherent state can generate the Gaussian kernel with a constant value of hyperparameter.","This constant hyperparameter has limited the application of the Gaussian kernel when it is applied to complex learning problems.","We realize the variable hyperparameterized Gaussian kernel with a multimode-displaced squeezed vacuum state.","The learning capacity of this kernel is tested with the support vector machines over some synthesized data sets as well as public benchmark data sets.","We establish that the proposed variable hyperparameterized Gaussian kernel offers better accuracy over the constant Gaussian kernel."],"url":"http://arxiv.org/abs/2403.11560v1","category":"quant-ph"}
{"created":"2024-03-18 08:18:37","title":"Reinforcement Learning with Token-level Feedback for Controllable Text Generation","abstract":"To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \"first-quantize-then-noise\" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG","sentences":["To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs).","Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods).","However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences.","To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \"first-quantize-then-noise\" paradigm to enhance the robustness of the RL algorithm.","Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense.","Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks.","We have released our codes at https://github.com/WindyLee0822/CTG"],"url":"http://arxiv.org/abs/2403.11558v1","category":"cs.CL"}
{"created":"2024-03-18 08:03:47","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning","abstract":"Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them. Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies un- derscore the significant contribution of motion failure reasoning to the success of LLM^3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.","sentences":["Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation.","These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings.","Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface.","Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning.","Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure.","Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them.","Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP problems and the efficiency in selecting action parameters.","Ablation studies un- derscore the significant contribution of motion failure reasoning to the success of LLM^3.","Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings."],"url":"http://arxiv.org/abs/2403.11552v1","category":"cs.RO"}
{"created":"2024-03-18 08:01:23","title":"TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling","abstract":"As a cross-modal task, visual storytelling aims to generate a story for an ordered image sequence automatically. Different from the image captioning task, visual storytelling requires not only modeling the relationships between objects in the image but also mining the connections between adjacent images. Recent approaches primarily utilize either end-to-end frameworks or multi-stage frameworks to generate relevant stories, but they usually overlook latent topic information. In this paper, in order to generate a more coherent and relevant story, we propose a novel method, Topic Aware Reinforcement Network for VIsual StoryTelling (TARN-VIST). In particular, we pre-extracted the topic information of stories from both visual and linguistic perspectives. Then we apply two topic-consistent reinforcement learning rewards to identify the discrepancy between the generated story and the human-labeled story so as to refine the whole generation process. Extensive experimental results on the VIST dataset and human evaluation demonstrate that our proposed model outperforms most of the competitive models across multiple evaluation metrics.","sentences":["As a cross-modal task, visual storytelling aims to generate a story for an ordered image sequence automatically.","Different from the image captioning task, visual storytelling requires not only modeling the relationships between objects in the image but also mining the connections between adjacent images.","Recent approaches primarily utilize either end-to-end frameworks or multi-stage frameworks to generate relevant stories, but they usually overlook latent topic information.","In this paper, in order to generate a more coherent and relevant story, we propose a novel method, Topic Aware Reinforcement Network for VIsual StoryTelling (TARN-VIST).","In particular, we pre-extracted the topic information of stories from both visual and linguistic perspectives.","Then we apply two topic-consistent reinforcement learning rewards to identify the discrepancy between the generated story and the human-labeled story so as to refine the whole generation process.","Extensive experimental results on the VIST dataset and human evaluation demonstrate that our proposed model outperforms most of the competitive models across multiple evaluation metrics."],"url":"http://arxiv.org/abs/2403.11550v1","category":"cs.CV"}
{"created":"2024-03-18 07:58:57","title":"Norm-induced Cuts: Optimization with Lipschitzian Black-box Functions","abstract":"In this paper, we consider a finite dimensional optimization problem minimizing a continuous objective on a compact domain subject to a multi-dimensional constraint function. For the latter, we only assume the availability of a Lipschitz property. In recent literature methods based on non-convex outer approximation are proposed for tackling one dimensional equality constraints on bounded polyhedral domains, which are Lipschitz with respect to the maximum norm. To the best of our knowledge, however, there exists no non-convex outer approximation method for a general problem class. We introduce a meta-level solution framework to solve such problems and tackle the underlying theoretical foundations. Considering the feasible domain without the constraint function as manageable, our method relaxes the multidimensional constraint and iteratively refines the feasible region by means of norm-induced cuts, relying on an oracle for the resulting sub-problems. We show the method's correctness and investigate the problem complexity. In order to account for discussions about functionality, limits, and extensions, we present computational examples including illustrations.","sentences":["In this paper, we consider a finite dimensional optimization problem minimizing a continuous objective on a compact domain subject to a multi-dimensional constraint function.","For the latter, we only assume the availability of a Lipschitz property.","In recent literature methods based on non-convex outer approximation are proposed for tackling one dimensional equality constraints on bounded polyhedral domains, which are Lipschitz with respect to the maximum norm.","To the best of our knowledge, however, there exists no non-convex outer approximation method for a general problem class.","We introduce a meta-level solution framework to solve such problems and tackle the underlying theoretical foundations.","Considering the feasible domain without the constraint function as manageable, our method relaxes the multidimensional constraint and iteratively refines the feasible region by means of norm-induced cuts, relying on an oracle for the resulting sub-problems.","We show the method's correctness and investigate the problem complexity.","In order to account for discussions about functionality, limits, and extensions, we present computational examples including illustrations."],"url":"http://arxiv.org/abs/2403.11546v1","category":"math.OC"}
{"created":"2024-03-18 07:56:49","title":"First-order factors of linear Mahler operators","abstract":"We develop and compare two algorithms for computing first-order right-hand factors in the ring of linear Mahler operators$\\ell_r M^r + \\dots + \\ell_1 M + \\ell_0$where $\\ell_0, \\dots, \\ell_r$ are polynomials in~$x$ and $Mx = x^b M$ for some integer $b \\geq 2$. In other words, we give algorithms for finding all formal infinite product solutions of linear functional equations$\\ell_r(x) f(x^{b^r}) + \\dots + \\ell_1(x) f(x^b) + \\ell_0(x) f(x) = 0$. The first of our algorithms is adapted from Petkov\\v{s}ek's classical algorithm forthe analogous problem in the case of linear recurrences. The second one proceeds by computing a basis of generalized power series solutions of the functional equation and by using Hermite-Pad{\\'e} approximants to detect those linear combinations of the solutions that correspond to first-order factors. We present implementations of both algorithms and discuss their use in combination with criteria from the literature to prove the differential transcendence of power series solutions of Mahler equations.","sentences":["We develop and compare two algorithms for computing first-order right-hand factors in the ring of linear Mahler operators$\\ell_r M^r + \\dots + \\ell_1 M + \\ell_0$where $\\ell_0, \\dots, \\ell_r$ are polynomials in~$x$ and $Mx = x^b M$ for some integer $b \\geq 2$.","In other words, we give algorithms for finding all formal infinite product solutions of linear functional equations$\\ell_r(x) f(x^{b^r})","+","\\dots + \\ell_1(x) f(x^b)","+ \\ell_0(x) f(x)","= 0$.","The first of our algorithms is adapted from Petkov\\v{s}ek's classical algorithm forthe analogous problem in the case of linear recurrences.","The second one proceeds by computing a basis of generalized power series solutions of the functional equation and by using Hermite-Pad{\\'e} approximants to detect those linear combinations of the solutions that correspond to first-order factors.","We present implementations of both algorithms and discuss their use in combination with criteria from the literature to prove the differential transcendence of power series solutions of Mahler equations."],"url":"http://arxiv.org/abs/2403.11545v1","category":"cs.SC"}
{"created":"2024-03-18 07:54:11","title":"RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model","abstract":"Efficiently learning equilibria with large state and action spaces in general-sum Markov games while overcoming the curse of multi-agency is a challenging problem. Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent. However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\\varepsilon$ or the action space. In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states. Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\\epsilon$-CCE with a provable optimal accuracy bound $O(\\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with relevant problem parameters (such as the number of agents and time horizon). Moreover, our analysis of Linear-Confident-FTRL generalizes the virtual policy iteration technique in the single-agent local planning literature, which yields a new computationally efficient algorithm with a tighter sample complexity bound when assuming random access to the simulator.","sentences":["Efficiently learning equilibria with large state and action spaces in general-sum Markov games while overcoming the curse of multi-agency is a challenging problem.","Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent.","However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\\varepsilon$ or the action space.","In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states.","Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\\epsilon$-CCE with a provable optimal accuracy bound $O(\\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with relevant problem parameters (such as the number of agents and time horizon).","Moreover, our analysis of Linear-Confident-FTRL generalizes the virtual policy iteration technique in the single-agent local planning literature, which yields a new computationally efficient algorithm with a tighter sample complexity bound when assuming random access to the simulator."],"url":"http://arxiv.org/abs/2403.11544v1","category":"cs.LG"}
{"created":"2024-03-18 07:48:25","title":"Accretion disks and relativistic line broadening in boson star spacetimes","abstract":"In this work, we analyze the observational properties of static, spherically symmetric boson stars with fourth and sixth-order self-interactions, using the Julia-based general-relativistic radiative transfer code Skylight. We assume the boson stars are surrounded by an optically thick, geometrically thin accretion disk. We use the Novikov-Thorne model to compute the energy flux, introducing a physically based accretion model around these boson star configurations. Additionally, we calculate the relativistic broadening of emission lines, incorporating a lamppost corona model with full relativistic effects for the first time around a boson star. Our results show distinct observational features between quartic-potential boson stars and Schwarzschild black holes, owing to the presence of stable circular orbits at all radii around the former. On the other hand, compact solitonic boson stars, which possess an innermost stable circular orbit, have observational features closely similar to black holes. This similarity emphasizes their potential as black-hole mimickers. However, the compact boson stars, lacking an event horizon, have complex light-ring structures that produce potentially observable differences from black holes with future generations of experiments.","sentences":["In this work, we analyze the observational properties of static, spherically symmetric boson stars with fourth and sixth-order self-interactions, using the Julia-based general-relativistic radiative transfer code Skylight.","We assume the boson stars are surrounded by an optically thick, geometrically thin accretion disk.","We use the Novikov-Thorne model to compute the energy flux, introducing a physically based accretion model around these boson star configurations.","Additionally, we calculate the relativistic broadening of emission lines, incorporating a lamppost corona model with full relativistic effects for the first time around a boson star.","Our results show distinct observational features between quartic-potential boson stars and Schwarzschild black holes, owing to the presence of stable circular orbits at all radii around the former.","On the other hand, compact solitonic boson stars, which possess an innermost stable circular orbit, have observational features closely similar to black holes.","This similarity emphasizes their potential as black-hole mimickers.","However, the compact boson stars, lacking an event horizon, have complex light-ring structures that produce potentially observable differences from black holes with future generations of experiments."],"url":"http://arxiv.org/abs/2403.11540v1","category":"gr-qc"}
{"created":"2024-03-18 07:47:18","title":"Detecting superfluid transition in the pulsar core","abstract":"It is believed that the core of a neutron star can be host to various novel phases of matter, from nucleon superfluid phase to exotic high baryon density QCD phases. Different observational signals for such phase transitions have been discussed in the literature. Here, we point out a unique phenomenon associated with phase transition to a superfluid phase, which may be the nucleon superfluid phase or a phase like the CFL phase, allowing for superfluid vortices. In any superfluid phase transition, a random network of vortices forms via the so-called Kibble-Zurek mechanism, which eventually mostly decays away, finally leaving primarily vortices arising from the initial angular momentum of the core. This transient, random vortex network can have a non-zero net angular momentum for the superfluid component, which will generally be oriented in an arbitrary direction. This is in contrast to the final vortices, which arise from initial rotation and hence have the initial angular momentum of the neutron star. The angular momentum of the random vortex network is balanced by an equal and opposite angular momentum in the normal fluid due to the conservation of angular momentum, thereby imparting an arbitrarily oriented angular momentum component to the outer shell of the neutron star. This will affect the pulse timing and pulse profile of a pulsar. These changes in the pulses will decay away in a characteristic manner as the random vortex network decays, obeying specific scaling laws leading to universal features for the detection of superfluid transitions occurring in a pulsar core.","sentences":["It is believed that the core of a neutron star can be host to various novel phases of matter, from nucleon superfluid phase to exotic high baryon density QCD phases.","Different observational signals for such phase transitions have been discussed in the literature.","Here, we point out a unique phenomenon associated with phase transition to a superfluid phase, which may be the nucleon superfluid phase or a phase like the CFL phase, allowing for superfluid vortices.","In any superfluid phase transition, a random network of vortices forms via the so-called Kibble-Zurek mechanism, which eventually mostly decays away, finally leaving primarily vortices arising from the initial angular momentum of the core.","This transient, random vortex network can have a non-zero net angular momentum for the superfluid component, which will generally be oriented in an arbitrary direction.","This is in contrast to the final vortices, which arise from initial rotation and hence have the initial angular momentum of the neutron star.","The angular momentum of the random vortex network is balanced by an equal and opposite angular momentum in the normal fluid due to the conservation of angular momentum, thereby imparting an arbitrarily oriented angular momentum component to the outer shell of the neutron star.","This will affect the pulse timing and pulse profile of a pulsar.","These changes in the pulses will decay away in a characteristic manner as the random vortex network decays, obeying specific scaling laws leading to universal features for the detection of superfluid transitions occurring in a pulsar core."],"url":"http://arxiv.org/abs/2403.11539v1","category":"astro-ph.HE"}
{"created":"2024-03-18 07:41:39","title":"OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System","abstract":"Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in optical character recognition (OCR), to extract statistical features from images as a second modality to enhance performance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the alignment of external modality features, extracted using a single modality-aware model, with image features encoded by a convolutional neural network. This synergy enables a more refined fusion of semantic representations from different modalities. We further introduce feature refinement and a gating function in our OANet to optimize the combination of these features, enhancing inference and decision-making capabilities. Experimental outcomes show that our methodology considerably boosts the recall rate of the defect detection model and maintains high robustness even in challenging scenarios.","sentences":["Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes.","It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control.","Despite its importance, the deployment of models for AOI often faces challenges.","These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging.","These factors collectively compromise the accuracy of model predictions.","Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification.","To address this, we introduce an external modality-guided data mining framework, primarily rooted in optical character recognition (OCR), to extract statistical features from images as a second modality to enhance performance, termed OANet (Ocr-Aoi-Net).","A key aspect of our approach is the alignment of external modality features, extracted using a single modality-aware model, with image features encoded by a convolutional neural network.","This synergy enables a more refined fusion of semantic representations from different modalities.","We further introduce feature refinement and a gating function in our OANet to optimize the combination of these features, enhancing inference and decision-making capabilities.","Experimental outcomes show that our methodology considerably boosts the recall rate of the defect detection model and maintains high robustness even in challenging scenarios."],"url":"http://arxiv.org/abs/2403.11536v1","category":"cs.CV"}
{"created":"2024-03-18 07:41:19","title":"EchoReel: Enhancing Action Generation of Existing Video Diffusion Models","abstract":"Recent large-scale video datasets have facilitated the generation of diverse open-domain videos of Video Diffusion Models (VDMs). Nonetheless, the efficacy of VDMs in assimilating complex knowledge from these datasets remains constrained by their inherent scale, leading to suboptimal comprehension and synthesis of numerous actions. In this paper, we introduce EchoReel, a novel approach to augment the capability of VDMs in generating intricate actions by emulating motions from pre-existing videos, which are readily accessible from databases or online repositories. EchoReel seamlessly integrates with existing VDMs, enhancing their ability to produce realistic motions without compromising their fundamental capabilities. Specifically, the Action Prism (AP), is introduced to distill motion information from reference videos, which requires training on only a small dataset. Leveraging the knowledge from pre-trained VDMs, EchoReel incorporates new action features into VDMs through the additional layers, eliminating the need for any further fine-tuning of untrained actions. Extensive experiments demonstrate that EchoReel is not merely replicating the whole content from references, and it significantly improves the generation of realistic actions, even in situations where existing VDMs might directly fail.","sentences":["Recent large-scale video datasets have facilitated the generation of diverse open-domain videos of Video Diffusion Models (VDMs).","Nonetheless, the efficacy of VDMs in assimilating complex knowledge from these datasets remains constrained by their inherent scale, leading to suboptimal comprehension and synthesis of numerous actions.","In this paper, we introduce EchoReel, a novel approach to augment the capability of VDMs in generating intricate actions by emulating motions from pre-existing videos, which are readily accessible from databases or online repositories.","EchoReel seamlessly integrates with existing VDMs, enhancing their ability to produce realistic motions without compromising their fundamental capabilities.","Specifically, the Action Prism (AP), is introduced to distill motion information from reference videos, which requires training on only a small dataset.","Leveraging the knowledge from pre-trained VDMs, EchoReel incorporates new action features into VDMs through the additional layers, eliminating the need for any further fine-tuning of untrained actions.","Extensive experiments demonstrate that EchoReel is not merely replicating the whole content from references, and it significantly improves the generation of realistic actions, even in situations where existing VDMs might directly fail."],"url":"http://arxiv.org/abs/2403.11535v1","category":"cs.CV"}
{"created":"2024-03-18 07:38:36","title":"Probabilistic Model for the Gravitational Wave Signal from Merging Black Holes","abstract":"Parameterised models that predict the gravitational-wave (GW) signal from merging black holes are used to extract source properties from GW observations. The majority of research in this area has focused on developing methods capable of producing highly accurate, point-estimate, predictions for the GW signal. A key element missing from every model used in the analysis of GW data is an estimate for how confident the model is in its prediction. This omission increases the risk of biased parameter estimation of source properties. Current strategies include running analyses with multiple models to measure systematic bias however, this fails to accurately reflect the true uncertainty in the models. In this work we develop a probabilistic extension to the phenomenological modelling workflow for non-spinning black holes and demonstrate that the model not only produces accurate point-estimates for the GW signal but can be used to provide well-calibrated local estimates for its uncertainty. Our analysis highlights that there is a lack of Numerical Relativity (NR) simulations available at multiple resolutions which can be used to estimate their numerical error and implore the NR community to continue to improve their estimates for the error in NR solutions published. Waveform models that are not only accurate in their point-estimate predictions but also in their error estimates are a potential way to mitigate bias in GW parameter estimation of compact binaries due to unconfident waveform model extrapolations.","sentences":["Parameterised models that predict the gravitational-wave (GW) signal from merging black holes are used to extract source properties from GW observations.","The majority of research in this area has focused on developing methods capable of producing highly accurate, point-estimate, predictions for the GW signal.","A key element missing from every model used in the analysis of GW data is an estimate for how confident the model is in its prediction.","This omission increases the risk of biased parameter estimation of source properties.","Current strategies include running analyses with multiple models to measure systematic bias however, this fails to accurately reflect the true uncertainty in the models.","In this work we develop a probabilistic extension to the phenomenological modelling workflow for non-spinning black holes and demonstrate that the model not only produces accurate point-estimates for the GW signal but can be used to provide well-calibrated local estimates for its uncertainty.","Our analysis highlights that there is a lack of Numerical Relativity (NR) simulations available at multiple resolutions which can be used to estimate their numerical error and implore the NR community to continue to improve their estimates for the error in NR solutions published.","Waveform models that are not only accurate in their point-estimate predictions but also in their error estimates are a potential way to mitigate bias in GW parameter estimation of compact binaries due to unconfident waveform model extrapolations."],"url":"http://arxiv.org/abs/2403.11534v1","category":"gr-qc"}
{"created":"2024-03-18 07:36:58","title":"Towards Scalable Semidefinite Programming: Optimal Metric ADMM with A Worst-case Performance Guarantee","abstract":"Despite the numerous uses of semidefinite programming (SDP) and its universal solvability via interior point methods (IPMs), it is rarely applied to practical large-scale problems. This mainly owes to the computational cost of IPMs that increases in a bad exponential way with the data size. While first-order algorithms such as ADMM can alleviate this issue, but the scalability improvement appears far not enough. In this work, we aim to achieve extra acceleration for ADMM by appealing to a non-Euclidean metric space, while maintaining everything in closed-form expressions. The efficiency gain comes from the extra degrees of freedom of a variable metric compared to a scalar step-size, which allows us to capture some additional ill-conditioning structures.   On the application side, we consider the quadratically constrained quadratic program (QCQP), which naturally appears in an SDP form after a dualization procedure. This technique, known as semidefinite relaxation, has important uses across different fields, particularly in wireless communications. Numerically, we observe that the scalability property is significantly improved. Depending on the data generation process, the extra acceleration can easily surpass the scalar-parameter efficiency limit, and the advantage is rapidly increasing as the data conditioning becomes worse.","sentences":["Despite the numerous uses of semidefinite programming (SDP) and its universal solvability via interior point methods (IPMs), it is rarely applied to practical large-scale problems.","This mainly owes to the computational cost of IPMs that increases in a bad exponential way with the data size.","While first-order algorithms such as ADMM can alleviate this issue, but the scalability improvement appears far not enough.","In this work, we aim to achieve extra acceleration for ADMM by appealing to a non-Euclidean metric space, while maintaining everything in closed-form expressions.","The efficiency gain comes from the extra degrees of freedom of a variable metric compared to a scalar step-size, which allows us to capture some additional ill-conditioning structures.   ","On the application side, we consider the quadratically constrained quadratic program (QCQP), which naturally appears in an SDP form after a dualization procedure.","This technique, known as semidefinite relaxation, has important uses across different fields, particularly in wireless communications.","Numerically, we observe that the scalability property is significantly improved.","Depending on the data generation process, the extra acceleration can easily surpass the scalar-parameter efficiency limit, and the advantage is rapidly increasing as the data conditioning becomes worse."],"url":"http://arxiv.org/abs/2403.11533v1","category":"math.OC"}
{"created":"2024-03-18 07:29:28","title":"Connecting 2-Forms, Conformal Transformations, Curvature Invariants and Topological Classes in Einstein Spacetimes","abstract":"The unique Nature of the Lorentz group in four dimensions is the root cause of the many remarkable properties of the Einstein spacetimes, in particular their operational structure on the 2-forms. We show how this operational structure can be used for two ends. First, it allows for a simple generalization of the Birkhoff theorem to Schwarzschild (A)de-Sitter spacetime. Second, it provides the means to construct an Abelian endomorphism group on the space of 2-forms. It is observed that taking the trace over this group element-wise induces a further Abelian group which may be identified with a tensor representation of conformal transformations, giving Einstein spacetimes access to their own conformal equivalence class. A further trace over the group yields the curvature invariants of the spacetime. The Kretschmann scalar becomes the topological Euler density, which may be linked in a simple way to the Hawking temperature of horizons.","sentences":["The unique Nature of the Lorentz group in four dimensions is the root cause of the many remarkable properties of the Einstein spacetimes, in particular their operational structure on the 2-forms.","We show how this operational structure can be used for two ends.","First, it allows for a simple generalization of the Birkhoff theorem to Schwarzschild (A)de-Sitter spacetime.","Second, it provides the means to construct an Abelian endomorphism group on the space of 2-forms.","It is observed that taking the trace over this group element-wise induces a further Abelian group which may be identified with a tensor representation of conformal transformations, giving Einstein spacetimes access to their own conformal equivalence class.","A further trace over the group yields the curvature invariants of the spacetime.","The Kretschmann scalar becomes the topological Euler density, which may be linked in a simple way to the Hawking temperature of horizons."],"url":"http://arxiv.org/abs/2403.11527v1","category":"gr-qc"}
{"created":"2024-03-18 07:26:36","title":"A supersymmetric quantum perspective on the explicit large deviations for reversible Markov jump processes, with applications to pure and random spin chains","abstract":"The large deviations at various levels that are explicit for Markov jump processes satisfying detailed-balance are revisited in terms of the supersymmetric quantum Hamiltonian $H$ that can be obtained from the Markov generator via a similarity transformation. We first focus on the large deviations at level 2 for the empirical density ${\\hat p}(C) $ of the configurations $C$ seen during a trajectory over the large time-window $[0,T]$ and rewrite the explicit Donsker-Varadhan rate function as the matrix element $I^{[2]}[{\\hat p}(.) ] = \\langle \\sqrt{ {\\hat p} } \\vert H \\vert \\sqrt{{\\hat p}} \\rangle $ involving the square-root ket $\\vert \\sqrt{{\\hat p}} \\rangle $. [The analog formula is also discussed for reversible diffusion processes as a comparison.] We then consider the explicit rate functions at higher levels, in particular for the joint probability of the empirical density ${\\hat p}(C) $ and the empirical local activities ${\\hat a}(C,C') $ characterizing the density of jumps between two configurations $(C,C')$. Finally, the explicit rate function for the joint probability of the empirical density ${\\hat p}(C) $ and of the empirical total activity ${\\hat A} $ that represents the total density of jumps of a long trajectory is written in terms of the two matrix elements $ \\langle \\sqrt{ {\\hat p} } \\vert H \\vert \\sqrt{{\\hat p}} \\rangle$ and $\\langle \\sqrt{ {\\hat p} } \\vert H^{off} \\vert \\sqrt{{\\hat p}} \\rangle $, where $H^{off} $ represents the off-diagonal part of the supersymmetric Hamiltonian $H$. This general formalism is then applied to pure or random spin chains with single-spin-flip or two-spin-flip transition rates, where the supersymmetric Hamiltonian $H$ correspond to quantum spin chains with local interactions involving Pauli matrices of two or three neighboring sites.","sentences":["The large deviations at various levels that are explicit for Markov jump processes satisfying detailed-balance are revisited in terms of the supersymmetric quantum Hamiltonian $H$ that can be obtained from the Markov generator via a similarity transformation.","We first focus on the large deviations at level 2 for the empirical density ${\\hat p}(C) $ of the configurations $C$ seen during a trajectory over the large time-window $[0,T]$ and rewrite the explicit Donsker-Varadhan rate function as the matrix element $I^{[2]}[{\\hat p}(.) ]","= \\langle \\sqrt{ {\\hat p} } \\vert H \\vert \\sqrt{{\\hat p}} \\rangle $ involving the square-root ket $\\vert \\sqrt{{\\hat p}} \\rangle $.","[The analog formula is also discussed for reversible diffusion processes as a comparison.]","We then consider the explicit rate functions at higher levels, in particular for the joint probability of the empirical density ${\\hat p}(C) $ and the empirical local activities ${\\hat a}(C,C')","$ characterizing the density of jumps between two configurations $(C,C')$. Finally, the explicit rate function for the joint probability of the empirical density ${\\hat p}(C) $ and of the empirical total activity ${\\hat A} $ that represents the total density of jumps of a long trajectory is written in terms of the two matrix elements $ \\langle \\sqrt{ {\\hat p} } \\vert H \\vert \\sqrt{{\\hat p}} \\rangle$ and $\\langle \\sqrt{ {\\hat p} } \\vert H^{off} \\vert \\sqrt{{\\hat p}} \\rangle $, where $H^{off} $ represents the off-diagonal part of the supersymmetric Hamiltonian $H$. This general formalism is then applied to pure or random spin chains with single-spin-flip or two-spin-flip transition rates, where the supersymmetric Hamiltonian $H$ correspond to quantum spin chains with local interactions involving Pauli matrices of two or three neighboring sites."],"url":"http://arxiv.org/abs/2403.11525v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-18 07:26:28","title":"Probing cold gas with Mg II and Ly$\u03b1$ radiative transfer","abstract":"The Mg II resonance doublet at 2796 {\\AA} and 2803 {\\AA} is an increasingly important tool to study cold, $T \\sim 10^{4}\\,$K, gas -- an observational driven development requiring theoretical support. We develop a new Monte Carlo radiative transfer code to systematically study the joined Mg II and Ly$\\alpha$ escape through homogeneous and `clumpy' multiphase gas with dust in arbitrary 3D geometries. Our main findings are: (i) The Mg II spectrum differs from Ly$\\alpha$ due to the large difference in column densities, even though the atomic physics of the two lines are similar. (ii) the Mg II escape fraction is generally higher than that of Ly$\\alpha$ because of lower dust optical depths and path lengths -- but large variations due to differences in dust models and the clumpiness of the cold medium exist. (iii) Clumpy media possess a `critical covering factor' above which Mg II radiative transfer matches a homogeneous medium. The critical covering factors for Mg II and Ly$\\alpha$ differ, allowing constraints on the cold gas structure. (iv) The Mg II doublet ratio $R_{\\rm MgII}$ varies for strong outflows/inflows ($\\gtrsim 700 \\mathrm{km\\,s}^{-1}$), in particular, $R_{\\rm MgII}<1$ being an unambiguous tracer for powerful galactic winds. (v) Scattering of stellar continuum photons can decrease $R_{\\rm MgII}$ from two to one, allowing constraints on the scattering medium. Notably, we introduce a novel probe of the cold gas column density -- the halo doublet ratio -- which we show to be a powerful indicator of ionizing photon escape. We discuss our results in the context of interpreting and modeling observations as well as their implications for other resonant doublets.","sentences":["The Mg II resonance doublet at 2796 {\\AA} and 2803 {\\AA} is an increasingly important tool to study cold, $T \\sim 10^{4}\\,$K, gas -- an observational driven development requiring theoretical support.","We develop a new Monte Carlo radiative transfer code to systematically study the joined Mg II and Ly$\\alpha$ escape through homogeneous and `clumpy' multiphase gas with dust in arbitrary 3D geometries.","Our main findings are: (i)","The Mg II spectrum differs from Ly$\\alpha$ due to the large difference in column densities, even though the atomic physics of the two lines are similar.","(ii) the Mg II escape fraction is generally higher than that of Ly$\\alpha$ because of lower dust optical depths and path lengths -- but large variations due to differences in dust models and the clumpiness of the cold medium exist.","(iii) Clumpy media possess a `critical covering factor' above which Mg II radiative transfer matches a homogeneous medium.","The critical covering factors for Mg II and Ly$\\alpha$ differ, allowing constraints on the cold gas structure.","(iv) The Mg II doublet ratio $R_{\\rm MgII}$ varies for strong outflows/inflows ($\\gtrsim 700 \\mathrm{km\\,s}^{-1}$), in particular, $R_{\\rm MgII}<1$ being an unambiguous tracer for powerful galactic winds.","(v) Scattering of stellar continuum photons can decrease $R_{\\rm MgII}$ from two to one, allowing constraints on the scattering medium.","Notably, we introduce a novel probe of the cold gas column density -- the halo doublet ratio -- which we show to be a powerful indicator of ionizing photon escape.","We discuss our results in the context of interpreting and modeling observations as well as their implications for other resonant doublets."],"url":"http://arxiv.org/abs/2403.11524v1","category":"astro-ph.GA"}
{"created":"2024-03-18 07:22:31","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","abstract":"While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. In this paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs. It supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations. It also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs. We implement and evaluate LOOPer and show that it achieves speedups over the state-of-the-art. On the Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over Tiramisu. LOOPer also achieves competitive speedups with a geometric mean speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model.","sentences":["While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups.","This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations.","State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach.","While such a proof-of-concept has shown promise, it still has significant limitations.","State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations.","They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs.","These limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach.","In this paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs.","It supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations.","It also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs.","We implement and evaluate LOOPer and show that it achieves speedups over the state-of-the-art.","On the Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over Tiramisu.","LOOPer also achieves competitive speedups with a geometric mean speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model."],"url":"http://arxiv.org/abs/2403.11522v1","category":"cs.PL"}
{"created":"2024-03-18 07:15:01","title":"A Data-driven Approach for Rapid Detection of Aeroelastic Modes from Flutter Flight Test Based on Limited Sensor Measurements","abstract":"Flutter flight test involves the evaluation of the airframes aeroelastic stability by applying artificial excitation on the aircraft lifting surfaces. The subsequent responses are captured and analyzed to extract the frequencies and damping characteristics of the system. However, noise contamination, turbulence, non-optimal excitation of modes, and sensor malfunction in one or more sensors make it time-consuming and corrupt the extraction process. In order to expedite the process of identifying and analyzing aeroelastic modes, this study implements a time-delay embedded Dynamic Mode Decomposition technique. This approach is complemented by Robust Principal Component Analysis methodology, and a sparsity promoting criterion which enables the automatic and optimal selection of sparse modes. The anonymized flutter flight test data, provided by the fifth author of this research paper, is utilized in this implementation. The methodology assumes no knowledge of the input excitation, only deals with the responses captured by accelerometer channels, and rapidly identifies the aeroelastic modes. By incorporating a compressed sensing algorithm, the methodology gains the ability to identify aeroelastic modes, even when the number of available sensors is limited. This augmentation greatly enhances the methodology's robustness and effectiveness, making it an excellent choice for real-time implementation during flutter test campaigns.","sentences":["Flutter flight test involves the evaluation of the airframes aeroelastic stability by applying artificial excitation on the aircraft lifting surfaces.","The subsequent responses are captured and analyzed to extract the frequencies and damping characteristics of the system.","However, noise contamination, turbulence, non-optimal excitation of modes, and sensor malfunction in one or more sensors make it time-consuming and corrupt the extraction process.","In order to expedite the process of identifying and analyzing aeroelastic modes, this study implements a time-delay embedded Dynamic Mode Decomposition technique.","This approach is complemented by Robust Principal Component Analysis methodology, and a sparsity promoting criterion which enables the automatic and optimal selection of sparse modes.","The anonymized flutter flight test data, provided by the fifth author of this research paper, is utilized in this implementation.","The methodology assumes no knowledge of the input excitation, only deals with the responses captured by accelerometer channels, and rapidly identifies the aeroelastic modes.","By incorporating a compressed sensing algorithm, the methodology gains the ability to identify aeroelastic modes, even when the number of available sensors is limited.","This augmentation greatly enhances the methodology's robustness and effectiveness, making it an excellent choice for real-time implementation during flutter test campaigns."],"url":"http://arxiv.org/abs/2403.11521v1","category":"eess.SP"}
{"created":"2024-03-18 07:06:03","title":"Perceptual learning in contour detection transfer across changes in contour path and orientation","abstract":"The integration of local elements into shape contours is critical for target detection and identification in cluttered scenes. Previous studies have shown that observers can learn to use image regularities for contour integration and target identification. However, we still know little about the generalization of perceptual learning in contour integration. Specifically, whether training in contour detection task could transfer to untrained contour type, path or orientation is still unclear. In a series of four experiments, human perceptual learning in contour detection was studied using psychophysical methods. We trained participants to detect contours in cluttered scenes over several days, which resulted in a significant improvement in sensitivity to trained contour type. This improved sensitivity was highly specific to contour type, but transfer across changes in contour path and contour orientation. These results suggest that short-term training improves the ability to integrate specific types of contours by optimizing the ability of the visual system to extract specific image regularities. The differential specificity and generalization across different stimulus features may support the involvement of both low-level and higher-level visual areas in perceptual learning in contour detection. These findings provide further insights into understanding the nature and the brain plasticity mechanism of contour integration learning.","sentences":["The integration of local elements into shape contours is critical for target detection and identification in cluttered scenes.","Previous studies have shown that observers can learn to use image regularities for contour integration and target identification.","However, we still know little about the generalization of perceptual learning in contour integration.","Specifically, whether training in contour detection task could transfer to untrained contour type, path or orientation is still unclear.","In a series of four experiments, human perceptual learning in contour detection was studied using psychophysical methods.","We trained participants to detect contours in cluttered scenes over several days, which resulted in a significant improvement in sensitivity to trained contour type.","This improved sensitivity was highly specific to contour type, but transfer across changes in contour path and contour orientation.","These results suggest that short-term training improves the ability to integrate specific types of contours by optimizing the ability of the visual system to extract specific image regularities.","The differential specificity and generalization across different stimulus features may support the involvement of both low-level and higher-level visual areas in perceptual learning in contour detection.","These findings provide further insights into understanding the nature and the brain plasticity mechanism of contour integration learning."],"url":"http://arxiv.org/abs/2403.11516v1","category":"q-bio.NC"}
{"created":"2024-03-18 06:32:23","title":"GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects","abstract":"Despite the progress of learning-based methods for 6D object pose estimation, the trade-off between accuracy and scalability for novel objects still exists. Specifically, previous methods for novel objects do not make good use of the target object's 3D shape information since they focus on generalization by processing the shape indirectly, making them less effective. We present GenFlow, an approach that enables both accuracy and generalization to novel objects with the guidance of the target object's shape. Our method predicts optical flow between the rendered image and the observed image and refines the 6D pose iteratively. It boosts the performance by a constraint of the 3D shape and the generalizable geometric knowledge learned from an end-to-end differentiable system. We further improve our model by designing a cascade network architecture to exploit the multi-scale correlations and coarse-to-fine refinement. GenFlow ranked first on the unseen object pose estimation benchmarks in both the RGB and RGB-D cases. It also achieves performance competitive with existing state-of-the-art methods for the seen object pose estimation without any fine-tuning.","sentences":["Despite the progress of learning-based methods for 6D object pose estimation, the trade-off between accuracy and scalability for novel objects still exists.","Specifically, previous methods for novel objects do not make good use of the target object's 3D shape information since they focus on generalization by processing the shape indirectly, making them less effective.","We present GenFlow, an approach that enables both accuracy and generalization to novel objects with the guidance of the target object's shape.","Our method predicts optical flow between the rendered image and the observed image and refines the 6D pose iteratively.","It boosts the performance by a constraint of the 3D shape and the generalizable geometric knowledge learned from an end-to-end differentiable system.","We further improve our model by designing a cascade network architecture to exploit the multi-scale correlations and coarse-to-fine refinement.","GenFlow ranked first on the unseen object pose estimation benchmarks in both the RGB and RGB-D cases.","It also achieves performance competitive with existing state-of-the-art methods for the seen object pose estimation without any fine-tuning."],"url":"http://arxiv.org/abs/2403.11510v1","category":"cs.CV"}
{"created":"2024-03-18 06:30:41","title":"DEE: Dual-stage Explainable Evaluation Method for Text Generation","abstract":"Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation. Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diagnostic reports in the second stage. DEE is fine-tuned on our elaborately assembled dataset AntEval, which encompasses 15K examples from 4 real-world applications of Alipay that employ generative systems. The dataset concerns newly emerged issues like hallucination and toxicity, thereby broadening the scope of DEE's evaluation criteria. Experimental results affirm that DEE's superiority over existing evaluation methods, achieving significant improvements in both human correlation as well as efficiency.","sentences":["Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems.","Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome.","Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount.","To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation.","Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diagnostic reports in the second stage.","DEE is fine-tuned on our elaborately assembled dataset AntEval, which encompasses 15K examples from 4 real-world applications of Alipay that employ generative systems.","The dataset concerns newly emerged issues like hallucination and toxicity, thereby broadening the scope of DEE's evaluation criteria.","Experimental results affirm that DEE's superiority over existing evaluation methods, achieving significant improvements in both human correlation as well as efficiency."],"url":"http://arxiv.org/abs/2403.11509v1","category":"cs.CL"}
{"created":"2024-03-18 06:26:32","title":"Discriminative Neighborhood Smoothing for Generative Anomalous Sound Detection","abstract":"We propose discriminative neighborhood smoothing of generative anomaly scores for anomalous sound detection. While the discriminative approach is known to achieve better performance than generative approaches often, we have found that it sometimes causes significant performance degradation due to the discrepancy between the training and test data, making it less robust than the generative approach. Our proposed method aims to compensate for the disadvantages of generative and discriminative approaches by combining them. Generative anomaly scores are smoothed using multiple samples with similar discriminative features to improve the performance of the generative approach in an ensemble manner while keeping its robustness. Experimental results show that our proposed method greatly improves the original generative method, including absolute improvement of 22% in AUC and robustly works, while a discriminative method suffers from the discrepancy.","sentences":["We propose discriminative neighborhood smoothing of generative anomaly scores for anomalous sound detection.","While the discriminative approach is known to achieve better performance than generative approaches often, we have found that it sometimes causes significant performance degradation due to the discrepancy between the training and test data, making it less robust than the generative approach.","Our proposed method aims to compensate for the disadvantages of generative and discriminative approaches by combining them.","Generative anomaly scores are smoothed using multiple samples with similar discriminative features to improve the performance of the generative approach in an ensemble manner while keeping its robustness.","Experimental results show that our proposed method greatly improves the original generative method, including absolute improvement of 22% in AUC and robustly works, while a discriminative method suffers from the discrepancy."],"url":"http://arxiv.org/abs/2403.11508v1","category":"eess.AS"}
{"created":"2024-03-18 06:24:46","title":"End-To-End Underwater Video Enhancement: Dataset and Model","abstract":"Underwater video enhancement (UVE) aims to improve the visibility and frame quality of underwater videos, which has significant implications for marine research and exploration. However, existing methods primarily focus on developing image enhancement algorithms to enhance each frame independently. There is a lack of supervised datasets and models specifically tailored for UVE tasks. To fill this gap, we construct the Synthetic Underwater Video Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos paired with ground-truth reference videos. Based on this dataset, we train a novel underwater video enhancement model, UVENet, which utilizes inter-frame relationships to achieve better enhancement performance. Through extensive experiments on both synthetic and real underwater videos, we demonstrate the effectiveness of our approach. This study represents the first comprehensive exploration of UVE to our knowledge. The code is available at https://anonymous.4open.science/r/UVENet.","sentences":["Underwater video enhancement (UVE) aims to improve the visibility and frame quality of underwater videos, which has significant implications for marine research and exploration.","However, existing methods primarily focus on developing image enhancement algorithms to enhance each frame independently.","There is a lack of supervised datasets and models specifically tailored for UVE tasks.","To fill this gap, we construct the Synthetic Underwater Video Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos paired with ground-truth reference videos.","Based on this dataset, we train a novel underwater video enhancement model, UVENet, which utilizes inter-frame relationships to achieve better enhancement performance.","Through extensive experiments on both synthetic and real underwater videos, we demonstrate the effectiveness of our approach.","This study represents the first comprehensive exploration of UVE to our knowledge.","The code is available at https://anonymous.4open.science/r/UVENet."],"url":"http://arxiv.org/abs/2403.11506v1","category":"cs.CV"}
{"created":"2024-03-18 06:19:37","title":"MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning","abstract":"Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers the model to detect diagnostically meaningful patterns while reducing redundancy effectively. By enhancing the variance and covariance of the learned embeddings, MLVICX promotes the retention of critical medical insights by adapting both global and local contextual details. We demonstrate the performance of MLVICX in advancing self-supervised chest X-ray representation learning through comprehensive experiments. The performance enhancements we observe across various downstream tasks highlight the significance of the proposed approach in enhancing the utility of chest X-ray embeddings for precision medical diagnosis and comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR, RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more than 3% performance gains over SOTA SSL approaches in various downstream tasks.","sentences":["Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks.","By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning.","However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information.","In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images.","Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers the model to detect diagnostically meaningful patterns while reducing redundancy effectively.","By enhancing the variance and covariance of the learned embeddings, MLVICX promotes the retention of critical medical insights by adapting both global and local contextual details.","We demonstrate the performance of MLVICX in advancing self-supervised chest X-ray representation learning through comprehensive experiments.","The performance enhancements we observe across various downstream tasks highlight the significance of the proposed approach in enhancing the utility of chest X-ray embeddings for precision medical diagnosis and comprehensive image analysis.","For pertaining, we used the NIH-Chest X-ray dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR, RSNA pneumonia, and SIIM-ACR Pneumothorax datasets.","Overall, we observe more than 3% performance gains over SOTA SSL approaches in various downstream tasks."],"url":"http://arxiv.org/abs/2403.11504v1","category":"eess.IV"}
{"created":"2024-03-18 06:18:59","title":"Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors","abstract":"We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation. Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training specialized models, thus constraining their effectiveness on open-domain images featuring significantly more varied layouts and styles. In contrast, our method directly leverages powerful image diffusion models trained on a broad spectrum of text-image pairs and thus retain their exceptional generalization abilities. This objective is realized through the development of an iterative novel view synthesis and geometry alignment algorithm. The algorithm harnesses diffusion models for dual purposes: they provide appearance prior by predicting novel views of the selected object using estimated depth maps, and they act as a geometry critic by correcting misalignments in 3D shapes across the sampled views. Our method can generate high-quality 3D-aware image edits with large viewpoint transformations and high appearance and shape consistency with the input image, pushing the boundaries of what is possible with single-image 3D-aware editing.","sentences":["We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation.","Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training specialized models, thus constraining their effectiveness on open-domain images featuring significantly more varied layouts and styles.","In contrast, our method directly leverages powerful image diffusion models trained on a broad spectrum of text-image pairs and thus retain their exceptional generalization abilities.","This objective is realized through the development of an iterative novel view synthesis and geometry alignment algorithm.","The algorithm harnesses diffusion models for dual purposes: they provide appearance prior by predicting novel views of the selected object using estimated depth maps, and they act as a geometry critic by correcting misalignments in 3D shapes across the sampled views.","Our method can generate high-quality 3D-aware image edits with large viewpoint transformations and high appearance and shape consistency with the input image, pushing the boundaries of what is possible with single-image 3D-aware editing."],"url":"http://arxiv.org/abs/2403.11503v1","category":"cs.CV"}
{"created":"2024-03-18 06:07:45","title":"Domain Adaptation Using Pseudo Labels for COVID-19 Detection","abstract":"In response to the need for rapid and accurate COVID-19 diagnosis during the global pandemic, we present a two-stage framework that leverages pseudo labels for domain adaptation to enhance the detection of COVID-19 from CT scans. By utilizing annotated data from one domain and non-annotated data from another, the model overcomes the challenge of data scarcity and variability, common in emergent health crises. The innovative approach of generating pseudo labels enables the model to iteratively refine its learning process, thereby improving its accuracy and adaptability across different hospitals and medical centres. Experimental results on COV19-CT-DB database showcase the model's potential to achieve high diagnostic precision, significantly contributing to efficient patient management and alleviating the strain on healthcare systems. Our method achieves 0.92 Macro F1 Score on the validation set of Covid-19 domain adaptation challenge.","sentences":["In response to the need for rapid and accurate COVID-19 diagnosis during the global pandemic, we present a two-stage framework that leverages pseudo labels for domain adaptation to enhance the detection of COVID-19 from CT scans.","By utilizing annotated data from one domain and non-annotated data from another, the model overcomes the challenge of data scarcity and variability, common in emergent health crises.","The innovative approach of generating pseudo labels enables the model to iteratively refine its learning process, thereby improving its accuracy and adaptability across different hospitals and medical centres.","Experimental results on COV19-CT-DB database showcase the model's potential to achieve high diagnostic precision, significantly contributing to efficient patient management and alleviating the strain on healthcare systems.","Our method achieves 0.92 Macro F1 Score on the validation set of Covid-19 domain adaptation challenge."],"url":"http://arxiv.org/abs/2403.11498v1","category":"eess.IV"}
{"created":"2024-03-18 06:04:02","title":"Do CLIPs Always Generalize Better than ImageNet Models?","abstract":"Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. We provide both theoretical and empirical explanations for why CLIPs still learn spurious features. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different scale and distribution.","sentences":["Large vision language models, such as CLIPs, have revolutionized modern machine learning.","CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature.","However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations.","To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos.","CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds.","The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals.","We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group.","Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs.","We provide both theoretical and empirical explanations for why CLIPs still learn spurious features.","Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different scale and distribution."],"url":"http://arxiv.org/abs/2403.11497v1","category":"cs.CV"}
{"created":"2024-03-18 06:00:38","title":"MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception","abstract":"Perception plays a crucial role in various robot applications. However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations. To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses. MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely unexplored lidar modality. Finally, we propose, for the first time to the best of our knowledge, continuous-time ground truth based on optimization-based registration of lidar-inertial data on large survey-grade prior maps, which are also publicly released, each several times the size of existing ones. We conduct a rigorous evaluation of numerous state-of-the-art algorithms on MCD, report their performance, and highlight the challenges awaiting solutions from the research community.","sentences":["Perception plays a crucial role in various robot applications.","However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations.","To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses.","MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors.","Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely unexplored lidar modality.","Finally, we propose, for the first time to the best of our knowledge, continuous-time ground truth based on optimization-based registration of lidar-inertial data on large survey-grade prior maps, which are also publicly released, each several times the size of existing ones.","We conduct a rigorous evaluation of numerous state-of-the-art algorithms on MCD, report their performance, and highlight the challenges awaiting solutions from the research community."],"url":"http://arxiv.org/abs/2403.11496v1","category":"cs.RO"}
{"created":"2024-03-18 05:59:56","title":"Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics","abstract":"In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.","sentences":["In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks.","Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics.","To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment.","Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks.","DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively.","With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications.","Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin."],"url":"http://arxiv.org/abs/2403.11495v1","category":"cs.LG"}
{"created":"2024-03-18 05:58:13","title":"CCC++: Optimized Color Classified Colorization with Segment Anything Model (SAM) Empowered Object Selective Color Harmonization","abstract":"In this paper, we formulate the colorization problem into a multinomial classification problem and then apply a weighted function to classes. We propose a set of formulas to transform color values into color classes and vice versa. To optimize the classes, we experiment with different bin sizes for color class transformation. Observing class appearance, standard deviation, and model parameters on various extremely large-scale real-time images in practice we propose 532 color classes for our classification task. During training, we propose a class-weighted function based on true class appearance in each batch to ensure proper saturation of individual objects. We adjust the weights of the major classes, which are more frequently observed, by lowering them, while escalating the weights of the minor classes, which are less commonly observed. In our class re-weight formula, we propose a hyper-parameter for finding the optimal trade-off between the major and minor appeared classes. As we apply regularization to enhance the stability of the minor class, occasional minor noise may appear at the object's edges. We propose a novel object-selective color harmonization method empowered by the Segment Anything Model (SAM) to refine and enhance these edges. We propose two new color image evaluation metrics, the Color Class Activation Ratio (CCAR), and the True Activation Ratio (TAR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using six different dataset: Place, ADE, Celeba, COCO, Oxford 102 Flower, and ImageNet, in qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization, CNR and in our proposed CCAR and TAR measurement criteria while maintaining satisfactory performance in regression (MSE, PSNR), similarity (SSIM, LPIPS, UIUI), and generative criteria (FID).","sentences":["In this paper, we formulate the colorization problem into a multinomial classification problem and then apply a weighted function to classes.","We propose a set of formulas to transform color values into color classes and vice versa.","To optimize the classes, we experiment with different bin sizes for color class transformation.","Observing class appearance, standard deviation, and model parameters on various extremely large-scale real-time images in practice we propose 532 color classes for our classification task.","During training, we propose a class-weighted function based on true class appearance in each batch to ensure proper saturation of individual objects.","We adjust the weights of the major classes, which are more frequently observed, by lowering them, while escalating the weights of the minor classes, which are less commonly observed.","In our class re-weight formula, we propose a hyper-parameter for finding the optimal trade-off between the major and minor appeared classes.","As we apply regularization to enhance the stability of the minor class, occasional minor noise may appear at the object's edges.","We propose a novel object-selective color harmonization method empowered by the Segment Anything Model (SAM) to refine and enhance these edges.","We propose two new color image evaluation metrics, the Color Class Activation Ratio (CCAR), and the True Activation Ratio (TAR), to quantify the richness of color components.","We compare our proposed model with state-of-the-art models using six different dataset: Place, ADE, Celeba, COCO, Oxford 102 Flower, and ImageNet, in qualitative and quantitative approaches.","The experimental results show that our proposed model outstrips other models in visualization, CNR and in our proposed CCAR and TAR measurement criteria while maintaining satisfactory performance in regression (MSE, PSNR), similarity (SSIM, LPIPS, UIUI), and generative criteria (FID)."],"url":"http://arxiv.org/abs/2403.11494v1","category":"cs.CV"}
{"created":"2024-03-18 05:55:40","title":"Forward-backward-forward dynamics for bilevel equilibrium problem","abstract":"We introduce a forward-backward-forward (FBF) algorithm for solving bilevel equilibrium problem associated with bifunctions on a real Hilbert space. This modifies the forward-backward algorithm by relaxing cocoercivity with monotone and Lipschitzness. Further, we present the FBF dynamical system and investigate the generated trajectory's existence, uniqueness and weak convergence. We illustrate the proposed method for equilibrium problem under saddle point constraint.","sentences":["We introduce a forward-backward-forward (FBF) algorithm for solving bilevel equilibrium problem associated with bifunctions on a real Hilbert space.","This modifies the forward-backward algorithm by relaxing cocoercivity with monotone and Lipschitzness.","Further, we present the FBF dynamical system and investigate the generated trajectory's existence, uniqueness and weak convergence.","We illustrate the proposed method for equilibrium problem under saddle point constraint."],"url":"http://arxiv.org/abs/2403.11493v1","category":"math.OC"}
{"created":"2024-03-18 05:53:20","title":"SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction","abstract":"Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario. SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models. Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models. Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission. Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement. Codes are available at https://github.com/opendilab/SmartRefine/","sentences":["Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments.","Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction.","To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement.","However, they either incur a large amount of computation or bring limited improvement, if not both.","In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation.","Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario.","SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models.","Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models.","Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission.","Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement.","Codes are available at https://github.com/opendilab/SmartRefine/"],"url":"http://arxiv.org/abs/2403.11492v1","category":"cs.CV"}
{"created":"2024-03-18 05:47:03","title":"Gedanken Experiments to Destroy a Black Hole by a Test Particle: Multiply Charged Black Hole with Higher Derivative Corrections","abstract":"We investigate a gedanken experiment to destroy an extremally charged black hole by dropping a test particle, provided that there are multiple $U(1)$ gauge fields coupled with each other through higher derivative interactions. In the absence of higher derivative corrections, it is known that the Coulomb repulsion prevents a test particle that would break the extremal condition from falling into an extremal black hole and therefore the black hole cannot be destroyed. We extend this observation to include higher derivative corrections. Although the extremal condition is modified by the higher derivative interactions, we find that the repulsive force induced by the higher derivative couplings is responsible for preventing a test particle that would break the modified extremal condition to reach the event horizon. Thus, we confirm that the weak cosmic censorship conjecture holds for extremally charged black holes even in the presence of higher derivative corrections, as long as the test particle approximation is justified.","sentences":["We investigate a gedanken experiment to destroy an extremally charged black hole by dropping a test particle, provided that there are multiple $U(1)$ gauge fields coupled with each other through higher derivative interactions.","In the absence of higher derivative corrections, it is known that the Coulomb repulsion prevents a test particle that would break the extremal condition from falling into an extremal black hole and therefore the black hole cannot be destroyed.","We extend this observation to include higher derivative corrections.","Although the extremal condition is modified by the higher derivative interactions, we find that the repulsive force induced by the higher derivative couplings is responsible for preventing a test particle that would break the modified extremal condition to reach the event horizon.","Thus, we confirm that the weak cosmic censorship conjecture holds for extremally charged black holes even in the presence of higher derivative corrections, as long as the test particle approximation is justified."],"url":"http://arxiv.org/abs/2403.11488v1","category":"hep-th"}
{"created":"2024-03-18 05:38:07","title":"Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis","abstract":"We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. To the best of our knowledge, ours is the first LLM-driven approach capable of generating \"human-like\" instructions in a platform-agnostic manner, without requiring any form of training.","sentences":["We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent.","In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references.","Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis.","We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.","We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions.","Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data.","To the best of our knowledge, ours is the first LLM-driven approach capable of generating \"human-like\" instructions in a platform-agnostic manner, without requiring any form of training."],"url":"http://arxiv.org/abs/2403.11487v1","category":"cs.RO"}
{"created":"2024-03-18 05:14:40","title":"Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap","abstract":"This paper presents a novel reactive motion planning framework for navigating robots in unknown and cluttered 2D workspace. Typical existing methods are developed by enforcing the robot staying in free regions represented by the locally extracted ellipse or polygon. Instead, we navigate the robot in free space with an alternate starshaped decomposition, which is calculated directly from real-time sensor data. Additionally, a roadmap is constructed incrementally to maintain the connectivity information of the starshaped regions. Compared to the roadmap built upon connected polygons or ellipses in the conventional approaches, the concave starshaped region is better suited to capture the natural distribution of sensor data, so that the perception information can be fully exploited for robot navigation. In this sense, conservative and myopic behaviors are avoided with the proposed approach, and intricate obstacle configurations can be suitably accommodated in unknown and cluttered environments. Then, we design a heuristic exploration algorithm on the roadmap to determine the frontier points of the starshaped regions, from which short-term goals are selected to attract the robot towards the goal configuration. It is noteworthy that, a recovery mechanism is developed on the roadmap that is triggered once a non-extendable short-term goal is reached. This mechanism renders it possible to deal with dead-end situations that can be typically encountered in unknown and cluttered environments. Furthermore, safe and smooth motion within the starshaped regions is generated by employing the Dynamical System Modulation (DSM) approach on the constructed roadmap. Through comprehensive evaluation in both simulations and real-world experiments, the proposed method outperforms the benchmark methods in terms of success rate and traveling time.","sentences":["This paper presents a novel reactive motion planning framework for navigating robots in unknown and cluttered 2D workspace.","Typical existing methods are developed by enforcing the robot staying in free regions represented by the locally extracted ellipse or polygon.","Instead, we navigate the robot in free space with an alternate starshaped decomposition, which is calculated directly from real-time sensor data.","Additionally, a roadmap is constructed incrementally to maintain the connectivity information of the starshaped regions.","Compared to the roadmap built upon connected polygons or ellipses in the conventional approaches, the concave starshaped region is better suited to capture the natural distribution of sensor data, so that the perception information can be fully exploited for robot navigation.","In this sense, conservative and myopic behaviors are avoided with the proposed approach, and intricate obstacle configurations can be suitably accommodated in unknown and cluttered environments.","Then, we design a heuristic exploration algorithm on the roadmap to determine the frontier points of the starshaped regions, from which short-term goals are selected to attract the robot towards the goal configuration.","It is noteworthy that, a recovery mechanism is developed on the roadmap that is triggered once a non-extendable short-term goal is reached.","This mechanism renders it possible to deal with dead-end situations that can be typically encountered in unknown and cluttered environments.","Furthermore, safe and smooth motion within the starshaped regions is generated by employing the Dynamical System Modulation (DSM) approach on the constructed roadmap.","Through comprehensive evaluation in both simulations and real-world experiments, the proposed method outperforms the benchmark methods in terms of success rate and traveling time."],"url":"http://arxiv.org/abs/2403.11484v1","category":"cs.RO"}
{"created":"2024-03-18 05:12:54","title":"Open-World Semi-Supervised Learning for Node Classification","abstract":"Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised node classification, which trains the node classification model from scratch via contrastive learning with bias-reduced pseudo labels. Extensive experiments on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and the source code has been available on GitHub.","sentences":["Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community.","As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes).","Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance.","Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes.","However, creating general pre-trained encoders for various types of graph data has been proven to be challenging.","As such, there is a demand for an effective method that does not rely on pre-trained graph encoders.","In this paper, we propose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised node classification, which trains the node classification model from scratch via contrastive learning with bias-reduced pseudo labels.","Extensive experiments on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and the source code has been available on GitHub."],"url":"http://arxiv.org/abs/2403.11483v1","category":"cs.LG"}
{"created":"2024-03-18 05:10:13","title":"SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction","abstract":"Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing. Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data. With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities. However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset. Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation. In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data. To constrain the results generated by the diffusion model, we introduce conditional supervision constraints into the diffusion model, constraining the generated data of the diffusion model based on the input data to be reconstructed. We introduce a 3D neural network architecture into the diffusion model, successfully extending the 2D diffusion model to 3D space. Additionally, we refine the model's generation process by incorporating missing data into the generation process, resulting in reconstructions with higher consistency. Through ablation studies determining optimal parameter values, our method exhibits superior reconstruction accuracy when applied to both field datasets and synthetic datasets, effectively addressing a wide range of complex missing patterns. Our implementation is available at https://github.com/WAL-l/SeisFusion.","sentences":["Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing.","Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data.","With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities.","However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset.","Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation.","In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data.","To constrain the results generated by the diffusion model, we introduce conditional supervision constraints into the diffusion model, constraining the generated data of the diffusion model based on the input data to be reconstructed.","We introduce a 3D neural network architecture into the diffusion model, successfully extending the 2D diffusion model to 3D space.","Additionally, we refine the model's generation process by incorporating missing data into the generation process, resulting in reconstructions with higher consistency.","Through ablation studies determining optimal parameter values, our method exhibits superior reconstruction accuracy when applied to both field datasets and synthetic datasets, effectively addressing a wide range of complex missing patterns.","Our implementation is available at https://github.com/WAL-l/SeisFusion."],"url":"http://arxiv.org/abs/2403.11482v1","category":"cs.LG"}
{"created":"2024-03-18 05:07:59","title":"VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding","abstract":"We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs. VideoAgent demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro.","sentences":["We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos.","In particular, the proposed multimodal agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs.","VideoAgent demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro."],"url":"http://arxiv.org/abs/2403.11481v1","category":"cs.CV"}
{"created":"2024-03-18 04:52:11","title":"Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs","abstract":"We study the sample complexity of learning an $\\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To establish the optimality of this reduction, we develop improved bounds for $\\gamma$-discounted MDPs, showing that $\\tilde{\\Omega}\\left(SA\\frac{H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples suffice to learn an $\\epsilon$-optimal policy in weakly communicating MDPs under the regime that $\\gamma\\geq 1-1/H$, and $\\tilde{\\Omega}\\left(SA\\frac{B+H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples suffice in general MDPs when $\\gamma\\geq 1-\\frac{1}{B+H}$. Both these results circumvent the well-known lower bound of $\\tilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\epsilon^2}\\right)$ for arbitrary $\\gamma$-discounted MDPs. Our analysis develops upper bounds on certain instance-dependent variance parameters in terms of the span and transient time parameters. The weakly communicating bounds are tighter than those based on the mixing time or diameter of the MDP and may be of broader use.","sentences":["We study the sample complexity of learning an $\\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model.","For weakly communicating MDPs, we establish the complexity bound $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space.","Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.","We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs.","We argue a new transient time parameter $B$ is necessary, establish an $\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound.","Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting.","To establish the optimality of this reduction, we develop improved bounds for $\\gamma$-discounted MDPs, showing that $\\tilde{\\Omega}\\left(SA\\frac{H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples suffice to learn an $\\epsilon$-optimal policy in weakly communicating MDPs under the regime that $\\gamma\\geq 1-1/H$, and $\\tilde{\\Omega}\\left(SA\\frac{B+H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples suffice in general MDPs when $\\gamma\\geq 1-\\frac{1}{B+H}$. Both these results circumvent the well-known lower bound of $\\tilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\epsilon^2}\\right)$ for arbitrary $\\gamma$-discounted MDPs.","Our analysis develops upper bounds on certain instance-dependent variance parameters in terms of the span and transient time parameters.","The weakly communicating bounds are tighter than those based on the mixing time or diameter of the MDP and may be of broader use."],"url":"http://arxiv.org/abs/2403.11477v1","category":"cs.LG"}
{"created":"2024-03-18 04:50:42","title":"Genuine N-partite entanglement in Schwarzschild-de Sitter black hole spacetime","abstract":"Complex quantum information tasks in a gravitational background require multipartite entanglement for effective processing. Therefore, it is necessary to investigate the properties of multipartite entanglement in a relativistic setting. In this paper, we study genuine N-partite entanglement of massless Dirac fields in the Schwarzschild-de Sitter (SdS) spacetime, characterized by the presence of a black hole event horizon (BEH) and a cosmological event horizon (CEH). We obtain the general analytical expression of genuine N-partite entanglement shared by n observers near BEH and m (n+m = N) observers near CEH. It is shown that genuine N-partite entanglement monotonically decreases with the decrease of the mass of the black hole, suggesting that the Hawking effect of the black hole destroys quantum entanglement. It is interesting to note that genuine N-partite entanglement is a non-monotonic function of the cosmological constant, meaning that the Hawking effect of the expanding universe can enhance quantum entanglement. This result contrasts with multipartite entanglement in single-event horizon spacetime, offering a new perspective on the Hawking effect in multi-event horizon spacetime.","sentences":["Complex quantum information tasks in a gravitational background require multipartite entanglement for effective processing.","Therefore, it is necessary to investigate the properties of multipartite entanglement in a relativistic setting.","In this paper, we study genuine N-partite entanglement of massless Dirac fields in the Schwarzschild-de Sitter (SdS) spacetime, characterized by the presence of a black hole event horizon (BEH) and a cosmological event horizon (CEH).","We obtain the general analytical expression of genuine N-partite entanglement shared by n observers near BEH and m (n+m = N) observers near CEH.","It is shown that genuine N-partite entanglement monotonically decreases with the decrease of the mass of the black hole, suggesting that the Hawking effect of the black hole destroys quantum entanglement.","It is interesting to note that genuine N-partite entanglement is a non-monotonic function of the cosmological constant, meaning that the Hawking effect of the expanding universe can enhance quantum entanglement.","This result contrasts with multipartite entanglement in single-event horizon spacetime, offering a new perspective on the Hawking effect in multi-event horizon spacetime."],"url":"http://arxiv.org/abs/2403.11476v1","category":"gr-qc"}
{"created":"2024-03-18 04:45:44","title":"Word Order's Impacts: Insights from Reordering and Generation Analysis","abstract":"Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.","sentences":["Existing works have studied the impacts of the order of words within natural text.","They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences.","The experimental results demonstrate marginal drops.","Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''.","In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum.","Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks.","Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics."],"url":"http://arxiv.org/abs/2403.11473v1","category":"cs.CL"}
{"created":"2024-03-18 04:41:59","title":"Generative Motion Stylization within Canonical Motion Space","abstract":"Stylized motion breathes life into characters. However, the fixed skeleton structure and style representation hinder existing data-driven motion synthesis methods from generating stylized motion for various characters. In this work, we propose a generative motion stylization pipeline, named MotionS, for synthesizing diverse and stylized motion on cross-structure characters using cross-modality style prompts. Our key insight is to embed motion style into a cross-modality latent space and perceive the cross-structure skeleton topologies, allowing for motion stylization within a canonical motion space. Specifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP) model is leveraged to construct the cross-modality latent space, enabling flexible style representation within this space. Additionally, two topology-encoded tokens are learned to capture the canonical and specific skeleton topologies, facilitating cross-structure topology shifting. Subsequently, the topology-shifted stylization diffusion is designed to generate motion content for the specific skeleton and stylize it in the shifted canonical motion space using multi-modality style descriptions. Through an extensive set of examples, we demonstrate the flexibility and generalizability of our pipeline across various characters and style descriptions. Qualitative and quantitative experiments underscore the superiority of our pipeline over state-of-the-art methods, consistently delivering high-quality stylized motion across a broad spectrum of skeletal structures.","sentences":["Stylized motion breathes life into characters.","However, the fixed skeleton structure and style representation hinder existing data-driven motion synthesis methods from generating stylized motion for various characters.","In this work, we propose a generative motion stylization pipeline, named MotionS, for synthesizing diverse and stylized motion on cross-structure characters using cross-modality style prompts.","Our key insight is to embed motion style into a cross-modality latent space and perceive the cross-structure skeleton topologies, allowing for motion stylization within a canonical motion space.","Specifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP) model is leveraged to construct the cross-modality latent space, enabling flexible style representation within this space.","Additionally, two topology-encoded tokens are learned to capture the canonical and specific skeleton topologies, facilitating cross-structure topology shifting.","Subsequently, the topology-shifted stylization diffusion is designed to generate motion content for the specific skeleton and stylize it in the shifted canonical motion space using multi-modality style descriptions.","Through an extensive set of examples, we demonstrate the flexibility and generalizability of our pipeline across various characters and style descriptions.","Qualitative and quantitative experiments underscore the superiority of our pipeline over state-of-the-art methods, consistently delivering high-quality stylized motion across a broad spectrum of skeletal structures."],"url":"http://arxiv.org/abs/2403.11469v1","category":"cs.CV"}
{"created":"2024-03-18 04:41:38","title":"Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V","abstract":"Recent advancements in generative AI have suggested that by taking visual prompt, GPT-4V can demonstrate significant proficiency in image recognition task. Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use. To address this challenge, our work introduces Collage Prompting, a budget-friendly prompting approach that concatenates multiple images into a single visual input. With collage prompt, GPT-4V is able to perform image recognition on several images simultaneously. Based on the observation that the accuracy of GPT-4V's image recognition varies significantly with the order of images within the collage prompt, our method further learns to optimize the arrangement of images for maximum recognition accuracy. A graph predictor is trained to indicate the accuracy of each collage prompt, then we propose an optimization method to navigate the search space of possible image arrangements. Experiment results across various datasets demonstrate the cost-efficiency score of collage prompt is much larger than standard prompt. Additionally, collage prompt with learned arrangement achieves clearly better accuracy than collage prompt with random arrangement in GPT-4V's visual recognition.","sentences":["Recent advancements in generative AI have suggested that by taking visual prompt, GPT-4V can demonstrate significant proficiency in image recognition task.","Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use.","To address this challenge, our work introduces Collage Prompting, a budget-friendly prompting approach that concatenates multiple images into a single visual input.","With collage prompt, GPT-4V is able to perform image recognition on several images simultaneously.","Based on the observation that the accuracy of GPT-4V's image recognition varies significantly with the order of images within the collage prompt, our method further learns to optimize the arrangement of images for maximum recognition accuracy.","A graph predictor is trained to indicate the accuracy of each collage prompt, then we propose an optimization method to navigate the search space of possible image arrangements.","Experiment results across various datasets demonstrate the cost-efficiency score of collage prompt is much larger than standard prompt.","Additionally, collage prompt with learned arrangement achieves clearly better accuracy than collage prompt with random arrangement in GPT-4V's visual recognition."],"url":"http://arxiv.org/abs/2403.11468v1","category":"cs.CV"}
{"created":"2024-03-18 04:33:53","title":"Quantum Scissor from Exact Generalized Photon Number Statistics","abstract":"We report the close form expressions of the photon number statistics for a generalized coherent state and a generalized photon-added coherent state, which are shown to be crucial for proposing a variety of quantum scissor operations. The analytically obtained distributions are also capable of predicting the precise laser intensity windows for realizing a variety of quantum scissors. Truncating a photon added state overcomes the selection rule of obtaining the lower order Fock states. Photon addition also enables us to obtain a higher order Fock state in a lower order superposition. The importance of circular geometry is also demonstrated for engineering such quantum scissors.","sentences":["We report the close form expressions of the photon number statistics for a generalized coherent state and a generalized photon-added coherent state, which are shown to be crucial for proposing a variety of quantum scissor operations.","The analytically obtained distributions are also capable of predicting the precise laser intensity windows for realizing a variety of quantum scissors.","Truncating a photon added state overcomes the selection rule of obtaining the lower order Fock states.","Photon addition also enables us to obtain a higher order Fock state in a lower order superposition.","The importance of circular geometry is also demonstrated for engineering such quantum scissors."],"url":"http://arxiv.org/abs/2403.11466v1","category":"quant-ph"}
{"created":"2024-03-18 04:33:48","title":"Ultra-Long Homochiral Graphene Nanoribbons Grown Within h-BN Stacks for High-Performance Electronics","abstract":"Van der Waals encapsulation of two-dimensional materials within hexagonal boron nitride (h-BN) stacks has proven to be a promising way to create ultrahigh-performance electronic devices. However, contemporary approaches for achieving van der Waals encapsulation, which involve artificial layer stacking using mechanical transfer techniques, are difficult to control, prone to contamination, and unscalable. Here, we report on the transfer-free direct growth of high-quality graphene nanoribbons (GNRs) within h-BN stacks. The as-grown embedded GNRs exhibit highly desirable features being ultralong (up to 0.25 mm), ultranarrow ( < 5 nm), and homochiral with zigzag edges. Our atomistic simulations reveal that the mechanism underlying the embedded growth involves ultralow GNR friction when sliding between AA'-stacked h-BN layers. Using the grown structures, we demonstrate the transfer-free fabrication of embedded GNR field-effect devices that exhibit excellent performance at room temperature with mobilities of up to 4,600 $cm^{2} V^{-1} s^{-1}$ and on-off ratios of up to $10^{6}$. This paves the way to the bottom-up fabrication of high-performance electronic devices based on embedded layered materials.","sentences":["Van der Waals encapsulation of two-dimensional materials within hexagonal boron nitride (h-BN) stacks has proven to be a promising way to create ultrahigh-performance electronic devices.","However, contemporary approaches for achieving van der Waals encapsulation, which involve artificial layer stacking using mechanical transfer techniques, are difficult to control, prone to contamination, and unscalable.","Here, we report on the transfer-free direct growth of high-quality graphene nanoribbons (GNRs) within h-BN stacks.","The as-grown embedded GNRs exhibit highly desirable features being ultralong (up to 0.25 mm), ultranarrow ( < 5 nm), and homochiral with zigzag edges.","Our atomistic simulations reveal that the mechanism underlying the embedded growth involves ultralow GNR friction when sliding between AA'-stacked h-BN layers.","Using the grown structures, we demonstrate the transfer-free fabrication of embedded GNR field-effect devices that exhibit excellent performance at room temperature with mobilities of up to 4,600 $cm^{2} V^{-1} s^{-1}$ and on-off ratios of up to $10^{6}$. This paves the way to the bottom-up fabrication of high-performance electronic devices based on embedded layered materials."],"url":"http://arxiv.org/abs/2403.11465v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 04:30:26","title":"Redefining fractals by Suzuki contraction","abstract":"There has been a significant effort in recent years to generalize the traditional concept of iterated function systems (IFS).In this article, we proposed Suzuki contraction in hyperspace and finding out the fixed point for Hutchinson mapping, which is called a deterministic fractal. The deterministic fractal for such a Suzuki contraction mapping is shown to exist and to be unique. We propose the Suzuki IFS (SIFS) in the literature for fractal creation based on this conclusion. Keywords: Fixed point, iterated function system, Suzuki contraction, Suzuki iterated function system, Attractor (Deterministic Fractal).","sentences":["There has been a significant effort in recent years to generalize the traditional concept of iterated function systems (IFS).In this article, we proposed Suzuki contraction in hyperspace and finding out the fixed point for Hutchinson mapping, which is called a deterministic fractal.","The deterministic fractal for such a Suzuki contraction mapping is shown to exist and to be unique.","We propose the Suzuki IFS (SIFS) in the literature for fractal creation based on this conclusion.","Keywords: Fixed point, iterated function system, Suzuki contraction, Suzuki iterated function system, Attractor (Deterministic Fractal)."],"url":"http://arxiv.org/abs/2403.11462v1","category":"math.DS"}
{"created":"2024-03-18 04:20:16","title":"ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping","abstract":"To tackle the \"reality gap\" encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the simulation settings and realistic environments. The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM). Then, leverage the ALDM approach to enhance the simulation environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training. Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions. Specifically, it achieves a 75\\% success rate in grasping tasks under plain backgrounds and maintains a 65\\% success rate in more complex scenarios. This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating zero-shot learning in complex, unseen scenarios.","sentences":["To tackle the \"reality gap\" encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the simulation settings and realistic environments.","The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM).","Then, leverage the ALDM approach to enhance the simulation environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training.","Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions.","Specifically, it achieves a 75\\% success rate in grasping tasks under plain backgrounds and maintains a 65\\% success rate in more complex scenarios.","This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating zero-shot learning in complex, unseen scenarios."],"url":"http://arxiv.org/abs/2403.11459v1","category":"cs.RO"}
{"created":"2024-03-18 04:12:35","title":"HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models","abstract":"The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how \"offensive content\" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fine-tuning in the low-resource settings.","sentences":["The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects.","This has led to a proliferation of datasets and models related to detecting offensive content.","While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how \"offensive content\" is conceptualized, and the resulting differences in how these datasets are labeled.","In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated.","We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.}","We further find that HateCOT enables effective K-shot fine-tuning in the low-resource settings."],"url":"http://arxiv.org/abs/2403.11456v1","category":"cs.CL"}
{"created":"2024-03-18 04:01:11","title":"STAR-RIS Aided Integrated Sensing and Communication over High Mobility Scenario","abstract":"Integrated sensing and communication (ISAC) has become a promising technology for future communication system. In this paper, we consider a millimeter wave system over high mobility scenario, and propose a novel simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) aided ISAC scheme. To improve the communication service of the in-vehicle user equipment (UE) and simultaneously track and sense the vehicle with the help of nearby roadside units (RSUs), a STAR-RIS is equipped on the outside surface of the vehicle. Firstly, an efficient transmission structure is developed, where a number of training sequences with orthogonal precoders and combiners are respectively utilized at BS and RSUs for channel parameter extraction. Then, the near-field static channel model between the STAR-RIS and in-vehicle UE as well as the far-field time-frequency selective BS-RIS-RSUs channel model are characterized. By utilizing the multidimensional orthogonal matching pursuit (MOMP) algorithm, the cascaded channel parameters of the BS-RIS-RSUs links can be obtained at the RSUs. Thus, the vehicle localization and its velocity measurement can be acquired by jointly utilizing these extracted cascaded channel parameters of all RSUs. Note that the MOMP algorithm can be further utilized to extract the channel parameters of the BS-RIS-UE link for communication. With the help of sensing results, the phase shifts of the STAR-RIS are delicately designed, which can significantly improve the received signal strength for both the RSUs and the in-vehicle UE, and can finally enhance the sensing and communication performance. Moreover, the trade-off for sensing and communication is designed by optimizing the energy splitting factors of the STAR-RIS. Finally, simulation results are provided to validate the feasibility and effectiveness of our proposed STAR-RIS aided ISAC scheme.","sentences":["Integrated sensing and communication (ISAC) has become a promising technology for future communication system.","In this paper, we consider a millimeter wave system over high mobility scenario, and propose a novel simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) aided ISAC scheme.","To improve the communication service of the in-vehicle user equipment (UE) and simultaneously track and sense the vehicle with the help of nearby roadside units (RSUs), a STAR-RIS is equipped on the outside surface of the vehicle.","Firstly, an efficient transmission structure is developed, where a number of training sequences with orthogonal precoders and combiners are respectively utilized at BS and RSUs for channel parameter extraction.","Then, the near-field static channel model between the STAR-RIS and in-vehicle UE as well as the far-field time-frequency selective BS-RIS-RSUs channel model are characterized.","By utilizing the multidimensional orthogonal matching pursuit (MOMP) algorithm, the cascaded channel parameters of the BS-RIS-RSUs links can be obtained at the RSUs.","Thus, the vehicle localization and its velocity measurement can be acquired by jointly utilizing these extracted cascaded channel parameters of all RSUs.","Note that the MOMP algorithm can be further utilized to extract the channel parameters of the BS-RIS-UE link for communication.","With the help of sensing results, the phase shifts of the STAR-RIS are delicately designed, which can significantly improve the received signal strength for both the RSUs and the in-vehicle UE, and can finally enhance the sensing and communication performance.","Moreover, the trade-off for sensing and communication is designed by optimizing the energy splitting factors of the STAR-RIS.","Finally, simulation results are provided to validate the feasibility and effectiveness of our proposed STAR-RIS aided ISAC scheme."],"url":"http://arxiv.org/abs/2403.11452v1","category":"eess.SP"}
{"created":"2024-03-18 03:59:43","title":"CasSR: Activating Image Power for Real-World Image Super-Resolution","abstract":"The objective of image super-resolution is to generate clean and high-resolution images from degraded versions. Recent advancements in diffusion modeling have led to the emergence of various image super-resolution techniques that leverage pretrained text-to-image (T2I) models. Nevertheless, due to the prevalent severe degradation in low-resolution images and the inherent characteristics of diffusion models, achieving high-fidelity image restoration remains challenging. Existing methods often exhibit issues including semantic loss, artifacts, and the introduction of spurious content not present in the original image. To tackle this challenge, we propose Cascaded diffusion for Super-Resolution, CasSR , a novel method designed to produce highly detailed and realistic images. In particular, we develop a cascaded controllable diffusion model that aims to optimize the extraction of information from low-resolution images. This model generates a preliminary reference image to facilitate initial information extraction and degradation mitigation. Furthermore, we propose a multi-attention mechanism to enhance the T2I model's capability in maximizing the restoration of the original image content. Through a comprehensive blend of qualitative and quantitative analyses, we substantiate the efficacy and superiority of our approach.","sentences":["The objective of image super-resolution is to generate clean and high-resolution images from degraded versions.","Recent advancements in diffusion modeling have led to the emergence of various image super-resolution techniques that leverage pretrained text-to-image (T2I) models.","Nevertheless, due to the prevalent severe degradation in low-resolution images and the inherent characteristics of diffusion models, achieving high-fidelity image restoration remains challenging.","Existing methods often exhibit issues including semantic loss, artifacts, and the introduction of spurious content not present in the original image.","To tackle this challenge, we propose Cascaded diffusion for Super-Resolution, CasSR , a novel method designed to produce highly detailed and realistic images.","In particular, we develop a cascaded controllable diffusion model that aims to optimize the extraction of information from low-resolution images.","This model generates a preliminary reference image to facilitate initial information extraction and degradation mitigation.","Furthermore, we propose a multi-attention mechanism to enhance the T2I model's capability in maximizing the restoration of the original image content.","Through a comprehensive blend of qualitative and quantitative analyses, we substantiate the efficacy and superiority of our approach."],"url":"http://arxiv.org/abs/2403.11451v1","category":"cs.CV"}
{"created":"2024-03-18 03:54:01","title":"Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM","abstract":"Numerous studies have demonstrated the susceptibility of deep neural networks (DNNs) to subtle adversarial perturbations, prompting the development of many advanced adversarial defense methods aimed at mitigating adversarial attacks. Current defense strategies usually train DNNs for a specific adversarial attack method and can achieve good robustness in defense against this type of adversarial attack. Nevertheless, when subjected to evaluations involving unfamiliar attack modalities, empirical evidence reveals a pronounced deterioration in the robustness of DNNs. Meanwhile, there is a trade-off between the classification accuracy of clean examples and adversarial examples. Most defense methods often sacrifice the accuracy of clean examples in order to improve the adversarial robustness of DNNs. To alleviate these problems and enhance the overall robust generalization of DNNs, we propose the Test-Time Pixel-Level Adversarial Purification (TPAP) method. This approach is based on the robust overfitting characteristic of DNNs to the fast gradient sign method (FGSM) on training and test datasets. It utilizes FGSM for adversarial purification, to process images for purifying unknown adversarial perturbations from pixels at testing time in a \"counter changes with changelessness\" manner, thereby enhancing the defense capability of DNNs against various unknown adversarial attacks. Extensive experimental results show that our method can effectively improve both overall robust generalization of DNNs, notably over previous methods.","sentences":["Numerous studies have demonstrated the susceptibility of deep neural networks (DNNs) to subtle adversarial perturbations, prompting the development of many advanced adversarial defense methods aimed at mitigating adversarial attacks.","Current defense strategies usually train DNNs for a specific adversarial attack method and can achieve good robustness in defense against this type of adversarial attack.","Nevertheless, when subjected to evaluations involving unfamiliar attack modalities, empirical evidence reveals a pronounced deterioration in the robustness of DNNs.","Meanwhile, there is a trade-off between the classification accuracy of clean examples and adversarial examples.","Most defense methods often sacrifice the accuracy of clean examples in order to improve the adversarial robustness of DNNs.","To alleviate these problems and enhance the overall robust generalization of DNNs, we propose the Test-Time Pixel-Level Adversarial Purification (TPAP) method.","This approach is based on the robust overfitting characteristic of DNNs to the fast gradient sign method (FGSM) on training and test datasets.","It utilizes FGSM for adversarial purification, to process images for purifying unknown adversarial perturbations from pixels at testing time in a \"counter changes with changelessness\" manner, thereby enhancing the defense capability of DNNs against various unknown adversarial attacks.","Extensive experimental results show that our method can effectively improve both overall robust generalization of DNNs, notably over previous methods."],"url":"http://arxiv.org/abs/2403.11448v1","category":"cs.CV"}
{"created":"2024-03-18 03:44:55","title":"LLM Guided Evolution - The Automation of Models Advancing Models","abstract":"In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming. Our study introduces \"Guided Evolution\" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code. GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers. Our unique \"Evolution of Thought\" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations. This results in a self-sustaining feedback loop that augments decision-making in model evolution. GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature. This not only accelerates the evolution process but also injects expert like creativity and insight into the process. Our application of GE in evolving the ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously produced variants with improved accuracy, increasing from 92.52% to 93.34%, without compromising model compactness. This underscores the potential of LLMs to accelerate the traditional model design pipeline, enabling models to autonomously evolve and enhance their own designs.","sentences":["In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming.","Our study introduces \"Guided Evolution\" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code.","GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers.","Our unique \"Evolution of Thought\" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations.","This results in a self-sustaining feedback loop that augments decision-making in model evolution.","GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature.","This not only accelerates the evolution process but also injects expert like creativity and insight into the process.","Our application of GE in evolving the ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously produced variants with improved accuracy, increasing from 92.52% to 93.34%, without compromising model compactness.","This underscores the potential of LLMs to accelerate the traditional model design pipeline, enabling models to autonomously evolve and enhance their own designs."],"url":"http://arxiv.org/abs/2403.11446v1","category":"cs.NE"}
{"created":"2024-03-18 03:43:45","title":"Budget Recycling Differential Privacy","abstract":"Differential Privacy (DP) mechanisms usually {force} reduction in data utility by producing ``out-of-bound'' noisy results for a tight privacy budget. We introduce the Budget Recycling Differential Privacy (BR-DP) framework, designed to provide soft-bounded noisy outputs for a broad range of existing DP mechanisms. By ``soft-bounded,\" we refer to the mechanism's ability to release most outputs within a predefined error boundary, thereby improving utility and maintaining privacy simultaneously. The core of BR-DP consists of two components: a DP kernel responsible for generating a noisy answer per iteration, and a recycler that probabilistically recycles/regenerates or releases the noisy answer. We delve into the privacy accounting of BR-DP, culminating in the development of a budgeting principle that optimally sub-allocates the available budget between the DP kernel and the recycler. Furthermore, we introduce algorithms for tight BR-DP accounting in composition scenarios, and our findings indicate that BR-DP achieves reduced privacy leakage post-composition compared to DP. Additionally, we explore the concept of privacy amplification via subsampling within the BR-DP framework and propose optimal sampling rates for BR-DP across various queries. We experiment with real data, and the results demonstrate BR-DP's effectiveness in lifting the utility-privacy tradeoff provided by DP mechanisms.","sentences":["Differential Privacy (DP) mechanisms usually {force} reduction in data utility by producing ``out-of-bound'' noisy results for a tight privacy budget.","We introduce the Budget Recycling Differential Privacy (BR-DP) framework, designed to provide soft-bounded noisy outputs for a broad range of existing DP mechanisms.","By ``soft-bounded,\" we refer to the mechanism's ability to release most outputs within a predefined error boundary, thereby improving utility and maintaining privacy simultaneously.","The core of BR-DP consists of two components: a DP kernel responsible for generating a noisy answer per iteration, and a recycler that probabilistically recycles/regenerates or releases the noisy answer.","We delve into the privacy accounting of BR-DP, culminating in the development of a budgeting principle that optimally sub-allocates the available budget between the DP kernel and the recycler.","Furthermore, we introduce algorithms for tight BR-DP accounting in composition scenarios, and our findings indicate that BR-DP achieves reduced privacy leakage post-composition compared to DP.","Additionally, we explore the concept of privacy amplification via subsampling within the BR-DP framework and propose optimal sampling rates for BR-DP across various queries.","We experiment with real data, and the results demonstrate BR-DP's effectiveness in lifting the utility-privacy tradeoff provided by DP mechanisms."],"url":"http://arxiv.org/abs/2403.11445v1","category":"cs.CR"}
{"created":"2024-03-18 03:41:51","title":"Impact of local CP-odd domain in hot QCD on axionic domain-wall interpretation for NANOGrav 15-year Data","abstract":"We argue that the axionic domain-wall with a QCD bias may be incompatible with the NANOGrav 15-year data on a stochastic gravitational wave (GW) background, when the domain wall network collapses in the hot-QCD induced local CP-odd domain. This is due to the drastic suppression of the QCD bias set by the QCD topological susceptibility in the presence of the CP-odd domain with nonzero $\\theta$ parameter of order one which the QCD sphaleron could generate. We quantify the effect on the GW signals by working on a low-energy effective model of Nambu-Jona-Lasinio type in the mean field approximation. We find that only at $\\theta=\\pi$, the QCD bias tends to get significantly large enough due to the criticality of the thermal CP restoration, which would, however, give too big signal strengths to be consistent with the NANOGrav 15-year data and would also be subject to the strength of the phase transition at the criticality.","sentences":["We argue that the axionic domain-wall with a QCD bias may be incompatible with the NANOGrav 15-year data on a stochastic gravitational wave (GW) background, when the domain wall network collapses in the hot-QCD induced local CP-odd domain.","This is due to the drastic suppression of the QCD bias set by the QCD topological susceptibility in the presence of the CP-odd domain with nonzero $\\theta$ parameter of order one which the QCD sphaleron could generate.","We quantify the effect on the GW signals by working on a low-energy effective model of Nambu-Jona-Lasinio type in the mean field approximation.","We find that only at $\\theta=\\pi$, the QCD bias tends to get significantly large enough due to the criticality of the thermal CP restoration, which would, however, give too big signal strengths to be consistent with the NANOGrav 15-year data and would also be subject to the strength of the phase transition at the criticality."],"url":"http://arxiv.org/abs/2403.11444v1","category":"hep-ph"}
{"created":"2024-03-18 03:39:57","title":"On Linear Threshold Policies for Continuous-Time Dynamic Yield Management","abstract":"We study the finite-horizon continuous-time yield management problem with stationary arrival rates and two customer classes. We consider a class of linear threshold policies proposed by Hodge (2008), in which each online (i.e., less desirable) customer is accepted if and only if the remaining inventory at the customer's arrival time exceeds a threshold that linearly decreases over the selling horizon. Using a discrete-time Markov chain representation of sample paths over inventory-time space, we show that a range of such linear threshold policies achieve uniformly bounded regret. We then generalize this result to analogous policies for the same problem with arbitrarily many customer classes. Numerical simulations demonstrate linear threshold policies' competitiveness with existing heuristics and illustrate the effects of the linear threshold's slope.","sentences":["We study the finite-horizon continuous-time yield management problem with stationary arrival rates and two customer classes.","We consider a class of linear threshold policies proposed by Hodge (2008), in which each online (i.e., less desirable) customer is accepted if and only if the remaining inventory at the customer's arrival time exceeds a threshold that linearly decreases over the selling horizon.","Using a discrete-time Markov chain representation of sample paths over inventory-time space, we show that a range of such linear threshold policies achieve uniformly bounded regret.","We then generalize this result to analogous policies for the same problem with arbitrarily many customer classes.","Numerical simulations demonstrate linear threshold policies' competitiveness with existing heuristics and illustrate the effects of the linear threshold's slope."],"url":"http://arxiv.org/abs/2403.11443v1","category":"math.OC"}
{"created":"2024-03-18 03:26:18","title":"StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation","abstract":"Large Language Models (LLMs) demonstrate superior performance in generative scenarios and have attracted widespread attention. Among them, stylized dialogue generation is essential in the context of LLMs for building intelligent and engaging dialogue agent. However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks. In particular, stylized dialogue generation suffers from a severe lack of supervised data. Furthermore, although many prompt-based methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement. In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control. Based on this, we propose the stylized dialogue framework StyleChat via recitation-augmented memory strategy and multi-task style learning strategy to promote generalization ability. To evaluate the effectiveness of our approach, we created a test benchmark that included both a generation task and a choice task to comprehensively evaluate trained models and assess whether styles and preferences are remembered and understood. Experimental results show that our proposed framework StyleChat outperforms all the baselines and helps to break the style boundary of LLMs.","sentences":["Large Language Models (LLMs) demonstrate superior performance in generative scenarios and have attracted widespread attention.","Among them, stylized dialogue generation is essential in the context of LLMs for building intelligent and engaging dialogue agent.","However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks.","In particular, stylized dialogue generation suffers from a severe lack of supervised data.","Furthermore, although many prompt-based methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement.","In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control.","Based on this, we propose the stylized dialogue framework StyleChat via recitation-augmented memory strategy and multi-task style learning strategy to promote generalization ability.","To evaluate the effectiveness of our approach, we created a test benchmark that included both a generation task and a choice task to comprehensively evaluate trained models and assess whether styles and preferences are remembered and understood.","Experimental results show that our proposed framework StyleChat outperforms all the baselines and helps to break the style boundary of LLMs."],"url":"http://arxiv.org/abs/2403.11439v1","category":"cs.CL"}
{"created":"2024-03-18 03:12:40","title":"Deep Holes of Twisted Reed-Solomon Codes","abstract":"The deep holes of a linear code are the vectors that achieve the maximum error distance to the code. There has been extensive research on the topic of deep holes in Reed-Solomon codes. As a generalization of Reed-Solomon codes, we investigate the problem of deep holes of a class of twisted Reed-Solomon codes in this paper. The covering radius and a standard class of deep holes of twisted Reed-Solomon codes ${\\rm TRS}_k(\\mathcal{A}, \\theta)$ are obtained for a general evaluation set $\\mathcal{A} \\subseteq \\mathbb{F}_q$. Furthermore, we consider the problem of determining all deep holes of the full-length twisted Reed-Solomon codes ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$. Specifically, we prove that there are no other deep holes of ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$ for $\\frac{3q-8}{4} \\leq k\\leq q-4$ when $q$ is even, and $\\frac{3q+2\\sqrt{q}-7}{4} \\leq k\\leq q-4$ when $q$ is odd. We also completely determine their deep holes for $q-3 \\leq k \\leq q-1$.","sentences":["The deep holes of a linear code are the vectors that achieve the maximum error distance to the code.","There has been extensive research on the topic of deep holes in Reed-Solomon codes.","As a generalization of Reed-Solomon codes, we investigate the problem of deep holes of a class of twisted Reed-Solomon codes in this paper.","The covering radius and a standard class of deep holes of twisted Reed-Solomon codes ${\\rm TRS}_k(\\mathcal{A}, \\theta)$ are obtained for a general evaluation set $\\mathcal{A} \\subseteq \\mathbb{F}_q$.","Furthermore, we consider the problem of determining all deep holes of the full-length twisted Reed-Solomon codes ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$. Specifically, we prove that there are no other deep holes of ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$ for $\\frac{3q-8}{4} \\leq k\\leq q-4$ when $q$ is even, and $\\frac{3q+2\\sqrt{q}-7}{4} \\leq k\\leq q-4$ when $q$ is odd.","We also completely determine their deep holes for $q-3 \\leq k \\leq q-1$."],"url":"http://arxiv.org/abs/2403.11436v1","category":"cs.IT"}
{"created":"2024-03-18 02:59:13","title":"Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making","abstract":"With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained models in terms of explainability and causality for spatial and temporal correlations. We show that the weights in the first head encode the positions of the neighboring vehicles while the second head focuses on the leader vehicle exclusively. Also, the ego vehicle's action is causally dependent on the vehicles in the target lane spatially and temporally. Through these findings, we reliably show that these techniques can help practitioners decipher the results of the DRL algorithms.","sentences":["With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded.","Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output.","However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs).","Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework.","We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment.","We provide some analytical techniques for discussing the interpretability of the trained models in terms of explainability and causality for spatial and temporal correlations.","We show that the weights in the first head encode the positions of the neighboring vehicles while the second head focuses on the leader vehicle exclusively.","Also, the ego vehicle's action is causally dependent on the vehicles in the target lane spatially and temporally.","Through these findings, we reliably show that these techniques can help practitioners decipher the results of the DRL algorithms."],"url":"http://arxiv.org/abs/2403.11432v1","category":"cs.RO"}
{"created":"2024-03-18 02:48:06","title":"Long-range Ising model for regional-scale seismic risk analysis","abstract":"This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis. The application of the PBEE framework at a regional scale entails estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations. However, these simulations often assume independence or employ simplistic dependency models among capacities of structures, leading to significant misrepresentation of risk. The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy. The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data. To demonstrate the proposed method, we applied the Ising model to $156$ buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to $182$ buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool. In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damages. The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach.","sentences":["This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis.","The application of the PBEE framework at a regional scale entails estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations.","However, these simulations often assume independence or employ simplistic dependency models among capacities of structures, leading to significant misrepresentation of risk.","The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy.","The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data.","To demonstrate the proposed method, we applied the Ising model to $156$ buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to $182$ buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool.","In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damages.","The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach."],"url":"http://arxiv.org/abs/2403.11429v1","category":"stat.AP"}
{"created":"2024-03-18 02:38:55","title":"VmambaIR: Visual State Space Model for Image Restoration","abstract":"Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact. However, CNNs have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. We utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks.","sentences":["Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs.","Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact.","However, CNNs have limitations in capturing long-range dependencies.","DMs require large prior models and computationally intensive denoising steps.","Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size.","To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks.","We utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN).","Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions.","Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution.","Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters.","Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks."],"url":"http://arxiv.org/abs/2403.11423v1","category":"cs.CV"}
{"created":"2024-03-18 02:30:23","title":"FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines","abstract":"Cost of serving large language models (LLM) is high, but the expensive and scarce GPUs are poorly efficient when generating tokens sequentially, unless the batch of sequences is enlarged. However, the batch size is limited by some constantly reused intermediate results, namely KV-Cache. They occupy too much memory to fit more sequences into a GPU simultaneously. While they could be offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.   We find a way to decompose the transformer models into two parts of different characteristics, one of which includes the memory-bound KV-Cache accessing. Our key insight is that the aggregated memory capacity, bandwidth, and computing power of CPUs across multiple nodes is an efficient option to process this part. Performance improvement comes from reduced data transmission overhead and boosted GPU throughput to process the other model part. Moreover, we address efficiency challenges brought by heterogeneity at both temporal and inter-device scopes using scheduling and performance modeling techniques. Evaluation results show that our system achieves 1.88x - 5.04x the throughput of vLLM when serving modern LLMs with the same GPU.","sentences":["Cost of serving large language models (LLM) is high, but the expensive and scarce GPUs are poorly efficient when generating tokens sequentially, unless the batch of sequences is enlarged.","However, the batch size is limited by some constantly reused intermediate results, namely KV-Cache.","They occupy too much memory to fit more sequences into a GPU simultaneously.","While they could be offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.   ","We find a way to decompose the transformer models into two parts of different characteristics, one of which includes the memory-bound KV-Cache accessing.","Our key insight is that the aggregated memory capacity, bandwidth, and computing power of CPUs across multiple nodes is an efficient option to process this part.","Performance improvement comes from reduced data transmission overhead and boosted GPU throughput to process the other model part.","Moreover, we address efficiency challenges brought by heterogeneity at both temporal and inter-device scopes using scheduling and performance modeling techniques.","Evaluation results show that our system achieves 1.88x - 5.04x the throughput of vLLM when serving modern LLMs with the same GPU."],"url":"http://arxiv.org/abs/2403.11421v1","category":"cs.DC"}
{"created":"2024-03-18 02:20:22","title":"Neural network representation of quantum systems","abstract":"It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.","sentences":["It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points.","In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters.","Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral.","The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit.","Our findings bring machine learning closer to the quantum world."],"url":"http://arxiv.org/abs/2403.11420v1","category":"hep-th"}
{"created":"2024-03-18 02:12:12","title":"Variational Sampling of Temporal Trajectories","abstract":"A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \\in \\mathcal{Z}$ and (b) transition function $f: (\\mathcal{Z}, \\mathcal{T}) \\to \\mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a recurrent neural network. Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inference, i.e., uncertainty estimation, likelihood evaluations and out of distribution detection for abnormal trajectories. These capabilities can have implications for various downstream tasks, e.g., simulation and evaluation for reinforcement learning.","sentences":["A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \\in \\mathcal{Z}$ and (b) transition function $f: (\\mathcal{Z}, \\mathcal{T})","\\to \\mathcal{Z}$ often influenced by the control of the underlying dynamical system.","Existing methods often model the transition function as a differential equation or as a recurrent neural network.","Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization.","In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space.","Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inference, i.e., uncertainty estimation, likelihood evaluations and out of distribution detection for abnormal trajectories.","These capabilities can have implications for various downstream tasks, e.g., simulation and evaluation for reinforcement learning."],"url":"http://arxiv.org/abs/2403.11418v1","category":"cs.LG"}
{"created":"2024-03-18 02:11:34","title":"Positioning Using Wireless Networks: Applications, Recent Progress and Future Challenges","abstract":"Positioning has recently received considerable attention as a key enabler in emerging applications such as extended reality, unmanned aerial vehicles and smart environments. These applications require both data communication and high-precision positioning, and thus they are particularly well-suited to be offered in wireless networks (WNs). The purpose of this paper is to provide a comprehensive overview of existing works and new trends in the field of positioning techniques from both the academic and industrial perspectives. The paper provides a comprehensive overview of positioning in WNs, covering the background, applications, measurements, state-of-the-art technologies and future challenges. The paper outlines the applications of positioning from the perspectives of public facilities, enterprises and individual users. We investigate the key performance indicators and measurements of positioning systems, followed by the review of the key enabler techniques such as artificial intelligence/large models and adaptive systems. Next, we discuss a number of typical wireless positioning technologies. We extend our overview beyond the academic progress, to include the standardization efforts, and finally, we provide insight into the challenges that remain. The comprehensive overview of exisitng efforts and new trends in the field of positioning from both the academic and industrial communities would be a useful reference to researchers in the field.","sentences":["Positioning has recently received considerable attention as a key enabler in emerging applications such as extended reality, unmanned aerial vehicles and smart environments.","These applications require both data communication and high-precision positioning, and thus they are particularly well-suited to be offered in wireless networks (WNs).","The purpose of this paper is to provide a comprehensive overview of existing works and new trends in the field of positioning techniques from both the academic and industrial perspectives.","The paper provides a comprehensive overview of positioning in WNs, covering the background, applications, measurements, state-of-the-art technologies and future challenges.","The paper outlines the applications of positioning from the perspectives of public facilities, enterprises and individual users.","We investigate the key performance indicators and measurements of positioning systems, followed by the review of the key enabler techniques such as artificial intelligence/large models and adaptive systems.","Next, we discuss a number of typical wireless positioning technologies.","We extend our overview beyond the academic progress, to include the standardization efforts, and finally, we provide insight into the challenges that remain.","The comprehensive overview of exisitng efforts and new trends in the field of positioning from both the academic and industrial communities would be a useful reference to researchers in the field."],"url":"http://arxiv.org/abs/2403.11417v1","category":"eess.SP"}
{"created":"2024-03-18 02:08:58","title":"DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation","abstract":"Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive performance of DreamSampler compared to existing approaches, while providing new applications.","sentences":["Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs).","While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing.","To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization.","Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction.","Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive performance of DreamSampler compared to existing approaches, while providing new applications."],"url":"http://arxiv.org/abs/2403.11415v1","category":"cs.CV"}
{"created":"2024-03-18 02:01:58","title":"Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems","abstract":"When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.","sentences":["When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly.","Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification.","This work aims to bridge the gap by developing a suggestion question generator.","To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts.","Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches."],"url":"http://arxiv.org/abs/2403.11413v1","category":"cs.CL"}
{"created":"2024-03-18 01:53:21","title":"Dynamic Home Care Routing and Scheduling with Uncertain Number of Visits per Referral","abstract":"Despite the rapid growth of the home care industry, research on the scheduling and routing of home care visits in the presence of uncertainty is still limited. This paper investigates a dynamic version of this problem in which the number of referrals and their required number of visits are uncertain. We develop a Markov decision process (MDP) model for the single-nurse problem to minimize the expected weighted sum of the rejection, diversion, overtime, and travel time costs. Since optimally solving the MDP is intractable, we employ an approximate linear program (ALP) to obtain a feasible policy. The typical ALP approach can only solve very small-scale instances of the problem. We derive an intuitively explainable closed-form solution for the optimal ALP parameters in a special case of the problem. Inspired by this form, we provide two heuristic reduction techniques for the ALP model in the general problem to solve large-scale instances in an acceptable time. Numerical results show that the ALP policy outperforms a myopic policy that reflects current practice, and is better than a scenario-based policy in most instances considered.","sentences":["Despite the rapid growth of the home care industry, research on the scheduling and routing of home care visits in the presence of uncertainty is still limited.","This paper investigates a dynamic version of this problem in which the number of referrals and their required number of visits are uncertain.","We develop a Markov decision process (MDP) model for the single-nurse problem to minimize the expected weighted sum of the rejection, diversion, overtime, and travel time costs.","Since optimally solving the MDP is intractable, we employ an approximate linear program (ALP) to obtain a feasible policy.","The typical ALP approach can only solve very small-scale instances of the problem.","We derive an intuitively explainable closed-form solution for the optimal ALP parameters in a special case of the problem.","Inspired by this form, we provide two heuristic reduction techniques for the ALP model in the general problem to solve large-scale instances in an acceptable time.","Numerical results show that the ALP policy outperforms a myopic policy that reflects current practice, and is better than a scenario-based policy in most instances considered."],"url":"http://arxiv.org/abs/2403.11410v1","category":"math.OC"}
{"created":"2024-03-18 01:48:50","title":"Layer-diverse Negative Sampling for Graph Neural Networks","abstract":"Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.","sentences":["Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data.","However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing.","To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation.","This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples.","To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs.","Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance.","Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing."],"url":"http://arxiv.org/abs/2403.11408v1","category":"cs.LG"}
{"created":"2024-03-18 01:47:24","title":"Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors","abstract":"Interest in the use of Denoising Diffusion Models (DDM) as priors for solving inverse Bayesian problems has recently increased significantly. However, sampling from the resulting posterior distribution poses a challenge. To solve this problem, previous works have proposed approximations to bias the drift term of the diffusion. In this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods. We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks.","sentences":["Interest in the use of Denoising Diffusion Models (DDM) as priors for solving inverse Bayesian problems has recently increased significantly.","However, sampling from the resulting posterior distribution poses a challenge.","To solve this problem, previous works have proposed approximations to bias the drift term of the diffusion.","In this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods.","We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks."],"url":"http://arxiv.org/abs/2403.11407v1","category":"eess.IV"}
{"created":"2024-03-18 01:36:22","title":"A Deep Learning Method for Beat-Level Risk Analysis and Interpretation of Atrial Fibrillation Patients during Sinus Rhythm","abstract":"Atrial Fibrillation (AF) is a common cardiac arrhythmia. Many AF patients experience complications such as stroke and other cardiovascular issues. Early detection of AF is crucial. Existing algorithms can only distinguish ``AF rhythm in AF patients'' from ``sinus rhythm in normal individuals'' . However, AF patients do not always exhibit AF rhythm, posing a challenge for diagnosis when the AF rhythm is absent. To address this, this paper proposes a novel artificial intelligence (AI) algorithm to distinguish ``sinus rhythm in AF patients'' and ``sinus rhythm in normal individuals'' in beat-level. We introduce beat-level risk interpreters, trend risk interpreters, addressing the interpretability issues of deep learning models and the difficulty in explaining AF risk trends. Additionally, the beat-level information fusion decision is presented to enhance model accuracy. The experimental results demonstrate that the average AUC for single beats used as testing data from CPSC 2021 dataset is 0.7314. By employing 150 beats for information fusion decision algorithm, the average AUC can reach 0.7591. Compared to previous segment-level algorithms, we utilized beats as input, reducing data dimensionality and making the model more lightweight, facilitating deployment on portable medical devices. Furthermore, we draw new and interesting findings through average beat analysis and subgroup analysis, considering varying risk levels.","sentences":["Atrial Fibrillation (AF) is a common cardiac arrhythmia.","Many AF patients experience complications such as stroke and other cardiovascular issues.","Early detection of AF is crucial.","Existing algorithms can only distinguish ``AF rhythm in AF patients'' from ``sinus rhythm in normal individuals'' .","However, AF patients do not always exhibit AF rhythm, posing a challenge for diagnosis when the AF rhythm is absent.","To address this, this paper proposes a novel artificial intelligence (AI) algorithm to distinguish ``sinus rhythm in AF patients'' and ``sinus rhythm in normal individuals'' in beat-level.","We introduce beat-level risk interpreters, trend risk interpreters, addressing the interpretability issues of deep learning models and the difficulty in explaining AF risk trends.","Additionally, the beat-level information fusion decision is presented to enhance model accuracy.","The experimental results demonstrate that the average AUC for single beats used as testing data from CPSC 2021 dataset is 0.7314.","By employing 150 beats for information fusion decision algorithm, the average AUC can reach 0.7591.","Compared to previous segment-level algorithms, we utilized beats as input, reducing data dimensionality and making the model more lightweight, facilitating deployment on portable medical devices.","Furthermore, we draw new and interesting findings through average beat analysis and subgroup analysis, considering varying risk levels."],"url":"http://arxiv.org/abs/2403.11405v1","category":"eess.SP"}
{"created":"2024-03-18 01:27:01","title":"Scalable and programmable quantum computing platform for optical non-Gaussian input states","abstract":"Quantum computing has been pursued with various hardware platforms, and an optical system is one of the most reasonable choices for large-scale computation. In the optical continuous-variable computation scheme, the incorporation of Gaussian gates and a highly non-classical non-Gaussian state enables universal quantum computation. Although basic technologies for Gaussian gates and non-Gaussian state generations have long been developed, these building blocks have not yet been integrated in a scalable fashion. Here, we integrate them to develop a scalable and programmable optical quantum computing platform that can sequentially perform an essential Gaussian gate, the squeezing gate, on a non-Gaussian input state. The key enablers are a loop-based optical circuit with dynamical and programmable controllability and its time-synchronization with the probabilistic non-Gaussian state generation. We verify the deterministic, programmable, and repeatable quantum gates on a typical non-Gaussian state by implementing up to three-step gates. The gates implemented are so high-quality that strong evidence of the states' non-classicalities, negativities of the Wigner functions, are preserved even after multistep gates. This platform is compatible with other non-Gaussian states and realizes large-scale universal quantum computing by incorporating other existing processing technologies.","sentences":["Quantum computing has been pursued with various hardware platforms, and an optical system is one of the most reasonable choices for large-scale computation.","In the optical continuous-variable computation scheme, the incorporation of Gaussian gates and a highly non-classical non-Gaussian state enables universal quantum computation.","Although basic technologies for Gaussian gates and non-Gaussian state generations have long been developed, these building blocks have not yet been integrated in a scalable fashion.","Here, we integrate them to develop a scalable and programmable optical quantum computing platform that can sequentially perform an essential Gaussian gate, the squeezing gate, on a non-Gaussian input state.","The key enablers are a loop-based optical circuit with dynamical and programmable controllability and its time-synchronization with the probabilistic non-Gaussian state generation.","We verify the deterministic, programmable, and repeatable quantum gates on a typical non-Gaussian state by implementing up to three-step gates.","The gates implemented are so high-quality that strong evidence of the states' non-classicalities, negativities of the Wigner functions, are preserved even after multistep gates.","This platform is compatible with other non-Gaussian states and realizes large-scale universal quantum computing by incorporating other existing processing technologies."],"url":"http://arxiv.org/abs/2403.11404v1","category":"quant-ph"}
{"created":"2024-03-18 01:20:38","title":"Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT","abstract":"The rapid advancement of generative Artificial Intelligence (AI) technologies, particularly Generative Pre-trained Transformer (GPT) models such as ChatGPT, has the potential to significantly impact cybersecurity. In this study, we investigated the impact of GPTs, specifically ChatGPT, on tertiary education in cybersecurity, and provided recommendations for universities to adapt their curricula to meet the evolving needs of the industry. Our research highlighted the importance of understanding the alignment between GPT's ``mental model'' and human cognition, as well as the enhancement of GPT capabilities to human skills based on Bloom's taxonomy. By analyzing current educational practices and the alignment of curricula with industry requirements, we concluded that universities providing practical degrees like cybersecurity should align closely with industry demand and embrace the inevitable generative AI revolution, while applying stringent ethics oversight to safeguard responsible GPT usage. We proposed a set of recommendations focused on updating university curricula, promoting agility within universities, fostering collaboration between academia, industry, and policymakers, and evaluating and assessing educational outcomes.","sentences":["The rapid advancement of generative Artificial Intelligence (AI) technologies, particularly Generative Pre-trained Transformer (GPT) models such as ChatGPT, has the potential to significantly impact cybersecurity.","In this study, we investigated the impact of GPTs, specifically ChatGPT, on tertiary education in cybersecurity, and provided recommendations for universities to adapt their curricula to meet the evolving needs of the industry.","Our research highlighted the importance of understanding the alignment between GPT's ``mental model'' and human cognition, as well as the enhancement of GPT capabilities to human skills based on Bloom's taxonomy.","By analyzing current educational practices and the alignment of curricula with industry requirements, we concluded that universities providing practical degrees like cybersecurity should align closely with industry demand and embrace the inevitable generative AI revolution, while applying stringent ethics oversight to safeguard responsible GPT usage.","We proposed a set of recommendations focused on updating university curricula, promoting agility within universities, fostering collaboration between academia, industry, and policymakers, and evaluating and assessing educational outcomes."],"url":"http://arxiv.org/abs/2403.11402v1","category":"cs.CY"}
{"created":"2024-03-18 01:18:48","title":"Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning","abstract":"This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the scene. Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, question answering, and interactive planning. We believe Scene-LLM advances the field of 3D visual understanding and reasoning, offering new possibilities for sophisticated agent interactions in indoor settings.","sentences":["This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs).","Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates.","The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information.","Unique to our approach is the integration of both scene-level and ego-centric 3D information.","This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization.","Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the scene.","Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, question answering, and interactive planning.","We believe Scene-LLM advances the field of 3D visual understanding and reasoning, offering new possibilities for sophisticated agent interactions in indoor settings."],"url":"http://arxiv.org/abs/2403.11401v1","category":"cs.CV"}
{"created":"2024-03-18 01:07:48","title":"Automated data processing and feature engineering for deep learning and big data applications: a survey","abstract":"Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for Big Data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing--e.g., data cleaning, labeling, missing data imputation, and categorical data encoding--as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering--specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.","sentences":["Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data.","This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning.","It has also simplified the design of machine learning systems as the learning process is highly automated.","However, not all data processing tasks in conventional deep learning pipelines have been automated.","In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training.","Recently, special techniques for automating these tasks have emerged.","The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications.","Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for Big Data tasks by automating all intermediate processing stages.","In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing--e.g., data cleaning, labeling, missing data imputation, and categorical data encoding--as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering--specifically, automated feature extraction, feature construction and feature selection.","In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline."],"url":"http://arxiv.org/abs/2403.11395v1","category":"cs.LG"}
{"created":"2024-03-18 01:01:24","title":"Branching algebras for the general linear Lie superalgebra","abstract":"We develop an algebraic approach to the branching of representations of the general linear Lie superalgebra $\\mathfrak{gl}_{p|q}({\\mathbb C})$, by constructing certain super commutative algebras whose structure encodes the branching rules. Using this approach, we derive the branching rules for restricting any irreducible polynomial representation $V$ of $\\mathfrak{gl}_{p|q}({\\mathbb C})$ to a regular subalgebra isomorphic to $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus \\mathfrak{gl}_{r'|s'}({\\mathbb C})$, $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus\\mathfrak{gl}_1({\\mathbb C})^{r'+s'}$ or $\\mathfrak{gl}_{r|s}({\\mathbb C})$, with $r+r'=p$ and $s+s'=q$. In the case of $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus\\mathfrak{gl}_1({\\mathbb C})^{r'+s'}$ with $s=0$ or $s=1$ but general $r$, we also construct a basis for the space of $\\mathfrak{gl}_{r|s}({\\mathbb C})$ highest weight vectors in $V$; when $r=s=0$, the branching rule leads to explicit expressions for the weight multiplicities of $V$ in terms of Kostka numbers.","sentences":["We develop an algebraic approach to the branching of representations of the general linear Lie superalgebra $\\mathfrak{gl}_{p|q}({\\mathbb C})$, by constructing certain super commutative algebras whose structure encodes the branching rules.","Using this approach, we derive the branching rules for restricting any irreducible polynomial representation $V$ of $\\mathfrak{gl}_{p|q}({\\mathbb C})$ to a regular subalgebra isomorphic to $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus \\mathfrak{gl}_{r'|s'}({\\mathbb C})$, $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus\\mathfrak{gl}_1({\\mathbb C})^{r'+s'}$ or $\\mathfrak{gl}_{r|s}({\\mathbb C})$, with $r+r'=p$ and $s+s'=q$. In the case of $\\mathfrak{gl}_{r|s}({\\mathbb C})\\oplus\\mathfrak{gl}_1({\\mathbb C})^{r'+s'}$ with $s=0$ or $s=1$ but general $r$, we also construct a basis for the space of $\\mathfrak{gl}_{r|s}({\\mathbb C})$ highest weight vectors in $V$; when $r=s=0$, the branching rule leads to explicit expressions for the weight multiplicities of $V$ in terms of Kostka numbers."],"url":"http://arxiv.org/abs/2403.11393v1","category":"math.RT"}
{"created":"2024-03-18 01:00:23","title":"Gravitational Wave Searches for Post-Merger Remnants of GW170817 and GW190425","abstract":"We present the results of two searches for gravitational waves from the post-merger remnants of the binary neutron star coalescence events GW170817 and GW190425. The searches are fully coherent over 1800~s of data from the 2nd (for GW170817) and 3rd (for GW190425) observing runs of the LIGO and Virgo observatories. The searches compute the matched filter $\\mathcal{F}$-statistic, and use a piecewise model of the rapidly changing frequency evolution appropriate for young neutron stars. No detection is claimed. The peak root-sum-squared strain upper limit at 50\\% detection probability ($h_{\\text{rss}}^{50\\%})$ of both searches occurs at 1700~Hz and is estimated at $1.64 \\times 10^{-22}~\\text{Hz}^{-1/2}$ for GW170817, and $1.0 \\times 10^{-22}~\\text{Hz}^{-1/2}$ for GW190425. This is the first gravitational wave search for a neutron star remnant of GW190425.","sentences":["We present the results of two searches for gravitational waves from the post-merger remnants of the binary neutron star coalescence events GW170817 and GW190425.","The searches are fully coherent over 1800~s of data from the 2nd (for GW170817) and 3rd (for GW190425) observing runs of the LIGO and Virgo observatories.","The searches compute the matched filter $\\mathcal{F}$-statistic, and use a piecewise model of the rapidly changing frequency evolution appropriate for young neutron stars.","No detection is claimed.","The peak root-sum-squared strain upper limit at 50\\% detection probability ($h_{\\text{rss}}^{50\\%})$ of both searches occurs at 1700~Hz and is estimated at $1.64 \\times 10^{-22}~\\text{Hz}^{-1/2}$ for GW170817, and $1.0 \\times 10^{-22}~\\text{Hz}^{-1/2}$ for GW190425.","This is the first gravitational wave search for a neutron star remnant of GW190425."],"url":"http://arxiv.org/abs/2403.11392v1","category":"gr-qc"}
{"created":"2024-03-18 00:47:28","title":"Resolving the Mechanical and Radiative Feedback in J1044+0353 with KCWI Spectral Mapping","abstract":"We present integral field spectroscopy toward and around J1044+0353, a rapidly growing, low-metallicity galaxy which produces extreme [O III] line emission. A new map of the O32 flux ratio reveals a density-bounded ionization cone emerging from the starburst. The interaction of the hydrogen ionizing radiation, produced by the very young starburst, with a cavity previously carved out by a galactic outflow, whose apex lies well outside the starburst region, determines the pathway for global Lyman continuum (LyC) escape. In the region within a few hundred parsecs of the young starburst, we demonstrate that superbubble breakthrough and blowout contribute distinct components to the [O III] line profile, broad and very-broad emission line wings, respectively. We draw attention to the large [O III] luminosity of the broad component and argue that this emission comes from photoionized, superbubble shells rather than a galactic wind as is often assumed. The spatially resolved H eII 4686 nebula appears to be photoionized by young star clusters. Stellar wind emission from these stars is likely the source of line wings detected on the He II line profile. This broader He II component indicates slow stellar winds, consistent with an increase in stellar rotation (and a decrease in effective escape speed) at the metallicity of J1044+0353. At least in J1044+0353, the recent star formation history plays a critical role in generating a global pathway for LyC escape, and the anisotropic escape would likely be missed by direct observations of the LyC.","sentences":["We present integral field spectroscopy toward and around J1044+0353, a rapidly growing, low-metallicity galaxy which produces extreme [O III] line emission.","A new map of the O32 flux ratio reveals a density-bounded ionization cone emerging from the starburst.","The interaction of the hydrogen ionizing radiation, produced by the very young starburst, with a cavity previously carved out by a galactic outflow, whose apex lies well outside the starburst region, determines the pathway for global Lyman continuum (LyC) escape.","In the region within a few hundred parsecs of the young starburst, we demonstrate that superbubble breakthrough and blowout contribute distinct components to the [O III] line profile, broad and very-broad emission line wings, respectively.","We draw attention to the large [O III] luminosity of the broad component and argue that this emission comes from photoionized, superbubble shells rather than a galactic wind as is often assumed.","The spatially resolved H eII 4686 nebula appears to be photoionized by young star clusters.","Stellar wind emission from these stars is likely the source of line wings detected on the He II line profile.","This broader He II component indicates slow stellar winds, consistent with an increase in stellar rotation (and a decrease in effective escape speed) at the metallicity of J1044+0353.","At least in J1044+0353, the recent star formation history plays a critical role in generating a global pathway for LyC escape, and the anisotropic escape would likely be missed by direct observations of the LyC."],"url":"http://arxiv.org/abs/2403.11390v1","category":"astro-ph.GA"}
{"created":"2024-03-18 00:30:09","title":"Traffic Weaver: semi-synthetic time-varying traffic generator based on averaged time series","abstract":"Traffic Weaver is a Python package developed to generate a semi-synthetic signal (time series) with finer granularity, based on averaged time series, in a manner that, upon averaging, closely matches the original signal provided. The key components utilized to recreate the signal encompass oversampling with a given strategy, stretching to match the integral of the original time series, smoothing, repeating, applying trend, and adding noise. The primary motivation behind Traffic Weaver is to furnish semi-synthetic time-varying traffic in telecommunication networks, facilitating the development and validation of traffic prediction models, as well as aiding in the deployment of network optimization algorithms tailored for time-varying traffic.","sentences":["Traffic Weaver is a Python package developed to generate a semi-synthetic signal (time series) with finer granularity, based on averaged time series, in a manner that, upon averaging, closely matches the original signal provided.","The key components utilized to recreate the signal encompass oversampling with a given strategy, stretching to match the integral of the original time series, smoothing, repeating, applying trend, and adding noise.","The primary motivation behind Traffic Weaver is to furnish semi-synthetic time-varying traffic in telecommunication networks, facilitating the development and validation of traffic prediction models, as well as aiding in the deployment of network optimization algorithms tailored for time-varying traffic."],"url":"http://arxiv.org/abs/2403.11388v1","category":"cs.NI"}
{"created":"2024-03-18 00:23:24","title":"Holistic HMI Design for Automated Vehicles: Bridging In-Vehicle and External Communication","abstract":"As the field of automated vehicles (AVs) advances, it has become increasingly critical to develop human-machine interfaces (HMI) for both internal and external communication. Critical dialogue is emerging around the potential necessity for a holistic approach to HMI designs, which promotes the integration of both in-vehicle user and external road user perspectives. This approach aims to create a unified and coherent experience for different stakeholders interacting with AVs. This workshop seeks to bring together designers, engineers, researchers, and other stakeholders to delve into relevant use cases, exploring the potential advantages and challenges of this approach. The insights generated from this workshop aim to inform further design and research in the development of coherent HMIs for AVs, ultimately for more seamless integration of AVs into existing traffic.","sentences":["As the field of automated vehicles (AVs) advances, it has become increasingly critical to develop human-machine interfaces (HMI) for both internal and external communication.","Critical dialogue is emerging around the potential necessity for a holistic approach to HMI designs, which promotes the integration of both in-vehicle user and external road user perspectives.","This approach aims to create a unified and coherent experience for different stakeholders interacting with AVs.","This workshop seeks to bring together designers, engineers, researchers, and other stakeholders to delve into relevant use cases, exploring the potential advantages and challenges of this approach.","The insights generated from this workshop aim to inform further design and research in the development of coherent HMIs for AVs, ultimately for more seamless integration of AVs into existing traffic."],"url":"http://arxiv.org/abs/2403.11386v1","category":"cs.HC"}
{"created":"2024-03-18 00:13:43","title":"Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot","abstract":"As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of metrics tied to the Melting Pot's \"Commons Harvest\" game. The paper closes, by discussing the limitations of the current architectural framework and the potential of a new set of modules that fosters better cooperation among LAAs.","sentences":["As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems.","This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5.","Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures.","The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of metrics tied to the Melting Pot's \"Commons Harvest\" game.","The paper closes, by discussing the limitations of the current architectural framework and the potential of a new set of modules that fosters better cooperation among LAAs."],"url":"http://arxiv.org/abs/2403.11381v1","category":"cs.AI"}
{"created":"2024-03-18 00:12:58","title":"The Role of Extended Horizon Methodology in Renewable-Dense Grids With Inter-Day Long-Duration Energy Storage","abstract":"This study addresses the challenges in optimizing long-duration energy storage (LDES) dispatch within future power systems featuring high integration of variable renewable energy (VRE). The research focuses on conducting a comparative analysis between traditional and extended horizon methods for the optimization of LDES dispatch, using open-source and commercial production cost models (PCMs), tested on a futuristic Electric Reliability Council of Texas (ERCOT) grid. The findings indicate that, despite its complexity and longer solution times, the extended horizon approach demonstrates superior performance in LDES dispatch and effectively reduces the impact of degenerate solutions in sequential simulations. This study underscores the trade-offs between computational efficiency and improvement in storage dispatch, which is crucial for future energy systems. The analysis highlights the necessity of addressing the degeneracy issue in storage dispatch in grids dominated by zero operating cost VRE generators and low operating cost energy storage devices. Additionally, the research reveals revenue discrepancies for LDES operators across different models, a consequence of the persistent presence of degeneracy in high VRE systems. These findings suggest an urgent need for refined modeling techniques in the planning and operation of future energy systems.","sentences":["This study addresses the challenges in optimizing long-duration energy storage (LDES) dispatch within future power systems featuring high integration of variable renewable energy (VRE).","The research focuses on conducting a comparative analysis between traditional and extended horizon methods for the optimization of LDES dispatch, using open-source and commercial production cost models (PCMs), tested on a futuristic Electric Reliability Council of Texas (ERCOT) grid.","The findings indicate that, despite its complexity and longer solution times, the extended horizon approach demonstrates superior performance in LDES dispatch and effectively reduces the impact of degenerate solutions in sequential simulations.","This study underscores the trade-offs between computational efficiency and improvement in storage dispatch, which is crucial for future energy systems.","The analysis highlights the necessity of addressing the degeneracy issue in storage dispatch in grids dominated by zero operating cost VRE generators and low operating cost energy storage devices.","Additionally, the research reveals revenue discrepancies for LDES operators across different models, a consequence of the persistent presence of degeneracy in high VRE systems.","These findings suggest an urgent need for refined modeling techniques in the planning and operation of future energy systems."],"url":"http://arxiv.org/abs/2403.11379v1","category":"eess.SY"}
{"created":"2024-03-17 23:44:20","title":"Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration","abstract":"Pre-trained large multi-modal models (LMMs) exploit fine-tuning to adapt diverse user applications. Nevertheless, fine-tuning may face challenges due to deactivated sensors (e.g., cameras turned off for privacy or technical issues), yielding modality-incomplete data and leading to inconsistency in training data and the data for inference. Additionally, continuous training leads to catastrophic forgetting, diluting the knowledge in pre-trained LMMs. To overcome these challenges, we introduce a novel task, Continual Missing Modality Learning (CMML), to investigate how models can generalize when data of certain modalities is missing during continual fine-tuning. Our preliminary benchmarks reveal that existing methods suffer from a significant performance drop in CMML, even with the aid of advanced continual learning techniques. Therefore, we devise a framework termed Reconstruct before Query (RebQ). It decomposes prompts into modality-specific ones and breaks them into components stored in pools accessible via a key-query mechanism, which facilitates ParameterEfficient Fine-Tuning and enhances knowledge transferability for subsequent tasks. Meanwhile, our RebQ leverages extensive multi-modal knowledge from pre-trained LMMs to reconstruct the data of missing modality. Comprehensive experiments demonstrate that RebQ effectively reconstructs the missing modality information and retains pre-trained knowledge. Specifically, compared with the baseline, RebQ improves average precision from 20.00 to 50.92 and decreases average forgetting from 75.95 to 8.56. Code and datasets are available on https://github.com/Tree-Shu-Zhao/RebQ.pytorch","sentences":["Pre-trained large multi-modal models (LMMs) exploit fine-tuning to adapt diverse user applications.","Nevertheless, fine-tuning may face challenges due to deactivated sensors (e.g., cameras turned off for privacy or technical issues), yielding modality-incomplete data and leading to inconsistency in training data and the data for inference.","Additionally, continuous training leads to catastrophic forgetting, diluting the knowledge in pre-trained LMMs.","To overcome these challenges, we introduce a novel task, Continual Missing Modality Learning (CMML), to investigate how models can generalize when data of certain modalities is missing during continual fine-tuning.","Our preliminary benchmarks reveal that existing methods suffer from a significant performance drop in CMML, even with the aid of advanced continual learning techniques.","Therefore, we devise a framework termed Reconstruct before Query (RebQ).","It decomposes prompts into modality-specific ones and breaks them into components stored in pools accessible via a key-query mechanism, which facilitates ParameterEfficient Fine-Tuning and enhances knowledge transferability for subsequent tasks.","Meanwhile, our RebQ leverages extensive multi-modal knowledge from pre-trained LMMs to reconstruct the data of missing modality.","Comprehensive experiments demonstrate that RebQ effectively reconstructs the missing modality information and retains pre-trained knowledge.","Specifically, compared with the baseline, RebQ improves average precision from 20.00 to 50.92 and decreases average forgetting from 75.95 to 8.56.","Code and datasets are available on https://github.com/Tree-Shu-Zhao/RebQ.pytorch"],"url":"http://arxiv.org/abs/2403.11373v1","category":"cs.CV"}
{"created":"2024-03-17 23:38:18","title":"Approximations of the quasi-local Bartnik mass in general relativity","abstract":"In this study, we employ eth-operators and spin-weighted spherical harmonics to express the ADM mass of a static space-time based on the mean values of its components over a a radius-$r$ sphere. While initially derived for standard spherical coordinates, we showcase its adaptability by demonstrating its usefulness in expressing a quasilocal mass; specifically, the Bartnik mass, of an almost round 2D-hypersurface in terms of some specific boundary conditions. Additionally, we utilize this formulation to propose a deep learning methodology for numerically constructing static metrics that incorporate 2D-hypersurfaces with specified Bartnik mass.","sentences":["In this study, we employ eth-operators and spin-weighted spherical harmonics to express the ADM mass of a static space-time based on the mean values of its components over a a radius-$r$ sphere.","While initially derived for standard spherical coordinates, we showcase its adaptability by demonstrating its usefulness in expressing a quasilocal mass; specifically, the Bartnik mass, of an almost round 2D-hypersurface in terms of some specific boundary conditions.","Additionally, we utilize this formulation to propose a deep learning methodology for numerically constructing static metrics that incorporate 2D-hypersurfaces with specified Bartnik mass."],"url":"http://arxiv.org/abs/2403.11372v1","category":"gr-qc"}
{"created":"2024-03-17 23:29:41","title":"V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions","abstract":"Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception systems have shown the significant success on 3D object detection. While these models perform well in the trained clean weather, they struggle in unseen adverse weather conditions with the real-world domain gap. In this paper, we propose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D object detection on multi-agent perception system under adverse weather conditions. Not only in the clean weather does our research aim to ensure favorable multi-agent performance, but also in the unseen adverse weather conditions by learning only on the clean weather data. To advance research in this area, we have simulated the impact of three prevalent adverse weather conditions on two widely-used multi-agent datasets, resulting in the creation of two novel benchmark datasets: OPV2V-w and V2XSet-w.   To this end, we first introduce the Adaptive Weather Augmentation (AWA) to mimic the unseen adverse weather conditions, and then propose two alignments for generalizable representation learning: Trust-region Weather-invariant Alignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensive experimental results demonstrate that our V2X-DGW achieved improvements in the unseen adverse weather conditions.","sentences":["Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception systems have shown the significant success on 3D object detection.","While these models perform well in the trained clean weather, they struggle in unseen adverse weather conditions with the real-world domain gap.","In this paper, we propose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D object detection on multi-agent perception system under adverse weather conditions.","Not only in the clean weather does our research aim to ensure favorable multi-agent performance, but also in the unseen adverse weather conditions by learning only on the clean weather data.","To advance research in this area, we have simulated the impact of three prevalent adverse weather conditions on two widely-used multi-agent datasets, resulting in the creation of two novel benchmark datasets: OPV2V-w and V2XSet-w.   ","To this end, we first introduce the Adaptive Weather Augmentation (AWA) to mimic the unseen adverse weather conditions, and then propose two alignments for generalizable representation learning: Trust-region Weather-invariant Alignment (TWA) and Agent-aware Contrastive Alignment (ACA).","Extensive experimental results demonstrate that our V2X-DGW achieved improvements in the unseen adverse weather conditions."],"url":"http://arxiv.org/abs/2403.11371v1","category":"cs.CV"}
{"created":"2024-03-17 23:07:13","title":"Driving Style Alignment for LLM-powered Driver Agent","abstract":"Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities.However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The framework's effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into designing driving agents with diverse driving styles.The implementation of the framework and details of the dataset can be found at the link.","sentences":["Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities.","However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.","To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback.","Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment.","The framework's effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations.","Our research offers valuable insights into designing driving agents with diverse driving styles.","The implementation of the framework and details of the dataset can be found at the link."],"url":"http://arxiv.org/abs/2403.11368v1","category":"cs.RO"}
{"created":"2024-03-17 23:02:04","title":"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning","abstract":"The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 12x improvement in runtime compared to Hugging Face/DeepSpeed implementation with four GPUs while consuming less than half the VRAM per GPU. Our library will be open-sourced in due course.","sentences":["The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences.","Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context.","Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training.","Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements.","This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources.","Our experiments show more than 12x improvement in runtime compared to Hugging Face/DeepSpeed implementation with four GPUs while consuming less than half the VRAM per GPU.","Our library will be open-sourced in due course."],"url":"http://arxiv.org/abs/2403.11366v1","category":"cs.LG"}
{"created":"2024-03-17 22:44:36","title":"IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight","abstract":"Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training. This ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse serves as an exploratory tool for information systems researchers to unveil important non-linear relationships in domains that are characterized by complex patterns. Our ongoing research is directed at a thorough evaluation of the IGANN Sparse model, including user studies that allow to assess how well users of the model can benefit from the reduced number of features. This will allow for a deeper understanding of the interactions between linear vs. non-linear modeling, number of selected features, and predictive performance.","sentences":["Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models.","Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data.","Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection.","However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets.","In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training.","This ensures interpretability through improved model sparsity without sacrificing predictive performance.","Moreover, IGANN Sparse serves as an exploratory tool for information systems researchers to unveil important non-linear relationships in domains that are characterized by complex patterns.","Our ongoing research is directed at a thorough evaluation of the IGANN Sparse model, including user studies that allow to assess how well users of the model can benefit from the reduced number of features.","This will allow for a deeper understanding of the interactions between linear vs. non-linear modeling, number of selected features, and predictive performance."],"url":"http://arxiv.org/abs/2403.11363v1","category":"cs.LG"}
{"created":"2024-03-17 22:38:08","title":"Graph Theory for Consent Management: A New Approach for Complex Data Flows","abstract":"Through legislation and technical advances users gain more control over how their data is processed, and they expect online services to respect their privacy choices and preferences. However, data may be processed for many different purposes by several layers of algorithms that create complex data workflows. To date, there is no existing approach to automatically satisfy fine-grained privacy constraints of a user in a way which optimises the service provider's gains from processing. In this article, we propose a solution to this problem by modelling a data flow as a graph. User constraints and processing purposes are pairs of vertices which need to be disconnected in this graph. In general, this problem is NP-hard, thus, we propose several heuristics and algorithms. We discuss the optimality versus efficiency of our algorithms and evaluate them using synthetically generated data. On the practical side, our algorithms can provide nearly optimal solutions for tens of constraints and graphs of thousands of nodes, in a few seconds.","sentences":["Through legislation and technical advances users gain more control over how their data is processed, and they expect online services to respect their privacy choices and preferences.","However, data may be processed for many different purposes by several layers of algorithms that create complex data workflows.","To date, there is no existing approach to automatically satisfy fine-grained privacy constraints of a user in a way which optimises the service provider's gains from processing.","In this article, we propose a solution to this problem by modelling a data flow as a graph.","User constraints and processing purposes are pairs of vertices which need to be disconnected in this graph.","In general, this problem is NP-hard, thus, we propose several heuristics and algorithms.","We discuss the optimality versus efficiency of our algorithms and evaluate them using synthetically generated data.","On the practical side, our algorithms can provide nearly optimal solutions for tens of constraints and graphs of thousands of nodes, in a few seconds."],"url":"http://arxiv.org/abs/2403.11361v1","category":"cs.DB"}
{"created":"2024-03-17 22:15:33","title":"Decidability of the isomorphism problem between multidimensional substitutive subshifts","abstract":"An important question in dynamical systems is the classification problem, i.e., the ability to distinguish between two isomorphic systems. In this work, we study the topological factors between a family of multidimensional substitutive subshifts generated by morphisms with uniform support. We prove that it is decidable to check whether two minimal aperiodic substitutive subshifts are isomorphic, under having the same combinatorial structure. The strategy followed in this work consists of giving a complete description of the factor maps between these subshifts. Then, we deduce some interesting consequences on coalescence, automorphism groups, and the number of aperiodic symbolic factors of substitutive subshifts. We also prove other combinatorial results on these substitutions, such as the decidability of defining a subshift, the computability of the recognizability radius, and the conjugacy between substitutions with different supports.","sentences":["An important question in dynamical systems is the classification problem, i.e., the ability to distinguish between two isomorphic systems.","In this work, we study the topological factors between a family of multidimensional substitutive subshifts generated by morphisms with uniform support.","We prove that it is decidable to check whether two minimal aperiodic substitutive subshifts are isomorphic, under having the same combinatorial structure.","The strategy followed in this work consists of giving a complete description of the factor maps between these subshifts.","Then, we deduce some interesting consequences on coalescence, automorphism groups, and the number of aperiodic symbolic factors of substitutive subshifts.","We also prove other combinatorial results on these substitutions, such as the decidability of defining a subshift, the computability of the recognizability radius, and the conjugacy between substitutions with different supports."],"url":"http://arxiv.org/abs/2403.11357v1","category":"math.DS"}
{"created":"2024-03-17 21:52:51","title":"Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies","abstract":"Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR shifts and assigning peaks in experimental spectra. Our model undergoes an initial pre-training phase employing a Multi-Task Training (MTT) approach, which simultaneously leverages annotated 1D NMR datasets of both $^{1}\\text{H}$ and $^{13}\\text{C}$ spectra to enhance its understanding of NMR spectra. Subsequently, the pre-trained model is utilized to generate pseudo-annotations for unlabelled 2D NMR spectra, which are subsequently used to refine the 2D NMR prediction model. Our approach iterates between annotated unlabelled 2D NMR data and refining our 2D NMR prediction model until convergence. Finally, our model is able to not only accurately predict 2D NMR but also annotate peaks in experimental 2D NMR spectra. Experimental results show that our model is capable of accurately handling medium-sized and large molecules, including polysaccharides, underscoring its effectiveness.","sentences":["Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules.","Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra.","This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation.","Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures.","Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets.","To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR shifts and assigning peaks in experimental spectra.","Our model undergoes an initial pre-training phase employing a Multi-Task Training (MTT) approach, which simultaneously leverages annotated 1D NMR datasets of both $^{1}\\text{H}$ and $^{13}\\text{C}$ spectra to enhance its understanding of NMR spectra.","Subsequently, the pre-trained model is utilized to generate pseudo-annotations for unlabelled 2D NMR spectra, which are subsequently used to refine the 2D NMR prediction model.","Our approach iterates between annotated unlabelled 2D NMR data and refining our 2D NMR prediction model until convergence.","Finally, our model is able to not only accurately predict 2D NMR but also annotate peaks in experimental 2D NMR spectra.","Experimental results show that our model is capable of accurately handling medium-sized and large molecules, including polysaccharides, underscoring its effectiveness."],"url":"http://arxiv.org/abs/2403.11353v1","category":"cs.LG"}
{"created":"2024-03-17 21:43:19","title":"An SDP-based Branch-and-Cut Algorithm for Biclustering","abstract":"Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved at each node. Computational results on both synthetic and real-world instances show that the proposed algorithm can solve instances approximately 20 times larger than those handled by general-purpose solvers.","sentences":["Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns.","As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized.","To address this problem, we present a tailored branch-and-cut algorithm.","For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound.","We solve this relaxation in a cutting-plane fashion using a first-order method.","For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved at each node.","Computational results on both synthetic and real-world instances show that the proposed algorithm can solve instances approximately 20 times larger than those handled by general-purpose solvers."],"url":"http://arxiv.org/abs/2403.11351v1","category":"math.OC"}
{"created":"2024-03-17 21:27:06","title":"Discrete Painlev\u00e9 equations and pencils of quadrics in $\\mathbb P^3$","abstract":"Discrete Painlev\\'e equations constitute a famous class of integrable non-autonomous second order difference equations. A classification scheme proposed by Sakai interprets a discrete Painlev\\'e equation as a birational map between generalized Halphen surfaces (surfaces obtained from $\\mathbb P^1\\times\\mathbb P^1$ by blowing up at eight points). Sakai's classification is thus based on the classification of generalized Halphen surfaces. We propose a novel geometric interpretation of discrete Painlev\\'e equations, where the family of generalized Halphen surfaces is replaced by a pencil of quadrics in $\\mathbb P^3$. A discrete Painlev\\'e equation is viewed as a birational transformation of $\\mathbb P^3$ that preserves the pencil and maps each quadric of the pencil to a different one, according to a M\\\"obius transformation of the pencil parameter. Thus, our scheme is based on the classification of pencils of quadrics in $\\mathbb P^3$.   While historically discrete Painlev\\'e equations appeared as de-autonomizations of QRT maps, in our scheme they are viewed as deformations of 3D QRT maps introduced in our previous paper. A 3D QRT map is defined geometrically as a composition of involutions along generators of quadrics of a pencil, preserving the intersection curves with a second pencil of quadrics. The base set of the net of quadrics spanned by both pencils consists of eight points (which play the role of the eight blow-up points of generalized Halphen surfaces). A Painlev\\'e deformation of a 3D QRT map is obtained by composing involutions along generators with a birational (often linear) transformation of $\\mathbb P^3$ under which the pencil remains invariant, but the individual quadrics are mapped according to a M\\\"obius transformation of the pencil parameter.","sentences":["Discrete Painlev\\'e equations constitute a famous class of integrable non-autonomous second order difference equations.","A classification scheme proposed by Sakai interprets a discrete Painlev\\'e equation as a birational map between generalized Halphen surfaces (surfaces obtained from $\\mathbb P^1\\times\\mathbb P^1$ by blowing up at eight points).","Sakai's classification is thus based on the classification of generalized Halphen surfaces.","We propose a novel geometric interpretation of discrete Painlev\\'e equations, where the family of generalized Halphen surfaces is replaced by a pencil of quadrics in $\\mathbb P^3$.","A discrete Painlev\\'e equation is viewed as a birational transformation of $\\mathbb P^3$ that preserves the pencil and maps each quadric of the pencil to a different one, according to a M\\\"obius transformation of the pencil parameter.","Thus, our scheme is based on the classification of pencils of quadrics in $\\mathbb P^3$.   While historically discrete Painlev\\'e equations appeared as de-autonomizations of QRT maps, in our scheme they are viewed as deformations of 3D QRT maps introduced in our previous paper.","A 3D QRT map is defined geometrically as a composition of involutions along generators of quadrics of a pencil, preserving the intersection curves with a second pencil of quadrics.","The base set of the net of quadrics spanned by both pencils consists of eight points (which play the role of the eight blow-up points of generalized Halphen surfaces).","A Painlev\\'e deformation of a 3D QRT map is obtained by composing involutions along generators with a birational (often linear) transformation of $\\mathbb P^3$ under which the pencil remains invariant, but the individual quadrics are mapped according to a M\\\"obius transformation of the pencil parameter."],"url":"http://arxiv.org/abs/2403.11349v1","category":"nlin.SI"}
{"created":"2024-03-17 21:23:45","title":"COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits","abstract":"Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of prediction coverage for COLEP in the presence of bounded adversarial perturbations. We also provide certified coverage considering the finite size of the calibration set. Furthermore, we prove that COLEP achieves higher prediction coverage and accuracy over a single model as long as the utilities of knowledge models are non-trivial. Empirically, we show the validity and tightness of our certified coverage, demonstrating the robust conformal prediction of COLEP on various datasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to 12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on AwA2.","sentences":["Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable.","However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage.","In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning.","To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component.","Theoretically, we provide end-to-end certification of prediction coverage for COLEP in the presence of bounded adversarial perturbations.","We also provide certified coverage considering the finite size of the calibration set.","Furthermore, we prove that COLEP achieves higher prediction coverage and accuracy over a single model as long as the utilities of knowledge models are non-trivial.","Empirically, we show the validity and tightness of our certified coverage, demonstrating the robust conformal prediction of COLEP on various datasets, including GTSRB, CIFAR10, and AwA2.","We show that COLEP achieves up to 12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on AwA2."],"url":"http://arxiv.org/abs/2403.11348v1","category":"cs.LG"}
{"created":"2024-03-17 21:22:06","title":"Phonon predictions with E(3)-equivariant graph neural networks","abstract":"We present an equivariant neural network for predicting vibrational and phonon modes of molecules and periodic crystals, respectively. These predictions are made by evaluating the second derivative Hessian matrices of the learned energy model that is trained with the energy and force data. Using this method, we are able to efficiently predict phonon dispersion and the density of states for inorganic crystal materials. For molecules, we also derive the symmetry constraints for IR/Raman active modes by analyzing the phonon mode irreducible representations. Additionally, we demonstrate that using Hessian as a new type of higher-order training data improves energy models beyond models that only use lower-order energy and force data. With this second derivative approach, one can directly relate the energy models to the experimental observations for the vibrational properties. This approach further connects to a broader class of physical observables with a generalized energy model that includes external fields.","sentences":["We present an equivariant neural network for predicting vibrational and phonon modes of molecules and periodic crystals, respectively.","These predictions are made by evaluating the second derivative Hessian matrices of the learned energy model that is trained with the energy and force data.","Using this method, we are able to efficiently predict phonon dispersion and the density of states for inorganic crystal materials.","For molecules, we also derive the symmetry constraints for IR/Raman active modes by analyzing the phonon mode irreducible representations.","Additionally, we demonstrate that using Hessian as a new type of higher-order training data improves energy models beyond models that only use lower-order energy and force data.","With this second derivative approach, one can directly relate the energy models to the experimental observations for the vibrational properties.","This approach further connects to a broader class of physical observables with a generalized energy model that includes external fields."],"url":"http://arxiv.org/abs/2403.11347v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-17 21:16:17","title":"CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data","abstract":"Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\\textsc{ CantonMT} toolkit \\url{https://github.com/kenrickkung/CantoneseTranslation}.","sentences":["Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers.","In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English.","We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART.","We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based.","Furthermore.","we create a user-friendly interface for the models we included in this\\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research.","Researchers can add more models into this platform via our open-source\\textsc{ CantonMT} toolkit \\url{https://github.com/kenrickkung/CantoneseTranslation}."],"url":"http://arxiv.org/abs/2403.11346v1","category":"cs.CL"}
{"created":"2024-03-17 21:11:55","title":"Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective","abstract":"We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team minimizes its cumulative cost independently in a receding-horizon manner. Despite the non-convexity of the problem, we establish that the resulting algorithm converges to a global NE through a novel problem decomposition into sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs (HJI) equations, in which independent natural policy gradient is shown to exhibit linear convergence under time-independent diagonal dominance. Experiments illuminate the merits of this approach in practice.","sentences":["We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams.","To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure.","Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting.","This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs).","We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition.","This MFTG NE is then shown to be $\\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team.","These structural results motivate an algorithm called Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team minimizes its cumulative cost independently in a receding-horizon manner.","Despite the non-convexity of the problem, we establish that the resulting algorithm converges to a global NE through a novel problem decomposition into sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs (HJI) equations, in which independent natural policy gradient is shown to exhibit linear convergence under time-independent diagonal dominance.","Experiments illuminate the merits of this approach in practice."],"url":"http://arxiv.org/abs/2403.11345v1","category":"cs.LG"}
{"created":"2024-03-17 20:47:52","title":"StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining","abstract":"Hematoxylin and Eosin (H&E) staining is the most commonly used for disease diagnosis and tumor recurrence tracking. Hematoxylin excels at highlighting nuclei, whereas eosin stains the cytoplasm. However, H&E stain lacks details for differentiating different types of cells relevant to identifying the grade of the disease or response to specific treatment variations. Pathologists require special immunohistochemical (IHC) stains that highlight different cell types. These stains help in accurately identifying different regions of disease growth and their interactions with the cell's microenvironment. The advent of deep learning models has made Image-to-Image (I2I) translation a key research area, reducing the need for expensive physical staining processes. Pix2Pix and CycleGAN are still the most commonly used methods for virtual staining applications. However, both suffer from hallucinations or staining irregularities when H&E stain has less discriminate information about the underlying cells IHC needs to highlight (e.g.,CD3 lymphocytes). Diffusion models are currently the state-of-the-art models for image generation and conditional generation tasks. However, they require extensive and diverse datasets (millions of samples) to converge, which is less feasible for virtual staining applications.Inspired by the success of multitask deep learning models for limited dataset size, we propose StainDiffuser, a novel multitask dual diffusion architecture for virtual staining that converges under a limited training budget. StainDiffuser trains two diffusion processes simultaneously: (a) generation of cell-specific IHC stain from H&E and (b) H&E-based cell segmentation using coarse segmentation only during training. Our results show that StainDiffuser produces high-quality results for easier (CK8/18,epithelial marker) and difficult stains(CD3, Lymphocytes).","sentences":["Hematoxylin and Eosin (H&E) staining is the most commonly used for disease diagnosis and tumor recurrence tracking.","Hematoxylin excels at highlighting nuclei, whereas eosin stains the cytoplasm.","However, H&E stain lacks details for differentiating different types of cells relevant to identifying the grade of the disease or response to specific treatment variations.","Pathologists require special immunohistochemical (IHC) stains that highlight different cell types.","These stains help in accurately identifying different regions of disease growth and their interactions with the cell's microenvironment.","The advent of deep learning models has made Image-to-Image (I2I) translation a key research area, reducing the need for expensive physical staining processes.","Pix2Pix and CycleGAN are still the most commonly used methods for virtual staining applications.","However, both suffer from hallucinations or staining irregularities when H&E stain has less discriminate information about the underlying cells IHC needs to highlight (e.g.,CD3 lymphocytes).","Diffusion models are currently the state-of-the-art models for image generation and conditional generation tasks.","However, they require extensive and diverse datasets (millions of samples) to converge, which is less feasible for virtual staining applications.","Inspired by the success of multitask deep learning models for limited dataset size, we propose StainDiffuser, a novel multitask dual diffusion architecture for virtual staining that converges under a limited training budget.","StainDiffuser trains two diffusion processes simultaneously: (a) generation of cell-specific IHC stain from H&E and (b) H&E-based cell segmentation using coarse segmentation only during training.","Our results show that StainDiffuser produces high-quality results for easier (CK8/18,epithelial marker) and difficult stains(CD3, Lymphocytes)."],"url":"http://arxiv.org/abs/2403.11340v1","category":"eess.IV"}
{"created":"2024-03-17 20:44:38","title":"Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans","abstract":"Since the emergence of Covid-19 in late 2019, medical image analysis using artificial intelligence (AI) has emerged as a crucial research area, particularly with the utility of CT-scan imaging for disease diagnosis. This paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection and Covid-19 Domain Adaptation Challenges. Our approach centers on lung segmentation and Covid-19 infection segmentation employing the recent CNN-based segmentation architecture PDAtt-Unet, which simultaneously segments lung regions and infections. Departing from traditional methods, we concatenate the input slice (grayscale) with segmented lung and infection, generating three input channels akin to color channels. Additionally, we employ three 3D CNN backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and 3D-Resnet-50 models to train Covid-19 recognition for both challenges. Furthermore, we explore ensemble approaches and testing augmentation to enhance performance. Comparison with baseline results underscores the substantial efficiency of our approach, with a significant margin in terms of F1-score (14 %). This study advances the field by presenting a comprehensive methodology for accurate Covid-19 detection and adaptation, leveraging cutting-edge AI techniques in medical image analysis.","sentences":["Since the emergence of Covid-19 in late 2019, medical image analysis using artificial intelligence (AI) has emerged as a crucial research area, particularly with the utility of CT-scan imaging for disease diagnosis.","This paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection and Covid-19 Domain Adaptation Challenges.","Our approach centers on lung segmentation and Covid-19 infection segmentation employing the recent CNN-based segmentation architecture PDAtt-Unet, which simultaneously segments lung regions and infections.","Departing from traditional methods, we concatenate the input slice (grayscale) with segmented lung and infection, generating three input channels akin to color channels.","Additionally, we employ three 3D CNN backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and 3D-Resnet-50 models to train Covid-19 recognition for both challenges.","Furthermore, we explore ensemble approaches and testing augmentation to enhance performance.","Comparison with baseline results underscores the substantial efficiency of our approach, with a significant margin in terms of F1-score (14 %).","This study advances the field by presenting a comprehensive methodology for accurate Covid-19 detection and adaptation, leveraging cutting-edge AI techniques in medical image analysis."],"url":"http://arxiv.org/abs/2403.11338v1","category":"eess.IV"}
{"created":"2024-03-17 20:36:43","title":"Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction","abstract":"We propose a deep learning based novel prediction framework for enhanced bandwidth reduction in motion transfer enabled video applications such as video conferencing, virtual reality gaming and privacy preservation for patient health monitoring. To model complex motion, we use the First Order Motion Model (FOMM) that represents dynamic objects using learned keypoints along with their local affine transformations. Keypoints are extracted by a self-supervised keypoint detector and organized in a time series corresponding to the video frames. Prediction of keypoints, to enable transmission using lower frames per second on the source device, is performed using a Variational Recurrent Neural Network (VRNN). The predicted keypoints are then synthesized to video frames using an optical flow estimator and a generator network. This efficacy of leveraging keypoint based representations in conjunction with VRNN based prediction for both video animation and reconstruction is demonstrated on three diverse datasets. For real-time applications, our results show the effectiveness of our proposed architecture by enabling up to 2x additional bandwidth reduction over existing keypoint based video motion transfer frameworks without significantly compromising video quality.","sentences":["We propose a deep learning based novel prediction framework for enhanced bandwidth reduction in motion transfer enabled video applications such as video conferencing, virtual reality gaming and privacy preservation for patient health monitoring.","To model complex motion, we use the First Order Motion Model (FOMM) that represents dynamic objects using learned keypoints along with their local affine transformations.","Keypoints are extracted by a self-supervised keypoint detector and organized in a time series corresponding to the video frames.","Prediction of keypoints, to enable transmission using lower frames per second on the source device, is performed using a Variational Recurrent Neural Network (VRNN).","The predicted keypoints are then synthesized to video frames using an optical flow estimator and a generator network.","This efficacy of leveraging keypoint based representations in conjunction with VRNN based prediction for both video animation and reconstruction is demonstrated on three diverse datasets.","For real-time applications, our results show the effectiveness of our proposed architecture by enabling up to 2x additional bandwidth reduction over existing keypoint based video motion transfer frameworks without significantly compromising video quality."],"url":"http://arxiv.org/abs/2403.11337v1","category":"cs.CV"}
{"created":"2024-03-17 20:34:40","title":"ConvSDG: Session Data Generation for Conversational Search","abstract":"Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines.","sentences":["Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine.","However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning.","Thus, generating more training conversational sessions with relevant labels could potentially improve search performance.","Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation.","Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments.","The generated data are used to fine-tune the conversational dense retriever.","Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines."],"url":"http://arxiv.org/abs/2403.11335v1","category":"cs.IR"}
{"created":"2024-03-17 20:29:01","title":"Bridging the Gap between Discrete Agent Strategies in Game Theory and Continuous Motion Planning in Dynamic Environments","abstract":"Generating competitive strategies and performing continuous motion planning simultaneously in an adversarial setting is a challenging problem. In addition, understanding the intent of other agents is crucial to deploying autonomous systems in adversarial multi-agent environments. Existing approaches either discretize agent action by grouping similar control inputs, sacrificing performance in motion planning, or plan in uninterpretable latent spaces, producing hard-to-understand agent behaviors. This paper proposes an agent strategy representation via Policy Characteristic Space that maps the agent policies to a pre-specified low-dimensional space. Policy Characteristic Space enables the discretization of agent policy switchings while preserving continuity in control. Also, it provides intepretability of agent policies and clear intentions of policy switchings. Then, regret-based game-theoretic approaches can be applied in the Policy Characteristic Space to obtain high performance in adversarial environments. Our proposed method is assessed by conducting experiments in an autonomous racing scenario using scaled vehicles. Statistical evidence shows that our method significantly improves the win rate of ego agent and the method also generalizes well to unseen environments.","sentences":["Generating competitive strategies and performing continuous motion planning simultaneously in an adversarial setting is a challenging problem.","In addition, understanding the intent of other agents is crucial to deploying autonomous systems in adversarial multi-agent environments.","Existing approaches either discretize agent action by grouping similar control inputs, sacrificing performance in motion planning, or plan in uninterpretable latent spaces, producing hard-to-understand agent behaviors.","This paper proposes an agent strategy representation via Policy Characteristic Space that maps the agent policies to a pre-specified low-dimensional space.","Policy Characteristic Space enables the discretization of agent policy switchings while preserving continuity in control.","Also, it provides intepretability of agent policies and clear intentions of policy switchings.","Then, regret-based game-theoretic approaches can be applied in the Policy Characteristic Space to obtain high performance in adversarial environments.","Our proposed method is assessed by conducting experiments in an autonomous racing scenario using scaled vehicles.","Statistical evidence shows that our method significantly improves the win rate of ego agent and the method also generalizes well to unseen environments."],"url":"http://arxiv.org/abs/2403.11334v1","category":"cs.RO"}
{"created":"2024-03-17 20:25:38","title":"Identification of Information Structures in Bayesian Games","abstract":"To what extent can an external observer observing an equilibrium action distribution in an incomplete information game infer the underlying information structure? We investigate this issue in a general linear-quadratic-Gaussian framework. A simple class of canonical information structures is offered and proves rich enough to rationalize any possible equilibrium action distribution that can arise under an arbitrary information structure. We show that the class is parsimonious in the sense that the relevant parameters can be uniquely pinned down by an observed equilibrium outcome, up to some qualifications. Our result implies, for example, that the accuracy of each agent's signal about the state is identified, as measured by how much observing the signal reduces the state variance. Moreover, we show that a canonical information structure characterizes the lower bound on the amount by which each agent's signal can reduce the state variance, across all observationally equivalent information structures. The lower bound is tight, for example, when the actual information structure is uni-dimensional, or when there are no strategic interactions among agents, but in general, there is a gap since agents' strategic motives confound their private information about fundamental and strategic uncertainty.","sentences":["To what extent can an external observer observing an equilibrium action distribution in an incomplete information game infer the underlying information structure?","We investigate this issue in a general linear-quadratic-Gaussian framework.","A simple class of canonical information structures is offered and proves rich enough to rationalize any possible equilibrium action distribution that can arise under an arbitrary information structure.","We show that the class is parsimonious in the sense that the relevant parameters can be uniquely pinned down by an observed equilibrium outcome, up to some qualifications.","Our result implies, for example, that the accuracy of each agent's signal about the state is identified, as measured by how much observing the signal reduces the state variance.","Moreover, we show that a canonical information structure characterizes the lower bound on the amount by which each agent's signal can reduce the state variance, across all observationally equivalent information structures.","The lower bound is tight, for example, when the actual information structure is uni-dimensional, or when there are no strategic interactions among agents, but in general, there is a gap since agents' strategic motives confound their private information about fundamental and strategic uncertainty."],"url":"http://arxiv.org/abs/2403.11333v1","category":"econ.TH"}
{"created":"2024-03-17 20:23:06","title":"Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability","abstract":"Due to the heterogeneity of the global distribution of ecological and hydrological ground-truth observations, machine learning models can have limited adaptability when applied to unknown locations, which is referred to as weak extrapolability. Domain adaptation techniques have been widely used in machine learning domains such as image classification, which can improve the model generalization ability by adjusting the difference or inconsistency of the domain distribution between the training and test sets. However, this approach has rarely been used explicitly in machine learning models in ecology and hydrology at the global scale, although these models have often been questioned due to geographic extrapolability issues. This paper briefly describes the shortcomings of current machine learning models of ecology and hydrology in terms of the global representativeness of the distribution of observations and the resulting limitations of the lack of extrapolability and suggests that future related modelling efforts should consider the use of domain adaptation techniques to improve extrapolability.","sentences":["Due to the heterogeneity of the global distribution of ecological and hydrological ground-truth observations, machine learning models can have limited adaptability when applied to unknown locations, which is referred to as weak extrapolability.","Domain adaptation techniques have been widely used in machine learning domains such as image classification, which can improve the model generalization ability by adjusting the difference or inconsistency of the domain distribution between the training and test sets.","However, this approach has rarely been used explicitly in machine learning models in ecology and hydrology at the global scale, although these models have often been questioned due to geographic extrapolability issues.","This paper briefly describes the shortcomings of current machine learning models of ecology and hydrology in terms of the global representativeness of the distribution of observations and the resulting limitations of the lack of extrapolability and suggests that future related modelling efforts should consider the use of domain adaptation techniques to improve extrapolability."],"url":"http://arxiv.org/abs/2403.11331v1","category":"physics.geo-ph"}
{"created":"2024-03-17 20:21:26","title":"Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback","abstract":"We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.","sentences":["We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals.","At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step.","This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent.","We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods."],"url":"http://arxiv.org/abs/2403.11330v1","category":"cs.CL"}
{"created":"2024-03-17 20:14:57","title":"Domain-Guided Masked Autoencoders for Unique Player Identification","abstract":"Unique player identification is a fundamental module in vision-driven sports analytics. Identifying players from broadcast videos can aid with various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatic detection of jersey numbers using deep features is challenging primarily due to: a) motion blur, b) low resolution video feed, and c) occlusions. With their recent success in various vision tasks, masked autoencoders (MAEs) have emerged as a superior alternative to conventional feature extractors. However, most MAEs simply zero-out image patches either randomly or focus on where to mask rather than how to mask. Motivated by human vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to facilitate robust feature extraction in the presence of motion blur for player identification. We further introduce a new spatio-temporal network leveraging our novel d-MAE for unique player identification. We conduct experiments on three large-scale sports datasets, including a curated baseball dataset, the SoccerNet dataset, and an in-house ice hockey dataset. We preprocess the datasets using an upgraded keyframe identification (KfID) module by focusing on frames containing jersey numbers. Additionally, we propose a keyframe-fusion technique to augment keyframes, preserving spatial and temporal context. Our spatio-temporal network showcases significant improvements, surpassing the current state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies, respectively. Rigorous ablations highlight the effectiveness of our domain-guided masking approach and the refined KfID module, resulting in performance enhancements of 1.48% and 1.84% respectively, compared to original architectures.","sentences":["Unique player identification is a fundamental module in vision-driven sports analytics.","Identifying players from broadcast videos can aid with various downstream tasks such as player assessment, in-game analysis, and broadcast production.","However, automatic detection of jersey numbers using deep features is challenging primarily due to: a) motion blur, b) low resolution video feed, and c) occlusions.","With their recent success in various vision tasks, masked autoencoders (MAEs) have emerged as a superior alternative to conventional feature extractors.","However, most MAEs simply zero-out image patches either randomly or focus on where to mask rather than how to mask.","Motivated by human vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to facilitate robust feature extraction in the presence of motion blur for player identification.","We further introduce a new spatio-temporal network leveraging our novel d-MAE for unique player identification.","We conduct experiments on three large-scale sports datasets, including a curated baseball dataset, the SoccerNet dataset, and an in-house ice hockey dataset.","We preprocess the datasets using an upgraded keyframe identification (KfID) module by focusing on frames containing jersey numbers.","Additionally, we propose a keyframe-fusion technique to augment keyframes, preserving spatial and temporal context.","Our spatio-temporal network showcases significant improvements, surpassing the current state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies, respectively.","Rigorous ablations highlight the effectiveness of our domain-guided masking approach and the refined KfID module, resulting in performance enhancements of 1.48% and 1.84% respectively, compared to original architectures."],"url":"http://arxiv.org/abs/2403.11328v1","category":"cs.CV"}
{"created":"2024-03-17 20:09:14","title":"Borel Complexity of the Isomorphism Relation of Archimedean Orders in Finitely Generated Groups","abstract":"In 2020, Calderoni, Marker, Motto Ros and Shani asked what the Borel complexity of the isomorphism relation of Archimedean orders on $\\mathbb{Q}^n$ is. We answer this question by proving that the isomorphism relation of Archimedean orders on $\\mathbb{Z}^n$ is not hyperfinite when $n \\geq 3$ and not treeable when $n \\geq 4$. As a corollary, we get that the isomorphism relation of Archimedean orders on $\\mathbb{Q}^n$ is not hyperfinite when $n \\geq 3$ and not treeable when $n \\geq 4$.","sentences":["In 2020, Calderoni, Marker, Motto Ros and Shani asked what the Borel complexity of the isomorphism relation of Archimedean orders on $\\mathbb{Q}^n$ is.","We answer this question by proving that the isomorphism relation of Archimedean orders on $\\mathbb{Z}^n$ is not hyperfinite when $n \\geq 3$ and not treeable when $n \\geq 4$.","As a corollary, we get that the isomorphism relation of Archimedean orders on $\\mathbb{Q}^n$ is not hyperfinite when $n \\geq 3$ and not treeable when $n \\geq 4$."],"url":"http://arxiv.org/abs/2403.11326v1","category":"math.LO"}
{"created":"2024-03-17 20:06:41","title":"GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering","abstract":"During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces. This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data. To mitigate this issue, we propose a novel approach called GeoGaussian. Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy. Finally, the pipeline ensures that the scene's geometry and texture are maintained through constrained optimization processes with explicit geometry constraints. Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions. Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets.","sentences":["During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces.","This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data.","To mitigate this issue, we propose a novel approach called GeoGaussian.","Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy.","Finally, the pipeline ensures that the scene's geometry and texture are maintained through constrained optimization processes with explicit geometry constraints.","Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions.","Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets."],"url":"http://arxiv.org/abs/2403.11324v1","category":"cs.CV"}
{"created":"2024-03-17 19:58:35","title":"Diffusion and Multi-Domain Adaptation Methods for Eosinophil Segmentation","abstract":"Eosinophilic Esophagitis (EoE) represents a challenging condition for medical providers today. The cause is currently unknown, the impact on a patient's daily life is significant, and it is increasing in prevalence. Traditional approaches for medical image diagnosis such as standard deep learning algorithms are limited by the relatively small amount of data and difficulty in generalization. As a response, two methods have arisen that seem to perform well: Diffusion and Multi-Domain methods with current research efforts favoring diffusion methods. For the EoE dataset, we discovered that a Multi-Domain Adversarial Network outperformed a Diffusion based method with a FID of 42.56 compared to 50.65. Future work with diffusion methods should include a comparison with Multi-Domain adaptation methods to ensure that the best performance is achieved.","sentences":["Eosinophilic Esophagitis (EoE) represents a challenging condition for medical providers today.","The cause is currently unknown, the impact on a patient's daily life is significant, and it is increasing in prevalence.","Traditional approaches for medical image diagnosis such as standard deep learning algorithms are limited by the relatively small amount of data and difficulty in generalization.","As a response, two methods have arisen that seem to perform well: Diffusion and Multi-Domain methods with current research efforts favoring diffusion methods.","For the EoE dataset, we discovered that a Multi-Domain Adversarial Network outperformed a Diffusion based method with a FID of 42.56 compared to 50.65.","Future work with diffusion methods should include a comparison with Multi-Domain adaptation methods to ensure that the best performance is achieved."],"url":"http://arxiv.org/abs/2403.11323v1","category":"eess.IV"}
{"created":"2024-03-17 19:54:16","title":"StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows","abstract":"It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evaluations on the InterCode SQL and Bash benchmarks show that StateFlow significantly enhances LLMs' efficiency.","sentences":["It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments.","In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines.","With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process.","Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed.","State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model.","Evaluations on the InterCode SQL and Bash benchmarks show that StateFlow significantly enhances LLMs' efficiency."],"url":"http://arxiv.org/abs/2403.11322v1","category":"cs.CL"}
{"created":"2024-03-17 19:53:02","title":"A constant time complexity algorithm for the unbounded knapsack problem with bounded coefficients","abstract":"Benchmark instances for the unbounded knapsack problem are typically generated according to specific criteria within a given constant range $R$, and these instances can be referred to as the unbounded knapsack problem with bounded coefficients (UKPB). In order to increase the difficulty of solving these instances, the knapsack capacity $C$ is usually set to a very large value. Therefore, an exact algorithm that neither time complexity nor space complexity includes the capacity coefficient $C$ is highly anticipated.   In this paper, we propose an exact algorithm with time complexity of $O(R^4)$ and space complexity of $O(R^3)$. The algorithm initially divides the multiset $N$ into two multisubsets, $N_1$ and $N_2$, based on the profit density of their types. For the multisubset $N_2$ composed of types with profit density lower than the maximum profit density type, we utilize a recent branch and bound (B\\&B) result by Dey et al. (Math. Prog., pp 569-587, 2023) to determine the maximum selection number for types in $N_2$. We then employ the Unbounded-DP algorithm to exactly solve for the types in $N_2$. For the multisubset $N_1$ composed of the maximum profit density type and its counterparts with the same profit density, we transform it into a linear Diophantine equation and leverage relevant conclusions from the Frobenius problem to solve it efficiently. In particular, the proof techniques required by the algorithm are primarily covered in the first-year mathematics curriculum, which is convenient for subsequent researchers to grasp.","sentences":["Benchmark instances for the unbounded knapsack problem are typically generated according to specific criteria within a given constant range $R$, and these instances can be referred to as the unbounded knapsack problem with bounded coefficients (UKPB).","In order to increase the difficulty of solving these instances, the knapsack capacity $C$ is usually set to a very large value.","Therefore, an exact algorithm that neither time complexity nor space complexity includes the capacity coefficient $C$ is highly anticipated.   ","In this paper, we propose an exact algorithm with time complexity of $O(R^4)$ and space complexity of $O(R^3)$. The algorithm initially divides the multiset $N$ into two multisubsets, $N_1$ and $N_2$, based on the profit density of their types.","For the multisubset $N_2$ composed of types with profit density lower than the maximum profit density type, we utilize a recent branch and bound (B\\&B) result by Dey et al.","(Math.","Prog., pp 569-587, 2023) to determine the maximum selection number for types in $N_2$. We then employ the Unbounded-DP algorithm to exactly solve for the types in $N_2$. For the multisubset $N_1$ composed of the maximum profit density type and its counterparts with the same profit density, we transform it into a linear Diophantine equation and leverage relevant conclusions from the Frobenius problem to solve it efficiently.","In particular, the proof techniques required by the algorithm are primarily covered in the first-year mathematics curriculum, which is convenient for subsequent researchers to grasp."],"url":"http://arxiv.org/abs/2403.11320v1","category":"cs.DS"}
{"created":"2024-03-17 19:32:12","title":"Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts","abstract":"Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not. For SIP-BART, we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model. These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls.","sentences":["Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning.","However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning.","In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data.","To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem.","We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker.","We find that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.","For SIP-BART, we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.","These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls."],"url":"http://arxiv.org/abs/2403.11314v1","category":"cs.LG"}
{"created":"2024-03-17 19:23:46","title":"Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models","abstract":"Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties.","sentences":["Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization.","Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect.","The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes.","Predictive models often need to trade-off speed for prediction accuracy and generalization.","This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization.","Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions.","We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs.","Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning.","The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties."],"url":"http://arxiv.org/abs/2403.11313v1","category":"cs.RO"}
{"created":"2024-03-17 19:14:28","title":"Forging the Industrial Metaverse - Where Industry 5.0, Augmented and Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","abstract":"The Metaverse is a concept that proposes to immerse users into real-time rendered 3D content virtual worlds delivered through Extended Reality (XR) devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual Reality (VR) headsets. When the Metaverse concept is applied to industrial environments, it is called Industrial Metaverse, a hybrid world where industrial operators work by using some of the latest technologies. Currently, such technologies are related to the ones fostered by Industry 4.0, which is evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by creating a sustainable and resilient world of industrial human-centric applications. The Industrial Metaverse can benefit from Industry 5.0, since it implies making use of dynamic and up-to-date content, as well as fast human-to-machine interactions. To enable such enhancements, this article proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts with Industrial Metaverse applications and with his/her surroundings through advanced XR devices. This article provides a description of the technologies that support Meta-Operators: the main components of the Industrial Metaverse, the latest XR technologies and the use of Opportunistic Edge Computing communications (to interact with surrounding IoT/IioT devices). Moreover, this paper analyzes how to create the next generation of Industrial Metaverse applications based on Industry 5.0, including the integration of AR/MR devices with IoT/IIoT solutions, the development of advanced communications or the creation of shared experiences. Finally, this article provides a list of potential Industry 5.0 applications for the Industrial Metaverse and analyzes the main challenges and research lines. Thus, this article provides useful guidelines for the researchers that will create the next generation of applications for the Industrial Metaverse.","sentences":["The Metaverse is a concept that proposes to immerse users into real-time rendered 3D content virtual worlds delivered through Extended Reality (XR) devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual Reality (VR) headsets.","When the Metaverse concept is applied to industrial environments, it is called Industrial Metaverse, a hybrid world where industrial operators work by using some of the latest technologies.","Currently, such technologies are related to the ones fostered by Industry 4.0, which is evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by creating a sustainable and resilient world of industrial human-centric applications.","The Industrial Metaverse can benefit from Industry 5.0, since it implies making use of dynamic and up-to-date content, as well as fast human-to-machine interactions.","To enable such enhancements, this article proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts with Industrial Metaverse applications and with his/her surroundings through advanced XR devices.","This article provides a description of the technologies that support Meta-Operators: the main components of the Industrial Metaverse, the latest XR technologies and the use of Opportunistic Edge Computing communications (to interact with surrounding IoT/IioT devices).","Moreover, this paper analyzes how to create the next generation of Industrial Metaverse applications based on Industry 5.0, including the integration of AR/MR devices with IoT/IIoT solutions, the development of advanced communications or the creation of shared experiences.","Finally, this article provides a list of potential Industry 5.0 applications for the Industrial Metaverse and analyzes the main challenges and research lines.","Thus, this article provides useful guidelines for the researchers that will create the next generation of applications for the Industrial Metaverse."],"url":"http://arxiv.org/abs/2403.11312v1","category":"cs.ET"}
{"created":"2024-03-17 19:12:26","title":"Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding","abstract":"Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.","sentences":["Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence.","The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning.","In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA).","To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM).","Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction.","Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion.","On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods."],"url":"http://arxiv.org/abs/2403.11311v1","category":"cs.CL"}
{"created":"2024-03-17 19:10:07","title":"A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation","abstract":"3D human pose data collected in controlled laboratory settings present challenges for pose estimators that generalize across diverse scenarios. To address this, domain generalization is employed. Current methodologies in domain generalization for 3D human pose estimation typically utilize adversarial training to generate synthetic poses for training. Nonetheless, these approaches exhibit several limitations. First, the lack of prior information about the target domain complicates the application of suitable augmentation through a single pose augmentor, affecting generalization on target domains. Moreover, adversarial training's discriminator tends to enforce similarity between source and synthesized poses, impeding the exploration of out-of-source distributions. Furthermore, the pose estimator's optimization is not exposed to domain shifts, limiting its overall generalization ability.   To address these limitations, we propose a novel framework featuring two pose augmentors: the weak and the strong augmentors. Our framework employs differential strategies for generation and discrimination processes, facilitating the preservation of knowledge related to source poses and the exploration of out-of-source distributions without prior information about target poses. Besides, we leverage meta-optimization to simulate domain shifts in the optimization process of the pose estimator, thereby improving its generalization ability. Our proposed approach significantly outperforms existing methods, as demonstrated through comprehensive experiments on various benchmark datasets.","sentences":["3D human pose data collected in controlled laboratory settings present challenges for pose estimators that generalize across diverse scenarios.","To address this, domain generalization is employed.","Current methodologies in domain generalization for 3D human pose estimation typically utilize adversarial training to generate synthetic poses for training.","Nonetheless, these approaches exhibit several limitations.","First, the lack of prior information about the target domain complicates the application of suitable augmentation through a single pose augmentor, affecting generalization on target domains.","Moreover, adversarial training's discriminator tends to enforce similarity between source and synthesized poses, impeding the exploration of out-of-source distributions.","Furthermore, the pose estimator's optimization is not exposed to domain shifts, limiting its overall generalization ability.   ","To address these limitations, we propose a novel framework featuring two pose augmentors: the weak and the strong augmentors.","Our framework employs differential strategies for generation and discrimination processes, facilitating the preservation of knowledge related to source poses and the exploration of out-of-source distributions without prior information about target poses.","Besides, we leverage meta-optimization to simulate domain shifts in the optimization process of the pose estimator, thereby improving its generalization ability.","Our proposed approach significantly outperforms existing methods, as demonstrated through comprehensive experiments on various benchmark datasets."],"url":"http://arxiv.org/abs/2403.11310v1","category":"cs.CV"}
{"created":"2024-03-17 19:05:40","title":"An upper bound of the mutation probability in the genetic algorithm for general 0-1 knapsack problem","abstract":"As an important part of genetic algorithms (GAs), mutation operators is widely used in evolutionary algorithms to solve $\\mathcal{NP}$-hard problems because it can increase the population diversity of individual. Due to limitations in mathematical tools, the mutation probability of the mutation operator is primarily empirically set in practical applications.   In this paper, we propose a novel reduction method for the 0-1 knapsack problem(0-1 KP) and an improved mutation operator (IMO) based on the assumption $\\mathcal{NP}\\neq\\mathcal{P}$, along with the utilization of linear relaxation techniques and a recent result by Dey et al. (Math. Prog., pp 569-587, 2022). We employ this method to calculate an upper bound of the mutation probability in general instances of the 0-1 KP, and construct an instance where the mutation probability does not tend towards 0 as the problem size increases. Finally, we prove that the probability of the IMO hitting the optimal solution within only a single iteration in large-scale instances is superior to that of the traditional mutation operator.","sentences":["As an important part of genetic algorithms (GAs), mutation operators is widely used in evolutionary algorithms to solve $\\mathcal{NP}$-hard problems because it can increase the population diversity of individual.","Due to limitations in mathematical tools, the mutation probability of the mutation operator is primarily empirically set in practical applications.   ","In this paper, we propose a novel reduction method for the 0-1 knapsack problem(0-1 KP) and an improved mutation operator (IMO) based on the assumption $\\mathcal{NP}\\neq\\mathcal{P}$, along with the utilization of linear relaxation techniques and a recent result by Dey et al.","(Math.","Prog., pp 569-587, 2022).","We employ this method to calculate an upper bound of the mutation probability in general instances of the 0-1 KP, and construct an instance where the mutation probability does not tend towards 0 as the problem size increases.","Finally, we prove that the probability of the IMO hitting the optimal solution within only a single iteration in large-scale instances is superior to that of the traditional mutation operator."],"url":"http://arxiv.org/abs/2403.11307v1","category":"cs.NE"}
{"created":"2024-03-17 19:01:07","title":"Massive Scalar Field Perturbations of Black Holes Immersed in Chaplygin-Like Dark Fluid","abstract":"We consider massive scalar field perturbations in the background of black holes immersed in Chaplygin-like dark fluid (CDF), and we analyze the photon sphere modes as well as the de Sitter modes and discuss their dominance, by using the pseudospectral Chebyshev method and the third order Wentzel-Kramers-Brillouin approximation. We also discuss the impact of the parameter representing the intensity of the CDF on both families of quasinormal modes. Mainly, we find that the propagation of a massive scalar field is stable in this background, and it is characterized by quasinormal frequencies with a smaller oscillation frequency and a longer decay time compared to the propagation of the same massive scalar field within the Schwarzschild-de Sitter background.","sentences":["We consider massive scalar field perturbations in the background of black holes immersed in Chaplygin-like dark fluid (CDF), and we analyze the photon sphere modes as well as the de Sitter modes and discuss their dominance, by using the pseudospectral Chebyshev method and the third order Wentzel-Kramers-Brillouin approximation.","We also discuss the impact of the parameter representing the intensity of the CDF on both families of quasinormal modes.","Mainly, we find that the propagation of a massive scalar field is stable in this background, and it is characterized by quasinormal frequencies with a smaller oscillation frequency and a longer decay time compared to the propagation of the same massive scalar field within the Schwarzschild-de Sitter background."],"url":"http://arxiv.org/abs/2403.11306v1","category":"gr-qc"}
{"created":"2024-03-17 19:01:03","title":"Flipping Out: Role of Arginine in Hydrophobic Polymer Collapse","abstract":"Arginine has been a mainstay in biological formulation development for decades. To date, the way arginine modulates protein stability has been widely studied and debated. Here, we employed a hydrophobic polymer to decouple hydrophobic effects from other interactions relevant to protein folding. While existing hypotheses for the effects of arginine can generally be categorized as either direct or indirect, our results indicate that direct and indirect mechanisms of arginine co-exist and oppose each other. At low concentrations, arginine was observed to stabilize hydrophobic polymer collapse via a sidechain-dominated direct mechanism, while at high concentrations, arginine stabilized polymer collapse via a backbone-dominated indirect mechanism. These findings highlight the modular nature of the widely used additive arginine, with relevance in the design of stable biological formulations.","sentences":["Arginine has been a mainstay in biological formulation development for decades.","To date, the way arginine modulates protein stability has been widely studied and debated.","Here, we employed a hydrophobic polymer to decouple hydrophobic effects from other interactions relevant to protein folding.","While existing hypotheses for the effects of arginine can generally be categorized as either direct or indirect, our results indicate that direct and indirect mechanisms of arginine co-exist and oppose each other.","At low concentrations, arginine was observed to stabilize hydrophobic polymer collapse via a sidechain-dominated direct mechanism, while at high concentrations, arginine stabilized polymer collapse via a backbone-dominated indirect mechanism.","These findings highlight the modular nature of the widely used additive arginine, with relevance in the design of stable biological formulations."],"url":"http://arxiv.org/abs/2403.11305v1","category":"cond-mat.soft"}
{"created":"2024-03-17 18:53:46","title":"Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving","abstract":"Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates a momentum from embedded vehicle positions toward the route in latent space while keeping the equivariance property. Route attraction enables goal-oriented behavior without forcing the vehicle to stick to the exact route. We conduct experiments on the challenging nuScenes dataset to investigate the capability of our planner. The results show that the planned trajectory is stable under roto-translations of the input scene which demonstrates the equivariance of our model. Despite using only a small split of the dataset for training, our method improves L2 distance at 3 s by 20.6 % and surpasses the state of the art.","sentences":["Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving.","As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions.","Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene.","However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space.","We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan.","The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters.","We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system.","This module creates a momentum from embedded vehicle positions toward the route in latent space while keeping the equivariance property.","Route attraction enables goal-oriented behavior without forcing the vehicle to stick to the exact route.","We conduct experiments on the challenging nuScenes dataset to investigate the capability of our planner.","The results show that the planned trajectory is stable under roto-translations of the input scene which demonstrates the equivariance of our model.","Despite using only a small split of the dataset for training, our method improves L2 distance at 3 s by 20.6 % and surpasses the state of the art."],"url":"http://arxiv.org/abs/2403.11304v1","category":"cs.RO"}
{"created":"2024-03-17 18:48:46","title":"A Brief Study of Computer Network Security Technologies","abstract":"The rapid development of computer network system brings both a great convenience and new security threats for users. Network security problem generally includes network system security and data security. Specifically, it refers to the reliability of network system, confidentiality, integrity and availability of data information in the system. This paper introduces the significance of network security systems and highlights related technologies, mainly authentication, data encryption, firewall and antivirus technology. Network security problems can be faced by any network user, therefore we must greatly prioritize network security, try to prevent hostile attacks and ensure the overall security of the network system.","sentences":["The rapid development of computer network system brings both a great convenience and new security threats for users.","Network security problem generally includes network system security and data security.","Specifically, it refers to the reliability of network system, confidentiality, integrity and availability of data information in the system.","This paper introduces the significance of network security systems and highlights related technologies, mainly authentication, data encryption, firewall and antivirus technology.","Network security problems can be faced by any network user, therefore we must greatly prioritize network security, try to prevent hostile attacks and ensure the overall security of the network system."],"url":"http://arxiv.org/abs/2403.11303v1","category":"cs.CR"}
{"created":"2024-03-17 18:47:46","title":"Koopman Regularization","abstract":"Restoration, generalization, and dimensionality reduction of a vector field from samples are the most common and crucial tasks in dynamical system analysis. An optimization-based algorithm to fulfill these tasks is suggested. Given noisy, sparse, or redundant sampled vector fields, the optimization process encapsulates the inherent geometry of the dynamical system derived from the Koopman eigenfunction space. The dynamic geometry is revealed via the exact penalty method, compromising accuracy and smoothness. This algorithm is backed up by promising results of denoising and generalization with a concise dynamics representation leading to dimensionality reduction.","sentences":["Restoration, generalization, and dimensionality reduction of a vector field from samples are the most common and crucial tasks in dynamical system analysis.","An optimization-based algorithm to fulfill these tasks is suggested.","Given noisy, sparse, or redundant sampled vector fields, the optimization process encapsulates the inherent geometry of the dynamical system derived from the Koopman eigenfunction space.","The dynamic geometry is revealed via the exact penalty method, compromising accuracy and smoothness.","This algorithm is backed up by promising results of denoising and generalization with a concise dynamics representation leading to dimensionality reduction."],"url":"http://arxiv.org/abs/2403.11302v1","category":"math.DS"}
{"created":"2024-03-17 18:46:13","title":"Can nonlocal gravity explain dark energy?","abstract":"In view to scrutinise the idea that nonlocal modifications of \\GR~could dynamically address the dark energy problem, we investigate the evolution of the Universe at infrared scales as an \\IDG~model of the Ricci scalar, without introducing the cosmological constant $\\Lambda$ or any scalar field. The accelerated expansion of the late Universe is shown to be compatible with the emergence of nonlocal gravitational effects at sufficiently low energies. A technique for circumventing the mathematical complexity of the nonlocal cosmological equations is explained and, after drawing a connection with the Starobinsky gravity, verifiable predictions are considered, like a possible decreasing in the strength of the effective gravitational constant.","sentences":["In view to scrutinise the idea that nonlocal modifications of \\GR~could dynamically address the dark energy problem, we investigate the evolution of the Universe at infrared scales as an \\IDG~model of the Ricci scalar, without introducing the cosmological constant $\\Lambda$ or any scalar field.","The accelerated expansion of the late Universe is shown to be compatible with the emergence of nonlocal gravitational effects at sufficiently low energies.","A technique for circumventing the mathematical complexity of the nonlocal cosmological equations is explained and, after drawing a connection with the Starobinsky gravity, verifiable predictions are considered, like a possible decreasing in the strength of the effective gravitational constant."],"url":"http://arxiv.org/abs/2403.11301v1","category":"gr-qc"}
{"created":"2024-03-17 18:42:38","title":"SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant","abstract":"Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-related questions while analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent performance improvement compared with traditional visual-instruction tuning methods. This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts.","sentences":["Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning.","However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck.","To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain.","However, the image contains rich contextual information that has been largely under-explored.","This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions.","In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant.","SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-related questions while analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding.","Moreover, fine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent performance improvement compared with traditional visual-instruction tuning methods.","This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts."],"url":"http://arxiv.org/abs/2403.11299v1","category":"cs.CV"}
{"created":"2024-03-17 18:39:14","title":"A Modified Word Saliency-Based Adversarial Attack on Text Classification Models","abstract":"This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence. By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning. Empirical evaluations conducted on diverse text classification datasets demonstrate the effectiveness of the proposed method in generating adversarial examples capable of successfully deceiving state-of-the-art classification models. Comparative analyses with existing adversarial attack techniques further indicate the superiority of the proposed approach in terms of both attack success rate and preservation of text coherence.","sentences":["This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA).","The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence.","By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems.","The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process.","Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning.","Empirical evaluations conducted on diverse text classification datasets demonstrate the effectiveness of the proposed method in generating adversarial examples capable of successfully deceiving state-of-the-art classification models.","Comparative analyses with existing adversarial attack techniques further indicate the superiority of the proposed approach in terms of both attack success rate and preservation of text coherence."],"url":"http://arxiv.org/abs/2403.11297v1","category":"cs.CL"}
{"created":"2024-03-17 18:28:24","title":"Order-One Rolling Shutter Cameras","abstract":"Rolling shutter (RS) cameras dominate consumer and smartphone markets. Several methods for computing the absolute pose of RS cameras have appeared in the last 20 years, but the relative pose problem has not been fully solved yet. We provide a unified theory for the important class of order-one rolling shutter (RS$_1$) cameras. These cameras generalize the perspective projection to RS cameras, projecting a generic space point to exactly one image point via a rational map. We introduce a new back-projection RS camera model, characterize RS$_1$ cameras, construct explicit parameterizations of such cameras, and determine the image of a space line. We classify all minimal problems for solving the relative camera pose problem with linear RS$_1$ cameras and discover new practical cases. Finally, we show how the theory can be used to explain RS models previously used for absolute pose computation.","sentences":["Rolling shutter (RS) cameras dominate consumer and smartphone markets.","Several methods for computing the absolute pose of RS cameras have appeared in the last 20 years, but the relative pose problem has not been fully solved yet.","We provide a unified theory for the important class of order-one rolling shutter (RS$_1$) cameras.","These cameras generalize the perspective projection to RS cameras, projecting a generic space point to exactly one image point via a rational map.","We introduce a new back-projection RS camera model, characterize RS$_1$ cameras, construct explicit parameterizations of such cameras, and determine the image of a space line.","We classify all minimal problems for solving the relative camera pose problem with linear RS$_1$ cameras and discover new practical cases.","Finally, we show how the theory can be used to explain RS models previously used for absolute pose computation."],"url":"http://arxiv.org/abs/2403.11295v1","category":"cs.CV"}
{"created":"2024-03-17 18:08:22","title":"Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction","abstract":"Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relational embeddings that have produced it. We also propose five benchmarks based on two retail domains, where we show that GOOD can effectively generalize predictions out of known relationship types and achieve state-of-the-art results. Most importantly, we provide insights into problems where out-of-domain prediction might be preferred to an in-domain formulation, that is, where the relationship to be predicted has very few positive examples.","sentences":["Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time.","Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution.","In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph.","We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem.","GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relational embeddings that have produced it.","We also propose five benchmarks based on two retail domains, where we show that GOOD can effectively generalize predictions out of known relationship types and achieve state-of-the-art results.","Most importantly, we provide insights into problems where out-of-domain prediction might be preferred to an in-domain formulation, that is, where the relationship to be predicted has very few positive examples."],"url":"http://arxiv.org/abs/2403.11292v1","category":"cs.LG"}
{"created":"2024-03-17 17:59:59","title":"ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models","abstract":"The integration of Multimodal Large Language Models (MLLMs) with robotic systems has significantly enhanced the ability of robots to interpret and act upon natural language instructions. Despite these advancements, conventional MLLMs are typically trained on generic image-text pairs, lacking essential robotics knowledge such as affordances and physical knowledge, which hampers their efficacy in manipulation tasks. To bridge this gap, we introduce ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric knowledge through a Visual Question-Answering format. This approach not only encompasses tool detection and affordance recognition but also extends to a comprehensive understanding of physical concepts. Our approach starts with collecting a varied set of images displaying interactive objects, which presents a broad range of challenges in tool object detection, affordance, and physical concept predictions. To seamlessly integrate this robotic-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a unified VQA format and devise a fine-tuning strategy that preserves the original vision-reasoning abilities while incorporating the new robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. Code and dataset will be made publicly available at https://github.com/SiyuanHuang95/ManipVQA.","sentences":["The integration of Multimodal Large Language Models (MLLMs) with robotic systems has significantly enhanced the ability of robots to interpret and act upon natural language instructions.","Despite these advancements, conventional MLLMs are typically trained on generic image-text pairs, lacking essential robotics knowledge such as affordances and physical knowledge, which hampers their efficacy in manipulation tasks.","To bridge this gap, we introduce ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric knowledge through a Visual Question-Answering format.","This approach not only encompasses tool detection and affordance recognition but also extends to a comprehensive understanding of physical concepts.","Our approach starts with collecting a varied set of images displaying interactive objects, which presents a broad range of challenges in tool object detection, affordance, and physical concept predictions.","To seamlessly integrate this robotic-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a unified VQA format and devise a fine-tuning strategy that preserves the original vision-reasoning abilities while incorporating the new robotic insights.","Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA.","Code and dataset will be made publicly available at https://github.com/SiyuanHuang95/ManipVQA."],"url":"http://arxiv.org/abs/2403.11289v1","category":"cs.RO"}
{"created":"2024-03-17 17:42:02","title":"Fast Personalized Text-to-Image Syntheses With Attention Injection","abstract":"Currently, personalized image generation methods mostly require considerable time to finetune and often overfit the concept resulting in generated images that are similar to custom concepts but difficult to edit by prompts. We propose an effective and fast approach that could balance the text-image consistency and identity consistency of the generated image and reference image. Our method can generate personalized images without any fine-tuning while maintaining the inherent text-to-image generation ability of diffusion models. Given a prompt and a reference image, we merge the custom concept into generated images by manipulating cross-attention and self-attention layers of the original diffusion model to generate personalized images that match the text description. Comprehensive experiments highlight the superiority of our method.","sentences":["Currently, personalized image generation methods mostly require considerable time to finetune and often overfit the concept resulting in generated images that are similar to custom concepts but difficult to edit by prompts.","We propose an effective and fast approach that could balance the text-image consistency and identity consistency of the generated image and reference image.","Our method can generate personalized images without any fine-tuning while maintaining the inherent text-to-image generation ability of diffusion models.","Given a prompt and a reference image, we merge the custom concept into generated images by manipulating cross-attention and self-attention layers of the original diffusion model to generate personalized images that match the text description.","Comprehensive experiments highlight the superiority of our method."],"url":"http://arxiv.org/abs/2403.11284v1","category":"cs.CV"}
{"created":"2024-03-17 17:41:01","title":"Pattern-Based Peephole Optimizations with Java JIT Tests","abstract":"We present JOG, a framework that facilitates developing Java JIT peephole optimizations alongside JIT tests. JOG enables developers to write a pattern, in Java itself, that specifies desired code transformations by writing code before and after the optimization, as well as any necessary preconditions. Such patterns can be written in the same way that tests of the optimization are already written in OpenJDK. JOG translates each pattern into C/C++ code that can be integrated as a JIT optimization pass. JOG also generates Java tests for optimizations from patterns. Furthermore, JOG can automatically detect possible shadow relation between a pair of optimizations where the effect of the shadowed optimization is overridden by another. Our evaluation shows that JOG makes it easier to write readable JIT optimizations alongside tests without decreasing the effectiveness of JIT optimizations. We wrote 162 patterns, including 68 existing optimizations in OpenJDK, 92 new optimizations adapted from LLVM, and two new optimizations that we proposed. We opened eight pull requests (PRs) for OpenJDK, including six for new optimizations, one on removing shadowed optimizations, and one for newly generated JIT tests; seven PRs have already been integrated into the master branch of OpenJDK.","sentences":["We present JOG, a framework that facilitates developing Java JIT peephole optimizations alongside JIT tests.","JOG enables developers to write a pattern, in Java itself, that specifies desired code transformations by writing code before and after the optimization, as well as any necessary preconditions.","Such patterns can be written in the same way that tests of the optimization are already written in OpenJDK.","JOG translates each pattern into C/C++ code that can be integrated as a JIT optimization pass.","JOG also generates Java tests for optimizations from patterns.","Furthermore, JOG can automatically detect possible shadow relation between a pair of optimizations where the effect of the shadowed optimization is overridden by another.","Our evaluation shows that JOG makes it easier to write readable JIT optimizations alongside tests without decreasing the effectiveness of JIT optimizations.","We wrote 162 patterns, including 68 existing optimizations in OpenJDK, 92 new optimizations adapted from LLVM, and two new optimizations that we proposed.","We opened eight pull requests (PRs) for OpenJDK, including six for new optimizations, one on removing shadowed optimizations, and one for newly generated JIT tests; seven PRs have already been integrated into the master branch of OpenJDK."],"url":"http://arxiv.org/abs/2403.11283v1","category":"cs.SE"}
{"created":"2024-03-17 17:40:27","title":"On non-Newtonian Helices in Multiplicative Euclidean Space","abstract":"In this article, spherical indicatrices of a curve and helices are re-examined using both the algebraic structure and the geometric structure of non-Newtonian (multiplicative) Euclidean space. Indicatrices of a multiplicative curve on the multiplicative sphere in multiplicative space are obtained. In addition, multiplicative general helix, multiplicative slant helix and multiplicative clad and multiplicative g-clad helix characterizations are provided. Finally, examples and drawings are given.","sentences":["In this article, spherical indicatrices of a curve and helices are re-examined using both the algebraic structure and the geometric structure of non-Newtonian (multiplicative) Euclidean space.","Indicatrices of a multiplicative curve on the multiplicative sphere in multiplicative space are obtained.","In addition, multiplicative general helix, multiplicative slant helix and multiplicative clad and multiplicative g-clad helix characterizations are provided.","Finally, examples and drawings are given."],"url":"http://arxiv.org/abs/2403.11282v1","category":"math.GM"}
{"created":"2024-03-17 17:39:27","title":"Java JIT Testing with Template Extraction","abstract":"We present LeJit, a template-based framework for testing Java just-in-time (JIT) compilers. Like recent template-based frameworks, LeJit executes a template -- a program with holes to be filled -- to generate concrete programs given as inputs to Java JIT compilers. LeJit automatically generates template programs from existing Java code by converting expressions to holes, as well as generating necessary glue code (i.e., code that generates instances of non-primitive types) to make generated templates executable. We have successfully used LeJit to test a range of popular Java JIT compilers, revealing five bugs in HotSpot, nine bugs in OpenJ9, and one bug in GraalVM. All of these bugs have been confirmed by Oracle and IBM developers, and 11 of these bugs were previously unknown, including two CVEs (Common Vulnerabilities and Exposures). Our comparison with several existing approaches shows that LeJit is complementary to them and is a powerful technique for ensuring Java JIT compiler correctness.","sentences":["We present LeJit, a template-based framework for testing Java just-in-time (JIT) compilers.","Like recent template-based frameworks, LeJit executes a template -- a program with holes to be filled -- to generate concrete programs given as inputs to Java JIT compilers.","LeJit automatically generates template programs from existing Java code by converting expressions to holes, as well as generating necessary glue code (i.e., code that generates instances of non-primitive types) to make generated templates executable.","We have successfully used LeJit to test a range of popular Java JIT compilers, revealing five bugs in HotSpot, nine bugs in OpenJ9, and one bug in GraalVM.","All of these bugs have been confirmed by Oracle and IBM developers, and 11 of these bugs were previously unknown, including two CVEs (Common Vulnerabilities and Exposures).","Our comparison with several existing approaches shows that LeJit is complementary to them and is a powerful technique for ensuring Java JIT compiler correctness."],"url":"http://arxiv.org/abs/2403.11281v1","category":"cs.SE"}
{"created":"2024-03-17 17:37:37","title":"Revisiting the excitation of the low-lying $^{181\\text{m}}$Ta isomer in optical laser-generated plasma","abstract":"The excitation of the $^{181\\text{m}}$Ta isomer in the laser-plasma scenario was claimed to have been observed more than two decades ago. However, the reported experimental findings - and the respective high excitation rate - were later questioned as they could not be reproduced theoretically. The controversy has remained open ever since. In this work, we reinvestigate both theoretically and experimentally the $^{181\\text{m}}$Ta nuclear excitation in an optical laser-generated plasma. Experimentally we have found no evidence for such an excitation process as consistently predicted by previous and our theoretical models.","sentences":["The excitation of the $^{181\\text{m}}$Ta isomer in the laser-plasma scenario was claimed to have been observed more than two decades ago.","However, the reported experimental findings - and the respective high excitation rate - were later questioned as they could not be reproduced theoretically.","The controversy has remained open ever since.","In this work, we reinvestigate both theoretically and experimentally the $^{181\\text{m}}$Ta nuclear excitation in an optical laser-generated plasma.","Experimentally we have found no evidence for such an excitation process as consistently predicted by previous and our theoretical models."],"url":"http://arxiv.org/abs/2403.11280v1","category":"nucl-ex"}
{"created":"2024-03-17 17:35:41","title":"A non-Newtonian some partner curves in multiplicative Euclidean space","abstract":"The aim of this article is to characterize pairs of curves within multiplicative (non-Newtonian) spaces. Specifically, we investigate how famous curve pairs such as Bertrand partner curves, Mannheim partner curves, which are prominent in differential geometry, are transformed under the influence of multiplicative analysis. By leveraging the relationships between multiplicative Frenet vectors, we introduce multiplicative versions of Bertrand, Mannheim curve pairs. Subsequently, we characterize these curve pairs using multiplicative arguments. Examples are provided, and multiplicative graphs are presented to enhance understanding of the subject matter. Through this analysis, we aim to elucidate the behavior and properties of these curve pairs within the context of multiplicative geometry.","sentences":["The aim of this article is to characterize pairs of curves within multiplicative (non-Newtonian) spaces.","Specifically, we investigate how famous curve pairs such as Bertrand partner curves, Mannheim partner curves, which are prominent in differential geometry, are transformed under the influence of multiplicative analysis.","By leveraging the relationships between multiplicative Frenet vectors, we introduce multiplicative versions of Bertrand, Mannheim curve pairs.","Subsequently, we characterize these curve pairs using multiplicative arguments.","Examples are provided, and multiplicative graphs are presented to enhance understanding of the subject matter.","Through this analysis, we aim to elucidate the behavior and properties of these curve pairs within the context of multiplicative geometry."],"url":"http://arxiv.org/abs/2403.11278v1","category":"math.GM"}
{"created":"2024-03-17 17:19:05","title":"Effects of model misspecification on small area estimators","abstract":"Nested error regression models are commonly used to incorporate observational unit specific auxiliary variables to improve small area estimates. When the mean structure of this model is misspecified, there is generally an increase in the mean square prediction error (MSPE) of Empirical Best Linear Unbiased Predictors (EBLUP). Observed Best Prediction (OBP) method has been proposed with the intent to improve on the MSPE over EBLUP. We conduct a Monte Carlo simulation experiment to understand the effect of mispsecification of mean structures on different small area estimators. Our simulation results lead to an unexpected result that OBP may perform very poorly when observational unit level auxiliary variables are used and that OBP can be improved significantly when population means of those auxiliary variables (area level auxiliary variables) are used in the nested error regression model or when a corresponding area level model is used. Our simulation also indicates that the MSPE of OBP in an increasing function of the difference between the sample and population means of the auxiliary variables.","sentences":["Nested error regression models are commonly used to incorporate observational unit specific auxiliary variables to improve small area estimates.","When the mean structure of this model is misspecified, there is generally an increase in the mean square prediction error (MSPE) of Empirical Best Linear Unbiased Predictors (EBLUP).","Observed Best Prediction (OBP) method has been proposed with the intent to improve on the MSPE over EBLUP.","We conduct a Monte Carlo simulation experiment to understand the effect of mispsecification of mean structures on different small area estimators.","Our simulation results lead to an unexpected result that OBP may perform very poorly when observational unit level auxiliary variables are used and that OBP can be improved significantly when population means of those auxiliary variables (area level auxiliary variables) are used in the nested error regression model or when a corresponding area level model is used.","Our simulation also indicates that the MSPE of OBP in an increasing function of the difference between the sample and population means of the auxiliary variables."],"url":"http://arxiv.org/abs/2403.11276v1","category":"stat.ME"}
{"created":"2024-03-17 17:04:45","title":"BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis","abstract":"Text-to-3D synthesis has recently seen intriguing advances by combining the text-to-image models with 3D representation methods, e.g., Gaussian Splatting (GS), via Score Distillation Sampling (SDS). However, a hurdle of existing methods is the low efficiency, per-prompt optimization for a single 3D object. Therefore, it is imperative for a paradigm shift from per-prompt optimization to one-stage generation for any unseen text prompts, which yet remains challenging. A hurdle is how to directly generate a set of millions of 3D Gaussians to represent a 3D object. This paper presents BrightDreamer, an end-to-end single-stage approach that can achieve generalizable and fast (77 ms) text-to-3D generation. Our key idea is to formulate the generation process as estimating the 3D deformation from an anchor shape with predefined positions. For this, we first propose a Text-guided Shape Deformation (TSD) network to predict the deformed shape and its new positions, used as the centers (one attribute) of 3D Gaussians. To estimate the other four attributes (i.e., scaling, rotation, opacity, and SH coefficient), we then design a novel Text-guided Triplane Generator (TTG) to generate a triplane representation for a 3D object. The center of each Gaussian enables us to transform the triplane feature into the four attributes. The generated 3D Gaussians can be finally rendered at 705 frames per second. Extensive experiments demonstrate the superiority of our method over existing methods. Also, BrightDreamer possesses a strong semantic understanding capability even for complex text prompts. The project code is available at https://vlislab22.github.io/BrightDreamer.","sentences":["Text-to-3D synthesis has recently seen intriguing advances by combining the text-to-image models with 3D representation methods, e.g., Gaussian Splatting (GS), via Score Distillation Sampling (SDS).","However, a hurdle of existing methods is the low efficiency, per-prompt optimization for a single 3D object.","Therefore, it is imperative for a paradigm shift from per-prompt optimization to one-stage generation for any unseen text prompts, which yet remains challenging.","A hurdle is how to directly generate a set of millions of 3D Gaussians to represent a 3D object.","This paper presents BrightDreamer, an end-to-end single-stage approach that can achieve generalizable and fast (77 ms) text-to-3D generation.","Our key idea is to formulate the generation process as estimating the 3D deformation from an anchor shape with predefined positions.","For this, we first propose a Text-guided Shape Deformation (TSD) network to predict the deformed shape and its new positions, used as the centers (one attribute) of 3D Gaussians.","To estimate the other four attributes (i.e., scaling, rotation, opacity, and SH coefficient), we then design a novel Text-guided Triplane Generator (TTG) to generate a triplane representation for a 3D object.","The center of each Gaussian enables us to transform the triplane feature into the four attributes.","The generated 3D Gaussians can be finally rendered at 705 frames per second.","Extensive experiments demonstrate the superiority of our method over existing methods.","Also, BrightDreamer possesses a strong semantic understanding capability even for complex text prompts.","The project code is available at https://vlislab22.github.io/BrightDreamer."],"url":"http://arxiv.org/abs/2403.11273v1","category":"cs.CV"}
{"created":"2024-03-17 16:48:46","title":"Bilateral Propagation Network for Depth Completion","abstract":"Depth completion aims to derive a dense depth map from sparse depth measurements with a synchronized color image. Current state-of-the-art (SOTA) methods are predominantly propagation-based, which work as an iterative refinement on the initial estimated dense depth. However, the initial depth estimations mostly result from direct applications of convolutional layers on the sparse depth map. In this paper, we present a Bilateral Propagation Network (BP-Net), that propagates depth at the earliest stage to avoid directly convolving on sparse data. Specifically, our approach propagates the target depth from nearby depth measurements via a non-linear model, whose coefficients are generated through a multi-layer perceptron conditioned on both \\emph{radiometric difference} and \\emph{spatial distance}. By integrating bilateral propagation with multi-modal fusion and depth refinement in a multi-scale framework, our BP-Net demonstrates outstanding performance on both indoor and outdoor scenes. It achieves SOTA on the NYUv2 dataset and ranks 1st on the KITTI depth completion benchmark at the time of submission. Experimental results not only show the effectiveness of bilateral propagation but also emphasize the significance of early-stage propagation in contrast to the refinement stage. Our code and trained models will be available on the project page.","sentences":["Depth completion aims to derive a dense depth map from sparse depth measurements with a synchronized color image.","Current state-of-the-art (SOTA) methods are predominantly propagation-based, which work as an iterative refinement on the initial estimated dense depth.","However, the initial depth estimations mostly result from direct applications of convolutional layers on the sparse depth map.","In this paper, we present a Bilateral Propagation Network (BP-Net), that propagates depth at the earliest stage to avoid directly convolving on sparse data.","Specifically, our approach propagates the target depth from nearby depth measurements via a non-linear model, whose coefficients are generated through a multi-layer perceptron conditioned on both \\emph{radiometric difference} and \\emph{spatial distance}.","By integrating bilateral propagation with multi-modal fusion and depth refinement in a multi-scale framework, our BP-Net demonstrates outstanding performance on both indoor and outdoor scenes.","It achieves SOTA on the NYUv2 dataset and ranks 1st on the KITTI depth completion benchmark at the time of submission.","Experimental results not only show the effectiveness of bilateral propagation but also emphasize the significance of early-stage propagation in contrast to the refinement stage.","Our code and trained models will be available on the project page."],"url":"http://arxiv.org/abs/2403.11270v1","category":"cs.CV"}
{"created":"2024-03-17 16:47:15","title":"A signed graph product and its Application","abstract":"A signed graph product is defined for a new product, and initially the unsigned graph product's Laplacian spectrum and signless Laplacian spectrum are found. Next, for the signed graph product, the adjacency spectrum, Laplacian spectrum, and signless Laplacian spectrum are found. In the end, we determined the prerequisite for the signed graph product to be integral and we also have generated a sequence of co-spectral signed graphs and a sequence of non-co-spectral equienergetic signed graphs.","sentences":["A signed graph product is defined for a new product, and initially the unsigned graph product's Laplacian spectrum and signless Laplacian spectrum are found.","Next, for the signed graph product, the adjacency spectrum, Laplacian spectrum, and signless Laplacian spectrum are found.","In the end, we determined the prerequisite for the signed graph product to be integral and we also have generated a sequence of co-spectral signed graphs and a sequence of non-co-spectral equienergetic signed graphs."],"url":"http://arxiv.org/abs/2403.11269v1","category":"math.CO"}
{"created":"2024-03-17 16:43:15","title":"Barely Random Algorithms for Metrical Task Systems","abstract":"We consider metrical task systems on general metric spaces with $n$ points, and show that any fully randomized algorithm can be turned into a randomized algorithm that uses only $2\\log n$ random bits, and achieves the same competitive ratio up to a factor $2$. This provides the first order-optimal barely random algorithms for metrical task systems, i.e. which use a number of random bits that does not depend on the number of requests addressed to the system. We put forward an equivalent view that we call collective metrical task systems where $k$ agents in a metrical task system team up, and suffer the average cost paid by each agent. Our results imply that such team can be $O(\\log n^2)$-competitive, as soon as $k\\geq n^2$ (in comparison, a single agent is $\\Omega(n)$-competitive at best). We discuss implications on various aspects of online decision making such as: distributed systems, transaction costs, and advice complexity, suggesting broad applicability.","sentences":["We consider metrical task systems on general metric spaces with $n$ points, and show that any fully randomized algorithm can be turned into a randomized algorithm that uses only $2\\log n$ random bits, and achieves the same competitive ratio up to a factor $2$.","This provides the first order-optimal barely random algorithms for metrical task systems, i.e. which use a number of random bits that does not depend on the number of requests addressed to the system.","We put forward an equivalent view that we call collective metrical task systems where $k$ agents in a metrical task system team up, and suffer the average cost paid by each agent.","Our results imply that such team can be $O(\\log n^2)$-competitive, as soon as $k\\geq n^2$ (in comparison, a single agent is $\\Omega(n)$-competitive at best).","We discuss implications on various aspects of online decision making such as: distributed systems, transaction costs, and advice complexity, suggesting broad applicability."],"url":"http://arxiv.org/abs/2403.11267v1","category":"cs.DS"}
{"created":"2024-03-17 16:36:26","title":"Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation","abstract":"Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT model) and with two training strategies (one inspired by standard Language Models, and another inspired by Wasserstein Generative Adversarial Networks). We evaluate our hypothesis on five datasets (three of which have been specifically collected to represent an adversarial setting) and using two learning algorithms for the AV classifier (Support Vector Machines and Convolutional Neural Networks). This experimentation has yielded negative results, revealing that, although our methodology proves effective in many adversarial settings, its benefits are too sporadic for a pragmatical application.","sentences":["Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else.","It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author.","In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples.","These synthetic examples are generated to imitate the style of the author of interest.","We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting.","In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT model) and with two training strategies (one inspired by standard Language Models, and another inspired by Wasserstein Generative Adversarial Networks).","We evaluate our hypothesis on five datasets (three of which have been specifically collected to represent an adversarial setting) and using two learning algorithms for the AV classifier (Support Vector Machines and Convolutional Neural Networks).","This experimentation has yielded negative results, revealing that, although our methodology proves effective in many adversarial settings, its benefits are too sporadic for a pragmatical application."],"url":"http://arxiv.org/abs/2403.11265v1","category":"cs.LG"}
{"created":"2024-03-17 16:26:17","title":"The exact solution of the Wegner flow equation with the Mielke generator for $3\\times 3$ Hermitian matrices","abstract":"The exact solution of the Wegner flow equation with the Mielke generator for $3\\times 3$ Hermitian matrices is presented. The general solutions for $N\\times N$ tridiagonal Hermitian matrices and partially for $4\\times 4$ real symmetric matrices are also given.","sentences":["The exact solution of the Wegner flow equation with the Mielke generator for $3\\times 3$ Hermitian matrices is presented.","The general solutions for $N\\times N$ tridiagonal Hermitian matrices and partially for $4\\times 4$ real symmetric matrices are also given."],"url":"http://arxiv.org/abs/2403.11264v1","category":"math-ph"}
{"created":"2024-03-17 16:25:25","title":"Stylized Face Sketch Extraction via Generative Prior with Limited Data","abstract":"Facial sketches are both a concise way of showing the identity of a person and a means to express artistic intention. While a few techniques have recently emerged that allow sketches to be extracted in different styles, they typically rely on a large amount of data that is difficult to obtain. Here, we propose StyleSketch, a method for extracting high-resolution stylized sketches from a face image. Using the rich semantics of the deep features from a pretrained StyleGAN, we are able to train a sketch generator with 16 pairs of face and the corresponding sketch images. The sketch generator utilizes part-based losses with two-stage learning for fast convergence during training for high-quality sketch extraction. Through a set of comparisons, we show that StyleSketch outperforms existing state-of-the-art sketch extraction methods and few-shot image adaptation methods for the task of extracting high-resolution abstract face sketches. We further demonstrate the versatility of StyleSketch by extending its use to other domains and explore the possibility of semantic editing. The project page can be found in https://kwanyun.github.io/stylesketch_project.","sentences":["Facial sketches are both a concise way of showing the identity of a person and a means to express artistic intention.","While a few techniques have recently emerged that allow sketches to be extracted in different styles, they typically rely on a large amount of data that is difficult to obtain.","Here, we propose StyleSketch, a method for extracting high-resolution stylized sketches from a face image.","Using the rich semantics of the deep features from a pretrained StyleGAN, we are able to train a sketch generator with 16 pairs of face and the corresponding sketch images.","The sketch generator utilizes part-based losses with two-stage learning for fast convergence during training for high-quality sketch extraction.","Through a set of comparisons, we show that StyleSketch outperforms existing state-of-the-art sketch extraction methods and few-shot image adaptation methods for the task of extracting high-resolution abstract face sketches.","We further demonstrate the versatility of StyleSketch by extending its use to other domains and explore the possibility of semantic editing.","The project page can be found in https://kwanyun.github.io/stylesketch_project."],"url":"http://arxiv.org/abs/2403.11263v1","category":"cs.CV"}
{"created":"2024-03-17 16:24:29","title":"Understanding Diffusion Models by Feynman's Path Integral","abstract":"Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum physics, for evaluating the negative log-likelihood to assess the performance disparity between stochastic and deterministic sampling schemes.","sentences":["Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear.","We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics.","We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.","The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics.","This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum physics, for evaluating the negative log-likelihood to assess the performance disparity between stochastic and deterministic sampling schemes."],"url":"http://arxiv.org/abs/2403.11262v1","category":"cs.LG"}
{"created":"2024-03-18 12:21:22","title":"On uniqueness of packing of three copies of 2-factors","abstract":"The packing of three copies of a graph $G$ is the union of three edge-disjoint copies (with the same vertex set) of $G$. In this paper, we completely solve the problem of the uniqueness of packing of three copies of 2-regular graphs. In particular, we show that $C_3,C_4,C_5,C_6$ and $2C_3$ have no packing of three copies, $C_7,C_8,C_3 \\cup C_4, C_4 \\cup C_4, C_3 \\cup C_5$ and $3C_3$ have unique packing, and any other collection of cycles has at least two distinct packings.","sentences":["The packing of three copies of a graph $G$ is the union of three edge-disjoint copies (with the same vertex set) of $G$. In this paper, we completely solve the problem of the uniqueness of packing of three copies of 2-regular graphs.","In particular, we show that $C_3,C_4,C_5,C_6$ and $2C_3$ have no packing of three copies, $C_7,C_8,C_3 \\cup C_4, C_4 \\cup C_4, C_3 \\cup C_5$ and $3C_3$ have unique packing, and any other collection of cycles has at least two distinct packings."],"url":"http://arxiv.org/abs/2403.11721v1","category":"math.CO"}
{"created":"2024-03-18 09:55:22","title":"Synthesizing multi-log grasp poses","abstract":"Multi-object grasping is a challenging task. It is important for energy and cost-efficient operation of industrial crane manipulators, such as those used to collect tree logs off the forest floor and onto forest machines. In this work, we used synthetic data from physics simulations to explore how data-driven modeling can be used to infer multi-object grasp poses from images. We showed that convolutional neural networks can be trained specifically for synthesizing multi-object grasps. Using RGB-Depth images and instance segmentation masks as input, a U-Net model outputs grasp maps with corresponding grapple orientation and opening width. Given an observation of a pile of logs, the model can be used to synthesize and rate the possible grasp poses and select the most suitable one, with the possibility to respect changing operational constraints such as lift capacity and reach. When tested on previously unseen data, the proposed model found successful grasp poses with an accuracy of 95%.","sentences":["Multi-object grasping is a challenging task.","It is important for energy and cost-efficient operation of industrial crane manipulators, such as those used to collect tree logs off the forest floor and onto forest machines.","In this work, we used synthetic data from physics simulations to explore how data-driven modeling can be used to infer multi-object grasp poses from images.","We showed that convolutional neural networks can be trained specifically for synthesizing multi-object grasps.","Using RGB-Depth images and instance segmentation masks as input, a U-Net model outputs grasp maps with corresponding grapple orientation and opening width.","Given an observation of a pile of logs, the model can be used to synthesize and rate the possible grasp poses and select the most suitable one, with the possibility to respect changing operational constraints such as lift capacity and reach.","When tested on previously unseen data, the proposed model found successful grasp poses with an accuracy of 95%."],"url":"http://arxiv.org/abs/2403.11623v1","category":"cs.RO"}
{"created":"2024-03-18 09:03:56","title":"UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling","abstract":"Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.","sentences":["Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics.","While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming.","Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering.","However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance.","This hinders the learning procedure of the Gaussians and tends to produce blurry textures.","Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures.","We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features.","Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality.","We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps.","Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose.","The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted."],"url":"http://arxiv.org/abs/2403.11589v1","category":"cs.CV"}
{"created":"2024-03-18 08:56:41","title":"Exploring active learning in physics with ISLE-based modules in high school","abstract":"This study presents a case study of active learning within the Investigative Science Learning Environment (ISLE), using the iOLab digital devices. We designed a pilot lab format to enhance student engagement and understanding through direct experimentation, taking advantage of the multifunctional capabilities of the iOLab devices. This paper evaluates the pedagogical effectiveness of integrating ISLE with digital tools for data collection and analysis in physics experiments. The initial findings provide insights into the pedagogical benefits and logistical considerations of using such technologies in a laboratory setting. Although no direct comparison with traditional teaching methods has been made, the observed student engagement and feedback suggest a positive impact on learning outcomes, even within the constraints of the short duration of the interventions.","sentences":["This study presents a case study of active learning within the Investigative Science Learning Environment (ISLE), using the iOLab digital devices.","We designed a pilot lab format to enhance student engagement and understanding through direct experimentation, taking advantage of the multifunctional capabilities of the iOLab devices.","This paper evaluates the pedagogical effectiveness of integrating ISLE with digital tools for data collection and analysis in physics experiments.","The initial findings provide insights into the pedagogical benefits and logistical considerations of using such technologies in a laboratory setting.","Although no direct comparison with traditional teaching methods has been made, the observed student engagement and feedback suggest a positive impact on learning outcomes, even within the constraints of the short duration of the interventions."],"url":"http://arxiv.org/abs/2403.11583v1","category":"physics.ed-ph"}
{"created":"2024-03-18 08:25:39","title":"Pore pressure study in the Irpinia area (Southern Apennines, Italy)","abstract":"Subsurface pore pressure studies are crucial for understanding the geomechanical behaviours of the geological formations and for preventing the failure conditions of the rocks. Although the interplay between pore pressure changes and rock deformation is nowadays widely treated in the literature, the magnitude and the distribution of the fluid pressure regimes at depth is not completely clear, especially in those areas, such as the fold and thrust belts, characterised by a complex tectonostratigraphic setting. The proposed study deals with the subsurface fluid dynamics of the Irpinia region, located in the Southern Apennines (Italy) and marked by intense tectonic activity and seismicity. In that area, the most recent and notable Italian earthquake occurred in November 1980 (6.9 Mw) and caused significant damage and loss of life. Irpinia area is also a site of deep gas rising to the surface and exhibits clear correlations between crustal deformation and groundwater circulation. The pressure analysis herein proposed has been performed using direct and indirect pressure measurements collected from 13 hydrocarbon exploration wells available in open source. It provides a detailed description of the methodology used to identify where overpressures develop within the sediments of both autochthonous and allochthonous layers. It also investigates the relationship between pore pressures, gas occurrences found at well sites, and the possible sources of overpressures. The results show that the carbonate successions of the South-Apennines and Apulian Platforms are characterized by predominantly hydrostatic pressure regimes, while the shale-rich successions of the Lagonegrese pelagic basin and the Miocene-Pliocene foredeep basin locally demonstrate moderate overpressured gradients. Finally, the highest overpressures are observed in the evaporitic deposits and Pliocene shales.","sentences":["Subsurface pore pressure studies are crucial for understanding the geomechanical behaviours of the geological formations and for preventing the failure conditions of the rocks.","Although the interplay between pore pressure changes and rock deformation is nowadays widely treated in the literature, the magnitude and the distribution of the fluid pressure regimes at depth is not completely clear, especially in those areas, such as the fold and thrust belts, characterised by a complex tectonostratigraphic setting.","The proposed study deals with the subsurface fluid dynamics of the Irpinia region, located in the Southern Apennines (Italy) and marked by intense tectonic activity and seismicity.","In that area, the most recent and notable Italian earthquake occurred in November 1980 (6.9 Mw) and caused significant damage and loss of life.","Irpinia area is also a site of deep gas rising to the surface and exhibits clear correlations between crustal deformation and groundwater circulation.","The pressure analysis herein proposed has been performed using direct and indirect pressure measurements collected from 13 hydrocarbon exploration wells available in open source.","It provides a detailed description of the methodology used to identify where overpressures develop within the sediments of both autochthonous and allochthonous layers.","It also investigates the relationship between pore pressures, gas occurrences found at well sites, and the possible sources of overpressures.","The results show that the carbonate successions of the South-Apennines and Apulian Platforms are characterized by predominantly hydrostatic pressure regimes, while the shale-rich successions of the Lagonegrese pelagic basin and the Miocene-Pliocene foredeep basin locally demonstrate moderate overpressured gradients.","Finally, the highest overpressures are observed in the evaporitic deposits and Pliocene shales."],"url":"http://arxiv.org/abs/2403.11559v1","category":"physics.geo-ph"}
{"created":"2024-03-18 07:59:46","title":"Adiabatic Bottlenecks in Quantum Annealing and Nonequilibrium Dynamics of Paramagnons","abstract":"The correspondence between long-range interacting quantum spin glasses and combinatorial optimization problems underpins the physical motivation for adiabatic quantum computing. On one hand, in disordered (quantum) spin systems, the focus is on exact methods such as the replica trick that allow the calculation of system quantities in the limit of infinite system and ensemble size. On the other hand, when solving a given instance of an optimization problem, disorder-averaged quantities are of no relevance, as one is solely interested in instance-specific, finite-size properties, in particular the true solution. Here, we apply the nonequilibrium Green-function formalism to the spin coherent-state path integral to obtain the statistical fluctuations and the collective-excitation spectrum along the annealing path. For the example of the quantum Sherrington-Kirkpatrick spin glass, by comparing to extensive numerically exact results, we show that this method provides access to the instance-specific bottlenecks of the annealing protocol.","sentences":["The correspondence between long-range interacting quantum spin glasses and combinatorial optimization problems underpins the physical motivation for adiabatic quantum computing.","On one hand, in disordered (quantum) spin systems, the focus is on exact methods such as the replica trick that allow the calculation of system quantities in the limit of infinite system and ensemble size.","On the other hand, when solving a given instance of an optimization problem, disorder-averaged quantities are of no relevance, as one is solely interested in instance-specific, finite-size properties, in particular the true solution.","Here, we apply the nonequilibrium Green-function formalism to the spin coherent-state path integral to obtain the statistical fluctuations and the collective-excitation spectrum along the annealing path.","For the example of the quantum Sherrington-Kirkpatrick spin glass, by comparing to extensive numerically exact results, we show that this method provides access to the instance-specific bottlenecks of the annealing protocol."],"url":"http://arxiv.org/abs/2403.11548v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-18 07:30:39","title":"Development of neutron beamline for laser-driven neutron resonance spectroscopy","abstract":"Recent progress of laser science provides laser-driven neutron source (LDNS), which has remarkable features such as the short pulse width. One of the key techniques to be developed for more efficient use of the LDNS is neutron collimation tubes to increase the number of neutrons arriving at a detector in the time-of-flight method. However, when a tube with a thick wall is used as a collimator the neutron collection efficiency at the detector increases but the time resolution becomes wider because of multiple scattering inside of the tube. In the present study, we have developed a collimation tube made of Ni-0, which is optimized for the increased neutron collection efficiency and a reasonable time resolution. This collimator has been demonstrated experimentally using neutron resonance spectroscopy with neutrons provided from LFEX laser.","sentences":["Recent progress of laser science provides laser-driven neutron source (LDNS), which has remarkable features such as the short pulse width.","One of the key techniques to be developed for more efficient use of the LDNS is neutron collimation tubes to increase the number of neutrons arriving at a detector in the time-of-flight method.","However, when a tube with a thick wall is used as a collimator the neutron collection efficiency at the detector increases but the time resolution becomes wider because of multiple scattering inside of the tube.","In the present study, we have developed a collimation tube made of Ni-0, which is optimized for the increased neutron collection efficiency and a reasonable time resolution.","This collimator has been demonstrated experimentally using neutron resonance spectroscopy with neutrons provided from LFEX laser."],"url":"http://arxiv.org/abs/2403.11528v1","category":"physics.ins-det"}
{"created":"2024-03-18 04:26:18","title":"Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning","abstract":"In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with federated learning. Existing city-scale reconstruction methods typically adopt a centralized approach, which gathers all data in a central server and reconstructs scenes. The approach hampers scalability because it places a heavy load on the server and demands extensive data storage when reconstructing scenes on a scale beyond city-scale. In pursuit of a more scalable 3D reconstruction, we propose a federated learning framework with 3DGS, which is a decentralized framework and can potentially use distributed computational resources across millions of clients. We tailor a distillation-based model update scheme for 3DGS and introduce appearance modeling for handling non-IID data in the scenario of 3D reconstruction with federated learning. We simulate our method on several large-scale benchmarks, and our method demonstrates rendered image quality comparable to centralized approaches. In addition, we also simulate our method with data collected in different seasons, demonstrating that our framework can reflect changes in the scenes and our appearance modeling captures changes due to seasonal variations.","sentences":["In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with federated learning.","Existing city-scale reconstruction methods typically adopt a centralized approach, which gathers all data in a central server and reconstructs scenes.","The approach hampers scalability because it places a heavy load on the server and demands extensive data storage when reconstructing scenes on a scale beyond city-scale.","In pursuit of a more scalable 3D reconstruction, we propose a federated learning framework with 3DGS, which is a decentralized framework and can potentially use distributed computational resources across millions of clients.","We tailor a distillation-based model update scheme for 3DGS and introduce appearance modeling for handling non-IID data in the scenario of 3D reconstruction with federated learning.","We simulate our method on several large-scale benchmarks, and our method demonstrates rendered image quality comparable to centralized approaches.","In addition, we also simulate our method with data collected in different seasons, demonstrating that our framework can reflect changes in the scenes and our appearance modeling captures changes due to seasonal variations."],"url":"http://arxiv.org/abs/2403.11460v1","category":"cs.CV"}
{"created":"2024-03-18 00:05:16","title":"Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces","abstract":"Augmented reality (AR) has the potential to fundamentally change how people engage with increasingly interactive urban environments. However, many challenges exist in designing and evaluating these new urban AR experiences, such as technical constraints and safety concerns associated with outdoor AR. We contribute to this domain by assessing the use of virtual reality (VR) for simulating wearable urban AR experiences, allowing participants to interact with future AR interfaces in a realistic, safe and controlled setting. This paper describes two wearable urban AR applications (pedestrian navigation and autonomous mobility) simulated in VR. Based on a thematic analysis of interview data collected across the two studies, we found that the VR simulation successfully elicited feedback on the functional benefits of AR concepts and the potential impact of urban contextual factors, such as safety concerns, attentional capacity, and social considerations. At the same time, we highlighted the limitations of this approach in terms of assessing the AR interface's visual quality and providing exhaustive contextual information. The paper concludes with recommendations for simulating wearable urban AR experiences in VR.","sentences":["Augmented reality (AR) has the potential to fundamentally change how people engage with increasingly interactive urban environments.","However, many challenges exist in designing and evaluating these new urban AR experiences, such as technical constraints and safety concerns associated with outdoor AR.","We contribute to this domain by assessing the use of virtual reality (VR) for simulating wearable urban AR experiences, allowing participants to interact with future AR interfaces in a realistic, safe and controlled setting.","This paper describes two wearable urban AR applications (pedestrian navigation and autonomous mobility) simulated in VR.","Based on a thematic analysis of interview data collected across the two studies, we found that the VR simulation successfully elicited feedback on the functional benefits of AR concepts and the potential impact of urban contextual factors, such as safety concerns, attentional capacity, and social considerations.","At the same time, we highlighted the limitations of this approach in terms of assessing the AR interface's visual quality and providing exhaustive contextual information.","The paper concludes with recommendations for simulating wearable urban AR experiences in VR."],"url":"http://arxiv.org/abs/2403.11377v1","category":"cs.HC"}
{"created":"2024-03-17 22:49:07","title":"Creating Seamless 3D Maps Using Radiance Fields","abstract":"It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning. The traditional methods of creating 3D maps, (such as photogrammetry), require a large number of images and odometry. Additionally, traditional methods have difficulty with reflective surfaces and specular reflections; windows and chrome in the scene can be problematic. Google Road View is a familiar application, which uses traditional methods to fuse a collection of 2D input images into the illusion of a 3D map. However, Google Road View does not create an actual 3D object model, only a collection of views. The objective of this work is to create an actual 3D object model using updated techniques. Neural Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the capability to produce more precise and intricate 3D maps. Gaussian Splatting[4] is another contemporary technique. This investigation compares Neural Radiance Fields to Gaussian Splatting, and describes some of their inner workings. Our primary contribution is a method for improving the results of the 3D reconstructed models. Our results indicate that Gaussian Splatting was superior to the NeRF technique.","sentences":["It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning.","The traditional methods of creating 3D maps, (such as photogrammetry), require a large number of images and odometry.","Additionally, traditional methods have difficulty with reflective surfaces and specular reflections; windows and chrome in the scene can be problematic.","Google Road View is a familiar application, which uses traditional methods to fuse a collection of 2D input images into the illusion of a 3D map.","However, Google Road View does not create an actual 3D object model, only a collection of views.","The objective of this work is to create an actual 3D object model using updated techniques.","Neural Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the capability to produce more precise and intricate 3D maps.","Gaussian Splatting[4] is another contemporary technique.","This investigation compares Neural Radiance Fields to Gaussian Splatting, and describes some of their inner workings.","Our primary contribution is a method for improving the results of the 3D reconstructed models.","Our results indicate that Gaussian Splatting was superior to the NeRF technique."],"url":"http://arxiv.org/abs/2403.11364v1","category":"cs.CV"}
{"created":"2024-03-17 21:45:22","title":"Observation of the J/$\u03c8$ $\\to$ $\u03bc^+\u03bc^-\u03bc^+\u03bc^-$ decay in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The J/$\\psi$ $\\to$ $\\mu^+\\mu^-\\mu^+\\mu^-$ decay has been observed with a statistical significance in excess of five standard deviations. The analysis is based on an event sample of proton-proton collisions at a center-of-mass energy of 13 TeV, collected by the CMS experiment in 2018 and corresponding to an integrated luminosity of 33.6 fb${-1}$. Normalizing to the J/$\\psi$ $\\to$ $\\mu^+\\mu^-$ decay mode leads to a branching fraction [10.1$^{+3.3}_{-2.7}$ (stat) $\\pm$ 0.4 (syst) ]$\\times$ 10$^{-7}$, a value that is consistent with the standard model prediction.","sentences":["The J/$\\psi$ $\\to$ $\\mu^+\\mu^-\\mu^+\\mu^-$ decay has been observed with a statistical significance in excess of five standard deviations.","The analysis is based on an event sample of proton-proton collisions at a center-of-mass energy of 13 TeV, collected by the CMS experiment in 2018 and corresponding to an integrated luminosity of 33.6 fb${-1}$. Normalizing to the J/$\\psi$ $\\to$ $\\mu^+\\mu^-$ decay mode leads to a branching fraction [10.1$^{+3.3}_{-2.7}$ (stat) $\\pm$ 0.4 (syst) ]$\\times$ 10$^{-7}$, a value that is consistent with the standard model prediction."],"url":"http://arxiv.org/abs/2403.11352v1","category":"hep-ex"}
{"created":"2024-03-17 19:36:18","title":"Multi-Scale Experimental Characterization for LS-DYNA MAT213 Modeling of Composite Structures under High Strain Rate","abstract":"Aerospace structures often experience high strain rate events such as ballistic impact, crash, or crush. A material model has been developed that enhances the capability to simulate the dynamic response of composite materials under these loading conditions. The material model has been implemented into the commercially available transient dynamic finite element code LS-DYNA as MAT213. The model can simulate the nonlinear deformation, damage, and failure that takes place in a composite under dynamic loading conditions. The specific goal of this work is to characterize the MAT213 input for the representative material. The specific composite material being examined consists of T700G unidirectional carbon fibers and a low-melt PolyArylEtherKetone (LMPAEK) thermoplastic resin system. It is formally referred to as Toray TC1225 LMPAEK T700G. As the initial part of this work, this paper is focused on characterizing the material parameters for the MAT213 deformation model based on results obtained from multi-scale experimentation. The effort concentrated on characterizing the in-plane material response suitable for use with thin shell elements. For shell elements within MAT213, tabulated stress-strain results from tension and compression tests in the longitudinal and transverse directions and in-plane shear tests are required. Due to the difficulty of measuring small strains in the transverse direction, a multi-scale testing method was developed. Macro-scale testing is performed per the typical ASTM methods while micro-scale testing uses a microscope along with smaller coupon sizes to obtain the smaller strains in the transverse direction of each test. For both testing methods, a VIC-2D camera and software for digital image correlation analysis are used. Using the DIC combined with each test fixture, reliable stress and strain data are collected.","sentences":["Aerospace structures often experience high strain rate events such as ballistic impact, crash, or crush.","A material model has been developed that enhances the capability to simulate the dynamic response of composite materials under these loading conditions.","The material model has been implemented into the commercially available transient dynamic finite element code LS-DYNA as MAT213.","The model can simulate the nonlinear deformation, damage, and failure that takes place in a composite under dynamic loading conditions.","The specific goal of this work is to characterize the MAT213 input for the representative material.","The specific composite material being examined consists of T700G unidirectional carbon fibers and a low-melt PolyArylEtherKetone (LMPAEK) thermoplastic resin system.","It is formally referred to as Toray TC1225 LMPAEK T700G.","As the initial part of this work, this paper is focused on characterizing the material parameters for the MAT213 deformation model based on results obtained from multi-scale experimentation.","The effort concentrated on characterizing the in-plane material response suitable for use with thin shell elements.","For shell elements within MAT213, tabulated stress-strain results from tension and compression tests in the longitudinal and transverse directions and in-plane shear tests are required.","Due to the difficulty of measuring small strains in the transverse direction, a multi-scale testing method was developed.","Macro-scale testing is performed per the typical ASTM methods while micro-scale testing uses a microscope along with smaller coupon sizes to obtain the smaller strains in the transverse direction of each test.","For both testing methods, a VIC-2D camera and software for digital image correlation analysis are used.","Using the DIC combined with each test fixture, reliable stress and strain data are collected."],"url":"http://arxiv.org/abs/2403.11316v1","category":"physics.app-ph"}
{"created":"2024-03-17 18:09:15","title":"Accelerating Gradient Tracking with Periodic Global Averaging","abstract":"Decentralized optimization algorithms have recently attracted increasing attention due to its wide applications in all areas of science and engineering. In these algorithms, a collection of agents collaborate to minimize the average of a set of heterogeneous cost functions in a decentralized manner. State-of-the-art decentralized algorithms like Gradient Tracking (GT) and Exact Diffusion (ED) involve communication at each iteration. Yet, communication between agents is often expensive, resource intensive, and can be very slow. To this end, several strategies have been developed to balance between communication overhead and convergence rate of decentralized methods. In this paper, we introduce GT-PGA, which incorporates~GT with periodic global averaging. With the additional PGA, the influence of poor network connectivity in the GT algorithm can be compensated or controlled by a careful selection of the global averaging period. Under the stochastic, nonconvex setup, our analysis quantifies the crucial trade-off between the connectivity of network topology and the PGA period. Thus, with a suitable design of the PGA period, GT-PGA improves the convergence rate of vanilla GT. Numerical experiments are conducted to support our theory, and simulation results reveal that the proposed GT-PGA accelerates practical convergence, especially when the network is sparse.","sentences":["Decentralized optimization algorithms have recently attracted increasing attention due to its wide applications in all areas of science and engineering.","In these algorithms, a collection of agents collaborate to minimize the average of a set of heterogeneous cost functions in a decentralized manner.","State-of-the-art decentralized algorithms like Gradient Tracking (GT) and Exact Diffusion (ED) involve communication at each iteration.","Yet, communication between agents is often expensive, resource intensive, and can be very slow.","To this end, several strategies have been developed to balance between communication overhead and convergence rate of decentralized methods.","In this paper, we introduce GT-PGA, which incorporates~GT with periodic global averaging.","With the additional PGA, the influence of poor network connectivity in the GT algorithm can be compensated or controlled by a careful selection of the global averaging period.","Under the stochastic, nonconvex setup, our analysis quantifies the crucial trade-off between the connectivity of network topology and the PGA period.","Thus, with a suitable design of the PGA period, GT-PGA improves the convergence rate of vanilla GT.","Numerical experiments are conducted to support our theory, and simulation results reveal that the proposed GT-PGA accelerates practical convergence, especially when the network is sparse."],"url":"http://arxiv.org/abs/2403.11293v1","category":"math.OC"}
{"created":"2024-03-17 16:24:07","title":"A Lie Group Approach to Riemannian Batch Normalization","abstract":"Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/LieBN.git.","sentences":["Manifold-valued measurements exist in numerous applications within computer vision and machine learning.","Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization.","Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds.","This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups.","Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance.","Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures.","Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups.","Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks.","We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification.","The code is available at https://github.com/GitZH-Chen/LieBN.git."],"url":"http://arxiv.org/abs/2403.11261v1","category":"cs.LG"}
{"created":"2024-03-17 16:23:09","title":"Principles and Optimization of Reflective Intelligent Surface Assisted mmWave Systems","abstract":"A conceptual example is first analyzed to show that efficient wireless communications is possible, when user equipment (UE) receiver, BS transmitter or/and the scatter (reflector) in wireless channels employ the required channel state information (CSI) to remove the randomness of signal phase. Then, the principles and optimization of three reflective intelligent surface (RIS) assisted mmWave (RIS-mmWave) models are introduced. The first model assumes one BS, one RIS and one UE; the second one assumes one BS, one RIS and multiple UEs; while the third RIS-mmWave model assumes one BS, multiple RISs and multiple UEs. Furthermore, the optimization of BS precoder and RIS phase-shifts is addressed in the context of the massive RIS-mmWave scenarios, where the number of BS antennas and that of RIS reflection elements are significantly larger than the number of supported UEs. The analyses demonstrate that, while the deployment of RISs with mmWave is capable of solving the blockage problem and has the potential to significantly improve efficiency, finding the near-optimum solutions for RIS phase-shifts is highly challenging in practice.","sentences":["A conceptual example is first analyzed to show that efficient wireless communications is possible, when user equipment (UE) receiver, BS transmitter or/and the scatter (reflector) in wireless channels employ the required channel state information (CSI) to remove the randomness of signal phase.","Then, the principles and optimization of three reflective intelligent surface (RIS) assisted mmWave (RIS-mmWave) models are introduced.","The first model assumes one BS, one RIS and one UE; the second one assumes one BS, one RIS and multiple UEs; while the third RIS-mmWave model assumes one BS, multiple RISs and multiple UEs.","Furthermore, the optimization of BS precoder and RIS phase-shifts is addressed in the context of the massive RIS-mmWave scenarios, where the number of BS antennas and that of RIS reflection elements are significantly larger than the number of supported UEs.","The analyses demonstrate that, while the deployment of RISs with mmWave is capable of solving the blockage problem and has the potential to significantly improve efficiency, finding the near-optimum solutions for RIS phase-shifts is highly challenging in practice."],"url":"http://arxiv.org/abs/2403.11260v1","category":"eess.SP"}
{"created":"2024-03-17 16:23:00","title":"A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty","abstract":"Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and their request rates, machine learning models generate decision variables for the first stage of the stochastic optimization model, which is the user-to-server request allocation, and are employed as independent decision agents that reliably mimic the optimization model. Support Vector Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to achieve practical decisions from the stochastic optimization models. The performance of each model has shown an execution effectiveness of over 80%. This research aims to provide a more efficient approach for tackling high-dimensional problems and scenarios with uncertainties in mobile edge computing by leveraging machine learning models for optimal decision-making in request allocation to edge servers. These results suggest that machine-learning models can significantly improve solution times compared to conventional approaches.","sentences":["Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests.","Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios.","Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints.","One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers.","Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers.","In this study, the problem is formulated as a two-stage stochastic programming.","A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model.","Then, based on the distance features of each user from the available servers and their request rates, machine learning models generate decision variables for the first stage of the stochastic optimization model, which is the user-to-server request allocation, and are employed as independent decision agents that reliably mimic the optimization model.","Support Vector Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to achieve practical decisions from the stochastic optimization models.","The performance of each model has shown an execution effectiveness of over 80%.","This research aims to provide a more efficient approach for tackling high-dimensional problems and scenarios with uncertainties in mobile edge computing by leveraging machine learning models for optimal decision-making in request allocation to edge servers.","These results suggest that machine-learning models can significantly improve solution times compared to conventional approaches."],"url":"http://arxiv.org/abs/2403.11259v1","category":"cs.LG"}
{"created":"2024-03-17 16:08:30","title":"Efficiently Detecting Reentrancy Vulnerabilities in Complex Smart Contracts","abstract":"Reentrancy vulnerability as one of the most notorious vulnerabilities, has been a prominent topic in smart contract security research. Research shows that existing vulnerability detection presents a range of challenges, especially as smart contracts continue to increase in complexity. Existing tools perform poorly in terms of efficiency and successful detection rates for vulnerabilities in complex contracts. To effectively detect reentrancy vulnerabilities in contracts with complex logic, we propose a tool named SliSE. SliSE's detection process consists of two stages: Warning Search and Symbolic Execution Verification. In Stage I, SliSE utilizes program slicing to analyze the Inter-contract Program Dependency Graph (I-PDG) of the contract, and collects suspicious vulnerability information as warnings. In Stage II, symbolic execution is employed to verify the reachability of these warnings, thereby enhancing vulnerability detection accuracy. SliSE obtained the best performance compared with eight state-of-the-art detection tools. It achieved an F1 score of 78.65%, surpassing the highest score recorded by an existing tool of 9.26%. Additionally, it attained a recall rate exceeding 90% for detection of contracts on Ethereum. Overall, SliSE provides a robust and efficient method for detection of Reentrancy vulnerabilities for complex contracts.","sentences":["Reentrancy vulnerability as one of the most notorious vulnerabilities, has been a prominent topic in smart contract security research.","Research shows that existing vulnerability detection presents a range of challenges, especially as smart contracts continue to increase in complexity.","Existing tools perform poorly in terms of efficiency and successful detection rates for vulnerabilities in complex contracts.","To effectively detect reentrancy vulnerabilities in contracts with complex logic, we propose a tool named SliSE.","SliSE's detection process consists of two stages:","Warning Search and Symbolic Execution Verification.","In Stage I, SliSE utilizes program slicing to analyze the Inter-contract Program Dependency Graph (I-PDG) of the contract, and collects suspicious vulnerability information as warnings.","In Stage II, symbolic execution is employed to verify the reachability of these warnings, thereby enhancing vulnerability detection accuracy.","SliSE obtained the best performance compared with eight state-of-the-art detection tools.","It achieved an F1 score of 78.65%, surpassing the highest score recorded by an existing tool of 9.26%.","Additionally, it attained a recall rate exceeding 90% for detection of contracts on Ethereum.","Overall, SliSE provides a robust and efficient method for detection of Reentrancy vulnerabilities for complex contracts."],"url":"http://arxiv.org/abs/2403.11254v1","category":"cs.SE"}
{"created":"2024-03-17 15:59:41","title":"Zutu: A Platform for Localization and Navigation of Swarm Robots Using Virtual Grids","abstract":"Swarm robots, which are inspired from the way insects behave collectively in order to achieve a common goal, have become a major part of research with applications involving search and rescue, area exploration, surveillance etc. In this paper, we present a swarm of robots that do not require individual extrinsic sensors to sense the environment but instead use a single central camera to locate and map the swarm. The robots can be easily built using readily available components with the main chassis being 3D printed, making the system low-cost, low-maintenance, and easy to replicate. We describe Zutu's hardware and software architecture, the algorithms to map the robots to the real world, and some experiments conducted using four of our robots. Eventually, we conclude the possible applications of our system in research, education, and industries.","sentences":["Swarm robots, which are inspired from the way insects behave collectively in order to achieve a common goal, have become a major part of research with applications involving search and rescue, area exploration, surveillance etc.","In this paper, we present a swarm of robots that do not require individual extrinsic sensors to sense the environment but instead use a single central camera to locate and map the swarm.","The robots can be easily built using readily available components with the main chassis being 3D printed, making the system low-cost, low-maintenance, and easy to replicate.","We describe Zutu's hardware and software architecture, the algorithms to map the robots to the real world, and some experiments conducted using four of our robots.","Eventually, we conclude the possible applications of our system in research, education, and industries."],"url":"http://arxiv.org/abs/2403.11252v1","category":"cs.RO"}
{"created":"2024-03-17 13:43:10","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations","abstract":"Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-Enhancer not only sets the new state of the art for object detection but also boosts the performance of other downstream vision tasks under unknown degradations.","sentences":["Object detection methods under known single degradations have been extensively investigated.","However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments.","To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations.","Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information.","To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks.","Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly.","Experimental results demonstrate that CPA-Enhancer not only sets the new state of the art for object detection but also boosts the performance of other downstream vision tasks under unknown degradations."],"url":"http://arxiv.org/abs/2403.11220v1","category":"cs.CV"}
{"created":"2024-03-17 13:39:43","title":"Causality from Bottom to Top: A Survey","abstract":"Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exemplifies the trustworthiness and explainability of causality models. We offer several ways to evaluate causality models and discuss future directions.","sentences":["Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study.","It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising.","In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it.","Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic.","We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches.","Additionally, the paper exemplifies the trustworthiness and explainability of causality models.","We offer several ways to evaluate causality models and discuss future directions."],"url":"http://arxiv.org/abs/2403.11219v1","category":"cs.AI"}
{"created":"2024-03-17 13:34:45","title":"Research on Personal Credit Risk Assessment Methods Based on Causal Inference","abstract":"The discussion on causality in human history dates back to ancient Greece, yet to this day, there is still no consensus. Fundamentally, this stems from the nature of human cognition, as understanding causality requires abstract tools to transcend the limitations of human cognition. In recent decades, the rapid development of mathematical and computational tools has provided new theoretical and technical means for exploring causality, creating more avenues for investigation.   Based on this, this paper introduces a new definition of causality using category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to avoid the self-referential contradictions in set theory, notably the Russell paradox. Within this framework, the feasibility of indicator synthesis in causal inference is demonstrated. Due to the limitations in the development of category theory-related technical tools, this paper adopts the widely-used probabilistic causal graph tool proposed by Judea Pearl in 1995 to study the application of causal inference in personal credit risk management. The specific work includes: research on the construction method of causal inference index system, definition of causality and feasibility proof of indicator synthesis causal inference within this framework, application methods of causal graph model and intervention alternative criteria in personal credit risk management, and so on.","sentences":["The discussion on causality in human history dates back to ancient Greece, yet to this day, there is still no consensus.","Fundamentally, this stems from the nature of human cognition, as understanding causality requires abstract tools to transcend the limitations of human cognition.","In recent decades, the rapid development of mathematical and computational tools has provided new theoretical and technical means for exploring causality, creating more avenues for investigation.   ","Based on this, this paper introduces a new definition of causality using category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to avoid the self-referential contradictions in set theory, notably the Russell paradox.","Within this framework, the feasibility of indicator synthesis in causal inference is demonstrated.","Due to the limitations in the development of category theory-related technical tools, this paper adopts the widely-used probabilistic causal graph tool proposed by Judea Pearl in 1995 to study the application of causal inference in personal credit risk management.","The specific work includes: research on the construction method of causal inference index system, definition of causality and feasibility proof of indicator synthesis causal inference within this framework, application methods of causal graph model and intervention alternative criteria in personal credit risk management, and so on."],"url":"http://arxiv.org/abs/2403.11217v1","category":"cs.AI"}
{"created":"2024-03-17 13:23:25","title":"RCdpia: A Renal Carcinoma Digital Pathology Image Annotation dataset based on pathologists","abstract":"The annotation of digital pathological slide data for renal cell carcinoma is of paramount importance for correct diagnosis of artificial intelligence models due to the heterogeneous nature of the tumor. This process not only facilitates a deeper understanding of renal cell cancer heterogeneity but also aims to minimize noise in the data for more accurate studies. To enhance the applicability of the data, two pathologists were enlisted to meticulously curate, screen, and label a kidney cancer pathology image dataset from The Cancer Genome Atlas Program (TCGA) database. Subsequently, a Resnet model was developed to validate the annotated dataset against an additional dataset from the First Affiliated Hospital of Zhejiang University. Based on these results, we have meticulously compiled the TCGA digital pathological dataset with independent labeling of tumor regions and adjacent areas (RCdpia), which includes 109 cases of kidney chromophobe cell carcinoma, 486 cases of kidney clear cell carcinoma, and 292 cases of kidney papillary cell carcinoma. This dataset is now publicly accessible at http://39.171.241.18:8888/RCdpia/. Furthermore, model analysis has revealed significant discrepancies in predictive outcomes when applying the same model to datasets from different centers. Leveraging the RCdpia, we can now develop more precise digital pathology artificial intelligence models for tasks such as normalization, classification, and segmentation. These advancements underscore the potential for more nuanced and accurate AI applications in the field of digital pathology.","sentences":["The annotation of digital pathological slide data for renal cell carcinoma is of paramount importance for correct diagnosis of artificial intelligence models due to the heterogeneous nature of the tumor.","This process not only facilitates a deeper understanding of renal cell cancer heterogeneity but also aims to minimize noise in the data for more accurate studies.","To enhance the applicability of the data, two pathologists were enlisted to meticulously curate, screen, and label a kidney cancer pathology image dataset from The Cancer Genome Atlas Program (TCGA) database.","Subsequently, a Resnet model was developed to validate the annotated dataset against an additional dataset from the First Affiliated Hospital of Zhejiang University.","Based on these results, we have meticulously compiled the TCGA digital pathological dataset with independent labeling of tumor regions and adjacent areas (RCdpia), which includes 109 cases of kidney chromophobe cell carcinoma, 486 cases of kidney clear cell carcinoma, and 292 cases of kidney papillary cell carcinoma.","This dataset is now publicly accessible at http://39.171.241.18:8888/RCdpia/.","Furthermore, model analysis has revealed significant discrepancies in predictive outcomes when applying the same model to datasets from different centers.","Leveraging the RCdpia, we can now develop more precise digital pathology artificial intelligence models for tasks such as normalization, classification, and segmentation.","These advancements underscore the potential for more nuanced and accurate AI applications in the field of digital pathology."],"url":"http://arxiv.org/abs/2403.11211v1","category":"cs.CV"}
{"created":"2024-03-17 13:15:22","title":"MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data","abstract":"Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.","sentences":["Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited.","This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results.","The present work showcases high-quality reconstructions using only 1 hour of fMRI training data.","We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject.","Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space.","We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text.","This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches.","MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility.","All code is available on GitHub."],"url":"http://arxiv.org/abs/2403.11207v1","category":"cs.CV"}
{"created":"2024-03-17 13:06:29","title":"Partitioned Neural Network Training via Synthetic Intermediate Labels","abstract":"The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate memory overhead and computational load. This approach results in a more efficient training process that minimizes data communication while maintaining model accuracy. To validate this method, a 6-layer fully connected neural network is partitioned into two parts and its performance is assessed on the extended MNIST dataset. Experimental results indicate that the proposed approach achieves similar testing accuracies to conventional training methods, while significantly reducing memory and computational requirements. This work contributes to mitigating the resource-intensive nature of training large neural networks, paving the way for more efficient deep learning model development.","sentences":["The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training.","GPU memory constraints have become a notable bottleneck in training such sizable models.","Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions.","Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training.","Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands.","Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments.","These labels, produced through a random process, mitigate memory overhead and computational load.","This approach results in a more efficient training process that minimizes data communication while maintaining model accuracy.","To validate this method, a 6-layer fully connected neural network is partitioned into two parts and its performance is assessed on the extended MNIST dataset.","Experimental results indicate that the proposed approach achieves similar testing accuracies to conventional training methods, while significantly reducing memory and computational requirements.","This work contributes to mitigating the resource-intensive nature of training large neural networks, paving the way for more efficient deep learning model development."],"url":"http://arxiv.org/abs/2403.11204v1","category":"cs.LG"}
{"created":"2024-03-17 13:01:03","title":"Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework","abstract":"Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script. To evaluate the effectiveness of our data augmentation method, we finetune Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.","sentences":["Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts.","Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design.","However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs.","Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers.","This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts.","For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template.","For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file.","For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script.","To evaluate the effectiveness of our data augmentation method, we finetune Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework.","The results demonstrate a significant improvement in the Verilog generation tasks with LLMs.","Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark.","Our 13B model (ChipGPT-FT) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data."],"url":"http://arxiv.org/abs/2403.11202v1","category":"cs.AR"}
{"created":"2024-03-17 12:55:23","title":"Graph Unitary Message Passing","abstract":"Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on various graph learning tasks.","sentences":["Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem.","Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure.","Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing.","To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias.","Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant.","Experimental results show the effectiveness of GUMP in improving the performance on various graph learning tasks."],"url":"http://arxiv.org/abs/2403.11199v1","category":"cs.LG"}
{"created":"2024-03-17 12:50:05","title":"Learning-Based Wiping Behavior of Low-Rigidity Robots Considering Various Surface Materials and Task Definitions","abstract":"Wiping behavior is a task of tracing the surface of an object while feeling the force with the palm of the hand. It is necessary to adjust the force and posture appropriately considering the various contact conditions felt by the hand. Several studies have been conducted on the wiping motion, however, these studies have only dealt with a single surface material, and have only considered the application of the amount of appropriate force, lacking intelligent movements to ensure that the force is applied either evenly to the entire surface or to a certain area. Depending on the surface material, the hand posture and pressing force should be varied appropriately, and this is highly dependent on the definition of the task. Also, most of the movements are executed by high-rigidity robots that are easy to model, and few movements are executed by robots that are low-rigidity but therefore have a small risk of damage due to excessive contact. So, in this study, we develop a method of motion generation based on the learned prediction of contact force during the wiping motion of a low-rigidity robot. We show that MyCobot, which is made of low-rigidity resin, can appropriately perform wiping behaviors on a plane with multiple surface materials based on various task definitions.","sentences":["Wiping behavior is a task of tracing the surface of an object while feeling the force with the palm of the hand.","It is necessary to adjust the force and posture appropriately considering the various contact conditions felt by the hand.","Several studies have been conducted on the wiping motion, however, these studies have only dealt with a single surface material, and have only considered the application of the amount of appropriate force, lacking intelligent movements to ensure that the force is applied either evenly to the entire surface or to a certain area.","Depending on the surface material, the hand posture and pressing force should be varied appropriately, and this is highly dependent on the definition of the task.","Also, most of the movements are executed by high-rigidity robots that are easy to model, and few movements are executed by robots that are low-rigidity but therefore have a small risk of damage due to excessive contact.","So, in this study, we develop a method of motion generation based on the learned prediction of contact force during the wiping motion of a low-rigidity robot.","We show that MyCobot, which is made of low-rigidity resin, can appropriately perform wiping behaviors on a plane with multiple surface materials based on various task definitions."],"url":"http://arxiv.org/abs/2403.11198v1","category":"cs.RO"}
{"created":"2024-03-17 12:38:58","title":"Self-Supervised Video Desmoking for Laparoscopic Surgery","abstract":"Due to the difficulty of collecting real paired data, most existing desmoking methods train the models by synthesizing smoke, generalizing poorly to real surgical scenarios. Although a few works have explored single-image real-world desmoking in unpaired learning manners, they still encounter challenges in handling dense smoke. In this work, we address these issues together by introducing the self-supervised surgery video desmoking (SelfSVD). On the one hand, we observe that the frame captured before the activation of high-energy devices is generally clear (named pre-smoke frame, PS frame), thus it can serve as supervision for other smoky frames, making real-world self-supervised video desmoking practically feasible. On the other hand, in order to enhance the desmoking performance, we further feed the valuable information from PS frame into models, where a masking strategy and a regularization term are presented to avoid trivial solutions. In addition, we construct a real surgery video dataset for desmoking, which covers a variety of smoky scenes. Extensive experiments on the dataset show that our SelfSVD can remove smoke more effectively and efficiently while recovering more photo-realistic details than the state-of-the-art methods. The dataset, codes, and pre-trained models are available at \\url{https://github.com/ZcsrenlongZ/SelfSVD}.","sentences":["Due to the difficulty of collecting real paired data, most existing desmoking methods train the models by synthesizing smoke, generalizing poorly to real surgical scenarios.","Although a few works have explored single-image real-world desmoking in unpaired learning manners, they still encounter challenges in handling dense smoke.","In this work, we address these issues together by introducing the self-supervised surgery video desmoking (SelfSVD).","On the one hand, we observe that the frame captured before the activation of high-energy devices is generally clear (named pre-smoke frame, PS frame), thus it can serve as supervision for other smoky frames, making real-world self-supervised video desmoking practically feasible.","On the other hand, in order to enhance the desmoking performance, we further feed the valuable information from PS frame into models, where a masking strategy and a regularization term are presented to avoid trivial solutions.","In addition, we construct a real surgery video dataset for desmoking, which covers a variety of smoky scenes.","Extensive experiments on the dataset show that our SelfSVD can remove smoke more effectively and efficiently while recovering more photo-realistic details than the state-of-the-art methods.","The dataset, codes, and pre-trained models are available at \\url{https://github.com/ZcsrenlongZ/SelfSVD}."],"url":"http://arxiv.org/abs/2403.11192v1","category":"cs.CV"}
{"created":"2024-03-17 12:12:33","title":"Decoding Continuous Character-based Language from Non-invasive Brain Recordings","abstract":"Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing decoders exhibit significantly inferior performance in cross-subject contexts. The ability to decode continuous language from single trials across subjects demonstrates the promising applications of non-invasive language brain-computer interfaces in both healthcare and neuroscience.","sentences":["Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge.","Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words.","We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures.","The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing decoders exhibit significantly inferior performance in cross-subject contexts.","The ability to decode continuous language from single trials across subjects demonstrates the promising applications of non-invasive language brain-computer interfaces in both healthcare and neuroscience."],"url":"http://arxiv.org/abs/2403.11183v1","category":"cs.CL"}
{"created":"2024-03-17 11:49:57","title":"usfAD Based Effective Unknown Attack Detection Focused IDS Framework","abstract":"The rapid expansion of varied network systems, including the Internet of Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing range of cyber threats. Ensuring robust protection against these threats necessitates the implementation of an effective Intrusion Detection System (IDS). For more than a decade, researchers have delved into supervised machine learning techniques to develop IDS to classify normal and attack traffic. However, building effective IDS models using supervised learning requires a substantial number of benign and attack samples. To collect a sufficient number of attack samples from real-life scenarios is not possible since cyber attacks occur occasionally. Further, IDS trained and tested on known datasets fails in detecting zero-day or unknown attacks due to the swift evolution of attack patterns. To address this challenge, we put forth two strategies for semi-supervised learning based IDS where training samples of attacks are not required: 1) training a supervised machine learning model using randomly and uniformly dispersed synthetic attack samples; 2) building a One Class Classification (OCC) model that is trained exclusively on benign network traffic. We have implemented both approaches and compared their performances using 10 recent benchmark IDS datasets. Our findings demonstrate that the OCC model based on the state-of-art anomaly detection technique called usfAD significantly outperforms conventional supervised classification and other OCC based techniques when trained and tested considering real-life scenarios, particularly to detect previously unseen attacks.","sentences":["The rapid expansion of varied network systems, including the Internet of Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing range of cyber threats.","Ensuring robust protection against these threats necessitates the implementation of an effective Intrusion Detection System (IDS).","For more than a decade, researchers have delved into supervised machine learning techniques to develop IDS to classify normal and attack traffic.","However, building effective IDS models using supervised learning requires a substantial number of benign and attack samples.","To collect a sufficient number of attack samples from real-life scenarios is not possible since cyber attacks occur occasionally.","Further, IDS trained and tested on known datasets fails in detecting zero-day or unknown attacks due to the swift evolution of attack patterns.","To address this challenge, we put forth two strategies for semi-supervised learning based IDS where training samples of attacks are not required: 1) training a supervised machine learning model using randomly and uniformly dispersed synthetic attack samples; 2) building a One Class Classification (OCC) model that is trained exclusively on benign network traffic.","We have implemented both approaches and compared their performances using 10 recent benchmark IDS datasets.","Our findings demonstrate that the OCC model based on the state-of-art anomaly detection technique called usfAD significantly outperforms conventional supervised classification and other OCC based techniques when trained and tested considering real-life scenarios, particularly to detect previously unseen attacks."],"url":"http://arxiv.org/abs/2403.11180v1","category":"cs.CR"}
{"created":"2024-03-17 11:23:51","title":"Prior-dependent analysis of posterior sampling reinforcement learning with function approximation","abstract":"This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to formalize Bayesian regret bounds more effectively.","sentences":["This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs.","We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions.","This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs.","Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to formalize Bayesian regret bounds more effectively."],"url":"http://arxiv.org/abs/2403.11175v1","category":"stat.ML"}
{"created":"2024-03-17 10:59:09","title":"Correcting misinformation on social media with a large language model","abstract":"Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes visuals and conducts multimodal searches for correcting multimodal misinformation. We recruit fact-checking and journalism experts to evaluate corrections to real social media posts across 13 dimensions, ranging from the factuality of explanation to the relevance of references. The results demonstrate MUSE's ability to correct misinformation promptly after appearing on social media; overall, MUSE outperforms GPT-4 by 37% and even high-quality corrections from laypeople by 29%. This work underscores the potential of LLMs to combat real-world misinformation effectively and efficiently.","sentences":["Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly.","Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies.","Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce.","LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information.","To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information.","By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references.","It also describes visuals and conducts multimodal searches for correcting multimodal misinformation.","We recruit fact-checking and journalism experts to evaluate corrections to real social media posts across 13 dimensions, ranging from the factuality of explanation to the relevance of references.","The results demonstrate MUSE's ability to correct misinformation promptly after appearing on social media; overall, MUSE outperforms GPT-4 by 37% and even high-quality corrections from laypeople by 29%.","This work underscores the potential of LLMs to combat real-world misinformation effectively and efficiently."],"url":"http://arxiv.org/abs/2403.11169v1","category":"cs.CL"}
{"created":"2024-03-17 10:06:38","title":"CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion","abstract":"Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.","sentences":["Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object.","Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process.","In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication.","Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models.","We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD).","The similarity between original and recovered images serves as a strong indicator of potential infringements.","Extensive experiments on the WikiArt and Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques.","Code implementation is available at https://github.com/Nicholas0228/Revelio."],"url":"http://arxiv.org/abs/2403.11162v1","category":"cs.CV"}
{"created":"2024-03-17 09:05:13","title":"Evaluation Ethics of LLMs in Legal Domain","abstract":"In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the suitability and performance of large language models in legal domains.","sentences":["In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains.","However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny.","The incorporation of legal ethics into the model has been overlooked by researchers.","We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic.","To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs).","The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the suitability and performance of large language models in legal domains."],"url":"http://arxiv.org/abs/2403.11152v1","category":"cs.CL"}
{"created":"2024-03-17 07:08:55","title":"Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment","abstract":"Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after fine-tuning. We also leverage it on data augmentation and conduct experiments to show its effect on different algorithms.","sentences":["Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback.","Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled.","Nonetheless, a straightforward comparison between their impact is absent.","In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence.","We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment.","Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits.","Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after fine-tuning.","We also leverage it on data augmentation and conduct experiments to show its effect on different algorithms."],"url":"http://arxiv.org/abs/2403.11124v1","category":"cs.CL"}
{"created":"2024-03-17 06:56:42","title":"Secrecy Performance Analysis of RIS Assisted Ambient Backscatter Communication Networks","abstract":"Reconfigurable intelligent surface (RIS) and ambient backscatter communication (AmBC) have been envisioned as two promising technologies due to their high transmission reliability as well as energy-efficiency. This paper investigates the secrecy performance of RIS assisted AmBC networks. New closed-form and asymptotic expressions of secrecy outage probability for RIS-AmBC networks are derived by taking into account both imperfect successive interference cancellation (ipSIC) and perfect SIC (pSIC) cases. On top of these, the secrecy diversity order of legitimate user is obtained in high signal-to-noise ratio region, which equals \\emph{zero} and is proportional to the number of RIS elements for ipSIC and pSIC, respectively. The secrecy throughput and energy efficiency are further surveyed to evaluate the secure effectiveness of RIS-AmBC networks. Numerical results are provided to verify the accuracy of theoretical analyses and manifest that: i) The secrecy outage behavior of RIS-AmBC networks exceeds that of conventional AmBC networks; ii) Due to the mutual interference between direct and backscattering links, the number of RIS elements has an optimal value to minimise the secrecy system outage probability; and iii) Secrecy throughput and energy efficiency are strongly influenced by the reflecting coefficient and eavesdropper's wiretapping ability.","sentences":["Reconfigurable intelligent surface (RIS) and ambient backscatter communication (AmBC) have been envisioned as two promising technologies due to their high transmission reliability as well as energy-efficiency.","This paper investigates the secrecy performance of RIS assisted AmBC networks.","New closed-form and asymptotic expressions of secrecy outage probability for RIS-AmBC networks are derived by taking into account both imperfect successive interference cancellation (ipSIC) and perfect SIC (pSIC) cases.","On top of these, the secrecy diversity order of legitimate user is obtained in high signal-to-noise ratio region, which equals \\emph{zero} and is proportional to the number of RIS elements for ipSIC and pSIC, respectively.","The secrecy throughput and energy efficiency are further surveyed to evaluate the secure effectiveness of RIS-AmBC networks.","Numerical results are provided to verify the accuracy of theoretical analyses and manifest that: i)","The secrecy outage behavior of RIS-AmBC networks exceeds that of conventional AmBC networks; ii) Due to the mutual interference between direct and backscattering links, the number of RIS elements has an optimal value to minimise the secrecy system outage probability; and iii) Secrecy throughput and energy efficiency are strongly influenced by the reflecting coefficient and eavesdropper's wiretapping ability."],"url":"http://arxiv.org/abs/2403.11117v1","category":"cs.IT"}
{"created":"2024-03-17 06:53:44","title":"PhD: A Prompted Visual Hallucination Evaluation Dataset","abstract":"The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions between the LVLM knowledge and actual images. Based on these taxonomies, we propose a more challenging benchmark named PhD to evaluate and explore IVL-Hallu. An automated pipeline is proposed for generating different types of IVL-Hallu data. Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM. The benchmark can be accessed at \\href{https://github.com/jiazhen-code/IntrinsicHallu}{this https URL}.","sentences":["The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs).","The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs.","However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations.","In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections.","Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions between the LVLM knowledge and actual images.","Based on these taxonomies, we propose a more challenging benchmark named PhD to evaluate and explore IVL-Hallu.","An automated pipeline is proposed for generating different types of IVL-Hallu data.","Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM.","The benchmark can be accessed at \\href{https://github.com/jiazhen-code/IntrinsicHallu}{this https URL}."],"url":"http://arxiv.org/abs/2403.11116v1","category":"cs.CV"}
{"created":"2024-03-17 06:41:09","title":"Phasic Diversity Optimization for Population-Based Reinforcement Learning","abstract":"Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation. Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm. We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo simulations. The results show that our proposed algorithm achieves better performance than baselines.","sentences":["Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.","Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space.","However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods.","We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function.","In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive.","The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation.","Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm.","We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo simulations.","The results show that our proposed algorithm achieves better performance than baselines."],"url":"http://arxiv.org/abs/2403.11114v1","category":"cs.LG"}
{"created":"2024-03-17 06:31:16","title":"3D Human Reconstruction in the Wild with Synthetic Data Using Generative Models","abstract":"In this work, we show that synthetic data created by generative models is complementary to computer graphics (CG) rendered data for achieving remarkable generalization performance on diverse real-world scenes for 3D human pose and shape estimation (HPS). Specifically, we propose an effective approach based on recent diffusion models, termed HumanWild, which can effortlessly generate human images and corresponding 3D mesh annotations. We first collect a large-scale human-centric dataset with comprehensive annotations, e.g., text captions and surface normal images. Then, we train a customized ControlNet model upon this dataset to generate diverse human images and initial ground-truth labels. At the core of this step is that we can easily obtain numerous surface normal images from a 3D human parametric model, e.g., SMPL-X, by rendering the 3D mesh onto the image plane. As there exists inevitable noise in the initial labels, we then apply an off-the-shelf foundation segmentation model, i.e., SAM, to filter negative data samples. Our data generation pipeline is flexible and customizable to facilitate different real-world tasks, e.g., ego-centric scenes and perspective-distortion scenes. The generated dataset comprises 0.79M images with corresponding 3D annotations, covering versatile viewpoints, scenes, and human identities. We train various HPS regressors on top of the generated data and evaluate them on a wide range of benchmarks (3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of the generated data. By exclusively employing generative models, we generate large-scale in-the-wild human images and high-quality annotations, eliminating the need for real-world data collection.","sentences":["In this work, we show that synthetic data created by generative models is complementary to computer graphics (CG) rendered data for achieving remarkable generalization performance on diverse real-world scenes for 3D human pose and shape estimation (HPS).","Specifically, we propose an effective approach based on recent diffusion models, termed HumanWild, which can effortlessly generate human images and corresponding 3D mesh annotations.","We first collect a large-scale human-centric dataset with comprehensive annotations, e.g., text captions and surface normal images.","Then, we train a customized ControlNet model upon this dataset to generate diverse human images and initial ground-truth labels.","At the core of this step is that we can easily obtain numerous surface normal images from a 3D human parametric model, e.g., SMPL-X, by rendering the 3D mesh onto the image plane.","As there exists inevitable noise in the initial labels, we then apply an off-the-shelf foundation segmentation model, i.e., SAM, to filter negative data samples.","Our data generation pipeline is flexible and customizable to facilitate different real-world tasks, e.g., ego-centric scenes and perspective-distortion scenes.","The generated dataset comprises 0.79M images with corresponding 3D annotations, covering versatile viewpoints, scenes, and human identities.","We train various HPS regressors on top of the generated data and evaluate them on a wide range of benchmarks (3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of the generated data.","By exclusively employing generative models, we generate large-scale in-the-wild human images and high-quality annotations, eliminating the need for real-world data collection."],"url":"http://arxiv.org/abs/2403.11111v1","category":"cs.CV"}
{"created":"2024-03-17 06:28:50","title":"Secure Communication of Active RIS Assisted NOMA Networks","abstract":"As a revolutionary technology, reconfigurable intelligent surface (RIS) has been deemed as an indispensable part of the 6th generation communications due to its inherent ability to regulate the wireless channels. However, passive RIS (PRIS) still suffers from some pressing issues, one of which is that the fading of the entire reflection link is proportional to the product of the distances from the base station to the PRIS and from the PRIS to the users, i.e., the productive attenuation. To tackle this problem, active RIS (ARIS) has been proposed to reconfigure the wireless propagation condition and alleviate the productive attenuation. In this paper, we investigate the physical layer security of the ARIS assisted non-orthogonal multiple access (NOMA) networks with the attendance of external and internal eavesdroppers. To be specific, the closed-form expressions of secrecy outage probability (SOP) and secrecy system throughput are derived by invoking both imperfect successive interference cancellation (ipSIC) and perfect SIC. The secrecy diversity orders of legitimate users are obtained at high signal-to-noise ratios. Numerical results are presented to verify the accuracy of the theoretical expressions and indicate that: i) The SOP of ARIS assisted NOMA networks exceeds that of PRIS-NOMA, ARIS/PRIS-assisted orthogonal multiple access (OMA); ii) Due to the balance between the thermal noise and residual interference, introducing excess reconfigurable elements at ARIS is not helpful to reduce the SOP; and iii) The secrecy throughput performance of ARIS-NOMA networks outperforms that of PRIS-NOMA and ARIS/PRIS-OMA networks.","sentences":["As a revolutionary technology, reconfigurable intelligent surface (RIS) has been deemed as an indispensable part of the 6th generation communications due to its inherent ability to regulate the wireless channels.","However, passive RIS (PRIS) still suffers from some pressing issues, one of which is that the fading of the entire reflection link is proportional to the product of the distances from the base station to the PRIS and from the PRIS to the users, i.e., the productive attenuation.","To tackle this problem, active RIS (ARIS) has been proposed to reconfigure the wireless propagation condition and alleviate the productive attenuation.","In this paper, we investigate the physical layer security of the ARIS assisted non-orthogonal multiple access (NOMA) networks with the attendance of external and internal eavesdroppers.","To be specific, the closed-form expressions of secrecy outage probability (SOP) and secrecy system throughput are derived by invoking both imperfect successive interference cancellation (ipSIC) and perfect SIC.","The secrecy diversity orders of legitimate users are obtained at high signal-to-noise ratios.","Numerical results are presented to verify the accuracy of the theoretical expressions and indicate that: i) The SOP of ARIS assisted NOMA networks exceeds that of PRIS-NOMA, ARIS/PRIS-assisted orthogonal multiple access (OMA); ii) Due to the balance between the thermal noise and residual interference, introducing excess reconfigurable elements at ARIS is not helpful to reduce the SOP; and iii)","The secrecy throughput performance of ARIS-NOMA networks outperforms that of PRIS-NOMA and ARIS/PRIS-OMA networks."],"url":"http://arxiv.org/abs/2403.11109v1","category":"cs.IT"}
{"created":"2024-03-17 06:20:28","title":"Self-Supervised Quantization-Aware Knowledge Distillation","abstract":"Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive evaluation shows that SQAKD substantially outperforms the state-of-the-art QAT and KD works for a variety of model architectures. Our code is at: https://github.com/kaiqi123/SQAKD.git.","sentences":["Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models.","However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance.","To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework.","SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works.","Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels.","A comprehensive evaluation shows that SQAKD substantially outperforms the state-of-the-art QAT and KD works for a variety of model architectures.","Our code is at: https://github.com/kaiqi123/SQAKD.git."],"url":"http://arxiv.org/abs/2403.11106v1","category":"cs.LG"}
{"created":"2024-03-17 06:18:03","title":"Deep Neural Network NMPC for Computationally Tractable Optimal Power Management of Hybrid Electric Vehicle","abstract":"This study presents a method for deep neural network nonlinear model predictive control (DNN-MPC) to reduce computational complexity, and we show its practical utility through its application in optimizing the energy management of hybrid electric vehicles (HEVs). For optimal power management of HEVs, we first design the online NMPC to collect the data set, and the deep neural network is trained to approximate the NMPC solutions. We assess the effectiveness of our approach by conducting comparative simulations with rule and online NMPC-based power management strategies for HEV, evaluating both fuel consumption and computational complexity. Lastly, we verify the real-time feasibility of our approach through process-in-the-loop (PIL) testing. The test results demonstrate that the proposed method closely approximates the NMPC performance while substantially reducing the computational burden.","sentences":["This study presents a method for deep neural network nonlinear model predictive control (DNN-MPC) to reduce computational complexity, and we show its practical utility through its application in optimizing the energy management of hybrid electric vehicles (HEVs).","For optimal power management of HEVs, we first design the online NMPC to collect the data set, and the deep neural network is trained to approximate the NMPC solutions.","We assess the effectiveness of our approach by conducting comparative simulations with rule and online NMPC-based power management strategies for HEV, evaluating both fuel consumption and computational complexity.","Lastly, we verify the real-time feasibility of our approach through process-in-the-loop (PIL) testing.","The test results demonstrate that the proposed method closely approximates the NMPC performance while substantially reducing the computational burden."],"url":"http://arxiv.org/abs/2403.11104v1","category":"eess.SY"}
{"created":"2024-03-17 05:35:09","title":"Secrecy Outage Probability Analysis for Downlink RIS-NOMA Networks with On-Off Control","abstract":"Reconfigurable intelligent surface (RIS) has been regarded as a promising technology since it has ability to create the favorable channel conditions. This paper investigates the secure communications of RIS assisted non-orthogonal multiple access (NOMA) networks, where both external and internal eavesdropping scenarios are taken into consideration. More specifically, novel approximate and asymptotic expressions of secrecy outage probability (SOP) for the k-th legitimate user (LU) are derived by invoking imperfect successive interference cancellation (ipSIC) and perfect successive interference cancellation (pSIC). To characterize the secrecy performance of RIS-NOMA networks, the diversity order of the k-th LU with ipSIC/pSIC is obtained in the high signal-to-noise ratio region. The secrecy system throughput of RIS-NOMA networks is discussed in delay-limited transmission mode. Numerical results are presented to verify theoretical analysis that: i) The SOP of RIS-NOMA networks is superior to that of RIS assisted orthogonal multiple access (OMA) and conventional cooperative communication schemes; ii) As the number of reflecting elements increases, the RIS-NOMA networks are capable of achieving the enhanced secrecy performance; and iii) The RIS-NOMA networks have better secrecy system throughput than that of RIS-OMA networks and conventional cooperative communication schemes.","sentences":["Reconfigurable intelligent surface (RIS) has been regarded as a promising technology since it has ability to create the favorable channel conditions.","This paper investigates the secure communications of RIS assisted non-orthogonal multiple access (NOMA) networks, where both external and internal eavesdropping scenarios are taken into consideration.","More specifically, novel approximate and asymptotic expressions of secrecy outage probability (SOP) for the k-th legitimate user (LU) are derived by invoking imperfect successive interference cancellation (ipSIC) and perfect successive interference cancellation (pSIC).","To characterize the secrecy performance of RIS-NOMA networks, the diversity order of the k-th LU with ipSIC/pSIC is obtained in the high signal-to-noise ratio region.","The secrecy system throughput of RIS-NOMA networks is discussed in delay-limited transmission mode.","Numerical results are presented to verify theoretical analysis that: i)","The SOP of RIS-NOMA networks is superior to that of RIS assisted orthogonal multiple access (OMA) and conventional cooperative communication schemes; ii)","As the number of reflecting elements increases, the RIS-NOMA networks are capable of achieving the enhanced secrecy performance; and iii)","The RIS-NOMA networks have better secrecy system throughput than that of RIS-OMA networks and conventional cooperative communication schemes."],"url":"http://arxiv.org/abs/2403.11097v1","category":"cs.IT"}
{"created":"2024-03-17 05:05:11","title":"Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts","abstract":"Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, \"Conceptual Coverage Across Languages\" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in the text domain with similarity scores. Our findings will guide the future development of T2I multilinguality metrics by providing analytical tools for practical translation decisions.","sentences":["Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set.","One such benchmark, \"Conceptual Coverage Across Languages\" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations.","Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese.","We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark.","We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in the text domain with similarity scores.","Our findings will guide the future development of T2I multilinguality metrics by providing analytical tools for practical translation decisions."],"url":"http://arxiv.org/abs/2403.11092v1","category":"cs.CL"}
{"created":"2024-03-17 04:59:30","title":"Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed","abstract":"The emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed. Prior art in INDP focus on deploying tree/forest models on the data plane. We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints. In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed. Many types of NNs (such as Recurrent Neural Network (RNN), and transformers) that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly. However, the challenge is significant: the recurrent computation scheme used in RNN inference is fundamentally different from the match-action paradigm used on the network data plane. BoS addresses this challenge by (i) designing a novel data plane friendly RNN architecture that can execute unlimited RNN time steps with limited data plane stages, effectively achieving line-speed RNN inference; and (ii) complementing the on-switch RNN model with an off-switch transformer-based traffic analysis module to further boost the overall performance. We implement a prototype of BoS using a P4 programmable switch as our data plane, and extensively evaluate it over multiple traffic analysis tasks. The results show that BoS outperforms state-of-the-art in both analysis accuracy and scalability.","sentences":["The emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed.","Prior art in INDP focus on deploying tree/forest models on the data plane.","We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints.","In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed.","Many types of NNs (such as Recurrent Neural Network (RNN), and transformers) that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly.","However, the challenge is significant: the recurrent computation scheme used in RNN inference is fundamentally different from the match-action paradigm used on the network data plane.","BoS addresses this challenge by (i) designing a novel data plane friendly RNN architecture that can execute unlimited RNN time steps with limited data plane stages, effectively achieving line-speed RNN inference; and (ii) complementing the on-switch RNN model with an off-switch transformer-based traffic analysis module to further boost the overall performance.","We implement a prototype of BoS using a P4 programmable switch as our data plane, and extensively evaluate it over multiple traffic analysis tasks.","The results show that BoS outperforms state-of-the-art in both analysis accuracy and scalability."],"url":"http://arxiv.org/abs/2403.11090v1","category":"cs.NI"}
{"created":"2024-03-17 04:44:48","title":"Programming Frameworks for Differential Privacy","abstract":"Many programming frameworks have been introduced to support the development of differentially private software applications. In this chapter, we survey some of the conceptual ideas underlying these frameworks in a way that we hope will be helpful for both practitioners and researchers. For practitioners, the survey can provide a starting point for understanding what features may be valuable when selecting a programming framework. For researchers, it can help organize existing work in a unified way and provide context for understanding new features in future frameworks.","sentences":["Many programming frameworks have been introduced to support the development of differentially private software applications.","In this chapter, we survey some of the conceptual ideas underlying these frameworks in a way that we hope will be helpful for both practitioners and researchers.","For practitioners, the survey can provide a starting point for understanding what features may be valuable when selecting a programming framework.","For researchers, it can help organize existing work in a unified way and provide context for understanding new features in future frameworks."],"url":"http://arxiv.org/abs/2403.11088v1","category":"cs.CR"}
{"created":"2024-03-17 04:29:45","title":"RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning","abstract":"Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing the BERTAttack success rate by almost half (from 75.51\\% to 38.81\\%). The framework also yields improvements of 1.59\\% and 0.23\\% in semantic textual similarity tasks and various transfer tasks, respectively.","sentences":["Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks.","Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings.","In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks.","Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings.","Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations.","Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing the BERTAttack success rate by almost half (from 75.51\\% to 38.81\\%).","The framework also yields improvements of 1.59\\% and 0.23\\% in semantic textual similarity tasks and various transfer tasks, respectively."],"url":"http://arxiv.org/abs/2403.11082v1","category":"cs.CL"}
{"created":"2024-03-17 03:52:52","title":"GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment","abstract":"Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context. In contrast, our approach can successfully generate concise verbal communication for the embodied assistant to effectively boost the performance of the cooperation as well as human users' perception of the assistant.","sentences":["Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state.","In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA).","GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals.","This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation.","We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator).","Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context.","In contrast, our approach can successfully generate concise verbal communication for the embodied assistant to effectively boost the performance of the cooperation as well as human users' perception of the assistant."],"url":"http://arxiv.org/abs/2403.11075v1","category":"cs.HC"}
{"created":"2024-03-17 03:45:14","title":"Audio-Visual Segmentation via Unlabeled Frame Exploitation","abstract":"Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues as the dynamic guidance to improve the objectness localization. Besides, we exploit the semantic cues in DFs by treating them as valid augmentations to the labeled frames, which are then used to enrich data diversity in a self-training manner. Extensive experimental results demonstrate the versatility and superiority of our method, unleashing the power of the abundant unlabeled frames.","sentences":["Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames.","Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue.","To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF).","NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects.","Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations.","Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS.","Specifically, for NFs, we exploit the motion cues as the dynamic guidance to improve the objectness localization.","Besides, we exploit the semantic cues in DFs by treating them as valid augmentations to the labeled frames, which are then used to enrich data diversity in a self-training manner.","Extensive experimental results demonstrate the versatility and superiority of our method, unleashing the power of the abundant unlabeled frames."],"url":"http://arxiv.org/abs/2403.11074v1","category":"cs.CV"}
{"created":"2024-03-17 03:38:50","title":"Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping","abstract":"Automatic karyotype analysis is often defined as a visual perception task focused solely on chromosomal object-level modeling. This definition has led most existing methods to overlook componential and holistic information, significantly constraining model performance. Moreover, the lack of interpretability in current technologies hinders clinical adoption. In this paper, we introduce Tokensome, a novel vision-language model based on chromosome tokenization for explainable and cognitive karyotyping. Tokensome elevates the method from the conventional visual perception layer to the cognitive decision-making layer. This elevation enables the integration of domain knowledge and cognitive reasoning via knowledge graphs and LLMs, markedly enhancing model's explainability and facilitating abnormality detection.","sentences":["Automatic karyotype analysis is often defined as a visual perception task focused solely on chromosomal object-level modeling.","This definition has led most existing methods to overlook componential and holistic information, significantly constraining model performance.","Moreover, the lack of interpretability in current technologies hinders clinical adoption.","In this paper, we introduce Tokensome, a novel vision-language model based on chromosome tokenization for explainable and cognitive karyotyping.","Tokensome elevates the method from the conventional visual perception layer to the cognitive decision-making layer.","This elevation enables the integration of domain knowledge and cognitive reasoning via knowledge graphs and LLMs, markedly enhancing model's explainability and facilitating abnormality detection."],"url":"http://arxiv.org/abs/2403.11073v1","category":"cs.CV"}
{"created":"2024-03-17 03:12:10","title":"Unsupervised Learning for Equitable DER Control","abstract":"In the context of managing distributed energy resources (DERs) within distribution networks (DNs), this work focuses on the task of developing local controllers. We propose an unsupervised learning framework to train functions that can closely approximate optimal power flow (OPF) solutions. The primary aim is to establish specific conditions under which these learned functions can collectively guide the network towards desired configurations asymptotically, leveraging an incremental control approach. The flexibility of the proposed methodology allows to integrate fairness-driven components into the cost function associated with the OPF problem. This addition seeks to mitigate power curtailment disparities among DERs, thereby promoting equitable power injections across the network. To demonstrate the effectiveness of the proposed approach, power flow simulations are conducted using the IEEE 37-bus feeder. The findings not only showcase the guaranteed system stability but also underscore its improved overall performance.","sentences":["In the context of managing distributed energy resources (DERs) within distribution networks (DNs), this work focuses on the task of developing local controllers.","We propose an unsupervised learning framework to train functions that can closely approximate optimal power flow (OPF) solutions.","The primary aim is to establish specific conditions under which these learned functions can collectively guide the network towards desired configurations asymptotically, leveraging an incremental control approach.","The flexibility of the proposed methodology allows to integrate fairness-driven components into the cost function associated with the OPF problem.","This addition seeks to mitigate power curtailment disparities among DERs, thereby promoting equitable power injections across the network.","To demonstrate the effectiveness of the proposed approach, power flow simulations are conducted using the IEEE 37-bus feeder.","The findings not only showcase the guaranteed system stability but also underscore its improved overall performance."],"url":"http://arxiv.org/abs/2403.11068v1","category":"eess.SY"}
{"created":"2024-03-17 02:24:09","title":"A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization","abstract":"Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effective across a variety of benchmark domains. Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco environments where the traditional CVaR-PG fails to learn a reasonable policy.","sentences":["Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications.","This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat.","To address these challenges, we propose a simple mixture policy parameterization.","This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy.","By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness.","Our empirical study reveals that this mixture parameterization is uniquely effective across a variety of benchmark domains.","Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco environments where the traditional CVaR-PG fails to learn a reasonable policy."],"url":"http://arxiv.org/abs/2403.11062v1","category":"cs.LG"}
{"created":"2024-03-17 02:16:28","title":"Beamforming Design for Double-Active-RIS-aided Communication Systems with Inter-Excitation","abstract":"In this paper, we investigate a double-active-reconfigurable intelligent surface (RIS)-aided downlink wireless communication system, where a multi-antenna base station (BS) serves multiple single-antenna users with both double reflection and single reflection links. Due to the signal amplification capability of active RISs, the mutual influence between active RISs, which is termed as the \"inter-excitation\" effect, cannot be ignored. Then, we develop a feedback-type model to characterize the signal containing the inter-excitation effect. Based on the signal model, we formulate a weighted sum rate (WSR) maximization problem by jointly optimizing the beamforming matrix at the BS and the reflecting coefficient matrices at the two active RISs, subject to power constraints at the BS and active RISs, as well as the maximum amplification gain constraints of the active RISs. To solve this non-convex problem, we first transform the problem into a more tractable form using the fractional programming (FP) method. Then, by introducing auxiliary variables, the problem can be converted into an equivalent form that can be solved by using a low-complexity penalty dual decomposition (PDD) algorithm. Finally, simulation results indicate that it is crucial to consider the inter-excitation effect between active RISs in beamforming design for double-active-RIS-aided communication systems. Additionally, it prevails over other benchmark schemes with single active RIS and double passive RISs in terms of achievable rate.","sentences":["In this paper, we investigate a double-active-reconfigurable intelligent surface (RIS)-aided downlink wireless communication system, where a multi-antenna base station (BS) serves multiple single-antenna users with both double reflection and single reflection links.","Due to the signal amplification capability of active RISs, the mutual influence between active RISs, which is termed as the \"inter-excitation\" effect, cannot be ignored.","Then, we develop a feedback-type model to characterize the signal containing the inter-excitation effect.","Based on the signal model, we formulate a weighted sum rate (WSR) maximization problem by jointly optimizing the beamforming matrix at the BS and the reflecting coefficient matrices at the two active RISs, subject to power constraints at the BS and active RISs, as well as the maximum amplification gain constraints of the active RISs.","To solve this non-convex problem, we first transform the problem into a more tractable form using the fractional programming (FP) method.","Then, by introducing auxiliary variables, the problem can be converted into an equivalent form that can be solved by using a low-complexity penalty dual decomposition (PDD) algorithm.","Finally, simulation results indicate that it is crucial to consider the inter-excitation effect between active RISs in beamforming design for double-active-RIS-aided communication systems.","Additionally, it prevails over other benchmark schemes with single active RIS and double passive RISs in terms of achievable rate."],"url":"http://arxiv.org/abs/2403.11061v1","category":"eess.SP"}
{"created":"2024-03-17 02:15:15","title":"Intelligent Railroad Grade Crossing: Leveraging Semantic Segmentation and Object Detection for Enhanced Safety","abstract":"Crashes and delays at Railroad Highway Grade Crossings (RHGC), where highways and railroads intersect, pose significant safety concerns for the U.S. Federal Railroad Administration (FRA). Despite the critical importance of addressing accidents and traffic delays at highway-railroad intersections, there is a notable dearth of research on practical solutions for managing these issues. In response to this gap in the literature, our study introduces an intelligent system that leverages machine learning and computer vision techniques to enhance safety at Railroad Highway Grade crossings (RHGC). This research proposed a Non-Maximum Suppression (NMS)- based ensemble model that integrates a variety of YOLO variants, specifically YOLOv5S, YOLOv5M, and YOLOv5L, for grade-crossing object detection, utilizes segmentation techniques from the UNet architecture for detecting approaching rail at a grade crossing. Both methods are implemented on a Raspberry Pi. Moreover, the strategy employs high-definition cameras installed at the RHGC. This framework enables the system to monitor objects within the Region of Interest (ROI) at crossings, detect the approach of trains, and clear the crossing area before a train arrives. Regarding accuracy, precision, recall, and Intersection over Union (IoU), the proposed state-of-the-art NMS-based object detection ensemble model achieved 96% precision. In addition, the UNet segmentation model obtained a 98% IoU value. This automated railroad grade crossing system powered by artificial intelligence represents a promising solution for enhancing safety at highway-railroad intersections.","sentences":["Crashes and delays at Railroad Highway Grade Crossings (RHGC), where highways and railroads intersect, pose significant safety concerns for the U.S. Federal Railroad Administration (FRA).","Despite the critical importance of addressing accidents and traffic delays at highway-railroad intersections, there is a notable dearth of research on practical solutions for managing these issues.","In response to this gap in the literature, our study introduces an intelligent system that leverages machine learning and computer vision techniques to enhance safety at Railroad Highway Grade crossings (RHGC).","This research proposed a Non-Maximum Suppression (NMS)- based ensemble model that integrates a variety of YOLO variants, specifically YOLOv5S, YOLOv5M, and YOLOv5L, for grade-crossing object detection, utilizes segmentation techniques from the UNet architecture for detecting approaching rail at a grade crossing.","Both methods are implemented on a Raspberry Pi.","Moreover, the strategy employs high-definition cameras installed at the RHGC.","This framework enables the system to monitor objects within the Region of Interest (ROI) at crossings, detect the approach of trains, and clear the crossing area before a train arrives.","Regarding accuracy, precision, recall, and Intersection over Union (IoU), the proposed state-of-the-art NMS-based object detection ensemble model achieved 96% precision.","In addition, the UNet segmentation model obtained a 98% IoU value.","This automated railroad grade crossing system powered by artificial intelligence represents a promising solution for enhancing safety at highway-railroad intersections."],"url":"http://arxiv.org/abs/2403.11060v1","category":"cs.CV"}
{"created":"2024-03-17 01:42:48","title":"OSTAF: A One-Shot Tuning Method for Improved Attribute-Focused T2I Personalization","abstract":"Personalized text-to-image (T2I) models not only produce lifelike and varied visuals but also allow users to tailor the images to fit their personal taste. These personalization techniques can grasp the essence of a concept through a collection of images, or adjust a pre-trained text-to-image model with a specific image input for subject-driven or attribute-aware guidance. Yet, accurately capturing the distinct visual attributes of an individual image poses a challenge for these methods. To address this issue, we introduce OSTAF, a novel parameter-efficient one-shot fine-tuning method which only utilizes one reference image for T2I personalization. A novel hypernetwork-powered attribute-focused fine-tuning mechanism is employed to achieve the precise learning of various attribute features (e.g., appearance, shape or drawing style) from the reference image. Comparing to existing image customization methods, our method shows significant superiority in attribute identification and application, as well as achieves a good balance between efficiency and output quality.","sentences":["Personalized text-to-image (T2I) models not only produce lifelike and varied visuals but also allow users to tailor the images to fit their personal taste.","These personalization techniques can grasp the essence of a concept through a collection of images, or adjust a pre-trained text-to-image model with a specific image input for subject-driven or attribute-aware guidance.","Yet, accurately capturing the distinct visual attributes of an individual image poses a challenge for these methods.","To address this issue, we introduce OSTAF, a novel parameter-efficient one-shot fine-tuning method which only utilizes one reference image for T2I personalization.","A novel hypernetwork-powered attribute-focused fine-tuning mechanism is employed to achieve the precise learning of various attribute features (e.g., appearance, shape or drawing style) from the reference image.","Comparing to existing image customization methods, our method shows significant superiority in attribute identification and application, as well as achieves a good balance between efficiency and output quality."],"url":"http://arxiv.org/abs/2403.11053v1","category":"cs.CV"}
{"created":"2024-03-17 01:26:38","title":"Gender differences in online communication: A case study of Soccer","abstract":"Social media and digital platforms allow us to express our opinions freely and easily to a vast number of people. In this study, we examine whether there are gender-based differences in how communication happens via Twitter in regard to soccer. Soccer is one of the most popular sports, and therefore, on social media, it engages a diverse audience regardless of their technical knowledge. We collected Twitter data for three months (March-June) for English and Portuguese that contains 9.5 million Tweets related to soccer, and only 18.38% tweets were identified as belonging to women, highlighting a possible gender gap already in the number of people who participated actively in this topic. We then conduct a fine-grained text-level and network-level analysis to identify the gender differences that might exist while communicating on Twitter. Our results show that women express their emotions more intensely than men, regardless of the differences in volume. The network generated from Portuguese has lower homophily than English. However, this difference in homophily does not impact how females express their emotions and sentiments, suggesting that these aspects are inherent norms or characteristics of genders. Our study unveils more gaps through qualitative and quantitative analyses, highlighting the importance of examining and reporting gender gaps in online communication to create a more inclusive space where people can openly share their opinions.","sentences":["Social media and digital platforms allow us to express our opinions freely and easily to a vast number of people.","In this study, we examine whether there are gender-based differences in how communication happens via Twitter in regard to soccer.","Soccer is one of the most popular sports, and therefore, on social media, it engages a diverse audience regardless of their technical knowledge.","We collected Twitter data for three months (March-June) for English and Portuguese that contains 9.5 million Tweets related to soccer, and only 18.38% tweets were identified as belonging to women, highlighting a possible gender gap already in the number of people who participated actively in this topic.","We then conduct a fine-grained text-level and network-level analysis to identify the gender differences that might exist while communicating on Twitter.","Our results show that women express their emotions more intensely than men, regardless of the differences in volume.","The network generated from Portuguese has lower homophily than English.","However, this difference in homophily does not impact how females express their emotions and sentiments, suggesting that these aspects are inherent norms or characteristics of genders.","Our study unveils more gaps through qualitative and quantitative analyses, highlighting the importance of examining and reporting gender gaps in online communication to create a more inclusive space where people can openly share their opinions."],"url":"http://arxiv.org/abs/2403.11051v1","category":"cs.SI"}
{"created":"2024-03-17 00:14:29","title":"From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting","abstract":"Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges. Recent studies have explored image-driven approaches using computer vision models to address these challenges, often employing lineplots as the visual representation of time series data. In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data. We introduce the use of a vision transformer for multimodal learning, showcasing the advantages of our approach across diverse datasets from different domains. To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input. Our experiments demonstrate the benefits of utilizing spectrograms as a visual representation for time series data, along with the advantages of employing a vision transformer for simultaneous learning in both the time and frequency domains.","sentences":["Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges.","Recent studies have explored image-driven approaches using computer vision models to address these challenges, often employing lineplots as the visual representation of time series data.","In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data.","We introduce the use of a vision transformer for multimodal learning, showcasing the advantages of our approach across diverse datasets from different domains.","To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input.","Our experiments demonstrate the benefits of utilizing spectrograms as a visual representation for time series data, along with the advantages of employing a vision transformer for simultaneous learning in both the time and frequency domains."],"url":"http://arxiv.org/abs/2403.11047v1","category":"cs.CV"}
{"created":"2024-03-17 00:11:15","title":"Regulating Chatbot Output via Inter-Informational Competition","abstract":"The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empirical evidence has demonstrated that market competition among information outlets can effectively mitigate most risks and that overreliance on regulation is not only unnecessary but detrimental, as well. This Article argues that sufficient competition among chatbots and other information outlets in the information marketplace can sufficiently mitigate and even resolve most content risks posed by generative AI technologies. This renders certain loudly advocated regulatory strategies, like mandatory prohibitions, licensure, curation of datasets, and notice-and-response regimes, truly unnecessary and even toxic to desirable competition and innovation throughout the AI industry. Ultimately, the ideas that I advance in this Article should pour some much-needed cold water on the regulatory frenzy over generative AI and steer the issue back to a rational track.","sentences":["The advent of ChatGPT has sparked over a year of regulatory frenzy.","However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs.","Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly.","This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets.","The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies.","In fact, a trove of empirical evidence has demonstrated that market competition among information outlets can effectively mitigate most risks and that overreliance on regulation is not only unnecessary but detrimental, as well.","This Article argues that sufficient competition among chatbots and other information outlets in the information marketplace can sufficiently mitigate and even resolve most content risks posed by generative AI technologies.","This renders certain loudly advocated regulatory strategies, like mandatory prohibitions, licensure, curation of datasets, and notice-and-response regimes, truly unnecessary and even toxic to desirable competition and innovation throughout the AI industry.","Ultimately, the ideas that I advance in this Article should pour some much-needed cold water on the regulatory frenzy over generative AI and steer the issue back to a rational track."],"url":"http://arxiv.org/abs/2403.11046v1","category":"cs.CY"}
{"created":"2024-03-16 22:51:02","title":"Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals","abstract":"Sound event detection (SED) is an active area of audio research that aims to detect the temporal occurrence of sounds. In this paper, we apply SED to engine fault detection by introducing a multimodal SED framework that detects fine-grained engine faults of automobile engines using audio and accelerometer-recorded vibration. We first introduce the problem of engine fault SED on a dataset collected from a large variety of vehicles with expertly-labeled engine fault sound events. Next, we propose a SED model to temporally detect ten fine-grained engine faults that occur within vehicle engines and further explore a pretraining strategy using a large-scale weakly-labeled engine fault dataset. Through multiple evaluations, we show our proposed framework is able to effectively detect engine fault sound events. Finally, we investigate the interaction and characteristics of each modality and show that fusing features from audio and vibration improves overall engine fault SED capabilities.","sentences":["Sound event detection (SED) is an active area of audio research that aims to detect the temporal occurrence of sounds.","In this paper, we apply SED to engine fault detection by introducing a multimodal SED framework that detects fine-grained engine faults of automobile engines using audio and accelerometer-recorded vibration.","We first introduce the problem of engine fault SED on a dataset collected from a large variety of vehicles with expertly-labeled engine fault sound events.","Next, we propose a SED model to temporally detect ten fine-grained engine faults that occur within vehicle engines and further explore a pretraining strategy using a large-scale weakly-labeled engine fault dataset.","Through multiple evaluations, we show our proposed framework is able to effectively detect engine fault sound events.","Finally, we investigate the interaction and characteristics of each modality and show that fusing features from audio and vibration improves overall engine fault SED capabilities."],"url":"http://arxiv.org/abs/2403.11037v1","category":"eess.AS"}
{"created":"2024-03-16 22:14:56","title":"Reward Guided Latent Consistency Distillation","abstract":"Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from the teacher LDM, representing a 25 times inference acceleration without quality loss.   As directly optimizing towards differentiable RMs can suffer from over-optimization, we overcome this difficulty by proposing the use of a latent proxy RM (LRM). This novel component serves as an intermediary, connecting our LCM with the RM. Empirically, we demonstrate that incorporating the LRM into our RG-LCD successfully avoids high-frequency noise in the generated images, contributing to both improved FID on MS-COCO and a higher HPSv2.1 score on HPSv2's test set, surpassing those achieved by the baseline LCM.","sentences":["Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis.","By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps.","However, the LCM's efficient inference is obtained at the cost of the sample quality.","In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training.","Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation.","As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from the teacher LDM, representing a 25 times inference acceleration without quality loss.   ","As directly optimizing towards differentiable RMs can suffer from over-optimization, we overcome this difficulty by proposing the use of a latent proxy RM (LRM).","This novel component serves as an intermediary, connecting our LCM with the RM.","Empirically, we demonstrate that incorporating the LRM into our RG-LCD successfully avoids high-frequency noise in the generated images, contributing to both improved FID on MS-COCO and a higher HPSv2.1 score on HPSv2's test set, surpassing those achieved by the baseline LCM."],"url":"http://arxiv.org/abs/2403.11027v1","category":"cs.CV"}
{"created":"2024-03-16 21:40:27","title":"Neuro-Symbolic Video Search","abstract":"The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reasoning improves the F1 score of complex event identification by 9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art self-driving datasets such as Waymo and NuScenes.","sentences":["The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks.","Long-term temporal reasoning is a key desideratum for frame retrieval systems.","While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames.","A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network.","Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification.","We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory.","Our TL-based reasoning improves the F1 score of complex event identification by 9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art self-driving datasets such as Waymo and NuScenes."],"url":"http://arxiv.org/abs/2403.11021v1","category":"cs.CV"}
{"created":"2024-03-16 20:56:22","title":"Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach","abstract":"In systems biology, attractor landscape analysis of gene regulatory networks is recognized as a powerful computational tool for studying various cellular states from proliferation and differentiation to senescence and apoptosis. Therefore, accurate identification of attractors plays a critical role in determination of the cell fates. On the other hand, in a real biological circuit, genetic/epigenetic alterations as well as varying environmental factors drastically take effect on the location, characteristics, and even the number of attractors. The central question is: Given a temporal gene expression profile of a real gene regulatory network, how can the attractors be robustly identified in the presence of huge amount of uncertainty? This paper addresses this question using a novel approach based on Zadeh Computing with Words. The proposed scheme could effectively identify the attractors from temporal gene expression data in terms of both fuzzy logic-based and linguistic descriptions which are simply interpretable by human experts. Therefore, this method can be considered as an effective step towards interpretable artificial intelligence. Without loss of generality, genetic toggle switch is considered as the case study. The nonlinear dynamics of this benchmark gene regulatory network is computationally modeled by the notion of uncertain stochastic differential equations. The results of in-silico study demonstrate the efficiency and robustness of the proposed method.","sentences":["In systems biology, attractor landscape analysis of gene regulatory networks is recognized as a powerful computational tool for studying various cellular states from proliferation and differentiation to senescence and apoptosis.","Therefore, accurate identification of attractors plays a critical role in determination of the cell fates.","On the other hand, in a real biological circuit, genetic/epigenetic alterations as well as varying environmental factors drastically take effect on the location, characteristics, and even the number of attractors.","The central question is: Given a temporal gene expression profile of a real gene regulatory network, how can the attractors be robustly identified in the presence of huge amount of uncertainty?","This paper addresses this question using a novel approach based on Zadeh Computing with Words.","The proposed scheme could effectively identify the attractors from temporal gene expression data in terms of both fuzzy logic-based and linguistic descriptions which are simply interpretable by human experts.","Therefore, this method can be considered as an effective step towards interpretable artificial intelligence.","Without loss of generality, genetic toggle switch is considered as the case study.","The nonlinear dynamics of this benchmark gene regulatory network is computationally modeled by the notion of uncertain stochastic differential equations.","The results of in-silico study demonstrate the efficiency and robustness of the proposed method."],"url":"http://arxiv.org/abs/2403.11015v1","category":"q-bio.MN"}
{"created":"2024-03-16 20:18:36","title":"DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages","abstract":"Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for language varieties and one step towards advancing it further. Code/data: https://github.com/ffaisal93/DialectBench","sentences":["Language technologies should be judged on their usefulness in real-world use cases.","An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties).","Most NLP benchmarks are limited to standard language varieties.","To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties).","This allows for a comprehensive evaluation of NLP system performance on different language varieties.","We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks.","We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for language varieties and one step towards advancing it further.","Code/data: https://github.com/ffaisal93/DialectBench"],"url":"http://arxiv.org/abs/2403.11009v1","category":"cs.CL"}
{"created":"2024-03-16 18:50:44","title":"N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields","abstract":"Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the CLIP embeddings using deferred volumetric rendering at varying physical scales, creating a coarse-to-fine representation. Extensive experiments show that our approach outperforms the state-of-the-art feature field distillation methods on tasks such as open-vocabulary 3D segmentation and localization, demonstrating the effectiveness of the learned nested feature field.","sentences":["Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision.","To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities.","Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes.","We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments.","Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the CLIP embeddings using deferred volumetric rendering at varying physical scales, creating a coarse-to-fine representation.","Extensive experiments show that our approach outperforms the state-of-the-art feature field distillation methods on tasks such as open-vocabulary 3D segmentation and localization, demonstrating the effectiveness of the learned nested feature field."],"url":"http://arxiv.org/abs/2403.10997v1","category":"cs.CV"}
{"created":"2024-03-16 18:44:56","title":"Edge Private Graph Neural Networks with Singular Value Perturbation","abstract":"Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications. However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model. Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation. The added perturbations cause the graph structure to be substantially morphed, reducing the model utility. We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains GNNs with a low-rank format of the graph via singular values decomposition (SVD), rather than the original graph. Using the low-rank format, Eclipse preserves the primary graph topology and removes the remaining residual edges. Eclipse adds noise to the low-rank singular values instead of the entire graph, thereby preserving the graph privacy while still maintaining enough of the graph structure to maintain model utility. We theoretically show Eclipse provide formal DP guarantee on edges. Experiments on benchmark graph datasets show that Eclipse achieves significantly better privacy-utility tradeoff compared to existing privacy-preserving GNN training methods. In particular, under strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant gains in the model utility by up to 46%. We further demonstrate that Eclipse also has better resilience against common edge attacks (e.g., LPA), lowering the attack AUC by up to 5% compared to other state-of-the-art baselines.","sentences":["Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications.","However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks.","This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model.","Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation.","The added perturbations cause the graph structure to be substantially morphed, reducing the model utility.","We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges.","Eclipse is based on two key observations.","First, adjacency matrices in graph structures exhibit low-rank behavior.","Thus, Eclipse trains GNNs with a low-rank format of the graph via singular values decomposition (SVD), rather than the original graph.","Using the low-rank format, Eclipse preserves the primary graph topology and removes the remaining residual edges.","Eclipse adds noise to the low-rank singular values instead of the entire graph, thereby preserving the graph privacy while still maintaining enough of the graph structure to maintain model utility.","We theoretically show Eclipse provide formal DP guarantee on edges.","Experiments on benchmark graph datasets show that Eclipse achieves significantly better privacy-utility tradeoff compared to existing privacy-preserving GNN training methods.","In particular, under strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant gains in the model utility by up to 46%.","We further demonstrate that Eclipse also has better resilience against common edge attacks (e.g., LPA), lowering the attack AUC by up to 5% compared to other state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.10995v1","category":"cs.LG"}
{"created":"2024-03-16 18:44:21","title":"SSUP-HRI: Social Signaling in Urban Public Human-Robot Interaction dataset","abstract":"This paper introduces our dataset featuring human-robot interactions (HRI) in urban public environments. This dataset is rich with social signals that we believe can be modeled to help understand naturalistic human-robot interaction. Our dataset currently comprises approximately 15 hours of video footage recorded from the robots' perspectives, within which we annotated a total of 274 observable interactions featuring a wide range of naturalistic human-robot interactions. The data was collected by two mobile trash barrel robots deployed in Astor Place, New York City, over the course of a week. We invite the HRI community to access and utilize our dataset. To the best of our knowledge, this is the first dataset showcasing robot deployments in a complete public, non-controlled setting involving urban residents.","sentences":["This paper introduces our dataset featuring human-robot interactions (HRI) in urban public environments.","This dataset is rich with social signals that we believe can be modeled to help understand naturalistic human-robot interaction.","Our dataset currently comprises approximately 15 hours of video footage recorded from the robots' perspectives, within which we annotated a total of 274 observable interactions featuring a wide range of naturalistic human-robot interactions.","The data was collected by two mobile trash barrel robots deployed in Astor Place, New York City, over the course of a week.","We invite the HRI community to access and utilize our dataset.","To the best of our knowledge, this is the first dataset showcasing robot deployments in a complete public, non-controlled setting involving urban residents."],"url":"http://arxiv.org/abs/2403.10994v1","category":"cs.RO"}
{"created":"2024-03-16 18:04:12","title":"Boosting Flow-based Generative Super-Resolution Models via Learned Prior","abstract":"Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models and enhances their performance in various SR scenarios. Our code is available at: https://github.com/liyuantsao/FlowSR-LP","sentences":["Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images.","However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature.","To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model.","This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image.","Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights.","We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses.","The proposed framework successfully addresses all the inherent issues in flow-based SR models and enhances their performance in various SR scenarios.","Our code is available at: https://github.com/liyuantsao/FlowSR-LP"],"url":"http://arxiv.org/abs/2403.10988v1","category":"cs.CV"}
{"created":"2024-03-16 17:32:59","title":"IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning","abstract":"To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \\textit{\\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\\pm21\\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \\carb are showcased through multiple user case studies.","sentences":["To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects.","Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL.","This paper introduces \\textit{\\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\\pm21\\%$ deviation in carbon footprint values compared to actual measurements across various DL models.","Additionally, practical applications of \\carb are showcased through multiple user case studies."],"url":"http://arxiv.org/abs/2403.10984v1","category":"cs.LG"}
{"created":"2024-03-16 17:30:15","title":"OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models","abstract":"Personalization is an important topic in text-to-image generation, especially the challenging multi-concept personalization. Current multi-concept methods are struggling with identity preservation, occlusion, and the harmony between foreground and background. In this work, we propose OMG, an occlusion-friendly personalized generation framework designed to seamlessly integrate multiple concepts within a single image. We propose a novel two-stage sampling solution. The first stage takes charge of layout generation and visual comprehension information collection for handling occlusions. The second one utilizes the acquired visual comprehension information and the designed noise blending to integrate multiple concepts while considering occlusions. We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout. Moreover, our method can be combined with various single-concept models, such as LoRA and InstantID without additional tuning. Especially, LoRA models on civitai.com can be exploited directly. Extensive experiments demonstrate that OMG exhibits superior performance in multi-concept personalization.","sentences":["Personalization is an important topic in text-to-image generation, especially the challenging multi-concept personalization.","Current multi-concept methods are struggling with identity preservation, occlusion, and the harmony between foreground and background.","In this work, we propose OMG, an occlusion-friendly personalized generation framework designed to seamlessly integrate multiple concepts within a single image.","We propose a novel two-stage sampling solution.","The first stage takes charge of layout generation and visual comprehension information collection for handling occlusions.","The second one utilizes the acquired visual comprehension information and the designed noise blending to integrate multiple concepts while considering occlusions.","We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout.","Moreover, our method can be combined with various single-concept models, such as LoRA and InstantID without additional tuning.","Especially, LoRA models on civitai.com can be exploited directly.","Extensive experiments demonstrate that OMG exhibits superior performance in multi-concept personalization."],"url":"http://arxiv.org/abs/2403.10983v1","category":"cs.CV"}
{"created":"2024-03-16 17:05:40","title":"CETASim: A numerical tool for beam collective effect study in storage rings","abstract":"We developed a 6D multi-particle tracking program CETASim in C++ programming language to simulate intensity-dependent effects in electron storage rings. The program can simulate the beam collective effects due to short-range/long-range wakefields for single/coupled-bunch instability studies. It also features to simulate interactions among charged ions and the trains of electron bunches, including both fast ion and ion trapping effects. The bunch-by-bunch feedback is also included so that the user can simulate the damping of the unstable motion when its growth rate is faster than the radiation damping rate. The particle dynamics is based on the one-turn map, including the nonlinear effects of amplitude-dependent tune shift, high-order chromaticity, and second-order momentum compaction factor. A skew quadrupole can also be introduced by the users, which is very useful for the emittance sharing and the emittance exchange studies. This paper describes the code structure, the physics models, and the algorithms used in CETASim. We also present the results of its application to PETRA-IV storage ring.","sentences":["We developed a 6D multi-particle tracking program CETASim in C++ programming language to simulate intensity-dependent effects in electron storage rings.","The program can simulate the beam collective effects due to short-range/long-range wakefields for single/coupled-bunch instability studies.","It also features to simulate interactions among charged ions and the trains of electron bunches, including both fast ion and ion trapping effects.","The bunch-by-bunch feedback is also included so that the user can simulate the damping of the unstable motion when its growth rate is faster than the radiation damping rate.","The particle dynamics is based on the one-turn map, including the nonlinear effects of amplitude-dependent tune shift, high-order chromaticity, and second-order momentum compaction factor.","A skew quadrupole can also be introduced by the users, which is very useful for the emittance sharing and the emittance exchange studies.","This paper describes the code structure, the physics models, and the algorithms used in CETASim.","We also present the results of its application to PETRA-IV storage ring."],"url":"http://arxiv.org/abs/2403.10973v1","category":"physics.acc-ph"}
{"created":"2024-03-16 16:45:28","title":"Enhancing IoT Security Against DDoS Attacks through Federated Learning","abstract":"The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices' collective intelligence for real-time attack detection without compromising sensitive data. This study proposes innovative deep autoencoder approaches for data dimensionality reduction, retraining, and partial selection to enhance the performance and stability of the proposed model. Additionally, two renowned aggregation algorithms, FedAvg and FedAvgM, are employed in this research. Various metrics, including true positive rate, false positive rate, and F1-score, are employed to evaluate the model. The dataset utilized in this research, N-BaIoT, exhibits non-IID data distribution, where data categories are distributed quite differently. The negative impact of these distribution disparities is managed by employing retraining and partial selection techniques, enhancing the final model's stability. Furthermore, evaluation results demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg, indicating that in non-IID datasets, FedAvgM provides better stability and performance.","sentences":["The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm.","Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks.","Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy.","This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead.","The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT.","Our proposed framework leverages IoT devices' collective intelligence for real-time attack detection without compromising sensitive data.","This study proposes innovative deep autoencoder approaches for data dimensionality reduction, retraining, and partial selection to enhance the performance and stability of the proposed model.","Additionally, two renowned aggregation algorithms, FedAvg and FedAvgM, are employed in this research.","Various metrics, including true positive rate, false positive rate, and F1-score, are employed to evaluate the model.","The dataset utilized in this research, N-BaIoT, exhibits non-IID data distribution, where data categories are distributed quite differently.","The negative impact of these distribution disparities is managed by employing retraining and partial selection techniques, enhancing the final model's stability.","Furthermore, evaluation results demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg, indicating that in non-IID datasets, FedAvgM provides better stability and performance."],"url":"http://arxiv.org/abs/2403.10968v1","category":"cs.CR"}
{"created":"2024-03-16 16:29:40","title":"Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization","abstract":"Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams'' of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at \\url{https://github.com/sai-prasanna/dreaming_of_many_worlds}.","sentences":["Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents.","To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state.","Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023).","This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics.","Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams'' of the world model.","We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts.","The code for all our experiments is available at \\url{https://github.com/sai-prasanna/dreaming_of_many_worlds}."],"url":"http://arxiv.org/abs/2403.10967v1","category":"cs.LG"}
{"created":"2024-03-16 16:25:34","title":"Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability","abstract":"Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization, and design optimization. This approach is particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems. This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization layer, maximizing the system's robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis. This analysis provides a formal guarantee of stability for a set of off-nominal states. The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole. Extensive simulations of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances.","sentences":["Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization, and design optimization.","This approach is particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems.","This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D).","An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR).","RTC-D augments RTC with a design optimization layer, maximizing the system's robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis.","This analysis provides a formal guarantee of stability for a set of off-nominal states.","The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole.","Extensive simulations of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances."],"url":"http://arxiv.org/abs/2403.10966v1","category":"cs.RO"}
{"created":"2024-03-16 16:16:22","title":"Investigation of Purcell enhancement of quantum dots emitting in the telecom O-band with an open fiber-cavity","abstract":"Single-photon emitters integrated in optical micro-cavities are key elements in quantum communication applications. However, for each combination of a cavity geometry with a quantum emitter system, there are specific challenges in the optimization of the emission properties and cavity-emitter interaction. Here, we present a thorough investigation of semiconductor quantum dots (QDs), emitting in the telecom O-band, integrated in an open fiber-cavity. The design provides an optical micro-cavity tunable in all spatial dimensions with intrinsic fiber-coupling. Consequently, it offers a promising approach to a high collection efficiency and the investigation of spatially and spectrally varying samples. On the other hand, the system is also susceptible to vibrational noise. Therefore, we provide a comprehensive study of the cavity and emitter properties together with an analysis of the fluctuations of the cavity length. Due to the Purcell enhancement, we observe a reduction of the decay times of up to a factor of ${2.46(2)}$.","sentences":["Single-photon emitters integrated in optical micro-cavities are key elements in quantum communication applications.","However, for each combination of a cavity geometry with a quantum emitter system, there are specific challenges in the optimization of the emission properties and cavity-emitter interaction.","Here, we present a thorough investigation of semiconductor quantum dots (QDs), emitting in the telecom O-band, integrated in an open fiber-cavity.","The design provides an optical micro-cavity tunable in all spatial dimensions with intrinsic fiber-coupling.","Consequently, it offers a promising approach to a high collection efficiency and the investigation of spatially and spectrally varying samples.","On the other hand, the system is also susceptible to vibrational noise.","Therefore, we provide a comprehensive study of the cavity and emitter properties together with an analysis of the fluctuations of the cavity length.","Due to the Purcell enhancement, we observe a reduction of the decay times of up to a factor of ${2.46(2)}$."],"url":"http://arxiv.org/abs/2403.10960v1","category":"quant-ph"}
{"created":"2024-03-16 15:30:34","title":"SelfIE: Self-Interpretation of Large Language Model Embeddings","abstract":"How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.","sentences":["How do large language models (LLMs) obtain their answers?","The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments.","We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage.","Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge.","SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning.","We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer.","We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets."],"url":"http://arxiv.org/abs/2403.10949v1","category":"cs.CL"}
{"created":"2024-03-16 15:17:20","title":"Zero-Inflated Stochastic Volatility Model for Disaggregated Inflation Data with Exact Zeros","abstract":"The disaggregated time-series data for Consumer Price Index often exhibits frequent instances of exact zero price changes, stemming from measurement errors inherent in the data collection process. However, the currently prominent stochastic volatility model of trend inflation is designed for aggregate measures of price inflation, where exact zero price changes rarely occur. We propose a zero-inflated stochastic volatility model applicable to such nonstationary real-valued multivariate time-series data with exact zeros, by a Bayesian dynamic generalized linear model that jointly specifies the dynamic zero-generating process. We also provide an efficient custom Gibbs sampler that leverages the P\\'olya-Gamma augmentation. Applying the model to disaggregated Japanese Consumer Price Index data, we find that the zero-inflated model provides more sensible and informative estimates of time-varying trend and volatility. Through an out-of-sample forecasting exercise, we find that the zero-inflated model provides improved point forecasts when zero-inflation is prominent, and better coverage of interval forecasts of the non-zero data by the non-zero distributional component.","sentences":["The disaggregated time-series data for Consumer Price Index often exhibits frequent instances of exact zero price changes, stemming from measurement errors inherent in the data collection process.","However, the currently prominent stochastic volatility model of trend inflation is designed for aggregate measures of price inflation, where exact zero price changes rarely occur.","We propose a zero-inflated stochastic volatility model applicable to such nonstationary real-valued multivariate time-series data with exact zeros, by a Bayesian dynamic generalized linear model that jointly specifies the dynamic zero-generating process.","We also provide an efficient custom Gibbs sampler that leverages the P\\'olya-Gamma augmentation.","Applying the model to disaggregated Japanese Consumer Price Index data, we find that the zero-inflated model provides more sensible and informative estimates of time-varying trend and volatility.","Through an out-of-sample forecasting exercise, we find that the zero-inflated model provides improved point forecasts when zero-inflation is prominent, and better coverage of interval forecasts of the non-zero data by the non-zero distributional component."],"url":"http://arxiv.org/abs/2403.10945v1","category":"stat.ME"}
{"created":"2024-03-16 15:17:13","title":"Human Centered AI for Indian Legal Text Analytics","abstract":"Legal research is a crucial task in the practice of law. It requires intense human effort and intellectual prudence to research a legal case and prepare arguments. Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and and the scarcity of specialized datasets for training Large Language Models (LLMs). This position paper explores the potential of LLMs within Legal Text Analytics (LTA), highlighting specific areas where the integration of human expertise can significantly enhance their performance to match that of experts. We introduce a novel dataset and describe a human centered, compound AI system that principally incorporates human inputs for performing LTA tasks with LLMs.","sentences":["Legal research is a crucial task in the practice of law.","It requires intense human effort and intellectual prudence to research a legal case and prepare arguments.","Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and and the scarcity of specialized datasets for training Large Language Models (LLMs).","This position paper explores the potential of LLMs within Legal Text Analytics (LTA), highlighting specific areas where the integration of human expertise can significantly enhance their performance to match that of experts.","We introduce a novel dataset and describe a human centered, compound AI system that principally incorporates human inputs for performing LTA tasks with LLMs."],"url":"http://arxiv.org/abs/2403.10944v1","category":"cs.HC"}
{"created":"2024-03-16 14:06:29","title":"Inducing Individual Students' Learning Strategies through Homomorphic POMDPs","abstract":"Optimizing students' learning strategies is a crucial component in intelligent tutoring systems. Previous research has demonstrated the effectiveness of devising personalized learning strategies for students by modelling their learning processes through partially observable Markov decision process (POMDP). However, the research holds the assumption that the student population adheres to a uniform cognitive pattern. While this assumption simplifies the POMDP modelling process, it evidently deviates from a real-world scenario, thus reducing the precision of inducing individual students' learning strategies. In this article, we propose the homomorphic POMDP (H-POMDP) model to accommodate multiple cognitive patterns and present the parameter learning approach to automatically construct the H-POMDP model. Based on the H-POMDP model, we are able to represent different cognitive patterns from the data and induce more personalized learning strategies for individual students. We conduct experiments to show that, in comparison to the general POMDP approach, the H-POMDP model demonstrates better precision when modelling mixed data from multiple cognitive patterns. Moreover, the learning strategies derived from H-POMDPs exhibit better personalization in the performance evaluation.","sentences":["Optimizing students' learning strategies is a crucial component in intelligent tutoring systems.","Previous research has demonstrated the effectiveness of devising personalized learning strategies for students by modelling their learning processes through partially observable Markov decision process (POMDP).","However, the research holds the assumption that the student population adheres to a uniform cognitive pattern.","While this assumption simplifies the POMDP modelling process, it evidently deviates from a real-world scenario, thus reducing the precision of inducing individual students' learning strategies.","In this article, we propose the homomorphic POMDP (H-POMDP) model to accommodate multiple cognitive patterns and present the parameter learning approach to automatically construct the H-POMDP model.","Based on the H-POMDP model, we are able to represent different cognitive patterns from the data and induce more personalized learning strategies for individual students.","We conduct experiments to show that, in comparison to the general POMDP approach, the H-POMDP model demonstrates better precision when modelling mixed data from multiple cognitive patterns.","Moreover, the learning strategies derived from H-POMDPs exhibit better personalization in the performance evaluation."],"url":"http://arxiv.org/abs/2403.10930v1","category":"cs.AI"}
{"created":"2024-03-16 13:35:15","title":"Interpretable Machine Learning for TabPFN","abstract":"The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoiding approximate retraining and enables the use of Leave-One-Covariate-Out (LOCO) even when working with large-scale Transformers. In addition, we demonstrate how data valuation methods can be used to address scalability challenges of TabPFN. Our proposed methods are implemented in a package tabpfn_iml and made available at https://github.com/david-rundel/tabpfn_iml.","sentences":["The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes.","The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning.","This makes TabPFN a very attractive option for a wide range of domain applications.","However, a major drawback of the method is its lack of interpretability.","Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN.","By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations.","In particular, we show how in-context learning facilitates the estimation of Shapley values by avoiding approximate retraining and enables the use of Leave-One-Covariate-Out (LOCO) even when working with large-scale Transformers.","In addition, we demonstrate how data valuation methods can be used to address scalability challenges of TabPFN.","Our proposed methods are implemented in a package tabpfn_iml and made available at https://github.com/david-rundel/tabpfn_iml."],"url":"http://arxiv.org/abs/2403.10923v1","category":"cs.LG"}
{"created":"2024-03-16 13:32:40","title":"Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surfaces Empowered Cooperative Rate Splitting with User Relaying","abstract":"In this work, we unveil the advantages of synergizing cooperative rate splitting (CRS) with user relaying and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR RIS). Specifically, we propose a novel STAR RIS-assisted CRS transmission framework, featuring six unique transmission modes that leverage various combination of the relaying protocols (including full duplex-FD and half duplex-HD) and the STAR RIS configuration protocols (including energy splitting-ES, mode switching-MS, and time splitting-TS). With the objective of maximizing the minimum user rate, we then propose a unified successive convex approximation (SCA)-based alternative optimization (AO) algorithm to jointly optimize the transmit active beamforming, common rate allocation, STAR RIS passive beamforming, as well as time allocation (for HD or TS protocols) subject to the transmit power constraint at the base station (BS) and the law of energy conservation at the STAR RIS. To alleviate the computational burden, we further propose a low-complexity algorithm that incorporates a closed-form passive beamforming design. Numerical results show that our proposed framework significantly enhances user fairness compared with conventional CRS schemes without STAR RIS or other STAR RIS empowered multiple access schemes. Moreover, the proposed low-complexity algorithm dramatically reduces the computational complexity while achieving very close performance to the AO method.","sentences":["In this work, we unveil the advantages of synergizing cooperative rate splitting (CRS) with user relaying and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR RIS).","Specifically, we propose a novel STAR RIS-assisted CRS transmission framework, featuring six unique transmission modes that leverage various combination of the relaying protocols (including full duplex-FD and half duplex-HD) and the STAR RIS configuration protocols (including energy splitting-ES, mode switching-MS, and time splitting-TS).","With the objective of maximizing the minimum user rate, we then propose a unified successive convex approximation (SCA)-based alternative optimization (AO) algorithm to jointly optimize the transmit active beamforming, common rate allocation, STAR RIS passive beamforming, as well as time allocation (for HD or TS protocols) subject to the transmit power constraint at the base station (BS) and the law of energy conservation at the STAR RIS.","To alleviate the computational burden, we further propose a low-complexity algorithm that incorporates a closed-form passive beamforming design.","Numerical results show that our proposed framework significantly enhances user fairness compared with conventional CRS schemes without STAR RIS or other STAR RIS empowered multiple access schemes.","Moreover, the proposed low-complexity algorithm dramatically reduces the computational complexity while achieving very close performance to the AO method."],"url":"http://arxiv.org/abs/2403.10921v1","category":"cs.IT"}
{"created":"2024-03-16 11:44:55","title":"HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering","abstract":"Recent advancements in the Neural Radiance Field (NeRF) have bolstered its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge. Addressing this, we propose HourglassNeRF, an effective regularization-based approach with a novel hourglass casting strategy. Our proposed hourglass is conceptualized as a bundle of additional rays within the area between the original input ray and its corresponding reflection ray, by featurizing the conical frustum via Integrated Positional Encoding (IPE). This design expands the coverage of unseen views and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Furthermore, we propose luminance consistency regularization based on the Lambertian assumption, which is known to be effective for training a set of augmented rays under the few-shot setting. Leveraging the inherent property of a Lambertian surface, which retains consistent luminance irrespective of the viewing angle, we assume our proposed hourglass as a collection of flipped diffuse reflection rays and enhance the luminance consistency between the original input ray and its corresponding hourglass, resulting in more physically grounded training framework and performance improvement. Our HourglassNeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details. The code will be available.","sentences":["Recent advancements in the Neural Radiance Field (NeRF) have bolstered its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge.","Addressing this, we propose HourglassNeRF, an effective regularization-based approach with a novel hourglass casting strategy.","Our proposed hourglass is conceptualized as a bundle of additional rays within the area between the original input ray and its corresponding reflection ray, by featurizing the conical frustum via Integrated Positional Encoding (IPE).","This design expands the coverage of unseen views and enables an adaptive high-frequency regularization based on target pixel photo-consistency.","Furthermore, we propose luminance consistency regularization based on the Lambertian assumption, which is known to be effective for training a set of augmented rays under the few-shot setting.","Leveraging the inherent property of a Lambertian surface, which retains consistent luminance irrespective of the viewing angle, we assume our proposed hourglass as a collection of flipped diffuse reflection rays and enhance the luminance consistency between the original input ray and its corresponding hourglass, resulting in more physically grounded training framework and performance improvement.","Our HourglassNeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.","The code will be available."],"url":"http://arxiv.org/abs/2403.10906v1","category":"cs.CV"}
{"created":"2024-03-16 11:38:31","title":"DTOR: Decision Tree Outlier Regressor to explain anomalies","abstract":"Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approaches, the generated rules are consistently satisfied by the points to be explained. Furthermore, our evaluation metrics indicate comparable performance to Anchors in outlier explanation tasks, with reduced execution time.","sentences":["Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains.","Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts.","The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging.","We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model.","This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score.","Our results demonstrate the robustness of DTOR even in datasets with a large number of features.","Additionally, in contrast to other rule-based approaches, the generated rules are consistently satisfied by the points to be explained.","Furthermore, our evaluation metrics indicate comparable performance to Anchors in outlier explanation tasks, with reduced execution time."],"url":"http://arxiv.org/abs/2403.10903v1","category":"cs.LG"}
{"created":"2024-03-16 10:26:38","title":"Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean","abstract":"Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human evaluation and GPT4. Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.","sentences":["Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources.","Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs).","This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs.","First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness.","Second, bilingual data were used for pretraining to align the high- and less-resourced languages.","Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL.","The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks.","Furthermore, a qualitative assessment was performed based on human evaluation and GPT4.","Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models."],"url":"http://arxiv.org/abs/2403.10882v1","category":"cs.CL"}
{"created":"2024-03-16 10:25:49","title":"Regularizing CNNs using Confusion Penalty Based Label Smoothing for Histopathology Images","abstract":"Deep Learning, particularly Convolutional Neural Networks (CNN), has been successful in computer vision tasks and medical image analysis. However, modern CNNs can be overconfident, making them difficult to deploy in real-world scenarios. Researchers propose regularizing techniques, such as Label Smoothing (LS), which introduces soft labels for training data, making the classifier more regularized. LS captures disagreements or lack of confidence in the training phase, making the classifier more regularized. Although LS is quite simple and effective, traditional LS techniques utilize a weighted average between target distribution and a uniform distribution across the classes, which limits the objective of LS as well as the performance. This paper introduces a novel LS technique based on the confusion penalty, which treats model confusion for each class with more importance than others. We have performed extensive experiments with well-known CNN architectures with this technique on publicly available Colorectal Histology datasets and got satisfactory results. Also, we have compared our findings with the State-of-the-art and shown our method's efficacy with Reliability diagrams and t-distributed Stochastic Neighbor Embedding (t-SNE) plots of feature space.","sentences":["Deep Learning, particularly Convolutional Neural Networks (CNN), has been successful in computer vision tasks and medical image analysis.","However, modern CNNs can be overconfident, making them difficult to deploy in real-world scenarios.","Researchers propose regularizing techniques, such as Label Smoothing (LS), which introduces soft labels for training data, making the classifier more regularized.","LS captures disagreements or lack of confidence in the training phase, making the classifier more regularized.","Although LS is quite simple and effective, traditional LS techniques utilize a weighted average between target distribution and a uniform distribution across the classes, which limits the objective of LS as well as the performance.","This paper introduces a novel LS technique based on the confusion penalty, which treats model confusion for each class with more importance than others.","We have performed extensive experiments with well-known CNN architectures with this technique on publicly available Colorectal Histology datasets and got satisfactory results.","Also, we have compared our findings with the State-of-the-art and shown our method's efficacy with Reliability diagrams and t-distributed Stochastic Neighbor Embedding (t-SNE) plots of feature space."],"url":"http://arxiv.org/abs/2403.10881v1","category":"cs.CV"}
{"created":"2024-03-16 10:25:07","title":"COVID-CT-H-UNet: a novel COVID-19 CT segmentation network based on attention mechanism and Bi-category Hybrid loss","abstract":"Since 2019, the global COVID-19 outbreak has emerged as a crucial focus in healthcare research. Although RT-PCR stands as the primary method for COVID-19 detection, its extended detection time poses a significant challenge. Consequently, supplementing RT-PCR with the pathological study of COVID-19 through CT imaging has become imperative. The current segmentation approach based on TVLoss enhances the connectivity of afflicted areas. Nevertheless, it tends to misclassify normal pixels between certain adjacent diseased regions as diseased pixels. The typical Binary cross entropy(BCE) based U-shaped network only concentrates on the entire CT images without emphasizing on the affected regions, which results in hazy borders and low contrast in the projected output. In addition, the fraction of infected pixels in CT images is much less, which makes it a challenge for segmentation models to make accurate predictions. In this paper, we propose COVID-CT-H-UNet, a COVID-19 CT segmentation network to solve these problems. To recognize the unaffected pixels between neighbouring diseased regions, extra visual layer information is captured by combining the attention module on the skip connections with the proposed composite function Bi-category Hybrid Loss. The issue of hazy boundaries and poor contrast brought on by the BCE Loss in conventional techniques is resolved by utilizing the composite function Bi-category Hybrid Loss that concentrates on the pixels in the diseased area. The experiment shows when compared to the previous COVID-19 segmentation networks, the proposed COVID-CT-H-UNet's segmentation impact has greatly improved, and it may be used to identify and study clinical COVID-19.","sentences":["Since 2019, the global COVID-19 outbreak has emerged as a crucial focus in healthcare research.","Although RT-PCR stands as the primary method for COVID-19 detection, its extended detection time poses a significant challenge.","Consequently, supplementing RT-PCR with the pathological study of COVID-19 through CT imaging has become imperative.","The current segmentation approach based on TVLoss enhances the connectivity of afflicted areas.","Nevertheless, it tends to misclassify normal pixels between certain adjacent diseased regions as diseased pixels.","The typical Binary cross entropy(BCE) based U-shaped network only concentrates on the entire CT images without emphasizing on the affected regions, which results in hazy borders and low contrast in the projected output.","In addition, the fraction of infected pixels in CT images is much less, which makes it a challenge for segmentation models to make accurate predictions.","In this paper, we propose COVID-CT-H-UNet, a COVID-19 CT segmentation network to solve these problems.","To recognize the unaffected pixels between neighbouring diseased regions, extra visual layer information is captured by combining the attention module on the skip connections with the proposed composite function Bi-category Hybrid Loss.","The issue of hazy boundaries and poor contrast brought on by the BCE Loss in conventional techniques is resolved by utilizing the composite function Bi-category Hybrid Loss that concentrates on the pixels in the diseased area.","The experiment shows when compared to the previous COVID-19 segmentation networks, the proposed COVID-CT-H-UNet's segmentation impact has greatly improved, and it may be used to identify and study clinical COVID-19."],"url":"http://arxiv.org/abs/2403.10880v1","category":"eess.IV"}
{"created":"2024-03-16 10:16:49","title":"Characterizing the Solana NFT Ecosystem","abstract":"Non-Fungible Tokens (NFTs) are digital assets recorded on the blockchain, providing cryptographic proof of ownership over digital or physical items. Although Solana has only begun to gain popularity in recent years, its NFT market has seen substantial transaction volumes. In this paper, we conduct the first systematic research on the characteristics of Solana NFTs from two perspectives: longitudinal measurement and wash trading security audit. We gathered 132,736 Solana NFT from Solscan and analyzed the sales data within these collections. Investigating users' economic activity and NFT owner information reveals that the top users in Solana NFT are skewed toward a higher distribution of purchases. Subsequently, we employ the Local Outlier Factor algorithm to conduct a wash trading audit on 2,175 popular Solana NFTs. We discovered that 138 NFT pools are involved in wash trading, with 8 of these NFTs having a wash trading rate exceeding 50%. Fortunately, none of these NFTs have been entirely washed out.","sentences":["Non-Fungible Tokens (NFTs) are digital assets recorded on the blockchain, providing cryptographic proof of ownership over digital or physical items.","Although Solana has only begun to gain popularity in recent years, its NFT market has seen substantial transaction volumes.","In this paper, we conduct the first systematic research on the characteristics of Solana NFTs from two perspectives: longitudinal measurement and wash trading security audit.","We gathered 132,736 Solana NFT from Solscan and analyzed the sales data within these collections.","Investigating users' economic activity and NFT owner information reveals that the top users in Solana NFT are skewed toward a higher distribution of purchases.","Subsequently, we employ the Local Outlier Factor algorithm to conduct a wash trading audit on 2,175 popular Solana NFTs.","We discovered that 138 NFT pools are involved in wash trading, with 8 of these NFTs having a wash trading rate exceeding 50%.","Fortunately, none of these NFTs have been entirely washed out."],"url":"http://arxiv.org/abs/2403.10879v1","category":"cs.CR"}
{"created":"2024-03-16 10:08:24","title":"Test of lepton universality and measurement of the form factors of $D^0\\to K^{*}(892)^-\u03bc^+\u03bd_\u03bc$","abstract":"We report a first study of the semileptonic decay $D^0\\rightarrow K^-\\pi^0\\mu^{+}\\nu_{\\mu}$ by analyzing an $e^+e^-$ annihilation data sample of $7.9~\\mathrm{fb}^{-1}$ collected at the center-of-mass energy of 3.773 GeV with the BESIII detector. The absolute branching fraction of $D^0\\to K^-\\pi^0\\mu^{+}\\nu_{\\mu}$ is measured for the first time to be $(0.729 \\pm 0.014_{\\rm stat} \\pm 0.011_{\\rm syst})\\%$. Based on an amplitude analysis, the $S\\text{-}{\\rm wave}$ contribution is determined to be $(5.76 \\pm 0.35_{\\rm stat} \\pm 0.29_{\\rm syst})\\%$ of the total decay rate in addition to the dominated $K^{*}(892)^-$ component. The branching fraction of $D^0\\to K^{*}(892)^-\\mu^+\\nu_\\mu$ is given to be $(2.062 \\pm 0.039_{\\rm stat} \\pm 0.032_{\\rm syst})\\%$, which improves the precision of the world average by a factor of 5. Combining with the world average of ${\\mathcal B}(D^0\\to K^{*}(892)^-e^+\\nu_e)$, the ratio of the branching fractions obtained is $\\frac{{\\mathcal B}(D^0\\to K^{*}(892)^-\\mu^+\\nu_\\mu)}{{\\mathcal B}(D^0\\to K^{*}(892)^-e^+\\nu_e)} = 0.96\\pm0.08$, in agreement with lepton flavor universality. Furthermore, assuming single-pole dominance parameterization, the most precise hadronic form factor ratios for $D^0\\to K^{*}(892)^{-} \\mu^+\\nu_\\mu$ are extracted to be $r_{V}=V(0)/A_1(0)=1.37 \\pm 0.09_{\\rm stat} \\pm 0.03_{\\rm syst}$ and $r_{2}=A_2(0)/A_1(0)=0.76 \\pm 0.06_{\\rm stat} \\pm 0.02_{\\rm syst}$.","sentences":["We report a first study of the semileptonic decay $D^0\\rightarrow K^-\\pi^0\\mu^{+}\\nu_{\\mu}$ by analyzing an $e^+e^-$ annihilation data sample of $7.9~\\mathrm{fb}^{-1}$ collected at the center-of-mass energy of 3.773 GeV with the BESIII detector.","The absolute branching fraction of $D^0\\to K^-\\pi^0\\mu^{+}\\nu_{\\mu}$ is measured for the first time to be $(0.729 \\pm 0.014_{\\rm stat} \\pm 0.011_{\\rm syst})\\%$. Based on an amplitude analysis, the $S\\text{-}{\\rm wave}$ contribution is determined to be $(5.76 \\pm 0.35_{\\rm stat} \\pm 0.29_{\\rm syst})\\%$ of the total decay rate in addition to the dominated $K^{*}(892)^-$ component.","The branching fraction of $D^0\\to K^{*}(892)^-\\mu^+\\nu_\\mu$ is given to be $(2.062 \\pm 0.039_{\\rm stat} \\pm 0.032_{\\rm syst})\\%$, which improves the precision of the world average by a factor of 5.","Combining with the world average of ${\\mathcal B}(D^0\\to K^{*}(892)^-e^+\\nu_e)$, the ratio of the branching fractions obtained is $\\frac{{\\mathcal B}(D^0\\to K^{*}(892)^-\\mu^+\\nu_\\mu)}{{\\mathcal B}(D^0\\to K^{*}(892)^-e^+\\nu_e)}","= 0.96\\pm0.08$, in agreement with lepton flavor universality.","Furthermore, assuming single-pole dominance parameterization, the most precise hadronic form factor ratios for $D^0\\to K^{*}(892)^{-} \\mu^+\\nu_\\mu$ are extracted to be $r_{V}=V(0)/A_1(0)=1.37 \\pm 0.09_{\\rm stat} \\pm 0.03_{\\rm syst}$ and $r_{2}=A_2(0)/A_1(0)=0.76 \\pm 0.06_{\\rm stat} \\pm 0.02_{\\rm syst}$."],"url":"http://arxiv.org/abs/2403.10877v1","category":"hep-ex"}
{"created":"2024-03-16 09:06:38","title":"stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation","abstract":"Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \\textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance, with the unmasked data serving as conditions. Additionally, it utilizes a GNN encoder to integrate the spatial position information, thereby enhancing model performance. The results obtained from spatial transcriptomics datasets elucidate the performance of our methods relative to existing approaches.","sentences":["Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations.","However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values.","Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution.","To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution.","We introduce \\textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance, with the unmasked data serving as conditions.","Additionally, it utilizes a GNN encoder to integrate the spatial position information, thereby enhancing model performance.","The results obtained from spatial transcriptomics datasets elucidate the performance of our methods relative to existing approaches."],"url":"http://arxiv.org/abs/2403.10863v1","category":"q-bio.GN"}
{"created":"2024-03-16 08:57:00","title":"Efficient Domain Adaptation for Endoscopic Visual Odometry","abstract":"Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training and testing datasets. Experimental evaluations conducted on two public endoscope datasets showcase that our method achieves state-of-the-art accuracy in visual odometry tasks while boasting the fastest training speeds. These results demonstrate significant promise for intra-operative surgery applications.","sentences":["Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge.","Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information.","However, existing methodologies suffer from inefficiencies in the training time.","In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes.","For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration.","Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training and testing datasets.","Experimental evaluations conducted on two public endoscope datasets showcase that our method achieves state-of-the-art accuracy in visual odometry tasks while boasting the fastest training speeds.","These results demonstrate significant promise for intra-operative surgery applications."],"url":"http://arxiv.org/abs/2403.10860v1","category":"cs.CV"}
{"created":"2024-03-16 08:31:25","title":"Zero-shot Generative Linguistic Steganography","abstract":"Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces $1.926\\times$ more innocent and intelligible stegotext than any other method.","sentences":["Generative linguistic steganography attempts to hide secret messages into covertext.","Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans.","In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility.","We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext.","Our experimental results indicate that our method produces $1.926\\times$ more innocent and intelligible stegotext than any other method."],"url":"http://arxiv.org/abs/2403.10856v1","category":"cs.CL"}
{"created":"2024-03-16 08:28:42","title":"Just Say the Name: Online Continual Learning with Category Names Only via Data Generation","abstract":"In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distribution (ID) and Out-of-Distribution (OOD) generalization evaluations, compared to naive generator-ensembling, web-supervised, and manually annotated data.","sentences":["In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs.","Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns.","Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL).","The proposed G-NoCL uses a set of generators G along with the learner.","When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data.","Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distribution (ID) and Out-of-Distribution (OOD) generalization evaluations, compared to naive generator-ensembling, web-supervised, and manually annotated data."],"url":"http://arxiv.org/abs/2403.10853v1","category":"cs.LG"}
{"created":"2024-03-18 12:20:46","title":"Tight concentration inequalities for quantum adversarial setups exploiting permutation symmetry","abstract":"We developed new concentration inequalities for a quantum state on an $N$-qudit system or measurement outcomes on it that apply to an adversarial setup, where an adversary prepares the quantum state. Our one-sided concentration inequalities for a quantum state require the $N$-qudit system to be permutation invariant and are thus de-Finetti type, but they are tighter than the one previously obtained. We show that the bound can further be tightened if each qudit system has an additional symmetry. Furthermore, our concentration inequality for the outcomes of independent and identical measurements on an $N$-qudit quantum system has no assumption on the adversarial quantum state and is much tighter than the conventional one obtained through Azuma's inequality. We numerically demonstrate the tightness of our bounds in simple quantum information processing tasks.","sentences":["We developed new concentration inequalities for a quantum state on an $N$-qudit system or measurement outcomes on it that apply to an adversarial setup, where an adversary prepares the quantum state.","Our one-sided concentration inequalities for a quantum state require the $N$-qudit system to be permutation invariant and are thus de-Finetti type, but they are tighter than the one previously obtained.","We show that the bound can further be tightened if each qudit system has an additional symmetry.","Furthermore, our concentration inequality for the outcomes of independent and identical measurements on an $N$-qudit quantum system has no assumption on the adversarial quantum state and is much tighter than the conventional one obtained through Azuma's inequality.","We numerically demonstrate the tightness of our bounds in simple quantum information processing tasks."],"url":"http://arxiv.org/abs/2403.11719v1","category":"quant-ph"}
{"created":"2024-03-18 12:19:39","title":"The intertwining property for Laguerre processes with a fixed parameter","abstract":"We investigate the intertwining of Laguerre processes of parameter $\\alpha$ in different dimensions.   We introduce a Feller kernel that depends on $\\alpha $ and intertwines the $\\alpha$-Laguerre process in $N+1$ dimensions and that in $N$ dimensions.   When $\\alpha $ is a non-negative integer, the new kernel is interpreted in terms of the conditional distribution of the squared singular values: if the singular values of a unitarily invariant random matrix of order $(N+\\alpha +1) \\times (N+1)$ are fixed, then the those of its $(N+\\alpha) \\times N $ truncation matrix are given by the new kernel.","sentences":["We investigate the intertwining of Laguerre processes of parameter $\\alpha$ in different dimensions.   ","We introduce a Feller kernel that depends on $\\alpha $ and intertwines the $\\alpha$-Laguerre process in $N+1$ dimensions and that in $N$ dimensions.   ","When $\\alpha $ is a non-negative integer, the new kernel is interpreted in terms of the conditional distribution of the squared singular values: if the singular values of a unitarily invariant random matrix of order","$(N+\\alpha +1) \\times (N+1)$ are fixed, then the those of its $(N+\\alpha) \\times N $ truncation matrix are given by the new kernel."],"url":"http://arxiv.org/abs/2403.11718v1","category":"math.PR"}
{"created":"2024-03-18 12:18:51","title":"Models for Storage in Database Backends","abstract":"This paper describes ongoing work on developing a formal specification of a database backend. We present the formalisation of the expected behaviour of a basic transactional system that calls into a simple store API, and instantiate in two semantic models. The first one is a map-based, classical versioned key-value store; the second one, journal-based, appends individual transaction effects to a journal. We formalise a significant part of the specification in the Coq proof assistant. This work will form the basis for a formalisation of a full-fledged backend store with features such as caching or write-ahead logging, as variations on maps and journals.","sentences":["This paper describes ongoing work on developing a formal specification of a database backend.","We present the formalisation of the expected behaviour of a basic transactional system that calls into a simple store API, and instantiate in two semantic models.","The first one is a map-based, classical versioned key-value store; the second one, journal-based, appends individual transaction effects to a journal.","We formalise a significant part of the specification in the Coq proof assistant.","This work will form the basis for a formalisation of a full-fledged backend store with features such as caching or write-ahead logging, as variations on maps and journals."],"url":"http://arxiv.org/abs/2403.11716v1","category":"cs.DB"}
{"created":"2024-03-18 12:13:20","title":"New insights on fission of 235U induced by high energy neutrons","abstract":"The $^{235}$U(n,f) reaction cross section was measured relative to neutron-proton elastic scattering for the first time in the energy region from 10 MeV to 440 MeV at the CERN n\\_TOF facility. Two independent detection systems were used simultaneously to extract the fission cross section. For neutron energies below 200 MeV, the present results agree within one standard deviation with the data available in literature and are well reproduced by the IAEA evaluation. Above 200 MeV, the comparison of model calculations to the present data indicates the need to introduce a transient time in the description of the neutron-induced fission process in order to allow the simultaneous description of (n,f) and (p,f) reactions.","sentences":["The $^{235}$U(n,f) reaction cross section was measured relative to neutron-proton elastic scattering for the first time in the energy region from 10 MeV to 440 MeV at the CERN n\\_TOF facility.","Two independent detection systems were used simultaneously to extract the fission cross section.","For neutron energies below 200 MeV, the present results agree within one standard deviation with the data available in literature and are well reproduced by the IAEA evaluation.","Above 200 MeV, the comparison of model calculations to the present data indicates the need to introduce a transient time in the description of the neutron-induced fission process in order to allow the simultaneous description of (n,f) and (p,f) reactions."],"url":"http://arxiv.org/abs/2403.11711v1","category":"nucl-ex"}
{"created":"2024-03-18 12:07:46","title":"Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach","abstract":"Frustrated itinerant magnets often exhibit complex noncollinear or noncoplanar magnetic orders which support topological electronic structures. A canonical example is the anomalous quantum Hall state with a chiral spin order stabilized by electron-spin interactions on a triangular lattice. While a long-range magnetic order cannot survive thermal fluctuations in two dimensions, the chiral order which results from the breaking of a discrete Ising symmetry persists even at finite temperatures. We present a scalable machine learning (ML) framework to model the complex electron-mediated spin-spin interactions that stabilize the chiral magnetic domains in a triangular lattice. Large-scale dynamical simulations, enabled by the ML force-field models, are performed to investigate the coarsening of chiral domains after a thermal quench. While the chiral phase is described by a broken $Z_2$ Ising-type symmetry, we find that the characteristic size of chiral domains increases linearly with time, in stark contrast to the expected Allen-Cahn domain growth law for a non-conserved Ising order parameter field. The linear growth of the chiral domains is attributed to the orientational anisotropy of domain boundaries. Our work also demonstrates the promising potential of ML models for large-scale spin dynamics of itinerant magnets.","sentences":["Frustrated itinerant magnets often exhibit complex noncollinear or noncoplanar magnetic orders which support topological electronic structures.","A canonical example is the anomalous quantum Hall state with a chiral spin order stabilized by electron-spin interactions on a triangular lattice.","While a long-range magnetic order cannot survive thermal fluctuations in two dimensions, the chiral order which results from the breaking of a discrete Ising symmetry persists even at finite temperatures.","We present a scalable machine learning (ML) framework to model the complex electron-mediated spin-spin interactions that stabilize the chiral magnetic domains in a triangular lattice.","Large-scale dynamical simulations, enabled by the ML force-field models, are performed to investigate the coarsening of chiral domains after a thermal quench.","While the chiral phase is described by a broken $Z_2$ Ising-type symmetry, we find that the characteristic size of chiral domains increases linearly with time, in stark contrast to the expected Allen-Cahn domain growth law for a non-conserved Ising order parameter field.","The linear growth of the chiral domains is attributed to the orientational anisotropy of domain boundaries.","Our work also demonstrates the promising potential of ML models for large-scale spin dynamics of itinerant magnets."],"url":"http://arxiv.org/abs/2403.11705v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 11:58:36","title":"Memory formation in dense persistent active matter","abstract":"Protocol-dependent states in structural glasses can encode a disordered, yet retrievable memory. While training such materials is typically done via a global drive, such as external shear, in dense active matter the driving is instead local and spatio-temporally correlated. Here we focus on the impact of such spatial correlation on memory formation. We investigate the mechanical response of a dense amorphous packing of athermal particles, subject to an oscillatory quasistatic driving with a tunable spatial correlation, akin to the instantaneous driving pattern in active matter. We find that the capacity to encode memory can be rendered comparable upon a proper rescaling on the spatial correlation, whereas the efficiency in memory formation increases with motion cooperativity.","sentences":["Protocol-dependent states in structural glasses can encode a disordered, yet retrievable memory.","While training such materials is typically done via a global drive, such as external shear, in dense active matter the driving is instead local and spatio-temporally correlated.","Here we focus on the impact of such spatial correlation on memory formation.","We investigate the mechanical response of a dense amorphous packing of athermal particles, subject to an oscillatory quasistatic driving with a tunable spatial correlation, akin to the instantaneous driving pattern in active matter.","We find that the capacity to encode memory can be rendered comparable upon a proper rescaling on the spatial correlation, whereas the efficiency in memory formation increases with motion cooperativity."],"url":"http://arxiv.org/abs/2403.11701v1","category":"cond-mat.soft"}
{"created":"2024-03-18 11:55:19","title":"Spin-Orbital Ordering in Alkali Superoxides","abstract":"Akali superoxides AO2 (A=Na, K, Rb, Cs), due to an open p shell of the oxygen ion O2^- with degenerate pi orbitals, have spin and orbital degrees of freedom. The complex magnetic, orbital, and structural phase transitions observed experimentally in this family of materials are only partially understood. Based on density functional theory, we derive a strong-coupling effective model for the isostructural compounds AO2 (A=K, Rb, Cs) from a two-orbital Hubbard model. We find that CsO2 has highly frustrated exchange interactions in the a-b plane, while the frustration is weaker for smaller alkali ions. We solve the resulting Kugel-Khomskii model in the mean-field approximation. We show that CsO2 exhibits an antiferro-orbital (AFO) order with the ordering vector q=(1,0,0) and a stripe antiferromagnetic order with q=(1/2,0,0), which is consistent with recent neutron scattering experiments. We discuss the role of the pi-orbital degrees of freedom for the experimentally observed magnetic transitions and interpret the as-yet-unidentified T_s2=70K transition in CsO2 as an orbital ordering transition.","sentences":["Akali superoxides AO2 (A=Na, K, Rb, Cs), due to an open p shell of the oxygen ion O2^- with degenerate pi orbitals, have spin and orbital degrees of freedom.","The complex magnetic, orbital, and structural phase transitions observed experimentally in this family of materials are only partially understood.","Based on density functional theory, we derive a strong-coupling effective model for the isostructural compounds AO2 (A=K, Rb, Cs) from a two-orbital Hubbard model.","We find that CsO2 has highly frustrated exchange interactions in the a-b plane, while the frustration is weaker for smaller alkali ions.","We solve the resulting Kugel-Khomskii model in the mean-field approximation.","We show that CsO2 exhibits an antiferro-orbital (AFO) order with the ordering vector q=(1,0,0) and a stripe antiferromagnetic order with q=(1/2,0,0), which is consistent with recent neutron scattering experiments.","We discuss the role of the pi-orbital degrees of freedom for the experimentally observed magnetic transitions and interpret the as-yet-unidentified T_s2=70K transition in CsO2 as an orbital ordering transition."],"url":"http://arxiv.org/abs/2403.11698v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 11:48:41","title":"TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction","abstract":"Autonomous driving systems are a rapidly evolving technology that enables driverless car production. Trajectory prediction is a critical component of autonomous driving systems, enabling cars to anticipate the movements of surrounding objects for safe navigation. Trajectory prediction using Lidar point-cloud data performs better than 2D images due to providing 3D information. However, processing point-cloud data is more complicated and time-consuming than 2D images. Hence, state-of-the-art 3D trajectory predictions using point-cloud data suffer from slow and erroneous predictions. This paper introduces TrajectoryNAS, a pioneering method that focuses on utilizing point cloud data for trajectory prediction. By leveraging Neural Architecture Search (NAS), TrajectoryNAS automates the design of trajectory prediction models, encompassing object detection, tracking, and forecasting in a cohesive manner. This approach not only addresses the complex interdependencies among these tasks but also emphasizes the importance of accuracy and efficiency in trajectory modeling. Through empirical studies, TrajectoryNAS demonstrates its effectiveness in enhancing the performance of autonomous driving systems, marking a significant advancement in the field.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8 higger accuracy and 1.1* lower latency over competing methods on the NuScenes dataset.","sentences":["Autonomous driving systems are a rapidly evolving technology that enables driverless car production.","Trajectory prediction is a critical component of autonomous driving systems, enabling cars to anticipate the movements of surrounding objects for safe navigation.","Trajectory prediction using Lidar point-cloud data performs better than 2D images due to providing 3D information.","However, processing point-cloud data is more complicated and time-consuming than 2D images.","Hence, state-of-the-art 3D trajectory predictions using point-cloud data suffer from slow and erroneous predictions.","This paper introduces TrajectoryNAS, a pioneering method that focuses on utilizing point cloud data for trajectory prediction.","By leveraging Neural Architecture Search (NAS), TrajectoryNAS automates the design of trajectory prediction models, encompassing object detection, tracking, and forecasting in a cohesive manner.","This approach not only addresses the complex interdependencies among these tasks but also emphasizes the importance of accuracy and efficiency in trajectory modeling.","Through empirical studies, TrajectoryNAS demonstrates its effectiveness in enhancing the performance of autonomous driving systems, marking a significant advancement in the field.","Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8 higger accuracy and 1.1* lower latency over competing methods on the NuScenes dataset."],"url":"http://arxiv.org/abs/2403.11695v1","category":"cs.CV"}
{"created":"2024-03-18 11:48:20","title":"Object Segmentation-Assisted Inter Prediction for Versatile Video Coding","abstract":"In modern video coding standards, block-based inter prediction is widely adopted, which brings high compression efficiency. However, in natural videos, there are usually multiple moving objects of arbitrary shapes, resulting in complex motion fields that are difficult to compactly represent. This problem has been tackled by more flexible block partitioning methods in the Versatile Video Coding (VVC) standard, but the more flexible partitions require more overhead bits to signal and still cannot be made arbitrary shaped. To address this limitation, we propose an object segmentation-assisted inter prediction method (SAIP), where objects in the reference frames are segmented by some advanced technologies. With a proper indication, the object segmentation mask is translated from the reference frame to the current frame as the arbitrary-shaped partition of different regions without any extra signal. Using the segmentation mask, motion compensation is separately performed for different regions, achieving higher prediction accuracy. The segmentation mask is further used to code the motion vectors of different regions more efficiently. Moreover, segmentation mask is considered in the joint rate-distortion optimization for motion estimation and partition estimation to derive the motion vector of different regions and partition more accurately. The proposed method is implemented into the VVC reference software, VTM version 12.0. Experimental results show that the proposed method achieves up to 1.98%, 1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common test sequences, under the Low-delay P, Low-delay B, and Random Access configurations, respectively.","sentences":["In modern video coding standards, block-based inter prediction is widely adopted, which brings high compression efficiency.","However, in natural videos, there are usually multiple moving objects of arbitrary shapes, resulting in complex motion fields that are difficult to compactly represent.","This problem has been tackled by more flexible block partitioning methods in the Versatile Video Coding (VVC) standard, but the more flexible partitions require more overhead bits to signal and still cannot be made arbitrary shaped.","To address this limitation, we propose an object segmentation-assisted inter prediction method (SAIP), where objects in the reference frames are segmented by some advanced technologies.","With a proper indication, the object segmentation mask is translated from the reference frame to the current frame as the arbitrary-shaped partition of different regions without any extra signal.","Using the segmentation mask, motion compensation is separately performed for different regions, achieving higher prediction accuracy.","The segmentation mask is further used to code the motion vectors of different regions more efficiently.","Moreover, segmentation mask is considered in the joint rate-distortion optimization for motion estimation and partition estimation to derive the motion vector of different regions and partition more accurately.","The proposed method is implemented into the VVC reference software, VTM version 12.0.","Experimental results show that the proposed method achieves up to 1.98%, 1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common test sequences, under the Low-delay P, Low-delay B, and Random Access configurations, respectively."],"url":"http://arxiv.org/abs/2403.11694v1","category":"eess.IV"}
{"created":"2024-03-18 11:45:43","title":"ARTEMIS emulator: exploring the effect of cosmology and galaxy formation physics on Milky Way-mass haloes and their satellites","abstract":"We present the new ARTEMIS emulator suite of high resolution (baryon mass of $2.23 \\times 10^{4}$ $h^{-1}$M$_{\\odot}$) zoom-in simulations of Milky Way mass systems. Here, three haloes from the original ARTEMIS sample have been rerun multiple times, systematically varying parameters for the stellar feedback model, the density threshold for star formation, the reionisation redshift and the assumed warm dark matter (WDM) particle mass (assuming a thermal relic). From these simulations emulators are trained for a wide range of statistics that allow for fast predictions at combinations of parameters not originally sampled, running in $\\sim 1$ms (a factor of $\\sim 10^{11}$ faster than the simulations). In this paper we explore the dependence of the central haloes' stellar mass on the varied parameters, finding the stellar feedback parameters to be the most important. When constraining the parameters to match the present-day stellar mass halo mass relation inferred from abundance matching we find that there is a strong degeneracy in the stellar feedback parameters, corresponding to a freedom in formation time of the stellar component for a fixed halo assembly history. We additionally explore the dependence of the satellite stellar mass function, where it is found that variations in stellar feedback, the reionisation redshift and the WDM mass all have a significant effect. The presented emulators are a powerful tool which allows for fundamentally new ways of analysing and interpreting cosmological hydrodynamic simulations. Crucially, allowing their free (subgrid) parameters to be varied and marginalised, leading to more robust constraints and predictions.","sentences":["We present the new ARTEMIS emulator suite of high resolution (baryon mass of $2.23 \\times 10^{4}$ $h^{-1}$M$_{\\odot}$) zoom-in simulations of Milky Way mass systems.","Here, three haloes from the original ARTEMIS sample have been rerun multiple times, systematically varying parameters for the stellar feedback model, the density threshold for star formation, the reionisation redshift and the assumed warm dark matter (WDM) particle mass (assuming a thermal relic).","From these simulations emulators are trained for a wide range of statistics that allow for fast predictions at combinations of parameters not originally sampled, running in $\\sim 1$ms (a factor of $\\sim 10^{11}$ faster than the simulations).","In this paper we explore the dependence of the central haloes' stellar mass on the varied parameters, finding the stellar feedback parameters to be the most important.","When constraining the parameters to match the present-day stellar mass halo mass relation inferred from abundance matching we find that there is a strong degeneracy in the stellar feedback parameters, corresponding to a freedom in formation time of the stellar component for a fixed halo assembly history.","We additionally explore the dependence of the satellite stellar mass function, where it is found that variations in stellar feedback, the reionisation redshift and the WDM mass all have a significant effect.","The presented emulators are a powerful tool which allows for fundamentally new ways of analysing and interpreting cosmological hydrodynamic simulations.","Crucially, allowing their free (subgrid) parameters to be varied and marginalised, leading to more robust constraints and predictions."],"url":"http://arxiv.org/abs/2403.11692v1","category":"astro-ph.GA"}
{"created":"2024-03-18 11:38:20","title":"Layer dependent topological phases and transitions in TaRhTe$_4$: From monolayer and bilayer to bulk","abstract":"The recently synthesized ternary quasi-2D material TaRhTe$_4$ is a bulk Weyl semimetal with an intrinsically layered structure, which poses the question how the topology of its electronic structure depends on layers separations. Experimentally these separations may be changed for instance by intercalation of the bulk, or by exfoliation to reach monolayer or few-layer structures. Here we show that in the monolayer limit a quantum spin Hall insulator (QSHI) state emerges, employing density functional calculations as well as a minimal four-orbital tight-binding model that we develop. Even for weak spin-orbit couplings the QSHI is present, which has an interesting edge state that features Rashba-split bands with quadratic band minima. Further we find that a weak topological insulator (WTI) manifests in the bilayer system due to sizable intralayer hopping, contrary to the common lore that only weak interlayer interactions between stacked QSHIs lead to WTIs. Stacked bilayers give rise to a phase diagram as function of the interlayer separation that comprises a Weyl semimetal, WTI and normal insulator phases. These insights on the evolution of topology with dimension can be transferred to the family of layered ternary transition metal tellurides.","sentences":["The recently synthesized ternary quasi-2D material TaRhTe$_4$ is a bulk Weyl semimetal with an intrinsically layered structure, which poses the question how the topology of its electronic structure depends on layers separations.","Experimentally these separations may be changed for instance by intercalation of the bulk, or by exfoliation to reach monolayer or few-layer structures.","Here we show that in the monolayer limit a quantum spin Hall insulator (QSHI) state emerges, employing density functional calculations as well as a minimal four-orbital tight-binding model that we develop.","Even for weak spin-orbit couplings the QSHI is present, which has an interesting edge state that features Rashba-split bands with quadratic band minima.","Further we find that a weak topological insulator (WTI) manifests in the bilayer system due to sizable intralayer hopping, contrary to the common lore that only weak interlayer interactions between stacked QSHIs lead to WTIs.","Stacked bilayers give rise to a phase diagram as function of the interlayer separation that comprises a Weyl semimetal, WTI and normal insulator phases.","These insights on the evolution of topology with dimension can be transferred to the family of layered ternary transition metal tellurides."],"url":"http://arxiv.org/abs/2403.11688v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 11:37:08","title":"Berezinskii-Kosterlitz-Thouless to BCS-like superconducting transition crossover driven by weak magnetic fields in ultra-thin NbN films","abstract":"The Berezinskii-Kosterlitz-Thouless (BKT) transition in ultra-thin NbN films is investigated in the presence of weak perpendicular magnetic fields. A jump in the phase stiffness at the BKT transition is detected up to 5 G, while the BKT features are smeared between 5 G and 50 G, disappearing altogether at 100 G, where conventional current-voltage behaviour is observed. Our findings demonstrate that weak magnetic fields, insignificant in bulk systems, deeply affect our ultra-thin system, promoting a crossover from Halperin-Nelson fluctuations to a BCS-like state with Ginzburg-Landau fluctuations, as the field increases. This behavior is related to field-induced free vortices that screen the vortex-antivortex interaction and smear the BKT transition.","sentences":["The Berezinskii-Kosterlitz-Thouless (BKT) transition in ultra-thin NbN films is investigated in the presence of weak perpendicular magnetic fields.","A jump in the phase stiffness at the BKT transition is detected up to 5 G, while the BKT features are smeared between 5 G and 50 G, disappearing altogether at 100 G, where conventional current-voltage behaviour is observed.","Our findings demonstrate that weak magnetic fields, insignificant in bulk systems, deeply affect our ultra-thin system, promoting a crossover from Halperin-Nelson fluctuations to a BCS-like state with Ginzburg-Landau fluctuations, as the field increases.","This behavior is related to field-induced free vortices that screen the vortex-antivortex interaction and smear the BKT transition."],"url":"http://arxiv.org/abs/2403.11685v1","category":"cond-mat.supr-con"}
{"created":"2024-03-18 11:36:32","title":"An in-depth analysis of the variable cyclotron lines in GX 301$-$2","abstract":"Context. The High-Mass X-ray Binary (HMXB) system GX 301$-$2 is a persistent source with a well-known variable cyclotron line centered at 35 keV. Recently, a second cyclotron line at 50 keV has been reported with a presumably different behavior than the 35 keV line.   Aims. We investigate the presence of the newly discovered cyclotron line in the phase-averaged and phase-resolved spectra at higher luminosities than before. We further aim to determine the pulse-phase variability of both lines.   Methods. We analyze a NuSTAR observation of GX 301$-$2 covering the pre-periastron flare, where the source luminosity reached its peak of ${\\sim} 4 \\times 10^{37}\\,\\mathrm{erg}\\,\\mathrm{s}^{-1}$ in the 5-50 keV range. We analyze the phase-averaged spectra in the NuSTAR energy range from 3.5-79 keV for both the complete observation and three time segments of it. We further analyze the phase-resolved spectra and the pulse-phase variability of continuum and cyclotron line parameters.   Results. We confirm that the description of the phase-averaged spectrum requires a second absorption feature at $51.5^{+1.1}_{-1.0}$ keV besides the established line at 35 keV. The statistical significance of this feature in the phase-averaged spectrum is $>99.999\\%$. We further find that the 50 keV cyclotron line is present in three of eight phase bins.   Conclusions. Based on the results of our analysis, we confirm that the detected absorption feature is very likely to be a cyclotron line. We discuss a variety of physical scenarios which could explain the proposed anharmonicity, but also outline circumstances under which the lines are harmonically related. We further present the cyclotron line history of GX 301$-$2 and evaluate concordance among each other. We also discuss an alternative spectral model including cyclotron line emission wings.","sentences":["Context.","The High-Mass X-ray Binary (HMXB) system GX 301$-$2 is a persistent source with a well-known variable cyclotron line centered at 35 keV.","Recently, a second cyclotron line at 50 keV has been reported with a presumably different behavior than the 35 keV line.   ","Aims.","We investigate the presence of the newly discovered cyclotron line in the phase-averaged and phase-resolved spectra at higher luminosities than before.","We further aim to determine the pulse-phase variability of both lines.   ","Methods.","We analyze a NuSTAR observation of GX 301$-$2 covering the pre-periastron flare, where the source luminosity reached its peak of ${\\sim} 4 \\times 10^{37}\\,\\mathrm{erg}\\,\\mathrm{s}^{-1}$ in the 5-50 keV range.","We analyze the phase-averaged spectra in the NuSTAR energy range from 3.5-79 keV for both the complete observation and three time segments of it.","We further analyze the phase-resolved spectra and the pulse-phase variability of continuum and cyclotron line parameters.   Results.","We confirm that the description of the phase-averaged spectrum requires a second absorption feature at $51.5^{+1.1}_{-1.0}$ keV besides the established line at 35 keV.","The statistical significance of this feature in the phase-averaged spectrum is $>99.999\\%$.","We further find that the 50 keV cyclotron line is present in three of eight phase bins.   ","Conclusions.","Based on the results of our analysis, we confirm that the detected absorption feature is very likely to be a cyclotron line.","We discuss a variety of physical scenarios which could explain the proposed anharmonicity, but also outline circumstances under which the lines are harmonically related.","We further present the cyclotron line history of GX 301$-$2 and evaluate concordance among each other.","We also discuss an alternative spectral model including cyclotron line emission wings."],"url":"http://arxiv.org/abs/2403.11682v1","category":"astro-ph.HE"}
{"created":"2024-03-18 11:31:03","title":"NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting","abstract":"We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time. In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption. Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View Pruning method to eliminate outlier GS points, thereby effectively enhancing the quality of scene representations. Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping.","sentences":["We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time.","In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping.","Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption.","Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View Pruning method to eliminate outlier GS points, thereby effectively enhancing the quality of scene representations.","Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping."],"url":"http://arxiv.org/abs/2403.11679v1","category":"cs.CV"}
{"created":"2024-03-18 11:29:43","title":"Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes","abstract":"We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes. We combine two techniques to improve the required training time and memory cost per scene. First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned. Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene. Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes. Our project page can be found at https://3da-ae.github.io .","sentences":["We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes.","We combine two techniques to improve the required training time and memory cost per scene.","First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned.","Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene.","Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.","Our project page can be found at https://3da-ae.github.io ."],"url":"http://arxiv.org/abs/2403.11678v1","category":"cs.CV"}
{"created":"2024-03-18 11:23:36","title":"Prismatic crystals and $q$-Higgs fields","abstract":"Similarly to the theory of crystalline cohomology, we give a local description of a prismatic crystal and its cohomology in terms of a $q$-Higgs module and the associated $q$-Higgs complex on the bounded prismatic envelope of an embedding into a framed smooth algebra, when the base bounded prism is defined over the $q$-crystalline prism. We also discuss its behavior under the scalar extension by the Frobenius lifting and tensor products, and a global description of the cohomology of a prismatic crystal via Zariski cohomological descent.","sentences":["Similarly to the theory of crystalline cohomology, we give a local description of a prismatic crystal and its cohomology in terms of a $q$-Higgs module and the associated $q$-Higgs complex on the bounded prismatic envelope of an embedding into a framed smooth algebra, when the base bounded prism is defined over the $q$-crystalline prism.","We also discuss its behavior under the scalar extension by the Frobenius lifting and tensor products, and a global description of the cohomology of a prismatic crystal via Zariski cohomological descent."],"url":"http://arxiv.org/abs/2403.11676v1","category":"math.AG"}
{"created":"2024-03-18 11:23:02","title":"Better (pseudo-)labels for semi-supervised instance segmentation","abstract":"Despite the availability of large datasets for tasks like image classification and image-text alignment, labeled data for more complex recognition tasks, such as detection and segmentation, is less abundant. In particular, for instance segmentation annotations are time-consuming to produce, and the distribution of instances is often highly skewed across classes. While semi-supervised teacher-student distillation methods show promise in leveraging vast amounts of unlabeled data, they suffer from miscalibration, resulting in overconfidence in frequently represented classes and underconfidence in rarer ones. Additionally, these methods encounter difficulties in efficiently learning from a limited set of examples. We introduce a dual-strategy to enhance the teacher model's training process, substantially improving the performance on few-shot learning. Secondly, we propose a calibration correction mechanism that that enables the student model to correct the teacher's calibration errors. Using our approach, we observed marked improvements over a state-of-the-art supervised baseline performance on the LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3% gain in AP for rare classes.","sentences":["Despite the availability of large datasets for tasks like image classification and image-text alignment, labeled data for more complex recognition tasks, such as detection and segmentation, is less abundant.","In particular, for instance segmentation annotations are time-consuming to produce, and the distribution of instances is often highly skewed across classes.","While semi-supervised teacher-student distillation methods show promise in leveraging vast amounts of unlabeled data, they suffer from miscalibration, resulting in overconfidence in frequently represented classes and underconfidence in rarer ones.","Additionally, these methods encounter difficulties in efficiently learning from a limited set of examples.","We introduce a dual-strategy to enhance the teacher model's training process, substantially improving the performance on few-shot learning.","Secondly, we propose a calibration correction mechanism that that enables the student model to correct the teacher's calibration errors.","Using our approach, we observed marked improvements over a state-of-the-art supervised baseline performance on the LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3% gain in AP for rare classes."],"url":"http://arxiv.org/abs/2403.11675v1","category":"cs.CV"}
{"created":"2024-03-18 11:18:33","title":"Advancing Quantum Software Engineering: A Vision of Hybrid Full-Stack Iterative Model","abstract":"This paper introduces a vision for Quantum Software Develop- ment lifecycle, proposing a hybrid full-stack iterative model that integrates quantum and classical computing. Addressing the cur- rent challenges in Quantum Computing (QC) such as the need for integrating diverse programming languages and managing the complexities of quantum-classical systems, this model is rooted in the principles of DevOps and continuous software engineering. It presents a comprehensive lifecycle for quantum software develop- ment, encompassing quantum-agnostic coding, testing, deployment, cloud computing services, orchestration, translation, execution, and interpretation phases. Each phase is designed to accommodate the unique demands of QC, enabling traditional software developers to engage with QC environments without needing in-depth QC expertise. The paper presents a detailed implementation roadmap, utilizing a range of existing tools and frameworks, thereby making quantum software development more accessible and efficient. The proposed model not only addresses current challenges in quantum software development but also makes a substantial contribution to the field of Quantum Software Engineering (QSE). By propos- ing a structured and accessible model, it sets the stage for further advancements and research in QSE, enhancing its practicality and relevance in a wide range of applications.","sentences":["This paper introduces a vision for Quantum Software Develop- ment lifecycle, proposing a hybrid full-stack iterative model that integrates quantum and classical computing.","Addressing the cur- rent challenges in Quantum Computing (QC) such as the need for integrating diverse programming languages and managing the complexities of quantum-classical systems, this model is rooted in the principles of DevOps and continuous software engineering.","It presents a comprehensive lifecycle for quantum software develop- ment, encompassing quantum-agnostic coding, testing, deployment, cloud computing services, orchestration, translation, execution, and interpretation phases.","Each phase is designed to accommodate the unique demands of QC, enabling traditional software developers to engage with QC environments without needing in-depth QC expertise.","The paper presents a detailed implementation roadmap, utilizing a range of existing tools and frameworks, thereby making quantum software development more accessible and efficient.","The proposed model not only addresses current challenges in quantum software development but also makes a substantial contribution to the field of Quantum Software Engineering (QSE).","By propos- ing a structured and accessible model, it sets the stage for further advancements and research in QSE, enhancing its practicality and relevance in a wide range of applications."],"url":"http://arxiv.org/abs/2403.11670v1","category":"cs.SE"}
{"created":"2024-03-18 11:12:39","title":"Normalized Validity Scores for DNNs in Regression based Eye Feature Extraction","abstract":"We propose an improvement to the landmark validity loss. Landmark detection is widely used in head pose estimation, eyelid shape extraction, as well as pupil and iris segmentation. There are numerous additional applications where landmark detection is used to estimate the shape of complex objects. One part of this process is the accurate and fine-grained detection of the shape. The other part is the validity or inaccuracy per landmark, which can be used to detect unreliable areas, where the shape possibly does not fit, and to improve the accuracy of the entire shape extraction by excluding inaccurate landmarks. We propose a normalization in the loss formulation, which improves the accuracy of the entire approach due to the numerical balance of the normalized inaccuracy. In addition, we propose a margin for the inaccuracy to reduce the impact of gradients, which are produced by negligible errors close to the ground truth.","sentences":["We propose an improvement to the landmark validity loss.","Landmark detection is widely used in head pose estimation, eyelid shape extraction, as well as pupil and iris segmentation.","There are numerous additional applications where landmark detection is used to estimate the shape of complex objects.","One part of this process is the accurate and fine-grained detection of the shape.","The other part is the validity or inaccuracy per landmark, which can be used to detect unreliable areas, where the shape possibly does not fit, and to improve the accuracy of the entire shape extraction by excluding inaccurate landmarks.","We propose a normalization in the loss formulation, which improves the accuracy of the entire approach due to the numerical balance of the normalized inaccuracy.","In addition, we propose a margin for the inaccuracy to reduce the impact of gradients, which are produced by negligible errors close to the ground truth."],"url":"http://arxiv.org/abs/2403.11665v1","category":"cs.CV"}
{"created":"2024-03-18 11:04:21","title":"Combining Local and Global Perception for Autonomous Navigation on Nano-UAVs","abstract":"A critical challenge in deploying unmanned aerial vehicles (UAVs) for autonomous tasks is their ability to navigate in an unknown environment. This paper introduces a novel vision-depth fusion approach for autonomous navigation on nano-UAVs. We combine the visual-based PULP-Dronet convolutional neural network for semantic information extraction, i.e., serving as the global perception, with 8x8px depth maps for close-proximity maneuvers, i.e., the local perception. When tested in-field, our integration strategy highlights the complementary strengths of both visual and depth sensory information. We achieve a 100% success rate over 15 flights in a complex navigation scenario, encompassing straight pathways, static obstacle avoidance, and 90{\\deg} turns.","sentences":["A critical challenge in deploying unmanned aerial vehicles (UAVs) for autonomous tasks is their ability to navigate in an unknown environment.","This paper introduces a novel vision-depth fusion approach for autonomous navigation on nano-UAVs.","We combine the visual-based PULP-Dronet convolutional neural network for semantic information extraction, i.e., serving as the global perception, with 8x8px depth maps for close-proximity maneuvers, i.e., the local perception.","When tested in-field, our integration strategy highlights the complementary strengths of both visual and depth sensory information.","We achieve a 100% success rate over 15 flights in a complex navigation scenario, encompassing straight pathways, static obstacle avoidance, and 90{\\deg} turns."],"url":"http://arxiv.org/abs/2403.11661v1","category":"cs.RO"}
{"created":"2024-03-18 11:03:37","title":"Tailoring topological band properties of twisted double bilayer graphene: effects due to spin-orbit coupling","abstract":"Our theoretical study unfolds the topological phase transitions (within bands of the Moir\\'e super-lattice) in small angle twisted double bilayer graphene (tDBLG) under the influence of external gate voltage and intrinsic spin-orbit coupling (SOC) for both AB-AB and AB-BA stacking configurations. Utilizing a low-energy continuum model, we investigate the band structure and perform a comprehensive topological characterization of the system by analysing the direct band gap closing as well as various Chern numbers. In the absence of SOC, the tDBLG exhibits characteristics of a valley Hall insulator. However, in the presence of SOC, we observe a transition to a quantum spin Hall insulator state and band topology emerges in the parameter spaces of non-topological regime without SOC. Furthermore, we conduct a comparative analysis between untwisted double bilayer graphene and tDBLG to assess the impact of twisting on the system's properties. Our findings reveal the construction of topological phase diagrams that showcase distinct phases arising from changes in the twist angle compared to the untwisted case. These phase diagrams provide valuable insights into the diverse topological phases achievable in tDBLG with SOC. Our findings contribute to the understanding of the interplay between small twist angle, SOC, and external electric field on the topological band properties of twisted multilayer graphene systems.","sentences":["Our theoretical study unfolds the topological phase transitions (within bands of the Moir\\'e super-lattice) in small angle twisted double bilayer graphene (tDBLG) under the influence of external gate voltage and intrinsic spin-orbit coupling (SOC) for both AB-AB and AB-BA stacking configurations.","Utilizing a low-energy continuum model, we investigate the band structure and perform a comprehensive topological characterization of the system by analysing the direct band gap closing as well as various Chern numbers.","In the absence of SOC, the tDBLG exhibits characteristics of a valley Hall insulator.","However, in the presence of SOC, we observe a transition to a quantum spin Hall insulator state and band topology emerges in the parameter spaces of non-topological regime without SOC.","Furthermore, we conduct a comparative analysis between untwisted double bilayer graphene and tDBLG to assess the impact of twisting on the system's properties.","Our findings reveal the construction of topological phase diagrams that showcase distinct phases arising from changes in the twist angle compared to the untwisted case.","These phase diagrams provide valuable insights into the diverse topological phases achievable in tDBLG with SOC.","Our findings contribute to the understanding of the interplay between small twist angle, SOC, and external electric field on the topological band properties of twisted multilayer graphene systems."],"url":"http://arxiv.org/abs/2403.11660v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 10:53:00","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model","abstract":"Previous work has shown that well-crafted adversarial perturbations can threaten the security of video recognition systems. Attackers can invade such models with a low query budget when the perturbations are semantic-invariant, such as StyleFool. Despite the query efficiency, the naturalness of the minutia areas still requires amelioration, since StyleFool leverages style transfer to all pixels in each frame. To close the gap, we propose LocalStyleFool, an improved black-box video adversarial attack that superimposes regional style-transfer-based perturbations on videos. Benefiting from the popularity and scalably usability of Segment Anything Model (SAM), we first extract different regions according to semantic information and then track them through the video stream to maintain the temporal consistency. Then, we add style-transfer-based perturbations to several regions selected based on the associative criterion of transfer-based gradient information and regional area. Perturbation fine adjustment is followed to make stylized videos adversarial. We demonstrate that LocalStyleFool can improve both intra-frame and inter-frame naturalness through a human-assessed survey, while maintaining competitive fooling rate and query efficiency. Successful experiments on the high-resolution dataset also showcase that scrupulous segmentation of SAM helps to improve the scalability of adversarial attacks under high-resolution data.","sentences":["Previous work has shown that well-crafted adversarial perturbations can threaten the security of video recognition systems.","Attackers can invade such models with a low query budget when the perturbations are semantic-invariant, such as StyleFool.","Despite the query efficiency, the naturalness of the minutia areas still requires amelioration, since StyleFool leverages style transfer to all pixels in each frame.","To close the gap, we propose LocalStyleFool, an improved black-box video adversarial attack that superimposes regional style-transfer-based perturbations on videos.","Benefiting from the popularity and scalably usability of Segment Anything Model (SAM), we first extract different regions according to semantic information and then track them through the video stream to maintain the temporal consistency.","Then, we add style-transfer-based perturbations to several regions selected based on the associative criterion of transfer-based gradient information and regional area.","Perturbation fine adjustment is followed to make stylized videos adversarial.","We demonstrate that LocalStyleFool can improve both intra-frame and inter-frame naturalness through a human-assessed survey, while maintaining competitive fooling rate and query efficiency.","Successful experiments on the high-resolution dataset also showcase that scrupulous segmentation of SAM helps to improve the scalability of adversarial attacks under high-resolution data."],"url":"http://arxiv.org/abs/2403.11656v1","category":"cs.CV"}
{"created":"2024-03-18 10:48:17","title":"The temperature of the neutral Interstellar Medium in the Galaxy","abstract":"Atomic Hydrogen-21 cm transition (HI) is an excellent tracer to study and understand the properties of the atomic gas in the Galaxy. Using the Westerbork Synthesis Radio Telescope (WSRT), we observed 12 quasar sightlines to detect galactic HI in absorption. We achieve an optical depth RMS of $\\sim 1-2 \\times 10^{-3}$, essential to detect the Warm Neutral Medium (WNM). We detect HI absorption in all our sightlines except along 1006+349, for which we set a strict upper limit on the spin temperature as $\\langle T_s \\rangle > 570$ K. We find around 50\\% of our sightlines have $\\langle T_s \\rangle > 500$ K, indicating a WNM dominance. Further, we calculate an upper limit of the CNM fraction along our sightlines and find a median CNM fraction of $\\sim 0.12$. With our observations, we reconfirm the existence of a threshold column density of $\\sim 2 \\times 10^{20} \\ cm^{-2}$ to form CNM in the ISM. Using a two-temperature model of the HI disk, we explore the distribution of spin temperature in the Galactic ISM. We find that a simple fixed axisymmetric two-temperature model could not produce either the observed column density or the integral optical depth. This indicates the existence of a more complex distribution of spin temperatures in the Galaxy.","sentences":["Atomic Hydrogen-21 cm transition (HI) is an excellent tracer to study and understand the properties of the atomic gas in the Galaxy.","Using the Westerbork Synthesis Radio Telescope (WSRT), we observed 12 quasar sightlines to detect galactic HI in absorption.","We achieve an optical depth RMS of $\\sim 1-2 \\times 10^{-3}$, essential to detect the Warm Neutral Medium (WNM).","We detect HI absorption in all our sightlines except along 1006+349, for which we set a strict upper limit on the spin temperature as $\\langle T_s \\rangle >","570$","K. We find around 50\\% of our sightlines have $\\langle T_s \\rangle >","500$ K, indicating a WNM dominance.","Further, we calculate an upper limit of the CNM fraction along our sightlines and find a median CNM fraction of $\\sim 0.12$. With our observations, we reconfirm the existence of a threshold column density of $\\sim 2 \\times 10^{20} \\ cm^{-2}$ to form CNM in the ISM.","Using a two-temperature model of the HI disk, we explore the distribution of spin temperature in the Galactic ISM.","We find that a simple fixed axisymmetric two-temperature model could not produce either the observed column density or the integral optical depth.","This indicates the existence of a more complex distribution of spin temperatures in the Galaxy."],"url":"http://arxiv.org/abs/2403.11653v1","category":"astro-ph.GA"}
{"created":"2024-03-18 10:45:56","title":"Overfitted image coding at reduced complexity","abstract":"Overfitted image codecs offer compelling compression performance and low decoder complexity, through the overfitting of a lightweight decoder for each image. Such codecs include Cool-chic, which presents image coding performance on par with VVC while requiring around 2000 multiplications per decoded pixel. This paper proposes to decrease Cool-chic encoding and decoding complexity. The encoding complexity is reduced by shortening Cool-chic training, up to the point where no overfitting is performed at all. It is also shown that a tiny neural decoder with 300 multiplications per pixel still outperforms HEVC. A near real-time CPU implementation of this decoder is made available at https://orange-opensource.github.io/Cool-Chic/.","sentences":["Overfitted image codecs offer compelling compression performance and low decoder complexity, through the overfitting of a lightweight decoder for each image.","Such codecs include Cool-chic, which presents image coding performance on par with VVC while requiring around 2000 multiplications per decoded pixel.","This paper proposes to decrease Cool-chic encoding and decoding complexity.","The encoding complexity is reduced by shortening Cool-chic training, up to the point where no overfitting is performed at all.","It is also shown that a tiny neural decoder with 300 multiplications per pixel still outperforms HEVC.","A near real-time CPU implementation of this decoder is made available at https://orange-opensource.github.io/Cool-Chic/."],"url":"http://arxiv.org/abs/2403.11651v1","category":"eess.IV"}
{"created":"2024-03-18 10:21:05","title":"An Accurate and Real-time Relative Pose Estimation from Triple Point-line Images by Decoupling Rotation and Translation","abstract":"Line features are valid complements for point features in man-made environments. 3D-2D constraints provided by line features have been widely used in Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how to accurately solve three-view relative motion only with 2D observations of points and lines in real time has not been fully explored. In this paper, we propose a novel three-view pose solver based on rotation-translation decoupled estimation. First, a high-precision rotation estimation method based on normal vector coplanarity constraints that consider the uncertainty of observations is proposed, which can be solved by Levenberg-Marquardt (LM) algorithm efficiently. Second, a robust linear translation constraint that minimizes the degree of the rotation components and feature observation components in equations is elaborately designed for estimating translations accurately. Experiments on synthetic data and real-world data show that the proposed approach improves both rotation and translation accuracy compared to the classical trifocal-tensor-based method and the state-of-the-art two-view algorithm in outdoor and indoor environments.","sentences":["Line features are valid complements for point features in man-made environments.","3D-2D constraints provided by line features have been widely used in Visual Odometry (VO) and Structure-from-Motion (SfM) systems.","However, how to accurately solve three-view relative motion only with 2D observations of points and lines in real time has not been fully explored.","In this paper, we propose a novel three-view pose solver based on rotation-translation decoupled estimation.","First, a high-precision rotation estimation method based on normal vector coplanarity constraints that consider the uncertainty of observations is proposed, which can be solved by Levenberg-Marquardt (LM)","algorithm efficiently.","Second, a robust linear translation constraint that minimizes the degree of the rotation components and feature observation components in equations is elaborately designed for estimating translations accurately.","Experiments on synthetic data and real-world data show that the proposed approach improves both rotation and translation accuracy compared to the classical trifocal-tensor-based method and the state-of-the-art two-view algorithm in outdoor and indoor environments."],"url":"http://arxiv.org/abs/2403.11639v1","category":"cs.RO"}
{"created":"2024-03-18 10:18:29","title":"A restricted additive smoother for finite cell flow problems","abstract":"In this work, we propose an adaptive geometric multigrid method for the solution of large-scale finite cell flow problems. The finite cell method seeks to circumvent the need for a boundary-conforming mesh through the embedding of the physical domain in a regular background mesh. As a result of the intersection between the physical domain and the background computational mesh, the resultant systems of equations are typically numerically ill-conditioned, rendering the appropriate treatment of cutcells a crucial aspect of the solver. To this end, we propose a smoother operator with favorable parallel properties and discuss its memory footprint and parallelization aspects. We propose three cache policies that offer a balance between cached and on-the-fly computation and discuss the optimization opportunities offered by the smoother operator. It is shown that the smoother operator, on account of its additive nature, can be replicated in parallel exactly with little communication overhead, which offers a major advantage in parallel settings as the geometric multigrid solver is consequently independent of the number of processes. The convergence and scalability of the geometric multigrid method is studied using numerical examples. It is shown that the iteration count of the solver remains bounded independent of the problem size and depth of the grid hierarchy. The solver is shown to obtain excellent weak and strong scaling using numerical benchmarks with more than 665 million degrees of freedom. The presented geometric multigrid solver is, therefore, an attractive option for the solution of large-scale finite cell problems in massively parallel high-performance computing environments.","sentences":["In this work, we propose an adaptive geometric multigrid method for the solution of large-scale finite cell flow problems.","The finite cell method seeks to circumvent the need for a boundary-conforming mesh through the embedding of the physical domain in a regular background mesh.","As a result of the intersection between the physical domain and the background computational mesh, the resultant systems of equations are typically numerically ill-conditioned, rendering the appropriate treatment of cutcells a crucial aspect of the solver.","To this end, we propose a smoother operator with favorable parallel properties and discuss its memory footprint and parallelization aspects.","We propose three cache policies that offer a balance between cached and on-the-fly computation and discuss the optimization opportunities offered by the smoother operator.","It is shown that the smoother operator, on account of its additive nature, can be replicated in parallel exactly with little communication overhead, which offers a major advantage in parallel settings as the geometric multigrid solver is consequently independent of the number of processes.","The convergence and scalability of the geometric multigrid method is studied using numerical examples.","It is shown that the iteration count of the solver remains bounded independent of the problem size and depth of the grid hierarchy.","The solver is shown to obtain excellent weak and strong scaling using numerical benchmarks with more than 665 million degrees of freedom.","The presented geometric multigrid solver is, therefore, an attractive option for the solution of large-scale finite cell problems in massively parallel high-performance computing environments."],"url":"http://arxiv.org/abs/2403.11636v1","category":"math.NA"}
{"created":"2024-03-18 10:17:05","title":"The FAST Galactic Plane Pulsar Snapshot Survey: V. PSR J1901+0658 in a double neutron star system","abstract":"Double neutron star (DNS) systems offer excellent opportunities to test gravity theories. We report the timing results of a pulsar, PSR J1901+0658, discovered in the FAST Galactic Plane Pulsar Snapshot (GPPS) survey. Based on timing observations by FAST over 5 years, we obtain the phase-coherent timing solutions and derive the precise measurements of its position, spin parameters, orbital parameters, and also the dispersion measure (DM). It has a period of 75.7 ms, a period derivative of 2.169(6)$\\times 10^{-19}$ s s$^{-1}$, and the characteristic age of 5.5 Gyr. This pulsar is in an orbit with a period of 14.45 days and an eccentricity of 0.366. One post-Keplerian parameter, periastron advance, has been well measured as being 0.00531(9) deg/yr, from which the total mass of this system is derived to be 2.79(7) $M_{\\sun}$. The pulsar has the upper limit of mass of 1.68 $M_{\\sun}$, so the lower limit for the companion mass is 1.11 $M_{\\sun}$. Because PSR J1901+0658 is a partially recycled pulsar in an eccentric binary orbit with such a large companion mass, it should be in a DNS system according to the evolution history of the binary system.","sentences":["Double neutron star (DNS) systems offer excellent opportunities to test gravity theories.","We report the timing results of a pulsar, PSR J1901+0658, discovered in the FAST Galactic Plane Pulsar Snapshot (GPPS) survey.","Based on timing observations by FAST over 5 years, we obtain the phase-coherent timing solutions and derive the precise measurements of its position, spin parameters, orbital parameters, and also the dispersion measure (DM).","It has a period of 75.7 ms, a period derivative of 2.169(6)$\\times 10^{-19}$ s s$^{-1}$, and the characteristic age of 5.5 Gyr.","This pulsar is in an orbit with a period of 14.45 days and an eccentricity of 0.366.","One post-Keplerian parameter, periastron advance, has been well measured as being 0.00531(9) deg/yr, from which the total mass of this system is derived to be 2.79(7)","$M_{\\sun}$. The pulsar has the upper limit of mass of 1.68 $M_{\\sun}$, so the lower limit for the companion mass is 1.11 $M_{\\sun}$. Because PSR J1901","+0658 is a partially recycled pulsar in an eccentric binary orbit with such a large companion mass, it should be in a DNS system according to the evolution history of the binary system."],"url":"http://arxiv.org/abs/2403.11635v1","category":"astro-ph.HE"}
{"created":"2024-03-18 09:59:18","title":"Characteristics of the fission fragments in the three-body model of binary fission","abstract":"The evaluated nuclide yields, fragment mass and charge distributions, and averaged total kinetic energy of fission fragments for the neutron-induced fission of 30 actinide nuclei are well described in the new model, which considers the fissioning scission system consisting of two heavy fragments and an alpha-particle between them. The alpha-particle has its origin in the neck nucleons. The yield of fission fragments in the model is linked to the number of states over the barrier of the saddle point, which is between the contacting and well-separated fission fragments. The alpha-particle is fused with the nearest heavy fragment after passing the saddle point, therefore, two final fragments appear during fission. The quadrupole deformations of heavy fragments are taken into account in the model. The fragment yields depend on the heights of corresponding saddle points and the values of the equilibrium quadrupole deformation of fragments.","sentences":["The evaluated nuclide yields, fragment mass and charge distributions, and averaged total kinetic energy of fission fragments for the neutron-induced fission of 30 actinide nuclei are well described in the new model, which considers the fissioning scission system consisting of two heavy fragments and an alpha-particle between them.","The alpha-particle has its origin in the neck nucleons.","The yield of fission fragments in the model is linked to the number of states over the barrier of the saddle point, which is between the contacting and well-separated fission fragments.","The alpha-particle is fused with the nearest heavy fragment after passing the saddle point, therefore, two final fragments appear during fission.","The quadrupole deformations of heavy fragments are taken into account in the model.","The fragment yields depend on the heights of corresponding saddle points and the values of the equilibrium quadrupole deformation of fragments."],"url":"http://arxiv.org/abs/2403.11628v1","category":"nucl-th"}
{"created":"2024-03-18 09:56:00","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","abstract":"Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges. It incorporates an explicit behavior pattern representation learner to capture the behavior patterns composed of multiplex user-item interaction relations, and includes a relation chain representation learning and a relation chain-aware encoder to discover the impact of various auxiliary relations on the target relation, the dependencies between different relations, and mine the appropriate order of relations in a behavior pattern. Extensive experiments on three real-world datasets demonstrate that our \\model surpasses various state-of-the-art recommendation methods. It outperforms the best baselines by 10.06\\% and 12.15\\% on average across all datasets in terms of R@10 and N@10 respectively.","sentences":["Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences.","Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms.","Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios.","In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges.","It incorporates an explicit behavior pattern representation learner to capture the behavior patterns composed of multiplex user-item interaction relations, and includes a relation chain representation learning and a relation chain-aware encoder to discover the impact of various auxiliary relations on the target relation, the dependencies between different relations, and mine the appropriate order of relations in a behavior pattern.","Extensive experiments on three real-world datasets demonstrate that our \\model surpasses various state-of-the-art recommendation methods.","It outperforms the best baselines by 10.06\\% and 12.15\\% on average across all datasets in terms of R@10 and N@10 respectively."],"url":"http://arxiv.org/abs/2403.11624v1","category":"cs.IR"}
{"created":"2024-03-18 09:54:18","title":"Ultracold dipolar bosons trapped in atomtronic circuits","abstract":"We consider a ring-shaped triple-well potential with few polar bosons with in-plane dipole orientation. By diagonalizing the extended Bose-Hubbard Hamiltonian, we investigate the ground state properties of the system as we rotate the dipole angle and vary the on-site and dipole-dipole interaction strengths. We find that the anisotropic character of the dipolar interactions, as well as the competition between dipole and on-site interactions lead to different ground states and that the entanglement between sites also depends on the number of particles. We further characterize the system by studying the condensed fraction and coherence properties, highlighting the possible effect of the dipolar interaction as manipulation tool.","sentences":["We consider a ring-shaped triple-well potential with few polar bosons with in-plane dipole orientation.","By diagonalizing the extended Bose-Hubbard Hamiltonian, we investigate the ground state properties of the system as we rotate the dipole angle and vary the on-site and dipole-dipole interaction strengths.","We find that the anisotropic character of the dipolar interactions, as well as the competition between dipole and on-site interactions lead to different ground states and that the entanglement between sites also depends on the number of particles.","We further characterize the system by studying the condensed fraction and coherence properties, highlighting the possible effect of the dipolar interaction as manipulation tool."],"url":"http://arxiv.org/abs/2403.11620v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-18 09:50:05","title":"Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments","abstract":"Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.","sentences":["Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems.","This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous.","Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages.","We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities.","We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies."],"url":"http://arxiv.org/abs/2403.11617v1","category":"cs.RO"}
{"created":"2024-03-18 09:47:16","title":"Effects of galaxy environment on merger fraction","abstract":"Aims. In this work, we intend to examine how environment influences the merger fraction, from the low density field environment to higher density groups and clusters. We also aim to study how the properties of a group or cluster, as well as the position of a galaxy in the group or cluster, influences the merger fraction.   Methods. We identified galaxy groups and clusters in the North Ecliptic Pole using a friends-of-friends algorithm and the local density. Once identified, we determined the central galaxies, group radii, velocity dispersions, and group masses of these groups and clusters. Merging systems were identified with a neural network as well as visually. With these, we examined how the merger fraction changes as the local density changes for all galaxies as well as how the merger fraction changes as the properties of the groups or clusters change.   Results. We find that the merger fraction increases as local density increases and decreases as the velocity dispersion increases, as is often found in literature. A decrease in merger fraction as the group mass increases is also found. We also find groups with larger radii have higher merger fractions. The number of galaxies in a group does not influence the merger fraction.   Conclusions. The decrease in merger fraction as group mass increases is a result of the link between group mass and velocity dispersion. Hence, this decrease of merger fraction with increasing mass is a result of the decrease of merger fraction with velocity dispersion. The increasing relation between group radii and merger fraction may be a result of larger groups having smaller velocity dispersion at a larger distance from the centre or larger groups hosting smaller, infalling groups with more mergers. However, we do not find evidence of smaller groups having higher merger fractions.","sentences":["Aims.","In this work, we intend to examine how environment influences the merger fraction, from the low density field environment to higher density groups and clusters.","We also aim to study how the properties of a group or cluster, as well as the position of a galaxy in the group or cluster, influences the merger fraction.   ","Methods.","We identified galaxy groups and clusters in the North Ecliptic Pole using a friends-of-friends algorithm and the local density.","Once identified, we determined the central galaxies, group radii, velocity dispersions, and group masses of these groups and clusters.","Merging systems were identified with a neural network as well as visually.","With these, we examined how the merger fraction changes as the local density changes for all galaxies as well as how the merger fraction changes as the properties of the groups or clusters change.   Results.","We find that the merger fraction increases as local density increases and decreases as the velocity dispersion increases, as is often found in literature.","A decrease in merger fraction as the group mass increases is also found.","We also find groups with larger radii have higher merger fractions.","The number of galaxies in a group does not influence the merger fraction.   ","Conclusions.","The decrease in merger fraction as group mass increases is a result of the link between group mass and velocity dispersion.","Hence, this decrease of merger fraction with increasing mass is a result of the decrease of merger fraction with velocity dispersion.","The increasing relation between group radii and merger fraction may be a result of larger groups having smaller velocity dispersion at a larger distance from the centre or larger groups hosting smaller, infalling groups with more mergers.","However, we do not find evidence of smaller groups having higher merger fractions."],"url":"http://arxiv.org/abs/2403.11615v1","category":"astro-ph.GA"}
{"created":"2024-03-18 09:34:45","title":"A splitting-based KPIK method for eddy current optimal control problems in an all-at-once approach","abstract":"In this paper, we focus on efficient methods to solve discretized linear systems obtained from eddy current optimal control problems in an all-at-once approach. We construct a new low-rank matrix equation method based on a special splitting of the coefficient matrix and the Krylov-plus-inverted-Krylov (KPIK) algorithm. Firstly, we rewrite the resulting discretized linear system in a matrix-equation form. Then using the KPIK algorithm, we can obtain the low-rank approximation solution. The new method is named the splitting-based Krylov-plus-inverted-Krylov (SKPIK) method. The SKPIK method can not only solve the large and sparse discretized systems fast but also overcomes the storage problem. Theoretical results about the existence of the low-rank solutions are given. Numerical experiments are used to illustrate the performance of the new low-rank matrix equation method by compared with some existing classical efficient methods.","sentences":["In this paper, we focus on efficient methods to solve discretized linear systems obtained from eddy current optimal control problems in an all-at-once approach.","We construct a new low-rank matrix equation method based on a special splitting of the coefficient matrix and the Krylov-plus-inverted-Krylov (KPIK) algorithm.","Firstly, we rewrite the resulting discretized linear system in a matrix-equation form.","Then using the KPIK algorithm, we can obtain the low-rank approximation solution.","The new method is named the splitting-based Krylov-plus-inverted-Krylov (SKPIK) method.","The SKPIK method can not only solve the large and sparse discretized systems fast but also overcomes the storage problem.","Theoretical results about the existence of the low-rank solutions are given.","Numerical experiments are used to illustrate the performance of the new low-rank matrix equation method by compared with some existing classical efficient methods."],"url":"http://arxiv.org/abs/2403.11611v1","category":"math.NA"}
{"created":"2024-03-18 09:31:59","title":"AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments","abstract":"The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings). However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a suboptimal trajectory under existing mapping-based and learning-based navigation methods. In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths. AGRNav contains a lightweight semantic scene completion network (SCONet) with self-attention to enable accurate obstacle predictions by capturing contextual information and occlusion area features. The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map. Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation. We validate AGRNav's performance through benchmarks in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods. The open-source code is available at https://github.com/jmwang0117/AGRNav.","sentences":["The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings).","However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a suboptimal trajectory under existing mapping-based and learning-based navigation methods.","In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths.","AGRNav contains a lightweight semantic scene completion network (SCONet) with self-attention to enable accurate obstacle predictions by capturing contextual information and occlusion area features.","The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map.","Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation.","We validate AGRNav's performance through benchmarks in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods.","The open-source code is available at https://github.com/jmwang0117/AGRNav."],"url":"http://arxiv.org/abs/2403.11607v1","category":"cs.RO"}
{"created":"2024-03-18 09:17:04","title":"A singular perturbation result for infinite-dimensional coupled systems","abstract":"This paper deals with an infinite dimensional version of the singular perturbation method. It is, more precisely, an exponential stability result for a system composed by a fast and a slow dynamics that may involve infinite-dimensional dynamics where, roughly speaking, the system can be approximated into two decoupled systems for which the stability conditions are easier to check. The classical singular perturbation result states that, as soon as the fast system is sufficiently fast, then the exponential stability conditions for the full-system can be reduced to the exponential stability conditions for the two decoupled systems. In this article, it is proved that the classical singular perturbation method can be applied when facing with a slow infinite-dimensional system coupled with a fast finite-dimensional system. The proof relies on a Lyapunov approach, more precisely with the construction a functional inspired by the forwarding method. The well-posedness proof is obtained thanks to the regular system theory. In addition to these result, a Tikhnov theorem is proved. Namely, supposing that the initial condition depends on $\\varepsilon$, the parameter modeling the difference between the two time-scales, the trajectory of the full-system is proved to depend linearly on $\\varepsilon$.","sentences":["This paper deals with an infinite dimensional version of the singular perturbation method.","It is, more precisely, an exponential stability result for a system composed by a fast and a slow dynamics that may involve infinite-dimensional dynamics where, roughly speaking, the system can be approximated into two decoupled systems for which the stability conditions are easier to check.","The classical singular perturbation result states that, as soon as the fast system is sufficiently fast, then the exponential stability conditions for the full-system can be reduced to the exponential stability conditions for the two decoupled systems.","In this article, it is proved that the classical singular perturbation method can be applied when facing with a slow infinite-dimensional system coupled with a fast finite-dimensional system.","The proof relies on a Lyapunov approach, more precisely with the construction a functional inspired by the forwarding method.","The well-posedness proof is obtained thanks to the regular system theory.","In addition to these result, a Tikhnov theorem is proved.","Namely, supposing that the initial condition depends on $\\varepsilon$, the parameter modeling the difference between the two time-scales, the trajectory of the full-system is proved to depend linearly on $\\varepsilon$."],"url":"http://arxiv.org/abs/2403.11596v1","category":"math.AP"}
{"created":"2024-03-18 09:12:16","title":"End-to-end multi-modal product matching in fashion e-commerce","abstract":"Product matching, the task of identifying different representations of the same product for better discoverability, curation, and pricing, is a key capability for online marketplace and e-commerce companies. We present a robust multi-modal product matching system in an industry setting, where large datasets, data distribution shifts and unseen domains pose challenges. We compare different approaches and conclude that a relatively straightforward projection of pretrained image and text encoders, trained through contrastive learning, yields state-of-the-art results, while balancing cost and performance. Our solution outperforms single modality matching systems and large pretrained models, such as CLIP. Furthermore we show how a human-in-the-loop process can be combined with model-based predictions to achieve near perfect precision in a production system.","sentences":["Product matching, the task of identifying different representations of the same product for better discoverability, curation, and pricing, is a key capability for online marketplace and e-commerce companies.","We present a robust multi-modal product matching system in an industry setting, where large datasets, data distribution shifts and unseen domains pose challenges.","We compare different approaches and conclude that a relatively straightforward projection of pretrained image and text encoders, trained through contrastive learning, yields state-of-the-art results, while balancing cost and performance.","Our solution outperforms single modality matching systems and large pretrained models, such as CLIP.","Furthermore we show how a human-in-the-loop process can be combined with model-based predictions to achieve near perfect precision in a production system."],"url":"http://arxiv.org/abs/2403.11593v1","category":"cs.CV"}
{"created":"2024-03-18 08:54:32","title":"Investigation of magnetic order influenced phonon and electron dynamics in MnBi$_{2}$Te$_{4}$ and Sb doped MnBi$_{2}$Te$_{4}$ through terahertz time-domain spectroscopy","abstract":"MnBi$_{2}$Te$_{4}$, the first topological insulator with inherent magnetic ordering, has attracted significant attention recently for providing a platform to realize several exotic quantum phenomena at relatively higher temperatures. In this work, we have carried out an exhaustive investigation of MnBi$_{2}$Te$_{4}$ and Sb doped MnBi$_{2}$Te$_{4}$ thin films using THz time-domain spectroscopy. The extracted real THz conductivity displays a strong IR active E$_u$ phonon absorption peak (at $\\sim$1.5 THz) merged on top of the Drude-like contributions from bulk and surface electrons. The extracted parameters from the THz conductivity data fitted to the Drude-Fano-Lorentz model, show significant changes in their temperature dependence around the magnetic ordering N\\'eel temperature of $\\sim$ 25K, which is suggestive of the coupling between magnetic ordering and electronic band structure. The frequency of the E$_u$ phonon displays an anomalous blue-shift with increasing temperatures by $\\sim$ 0.1 THz ($\\sim$7 %) for MnBi$_{2}$Te$_{4}$ and $\\sim$0.2 THz ($\\sim$13 %) for Sb doped MnBi$_{2}$Te$_{4}$ between 7K and 250K. The line-shape of the E$_u$ phonon mode in Sb doped MnBi$_{2}$Te$_{4}$ shows significant Fano asymmetry compared to that of MnBi$_{2}$Te$_{4}$, indicating that Sb doping plays an important role in the Fano interference between the phonons and the electrons, in this system. These results indicate that the anomalous phonon behaviour seen in MBT arise mainly from positive cubic anharmonicity induced self energy parameter, whereas both anharmonicity and the electron phonon coupling are at play in making the relatively higher anomalous blue shift of phonons in MBST. Our studies provide the first comprehensive understanding of the phonon and electron dynamics of MnBi$_{2}$Te$_{4}$ and Sb doped MnBi$_{2}$Te$_{4}$ in the THz range using time-domain THz spectroscopy.","sentences":["MnBi$_{2}$Te$_{4}$, the first topological insulator with inherent magnetic ordering, has attracted significant attention recently for providing a platform to realize several exotic quantum phenomena at relatively higher temperatures.","In this work, we have carried out an exhaustive investigation of MnBi$_{2}$Te$_{4}$ and Sb doped MnBi$_{2}$Te$_{4}$ thin films using THz time-domain spectroscopy.","The extracted real THz conductivity displays a strong IR active E$_u$ phonon absorption peak (at $\\sim$1.5 THz) merged on top of the Drude-like contributions from bulk and surface electrons.","The extracted parameters from the THz conductivity data fitted to the Drude-Fano-Lorentz model, show significant changes in their temperature dependence around the magnetic ordering N\\'eel temperature of $\\sim$ 25K, which is suggestive of the coupling between magnetic ordering and electronic band structure.","The frequency of the E$_u$ phonon displays an anomalous blue-shift with increasing temperatures by $\\sim$ 0.1 THz ($\\sim$7 %) for MnBi$_{2}$Te$_{4}$ and $\\sim$0.2 THz ($\\sim$13 %) for Sb doped MnBi$_{2}$Te$_{4}$ between 7K and 250K. The line-shape of the E$_u$ phonon mode in Sb doped MnBi$_{2}$Te$_{4}$ shows significant Fano asymmetry compared to that of MnBi$_{2}$Te$_{4}$, indicating that Sb doping plays an important role in the Fano interference between the phonons and the electrons, in this system.","These results indicate that the anomalous phonon behaviour seen in MBT arise mainly from positive cubic anharmonicity induced self energy parameter, whereas both anharmonicity and the electron phonon coupling are at play in making the relatively higher anomalous blue shift of phonons in MBST.","Our studies provide the first comprehensive understanding of the phonon and electron dynamics of MnBi$_{2}$Te$_{4}$ and Sb doped MnBi$_{2}$Te$_{4}$ in the THz range using time-domain THz spectroscopy."],"url":"http://arxiv.org/abs/2403.11580v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 08:52:14","title":"Task-Oriented Hybrid Beamforming for OFDM-DFRC Systems with Flexibly Controlled Space-Frequency Spectra","abstract":"This paper investigates the issues of the hybrid beamforming design for the orthogonal frequency division multiplexing dual-function radar-communication (DFRC) system in multiple task scenarios involving the radar scanning and detection task and the target tracking task. To meet different task requirements of the DFRC system, we introduce two novel radar beampattern metrics, the average integrated sidelobe to minimum mainlobe ratio (AISMMR) and average peak sidelobe to integrated mainlobe ratio (APSIMR), to characterize the space-frequency spectra in different scenarios. Then, two HBF design problems are formulated for two task scenarios by minimizing the AISMMR and APSIMR respectively subject to the constraints of communication quality-of-service (QoS), power budget, and hardware. Due to the non-linearity and close coupling between the analog and digital beamformers in both the objective functions and QoS constraint, the resultant formulated problems are challenging to solve. Towards that end, a unified optimization algorithm based on a consensus alternating direction method of multipliers (CADMM) is proposed to solve these two problems. Moreover, under the unified CADMM framework, the closed-form solutions of primal variables in the original two problems are obtained with low complexity. Numerical simulations are provided to demonstrate the feasibility and effectiveness of the proposed algorithm.","sentences":["This paper investigates the issues of the hybrid beamforming design for the orthogonal frequency division multiplexing dual-function radar-communication (DFRC) system in multiple task scenarios involving the radar scanning and detection task and the target tracking task.","To meet different task requirements of the DFRC system, we introduce two novel radar beampattern metrics, the average integrated sidelobe to minimum mainlobe ratio (AISMMR) and average peak sidelobe to integrated mainlobe ratio (APSIMR), to characterize the space-frequency spectra in different scenarios.","Then, two HBF design problems are formulated for two task scenarios by minimizing the AISMMR and APSIMR respectively subject to the constraints of communication quality-of-service (QoS), power budget, and hardware.","Due to the non-linearity and close coupling between the analog and digital beamformers in both the objective functions and QoS constraint, the resultant formulated problems are challenging to solve.","Towards that end, a unified optimization algorithm based on a consensus alternating direction method of multipliers (CADMM) is proposed to solve these two problems.","Moreover, under the unified CADMM framework, the closed-form solutions of primal variables in the original two problems are obtained with low complexity.","Numerical simulations are provided to demonstrate the feasibility and effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.11575v1","category":"eess.SP"}
{"created":"2024-03-18 08:33:56","title":"Advancing Neuromorphic Computing: Mixed-Signal Design Techniques Leveraging Brain Code Units and Fundamental Code Units","abstract":"This paper introduces a groundbreaking digital neuromorphic architecture that innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU) using mixedsignal design methodologies. Leveraging open-source datasets and the latest advances in materials science, our research focuses on enhancing the computational efficiency, accuracy, and adaptability of neuromorphic systems. The core of our approach lies in harmonizing the precision and scalability of digital systems with the robustness and energy efficiency of analog processing. Through experimentation, we demonstrate the effectiveness of our system across various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly improved latency and throughput, achieving a latency as low as 0.75 ms and throughput up to 213 TOP/s. These results firmly establish the potential of our architecture in neuromorphic computing, providing a solid foundation for future developments in this domain. Our study underscores the feasibility of mixedsignal neuromorphic systems and their promise in advancing the field, particularly in applications requiring high efficiency and adaptability","sentences":["This paper introduces a groundbreaking digital neuromorphic architecture that innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU) using mixedsignal design methodologies.","Leveraging open-source datasets and the latest advances in materials science, our research focuses on enhancing the computational efficiency, accuracy, and adaptability of neuromorphic systems.","The core of our approach lies in harmonizing the precision and scalability of digital systems with the robustness and energy efficiency of analog processing.","Through experimentation, we demonstrate the effectiveness of our system across various metrics.","The BCU achieved an accuracy of 88.0% and a power efficiency of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly improved latency and throughput, achieving a latency as low as 0.75 ms and throughput up to 213 TOP/s. These results firmly establish the potential of our architecture in neuromorphic computing, providing a solid foundation for future developments in this domain.","Our study underscores the feasibility of mixedsignal neuromorphic systems and their promise in advancing the field, particularly in applications requiring high efficiency and adaptability"],"url":"http://arxiv.org/abs/2403.11563v1","category":"cs.AR"}
{"created":"2024-03-18 08:07:55","title":"High-Fidelity Entangling Gates for Electron and Nuclear Spin Qubits in Diamond","abstract":"Motivated by the recent experimental progress in exploring the use of a nitrogen-vacancy (NV) center in diamond as a quantum computing platform, we propose schemes for fast and high-fidelity entangling gates on this platform. Using both analytical and numerical calculations, we demonstrate that synchronization effects between resonant and off-resonant transitions may be exploited such that spin-flip errors due to strong driving may be eliminated by adjusting the gate time or the driving field. This allows for fast, high fidelity entangling operations between the electron spin and one or several nuclear spins. We investigate a two-qubit system where the NV center is comprised of a $^{15}$N atom and a qubit-qutrit system for the case of a $^{14}$N atom. In both cases, we predict a complete suppression of off-resonant driving errors for two-qubit gates when addressing the NV electron spin conditioned on states of nuclear spins of the nitrogen atom of the defect. Additionally, we predict fidelities $>0.99$ for multi-qubit gates when including the surrounding $^{13}$C atoms in the diamond lattice in the conditioned logic.","sentences":["Motivated by the recent experimental progress in exploring the use of a nitrogen-vacancy (NV) center in diamond as a quantum computing platform, we propose schemes for fast and high-fidelity entangling gates on this platform.","Using both analytical and numerical calculations, we demonstrate that synchronization effects between resonant and off-resonant transitions may be exploited such that spin-flip errors due to strong driving may be eliminated by adjusting the gate time or the driving field.","This allows for fast, high fidelity entangling operations between the electron spin and one or several nuclear spins.","We investigate a two-qubit system where the NV center is comprised of a $^{15}$N atom and a qubit-qutrit system for the case of a $^{14}$N atom.","In both cases, we predict a complete suppression of off-resonant driving errors for two-qubit gates when addressing the NV electron spin conditioned on states of nuclear spins of the nitrogen atom of the defect.","Additionally, we predict fidelities $>0.99$ for multi-qubit gates when including the surrounding $^{13}$C atoms in the diamond lattice in the conditioned logic."],"url":"http://arxiv.org/abs/2403.11553v1","category":"quant-ph"}
{"created":"2024-03-18 07:59:42","title":"Photoneutron cross section measurements on $^{208}$Pb in the Giant Dipole Resonance region","abstract":"Photoneutron reactions on $^{208}$Pb in the Giant Dipole Resonance energy region have been investigated at the $\\gamma$-ray beam line of the NewSUBARU facility in Japan. The measurements made use of quasi-monochromatic laser Compton backscattering $\\gamma$-ray beams in a broad energy range, from the neutron threshold up to 38 MeV, and of a flat-efficiency moderated $^3$He neutron detection system along with associated neutron-multiplicity sorting methods. We report absolute cross sections and mean photoneutron energies for the $^{208}$Pb$(\\gamma,\\,inX)$ reactions with $i$~=~1 to 4. The fine structure present in the $^{208}$Pb$(\\gamma,\\,n)$ cross sections at incident energies lower than 13~MeV has been observed. The photoabsorption cross section has been obtained as the sum of the $(\\gamma,\\,inX)$ reaction cross sections. By reproducing the measured ring-ratio values at excitation energies below the two neutron separation energy, we were able to extract estimations on the $^{208}$Pb$(\\gamma,\\,n)$ photoneutron energy spectra and on the partial photoneutron cross sections for leaving the residual $^{207}$Pb in its ground and first two excited states. The present results are compared with data from the literature and statistical model calculations.","sentences":["Photoneutron reactions on $^{208}$Pb in the Giant Dipole Resonance energy region have been investigated at the $\\gamma$-ray beam line of the NewSUBARU facility in Japan.","The measurements made use of quasi-monochromatic laser Compton backscattering $\\gamma$-ray beams in a broad energy range, from the neutron threshold up to 38 MeV, and of a flat-efficiency moderated $^3$He neutron detection system along with associated neutron-multiplicity sorting methods.","We report absolute cross sections and mean photoneutron energies for the $^{208}$Pb$(\\gamma,\\,inX)$ reactions with $i$~=~1 to 4.","The fine structure present in the $^{208}$Pb$(\\gamma,\\,n)$ cross sections at incident energies lower than 13~MeV has been observed.","The photoabsorption cross section has been obtained as the sum of the $(\\gamma,\\,inX)$ reaction cross sections.","By reproducing the measured ring-ratio values at excitation energies below the two neutron separation energy, we were able to extract estimations on the $^{208}$Pb$(\\gamma,\\,n)$ photoneutron energy spectra and on the partial photoneutron cross sections for leaving the residual $^{207}$Pb in its ground and first two excited states.","The present results are compared with data from the literature and statistical model calculations."],"url":"http://arxiv.org/abs/2403.11547v1","category":"nucl-ex"}
{"created":"2024-03-18 07:51:32","title":"Topology Data Analysis-based Error Detection for Semantic Image Transmission with Incremental Knowledge-based HARQ","abstract":"Semantic communication (SemCom) aims to achieve high fidelity information delivery under low communication consumption by only guaranteeing semantic accuracy. Nevertheless, semantic communication still suffers from unexpected channel volatility and thus developing a re-transmission mechanism (e.g., hybrid automatic repeat request [HARQ]) is indispensable. In that regard, instead of discarding previously transmitted information, the incremental knowledge-based HARQ (IK-HARQ) is deemed as a more effective mechanism that could sufficiently utilize the information semantics. However, considering the possible existence of semantic ambiguity in image transmission, a simple bit-level cyclic redundancy check (CRC) might compromise the performance of IK-HARQ. Therefore, it emerges a strong incentive to revolutionize the CRC mechanism, so as to reap the benefits of both SemCom and HARQ. In this paper, built on top of swin transformer-based joint source-channel coding (JSCC) and IK-HARQ, we propose a semantic image transmission framework SC-TDA-HARQ. In particular, different from the conventional CRC, we introduce a topological data analysis (TDA)-based error detection method, which capably digs out the inner topological and geometric information of images, so as to capture semantic information and determine the necessity for re-transmission. Extensive numerical results validate the effectiveness and efficiency of the proposed SC-TDA-HARQ framework, especially under the limited bandwidth condition, and manifest the superiority of TDA-based error detection method in image transmission.","sentences":["Semantic communication (SemCom) aims to achieve high fidelity information delivery under low communication consumption by only guaranteeing semantic accuracy.","Nevertheless, semantic communication still suffers from unexpected channel volatility and thus developing a re-transmission mechanism (e.g., hybrid automatic repeat request","[HARQ]) is indispensable.","In that regard, instead of discarding previously transmitted information, the incremental knowledge-based HARQ (IK-HARQ) is deemed as a more effective mechanism that could sufficiently utilize the information semantics.","However, considering the possible existence of semantic ambiguity in image transmission, a simple bit-level cyclic redundancy check (CRC) might compromise the performance of IK-HARQ.","Therefore, it emerges a strong incentive to revolutionize the CRC mechanism, so as to reap the benefits of both SemCom and HARQ.","In this paper, built on top of swin transformer-based joint source-channel coding (JSCC) and IK-HARQ, we propose a semantic image transmission framework SC-TDA-HARQ.","In particular, different from the conventional CRC, we introduce a topological data analysis (TDA)-based error detection method, which capably digs out the inner topological and geometric information of images, so as to capture semantic information and determine the necessity for re-transmission.","Extensive numerical results validate the effectiveness and efficiency of the proposed SC-TDA-HARQ framework, especially under the limited bandwidth condition, and manifest the superiority of TDA-based error detection method in image transmission."],"url":"http://arxiv.org/abs/2403.11542v1","category":"eess.SY"}
{"created":"2024-03-18 07:26:17","title":"Crushing Surfaces of Positive Genus","abstract":"The operation of crushing a normal surface has proven to be a powerful tool in computational $3$-manifold topology, with applications both to triangulation complexity and to algorithms. The main difficulty with crushing is that it can drastically change the topology of a triangulation, so applications to date have been limited to relatively simple surfaces: $2$-spheres, discs, annuli, and closed boundary-parallel surfaces. We give the first detailed analysis of the topological effects of crushing closed essential surfaces of positive genus. To showcase the utility of this new analysis, we use it to prove some results about how triangulation complexity interacts with JSJ decompositions and satellite knots; although similar applications can also be obtained using techniques of Matveev, our approach has the advantage that it avoids the machinery of almost simple spines and handle decompositions.","sentences":["The operation of crushing a normal surface has proven to be a powerful tool in computational $3$-manifold topology, with applications both to triangulation complexity and to algorithms.","The main difficulty with crushing is that it can drastically change the topology of a triangulation, so applications to date have been limited to relatively simple surfaces: $2$-spheres, discs, annuli, and closed boundary-parallel surfaces.","We give the first detailed analysis of the topological effects of crushing closed essential surfaces of positive genus.","To showcase the utility of this new analysis, we use it to prove some results about how triangulation complexity interacts with JSJ decompositions and satellite knots; although similar applications can also be obtained using techniques of Matveev, our approach has the advantage that it avoids the machinery of almost simple spines and handle decompositions."],"url":"http://arxiv.org/abs/2403.11523v1","category":"math.GT"}
{"created":"2024-03-18 07:14:21","title":"State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards","abstract":"While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering bandits (Pike-Burke & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. Simulation studies demonstrate the superior performance of our algorithm across various settings.","sentences":["While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios.","This paper considers the setting of recovering bandits (Pike-Burke & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled.","We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states.","The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems.","Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity.","Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions.","Simulation studies demonstrate the superior performance of our algorithm across various settings."],"url":"http://arxiv.org/abs/2403.11520v1","category":"cs.LG"}
{"created":"2024-03-18 07:10:52","title":"Inter-individual and inter-site neural code conversion and image reconstruction without shared stimuli","abstract":"The human brain demonstrates substantial inter-individual variability in fine-grained functional topography, posing challenges in identifying common neural representations across individuals. Functional alignment has the potential to harmonize these individual differences. However, it typically requires an identical set of stimuli presented to different individuals, which is often unavailable. To address this, we propose a content loss-based neural code converter, designed to convert brain activity from one subject to another representing the same content. The converter is optimized so that the source subject's converted brain activity is decoded into a latent image representation that closely resembles that of the stimulus given to the source subject. We show that converters optimized using hierarchical image representations achieve conversion accuracy comparable to those optimized by paired brain activity as in conventional methods. The brain activity converted from a different individual and even from a different site sharing no stimuli produced reconstructions that approached the quality of within-individual reconstructions. The converted brain activity had a generalizable representation that can be read out by different decoding schemes. The converter required much fewer training samples than that typically required for decoder training to produce recognizable reconstructions. These results demonstrate that our method can effectively combine image representations to convert brain activity across individuals without the need for shared stimuli, providing a promising tool for flexibly aligning data from complex cognitive tasks and a basis for brain-to-brain communication.","sentences":["The human brain demonstrates substantial inter-individual variability in fine-grained functional topography, posing challenges in identifying common neural representations across individuals.","Functional alignment has the potential to harmonize these individual differences.","However, it typically requires an identical set of stimuli presented to different individuals, which is often unavailable.","To address this, we propose a content loss-based neural code converter, designed to convert brain activity from one subject to another representing the same content.","The converter is optimized so that the source subject's converted brain activity is decoded into a latent image representation that closely resembles that of the stimulus given to the source subject.","We show that converters optimized using hierarchical image representations achieve conversion accuracy comparable to those optimized by paired brain activity as in conventional methods.","The brain activity converted from a different individual and even from a different site sharing no stimuli produced reconstructions that approached the quality of within-individual reconstructions.","The converted brain activity had a generalizable representation that can be read out by different decoding schemes.","The converter required much fewer training samples than that typically required for decoder training to produce recognizable reconstructions.","These results demonstrate that our method can effectively combine image representations to convert brain activity across individuals without the need for shared stimuli, providing a promising tool for flexibly aligning data from complex cognitive tasks and a basis for brain-to-brain communication."],"url":"http://arxiv.org/abs/2403.11517v1","category":"q-bio.NC"}
{"created":"2024-03-18 07:01:21","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications","abstract":"Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity. Furthermore, our patch is trained to effectively address different scales and distances from the camera. Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for CNN-based MDE models. Additionally, we investigate the vulnerability of Transformer-based MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models.","sentences":["Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers.","However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation.","Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas.","In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications.","Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective.","Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity.","Furthermore, our patch is trained to effectively address different scales and distances from the camera.","Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for CNN-based MDE models.","Additionally, we investigate the vulnerability of Transformer-based MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models."],"url":"http://arxiv.org/abs/2403.11515v1","category":"cs.CV"}
{"created":"2024-03-18 06:59:23","title":"Measurement-Based Quantum Approximate Optimization","abstract":"Parameterized quantum circuits are attractive candidates for potential quantum advantage in the near term and beyond. At the same time, as quantum computing hardware not only continues to improve but also begins to incorporate new features such as mid-circuit measurement and adaptive control, opportunities arise for innovative algorithmic paradigms. In this work we focus on measurement-based quantum computing protocols for approximate optimization, in particular related to quantum alternating operator ans\\\"atze (QAOA), a popular quantum circuit model approach to combinatorial optimization. For the construction and analysis of our measurement-based protocols we demonstrate that diagrammatic approaches, specifically ZX-calculus and its extensions, are effective for adapting such algorithms to the measurement-based setting. In particular we derive measurement patterns for applying QAOA to the broad and important class of QUBO problems. We further outline how for constrained optimization, hard problem constraints may be directly incorporated into our protocol to guarantee the feasibility of the solution found and avoid the need for dealing with penalties. Finally we discuss the resource requirements and tradeoffs of our approach to that of more traditional quantum circuits.","sentences":["Parameterized quantum circuits are attractive candidates for potential quantum advantage in the near term and beyond.","At the same time, as quantum computing hardware not only continues to improve but also begins to incorporate new features such as mid-circuit measurement and adaptive control, opportunities arise for innovative algorithmic paradigms.","In this work we focus on measurement-based quantum computing protocols for approximate optimization, in particular related to quantum alternating operator ans\\\"atze (QAOA), a popular quantum circuit model approach to combinatorial optimization.","For the construction and analysis of our measurement-based protocols we demonstrate that diagrammatic approaches, specifically ZX-calculus and its extensions, are effective for adapting such algorithms to the measurement-based setting.","In particular we derive measurement patterns for applying QAOA to the broad and important class of QUBO problems.","We further outline how for constrained optimization, hard problem constraints may be directly incorporated into our protocol to guarantee the feasibility of the solution found and avoid the need for dealing with penalties.","Finally we discuss the resource requirements and tradeoffs of our approach to that of more traditional quantum circuits."],"url":"http://arxiv.org/abs/2403.11514v1","category":"quant-ph"}
{"created":"2024-03-18 06:25:41","title":"Circle Representation for Medical Instance Object Segmentation","abstract":"Recently, circle representation has been introduced for medical imaging, designed specifically to enhance the detection of instance objects that are spherically shaped (e.g., cells, glomeruli, and nuclei). Given its outstanding effectiveness in instance detection, it is compelling to consider the application of circle representation for segmenting instance medical objects. In this study, we introduce CircleSnake, a simple end-to-end segmentation approach that utilizes circle contour deformation for segmenting ball-shaped medical objects at the instance level. The innovation of CircleSnake lies in these three areas: (1) It substitutes the complex bounding box-to-octagon contour transformation with a more consistent and rotation-invariant bounding circle-to-circle contour adaptation. This adaptation specifically targets ball-shaped medical objects. (2) The circle representation employed in CircleSnake significantly reduces the degrees of freedom to two, compared to eight in the octagon representation. This reduction enhances both the robustness of the segmentation performance and the rotational consistency of the method. (3) CircleSnake is the first end-to-end deep instance segmentation pipeline to incorporate circle representation, encompassing consistent circle detection, circle contour proposal, and circular convolution in a unified framework. This integration is achieved through the novel application of circular graph convolution within the context of circle detection and instance segmentation. In practical applications, such as the detection of glomeruli, nuclei, and eosinophils in pathological images, CircleSnake has demonstrated superior performance and greater rotation invariance when compared to benchmarks. The code has been made publicly available: https://github.com/hrlblab/CircleSnake.","sentences":["Recently, circle representation has been introduced for medical imaging, designed specifically to enhance the detection of instance objects that are spherically shaped (e.g., cells, glomeruli, and nuclei).","Given its outstanding effectiveness in instance detection, it is compelling to consider the application of circle representation for segmenting instance medical objects.","In this study, we introduce CircleSnake, a simple end-to-end segmentation approach that utilizes circle contour deformation for segmenting ball-shaped medical objects at the instance level.","The innovation of CircleSnake lies in these three areas: (1) It substitutes the complex bounding box-to-octagon contour transformation with a more consistent and rotation-invariant bounding circle-to-circle contour adaptation.","This adaptation specifically targets ball-shaped medical objects.","(2) The circle representation employed in CircleSnake significantly reduces the degrees of freedom to two, compared to eight in the octagon representation.","This reduction enhances both the robustness of the segmentation performance and the rotational consistency of the method.","(3) CircleSnake is the first end-to-end deep instance segmentation pipeline to incorporate circle representation, encompassing consistent circle detection, circle contour proposal, and circular convolution in a unified framework.","This integration is achieved through the novel application of circular graph convolution within the context of circle detection and instance segmentation.","In practical applications, such as the detection of glomeruli, nuclei, and eosinophils in pathological images, CircleSnake has demonstrated superior performance and greater rotation invariance when compared to benchmarks.","The code has been made publicly available: https://github.com/hrlblab/CircleSnake."],"url":"http://arxiv.org/abs/2403.11507v1","category":"cs.CV"}
{"created":"2024-03-18 06:18:54","title":"Accelerating Handover in Mobile Satellite Network","abstract":"The construction of Low Earth Orbit (LEO) satellite constellations has recently spurred tremendous attention from academia and industry. 5G and 6G standards have specified LEO satellite network as a key component of 5G and 6G networks. However, ground terminals experience frequent, high-latency handover incurred by satellites' fast travelling speed, which deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a novel handover flowchart for mobile satellite networks, which can considerably reduce the handover latency. The innovation behind this scheme is to mitigate the interaction between the access and core networks that occupy the majority of time overhead by leveraging the predictable travelling trajectory and spatial distribution inherent in mobile satellite networks. Specifically, we design a fine-grained synchronized algorithm to address the synchronization problem due to the lack of control signalling delivery between the access and core networks. Moreover, we minimize the computational complexity of the core network using information such as the satellite access strategy and unique spatial distribution, which is caused by frequent prediction operations. We have built a prototype for a mobile satellite network using modified Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink and Kuiper. We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can considerably reduce the handover latency compared to the 3GPP Non-terrestrial Networks (NTN) and two other existing handover schemes.","sentences":["The construction of Low Earth Orbit (LEO) satellite constellations has recently spurred tremendous attention from academia and industry.","5G and 6G standards have specified LEO satellite network as a key component of 5G and 6G networks.","However, ground terminals experience frequent, high-latency handover incurred by satellites' fast travelling speed, which deteriorates the performance of latency-sensitive applications.","To address this challenge, we propose a novel handover flowchart for mobile satellite networks, which can considerably reduce the handover latency.","The innovation behind this scheme is to mitigate the interaction between the access and core networks that occupy the majority of time overhead by leveraging the predictable travelling trajectory and spatial distribution inherent in mobile satellite networks.","Specifically, we design a fine-grained synchronized algorithm to address the synchronization problem due to the lack of control signalling delivery between the access and core networks.","Moreover, we minimize the computational complexity of the core network using information such as the satellite access strategy and unique spatial distribution, which is caused by frequent prediction operations.","We have built a prototype for a mobile satellite network using modified Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink and Kuiper.","We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can considerably reduce the handover latency compared to the 3GPP Non-terrestrial Networks (NTN) and two other existing handover schemes."],"url":"http://arxiv.org/abs/2403.11502v1","category":"cs.NI"}
{"created":"2024-03-18 05:32:31","title":"Expanding the Resolution Boundary of Outcome-Based Imperfect-Recall Abstraction in Games with Ordered Signals","abstract":"In the development of advanced Texas Hold'em AI systems, abstraction technology has garnered widespread attention due to its significant effect in simplifying game complexity. This study adopts a more specific model, the games of ordered signal, to describe Texas Hold'em-style games and optimizes this model to streamline its mathematical representation and broaden its applicability. By transitioning from a broad imperfect information game model to a game with ordered signals model, we have separated the previously intertwined infoset abstraction and action abstraction into independent signal abstraction and action abstraction. Importantly, this signal abstraction provides a mathematical framework for the hand abstraction task, which is emphatically discussed in this paper. Additionally, a novel common refinement principle is introduced, revealing the limit performance of hand abstraction algorithms. We introduce potential outcome isomorphism (POI) and pinpoint that it suffers from the issue of excessive abstraction. Futher, We demonstrate that POI serves as a common refinement for leading outcome-based hand abstraction algorithms, such as E[HS] and PA\\&PAEMD. Consequently, excessive abstraction also inherently affects these algorithms, leading to suboptimal performance. Our investigation reveals the omission of historical data as a primary contributor to excessive abstraction. To remedy this, we propose the K-Recall Outcome Isomorphism (KROI) to incorporate the missing information. Compared with POI, KROI more accurately mirrors lossless isomorphism (LI), the ground truth, offering enhanced signal abstraction resolution. Experimental results in the Numeral211 Hold'em indicate that strategies developed through KROI approximate the exploitability of those developed through LI more closely than those trained through POI.","sentences":["In the development of advanced Texas Hold'em AI systems, abstraction technology has garnered widespread attention due to its significant effect in simplifying game complexity.","This study adopts a more specific model, the games of ordered signal, to describe Texas Hold'em-style games and optimizes this model to streamline its mathematical representation and broaden its applicability.","By transitioning from a broad imperfect information game model to a game with ordered signals model, we have separated the previously intertwined infoset abstraction and action abstraction into independent signal abstraction and action abstraction.","Importantly, this signal abstraction provides a mathematical framework for the hand abstraction task, which is emphatically discussed in this paper.","Additionally, a novel common refinement principle is introduced, revealing the limit performance of hand abstraction algorithms.","We introduce potential outcome isomorphism (POI) and pinpoint that it suffers from the issue of excessive abstraction.","Futher, We demonstrate that POI serves as a common refinement for leading outcome-based hand abstraction algorithms, such as E[HS] and PA\\&PAEMD.","Consequently, excessive abstraction also inherently affects these algorithms, leading to suboptimal performance.","Our investigation reveals the omission of historical data as a primary contributor to excessive abstraction.","To remedy this, we propose the K-Recall Outcome Isomorphism (KROI) to incorporate the missing information.","Compared with POI, KROI more accurately mirrors lossless isomorphism (LI), the ground truth, offering enhanced signal abstraction resolution.","Experimental results in the Numeral211 Hold'em indicate that strategies developed through KROI approximate the exploitability of those developed through LI more closely than those trained through POI."],"url":"http://arxiv.org/abs/2403.11486v1","category":"cs.GT"}
{"created":"2024-03-18 05:29:31","title":"A Browser Extension for in-place Signaling and Assessment of Misinformation","abstract":"The status-quo of misinformation moderation is a central authority, usually social platforms, deciding what content constitutes misinformation and how it should be handled. However, to preserve users' autonomy, researchers have explored democratized misinformation moderation. One proposition is to enable users to assess content accuracy and specify whose assessments they trust. We explore how these affordances can be provided on the web, without cooperation from the platforms where users consume content. We present a browser extension that empowers users to assess the accuracy of any content on the web and shows the user assessments from their trusted sources in-situ. Through a two-week user study, we report on how users perceive such a tool, the kind of content users want to assess, and the rationales they use in their assessments. We identify implications for designing tools that enable users to moderate content for themselves with the help of those they trust.","sentences":["The status-quo of misinformation moderation is a central authority, usually social platforms, deciding what content constitutes misinformation and how it should be handled.","However, to preserve users' autonomy, researchers have explored democratized misinformation moderation.","One proposition is to enable users to assess content accuracy and specify whose assessments they trust.","We explore how these affordances can be provided on the web, without cooperation from the platforms where users consume content.","We present a browser extension that empowers users to assess the accuracy of any content on the web and shows the user assessments from their trusted sources in-situ.","Through a two-week user study, we report on how users perceive such a tool, the kind of content users want to assess, and the rationales they use in their assessments.","We identify implications for designing tools that enable users to moderate content for themselves with the help of those they trust."],"url":"http://arxiv.org/abs/2403.11485v1","category":"cs.HC"}
{"created":"2024-03-18 05:03:07","title":"Towards understanding the nature of direct functional connectivity in visual brain network","abstract":"Recent advances in neuroimaging have enabled studies in functional connectivity (FC) of human brain, alongside investigation of the neuronal basis of cognition. One important FC study is the representation of vision in human brain. The release of publicly available dataset BOLD5000 has made it possible to study the brain dynamics during visual tasks in greater detail. In this paper, a comprehensive analysis of fMRI time series (TS) has been performed to explore different types of visual brain networks (VBN). The novelty of this work lies in (1) constructing VBN with consistently significant direct connectivity using both marginal and partial correlation, which is further analyzed using graph theoretic measures, (2) classification of VBNs as formed by image complexity-specific TS, using graphical features. In image complexity-specific VBN classification, XGBoost yields average accuracy in the range of 86.5% to 91.5% for positively correlated VBN, which is 2% greater than that using negative correlation. This result not only reflects the distinguishing graphical characteristics of each image complexity-specific VBN, but also highlights the importance of studying both positively correlated and negatively correlated VBN to understand the how differently brain functions while viewing different complexities of real-world images.","sentences":["Recent advances in neuroimaging have enabled studies in functional connectivity (FC) of human brain, alongside investigation of the neuronal basis of cognition.","One important FC study is the representation of vision in human brain.","The release of publicly available dataset BOLD5000 has made it possible to study the brain dynamics during visual tasks in greater detail.","In this paper, a comprehensive analysis of fMRI time series (TS) has been performed to explore different types of visual brain networks (VBN).","The novelty of this work lies in (1) constructing VBN with consistently significant direct connectivity using both marginal and partial correlation, which is further analyzed using graph theoretic measures, (2) classification of VBNs as formed by image complexity-specific TS, using graphical features.","In image complexity-specific VBN classification, XGBoost yields average accuracy in the range of 86.5% to 91.5% for positively correlated VBN, which is 2% greater than that using negative correlation.","This result not only reflects the distinguishing graphical characteristics of each image complexity-specific VBN, but also highlights the importance of studying both positively correlated and negatively correlated VBN to understand the how differently brain functions while viewing different complexities of real-world images."],"url":"http://arxiv.org/abs/2403.11480v1","category":"q-bio.NC"}
{"created":"2024-03-18 04:54:09","title":"Moore-Read state in Half-filled Moir\u00e9 Chern band from three-body Pseudo-potential","abstract":"The moir\\'e system provides a tunable platform for exploring exotic phases of materials. This article shows the possible realization of a non-Abelian state characterized by the Moore-Read wavefunction in a half-filled moir\\'e Chern band, exemplified by twisted $\\rm MoTe_2$. This is achieved by introducing short-range repulsive three-body interaction. Exact diagonalization is employed to examine the spectrum in finite size. The incompressibility of the system, the degeneracy of the ground states, and the number of low-energy states provide compelling evidence to identify the ground state as the Moore-Read state. We further interpolate between the three-body interaction and Coulomb interaction to show a phase transition between the composite Fermi-liquid and the Moore-Read state. Finally, we consider the effect of band mixing and derive the three-body interaction using perturbation theory. By exploring the conditions under which band mixing effects mimic short-range repulsive three-body interaction we provide insights towards realizing non-Abelian phases of matter in the moir\\'e system.","sentences":["The moir\\'e system provides a tunable platform for exploring exotic phases of materials.","This article shows the possible realization of a non-Abelian state characterized by the Moore-Read wavefunction in a half-filled moir\\'e Chern band, exemplified by twisted $\\rm MoTe_2$.","This is achieved by introducing short-range repulsive three-body interaction.","Exact diagonalization is employed to examine the spectrum in finite size.","The incompressibility of the system, the degeneracy of the ground states, and the number of low-energy states provide compelling evidence to identify the ground state as the Moore-Read state.","We further interpolate between the three-body interaction and Coulomb interaction to show a phase transition between the composite Fermi-liquid and the Moore-Read state.","Finally, we consider the effect of band mixing and derive the three-body interaction using perturbation theory.","By exploring the conditions under which band mixing effects mimic short-range repulsive three-body interaction we provide insights towards realizing non-Abelian phases of matter in the moir\\'e system."],"url":"http://arxiv.org/abs/2403.11478v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 04:50:11","title":"s-process Enriched Evolved Binaries in the Galaxy and the Magellanic Clouds","abstract":"Post-asymptotic giant branch stars (post-AGB) in binary systems, with typical orbital periods between ~100 to ~1000 days, result from a poorly understood interaction that terminates their precursory AGB phase. The majority of these binaries display a photospheric anomaly called 'chemical depletion', thought to arise from an interaction between the circumbinary disc and the post-AGB star, leading to the reaccretion of pure gas onto the star, devoid of refractory elements due to dust formation. In this paper, we focus on a subset of chemically peculiar binary post-AGBs in the Galaxy and the Magellanic Clouds (MCs) whose high-resolution optical spectroscopic study revealed a carbon and s-process enrichment, contrary to the commonly observed photospheric chemical depletion. Using spectral energy distribution (SED) fitting and period-luminosity-colour (PLC) relation methods, we determine the luminosity of the targets ($2700-8300L_{\\odot}$), which enables confirmation of their evolutionary phase and estimation of initial masses (as a function of metallicity) ($1-2.5M_{\\odot}$). In conjunction with predictions from dedicated ATON stellar evolutionary models, our results indicate a predominant intrinsic enrichment of carbon and s-process elements in our binary post-AGB targets. We qualitatively rule out extrinsic enrichment and inherited s-process enrichment from the host galaxy as plausible explanations for the observed overabundances. Our chemically peculiar subset of intrinsic carbon and s-process enriched binary post-AGBs also hints at potential variation in the efficiency of chemical depletion between stars with C-rich and O-rich circumbinary disc chemistries. However, critical observational studies of circumbinary disc chemistry are necessary to address gaps in our current understanding of disc-binary interactions inducing chemical depletion in binary post-AGB systems.","sentences":["Post-asymptotic giant branch stars (post-AGB) in binary systems, with typical orbital periods between ~100 to ~1000 days, result from a poorly understood interaction that terminates their precursory AGB phase.","The majority of these binaries display a photospheric anomaly called 'chemical depletion', thought to arise from an interaction between the circumbinary disc and the post-AGB star, leading to the reaccretion of pure gas onto the star, devoid of refractory elements due to dust formation.","In this paper, we focus on a subset of chemically peculiar binary post-AGBs in the Galaxy and the Magellanic Clouds (MCs) whose high-resolution optical spectroscopic study revealed a carbon and s-process enrichment, contrary to the commonly observed photospheric chemical depletion.","Using spectral energy distribution (SED) fitting and period-luminosity-colour (PLC) relation methods, we determine the luminosity of the targets ($2700-8300L_{\\odot}$), which enables confirmation of their evolutionary phase and estimation of initial masses (as a function of metallicity) ($1-2.5M_{\\odot}$).","In conjunction with predictions from dedicated ATON stellar evolutionary models, our results indicate a predominant intrinsic enrichment of carbon and s-process elements in our binary post-AGB targets.","We qualitatively rule out extrinsic enrichment and inherited s-process enrichment from the host galaxy as plausible explanations for the observed overabundances.","Our chemically peculiar subset of intrinsic carbon and s-process enriched binary post-AGBs also hints at potential variation in the efficiency of chemical depletion between stars with C-rich and O-rich circumbinary disc chemistries.","However, critical observational studies of circumbinary disc chemistry are necessary to address gaps in our current understanding of disc-binary interactions inducing chemical depletion in binary post-AGB systems."],"url":"http://arxiv.org/abs/2403.11475v1","category":"astro-ph.SR"}
{"created":"2024-03-18 04:44:00","title":"Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training","abstract":"Learned indexes use machine learning models to learn the mappings between keys and their corresponding positions in key-value indexes. These indexes use the mapping information as training data. Learned indexes require frequent retrainings of their models to incorporate the changes introduced by update queries. To efficiently retrain the models, existing learned index systems often harness a linear algebraic QR factorization technique that performs matrix decomposition. This factorization approach processes all key-position pairs during each retraining, resulting in compute operations that grow linearly with the total number of keys and their lengths. Consequently, the retrainings create a severe performance bottleneck, especially for variable-length string keys, while the retrainings are crucial for maintaining high prediction accuracy and in turn, ensuring low query service latency.   To address this performance problem, we develop an algorithm-hardware co-designed string-key learned index system, dubbed SIA. In designing SIA, we leverage a unique algorithmic property of the matrix decomposition-based training method. Exploiting the property, we develop a memoization-based incremental training scheme, which only requires computation over updated keys, while decomposition results of non-updated keys from previous computations can be reused. We further enhance SIA to offload a portion of this training process to an FPGA accelerator to not only relieve CPU resources for serving index queries (i.e., inference), but also accelerate the training itself. Our evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x higher throughput on the two real-world benchmark suites, YCSB and Twitter cache trace, respectively.","sentences":["Learned indexes use machine learning models to learn the mappings between keys and their corresponding positions in key-value indexes.","These indexes use the mapping information as training data.","Learned indexes require frequent retrainings of their models to incorporate the changes introduced by update queries.","To efficiently retrain the models, existing learned index systems often harness a linear algebraic QR factorization technique that performs matrix decomposition.","This factorization approach processes all key-position pairs during each retraining, resulting in compute operations that grow linearly with the total number of keys and their lengths.","Consequently, the retrainings create a severe performance bottleneck, especially for variable-length string keys, while the retrainings are crucial for maintaining high prediction accuracy and in turn, ensuring low query service latency.   ","To address this performance problem, we develop an algorithm-hardware co-designed string-key learned index system, dubbed SIA.","In designing SIA, we leverage a unique algorithmic property of the matrix decomposition-based training method.","Exploiting the property, we develop a memoization-based incremental training scheme, which only requires computation over updated keys, while decomposition results of non-updated keys from previous computations can be reused.","We further enhance SIA to offload a portion of this training process to an FPGA accelerator to not only relieve CPU resources for serving index queries (i.e., inference), but also accelerate the training itself.","Our evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x higher throughput on the two real-world benchmark suites, YCSB and Twitter cache trace, respectively."],"url":"http://arxiv.org/abs/2403.11472v1","category":"cs.LG"}
{"created":"2024-03-18 04:01:26","title":"Bridging 3D Gaussian and Mesh for Freeview Video Rendering","abstract":"This is only a preview version of GauMesh. Recently, primitive-based rendering has been proven to achieve convincing results in solving the problem of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in the context of novel view synthesis, each type of primitive has its inherent defects in terms of representation ability. It is difficult to exploit the mesh to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D Gaussian Splatting) method usually produces artifacts or blurry pixels in the area with smooth geometry and sharp textures. As a result, it is difficult, even not impossible, to represent the complex and dynamic scene with a single type of primitive. To this end, we propose a novel approach, GauMesh, to bridge the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a sequence of tracked mesh as initialization, our goal is to simultaneously optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians, and the deformation field. At a specific time, we perform $\\alpha$-blending on the RGB and opacity values based on the merged and re-ordered z-buffers from mesh and 3D Gaussian rasterizations. This produces the final rendering, which is supervised by the ground-truth image. Experiments demonstrate that our approach adapts the appropriate type of primitives to represent the different parts of the dynamic scene and outperforms all the baseline methods in both quantitative and qualitative comparisons without losing render speed.","sentences":["This is only a preview version of GauMesh.","Recently, primitive-based rendering has been proven to achieve convincing results in solving the problem of modeling and rendering the 3D dynamic scene from 2D images.","Despite this, in the context of novel view synthesis, each type of primitive has its inherent defects in terms of representation ability.","It is difficult to exploit the mesh to depict the fuzzy geometry.","Meanwhile, the point-based splatting (e.g. the 3D Gaussian Splatting) method usually produces artifacts or blurry pixels in the area with smooth geometry and sharp textures.","As a result, it is difficult, even not impossible, to represent the complex and dynamic scene with a single type of primitive.","To this end, we propose a novel approach, GauMesh, to bridge the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes.","Given a sequence of tracked mesh as initialization, our goal is to simultaneously optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians, and the deformation field.","At a specific time, we perform $\\alpha$-blending on the RGB and opacity values based on the merged and re-ordered z-buffers from mesh and 3D Gaussian rasterizations.","This produces the final rendering, which is supervised by the ground-truth image.","Experiments demonstrate that our approach adapts the appropriate type of primitives to represent the different parts of the dynamic scene and outperforms all the baseline methods in both quantitative and qualitative comparisons without losing render speed."],"url":"http://arxiv.org/abs/2403.11453v1","category":"cs.GR"}
{"created":"2024-03-18 03:59:24","title":"Zero-shot Compound Expression Recognition with Visual Language Model at the 6th ABAW Challenge","abstract":"Conventional approaches to facial expression recognition primarily focus on the classification of six basic facial expressions. Nevertheless, real-world situations present a wider range of complex compound expressions that consist of combinations of these basics ones due to limited availability of comprehensive training datasets. The 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) offered unlabeled datasets containing compound expressions. In this study, we propose a zero-shot approach for recognizing compound expressions by leveraging a pretrained visual language model integrated with some traditional CNN networks.","sentences":["Conventional approaches to facial expression recognition primarily focus on the classification of six basic facial expressions.","Nevertheless, real-world situations present a wider range of complex compound expressions that consist of combinations of these basics ones due to limited availability of comprehensive training datasets.","The 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) offered unlabeled datasets containing compound expressions.","In this study, we propose a zero-shot approach for recognizing compound expressions by leveraging a pretrained visual language model integrated with some traditional CNN networks."],"url":"http://arxiv.org/abs/2403.11450v1","category":"cs.CV"}
{"created":"2024-03-18 03:56:34","title":"Graph Partial Label Learning with Potential Cause Discovering","abstract":"Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains. However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs. To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted graph data, our model can effectively filter out the noise contained in the labels. We support the rationale behind our approach with a series of theoretical analyses. Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.","sentences":["Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains.","However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs.","To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL).","PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels.","Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels.","By incorporating auxiliary training based on the extracted graph data, our model can effectively filter out the noise contained in the labels.","We support the rationale behind our approach with a series of theoretical analyses.","Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method."],"url":"http://arxiv.org/abs/2403.11449v1","category":"cs.LG"}
{"created":"2024-03-18 03:30:34","title":"Rate distortion dimension of random Brody curves","abstract":"The main purpose of this paper is to propose an ergodic theoretic approach to the study of entire holomorphic curves. Brody curves are one-Lipschitz holomorphic maps from the complex plane to the complex projective space. They naturally form a dynamical system, and \"random Brody curves\" in the title refers to invariant probability measures on it. We study their geometric and dynamical properties. Given an invariant probability measure $\\mu$ on the space of Brody curves, our first main theorem claims that its rate distortion dimension is bounded by the integral of a \"potential function\" over $\\mu$. This result is analogous to the Ruelle inequality of smooth ergodic theory. Our second main theorem claims that there exists a rich variety of invariant probability measures attaining equality in this \"Ruelle inequality for Brody curves\". The main tools of the proofs are the deformation theory of Brody curves and the variational principle for mean dimension with potential. This approach is motivated by the theory of thermodynamic formalism for Axiom A diffeomorphisms.","sentences":["The main purpose of this paper is to propose an ergodic theoretic approach to the study of entire holomorphic curves.","Brody curves are one-Lipschitz holomorphic maps from the complex plane to the complex projective space.","They naturally form a dynamical system, and \"random Brody curves\" in the title refers to invariant probability measures on it.","We study their geometric and dynamical properties.","Given an invariant probability measure $\\mu$ on the space of Brody curves, our first main theorem claims that its rate distortion dimension is bounded by the integral of a \"potential function\" over $\\mu$. This result is analogous to the Ruelle inequality of smooth ergodic theory.","Our second main theorem claims that there exists a rich variety of invariant probability measures attaining equality in this \"Ruelle inequality for Brody curves\".","The main tools of the proofs are the deformation theory of Brody curves and the variational principle for mean dimension with potential.","This approach is motivated by the theory of thermodynamic formalism for Axiom A diffeomorphisms."],"url":"http://arxiv.org/abs/2403.11442v1","category":"math.CV"}
{"created":"2024-03-18 03:17:17","title":"Formalization of Complexity Analysis of the First-order Optimization Algorithms","abstract":"The convergence rate of various first-order optimization algorithms is a pivotal concern within the numerical optimization community, as it directly reflects the efficiency of these algorithms across different optimization problems. Our goal is making a significant step forward in the formal mathematical representation of optimization techniques using the Lean4 theorem prover. We first formalize the gradient for smooth functions and the subgradient for convex functions on a Hilbert space, laying the groundwork for the accurate formalization of algorithmic structures. Then, we extend our contribution by proving several properties of differentiable convex functions that have not yet been formalized in Mathlib. Finally, a comprehensive formalization of these algorithms is presented. These developments are not only noteworthy on their own but also serve as essential precursors to the formalization of a broader spectrum of numerical algorithms and their applications in machine learning as well as many other areas.","sentences":["The convergence rate of various first-order optimization algorithms is a pivotal concern within the numerical optimization community, as it directly reflects the efficiency of these algorithms across different optimization problems.","Our goal is making a significant step forward in the formal mathematical representation of optimization techniques using the Lean4 theorem prover.","We first formalize the gradient for smooth functions and the subgradient for convex functions on a Hilbert space, laying the groundwork for the accurate formalization of algorithmic structures.","Then, we extend our contribution by proving several properties of differentiable convex functions that have not yet been formalized in Mathlib.","Finally, a comprehensive formalization of these algorithms is presented.","These developments are not only noteworthy on their own but also serve as essential precursors to the formalization of a broader spectrum of numerical algorithms and their applications in machine learning as well as many other areas."],"url":"http://arxiv.org/abs/2403.11437v1","category":"math.OC"}
{"created":"2024-03-18 03:10:36","title":"InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions","abstract":"Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay.","sentences":["Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks.","Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting.","Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue.","However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy.","In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL).","InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions.","Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions.","According to InsInfo, InsCL guides the replay process more inclined to high-quality data.","We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL.","When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay."],"url":"http://arxiv.org/abs/2403.11435v1","category":"cs.CL"}
{"created":"2024-03-18 03:08:49","title":"Earth+: on-board satellite imagery compression leveraging historical earth observations","abstract":"With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground. To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images. To minimize the amount of changes, it is critical to make reference images as fresh as possible. Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation. To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection. In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites. Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available.","sentences":["With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground.","To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images.","To minimize the amount of changes, it is critical to make reference images as fresh as possible.","Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation.","To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection.","In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites.","Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available."],"url":"http://arxiv.org/abs/2403.11434v1","category":"cs.NI"}
{"created":"2024-03-18 03:07:09","title":"Measuring Quantum Information Leakage Under Detection Threat","abstract":"Gentle quantum leakage is proposed as a measure of information leakage to arbitrary eavesdroppers that aim to avoid detection. Gentle (also sometimes referred to as weak or non-demolition) measurements are used to encode the desire of the eavesdropper to evade detection. The gentle quantum leakage meets important axioms proposed for measures of information leakage including positivity, independence, and unitary invariance. Global depolarizing noise, an important family of physical noise in quantum devices, is shown to reduce gentle quantum leakage (and hence can be used as a mechanism to ensure privacy or security). A lower bound for the gentle quantum leakage based on asymmetric approximate cloning is presented. This lower bound relates information leakage to mutual incompatibility of quantum states. A numerical example, based on the encoding in the celebrated BB84 quantum key distribution algorithm, is used to demonstrate the results.","sentences":["Gentle quantum leakage is proposed as a measure of information leakage to arbitrary eavesdroppers that aim to avoid detection.","Gentle (also sometimes referred to as weak or non-demolition) measurements are used to encode the desire of the eavesdropper to evade detection.","The gentle quantum leakage meets important axioms proposed for measures of information leakage including positivity, independence, and unitary invariance.","Global depolarizing noise, an important family of physical noise in quantum devices, is shown to reduce gentle quantum leakage (and hence can be used as a mechanism to ensure privacy or security).","A lower bound for the gentle quantum leakage based on asymmetric approximate cloning is presented.","This lower bound relates information leakage to mutual incompatibility of quantum states.","A numerical example, based on the encoding in the celebrated BB84 quantum key distribution algorithm, is used to demonstrate the results."],"url":"http://arxiv.org/abs/2403.11433v1","category":"quant-ph"}
{"created":"2024-03-18 02:54:55","title":"Clustering theorem in 1D long-range interacting systems at arbitrary temperatures","abstract":"This paper delves into a fundamental aspect of quantum statistical mechanics -- the absence of thermal phase transitions in one-dimensional (1D) systems. Originating from Ising's analysis of the 1D spin chain, this concept has been pivotal in understanding 1D quantum phases, especially those with finite-range interactions as extended by Araki. In this work, we focus on quantum long-range interactions and successfully derive a clustering theorem applicable to a wide range of interaction decays at arbitrary temperatures. This theorem applies to any interaction forms that decay faster than $r^{-2}$ and does not rely on translation invariance or infinite system size assumptions. Also, we rigorously established that the temperature dependence of the correlation length is given by $e^{{\\rm const.} \\beta}$, which is the same as the classical cases. Our findings indicate the absence of phase transitions in 1D systems with super-polynomially decaying interactions, thereby expanding upon previous theoretical research. To overcome significant technical challenges originating from the divergence of the imaginary-time Lieb-Robinson bound, we utilize the quantum belief propagation to refine the cluster expansion method. This approach allowed us to address divergence issues effectively and contributed to a deeper understanding of low-temperature behaviors in 1D quantum systems.","sentences":["This paper delves into a fundamental aspect of quantum statistical mechanics -- the absence of thermal phase transitions in one-dimensional (1D) systems.","Originating from Ising's analysis of the 1D spin chain, this concept has been pivotal in understanding 1D quantum phases, especially those with finite-range interactions as extended by Araki.","In this work, we focus on quantum long-range interactions and successfully derive a clustering theorem applicable to a wide range of interaction decays at arbitrary temperatures.","This theorem applies to any interaction forms that decay faster than $r^{-2}$ and does not rely on translation invariance or infinite system size assumptions.","Also, we rigorously established that the temperature dependence of the correlation length is given by $e^{{\\rm const.}","\\beta}$, which is the same as the classical cases.","Our findings indicate the absence of phase transitions in 1D systems with super-polynomially decaying interactions, thereby expanding upon previous theoretical research.","To overcome significant technical challenges originating from the divergence of the imaginary-time Lieb-Robinson bound, we utilize the quantum belief propagation to refine the cluster expansion method.","This approach allowed us to address divergence issues effectively and contributed to a deeper understanding of low-temperature behaviors in 1D quantum systems."],"url":"http://arxiv.org/abs/2403.11431v1","category":"quant-ph"}
{"created":"2024-03-18 02:46:03","title":"The rate and contribution of mergers to mass assembly from NIRCam observations of galaxy candidates up to 13.3 billion years ago","abstract":"We present an analysis of the galaxy merger rate in the redshift range $4.0<z<9.0$ (i.e. about 1.5 to 0.5 Gyr after the Big Bang) based on visually identified galaxy mergers from morphological parameter analysis. Our dataset is based on high-resolution NIRCam JWST data (F150W and F2000W broad-band filters) in the low-to-moderate magnification ($\\mu<2$) regions of the Abell 2744 cluster field. From a parent set of 675 galaxies $(M_{UV}\\in[-26.6,-17.9])$, we identify 64 merger candidates from the Gini, $M_{20}$ and Asymmetry morphological parameters, leading to a merger fraction $f_m=0.11\\pm0.04$. There is no evidence of redshift evolution of $f_m$ even at the highest redshift considered, thus extending well into the epoch of reionization the constant trend seen previously at $z\\lesssim 6$. Furthermore, we investigate any potential redshift dependent differences in the specific star formation rates between mergers and non-mergers. Our analysis reveals no significant correlation in this regard, with deviations in the studied redshift range typically falling within $0.25$ dex (logarithmic scale) that can be attributed to sample variance and measurement errors. Finally, we also demonstrate that the classification of a merging system is robust with respect to the observed (and equivalently rest-frame) wavelength of the high-quality JWST broad-band images used. This preliminary study highlights the potential for progress in quantifying galaxy assembly through mergers during the epoch of reionization, with significant sample size growth expected from upcoming large JWST infrared imaging datasets.","sentences":["We present an analysis of the galaxy merger rate in the redshift range $4.0<z<9.0$ (i.e. about 1.5 to 0.5 Gyr after the Big Bang) based on visually identified galaxy mergers from morphological parameter analysis.","Our dataset is based on high-resolution NIRCam JWST data (F150W and F2000W broad-band filters) in the low-to-moderate magnification ($\\mu<2$) regions of the Abell 2744 cluster field.","From a parent set of 675 galaxies $(M_{UV}\\in[-26.6,-17.9])$, we identify 64 merger candidates from the Gini, $M_{20}$ and Asymmetry morphological parameters, leading to a merger fraction $f_m=0.11\\pm0.04$.","There is no evidence of redshift evolution of $f_m$ even at the highest redshift considered, thus extending well into the epoch of reionization the constant trend seen previously at $z\\lesssim 6$.","Furthermore, we investigate any potential redshift dependent differences in the specific star formation rates between mergers and non-mergers.","Our analysis reveals no significant correlation in this regard, with deviations in the studied redshift range typically falling within $0.25$ dex (logarithmic scale) that can be attributed to sample variance and measurement errors.","Finally, we also demonstrate that the classification of a merging system is robust with respect to the observed (and equivalently rest-frame) wavelength of the high-quality JWST broad-band images used.","This preliminary study highlights the potential for progress in quantifying galaxy assembly through mergers during the epoch of reionization, with significant sample size growth expected from upcoming large JWST infrared imaging datasets."],"url":"http://arxiv.org/abs/2403.11428v1","category":"astro-ph.GA"}
{"created":"2024-03-18 02:03:23","title":"Table-Lookup MAC: Scalable Processing of Quantised Neural Networks in FPGA Soft Logic","abstract":"Recent advancements in neural network quantisation have yielded remarkable outcomes, with three-bit networks reaching state-of-the-art full-precision accuracy in complex tasks. These achievements present valuable opportunities for accelerating neural networks by computing in reduced precision. Implementing it on FPGAs can take advantage of bit-level reconfigurability, which is not available on conventional CPUs and GPUs. Simultaneously, the high data intensity of neural network processing has inspired computing-in-memory paradigms, including on FPGA platforms. By programming the effects of trained model weights as lookup operations in soft logic, the transfer of weight data from memory units can be avoided, alleviating the memory bottleneck. However, previous methods face poor scalability - the high logic utilisation limiting them to small networks/sub-networks of binary models with low accuracy. In this paper, we introduce Table Lookup Multiply-Accumulate (TLMAC) as a framework to compile and optimise quantised neural networks for scalable lookup-based processing. TLMAC clusters and maps unique groups of weights to lookup-based processing elements, enabling highly parallel computation while taking advantage of parameter redundancy. Further place and route algorithms are proposed to reduce LUT utilisation and routing congestion. We demonstrate that TLMAC significantly improves the scalability of previous related works. Our efficient logic mapping and high degree of reuse enables entire ImageNet-scale quantised models with full-precision accuracy to be implemented using lookup-based computing on one commercially available FPGA.","sentences":["Recent advancements in neural network quantisation have yielded remarkable outcomes, with three-bit networks reaching state-of-the-art full-precision accuracy in complex tasks.","These achievements present valuable opportunities for accelerating neural networks by computing in reduced precision.","Implementing it on FPGAs can take advantage of bit-level reconfigurability, which is not available on conventional CPUs and GPUs.","Simultaneously, the high data intensity of neural network processing has inspired computing-in-memory paradigms, including on FPGA platforms.","By programming the effects of trained model weights as lookup operations in soft logic, the transfer of weight data from memory units can be avoided, alleviating the memory bottleneck.","However, previous methods face poor scalability - the high logic utilisation limiting them to small networks/sub-networks of binary models with low accuracy.","In this paper, we introduce Table Lookup Multiply-Accumulate (TLMAC) as a framework to compile and optimise quantised neural networks for scalable lookup-based processing.","TLMAC clusters and maps unique groups of weights to lookup-based processing elements, enabling highly parallel computation while taking advantage of parameter redundancy.","Further place and route algorithms are proposed to reduce LUT utilisation and routing congestion.","We demonstrate that TLMAC significantly improves the scalability of previous related works.","Our efficient logic mapping and high degree of reuse enables entire ImageNet-scale quantised models with full-precision accuracy to be implemented using lookup-based computing on one commercially available FPGA."],"url":"http://arxiv.org/abs/2403.11414v1","category":"cs.AR"}
{"created":"2024-03-18 01:57:50","title":"Laconic: Streamlined Load Balancers for SmartNICs","abstract":"Load balancers are pervasively used inside today's clouds to scalably distribute network requests across data center servers. Given the extensive use of load balancers and their associated operating costs, several efforts have focused on improving their efficiency by implementing Layer-4 load-balancing logic within the kernel or using hardware acceleration. This work explores whether the more complex and connection-oriented Layer-7 load-balancing capability can also benefit from hardware acceleration. In particular, we target the offloading of load-balancing capability onto programmable SmartNICs. We fully leverage the cost and energy efficiency of SmartNICs using three key ideas. First, we argue that a full and complex TCP/IP stack is not required for Layer-7 load balancers and instead propose a lightweight forwarding agent on the SmartNIC. Second, we develop connection management data structures with a high degree of concurrency with minimal synchronization when executed on multi-core SmartNICs. Finally, we describe how the load-balancing logic could be accelerated using custom packet-processing accelerators on SmartNICs. We prototype Laconic on two types of SmartNIC hardware, achieving over 150 Gbps throughput using all cores on BlueField-2, while a single SmartNIC core achieves 8.7x higher throughput and comparable latency to Nginx on a single x86 core.","sentences":["Load balancers are pervasively used inside today's clouds to scalably distribute network requests across data center servers.","Given the extensive use of load balancers and their associated operating costs, several efforts have focused on improving their efficiency by implementing Layer-4 load-balancing logic within the kernel or using hardware acceleration.","This work explores whether the more complex and connection-oriented Layer-7 load-balancing capability can also benefit from hardware acceleration.","In particular, we target the offloading of load-balancing capability onto programmable SmartNICs.","We fully leverage the cost and energy efficiency of SmartNICs using three key ideas.","First, we argue that a full and complex TCP/IP stack is not required for Layer-7 load balancers and instead propose a lightweight forwarding agent on the SmartNIC.","Second, we develop connection management data structures with a high degree of concurrency with minimal synchronization when executed on multi-core SmartNICs.","Finally, we describe how the load-balancing logic could be accelerated using custom packet-processing accelerators on SmartNICs.","We prototype Laconic on two types of SmartNIC hardware, achieving over 150 Gbps throughput using all cores on BlueField-2, while a single SmartNIC core achieves 8.7x higher throughput and comparable latency to Nginx on a single x86 core."],"url":"http://arxiv.org/abs/2403.11411v1","category":"cs.NI"}
{"created":"2024-03-18 01:37:58","title":"Dimensions of harmonic measures in percolation clusters on hyperbolic groups","abstract":"For the simple random walks in percolation clusters on hyperbolic groups, we show that the associated harmonic measures are exact dimensional and their Hausdorff dimensions are equal to the entropy over the speed. Our method is inspired by cluster relations introduced by Gaboriau and applies to a large class of random environments on the groups.","sentences":["For the simple random walks in percolation clusters on hyperbolic groups, we show that the associated harmonic measures are exact dimensional and their Hausdorff dimensions are equal to the entropy over the speed.","Our method is inspired by cluster relations introduced by Gaboriau and applies to a large class of random environments on the groups."],"url":"http://arxiv.org/abs/2403.11406v1","category":"math.PR"}
{"created":"2024-03-18 01:22:08","title":"Tidal Formation of dark matter deficit diffuse galaxy NGC1052-DF2 by SIDM","abstract":"Observations have revealed a significant dark matter deficit in the ultra-diffuse galaxy NGC1052-DF2 (DF2). It is widely accepted that the formation of this unique galaxy can be attributed to the tidal stripping of its host galaxy, NGC1052. In this study, we simulate the evolution of a satellite system containing globular clusters (GCs) within an accreting host halo in the framework of self-interacting dark matter (SIDM). Our simulation results suggest that the heightened tidal stripping resulting from self-interaction can give rise to the transformation of a conventional dwarf galaxy into a galaxy resembling DF2 in all its observed properties. By comparing the simulation results with identical initial conditions in both the standard cold dark matter (CDM) and SIDM models, we find that the latter is more likely to replicate the properties of DF2. Furthermore, we demonstrate that a DF2 analog can also be produced on an orbit with a greater pericenter distance by increasing the strength of self-interaction. This suggests that the issue of extreme orbital parameters can be mitigated by implementing the SIDM model. The distributions of the GC population derived in our SIDM simulation are consistent with the observed characteristics of DF2. For comparison, we also explored the potential for achieving GC distributions in the context of CDM.","sentences":["Observations have revealed a significant dark matter deficit in the ultra-diffuse galaxy NGC1052-DF2 (DF2).","It is widely accepted that the formation of this unique galaxy can be attributed to the tidal stripping of its host galaxy, NGC1052.","In this study, we simulate the evolution of a satellite system containing globular clusters (GCs) within an accreting host halo in the framework of self-interacting dark matter (SIDM).","Our simulation results suggest that the heightened tidal stripping resulting from self-interaction can give rise to the transformation of a conventional dwarf galaxy into a galaxy resembling DF2 in all its observed properties.","By comparing the simulation results with identical initial conditions in both the standard cold dark matter (CDM) and SIDM models, we find that the latter is more likely to replicate the properties of DF2.","Furthermore, we demonstrate that a DF2 analog can also be produced on an orbit with a greater pericenter distance by increasing the strength of self-interaction.","This suggests that the issue of extreme orbital parameters can be mitigated by implementing the SIDM model.","The distributions of the GC population derived in our SIDM simulation are consistent with the observed characteristics of DF2.","For comparison, we also explored the potential for achieving GC distributions in the context of CDM."],"url":"http://arxiv.org/abs/2403.11403v1","category":"astro-ph.CO"}
{"created":"2024-03-18 01:14:47","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","abstract":"The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.","sentences":["The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text.","However, the nature of multimodal models leads to significant expenses in the creation of training data.","Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity.","Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset.","Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches."],"url":"http://arxiv.org/abs/2403.11399v1","category":"cs.CL"}
{"created":"2024-03-18 01:03:38","title":"Spontaneous Symmetry Breaking and Panic Escape","abstract":"Panic-induced herding in individuals often leads to social disasters, resulting in people being trapped and trampled in crowd stampedes triggered by panic. We introduce a novel approach that offers fresh insights into studying the phenomenon of asymmetrical panic-induced escape. Our approach is based on the concept of Spontaneous Symmetry Breaking (SSB), a fundamental governing mechanism in the Physical Sciences. By applying the principles of SSB, we elucidate how asymmetrical panic-induced herding in individuals occurs. We highlight that understanding panic escape and preventing catastrophic situations can be achieved through two crucial parameters: \"population density\" control and \"communication (or information transfer)\" among individuals in a crowd. The interplay of these two parameters is responsible for either breaking or restoring the symmetry of a system. We describe how these parameters are set by design conditions as well as crowd control. Based on these parameters, we discuss strategies for preventing potential social disasters caused by asymmetrical panic escape.","sentences":["Panic-induced herding in individuals often leads to social disasters, resulting in people being trapped and trampled in crowd stampedes triggered by panic.","We introduce a novel approach that offers fresh insights into studying the phenomenon of asymmetrical panic-induced escape.","Our approach is based on the concept of Spontaneous Symmetry Breaking (SSB), a fundamental governing mechanism in the Physical Sciences.","By applying the principles of SSB, we elucidate how asymmetrical panic-induced herding in individuals occurs.","We highlight that understanding panic escape and preventing catastrophic situations can be achieved through two crucial parameters: \"population density\" control and \"communication (or information transfer)\" among individuals in a crowd.","The interplay of these two parameters is responsible for either breaking or restoring the symmetry of a system.","We describe how these parameters are set by design conditions as well as crowd control.","Based on these parameters, we discuss strategies for preventing potential social disasters caused by asymmetrical panic escape."],"url":"http://arxiv.org/abs/2403.11394v1","category":"physics.soc-ph"}
{"created":"2024-03-18 00:22:30","title":"A Systematic Review of XR-based Remote Human-Robot Interaction Systems","abstract":"This survey provides an exhaustive review of the applications of extended reality (XR) technologies in the field of remote human-computer interaction (HRI). We developed a systematic search strategy based on the PRISMA methodology. From the initial 2,561 articles selected, 100 research papers that met our inclusion criteria were included. We categorized and summarized the domain in detail, delving into XR technologies, including augmented reality (AR), virtual reality (VR), and mixed reality (MR), and their applications in facilitating intuitive and effective remote control and interaction with robotic systems.The survey highlights existing articles on the application of XR technologies, user experience enhancement, and various interaction designs for XR in remote HRI, providing insights into current trends and future directions. We also identified potential gaps and opportunities for future research to improve remote HRI systems through XR technology to guide and inform future XR and robotics research.","sentences":["This survey provides an exhaustive review of the applications of extended reality (XR) technologies in the field of remote human-computer interaction (HRI).","We developed a systematic search strategy based on the PRISMA methodology.","From the initial 2,561 articles selected, 100 research papers that met our inclusion criteria were included.","We categorized and summarized the domain in detail, delving into XR technologies, including augmented reality (AR), virtual reality (VR), and mixed reality (MR), and their applications in facilitating intuitive and effective remote control and interaction with robotic systems.","The survey highlights existing articles on the application of XR technologies, user experience enhancement, and various interaction designs for XR in remote HRI, providing insights into current trends and future directions.","We also identified potential gaps and opportunities for future research to improve remote HRI systems through XR technology to guide and inform future XR and robotics research."],"url":"http://arxiv.org/abs/2403.11384v1","category":"cs.HC"}
{"created":"2024-03-18 00:19:52","title":"On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion","abstract":"Quadrupedal robots excel in mobility, navigating complex terrains with agility. However, their complex control systems present challenges that are still far from being fully addressed. In this paper, we introduce the use of Sample-Based Stochastic control strategies for quadrupedal robots, as an alternative to traditional optimal control laws. We show that Sample-Based Stochastic methods, supported by GPU acceleration, can be effectively applied to real quadruped robots. In particular, in this work, we focus on achieving gait frequency adaptation, a notable challenge in quadrupedal locomotion for gradient-based methods. To validate the effectiveness of Sample-Based Stochastic controllers we test two distinct approaches for quadrupedal robots and compare them against a conventional gradient-based Model Predictive Control system. Our findings, validated both in simulation and on a real 21Kg Aliengo quadruped, demonstrate that our method is on par with a traditional Model Predictive Control strategy when the robot is subject to zero or moderate disturbance, while it surpasses gradient-based methods in handling sustained external disturbances, thanks to the straightforward gait adaptation strategy that is possible to achieve within their formulation.","sentences":["Quadrupedal robots excel in mobility, navigating complex terrains with agility.","However, their complex control systems present challenges that are still far from being fully addressed.","In this paper, we introduce the use of Sample-Based Stochastic control strategies for quadrupedal robots, as an alternative to traditional optimal control laws.","We show that Sample-Based Stochastic methods, supported by GPU acceleration, can be effectively applied to real quadruped robots.","In particular, in this work, we focus on achieving gait frequency adaptation, a notable challenge in quadrupedal locomotion for gradient-based methods.","To validate the effectiveness of Sample-Based Stochastic controllers we test two distinct approaches for quadrupedal robots and compare them against a conventional gradient-based Model Predictive Control system.","Our findings, validated both in simulation and on a real 21Kg Aliengo quadruped, demonstrate that our method is on par with a traditional Model Predictive Control strategy when the robot is subject to zero or moderate disturbance, while it surpasses gradient-based methods in handling sustained external disturbances, thanks to the straightforward gait adaptation strategy that is possible to achieve within their formulation."],"url":"http://arxiv.org/abs/2403.11383v1","category":"cs.RO"}
{"created":"2024-03-17 23:23:40","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic Environments using Graph Neural Networks","abstract":"The assumption of a static environment is common in many geometric computer vision tasks like SLAM but limits their applicability in highly dynamic scenes. Since these tasks rely on identifying point correspondences between input images within the static part of the environment, we propose a graph neural network-based sparse feature matching network designed to perform robust matching under challenging conditions while excluding keypoints on moving objects. We employ a similar scheme of attentional aggregation over graph edges to enhance keypoint representations as state-of-the-art feature-matching networks but augment the graph with epipolar and temporal information and vastly reduce the number of graph edges. Furthermore, we introduce a self-supervised training scheme to extract pseudo labels for image pairs in dynamic environments from exclusively unprocessed visual-inertial data. A series of experiments show the superior performance of our network as it excludes keypoints on moving objects compared to state-of-the-art feature matching networks while still achieving similar results regarding conventional matching metrics. When integrated into a SLAM system, our network significantly improves performance, especially in highly dynamic scenes.","sentences":["The assumption of a static environment is common in many geometric computer vision tasks like SLAM but limits their applicability in highly dynamic scenes.","Since these tasks rely on identifying point correspondences between input images within the static part of the environment, we propose a graph neural network-based sparse feature matching network designed to perform robust matching under challenging conditions while excluding keypoints on moving objects.","We employ a similar scheme of attentional aggregation over graph edges to enhance keypoint representations as state-of-the-art feature-matching networks but augment the graph with epipolar and temporal information and vastly reduce the number of graph edges.","Furthermore, we introduce a self-supervised training scheme to extract pseudo labels for image pairs in dynamic environments from exclusively unprocessed visual-inertial data.","A series of experiments show the superior performance of our network as it excludes keypoints on moving objects compared to state-of-the-art feature matching networks while still achieving similar results regarding conventional matching metrics.","When integrated into a SLAM system, our network significantly improves performance, especially in highly dynamic scenes."],"url":"http://arxiv.org/abs/2403.11370v1","category":"cs.CV"}
{"created":"2024-03-17 23:06:12","title":"3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization","abstract":"This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment. By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate. To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree. This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC). Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique. The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset.","sentences":["This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting.","Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment.","By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate.","To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.","This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC).","Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique.","The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset."],"url":"http://arxiv.org/abs/2403.11367v1","category":"cs.CV"}
{"created":"2024-03-17 22:44:26","title":"A continuous boostlet transform for acoustic waves in space-time","abstract":"Sparse representation systems that encode the signal architecture have had an exceptional impact on sampling and compression paradigms. Remarkable examples are multi-scale directional systems, which, similar to our vision system, encode the underlying architecture of natural images with sparse features. Inspired by this philosophy, the present study introduces a representation system for acoustic waves in 2D space-time, referred to as the boostlet transform, which encodes sparse features of natural acoustic fields with the Poincar\\'e group and isotropic dilations. Continuous boostlets, $\\psi_{a,\\theta,\\tau}(\\varsigma) = a^{-1} \\psi \\left(D_a^{-1} B_\\theta^{-1}(\\varsigma-\\tau)\\right) \\in L^2(\\mathbb{R}^2)$, are spatiotemporal functions parametrized with dilations $a > 0$, Lorentz boosts $\\theta \\in \\mathbb{R}$, and translations $\\smash{\\tau \\in \\mathbb{R}^2}$ in space--time. The admissibility condition requires that boostlets are supported away from the acoustic radiation cone, i.e., have phase velocities other than the speed of sound, resulting in a peculiar scaling function. The continuous boostlet transform is an isometry for $L^2(\\mathbb{R}^2)$, and a sparsity analysis with experimentally measured fields indicates that boostlet coefficients decay faster than wavelets, curvelets, wave atoms, and shearlets. The uncertainty principles and minimizers associated with the boostlet transform are derived and interpreted physically.","sentences":["Sparse representation systems that encode the signal architecture have had an exceptional impact on sampling and compression paradigms.","Remarkable examples are multi-scale directional systems, which, similar to our vision system, encode the underlying architecture of natural images with sparse features.","Inspired by this philosophy, the present study introduces a representation system for acoustic waves in 2D space-time, referred to as the boostlet transform, which encodes sparse features of natural acoustic fields with the Poincar\\'e group and isotropic dilations.","Continuous boostlets, $\\psi_{a,\\theta,\\tau}(\\varsigma) = a^{-1} \\psi \\left(D_a^{-1} B_\\theta^{-1}(\\varsigma-\\tau)\\right)","\\in L^2(\\mathbb{R}^2)$, are spatiotemporal functions parametrized with dilations $a > 0$, Lorentz boosts $\\theta \\in \\mathbb{R}$, and translations $\\smash{\\tau \\in \\mathbb{R}^2}$ in space--time.","The admissibility condition requires that boostlets are supported away from the acoustic radiation cone, i.e., have phase velocities other than the speed of sound, resulting in a peculiar scaling function.","The continuous boostlet transform is an isometry for $L^2(\\mathbb{R}^2)$, and a sparsity analysis with experimentally measured fields indicates that boostlet coefficients decay faster than wavelets, curvelets, wave atoms, and shearlets.","The uncertainty principles and minimizers associated with the boostlet transform are derived and interpreted physically."],"url":"http://arxiv.org/abs/2403.11362v1","category":"physics.flu-dyn"}
{"created":"2024-03-17 22:12:48","title":"Multiscale Quantile Regression with Local Error Control","abstract":"For robust and efficient detection of change points, we introduce a novel methodology MUSCLE (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile. It leverages multiple tests for quantile changes over different scales and locations, and variational estimation. Unlike the often adopted global error control, MUSCLE focuses on local errors defined on individual segments, significantly improving detection power in finding change points. Meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. Further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions. Both match (up to log factors) the minimax optimality results in the Gaussian setup. All theories hold under the only distributional assumption of serial independence. Incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing MUSCLE. Extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. An implementation via R package muscle is available from GitHub.","sentences":["For robust and efficient detection of change points, we introduce a novel methodology MUSCLE (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile.","It leverages multiple tests for quantile changes over different scales and locations, and variational estimation.","Unlike the often adopted global error control, MUSCLE focuses on local errors defined on individual segments, significantly improving detection power in finding change points.","Meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter.","Further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions.","Both match (up to log factors) the minimax optimality results in the Gaussian setup.","All theories hold under the only distributional assumption of serial independence.","Incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing MUSCLE.","Extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness.","An implementation via R package muscle is available from GitHub."],"url":"http://arxiv.org/abs/2403.11356v1","category":"stat.ME"}
{"created":"2024-03-17 22:07:47","title":"Coherent insulator at arbitrary frequency in a driven atomtronic transistor","abstract":"We use numerical approach to study non-equilibrium transport of atomic gas in a driven optical lattice atomtronic transistor. The shaken optical lattice transistor displays a property of insulator within some regions of shaking frequency and shaking strength. It is proved that appearance of the insulation is directly connected to coherence of the system. Coherence of the system is accompanied by coherent trapping of non-equilibrium atomic gas in one of the optical wells, which stops atomic currents. Comparing with the effective Hamiltonian approach in Floquet engineering, the time-dependent Hamiltonian approach could be used in any frequency regime of periodically driven quantum system.","sentences":["We use numerical approach to study non-equilibrium transport of atomic gas in a driven optical lattice atomtronic transistor.","The shaken optical lattice transistor displays a property of insulator within some regions of shaking frequency and shaking strength.","It is proved that appearance of the insulation is directly connected to coherence of the system.","Coherence of the system is accompanied by coherent trapping of non-equilibrium atomic gas in one of the optical wells, which stops atomic currents.","Comparing with the effective Hamiltonian approach in Floquet engineering, the time-dependent Hamiltonian approach could be used in any frequency regime of periodically driven quantum system."],"url":"http://arxiv.org/abs/2403.11355v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-17 21:52:58","title":"Kinetic inductance traveling wave amplifier designs for practical microwave readout applications","abstract":"A Kinetic Inductance Traveling Wave amplifier (KIT) utilizes the nonlinear kinetic inductance of superconducting films, particularly Niobium Titanium Nitride (NbTiN), for parametric amplification. These amplifiers achieve remarkable performance in terms of gain, bandwidth, compression power, and frequently approach the quantum limit for noise. However, most KIT demonstrations have been isolated from practical device readout systems. Using a KIT as the first amplifier in the readout chain of an unoptimized microwave SQUID multiplexer coupled to a transition-edge sensor microcalorimeter we see an initial improvement in the flux noise. One challenge in KIT integration is the considerable microwave pump power required to drive the non-linearity. To address this, we have initiated efforts to reduce the pump power by using thinner NbTiN films and an inverted microstrip transmission line design. In this article, we present the new transmission line design, fabrication procedure, and initial device characterization -- including gain and added noise. These devices exhibit over 10 dB of gain with a 3 dB bandwidth of approximately 5.5-7.25 GHz, a maximum practical gain of 12 dB and typical gain ripple under 4 dB peak-to-peak. We observe an appreciable impedance mismatch in the NbTiN transmission line, which is likely the source of the majority of the gain ripple. Finally we perform an initial noise characterization and demonstrate system-added noise of three quanta or less over nearly the entire 3 dB bandwidth.","sentences":["A Kinetic Inductance Traveling Wave amplifier (KIT) utilizes the nonlinear kinetic inductance of superconducting films, particularly Niobium Titanium Nitride (NbTiN), for parametric amplification.","These amplifiers achieve remarkable performance in terms of gain, bandwidth, compression power, and frequently approach the quantum limit for noise.","However, most KIT demonstrations have been isolated from practical device readout systems.","Using a KIT as the first amplifier in the readout chain of an unoptimized microwave SQUID multiplexer coupled to a transition-edge sensor microcalorimeter we see an initial improvement in the flux noise.","One challenge in KIT integration is the considerable microwave pump power required to drive the non-linearity.","To address this, we have initiated efforts to reduce the pump power by using thinner NbTiN films and an inverted microstrip transmission line design.","In this article, we present the new transmission line design, fabrication procedure, and initial device characterization -- including gain and added noise.","These devices exhibit over 10 dB of gain with a 3 dB bandwidth of approximately 5.5-7.25 GHz, a maximum practical gain of 12 dB and typical gain ripple under 4 dB peak-to-peak.","We observe an appreciable impedance mismatch in the NbTiN transmission line, which is likely the source of the majority of the gain ripple.","Finally we perform an initial noise characterization and demonstrate system-added noise of three quanta or less over nearly the entire 3 dB bandwidth."],"url":"http://arxiv.org/abs/2403.11354v1","category":"quant-ph"}
{"created":"2024-03-17 20:45:39","title":"Maximizing information obtainable by quantum sensors through the Quantum Zeno Effect","abstract":"Efficient quantum sensing technologies rely on precise control of quantum sensors, particularly two-level systems or qubits, to optimize estimation processes. We here exploit the Quantum Zeno Effect (QZE) as a tool for maximizing information obtainable by quantum sensors, with a specific focus on the level avoided crossing (LAC) phenomenon in qubit systems. While the estimation of the LAC energy splitting has been extensively studied, we emphasize the crucial role that the QZE can play in estimating the coupling strength. We introduce the concept of information amplification by the QZE for a LAC system under off-resonant conditions. The proposed approach has implications for AC magnetic field sensing and the caracterization of complex systems, including many-spin systems requiring the estimation of spin-spin couplings. Overall, our findings contribute to the advancement of quantum sensing by leveraging the QZE for improved control and information extraction.","sentences":["Efficient quantum sensing technologies rely on precise control of quantum sensors, particularly two-level systems or qubits, to optimize estimation processes.","We here exploit the Quantum Zeno Effect (QZE) as a tool for maximizing information obtainable by quantum sensors, with a specific focus on the level avoided crossing (LAC) phenomenon in qubit systems.","While the estimation of the LAC energy splitting has been extensively studied, we emphasize the crucial role that the QZE can play in estimating the coupling strength.","We introduce the concept of information amplification by the QZE for a LAC system under off-resonant conditions.","The proposed approach has implications for AC magnetic field sensing and the caracterization of complex systems, including many-spin systems requiring the estimation of spin-spin couplings.","Overall, our findings contribute to the advancement of quantum sensing by leveraging the QZE for improved control and information extraction."],"url":"http://arxiv.org/abs/2403.11339v1","category":"quant-ph"}
{"created":"2024-03-17 20:23:42","title":"Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects","abstract":"Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synthetic social network datasets reveals our method's on-par or superior efficacy in precise causal effect estimation. Further, we illustrate the practical application of our method through a case study that investigates the impact of Self-Help Group participation on financial risk tolerance. The results indicate a significant positive direct effect, underscoring the potential of our approach in social network analysis. Additionally, we explore the effects of network sparsity on estimation performance.","sentences":["Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units.","We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network.","Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects.","We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient.","A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synthetic social network datasets reveals our method's on-par or superior efficacy in precise causal effect estimation.","Further, we illustrate the practical application of our method through a case study that investigates the impact of Self-Help Group participation on financial risk tolerance.","The results indicate a significant positive direct effect, underscoring the potential of our approach in social network analysis.","Additionally, we explore the effects of network sparsity on estimation performance."],"url":"http://arxiv.org/abs/2403.11332v1","category":"cs.LG"}
{"created":"2024-03-17 20:19:17","title":"AQM: A Refresh of the Abstract Qubit Model for Quantum Co-design","abstract":"Qubits are the fundamental building blocks of quantum information science and applications, whose concept is widely utilized in both quantum physics and quantum computation. While the significance of qubits and their implementation in physical devices have been extensively examined, we posit that now is the right time to revisit this understanding. In this paper, we introduce an abstract qubit model (AQM), offering a mathematical framework for higher-level algorithms and applications, and setting forth criteria for lower-level physical devices to enable quantum computation. We first provide a comprehensive definition of \"qubits\", regarded as the foundational premise for quantum algorithms (bottom-up support), and examine their requisites for devices (top-down demand). We then investigate the feasibility of moderating specific requirements, thereby broadening device support while considering techniques that tradeoff extra costs to counterbalance this moderation. Lastly, we delve into the quantum applications that only require incomplete qubits, and discuss the physical systems having restricted AQM support but are still useful in quantum applications. AQM may serve as an intermediate interface between quantum algorithms and devices, facilitating quantum algorithm-device co-design.","sentences":["Qubits are the fundamental building blocks of quantum information science and applications, whose concept is widely utilized in both quantum physics and quantum computation.","While the significance of qubits and their implementation in physical devices have been extensively examined, we posit that now is the right time to revisit this understanding.","In this paper, we introduce an abstract qubit model (AQM), offering a mathematical framework for higher-level algorithms and applications, and setting forth criteria for lower-level physical devices to enable quantum computation.","We first provide a comprehensive definition of \"qubits\", regarded as the foundational premise for quantum algorithms (bottom-up support), and examine their requisites for devices (top-down demand).","We then investigate the feasibility of moderating specific requirements, thereby broadening device support while considering techniques that tradeoff extra costs to counterbalance this moderation.","Lastly, we delve into the quantum applications that only require incomplete qubits, and discuss the physical systems having restricted AQM support but are still useful in quantum applications.","AQM may serve as an intermediate interface between quantum algorithms and devices, facilitating quantum algorithm-device co-design."],"url":"http://arxiv.org/abs/2403.11329v1","category":"quant-ph"}
{"created":"2024-03-17 19:45:34","title":"Anomalous kaon correlations measured in Pb-Pb collisions at the LHC as evidence for the melting and refreezing of the QCD vacuum","abstract":"Measurements of the dynamical correlations between neutral and charged kaons in central Pb-Pb collisions at $\\sqrt{s_{NN}} = 2.76$ TeV by the ALICE Collaboration display anomalous behavior relative to conventional heavy-ion collision simulators. We consider other conventional statistical models, none of which can reproduce the magnitude and centrality dependence of the correlations. The data can be reproduced by coherent emission from domains which grow in number and volume with increasing centrality. We study the dynamical evolution of the strange quark condensate and show that the energy released during the expansion and cooling of the system may be sufficient to explain the anomaly.","sentences":["Measurements of the dynamical correlations between neutral and charged kaons in central Pb-Pb collisions at $\\sqrt{s_{NN}} = 2.76$ TeV by the ALICE Collaboration display anomalous behavior relative to conventional heavy-ion collision simulators.","We consider other conventional statistical models, none of which can reproduce the magnitude and centrality dependence of the correlations.","The data can be reproduced by coherent emission from domains which grow in number and volume with increasing centrality.","We study the dynamical evolution of the strange quark condensate and show that the energy released during the expansion and cooling of the system may be sufficient to explain the anomaly."],"url":"http://arxiv.org/abs/2403.11318v1","category":"hep-ph"}
{"created":"2024-03-17 18:14:24","title":"Probing multi-particle bunching from intermittency analysis in relativistic heavy-ion collisions","abstract":"It has been demonstrated that intermittency, a self-similar correlation regarding the size of the phase space volume, exhibits sensitivity to particle bunching within a system situated in the two-dimensional cell. Based on recent findings concerning fluctuations in charge particle density observed in central Au + Au collisions at various energies ranging from $\\sqrt{s_{NN}}$ = 7.7 to 200 GeV at RHIC/STAR, it is proposed that analyzing intermittency in relativistic heavy-ion collisions could serve as a method to investigate density fluctuations linked to correlation phenomena. Our understanding of particle production mechanisms may be enriched by using intermittency analysis to gain new information contained in them that was previously unavailable.","sentences":["It has been demonstrated that intermittency, a self-similar correlation regarding the size of the phase space volume, exhibits sensitivity to particle bunching within a system situated in the two-dimensional cell.","Based on recent findings concerning fluctuations in charge particle density observed in central Au + Au collisions at various energies ranging from $\\sqrt{s_{NN}}$ = 7.7 to 200 GeV at RHIC/STAR, it is proposed that analyzing intermittency in relativistic heavy-ion collisions could serve as a method to investigate density fluctuations linked to correlation phenomena.","Our understanding of particle production mechanisms may be enriched by using intermittency analysis to gain new information contained in them that was previously unavailable."],"url":"http://arxiv.org/abs/2403.11294v1","category":"nucl-th"}
{"created":"2024-03-17 18:06:06","title":"Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning","abstract":"The maintenance, archiving and usage of the design drawings is cumbersome in physical form in different industries for longer period. It is hard to extract information by simple scanning of drawing sheets. Converting them to their digital formats such as Computer-Aided Design (CAD), with needed knowledge extraction can solve this problem. The conversion of these machine drawings to its digital form is a crucial challenge which requires advanced techniques. This research proposes an innovative methodology utilizing Deep Learning methods. The approach employs object detection model, such as Yolov7, Faster R-CNN, to detect physical drawing objects present in the images followed by, edge detection algorithms such as canny filter to extract and refine the identified lines from the drawing region and curve detection techniques to detect circle. Also ornaments (complex shapes) within the drawings are extracted. To ensure comprehensive conversion, an Optical Character Recognition (OCR) tool is integrated to identify and extract the text elements from the drawings. The extracted data which includes the lines, shapes and text is consolidated and stored in a structured comma separated values(.csv) file format. The accuracy and the efficiency of conversion is evaluated. Through this, conversion can be automated to help organizations enhance their productivity, facilitate seamless collaborations and preserve valuable design information in a digital format easily accessible. Overall, this study contributes to the advancement of CAD conversions, providing accurate results from the translating process. Future research can focus on handling diverse drawing types, enhanced accuracy in shape and line detection and extraction.","sentences":["The maintenance, archiving and usage of the design drawings is cumbersome in physical form in different industries for longer period.","It is hard to extract information by simple scanning of drawing sheets.","Converting them to their digital formats such as Computer-Aided Design (CAD), with needed knowledge extraction can solve this problem.","The conversion of these machine drawings to its digital form is a crucial challenge which requires advanced techniques.","This research proposes an innovative methodology utilizing Deep Learning methods.","The approach employs object detection model, such as Yolov7, Faster R-CNN, to detect physical drawing objects present in the images followed by, edge detection algorithms such as canny filter to extract and refine the identified lines from the drawing region and curve detection techniques to detect circle.","Also ornaments (complex shapes) within the drawings are extracted.","To ensure comprehensive conversion, an Optical Character Recognition (OCR) tool is integrated to identify and extract the text elements from the drawings.","The extracted data which includes the lines, shapes and text is consolidated and stored in a structured comma separated values(.csv) file format.","The accuracy and the efficiency of conversion is evaluated.","Through this, conversion can be automated to help organizations enhance their productivity, facilitate seamless collaborations and preserve valuable design information in a digital format easily accessible.","Overall, this study contributes to the advancement of CAD conversions, providing accurate results from the translating process.","Future research can focus on handling diverse drawing types, enhanced accuracy in shape and line detection and extraction."],"url":"http://arxiv.org/abs/2403.11291v1","category":"cs.CV"}
{"created":"2024-03-17 17:48:51","title":"Benefits of non-adiabatic quantum control in quantum computation through spin qubit systems","abstract":"This is evident that the controllable quantum systems can be the reliable building blocks for Quantum computation. In reality we are witnessing the progress towards making the idea tractable enough, though optimistic but the threshold is not very near to us. The dawn of quantum computation has begun. In the future, we hope to see a full fledged operationally stable quantum computer which can solve the problems beyond the scope of classical digital computers. We may call it quantum supremacy. Nevertheless, we should not forget that there are problems which demand classical computers to be in the game for a better performance in comparison to the same through quantum devices. In the current stage of computing technology, the most beneficial area is nothing but an hybrid approach and that is for no doubt will reign the market for the next five to ten years. This hybrid aspect has several directions such as simulating quantum computation on a classical computer. Keeping both the aspect, computation through real physical devices and simulation on a classical computer by accessing available quantum computers for cloud computing, some advantages have been discussed in this article which will be elaborated as well in future articles. These advantages are inherent if we can achieve proper non-adiabatic control over the spin system in the laboratory. Otherwise these aspects can always be simulated by using quantum algorithms to see whether they can be useful in comparison to a purely classical computing machine. This is no doubt a new window for progress in the direction of quantum computation.","sentences":["This is evident that the controllable quantum systems can be the reliable building blocks for Quantum computation.","In reality we are witnessing the progress towards making the idea tractable enough, though optimistic but the threshold is not very near to us.","The dawn of quantum computation has begun.","In the future, we hope to see a full fledged operationally stable quantum computer which can solve the problems beyond the scope of classical digital computers.","We may call it quantum supremacy.","Nevertheless, we should not forget that there are problems which demand classical computers to be in the game for a better performance in comparison to the same through quantum devices.","In the current stage of computing technology, the most beneficial area is nothing but an hybrid approach and that is for no doubt will reign the market for the next five to ten years.","This hybrid aspect has several directions such as simulating quantum computation on a classical computer.","Keeping both the aspect, computation through real physical devices and simulation on a classical computer by accessing available quantum computers for cloud computing, some advantages have been discussed in this article which will be elaborated as well in future articles.","These advantages are inherent if we can achieve proper non-adiabatic control over the spin system in the laboratory.","Otherwise these aspects can always be simulated by using quantum algorithms to see whether they can be useful in comparison to a purely classical computing machine.","This is no doubt a new window for progress in the direction of quantum computation."],"url":"http://arxiv.org/abs/2403.11288v1","category":"quant-ph"}
{"created":"2024-03-17 17:37:21","title":"Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance","abstract":"We propose a hybrid feedback control scheme for the autonomous robot navigation problem in three-dimensional environments with arbitrarily-shaped convex obstacles. The proposed hybrid control strategy, which consists in switching between the move-to-target mode and the obstacle-avoidance mode, guarantees global asymptotic stability of the target location in the obstacle-free workspace. We also provide a procedure for the implementation of the proposed hybrid controller in a priori unknown environments and validate its effectiveness through simulation results.","sentences":["We propose a hybrid feedback control scheme for the autonomous robot navigation problem in three-dimensional environments with arbitrarily-shaped convex obstacles.","The proposed hybrid control strategy, which consists in switching between the move-to-target mode and the obstacle-avoidance mode, guarantees global asymptotic stability of the target location in the obstacle-free workspace.","We also provide a procedure for the implementation of the proposed hybrid controller in a priori unknown environments and validate its effectiveness through simulation results."],"url":"http://arxiv.org/abs/2403.11279v1","category":"cs.RO"}
{"created":"2024-03-17 17:33:25","title":"Bifurcation Analysis of an Influenza A (H1N1) Model with Treatment and Vaccination","abstract":"This study focuses on the modeling, mathematical analysis, developing theories, and numerical simulation of Influenza virus transmission. We have proved the existence, uniqueness, positivity, and boundedness of the solutions. Also, investigate the qualitative behavior of the models and find the basic reproduction number $(\\mathcal{R}_0)$ that guarantees the asymptotic stability of the disease-free and endemic equilibrium points. The local and global asymptotic stability of the disease free state and endemic equilibrium of the system is analyzed with the Lyapunov method, Routh-Hurwitz, and other criteria and presented graphically. This study helps to investigate the effectiveness of control policy and makes suggestions for alternative control policies. Bifurcation analyses are carried out to determine prevention strategies. Transcritical, Hopf, and backward bifurcation analyses are displayed analytically and numerically to show the dynamics of disease transmission in different cases. Moreover, analysis of contour plot, box plot, relative biases, phase portraits are presented to show the influential parameters to curtail the disease outbreak. We are interested in finding the nature of $\\mathcal{R}_0$, which determines whether the disease dies out or persists in the population. The findings indicate that the dynamics of the model are determined by the threshold parameter $\\mathcal{R}_0$.","sentences":["This study focuses on the modeling, mathematical analysis, developing theories, and numerical simulation of Influenza virus transmission.","We have proved the existence, uniqueness, positivity, and boundedness of the solutions.","Also, investigate the qualitative behavior of the models and find the basic reproduction number $(\\mathcal{R}_0)$ that guarantees the asymptotic stability of the disease-free and endemic equilibrium points.","The local and global asymptotic stability of the disease free state and endemic equilibrium of the system is analyzed with the Lyapunov method, Routh-Hurwitz, and other criteria and presented graphically.","This study helps to investigate the effectiveness of control policy and makes suggestions for alternative control policies.","Bifurcation analyses are carried out to determine prevention strategies.","Transcritical, Hopf, and backward bifurcation analyses are displayed analytically and numerically to show the dynamics of disease transmission in different cases.","Moreover, analysis of contour plot, box plot, relative biases, phase portraits are presented to show the influential parameters to curtail the disease outbreak.","We are interested in finding the nature of $\\mathcal{R}_0$, which determines whether the disease dies out or persists in the population.","The findings indicate that the dynamics of the model are determined by the threshold parameter $\\mathcal{R}_0$."],"url":"http://arxiv.org/abs/2403.11277v1","category":"q-bio.PE"}
{"created":"2024-03-17 17:12:28","title":"Greater than five-order-of-magnitude post-compression temporal contrast improvement with an ionization plasma grating","abstract":"High-intensity lasers require suppression of prepulses and other non-ideal temporal structure to avoid target disruption before the arrival of the main pulse. To address this, we demonstrate that ionization gratings act as a controllable optical switch for high-power light with a temporal contrast improvement of at least $3\\times10^5$ and a switching time less than 500 fs. We also show that a grating system can run for hours at 10 Hz without degradation. The contrast improvement from an ionization grating compares favorably to that achievable with plasma mirrors.","sentences":["High-intensity lasers require suppression of prepulses and other non-ideal temporal structure to avoid target disruption before the arrival of the main pulse.","To address this, we demonstrate that ionization gratings act as a controllable optical switch for high-power light with a temporal contrast improvement of at least $3\\times10^5$ and a switching time less than 500 fs.","We also show that a grating system can run for hours at 10 Hz without degradation.","The contrast improvement from an ionization grating compares favorably to that achievable with plasma mirrors."],"url":"http://arxiv.org/abs/2403.11275v1","category":"physics.optics"}
{"created":"2024-03-17 16:06:44","title":"Unveiling gravity's quantum fingerprint through gravitational waves","abstract":"A proposal for an improved theoretical model to illuminate the quantum nature of gravity is given. This model investigates the gravity-induced entanglement (GIE) phenomena, circumventing classical communication constraints of LOCC principle. Here a non-relativistic two dimensional quantum oscillator detector is coupled to linearly polarized gravitational waves (GWs). Exploiting the quantum nature of GWs, we observe the GIE within the oscillator quantum states. Since the model satisfies ``event'' as well as ``system'' localities, the observed GIE is much robust signature for quantum nature of gravity.","sentences":["A proposal for an improved theoretical model to illuminate the quantum nature of gravity is given.","This model investigates the gravity-induced entanglement (GIE) phenomena, circumventing classical communication constraints of LOCC principle.","Here a non-relativistic two dimensional quantum oscillator detector is coupled to linearly polarized gravitational waves (GWs).","Exploiting the quantum nature of GWs, we observe the GIE within the oscillator quantum states.","Since the model satisfies ``event'' as well as ``system'' localities, the observed GIE is much robust signature for quantum nature of gravity."],"url":"http://arxiv.org/abs/2403.11253v1","category":"gr-qc"}
{"created":"2024-03-17 15:51:21","title":"NeoNeXt: Novel neural network operator and architecture based on the patch-wise matrix multiplications","abstract":"Most of the computer vision architectures nowadays are built upon the well-known foundation operations: fully-connected layers, convolutions and multi-head self-attention blocks. In this paper we propose a novel foundation operation - NeoCell - which learns matrix patterns and performs patchwise matrix multiplications with the input data. The main advantages of the proposed operator are (1) simple implementation without need in operations like im2col, (2) low computational complexity (especially for large matrices) and (3) simple and flexible implementation of up-/down-sampling. We validate NeoNeXt family of models based on this operation on ImageNet-1K classification task and show that they achieve competitive quality.","sentences":["Most of the computer vision architectures nowadays are built upon the well-known foundation operations: fully-connected layers, convolutions and multi-head self-attention blocks.","In this paper we propose a novel foundation operation - NeoCell - which learns matrix patterns and performs patchwise matrix multiplications with the input data.","The main advantages of the proposed operator are (1) simple implementation without need in operations like im2col, (2) low computational complexity (especially for large matrices) and (3) simple and flexible implementation of up-/down-sampling.","We validate NeoNeXt family of models based on this operation on ImageNet-1K classification task and show that they achieve competitive quality."],"url":"http://arxiv.org/abs/2403.11251v1","category":"cs.CV"}
{"created":"2024-03-17 15:41:35","title":"Compact 3D Gaussian Splatting For Dense Visual SLAM","abstract":"Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes. However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed. To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids. A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids. Then we observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian geometric attributes, i.e., the parameters. Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss. Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.","sentences":["Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes.","However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed.","To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids.","A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids.","Then we observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian geometric attributes, i.e., the parameters.","Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss.","Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation."],"url":"http://arxiv.org/abs/2403.11247v1","category":"cs.CV"}
{"created":"2024-03-17 15:39:18","title":"Exploring Distance Query Processing in Edge Computing Environments","abstract":"In the context of changing travel behaviors and the expanding user base of Geographic Information System (GIS) services, conventional centralized architectures responsible for handling shortest distance queries are facing increasing challenges, such as heightened load pressure and longer response times. To mitigate these concerns, this study is the first to develop an edge computing framework specially tailored for processing distance queries. In conjunction with this innovative system, we have developed a straightforward, yet effective, labeling technique termed Border Labeling. Furthermore, we have devised and implemented a range of query strategies intended to capitalize on the capabilities of the edge computing infrastructure. Our experiments demonstrate that our solution surpasses other methods in terms of both indexing time and query speed across various road network datasets. The empirical evidence from our experiments supports the claim that our edge computing architecture significantly reduces the latency encountered by end-users, thus markedly decreasing their waiting times.","sentences":["In the context of changing travel behaviors and the expanding user base of Geographic Information System (GIS) services, conventional centralized architectures responsible for handling shortest distance queries are facing increasing challenges, such as heightened load pressure and longer response times.","To mitigate these concerns, this study is the first to develop an edge computing framework specially tailored for processing distance queries.","In conjunction with this innovative system, we have developed a straightforward, yet effective, labeling technique termed Border Labeling.","Furthermore, we have devised and implemented a range of query strategies intended to capitalize on the capabilities of the edge computing infrastructure.","Our experiments demonstrate that our solution surpasses other methods in terms of both indexing time and query speed across various road network datasets.","The empirical evidence from our experiments supports the claim that our edge computing architecture significantly reduces the latency encountered by end-users, thus markedly decreasing their waiting times."],"url":"http://arxiv.org/abs/2403.11246v1","category":"cs.DB"}
{"created":"2024-03-17 15:10:12","title":"Transverse Magnetic ENZ Resonators: Robustness and Optimal Shape Design","abstract":"We study certain ``geometric-invariant resonant cavities'' introduced by Liberal et. al in a 2016 Nature Comm. paper, modeled using the transverse magnetic reduction of Maxwell's equations. The cross-section consists of a dielectric inclusion surrounded by an ``epsilon-near-zero'' (ENZ) shell. When the shell has the right area, its interaction with the inclusion produces a resonance. Mathematically, the resonance is a nontrivial solution of a 2D divergence-form Helmoltz equation $\\nabla \\cdot \\left(\\varepsilon^{-1}(x,\\omega) \\nabla u \\right) + \\omega^2 \\mu u = 0$, where $\\varepsilon(x,\\omega)$ is the (complex-valued) dielectric permittivity, $\\omega$ is the frequency, $\\mu$ is the magnetic permeability, and a homogeneous Neumann condition is imposed at the outer boundary of the shell. This is a nonlinear eigenvalue problem, since $\\varepsilon$ depends on $\\omega$. Use of an ENZ material in the shell means that $\\varepsilon(x,\\omega)$ is nearly zero there, so the PDE is rather singular. Working with a Lorentz model for the dispersion of the ENZ material, we put the discussion of Liberal et.~al.~on a sound foundation by proving the existence of the anticipated resonance when the loss is sufficiently small. Our analysis is perturbative in character despite the apparently singular form of the PDE. While the existence of the resonance depends only on the area of the ENZ shell, the rate at which it decays depends on the shape of the shell. We consider an associated optimal design problem: what shape shell gives the slowest-decaying resonance? We prove that if the dielectric inclusion is a ball then the optimal shell is a concentric annulus. For an inclusion of any shape, we study a convex relaxation of the design problem using tools from convex duality, and discuss the conjecture that our relaxed problem amounts to considering homogenization-like limits of nearly optimal designs.","sentences":["We study certain ``geometric-invariant resonant cavities'' introduced by Liberal et.","al in a 2016 Nature Comm. paper, modeled using the transverse magnetic reduction of Maxwell's equations.","The cross-section consists of a dielectric inclusion surrounded by an ``epsilon-near-zero'' (ENZ) shell.","When the shell has the right area, its interaction with the inclusion produces a resonance.","Mathematically, the resonance is a nontrivial solution of a 2D divergence-form Helmoltz equation $\\nabla \\cdot \\left(\\varepsilon^{-1}(x,\\omega) \\nabla u \\right) + \\omega^2 \\mu u = 0$, where $\\varepsilon(x,\\omega)$ is the (complex-valued) dielectric permittivity, $\\omega$ is the frequency, $\\mu$ is the magnetic permeability, and a homogeneous Neumann condition is imposed at the outer boundary of the shell.","This is a nonlinear eigenvalue problem, since $\\varepsilon$ depends on $\\omega$. Use of an ENZ material in the shell means that $\\varepsilon(x,\\omega)$ is nearly zero there, so the PDE is rather singular.","Working with a Lorentz model for the dispersion of the ENZ material, we put the discussion of Liberal et.~al.~on a sound foundation by proving the existence of the anticipated resonance when the loss is sufficiently small.","Our analysis is perturbative in character despite the apparently singular form of the PDE.","While the existence of the resonance depends only on the area of the ENZ shell, the rate at which it decays depends on the shape of the shell.","We consider an associated optimal design problem: what shape shell gives the slowest-decaying resonance?","We prove that if the dielectric inclusion is a ball then the optimal shell is a concentric annulus.","For an inclusion of any shape, we study a convex relaxation of the design problem using tools from convex duality, and discuss the conjecture that our relaxed problem amounts to considering homogenization-like limits of nearly optimal designs."],"url":"http://arxiv.org/abs/2403.11242v1","category":"math.AP"}
{"created":"2024-03-17 15:06:39","title":"Speed, Accuracy, and Complexity","abstract":"This paper re-examines the validity of using response time to infer problem complexity. It revisits a canonical Wald model of optimal stopping, taking signal-to-noise ratio as a measure of problem complexity. While choice quality is monotone in problem complexity, expected stopping time is inverse $U$-shaped. Indeed decisions are fast in both very simple and very complex problems: in simple problems it is quick to understand which alternative is best, while in complex problems it would be too costly -- an insight which extends to general costly information acquisition models. This non-monotonicity also underlies an ambiguous relationship between response time and ability, whereby higher ability entails slower decisions in very complex problems, but faster decisions in simple problems. Finally, this paper proposes a new method to correctly infer problem complexity based on the finding that choices react more to changes in incentives in more complex problems.","sentences":["This paper re-examines the validity of using response time to infer problem complexity.","It revisits a canonical Wald model of optimal stopping, taking signal-to-noise ratio as a measure of problem complexity.","While choice quality is monotone in problem complexity, expected stopping time is inverse $U$-shaped.","Indeed decisions are fast in both very simple and very complex problems: in simple problems it is quick to understand which alternative is best, while in complex problems it would be too costly -- an insight which extends to general costly information acquisition models.","This non-monotonicity also underlies an ambiguous relationship between response time and ability, whereby higher ability entails slower decisions in very complex problems, but faster decisions in simple problems.","Finally, this paper proposes a new method to correctly infer problem complexity based on the finding that choices react more to changes in incentives in more complex problems."],"url":"http://arxiv.org/abs/2403.11240v1","category":"econ.TH"}
{"created":"2024-03-17 15:05:55","title":"A component-level co-rotational 3D continuum finite element framework for efficient flexible multibody analysis","abstract":"This paper proposes a systematic and novel component level co-rotational (CR) framework, for upgrading existing 3D continuum finite elements to flexible multibody analysis. Without using any model reduction techniques, the high efficiency is achieved through sophisticated operations in both modeling and numerical implementation phrases. In modeling phrase, as in conventional 3D nonlinear finite analysis, the nodal absolute coordinates are used as the system generalized coordinates, therefore simple formulations of the inertia force terms can be obtained. For the elastic force terms, inspired by existing floating frame of reference formulation (FFRF) and conventional element-level CR formulation, a component-level CR modeling strategy is developed. By in combination with Schur complement theory and fully exploring the nature of the component-level CR modeling method, an extremely efficient procedure is developed, which enables us to transform the linear equations raised from each Newton-Raphson iteration step into linear systems with constant coefficient matrix. The coefficient matrix thus can be pre-calculated and decomposed only once, and at all the subsequent time steps only back substitutions are needed, which avoids frequently updating the Jacobian matrix and avoids directly solving the large-scale linearized equation in each iteration. Multiple examples are presented to demonstrate the performance of the proposed framework.","sentences":["This paper proposes a systematic and novel component level co-rotational (CR) framework, for upgrading existing 3D continuum finite elements to flexible multibody analysis.","Without using any model reduction techniques, the high efficiency is achieved through sophisticated operations in both modeling and numerical implementation phrases.","In modeling phrase, as in conventional 3D nonlinear finite analysis, the nodal absolute coordinates are used as the system generalized coordinates, therefore simple formulations of the inertia force terms can be obtained.","For the elastic force terms, inspired by existing floating frame of reference formulation (FFRF) and conventional element-level CR formulation, a component-level CR modeling strategy is developed.","By in combination with Schur complement theory and fully exploring the nature of the component-level CR modeling method, an extremely efficient procedure is developed, which enables us to transform the linear equations raised from each Newton-Raphson iteration step into linear systems with constant coefficient matrix.","The coefficient matrix thus can be pre-calculated and decomposed only once, and at all the subsequent time steps only back substitutions are needed, which avoids frequently updating the Jacobian matrix and avoids directly solving the large-scale linearized equation in each iteration.","Multiple examples are presented to demonstrate the performance of the proposed framework."],"url":"http://arxiv.org/abs/2403.11239v1","category":"physics.comp-ph"}
{"created":"2024-03-17 14:53:38","title":"JUMBO: Fully Asynchronous BFT Consensus Made Truly Scalable","abstract":"Recent progresses in asynchronous Byzantine fault-tolerant (BFT) consensus, e.g. Dumbo-NG (CCS' 22) and Tusk (EuroSys' 22), show promising performance through decoupling transaction dissemination and block agreement. However, when executed with a larger number $n$ of nodes, like several hundreds, they would suffer from significant degradation in performance. Their dominating scalability bottleneck is the huge authenticator complexity: each node has to multicast $\\bigO(n)$ quorum certificates (QCs) and subsequently verify them for each block.   This paper systematically investigates and resolves the above scalability issue. We first propose a signature-free asynchronous BFT consensus FIN-NG that adapts a recent signature-free asynchronous common subset protocol FIN (CCS' 23) into the state-of-the-art framework of concurrent broadcast and agreement. The liveness of FIN-NG relies on our non-trivial redesign of FIN's multi-valued validated Byzantine agreement towards achieving optimal quality. FIN-NG greatly improves the performance of FIN and already outperforms Dumbo-NG in most deployment settings. To further overcome the scalability limit of FIN-NG due to $\\bigO(n^3)$ messages, we propose JUMBO, a scalable instantiation of Dumbo-NG, with only $\\bigO(n^2)$ complexities for both authenticators and messages. We use various aggregation and dispersal techniques for QCs to significantly reduce the authenticator complexity of original Dumbo-NG implementations by up to $\\bigO(n^2)$ orders. We also propose a ``fairness'' patch for JUMBO, thus preventing a flooding adversary from controlling an overwhelming portion of transactions in its output.","sentences":["Recent progresses in asynchronous Byzantine fault-tolerant (BFT) consensus, e.g. Dumbo-NG (CCS' 22) and Tusk (EuroSys' 22), show promising performance through decoupling transaction dissemination and block agreement.","However, when executed with a larger number $n$ of nodes, like several hundreds, they would suffer from significant degradation in performance.","Their dominating scalability bottleneck is the huge authenticator complexity: each node has to multicast $\\bigO(n)$ quorum certificates (QCs) and subsequently verify them for each block.   ","This paper systematically investigates and resolves the above scalability issue.","We first propose a signature-free asynchronous BFT consensus FIN-NG that adapts a recent signature-free asynchronous common subset protocol FIN (CCS' 23) into the state-of-the-art framework of concurrent broadcast and agreement.","The liveness of FIN-NG relies on our non-trivial redesign of FIN's multi-valued validated Byzantine agreement towards achieving optimal quality.","FIN-NG greatly improves the performance of FIN and already outperforms Dumbo-NG in most deployment settings.","To further overcome the scalability limit of FIN-NG due to $\\bigO(n^3)$ messages, we propose JUMBO, a scalable instantiation of Dumbo-NG, with only $\\bigO(n^2)$ complexities for both authenticators and messages.","We use various aggregation and dispersal techniques for QCs to significantly reduce the authenticator complexity of original Dumbo-NG implementations by up to $\\bigO(n^2)$ orders.","We also propose a ``fairness'' patch for JUMBO, thus preventing a flooding adversary from controlling an overwhelming portion of transactions in its output."],"url":"http://arxiv.org/abs/2403.11238v1","category":"cs.DC"}
{"created":"2024-03-17 14:14:15","title":"The Negative Energy Sea","abstract":"The Dirac negative energy sea introduced the concept of antimatter, and explained it, not least in its relationship to negative-energy solutions to the wave equation. Post-war, it was largely displaced by what I shall call the 'standard formalism', dependent, among other things, on normal-ordering. A much better explanation is provided by the 'two complex structures' viewpoint, as first introduced by Irving Segal: the one ('natural') kind of complex numbers at the level of covariant, local fields; and the other ('particle') complex numbers at the level of the one-particle Hilbert space and Fock space. The former is local, the latter non-local: therein lies the fundamental difference between relativistic and non-relativistic quantum theory.","sentences":["The Dirac negative energy sea introduced the concept of antimatter, and explained it, not least in its relationship to negative-energy solutions to the wave equation.","Post-war, it was largely displaced by what I shall call the 'standard formalism', dependent, among other things, on normal-ordering.","A much better explanation is provided by the 'two complex structures' viewpoint, as first introduced by Irving Segal: the one ('natural') kind of complex numbers at the level of covariant, local fields; and the other ('particle') complex numbers at the level of the one-particle Hilbert space and Fock space.","The former is local, the latter non-local: therein lies the fundamental difference between relativistic and non-relativistic quantum theory."],"url":"http://arxiv.org/abs/2403.11225v1","category":"physics.hist-ph"}
{"created":"2024-03-17 14:04:48","title":"An exact formula for the optical conductivity of the two dimensional Hubbard model and its application to the cuprate superconductors","abstract":"Understanding the origin of electron incoherence is believed to be the first step toward the resolution of the mysteries of the high-T$_{c}$ cuprate superconductors. Such electron incoherence manifests itself most evidently in the non-Drude form of the optical absorption spectrum of the system. The spectral weight transfer related to such dissipative response, which is absent in conventional Fermi liquid metal, has direct consequence on the dc transport property of the system. However, a theoretical study of the optical conductivity of a strongly correlated model is a formidable task. Here we present an exact formula for the optical conductivity of the 2D Hubbard model from the low energy effective theory perspective. We show that the optical conductivity in Matsubara frequency of the 2D Hubbard model can be represented as the ensemble average of the optical conductivity of non-interacting systems in the background of fluctuating local moment. We find that such an ensemble average can be done exactly with a sign-problem-free Monte Carlo simulation if we assume the widely adopted Millis-Monien-Pines spin susceptibility for the fluctuating local moment. For thermal fluctuation of the local moment, our formula can be used to calculate directly the optical conductivity in real frequency which can be compared with the result of optical measurements in the cuprate superconductors.","sentences":["Understanding the origin of electron incoherence is believed to be the first step toward the resolution of the mysteries of the high-T$_{c}$ cuprate superconductors.","Such electron incoherence manifests itself most evidently in the non-Drude form of the optical absorption spectrum of the system.","The spectral weight transfer related to such dissipative response, which is absent in conventional Fermi liquid metal, has direct consequence on the dc transport property of the system.","However, a theoretical study of the optical conductivity of a strongly correlated model is a formidable task.","Here we present an exact formula for the optical conductivity of the 2D Hubbard model from the low energy effective theory perspective.","We show that the optical conductivity in Matsubara frequency of the 2D Hubbard model can be represented as the ensemble average of the optical conductivity of non-interacting systems in the background of fluctuating local moment.","We find that such an ensemble average can be done exactly with a sign-problem-free Monte Carlo simulation if we assume the widely adopted Millis-Monien-Pines spin susceptibility for the fluctuating local moment.","For thermal fluctuation of the local moment, our formula can be used to calculate directly the optical conductivity in real frequency which can be compared with the result of optical measurements in the cuprate superconductors."],"url":"http://arxiv.org/abs/2403.11224v1","category":"cond-mat.str-el"}
{"created":"2024-03-17 13:28:14","title":"Observation of spin-electric transitions in a molecular exchange qubit","abstract":"Electric fields represent an ideal means for controlling spins at the nanoscale and, more specifically, for manipulating protected degrees of freedom in multispin systems. Here we perform low-temperature magnetic far-IR spectroscopy on a molecular spin triangle (Fe3) and provide the first experimental evidence of spin-electric transitions in polynuclear complexes. The co-presence of electric- and magnetic-dipole transitions, allows us to estimate the spin-electric coupling. Based on spin Hamiltonian simulations of the spectra, we identify the observed transitions and introduce the concept of a generalized exchange qubit. This applies to a wide class of molecular spin triangles, and includes the scalar chirality and the partial spin sum qubits as special cases.","sentences":["Electric fields represent an ideal means for controlling spins at the nanoscale and, more specifically, for manipulating protected degrees of freedom in multispin systems.","Here we perform low-temperature magnetic far-IR spectroscopy on a molecular spin triangle (Fe3) and provide the first experimental evidence of spin-electric transitions in polynuclear complexes.","The co-presence of electric- and magnetic-dipole transitions, allows us to estimate the spin-electric coupling.","Based on spin Hamiltonian simulations of the spectra, we identify the observed transitions and introduce the concept of a generalized exchange qubit.","This applies to a wide class of molecular spin triangles, and includes the scalar chirality and the partial spin sum qubits as special cases."],"url":"http://arxiv.org/abs/2403.11214v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-17 13:21:33","title":"Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations","abstract":"Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recognizing the race of a White TTS system of similar quality. A focus group with African American IT workers not only confirmed the representativeness of the African American voice we built, but also suggested that the surprising recognition results may have been caused by the inability or the latent prejudice from non-African Americans to associate educated, non-vernacular, professionally-sounding voices to African American people.","sentences":["Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use.","In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman.","The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers.","We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recognizing the race of a White TTS system of similar quality.","A focus group with African American IT workers not only confirmed the representativeness of the African American voice we built, but also suggested that the surprising recognition results may have been caused by the inability or the latent prejudice from non-African Americans to associate educated, non-vernacular, professionally-sounding voices to African American people."],"url":"http://arxiv.org/abs/2403.11209v1","category":"cs.CL"}
{"created":"2024-03-17 13:08:41","title":"Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL Using Reinforcement Learning","abstract":"We have developed a parallel wire-driven monopedal robot, RAMIEL, which has both speed and power due to the parallel wire mechanism and a long acceleration distance. RAMIEL is capable of jumping high and continuously, and so has high performance in traveling. On the other hand, one of the drawbacks of a minimal parallel wire-driven robot without joint encoders is that the current joint velocities estimated from the wire lengths oscillate due to the elongation of the wires, making the values unreliable. Therefore, despite its high performance, the control of the robot is unstable, and in 10 out of 16 jumps, the robot could only jump up to two times continuously. In this study, we propose a method to realize a continuous jumping motion by reinforcement learning in simulation, and its application to the actual robot. Because the joint velocities oscillate with the elongation of the wires, they are not used directly, but instead are inferred from the time series of joint angles. At the same time, noise that imitates the vibration caused by the elongation of the wires is added for transfer to the actual robot. The results show that the system can be applied to the actual robot RAMIEL as well as to the stable continuous jumping motion in simulation.","sentences":["We have developed a parallel wire-driven monopedal robot, RAMIEL, which has both speed and power due to the parallel wire mechanism and a long acceleration distance.","RAMIEL is capable of jumping high and continuously, and so has high performance in traveling.","On the other hand, one of the drawbacks of a minimal parallel wire-driven robot without joint encoders is that the current joint velocities estimated from the wire lengths oscillate due to the elongation of the wires, making the values unreliable.","Therefore, despite its high performance, the control of the robot is unstable, and in 10 out of 16 jumps, the robot could only jump up to two times continuously.","In this study, we propose a method to realize a continuous jumping motion by reinforcement learning in simulation, and its application to the actual robot.","Because the joint velocities oscillate with the elongation of the wires, they are not used directly, but instead are inferred from the time series of joint angles.","At the same time, noise that imitates the vibration caused by the elongation of the wires is added for transfer to the actual robot.","The results show that the system can be applied to the actual robot RAMIEL as well as to the stable continuous jumping motion in simulation."],"url":"http://arxiv.org/abs/2403.11205v1","category":"cs.RO"}
{"created":"2024-03-17 12:57:07","title":"From habitat decline to collapse: a spatially explicit approach connecting habitat degradation to destruction","abstract":"Habitat loss, through degradation and destruction of viable habitat, has a well-documented and undeniable impact on the sustainability of ecosystems [Diaz et al. (2019), Pimm et al. (2014), Pimm et al. (2000)]. Moreover, most habitat loss is anthropogenic [Jacobson et al. (2019)]. Understanding the relationships between varying degrees of habitat degradation, movement strategies, and population dynamics on species persistence is crucial. We establish a robust connection between habitat degradation and destruction using a reaction-diffusion equation framework. Motivated by the recent work [Salmaniw et al. (2022)], we consider an intrinsic growth rate function that features a logistic-type growth in the viable habitat but decays at rate $c\\geq0$ in the degraded region(s). In the limit as $c \\to \\infty$, the solution to the habitat degradation problem converges uniformly to the solution of a related habitat destruction problem. When the habitat destruction problem predicts deterministic extinction, a unique value $c_0$ exists, the extinction threshold, from which any further habitat degradation leads to deterministic extinction. This extinction threshold can be bounded below by a constant depending on the size of the degraded region and the habitat quality in the undisturbed region. To show these results, we investigate the eigenvalue problems related to each model formulation and establish a convergence result between the principal eigenvalues and their associated eigenfunctions, providing a precise analytical connection between habitat degradation and destruction in a general setting applicable to any species adopting diffusive movement strategies.","sentences":["Habitat loss, through degradation and destruction of viable habitat, has a well-documented and undeniable impact on the sustainability of ecosystems [Diaz et al. (2019), Pimm et al.","(2014),","Pimm et al. (2000)].","Moreover, most habitat loss is anthropogenic","[Jacobson et al. (2019)].","Understanding the relationships between varying degrees of habitat degradation, movement strategies, and population dynamics on species persistence is crucial.","We establish a robust connection between habitat degradation and destruction using a reaction-diffusion equation framework.","Motivated by the recent work","[Salmaniw et al. (2022)], we consider an intrinsic growth rate function that features a logistic-type growth in the viable habitat but decays at rate $c\\geq0$ in the degraded region(s).","In the limit as $c \\to \\infty$, the solution to the habitat degradation problem converges uniformly to the solution of a related habitat destruction problem.","When the habitat destruction problem predicts deterministic extinction, a unique value $c_0$ exists, the extinction threshold, from which any further habitat degradation leads to deterministic extinction.","This extinction threshold can be bounded below by a constant depending on the size of the degraded region and the habitat quality in the undisturbed region.","To show these results, we investigate the eigenvalue problems related to each model formulation and establish a convergence result between the principal eigenvalues and their associated eigenfunctions, providing a precise analytical connection between habitat degradation and destruction in a general setting applicable to any species adopting diffusive movement strategies."],"url":"http://arxiv.org/abs/2403.11200v1","category":"math.AP"}
{"created":"2024-03-17 12:48:58","title":"Mesh-free mixed finite element approximation for nonlinear time-fractional biharmonic problem using weighted b-splines","abstract":"In this article, we propose a fully-discrete scheme for the numerical solution of a nonlinear time-fractional biharmonic problem. This problem is first converted into an equivalent system by introducing a new variable. Then spatial and temporal discretizations are done by the weighted $b$-spline method and $L2$-$1_\\sigma$ approximation, respectively. The weighted $b$-spline method uses weighted $b$-splines on a tensor product grid as basis functions for the finite element space and by construction, it is a mesh-free method. This method combines the computational benefits of $b$-splines and standard mesh-based elements. We derive $\\alpha$-robust \\emph{a priori} bound and convergence estimate in the $L^2(\\Omega)$ norm for the proposed scheme. Finally, we carry out few numerical experiments to support our theoretical findings.","sentences":["In this article, we propose a fully-discrete scheme for the numerical solution of a nonlinear time-fractional biharmonic problem.","This problem is first converted into an equivalent system by introducing a new variable.","Then spatial and temporal discretizations are done by the weighted $b$-spline method and $L2$-$1_\\sigma$ approximation, respectively.","The weighted $b$-spline method uses weighted $b$-splines on a tensor product grid as basis functions for the finite element space and by construction, it is a mesh-free method.","This method combines the computational benefits of $b$-splines and standard mesh-based elements.","We derive $\\alpha$-robust \\emph{a priori} bound and convergence estimate in the $L^2(\\Omega)$ norm for the proposed scheme.","Finally, we carry out few numerical experiments to support our theoretical findings."],"url":"http://arxiv.org/abs/2403.11196v1","category":"math.NA"}
{"created":"2024-03-17 12:48:34","title":"Merons and magnetoelectric switching in centrosymmetric spiral magnets","abstract":"Spiral multiferroics exhibit a strong coupling between magnetic and ferroelectric orders, allowing cross-control. Since the seminal work of Kimura et al. in 2003, these materials have attracted great interest galvanized by the prospect of new high-efficiency memory devices, where magnetic (electric) bits are switched via an external electric (magnetic) field. Nevertheless, the mechanism underlying such a switching process - the electric field-driven dynamics of domain walls (DWs) - is still poorly understood. We address this problem for meron DWs, which represent one of the main DW types in spiral multiferroics and consist of an array of meron (half-skyrmion) strings. Minimum energy walls feature merons with alternating topological charges and move as relativistic massive particles, with the limiting velocity set by magnon speed. Low-energy defects in this alternating charge sequence, which appear during domain nucleation, can lead to DWs with net topological charge. This induces a peculiar non-local dynamics where all the spins in the system rotate, merons translate within the wall, and the DW mobility is suppressed. The topological charge of the wall and the meron helicity can be easily modified via an external magnetic and electric field, respectively, offering fine control over DW dynamics. Defects within the meron strings, analogous to Bloch points, have hedgehog-like spin texture and are strongly pinned to the lattice. The fascinating interplay between domain wall motion, translation of merons within the wall, and precession of spins in the entire domains opens a new playground for the electric manipulation of topological spin textures.","sentences":["Spiral multiferroics exhibit a strong coupling between magnetic and ferroelectric orders, allowing cross-control.","Since the seminal work of Kimura et al.","in 2003, these materials have attracted great interest galvanized by the prospect of new high-efficiency memory devices, where magnetic (electric) bits are switched via an external electric (magnetic) field.","Nevertheless, the mechanism underlying such a switching process - the electric field-driven dynamics of domain walls (DWs) - is still poorly understood.","We address this problem for meron DWs, which represent one of the main DW types in spiral multiferroics and consist of an array of meron (half-skyrmion) strings.","Minimum energy walls feature merons with alternating topological charges and move as relativistic massive particles, with the limiting velocity set by magnon speed.","Low-energy defects in this alternating charge sequence, which appear during domain nucleation, can lead to DWs with net topological charge.","This induces a peculiar non-local dynamics where all the spins in the system rotate, merons translate within the wall, and the DW mobility is suppressed.","The topological charge of the wall and the meron helicity can be easily modified via an external magnetic and electric field, respectively, offering fine control over DW dynamics.","Defects within the meron strings, analogous to Bloch points, have hedgehog-like spin texture and are strongly pinned to the lattice.","The fascinating interplay between domain wall motion, translation of merons within the wall, and precession of spins in the entire domains opens a new playground for the electric manipulation of topological spin textures."],"url":"http://arxiv.org/abs/2403.11195v1","category":"cond-mat.str-el"}
{"created":"2024-03-17 12:35:12","title":"Numerical Realization of Dynamical Fermionization and Bethe Rapidities in a cold quenched Bose gas","abstract":"In this numerical investigation, we explore the non-equilibrium dynamics of a cold Lieb-Liniger (LL) Bose gas -- a well established integrable quantum system in one dimension exhibiting repulsive interactions. Our study involves the presence of a hard wall potential during the ballistic expansion of the Bose gas from its ground state within an infinite deep box of length L0 to a final length L. The Quantum Monte Carlo method, based on the Generalized Feynman-Kac approach, serves as our computational tool. Given the integrability of the Lieb-Liniger model, strongly correlated systems resist thermalization. To capture the intricate dynamics we employ the concept of Bethe Rapidities(BRs), a holistic function that extends beyond atomic or energy density considerations. Our thought experiment involves a box-to-box expansion, providing a unique opportunity for direct numerical observation of Bethe Rapidities and the phenomenon of Dynamical Fermionization(DF). This investigation aims to contribute insights into the behavior of strongly correlated quantum systems during non-equilibrium processes, offering a detailed examination of Bethe Rapidities and the dynamic evolution of Fermionization throughout the expansion.","sentences":["In this numerical investigation, we explore the non-equilibrium dynamics of a cold Lieb-Liniger (LL)","Bose gas -- a well established integrable quantum system in one dimension exhibiting repulsive interactions.","Our study involves the presence of a hard wall potential during the ballistic expansion of the Bose gas from its ground state within an infinite deep box of length L0 to a final length L.","The Quantum Monte Carlo method, based on the Generalized Feynman-Kac approach, serves as our computational tool.","Given the integrability of the Lieb-Liniger model, strongly correlated systems resist thermalization.","To capture the intricate dynamics we employ the concept of Bethe Rapidities(BRs), a holistic function that extends beyond atomic or energy density considerations.","Our thought experiment involves a box-to-box expansion, providing a unique opportunity for direct numerical observation of Bethe Rapidities and the phenomenon of Dynamical Fermionization(DF).","This investigation aims to contribute insights into the behavior of strongly correlated quantum systems during non-equilibrium processes, offering a detailed examination of Bethe Rapidities and the dynamic evolution of Fermionization throughout the expansion."],"url":"http://arxiv.org/abs/2403.11190v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-17 12:16:44","title":"Designs in finite classical polar spaces","abstract":"Combinatorial designs have been studied for nearly 200 years. 50 years ago, Cameron, Delsarte, and Ray-Chaudhury started investigating their $q$-analogs, also known as subspace designs or designs over finite fields.   Designs can be defined analogously in finite classical polar spaces, too. The definition includes the $m$-regular systems from projective geometry as the special case where the blocks are generators of the polar space. The first nontrivial such designs for $t > 1$ were found by De Bruyn and Vanhove in 2012, and some more designs appeared recently in the PhD thesis of Lansdown.   In this article, we investigate the theory of classical and subspace designs for applicability to designs in polar spaces, explicitly allowing arbitrary block dimensions. In this way, we obtain divisibility conditions on the parameters, derived and residual designs, intersection numbers and an analog of Fisher's inequality. We classify the parameters of symmetric designs. Furthermore, we conduct a computer search to construct designs of strength $t=2$, resulting in designs for more than 140 previously unknown parameter sets in various classical polar spaces over $\\mathbb{F}_2$ and $\\mathbb{F}_3$.","sentences":["Combinatorial designs have been studied for nearly 200 years.","50 years ago, Cameron, Delsarte, and Ray-Chaudhury started investigating their $q$-analogs, also known as subspace designs or designs over finite fields.   ","Designs can be defined analogously in finite classical polar spaces, too.","The definition includes the $m$-regular systems from projective geometry as the special case where the blocks are generators of the polar space.","The first nontrivial such designs for $t > 1$ were found by De Bruyn and Vanhove in 2012, and some more designs appeared recently in the PhD thesis of Lansdown.   ","In this article, we investigate the theory of classical and subspace designs for applicability to designs in polar spaces, explicitly allowing arbitrary block dimensions.","In this way, we obtain divisibility conditions on the parameters, derived and residual designs, intersection numbers and an analog of Fisher's inequality.","We classify the parameters of symmetric designs.","Furthermore, we conduct a computer search to construct designs of strength $t=2$, resulting in designs for more than 140 previously unknown parameter sets in various classical polar spaces over $\\mathbb{F}_2$ and $\\mathbb{F}_3$."],"url":"http://arxiv.org/abs/2403.11188v1","category":"math.CO"}
{"created":"2024-03-17 12:15:56","title":"Task-Based Quantizer Design for Sensing With Random Signals","abstract":"In integrated sensing and communication (ISAC) systems, random signaling is used to convey useful information as well as sense the environment. Such randomness poses challenges in various components in sensing signal processing. In this paper, we investigate quantizer design for sensing in ISAC systems. Unlike quantizers for channel estimation in massive multiple-input-multiple-out (MIMO) communication systems, sensing in ISAC systems needs to deal with random nonorthogonal transmitted signals rather than a fixed orthogonal pilot. Considering sensing performance and hardware implementation, we focus on task-based hardware-limited quantization with spatial analog combining. We propose two strategies of quantizer optimization, i.e., data-dependent (DD) and data-independent (DI). The former achieves optimized sensing performance with high implementation overhead. To reduce hardware complexity, the latter optimizes the quantizer with respect to the random signal from a stochastic perspective. We derive the optimal quantizers for both strategies and formulate an algorithm based on sample average approximation (SAA) to solve the optimization in the DI strategy. Numerical results show that the optimized quantizers outperform digital-only quantizers in terms of sensing performance. Additionally, the DI strategy, despite its lower computational complexity compared to the DD strategy, achieves near-optimal sensing performance.","sentences":["In integrated sensing and communication (ISAC) systems, random signaling is used to convey useful information as well as sense the environment.","Such randomness poses challenges in various components in sensing signal processing.","In this paper, we investigate quantizer design for sensing in ISAC systems.","Unlike quantizers for channel estimation in massive multiple-input-multiple-out (MIMO) communication systems, sensing in ISAC systems needs to deal with random nonorthogonal transmitted signals rather than a fixed orthogonal pilot.","Considering sensing performance and hardware implementation, we focus on task-based hardware-limited quantization with spatial analog combining.","We propose two strategies of quantizer optimization, i.e., data-dependent (DD) and data-independent (DI).","The former achieves optimized sensing performance with high implementation overhead.","To reduce hardware complexity, the latter optimizes the quantizer with respect to the random signal from a stochastic perspective.","We derive the optimal quantizers for both strategies and formulate an algorithm based on sample average approximation (SAA) to solve the optimization in the DI strategy.","Numerical results show that the optimized quantizers outperform digital-only quantizers in terms of sensing performance.","Additionally, the DI strategy, despite its lower computational complexity compared to the DD strategy, achieves near-optimal sensing performance."],"url":"http://arxiv.org/abs/2403.11187v1","category":"cs.IT"}
{"created":"2024-03-17 12:15:02","title":"NetTrack: Tracking Highly Dynamic Objects with a Net","abstract":"The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions. Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects. To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity. Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues. Correspondingly, a fine-grained sampler and matching method have been incorporated. Furthermore, NetTrack learns object-text correspondence for fine-grained localization. To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios. Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world benchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without finetuning. Project page: https://george-zhuang.github.io/nettrack/.","sentences":["The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions.","Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects.","To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity.","Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues.","Correspondingly, a fine-grained sampler and matching method have been incorporated.","Furthermore, NetTrack learns object-text correspondence for fine-grained localization.","To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios.","Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world benchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without finetuning.","Project page: https://george-zhuang.github.io/nettrack/."],"url":"http://arxiv.org/abs/2403.11186v1","category":"cs.CV"}
{"created":"2024-03-17 12:06:15","title":"Periodicity of sub-pulses in a radio pulsar","abstract":"Pulsars are known to manifest complex phenomena, such as nulling, sub-pulse drifting, and periodicity. Within the purview of this investigation, we have harnessed the wavelet analysis technique to scrutinize the multifaceted periodicities and sub-pulse drifting characteristics exhibited by PSR J1926-0652, discovered by the Five-hundred-meter Aperture Spherical radio Telescope (FAST). Our analysis draws upon the rich dataset acquired from the FAST ultra-wide-bandwidth receiver (UWB), meticulously examining pulse attributes encompassing an entire pulse. It is notable that the pulse apex recurrently manifests approximately every 17.11 times the pulsar's period P, individual sub-pulses exhibit a drifting phenomenon, with a phase decrement of approximately $1.04^\\circ$ over each P. Intriguingly, the central phase of each sub-pulse track gradually increments over temporal evolution. Furthermore, the relative offset distribution between successive sub-pulse tracks emanating from the trailing and leading components remains comparatively stable, with a central tendency of approximately $\\sim$6.87$\\pm$2.56 P. Most notably, derived from the outcomes of wavelet analysis, we ascertain a negative correlation of -0.98 between the periods of sub-pulses and their drifting rates, alongside the intrinsic period of sub-pulses identified at 28.14 seconds.","sentences":["Pulsars are known to manifest complex phenomena, such as nulling, sub-pulse drifting, and periodicity.","Within the purview of this investigation, we have harnessed the wavelet analysis technique to scrutinize the multifaceted periodicities and sub-pulse drifting characteristics exhibited by PSR J1926-0652, discovered by the Five-hundred-meter Aperture Spherical radio Telescope (FAST).","Our analysis draws upon the rich dataset acquired from the FAST ultra-wide-bandwidth receiver (UWB), meticulously examining pulse attributes encompassing an entire pulse.","It is notable that the pulse apex recurrently manifests approximately every 17.11 times the pulsar's period P, individual sub-pulses exhibit a drifting phenomenon, with a phase decrement of approximately $1.04^\\circ$ over each P. Intriguingly, the central phase of each sub-pulse track gradually increments over temporal evolution.","Furthermore, the relative offset distribution between successive sub-pulse tracks emanating from the trailing and leading components remains comparatively stable, with a central tendency of approximately $\\sim$6.87$\\pm$2.56 P. Most notably, derived from the outcomes of wavelet analysis, we ascertain a negative correlation of -0.98 between the periods of sub-pulses and their drifting rates, alongside the intrinsic period of sub-pulses identified at 28.14 seconds."],"url":"http://arxiv.org/abs/2403.11182v1","category":"astro-ph.HE"}
{"created":"2024-03-18 12:12:23","title":"A Quantile Neural Network Framework for Two-stage Stochastic Optimization","abstract":"Two-stage stochastic programming is a popular framework for optimization under uncertainty, where decision variables are split between first-stage decisions, and second-stage (or recourse) decisions, with the latter being adjusted after uncertainty is realized. These problems are often formulated using Sample Average Approximation (SAA), where uncertainty is modeled as a finite set of scenarios, resulting in a large \"monolithic\" problem, i.e., where the model is repeated for each scenario. The resulting models can be challenging to solve, and several problem-specific decomposition approaches have been proposed. An alternative approach is to approximate the expected second-stage objective value using a surrogate model, which can then be embedded in the first-stage problem to produce good heuristic solutions. In this work, we propose to instead model the distribution of the second-stage objective, specifically using a quantile neural network. Embedding this distributional approximation enables capturing uncertainty and is not limited to expected-value optimization, e.g., the proposed approach enables optimization of the Conditional Value at Risk (CVaR). We discuss optimization formulations for embedding the quantile neural network and demonstrate the effectiveness of the proposed framework using several computational case studies including a set of mixed-integer optimization problems.","sentences":["Two-stage stochastic programming is a popular framework for optimization under uncertainty, where decision variables are split between first-stage decisions, and second-stage (or recourse) decisions, with the latter being adjusted after uncertainty is realized.","These problems are often formulated using Sample Average Approximation (SAA), where uncertainty is modeled as a finite set of scenarios, resulting in a large \"monolithic\" problem, i.e., where the model is repeated for each scenario.","The resulting models can be challenging to solve, and several problem-specific decomposition approaches have been proposed.","An alternative approach is to approximate the expected second-stage objective value using a surrogate model, which can then be embedded in the first-stage problem to produce good heuristic solutions.","In this work, we propose to instead model the distribution of the second-stage objective, specifically using a quantile neural network.","Embedding this distributional approximation enables capturing uncertainty and is not limited to expected-value optimization, e.g., the proposed approach enables optimization of the Conditional Value at Risk (CVaR).","We discuss optimization formulations for embedding the quantile neural network and demonstrate the effectiveness of the proposed framework using several computational case studies including a set of mixed-integer optimization problems."],"url":"http://arxiv.org/abs/2403.11707v1","category":"math.OC"}
{"created":"2024-03-18 11:16:24","title":"Mechanical effects of carboxymethylcellulose binder in hard carbon electrodes","abstract":"Electrodes in sodium-ion batteries endure mechanical stress during production and application, which can damage these fragile coatings, causing performance inefficiencies and early failure. Binder material provides elasticity in electrode composites to resist fracture, but evaluating the effectiveness of binder is complicated by substrate dependency of these films, while conventional cell tests are beset by multiple electrochemical variables. This work introduces a practical low-cost indentation test to determine the elasticity of hard carbon electrodes containing standard carboxymethylcellulose binder. Using the proposed method, relative elastic moduli of hard carbon electrodes were found to be 0.079 GPa (1% binder), 0.088 GPa (2% binder), 0.105 GPa (3% binder) and 0.113 GPa (4% binder), which were validated using a computational model of film deflection to predict mechanical deformation under stress. Effects on the electrochemical performance of hard carbon anodes were also demonstrated with impedance spectroscopy and galvanostatic cycling of sodium half-cells, revealing 8-9% higher capacity retention of anodes with 4% binder compared with those containing 1% binder. These findings suggest binder content in hard carbon electrodes should be selected according to requirements for both cycle life and film flexibility during cell manufacturing.","sentences":["Electrodes in sodium-ion batteries endure mechanical stress during production and application, which can damage these fragile coatings, causing performance inefficiencies and early failure.","Binder material provides elasticity in electrode composites to resist fracture, but evaluating the effectiveness of binder is complicated by substrate dependency of these films, while conventional cell tests are beset by multiple electrochemical variables.","This work introduces a practical low-cost indentation test to determine the elasticity of hard carbon electrodes containing standard carboxymethylcellulose binder.","Using the proposed method, relative elastic moduli of hard carbon electrodes were found to be 0.079 GPa (1% binder), 0.088 GPa (2% binder), 0.105 GPa (3% binder) and 0.113 GPa (4% binder), which were validated using a computational model of film deflection to predict mechanical deformation under stress.","Effects on the electrochemical performance of hard carbon anodes were also demonstrated with impedance spectroscopy and galvanostatic cycling of sodium half-cells, revealing 8-9% higher capacity retention of anodes with 4% binder compared with those containing 1% binder.","These findings suggest binder content in hard carbon electrodes should be selected according to requirements for both cycle life and film flexibility during cell manufacturing."],"url":"http://arxiv.org/abs/2403.11668v1","category":"physics.chem-ph"}
{"created":"2024-03-18 11:07:15","title":"FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames with Events","abstract":"Keypoint detection and tracking in traditional image frames are often compromised by image quality issues such as motion blur and extreme lighting conditions. Event cameras offer potential solutions to these challenges by virtue of their high temporal resolution and high dynamic range. However, they have limited performance in practical applications due to their inherent noise in event data. This paper advocates fusing the complementary information from image frames and event streams to achieve more robust keypoint detection and tracking. Specifically, we propose a novel keypoint detection network that fuses the textural and structural information from image frames with the high-temporal-resolution motion information from event streams, namely FE-DeTr. The network leverages a temporal response consistency for supervision, ensuring stable and efficient keypoint detection. Moreover, we use a spatio-temporal nearest-neighbor search strategy for robust keypoint tracking. Extensive experiments are conducted on a new dataset featuring both image frames and event data captured under extreme conditions. The experimental results confirm the superior performance of our method over both existing frame-based and event-based methods.","sentences":["Keypoint detection and tracking in traditional image frames are often compromised by image quality issues such as motion blur and extreme lighting conditions.","Event cameras offer potential solutions to these challenges by virtue of their high temporal resolution and high dynamic range.","However, they have limited performance in practical applications due to their inherent noise in event data.","This paper advocates fusing the complementary information from image frames and event streams to achieve more robust keypoint detection and tracking.","Specifically, we propose a novel keypoint detection network that fuses the textural and structural information from image frames with the high-temporal-resolution motion information from event streams, namely FE-DeTr.","The network leverages a temporal response consistency for supervision, ensuring stable and efficient keypoint detection.","Moreover, we use a spatio-temporal nearest-neighbor search strategy for robust keypoint tracking.","Extensive experiments are conducted on a new dataset featuring both image frames and event data captured under extreme conditions.","The experimental results confirm the superior performance of our method over both existing frame-based and event-based methods."],"url":"http://arxiv.org/abs/2403.11662v1","category":"cs.RO"}
{"created":"2024-03-18 10:13:51","title":"Cooperative Agri-Food Export under Minimum Quantity Commitments","abstract":"International trade can be a profitable business for agri-food communities. However, access to international markets can be costly and thus unattainable for small and medium sized enterprises (SMEs). This problem is exacerbated under trade policies which require minimum quantity commitments (MQCs) on export volumes, e.g., licensing tariff rate quota (TRQ) mechanisms.   We show how cooperative exporting among agri-food SMEs can tackle the barriers posed by the MQCs, and give market access to a broader range of SMEs. We formulate a class of cooperative games associated with these situations and find a gain-sharing mechanism that result in allocations in their corresponding cores. Thus, grand coalitions of cooperative exporting SMEs can form in stable manners.   This allocation rule shares the export surplus only among the \"essential\" SME exporters, that is, the players who are sufficiently cost efficient. Thus, less cost efficient \"complimentary\" SMEs whose capacities are needed to maintain MQCs receive no benefit from collaborative exporting and their participation have to be altruistic. We propose two modifications to our original allocation rule to share a portion of export surplus among the complementary SMEs through taxing the essential SMEs: the first through egalitarian, and the second through revenue-based rates. We compare the performance of these allocations with the numerical examples and discuss their practical implications.","sentences":["International trade can be a profitable business for agri-food communities.","However, access to international markets can be costly and thus unattainable for small and medium sized enterprises (SMEs).","This problem is exacerbated under trade policies which require minimum quantity commitments (MQCs) on export volumes, e.g., licensing tariff rate quota (TRQ) mechanisms.   ","We show how cooperative exporting among agri-food SMEs can tackle the barriers posed by the MQCs, and give market access to a broader range of SMEs.","We formulate a class of cooperative games associated with these situations and find a gain-sharing mechanism that result in allocations in their corresponding cores.","Thus, grand coalitions of cooperative exporting SMEs can form in stable manners.   ","This allocation rule shares the export surplus only among the \"essential\" SME exporters, that is, the players who are sufficiently cost efficient.","Thus, less cost efficient \"complimentary\" SMEs whose capacities are needed to maintain MQCs receive no benefit from collaborative exporting and their participation have to be altruistic.","We propose two modifications to our original allocation rule to share a portion of export surplus among the complementary SMEs through taxing the essential SMEs: the first through egalitarian, and the second through revenue-based rates.","We compare the performance of these allocations with the numerical examples and discuss their practical implications."],"url":"http://arxiv.org/abs/2403.11633v1","category":"cs.GT"}
{"created":"2024-03-18 10:03:22","title":"Nonlinear Pulse Dynamics in a Magnetized and Nonextensive Quark-Gluon Plasma","abstract":"We investigate the propagation of nonlinear energy density waves in a nonextensive quark-gluon plasma under the influence of a magnetic field using the reductive perturbation technique. For a nonextensive MIT bag equation of state, we obtain the governing equation for the first order perturbation of the energy density. We observe that an increase in the strength of the magnetic field results in the localization of the waves.","sentences":["We investigate the propagation of nonlinear energy density waves in a nonextensive quark-gluon plasma under the influence of a magnetic field using the reductive perturbation technique.","For a nonextensive MIT bag equation of state, we obtain the governing equation for the first order perturbation of the energy density.","We observe that an increase in the strength of the magnetic field results in the localization of the waves."],"url":"http://arxiv.org/abs/2403.11630v1","category":"hep-ph"}
{"created":"2024-03-18 09:54:07","title":"Theoretical Study on Optoelectronic properties of Layered In2O3 and Ga2O3","abstract":"Composite oxides have been indeed proved to be valuable materials in optoelectronic applications. The combination of indium oxide and gallium oxide and other materials can lead to enhanced optical and electronic properties, making them suitable for a variety of optoelectronic devices. Meticulous analysis of the various optical properties helped to draw conclusions about the heterostructure of Indium and Gallium oxide and its use as a suitable semiconducting material in the medium bandgap range. The density of states and the band structure have been obtained from the density functional theory calculations. Real frequency phonon density of states supports dynamical stability of the crystal structure. A favorable energy band gap is achieved in the visible region of the spectrum, indicating that this mixed oxide is well suited for optoelectronic devices such as LEDs and solar cells.","sentences":["Composite oxides have been indeed proved to be valuable materials in optoelectronic applications.","The combination of indium oxide and gallium oxide and other materials can lead to enhanced optical and electronic properties, making them suitable for a variety of optoelectronic devices.","Meticulous analysis of the various optical properties helped to draw conclusions about the heterostructure of Indium and Gallium oxide and its use as a suitable semiconducting material in the medium bandgap range.","The density of states and the band structure have been obtained from the density functional theory calculations.","Real frequency phonon density of states supports dynamical stability of the crystal structure.","A favorable energy band gap is achieved in the visible region of the spectrum, indicating that this mixed oxide is well suited for optoelectronic devices such as LEDs and solar cells."],"url":"http://arxiv.org/abs/2403.11619v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 09:21:32","title":"Finite element method coupled with multiscale finite element method for the non-stationary Stokes-Darcy model","abstract":"In this paper, we combine the multiscale flnite element method to propose an algorithm for solving the non-stationary Stokes-Darcy model, where the permeability coefflcient in the Darcy region exhibits multiscale characteristics. Our algorithm involves two steps: first, conducting the parallel computation of multiscale basis functions in the Darcy region. Second, based on these multiscale basis functions, we employ an implicitexplicit scheme to solve the Stokes-Darcy equations. One signiflcant feature of the algorithm is that it solves problems on relatively coarse grids, thus signiflcantly reducing computational costs. Moreover, under the same coarse grid size, it exhibits higher accuracy compared to standard flnite element method. Under the assumption that the permeability coefflcient is periodic and independent of time, this paper demonstrates the stability and convergence of the algorithm. Finally, the rationality and effectiveness of the algorithm are verifled through three numerical experiments, with experimental results consistent with theoretical analysis.","sentences":["In this paper, we combine the multiscale flnite element method to propose an algorithm for solving the non-stationary Stokes-Darcy model, where the permeability coefflcient in the Darcy region exhibits multiscale characteristics.","Our algorithm involves two steps: first, conducting the parallel computation of multiscale basis functions in the Darcy region.","Second, based on these multiscale basis functions, we employ an implicitexplicit scheme to solve the Stokes-Darcy equations.","One signiflcant feature of the algorithm is that it solves problems on relatively coarse grids, thus signiflcantly reducing computational costs.","Moreover, under the same coarse grid size, it exhibits higher accuracy compared to standard flnite element method.","Under the assumption that the permeability coefflcient is periodic and independent of time, this paper demonstrates the stability and convergence of the algorithm.","Finally, the rationality and effectiveness of the algorithm are verifled through three numerical experiments, with experimental results consistent with theoretical analysis."],"url":"http://arxiv.org/abs/2403.11600v1","category":"math.NA"}
{"created":"2024-03-18 09:16:04","title":"Mitigation of the Microbunching Instability Through Transverse Landau Damping","abstract":"The microbunching instability has been a long-standing issue for high-brightness free-electron lasers (FELs), and is a significant show-stopper to achieving full longitudinal coherence in the x-ray regime. This paper reports the first experimental demonstration of microbunching instability mitigation through transverse Landau damping, based on linear optics control in a dispersive region. Analytical predictions for the microbunching content are supported by numerical calculations of the instability gain and confirmed through the experimental characterization of the spectral brightness of the FERMI FEL under different transverse optics configurations of the transfer line between the linear accelerator and the FEL.","sentences":["The microbunching instability has been a long-standing issue for high-brightness free-electron lasers (FELs), and is a significant show-stopper to achieving full longitudinal coherence in the x-ray regime.","This paper reports the first experimental demonstration of microbunching instability mitigation through transverse Landau damping, based on linear optics control in a dispersive region.","Analytical predictions for the microbunching content are supported by numerical calculations of the instability gain and confirmed through the experimental characterization of the spectral brightness of the FERMI FEL under different transverse optics configurations of the transfer line between the linear accelerator and the FEL."],"url":"http://arxiv.org/abs/2403.11594v1","category":"physics.acc-ph"}
{"created":"2024-03-18 08:53:03","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration","abstract":"Reliable multimodal sensor fusion algorithms re- quire accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high compu- tational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new ren- dering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.","sentences":["Reliable multimodal sensor fusion algorithms re- quire accurate spatiotemporal calibration.","Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results.","Nevertheless, such methods are inherently slow to train given the high compu- tational overhead caused by the large number of sampled points required for volume rendering.","With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new ren- dering approach to achieve faster multi-sensor calibration.","We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations.","We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset."],"url":"http://arxiv.org/abs/2403.11577v1","category":"cs.CV"}
{"created":"2024-03-18 08:10:44","title":"Thiol-amine co-solvents aided direct synthesis of ZnTe thin films by spin coating for low cost optoelectronic applications","abstract":"Zinc telluride (ZnTe) thin films have special semiconducting characteristics that make them very promising for a broad range of optoelectronic applications. In this work, a novel approach for synthesizing ZnTe thin films by spin coating technique is followed using a unique solution process with ZnTe directly dissolving in thiol-amine co-solvents. Thin films are synthesized on glass substrates and air annealed at 250-350 {\\deg}C. The polycrystalline phase of ZnTe is revealed through the X-ray diffraction (XRD) study. The scanning electron microscopy (SEM) is used to observe the evolution of surface smoothness with annealing temperature. Moreover, elemental compositions of ZnTe thin film have been determined by energy dispersive spectroscopy (EDS) study. FTIR spectroscopy reveals that ZnTe has been successfully synthesized as confirmed by the characteristic peaks in the spectrum of 750-1000 cm-1. Optical properties of the ZnTe thin films have been investigated using UV-vis spectroscopy. The transmittance of the films increases with annealing temperature. Furthermore, the optical bandgaps of the films of 2.92, 2.84, and 2.5 eV have been found at 250, 300, and 350 {\\deg}C annealing temperatures, respectively. These results suggest that controlling the annealing environment serves as a valuable strategy for tailoring the ZnTe film properties to meet specific application requirements. These results reveal that spin coated ZnTe thin films are attractive ones for various applications in optoelectronic devices such as solar cells and photodetectors.","sentences":["Zinc telluride (ZnTe) thin films have special semiconducting characteristics that make them very promising for a broad range of optoelectronic applications.","In this work, a novel approach for synthesizing ZnTe thin films by spin coating technique is followed using a unique solution process with ZnTe directly dissolving in thiol-amine co-solvents.","Thin films are synthesized on glass substrates and air annealed at 250-350 {\\deg}C.","The polycrystalline phase of ZnTe is revealed through the X-ray diffraction (XRD) study.","The scanning electron microscopy (SEM) is used to observe the evolution of surface smoothness with annealing temperature.","Moreover, elemental compositions of ZnTe thin film have been determined by energy dispersive spectroscopy (EDS) study.","FTIR spectroscopy reveals that ZnTe has been successfully synthesized as confirmed by the characteristic peaks in the spectrum of 750-1000 cm-1.","Optical properties of the ZnTe thin films have been investigated using UV-vis spectroscopy.","The transmittance of the films increases with annealing temperature.","Furthermore, the optical bandgaps of the films of 2.92, 2.84, and 2.5 eV have been found at 250, 300, and 350 {\\deg}C annealing temperatures, respectively.","These results suggest that controlling the annealing environment serves as a valuable strategy for tailoring the ZnTe film properties to meet specific application requirements.","These results reveal that spin coated ZnTe thin films are attractive ones for various applications in optoelectronic devices such as solar cells and photodetectors."],"url":"http://arxiv.org/abs/2403.11554v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 07:51:22","title":"Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation","abstract":"Most Vision-and-Language Navigation (VLN) algorithms tend to make decision errors, primarily due to a lack of visual common sense and insufficient reasoning capabilities. To address this issue, this paper proposes a Hierarchical Spatial Proximity Reasoning (HSPR) model. Firstly, we design a Scene Understanding Auxiliary Task (SUAT) to assist the agent in constructing a knowledge base of hierarchical spatial proximity for reasoning navigation. Specifically, this task utilizes panoramic views and object features to identify regions in the navigation environment and uncover the adjacency relationships between regions, objects, and region-object pairs. Secondly, we dynamically construct a semantic topological map through agent-environment interactions and propose a Multi-step Reasoning Navigation Algorithm (MRNA) based on the map. This algorithm continuously plans various feasible paths from one region to another, utilizing the constructed proximity knowledge base, enabling more efficient exploration. Additionally, we introduce a Proximity Adaptive Attention Module (PAAM) and Residual Fusion Method (RFM) to enable the model to obtain more accurate navigation decision confidence. Finally, we conduct experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R to validate the effectiveness of the proposed approach.","sentences":["Most Vision-and-Language Navigation (VLN) algorithms tend to make decision errors, primarily due to a lack of visual common sense and insufficient reasoning capabilities.","To address this issue, this paper proposes a Hierarchical Spatial Proximity Reasoning (HSPR) model.","Firstly, we design a Scene Understanding Auxiliary Task (SUAT) to assist the agent in constructing a knowledge base of hierarchical spatial proximity for reasoning navigation.","Specifically, this task utilizes panoramic views and object features to identify regions in the navigation environment and uncover the adjacency relationships between regions, objects, and region-object pairs.","Secondly, we dynamically construct a semantic topological map through agent-environment interactions and propose a Multi-step Reasoning Navigation Algorithm (MRNA) based on the map.","This algorithm continuously plans various feasible paths from one region to another, utilizing the constructed proximity knowledge base, enabling more efficient exploration.","Additionally, we introduce a Proximity Adaptive Attention Module (PAAM) and Residual Fusion Method (RFM) to enable the model to obtain more accurate navigation decision confidence.","Finally, we conduct experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R to validate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2403.11541v1","category":"cs.CV"}
{"created":"2024-03-18 07:33:56","title":"Continual Forgetting for Pre-trained Vision Models","abstract":"For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners. These requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify two key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. To address them, we propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we use LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. GS-LoRA is effective, parameter-efficient, data-efficient, and easy to implement. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that GS-LoRA manages to forget specific classes with minimal impact on other classes. Codes will be released on \\url{https://github.com/bjzhb666/GS-LoRA}.","sentences":["For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays.","In real-world scenarios, erasure requests originate at any time from both users and model owners.","These requests usually form a sequence.","Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest.","We define this problem as continual forgetting and identify two key challenges.","(i) For unwanted knowledge, efficient and effective deleting is crucial.","(ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal.","To address them, we propose Group Sparse LoRA (GS-LoRA).","Specifically, towards (i), we use LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others.","GS-LoRA is effective, parameter-efficient, data-efficient, and easy to implement.","We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that GS-LoRA manages to forget specific classes with minimal impact on other classes.","Codes will be released on \\url{https://github.com/bjzhb666/GS-LoRA}."],"url":"http://arxiv.org/abs/2403.11530v1","category":"cs.CV"}
{"created":"2024-03-18 07:31:39","title":"Video Object Segmentation with Dynamic Query Modulation","abstract":"Storing intermediate frame segmentations as memory for long-range context modeling, spatial-temporal memory-based methods have recently showcased impressive results in semi-supervised video object segmentation (SVOS). However, these methods face two key limitations: 1) relying on non-local pixel-level matching to read memory, resulting in noisy retrieved features for segmentation; 2) segmenting each object independently without interaction. These shortcomings make the memory-based methods struggle in similar object and multi-object segmentation. To address these issues, we propose a query modulation method, termed QMVOS. This method summarizes object features into dynamic queries and then treats them as dynamic filters for mask prediction, thereby providing high-level descriptions and object-level perception for the model. Efficient and effective multi-object interactions are realized through inter-query attention. Extensive experiments demonstrate that our method can bring significant improvements to the memory-based SVOS method and achieve competitive performance on standard SVOS benchmarks. The code is available at https://github.com/zht8506/QMVOS.","sentences":["Storing intermediate frame segmentations as memory for long-range context modeling, spatial-temporal memory-based methods have recently showcased impressive results in semi-supervised video object segmentation (SVOS).","However, these methods face two key limitations: 1) relying on non-local pixel-level matching to read memory, resulting in noisy retrieved features for segmentation; 2) segmenting each object independently without interaction.","These shortcomings make the memory-based methods struggle in similar object and multi-object segmentation.","To address these issues, we propose a query modulation method, termed QMVOS.","This method summarizes object features into dynamic queries and then treats them as dynamic filters for mask prediction, thereby providing high-level descriptions and object-level perception for the model.","Efficient and effective multi-object interactions are realized through inter-query attention.","Extensive experiments demonstrate that our method can bring significant improvements to the memory-based SVOS method and achieve competitive performance on standard SVOS benchmarks.","The code is available at https://github.com/zht8506/QMVOS."],"url":"http://arxiv.org/abs/2403.11529v1","category":"cs.CV"}
{"created":"2024-03-18 05:49:45","title":"Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting","abstract":"Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and test data by adapting a given model w.r.t. any test sample. Although recent TTA has shown promising performance, we still face two key challenges: 1) prior methods perform backpropagation for each test sample, resulting in unbearable optimization costs to many applications; 2) while existing TTA can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as forgetting). To this end, we have proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which develops an active sample selection criterion to identify reliable and non-redundant samples for test-time entropy minimization. To alleviate forgetting, EATA introduces a Fisher regularizer estimated from test samples to constrain important model parameters from drastic changes. However, in EATA, the adopted entropy loss consistently assigns higher confidence to predictions even for samples that are underlying uncertain, leading to overconfident predictions. To tackle this, we further propose EATA with Calibration (EATA-C) to separately exploit the reducible model uncertainty and the inherent data uncertainty for calibrated TTA. Specifically, we measure the model uncertainty by the divergence between predictions from the full network and its sub-networks, on which we propose a divergence loss to encourage consistent predictions instead of overconfident ones. To further recalibrate prediction confidence, we utilize the disagreement among predicted labels as an indicator of the data uncertainty, and then devise a min-max entropy regularizer to selectively increase and decrease prediction confidence for different samples. Experiments on image classification and semantic segmentation verify the effectiveness of our methods.","sentences":["Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and test data by adapting a given model w.r.t.","any test sample.","Although recent TTA has shown promising performance, we still face two key challenges: 1) prior methods perform backpropagation for each test sample, resulting in unbearable optimization costs to many applications; 2) while existing TTA can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as forgetting).","To this end, we have proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which develops an active sample selection criterion to identify reliable and non-redundant samples for test-time entropy minimization.","To alleviate forgetting, EATA introduces a Fisher regularizer estimated from test samples to constrain important model parameters from drastic changes.","However, in EATA, the adopted entropy loss consistently assigns higher confidence to predictions even for samples that are underlying uncertain, leading to overconfident predictions.","To tackle this, we further propose EATA with Calibration (EATA-C) to separately exploit the reducible model uncertainty and the inherent data uncertainty for calibrated TTA.","Specifically, we measure the model uncertainty by the divergence between predictions from the full network and its sub-networks, on which we propose a divergence loss to encourage consistent predictions instead of overconfident ones.","To further recalibrate prediction confidence, we utilize the disagreement among predicted labels as an indicator of the data uncertainty, and then devise a min-max entropy regularizer to selectively increase and decrease prediction confidence for different samples.","Experiments on image classification and semantic segmentation verify the effectiveness of our methods."],"url":"http://arxiv.org/abs/2403.11491v1","category":"cs.LG"}
{"created":"2024-03-18 04:46:01","title":"The 2-adic valuations of the algebraic central $L$-values for quadratic twists of weight 2 newforms","abstract":"Let $f$ be a normalized newform of weight 2 on $\\Gamma_0(N)$ whose coefficients lie in $\\mathbb{Q}$ and let $\\chi_M$ be a primitive quadratic Dirichlet character with conductor $M$. In this paper, under mild assumptions on $M$, we give a sharp lower bound of the 2-adic valuation of the algebraic central $L$-value $L(f, \\chi_M, 1)$ and evaluate the 2-adic valuation for an infinite number of $M$.","sentences":["Let $f$ be a normalized newform of weight 2 on $\\Gamma_0(N)$ whose coefficients lie in $\\mathbb{Q}$ and let $\\chi_M$ be a primitive quadratic Dirichlet character with conductor $M$. In this paper, under mild assumptions on $M$, we give a sharp lower bound of the 2-adic valuation of the algebraic central $L$-value $L(f, \\chi_M, 1)$ and evaluate the 2-adic valuation for an infinite number of $M$."],"url":"http://arxiv.org/abs/2403.11474v1","category":"math.NT"}
{"created":"2024-03-18 04:17:30","title":"Pulsed loading of a magneto-optical trap on atom chip for fast pressure recovery in ultrahigh vacuum environment","abstract":"This study presents investigations on pulsed loading of a magneto-optical trap (MOT) on an atom chip in an UHV environment. Using three parallel resistively heated Rb-metal dispensers activated by pulsed current supply, approximately 3.0 $\\times$ $10^{7}$ cold $^{87}Rb$ atoms were loaded into the MOT. A current pulse of $\\sim$ 24 A with duration of $\\sim$ 10 s raised the pressure in the chamber from 2.0 $\\times$ $10^{-10}$ Torr to 3.3 $\\times$ $10^{-10}$ Torr. Remarkably, the pressure recovery time after switching off the dispensers current was found to be $\\sim$ 600 ms, making a significant advancement in achieving fast recovery of UHV environment surrounding the MOT region. This study is very useful for laser cooling and magnetic trapping / evaporative cooling of atoms on atom chip in the same UHV chamber.","sentences":["This study presents investigations on pulsed loading of a magneto-optical trap (MOT) on an atom chip in an UHV environment.","Using three parallel resistively heated Rb-metal dispensers activated by pulsed current supply, approximately 3.0 $\\times$ $10^{7}$ cold $^{87}Rb$ atoms were loaded into the MOT.","A current pulse of $\\sim$ 24 A with duration of $\\sim$ 10 s raised the pressure in the chamber from 2.0 $\\times$ $10^{-10}$ Torr to 3.3 $\\times$ $10^{-10}$ Torr.","Remarkably, the pressure recovery time after switching off the dispensers current was found to be $\\sim$ 600 ms, making a significant advancement in achieving fast recovery of UHV environment surrounding the MOT region.","This study is very useful for laser cooling and magnetic trapping / evaporative cooling of atoms on atom chip in the same UHV chamber."],"url":"http://arxiv.org/abs/2403.11458v1","category":"physics.atom-ph"}
{"created":"2024-03-18 04:15:59","title":"First-Principles Calculation of Hubbard U for Terbium Metal under High Pressure","abstract":"Using density functional theory (DFT) and linear response approaches, we compute the on-site Hubbard interaction $U$ of elemental Terbium (Tb) metal in the pressure range $\\sim 0-65$ GPa. The resulting first-principles $U$ values with experimental crystal structures enable us to examine the magnetic properties of Tb using a self-consistent DFT+U method. The lowest-energy magnetic states in our calculations for different high-pressure Tb phases -- including hcp, $\\alpha$-Sm, and dhcp -- are found to be compatible with the corresponding magnetic ordering vectors reported in experiments. The result shows that the inclusion of Hubbard $U$ substantially improves the accuracy and efficiency in modeling correlated rare-earth materials. Our study also provides the necessary $U$ information for other quantum many-body techniques to study Tb under extreme pressure conditions.","sentences":["Using density functional theory (DFT) and linear response approaches, we compute the on-site Hubbard interaction $U$ of elemental Terbium (Tb) metal in the pressure range $\\sim 0-65$ GPa.","The resulting first-principles $U$ values with experimental crystal structures enable us to examine the magnetic properties of Tb using a self-consistent DFT+U method.","The lowest-energy magnetic states in our calculations for different high-pressure Tb phases -- including hcp, $\\alpha$-Sm, and dhcp -- are found to be compatible with the corresponding magnetic ordering vectors reported in experiments.","The result shows that the inclusion of Hubbard $U$ substantially improves the accuracy and efficiency in modeling correlated rare-earth materials.","Our study also provides the necessary $U$ information for other quantum many-body techniques to study Tb under extreme pressure conditions."],"url":"http://arxiv.org/abs/2403.11457v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 04:11:16","title":"Antiferromagnetic Ground State, Charge Density Waves and Oxygen Vacancies Induced Metal-Insulator Transition in Pressurized La$_{3}$Ni$_{2}$O$_{7}$","abstract":"La$_{3}$Ni$_{2}$O$_{7}$ has garnered widespread interest recently due to its high-temperature superconductivity under pressure, accompanied by charge density wave (CDW) ordering and metal-insulator (MI) transitions in the phase diagram. Here, we reveal with comprehensive calculations that La$_{3}$Ni$_{2}$O$_{7}$ possesses an antiferromagnetic ground state under both low and high pressures, with the strong Fermi surface nesting contributed by the flat band that leads to phonon softening and electronic instabilities. Several stable CDW orders with oxygen octahedral distortions are identified, which can trigger the MI transitions. The estimated CDW transition temperature ($\\approx$120 K) at ambient pressure agrees nicely with experimental results. In the presence of apical oxygen vacancies, we identify two different phases, say, half distortion and full distortion phases, respectively, and their competition can lead to a pressure-induced MI transition, in good agreement with experimental observations. In addition, we find that the electron-phonon coupling is too small to contribute to superconductivity. These results appear to indicate an unconventional superconducting pairing mechanism mediated by antiferromagnetic fluctuations. A phase diagram that is consistent with the experimental results is given. The present results not only explain the origins of experimentally observed CDW and MI transitions, but also provide insight for deeply understanding the properties like superconductivity, CDW and the role of oxygen vacancies in pressurized La$_{3}$Ni$_{2}$O$_{7}$.","sentences":["La$_{3}$Ni$_{2}$O$_{7}$ has garnered widespread interest recently due to its high-temperature superconductivity under pressure, accompanied by charge density wave (CDW) ordering and metal-insulator (MI) transitions in the phase diagram.","Here, we reveal with comprehensive calculations that La$_{3}$Ni$_{2}$O$_{7}$ possesses an antiferromagnetic ground state under both low and high pressures, with the strong Fermi surface nesting contributed by the flat band that leads to phonon softening and electronic instabilities.","Several stable CDW orders with oxygen octahedral distortions are identified, which can trigger the MI transitions.","The estimated CDW transition temperature ($\\approx$120 K) at ambient pressure agrees nicely with experimental results.","In the presence of apical oxygen vacancies, we identify two different phases, say, half distortion and full distortion phases, respectively, and their competition can lead to a pressure-induced MI transition, in good agreement with experimental observations.","In addition, we find that the electron-phonon coupling is too small to contribute to superconductivity.","These results appear to indicate an unconventional superconducting pairing mechanism mediated by antiferromagnetic fluctuations.","A phase diagram that is consistent with the experimental results is given.","The present results not only explain the origins of experimentally observed CDW and MI transitions, but also provide insight for deeply understanding the properties like superconductivity, CDW and the role of oxygen vacancies in pressurized La$_{3}$Ni$_{2}$O$_{7}$."],"url":"http://arxiv.org/abs/2403.11455v1","category":"cond-mat.supr-con"}
{"created":"2024-03-18 03:46:26","title":"Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction","abstract":"3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction. However, existing methods focus mainly on extending static 3DGS into a time-variant representation, while overlooking the rich motion information carried by 2D observations, thus suffering from performance degradation and model redundancy. To address the above problem, we propose a novel motion-aware enhancement framework for dynamic scene reconstruction, which mines useful motion cues from optical flow to improve different paradigms of dynamic 3DGS. Specifically, we first establish a correspondence between 3D Gaussian movements and pixel-level flow. Then a novel flow augmentation method is introduced with additional insights into uncertainty and loss collaboration. Moreover, for the prevalent deformation-based paradigm that presents a harder optimization problem, a transient-aware deformation auxiliary module is proposed. We conduct extensive experiments on both multi-view and monocular scenes to verify the merits of our work. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.","sentences":["3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction.","However, existing methods focus mainly on extending static 3DGS into a time-variant representation, while overlooking the rich motion information carried by 2D observations, thus suffering from performance degradation and model redundancy.","To address the above problem, we propose a novel motion-aware enhancement framework for dynamic scene reconstruction, which mines useful motion cues from optical flow to improve different paradigms of dynamic 3DGS.","Specifically, we first establish a correspondence between 3D Gaussian movements and pixel-level flow.","Then a novel flow augmentation method is introduced with additional insights into uncertainty and loss collaboration.","Moreover, for the prevalent deformation-based paradigm that presents a harder optimization problem, a transient-aware deformation auxiliary module is proposed.","We conduct extensive experiments on both multi-view and monocular scenes to verify the merits of our work.","Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency."],"url":"http://arxiv.org/abs/2403.11447v1","category":"cs.CV"}
{"created":"2024-03-18 02:53:49","title":"A Novel Paradigm Boosting Translation Capabilities of Large Language Models","abstract":"This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2 model, particularly on Chinese-Llama2 after monolingual augmentation, demonstrate the improved translation capabilities of LLMs. A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of machine translation.","sentences":["This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks.","The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning.","Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited.","While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data.","We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT.","Experimental results conducted using the Llama2 model, particularly on Chinese-Llama2 after monolingual augmentation, demonstrate the improved translation capabilities of LLMs.","A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient.","Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process.","Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B.","This achievement establishes our method as a pioneering strategy in the field of machine translation."],"url":"http://arxiv.org/abs/2403.11430v1","category":"cs.CL"}
{"created":"2024-03-18 02:39:21","title":"Benchmarking the Robustness of UAV Tracking Against Common Corruptions","abstract":"The robustness of unmanned aerial vehicle (UAV) tracking is crucial in many tasks like surveillance and robotics. Despite its importance, little attention is paid to the performance of UAV trackers under common corruptions due to lack of a dedicated platform. Addressing this, we propose UAV-C, a large-scale benchmark for assessing robustness of UAV trackers under common corruptions. Specifically, UAV-C is built upon two popular UAV datasets by introducing 18 common corruptions from 4 representative categories including adversarial, sensor, blur, and composite corruptions in different levels. Finally, UAV-C contains more than 10K sequences. To understand the robustness of existing UAV trackers against corruptions, we extensively evaluate 12 representative algorithms on UAV-C. Our study reveals several key findings: 1) Current trackers are vulnerable to corruptions, indicating more attention needed in enhancing the robustness of UAV trackers; 2) When accompanying together, composite corruptions result in more severe degradation to trackers; and 3) While each tracker has its unique performance profile, some trackers may be more sensitive to specific corruptions. By releasing UAV-C, we hope it, along with comprehensive analysis, serves as a valuable resource for advancing the robustness of UAV tracking against corruption. Our UAV-C will be available at https://github.com/Xiaoqiong-Liu/UAV-C.","sentences":["The robustness of unmanned aerial vehicle (UAV) tracking is crucial in many tasks like surveillance and robotics.","Despite its importance, little attention is paid to the performance of UAV trackers under common corruptions due to lack of a dedicated platform.","Addressing this, we propose UAV-C, a large-scale benchmark for assessing robustness of UAV trackers under common corruptions.","Specifically, UAV-C is built upon two popular UAV datasets by introducing 18 common corruptions from 4 representative categories including adversarial, sensor, blur, and composite corruptions in different levels.","Finally, UAV-C contains more than 10K sequences.","To understand the robustness of existing UAV trackers against corruptions, we extensively evaluate 12 representative algorithms on UAV-C. Our study reveals several key findings: 1) Current trackers are vulnerable to corruptions, indicating more attention needed in enhancing the robustness of UAV trackers; 2) When accompanying together, composite corruptions result in more severe degradation to trackers; and 3)","While each tracker has its unique performance profile, some trackers may be more sensitive to specific corruptions.","By releasing UAV-C, we hope it, along with comprehensive analysis, serves as a valuable resource for advancing the robustness of UAV tracking against corruption.","Our UAV-C will be available at https://github.com/Xiaoqiong-Liu/UAV-C."],"url":"http://arxiv.org/abs/2403.11424v1","category":"cs.CV"}
{"created":"2024-03-18 02:33:16","title":"Reckoning with the wicked problems of nuclear technology: Philosophy, design, and pedagogical method underlying a course on Nuclear Technology, Policy, and Society","abstract":"This paper describes the underlying philosophy, design, and implementation of a course on \"Nuclear Technology, Policy, and Society\" taught in the Department of Nuclear Engineering and Radiological Sciences at the University of Michigan. The course explores some of nuclear technology's most pressing challenges or its 'wicked problems'. Through this course students explore the origins of these problems be they social or technical and, they are offered tools, both conceptual and methodological to make sense of these problems, and guided through a semester-long exploration of how scientists engineers can work towards their resolution, and to what degree these problems can be solved through institutional transformation or a transformation in our own practices and norms as a field. The underlying pedagogical philosophy, implementation, and response to the course are described here for other instructors who might wish to create a similar course, or for non-academic nuclear scientists and engineers, who might perhaps, in these pages, find a vocabulary for articulating and reflecting on the nature of these problems as encountered in their praxis.","sentences":["This paper describes the underlying philosophy, design, and implementation of a course on \"Nuclear Technology, Policy, and Society\" taught in the Department of Nuclear Engineering and Radiological Sciences at the University of Michigan.","The course explores some of nuclear technology's most pressing challenges or its 'wicked problems'.","Through this course students explore the origins of these problems be they social or technical and, they are offered tools, both conceptual and methodological to make sense of these problems, and guided through a semester-long exploration of how scientists engineers can work towards their resolution, and to what degree these problems can be solved through institutional transformation or a transformation in our own practices and norms as a field.","The underlying pedagogical philosophy, implementation, and response to the course are described here for other instructors who might wish to create a similar course, or for non-academic nuclear scientists and engineers, who might perhaps, in these pages, find a vocabulary for articulating and reflecting on the nature of these problems as encountered in their praxis."],"url":"http://arxiv.org/abs/2403.11422v1","category":"physics.soc-ph"}
{"created":"2024-03-18 01:08:18","title":"Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF","abstract":"This work proposes a novel approach to bolster both the robot's risk assessment and safety measures while deepening its understanding of 3D scenes, which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian Splatting. To further enhance these capabilities, we incorporate additional sampled views from the environment with the RF model. One of our key contributions is the introduction of Risk-aware Environment Masking (RaEM), which prioritizes crucial information by selecting the next-best-view that maximizes the expected information gain. This targeted approach aims to minimize uncertainties surrounding the robot's path and enhance the safety of its navigation. Our method offers a dual benefit: improved robot safety and increased efficiency in risk-aware 3D scene reconstruction and understanding. Extensive experiments in real-world scenarios demonstrate the effectiveness of our proposed approach, highlighting its potential to establish a robust and safety-focused framework for active robot exploration and 3D scene understanding.","sentences":["This work proposes a novel approach to bolster both the robot's risk assessment and safety measures while deepening its understanding of 3D scenes, which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian Splatting.","To further enhance these capabilities, we incorporate additional sampled views from the environment with the RF model.","One of our key contributions is the introduction of Risk-aware Environment Masking (RaEM), which prioritizes crucial information by selecting the next-best-view that maximizes the expected information gain.","This targeted approach aims to minimize uncertainties surrounding the robot's path and enhance the safety of its navigation.","Our method offers a dual benefit: improved robot safety and increased efficiency in risk-aware 3D scene reconstruction and understanding.","Extensive experiments in real-world scenarios demonstrate the effectiveness of our proposed approach, highlighting its potential to establish a robust and safety-focused framework for active robot exploration and 3D scene understanding."],"url":"http://arxiv.org/abs/2403.11396v1","category":"cs.RO"}
{"created":"2024-03-18 00:48:58","title":"Investigating the Benefits of Projection Head for Representation Learning","abstract":"An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more beneficial, highlighting the intricate interplay between data augmentation and input features. Additionally, we demonstrate that introducing non-linearity into the network allows lower layers to learn features that are completely absent in higher layers. Finally, we show how this mechanism improves the robustness in supervised contrastive learning and supervised learning. We empirically validate our results through various experiments on CIFAR-10/100, UrbanCars and shifted versions of ImageNet. We also introduce a potential alternative to projection head, which offers a more interpretable and controllable design.","sentences":["An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations.","Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood.","The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better?","In this work, we provide a rigorous theoretical answer to this question.","We start by examining linear models trained with self-supervised contrastive loss.","We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers.","Consequently, lower layers tend to have more normalized and less specialized representations.","We theoretically characterize scenarios where such representations are more beneficial, highlighting the intricate interplay between data augmentation and input features.","Additionally, we demonstrate that introducing non-linearity into the network allows lower layers to learn features that are completely absent in higher layers.","Finally, we show how this mechanism improves the robustness in supervised contrastive learning and supervised learning.","We empirically validate our results through various experiments on CIFAR-10/100, UrbanCars and shifted versions of ImageNet.","We also introduce a potential alternative to projection head, which offers a more interpretable and controllable design."],"url":"http://arxiv.org/abs/2403.11391v1","category":"cs.LG"}
{"created":"2024-03-18 00:02:48","title":"Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction","abstract":"For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions. However, existing pathology-genomic multi-modal algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic\" framework for cancer survival outcome prediction. First, to extract valuable biological insights, we regulate the embedding space of a foundation model, scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data. Second, to address the imbalance-between-modalities problem, we propose a gradient modulation mechanism tailored to the Cox partial likelihood loss for survival prediction. The contributions of the modalities are dynamically monitored and adjusted during the training process, encouraging that both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer Genome Atlas) datasets, our model achieves substantially improved survival prediction accuracy.","sentences":["For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions.","However, existing pathology-genomic multi-modal algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality.","In this paper, we introduce a new multi-modal ``Path-GPTOmic\" framework for cancer survival outcome prediction.","First, to extract valuable biological insights, we regulate the embedding space of a foundation model, scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data.","Second, to address the imbalance-between-modalities problem, we propose a gradient modulation mechanism tailored to the Cox partial likelihood loss for survival prediction.","The contributions of the modalities are dynamically monitored and adjusted during the training process, encouraging that both modalities are sufficiently trained.","Evaluated on two TCGA(The Cancer Genome Atlas) datasets, our model achieves substantially improved survival prediction accuracy."],"url":"http://arxiv.org/abs/2403.11375v1","category":"cs.CV"}
{"created":"2024-03-17 23:18:40","title":"What Makes Math Word Problems Challenging for LLMs?","abstract":"This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.","sentences":["This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs).","We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs.","In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs."],"url":"http://arxiv.org/abs/2403.11369v1","category":"cs.CL"}
{"created":"2024-03-17 21:05:44","title":"Bayesian multi-exposure image fusion for robust high dynamic range preprocessing in ptychography","abstract":"Image information is restricted by the dynamic range of the detector, which can be addressed using multi-exposure image fusion (MEF). The conventional MEF approach employed in ptychography is often inadequate under conditions of low signal-to-noise ratio (SNR) or variations in illumination intensity. To address this, we developed a Bayesian approach for MEF based on a modified Poisson noise model that considers the background and saturation. Our method outperforms conventional MEF under challenging experimental conditions, as demonstrated by the synthetic and experimental data. Furthermore, this method is versatile and applicable to any imaging scheme requiring high dynamic range (HDR).","sentences":["Image information is restricted by the dynamic range of the detector, which can be addressed using multi-exposure image fusion (MEF).","The conventional MEF approach employed in ptychography is often inadequate under conditions of low signal-to-noise ratio (SNR) or variations in illumination intensity.","To address this, we developed a Bayesian approach for MEF based on a modified Poisson noise model that considers the background and saturation.","Our method outperforms conventional MEF under challenging experimental conditions, as demonstrated by the synthetic and experimental data.","Furthermore, this method is versatile and applicable to any imaging scheme requiring high dynamic range (HDR)."],"url":"http://arxiv.org/abs/2403.11344v1","category":"eess.IV"}
{"created":"2024-03-17 21:04:48","title":"Federated Transfer Learning with Differential Privacy","abstract":"Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \\textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of differential privacy. Our analyses incorporate data heterogeneity and privacy, highlighting the fundamental costs of both in federated learning and underscoring the benefit of knowledge transfer across data sets.","sentences":["Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges.","In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints.","We rigorously formulate the notion of \\textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server.","Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression.","By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of differential privacy.","Our analyses incorporate data heterogeneity and privacy, highlighting the fundamental costs of both in federated learning and underscoring the benefit of knowledge transfer across data sets."],"url":"http://arxiv.org/abs/2403.11343v1","category":"cs.LG"}
{"created":"2024-03-17 19:09:05","title":"Nonparametric Identification and Estimation with Non-Classical Errors-in-Variables","abstract":"This paper considers nonparametric identification and estimation of the regression function when a covariate is mismeasured. The measurement error need not be classical. Employing the small measurement error approximation, we establish nonparametric identification under weak and easy-to-interpret conditions on the instrumental variable. The paper also provides nonparametric estimators of the regression function and derives their rates of convergence.","sentences":["This paper considers nonparametric identification and estimation of the regression function when a covariate is mismeasured.","The measurement error need not be classical.","Employing the small measurement error approximation, we establish nonparametric identification under weak and easy-to-interpret conditions on the instrumental variable.","The paper also provides nonparametric estimators of the regression function and derives their rates of convergence."],"url":"http://arxiv.org/abs/2403.11309v1","category":"econ.EM"}
{"created":"2024-03-17 19:06:31","title":"A categorification of cluster algebras of type B and C through symmetric quivers","abstract":"We associate a symmetric quiver with relations Q to any seed of a cluster algebra of type B and C. Under this correspondence, cluster variables of type B (resp. C) correspond to orthogonal (resp. symplectic) indecomposable representations of Q. We find a Caldero-Chapoton map in this setting. We also give a categorical interpretation of the cluster expansion formula in the case of acyclic quivers.","sentences":["We associate a symmetric quiver with relations Q to any seed of a cluster algebra of type B and C. Under this correspondence, cluster variables of type B (resp.","C) correspond to orthogonal (resp.","symplectic) indecomposable representations of Q. We find a Caldero-Chapoton map in this setting.","We also give a categorical interpretation of the cluster expansion formula in the case of acyclic quivers."],"url":"http://arxiv.org/abs/2403.11308v1","category":"math.RT"}
{"created":"2024-03-17 18:40:29","title":"Multi-Sample Long Range Path Planning under Sensing Uncertainty for Off-Road Autonomous Driving","abstract":"We focus on the problem of long-range dynamic replanning for off-road autonomous vehicles, where a robot plans paths through a previously unobserved environment while continuously receiving noisy local observations. An effective approach for planning under sensing uncertainty is determinization, where one converts a stochastic world into a deterministic one and plans under this simplification. This makes the planning problem tractable, but the cost of following the planned path in the real world may be different than in the determinized world. This causes collisions if the determinized world optimistically ignores obstacles, or causes unnecessarily long routes if the determinized world pessimistically imagines more obstacles. We aim to be robust to uncertainty over potential worlds while still achieving the efficiency benefits of determinization. We evaluate algorithms for dynamic replanning on a large real-world dataset of challenging long-range planning problems from the DARPA RACER program. Our method, Dynamic Replanning via Evaluating and Aggregating Multiple Samples (DREAMS), outperforms other determinization-based approaches in terms of combined traversal time and collision cost. https://sites.google.com/cs.washington.edu/dreams/","sentences":["We focus on the problem of long-range dynamic replanning for off-road autonomous vehicles, where a robot plans paths through a previously unobserved environment while continuously receiving noisy local observations.","An effective approach for planning under sensing uncertainty is determinization, where one converts a stochastic world into a deterministic one and plans under this simplification.","This makes the planning problem tractable, but the cost of following the planned path in the real world may be different than in the determinized world.","This causes collisions if the determinized world optimistically ignores obstacles, or causes unnecessarily long routes if the determinized world pessimistically imagines more obstacles.","We aim to be robust to uncertainty over potential worlds while still achieving the efficiency benefits of determinization.","We evaluate algorithms for dynamic replanning on a large real-world dataset of challenging long-range planning problems from the DARPA RACER program.","Our method, Dynamic Replanning via Evaluating and Aggregating Multiple Samples (DREAMS), outperforms other determinization-based approaches in terms of combined traversal time and collision cost.","https://sites.google.com/cs.washington.edu/dreams/"],"url":"http://arxiv.org/abs/2403.11298v1","category":"cs.RO"}
{"created":"2024-03-17 17:47:05","title":"Neural-network density functional theory","abstract":"Deep-learning density functional theory (DFT) shows great promise to significantly accelerate material discovery and potentially revolutionize materials research, which demands a close combination between neural networks and DFT computation. However, current research in this field primarily relies on supervised learning, making the developments of neural networks and DFT isolated from each other. In this work, we present a theoretical framework of neural-network DFT, which unifies the optimization of neural networks with the variational computation of DFT, enabling physics-informed unsupervised learning. Moreover, we develop a differential DFT code incorporated with deep-learning DFT Hamiltonian, and introduce algorithms of automatic differentiation and backpropagation to DFT, demonstrating the concept of neural-network DFT. The advanced neural-network architecture not only surpasses conventional approaches in accuracy and efficiency, but offers a new paradigm for developing deep-learning DFT methods.","sentences":["Deep-learning density functional theory (DFT) shows great promise to significantly accelerate material discovery and potentially revolutionize materials research, which demands a close combination between neural networks and DFT computation.","However, current research in this field primarily relies on supervised learning, making the developments of neural networks and DFT isolated from each other.","In this work, we present a theoretical framework of neural-network DFT, which unifies the optimization of neural networks with the variational computation of DFT, enabling physics-informed unsupervised learning.","Moreover, we develop a differential DFT code incorporated with deep-learning DFT Hamiltonian, and introduce algorithms of automatic differentiation and backpropagation to DFT, demonstrating the concept of neural-network DFT.","The advanced neural-network architecture not only surpasses conventional approaches in accuracy and efficiency, but offers a new paradigm for developing deep-learning DFT methods."],"url":"http://arxiv.org/abs/2403.11287v1","category":"physics.comp-ph"}
{"created":"2024-03-17 16:44:28","title":"Localized Orthogonal Decomposition Methods vs. Classical FEM for the Gross-Pitaevskii Equation","abstract":"The time-dependent Gross-Pitaevksii equation (GPE) is a nonlinear Schr\\\"odinger equation which is used in quantum physics to model the dynamics of Bose-Einstein condensates. In this work we consider numerical approximations of the GPE based on a multiscale approach known as the localized orthogonal decomposition. Combined with an energy preserving time integrator one derives a method which is of high order in space and time under mild regularity assumptions. In previous work, the method has been shown to be numerically very efficient compared to first order Lagrange FEM. In this paper, we further investigate the performance of the method and compare it with higher order Lagrange FEM. For rough problems we observe that the novel method performs very efficient and retains its high order, while the classical methods can only compete well for smooth problems.","sentences":["The time-dependent Gross-Pitaevksii equation (GPE) is a nonlinear Schr\\\"odinger equation which is used in quantum physics to model the dynamics of Bose-Einstein condensates.","In this work we consider numerical approximations of the GPE based on a multiscale approach known as the localized orthogonal decomposition.","Combined with an energy preserving time integrator one derives a method which is of high order in space and time under mild regularity assumptions.","In previous work, the method has been shown to be numerically very efficient compared to first order Lagrange FEM.","In this paper, we further investigate the performance of the method and compare it with higher order Lagrange FEM.","For rough problems we observe that the novel method performs very efficient and retains its high order, while the classical methods can only compete well for smooth problems."],"url":"http://arxiv.org/abs/2403.11268v1","category":"math.NA"}
{"created":"2024-03-17 16:19:40","title":"Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation","abstract":"Source-free unsupervised domain adaptation (SFUDA) aims to enable the utilization of a pre-trained source model in an unlabeled target domain without access to source data. Self-training is a way to solve SFUDA, where confident target samples are iteratively selected as pseudo-labeled samples to guide target model learning. However, prior heuristic noisy pseudo-label filtering methods all involve introducing extra models, which are sensitive to model assumptions and may introduce additional errors or mislabeling. In this work, we propose a method called Uncertainty-aware Pseudo-label-filtering Adaptation (UPA) to efficiently address this issue in a coarse-to-fine manner. Specially, we first introduce a sample selection module named Adaptive Pseudo-label Selection (APS), which is responsible for filtering noisy pseudo labels. The APS utilizes a simple sample uncertainty estimation method by aggregating knowledge from neighboring samples and confident samples are selected as clean pseudo-labeled. Additionally, we incorporate Class-Aware Contrastive Learning (CACL) to mitigate the memorization of pseudo-label noise by learning robust pair-wise representation supervised by pseudo labels. Through extensive experiments conducted on three widely used benchmarks, we demonstrate that our proposed method achieves competitive performance on par with state-of-the-art SFUDA methods. Code is available at https://github.com/chenxi52/UPA.","sentences":["Source-free unsupervised domain adaptation (SFUDA) aims to enable the utilization of a pre-trained source model in an unlabeled target domain without access to source data.","Self-training is a way to solve SFUDA, where confident target samples are iteratively selected as pseudo-labeled samples to guide target model learning.","However, prior heuristic noisy pseudo-label filtering methods all involve introducing extra models, which are sensitive to model assumptions and may introduce additional errors or mislabeling.","In this work, we propose a method called Uncertainty-aware Pseudo-label-filtering Adaptation (UPA) to efficiently address this issue in a coarse-to-fine manner.","Specially, we first introduce a sample selection module named Adaptive Pseudo-label Selection (APS), which is responsible for filtering noisy pseudo labels.","The APS utilizes a simple sample uncertainty estimation method by aggregating knowledge from neighboring samples and confident samples are selected as clean pseudo-labeled.","Additionally, we incorporate Class-Aware Contrastive Learning (CACL) to mitigate the memorization of pseudo-label noise by learning robust pair-wise representation supervised by pseudo labels.","Through extensive experiments conducted on three widely used benchmarks, we demonstrate that our proposed method achieves competitive performance on par with state-of-the-art SFUDA methods.","Code is available at https://github.com/chenxi52/UPA."],"url":"http://arxiv.org/abs/2403.11256v1","category":"cs.CV"}
{"created":"2024-03-17 16:19:13","title":"The interpolant existence problem for weak K4 and difference logic","abstract":"As well known, weak K4 and the difference logic DL do not enjoy the Craig inter- polation property. Our concern here is the problem of deciding whether any given implication does have an interpolant in these logics. We show that the nonexistence of an interpolant can always be witnessed by a pair of bisimilar models of polynomial size for DL and of triple-exponential size for weak K4, and so the interpolant existence problems for these logics are decidable in coNP and coN3ExpTime, respectively. We also establish coNExpTime-hardness of this problem for weak K4, which is higher than the PSpace-completeness of its decision problem.","sentences":["As well known, weak K4 and the difference logic DL do not enjoy the Craig inter- polation property.","Our concern here is the problem of deciding whether any given implication does have an interpolant in these logics.","We show that the nonexistence of an interpolant can always be witnessed by a pair of bisimilar models of polynomial size for DL and of triple-exponential size for weak K4, and so the interpolant existence problems for these logics are decidable in coNP and coN3ExpTime, respectively.","We also establish coNExpTime-hardness of this problem for weak K4, which is higher than the PSpace-completeness of its decision problem."],"url":"http://arxiv.org/abs/2403.11255v1","category":"cs.LO"}
{"created":"2024-03-17 14:42:05","title":"STAIR: Semantic-Targeted Active Implicit Reconstruction","abstract":"Many autonomous robotic applications require object-level understanding when deployed. Actively reconstructing objects of interest, i.e. objects with specific semantic meanings, is therefore relevant for a robot to perform downstream tasks in an initially unknown environment. In this work, we propose a novel framework for semantic-targeted active reconstruction using posed RGB-D measurements and 2D semantic labels as input. The key components of our framework are a semantic implicit neural representation and a compatible planning utility function based on semantic rendering and uncertainty estimation, enabling adaptive view planning to target objects of interest. Our planning approach achieves better reconstruction performance in terms of mesh and novel view rendering quality compared to implicit reconstruction baselines that do not consider semantics for view planning. Our framework further outperforms a state-of-the-art semantic-targeted active reconstruction pipeline based on explicit maps, justifying our choice of utilising implicit neural representations to tackle semantic-targeted active reconstruction problems.","sentences":["Many autonomous robotic applications require object-level understanding when deployed.","Actively reconstructing objects of interest, i.e. objects with specific semantic meanings, is therefore relevant for a robot to perform downstream tasks in an initially unknown environment.","In this work, we propose a novel framework for semantic-targeted active reconstruction using posed RGB-D measurements and 2D semantic labels as input.","The key components of our framework are a semantic implicit neural representation and a compatible planning utility function based on semantic rendering and uncertainty estimation, enabling adaptive view planning to target objects of interest.","Our planning approach achieves better reconstruction performance in terms of mesh and novel view rendering quality compared to implicit reconstruction baselines that do not consider semantics for view planning.","Our framework further outperforms a state-of-the-art semantic-targeted active reconstruction pipeline based on explicit maps, justifying our choice of utilising implicit neural representations to tackle semantic-targeted active reconstruction problems."],"url":"http://arxiv.org/abs/2403.11233v1","category":"cs.RO"}
{"created":"2024-03-17 13:30:34","title":"Reduced model and nonlinear analysis of localized instabilities of residually stressed cylinders under axial stretch","abstract":"In this paper we present a dimensional reduction to obtain a one-dimensional model to analyze localized necking or bulging in a residually stressed circular cylindrical solid. The nonlinear theory of elasticity is first specialized to obtain the equations governing the homogeneous deformation. Then, to analyze the non-homogeneous part, we include higher order correction terms of the axisymmetric displacement components leading to a three- dimensional form of the total potential energy functional. Details of the reduction to the one-dimensional form are given. We focus on a residually stressed Gent material and use numerical methods to solve the governing equations. Two loading conditions are considered. In the first, the residual stress is maintained constant, while the axial stretch is used as the loading parameter. In the second, we keep the pre-stretch constant and monotonically increase the residual stress until bifurcation occurs. We specify initial conditions, find the critical values for localized bifurcation and compute the change in radius during localized necking or bulging growth. Finally, we optimize material properties and use the one-dimensional model to simulate necking or bulging until the Maxwell values of stretch are reached.","sentences":["In this paper we present a dimensional reduction to obtain a one-dimensional model to analyze localized necking or bulging in a residually stressed circular cylindrical solid.","The nonlinear theory of elasticity is first specialized to obtain the equations governing the homogeneous deformation.","Then, to analyze the non-homogeneous part, we include higher order correction terms of the axisymmetric displacement components leading to a three- dimensional form of the total potential energy functional.","Details of the reduction to the one-dimensional form are given.","We focus on a residually stressed Gent material and use numerical methods to solve the governing equations.","Two loading conditions are considered.","In the first, the residual stress is maintained constant, while the axial stretch is used as the loading parameter.","In the second, we keep the pre-stretch constant and monotonically increase the residual stress until bifurcation occurs.","We specify initial conditions, find the critical values for localized bifurcation and compute the change in radius during localized necking or bulging growth.","Finally, we optimize material properties and use the one-dimensional model to simulate necking or bulging until the Maxwell values of stretch are reached."],"url":"http://arxiv.org/abs/2403.11215v1","category":"cond-mat.soft"}
{"created":"2024-03-17 13:23:03","title":"Error bounds for rank-one DNN reformulation of QAP and DC exact penalty approach","abstract":"This paper concerns the quadratic assignment problem (QAP), a class of challenging combinatorial optimization problems. We provide an equivalent rank-one doubly nonnegative (DNN) reformulation with fewer equality constraints, and derive the local error bounds for its feasible set. By leveraging these error bounds, we prove that the penalty problem induced by the difference of convexity (DC) reformulation of the rank-one constraint is a global exact penalty, and so is the penalty problem for its Burer-Monteiro (BM) factorization. As a byproduct, we verify that the penalty problem for the rank-one DNN reformulation proposed in \\cite{Jiang21} is a global exact penalty without the calmness assumption. Then, we develop a continuous relaxation approach by seeking approximate stationary points of a finite number of penalty problems for the BM factorization with an augmented Lagrangian method, whose asymptotic convergence certificate is also provided under a mild condition. Numerical comparison with Gurobi for \\textbf{131} benchmark instances validates the efficiency of the proposed DC exact penalty approach.","sentences":["This paper concerns the quadratic assignment problem (QAP), a class of challenging combinatorial optimization problems.","We provide an equivalent rank-one doubly nonnegative (DNN) reformulation with fewer equality constraints, and derive the local error bounds for its feasible set.","By leveraging these error bounds, we prove that the penalty problem induced by the difference of convexity (DC) reformulation of the rank-one constraint is a global exact penalty, and so is the penalty problem for its Burer-Monteiro (BM) factorization.","As a byproduct, we verify that the penalty problem for the rank-one DNN reformulation proposed in \\cite{Jiang21} is a global exact penalty without the calmness assumption.","Then, we develop a continuous relaxation approach by seeking approximate stationary points of a finite number of penalty problems for the BM factorization with an augmented Lagrangian method, whose asymptotic convergence certificate is also provided under a mild condition.","Numerical comparison with Gurobi for \\textbf{131} benchmark instances validates the efficiency of the proposed DC exact penalty approach."],"url":"http://arxiv.org/abs/2403.11210v1","category":"math.OC"}
{"created":"2024-03-17 12:40:46","title":"Neural Markov Random Field for Stereo Matching","abstract":"Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks. Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's graph inductive bias. To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity. The proposed approach ranks $1^{st}$ on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms. This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our method exhibits strong cross-domain generalization and can recover sharp edges. The codes at https://github.com/aeolusguan/NMRF .","sentences":["Stereo matching is a core task for many computer vision and robotics applications.","Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models.","While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing.","To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks.","Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's graph inductive bias.","To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity.","The proposed approach ranks $1^{st}$ on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms.","This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015.","In addition, our method exhibits strong cross-domain generalization and can recover sharp edges.","The codes at https://github.com/aeolusguan/NMRF ."],"url":"http://arxiv.org/abs/2403.11193v1","category":"cs.CV"}
{"created":"2024-03-17 11:36:57","title":"Modified Steinberg-Guinan elasticity model to describe softening-hardening dual anomaly in vanadium","abstract":"Constitutive models are essential for describing the mechanical behavior of materials under high temperatures and pressures, among which the Steinberg-Guinan (SG) model is widely adopted. Recent work has discovered a peculiar dual anomaly of compression-induced softening and heating-induced hardening in the elasticity of compressed vanadium [Phys. Rev. B 104, 134102 (2021)], which is beyond the capability of the SG model to describe. In this work, a modified SG constitutive model is proposed to embody such an anomalous behavior. Elemental vanadium is considered as an example to demonstrate the effectiveness of this improved model in describing the dual anomalies of mechanical elasticity. This new variant of the SG model can also be applied to other materials that present an irregular variation in the mechanical elasticity, and is important to faithfully model and simulate the mechanical response of materials under extreme conditions.","sentences":["Constitutive models are essential for describing the mechanical behavior of materials under high temperatures and pressures, among which the Steinberg-Guinan (SG) model is widely adopted.","Recent work has discovered a peculiar dual anomaly of compression-induced softening and heating-induced hardening in the elasticity of compressed vanadium [Phys. Rev. B 104, 134102 (2021)], which is beyond the capability of the SG model to describe.","In this work, a modified SG constitutive model is proposed to embody such an anomalous behavior.","Elemental vanadium is considered as an example to demonstrate the effectiveness of this improved model in describing the dual anomalies of mechanical elasticity.","This new variant of the SG model can also be applied to other materials that present an irregular variation in the mechanical elasticity, and is important to faithfully model and simulate the mechanical response of materials under extreme conditions."],"url":"http://arxiv.org/abs/2403.11179v1","category":"physics.app-ph"}
{"created":"2024-03-17 11:34:03","title":"The truncated EM scheme for multiple-delay SDEs with irregular coefficients and application to stochastic volatility model","abstract":"This paper focuses on the numerical scheme for multiple-delay stochastic differential equations with partially H\\\"older continuous drifts and locally H\\\"older continuous diffusion coefficients. To handle with the superlinear terms in coefficients, the truncated Euler-Maruyama scheme is employed. Under the given conditions, the convergence rates at time $T$ in both $\\mathcal{L}^{1}$ and $\\mathcal{L}^{2}$ senses are shown by virtue of the Yamada-Watanabe approximation technique. Moreover, the convergence rates over a finite time interval $[0,T]$ are also obtained. Additionally, it should be noted that the convergence rates will not be affected by the number of delay variables. Finally, we perform the numerical experiments on the stochastic volatility model to verify the reliability of the theoretical results.","sentences":["This paper focuses on the numerical scheme for multiple-delay stochastic differential equations with partially H\\\"older continuous drifts and locally H\\\"older continuous diffusion coefficients.","To handle with the superlinear terms in coefficients, the truncated Euler-Maruyama scheme is employed.","Under the given conditions, the convergence rates at time $T$ in both $\\mathcal{L}^{1}$ and $\\mathcal{L}^{2}$ senses are shown by virtue of the Yamada-Watanabe approximation technique.","Moreover, the convergence rates over a finite time interval $[0,T]$ are also obtained.","Additionally, it should be noted that the convergence rates will not be affected by the number of delay variables.","Finally, we perform the numerical experiments on the stochastic volatility model to verify the reliability of the theoretical results."],"url":"http://arxiv.org/abs/2403.11178v1","category":"math.NA"}
{"created":"2024-03-17 11:32:18","title":"Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment","abstract":"No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent representations for images with comparable quality. Our method achieves state-of-the-art performance on several datasets with authentic distortions. Moreover, despite not requiring MOS, QualiCLIP outperforms supervised methods when their training dataset differs from the testing one, thus proving to be more suitable for real-world scenarios. Furthermore, our approach demonstrates greater robustness and improved explainability than competing methods. The code and the model are publicly available at https://github.com/miccunifi/QualiCLIP.","sentences":["No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable.","The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios.","To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS.","In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images.","Starting from pristine images, we synthetically degrade them with increasing levels of intensity.","Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent representations for images with comparable quality.","Our method achieves state-of-the-art performance on several datasets with authentic distortions.","Moreover, despite not requiring MOS, QualiCLIP outperforms supervised methods when their training dataset differs from the testing one, thus proving to be more suitable for real-world scenarios.","Furthermore, our approach demonstrates greater robustness and improved explainability than competing methods.","The code and the model are publicly available at https://github.com/miccunifi/QualiCLIP."],"url":"http://arxiv.org/abs/2403.11176v1","category":"cs.CV"}
{"created":"2024-03-17 11:17:06","title":"Artifact Feature Purification for Cross-domain Detection of AI-generated Images","abstract":"In the era of AIGC, the fast development of visual content generation technologies, such as diffusion models, bring potential security risks to our society. Existing generated image detection methods suffer from performance drop when faced with out-of-domain generators and image scenes. To relieve this problem, we propose Artifact Purification Network (APN) to facilitate the artifact extraction from generated images through the explicit and implicit purification processes. For the explicit one, a suspicious frequency-band proposal method and a spatial feature decomposition method are proposed to extract artifact-related features. For the implicit one, a training strategy based on mutual information estimation is proposed to further purify the artifact-related features. Experiments show that for cross-generator detection, the average accuracy of APN is 5.6% ~ 16.4% higher than the previous 10 methods on GenImage dataset and 1.7% ~ 50.1% on DiffusionForensics dataset. For cross-scene detection, APN maintains its high performance. Via visualization analysis, we find that the proposed method extracts flexible forgery patterns and condenses the forgery information diluted in irrelevant features. We also find that the artifact features APN focuses on across generators and scenes are global and diverse. The code will be available on GitHub.","sentences":["In the era of AIGC, the fast development of visual content generation technologies, such as diffusion models, bring potential security risks to our society.","Existing generated image detection methods suffer from performance drop when faced with out-of-domain generators and image scenes.","To relieve this problem, we propose Artifact Purification Network (APN) to facilitate the artifact extraction from generated images through the explicit and implicit purification processes.","For the explicit one, a suspicious frequency-band proposal method and a spatial feature decomposition method are proposed to extract artifact-related features.","For the implicit one, a training strategy based on mutual information estimation is proposed to further purify the artifact-related features.","Experiments show that for cross-generator detection, the average accuracy of APN is 5.6% ~ 16.4% higher than the previous 10 methods on GenImage dataset and 1.7% ~ 50.1% on DiffusionForensics dataset.","For cross-scene detection, APN maintains its high performance.","Via visualization analysis, we find that the proposed method extracts flexible forgery patterns and condenses the forgery information diluted in irrelevant features.","We also find that the artifact features APN focuses on across generators and scenes are global and diverse.","The code will be available on GitHub."],"url":"http://arxiv.org/abs/2403.11172v1","category":"cs.CV"}
{"created":"2024-03-17 09:41:20","title":"Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model","abstract":"Universal image restoration is a practical and potential computer vision task for real-world applications. The main challenge of this task is handling the different degradation distributions at once. Existing methods mainly utilize task-specific conditions (e.g., prompt) to guide the model to learn different distributions separately, named multi-partite mapping. However, it is not suitable for universal model learning as it ignores the shared information between different tasks. In this work, we propose an advanced selective hourglass mapping strategy based on diffusion model, termed DiffUIR. Two novel considerations make our DiffUIR non-trivial. Firstly, we equip the model with strong condition guidance to obtain accurate generation direction of diffusion model (selective). More importantly, DiffUIR integrates a flexible shared distribution term (SDT) into the diffusion algorithm elegantly and naturally, which gradually maps different distributions into a shared one. In the reverse process, combined with SDT and strong condition guidance, DiffUIR iteratively guides the shared distribution to the task-specific distribution with high image quality (hourglass). Without bells and whistles, by only modifying the mapping strategy, we achieve state-of-the-art performance on five image restoration tasks, 22 benchmarks in the universal setting and zero-shot generalization setting. Surprisingly, by only using a lightweight model (only 0.89M), we could achieve outstanding performance. The source code and pre-trained models are available at https://github.com/iSEE-Laboratory/DiffUIR","sentences":["Universal image restoration is a practical and potential computer vision task for real-world applications.","The main challenge of this task is handling the different degradation distributions at once.","Existing methods mainly utilize task-specific conditions (e.g., prompt) to guide the model to learn different distributions separately, named multi-partite mapping.","However, it is not suitable for universal model learning as it ignores the shared information between different tasks.","In this work, we propose an advanced selective hourglass mapping strategy based on diffusion model, termed DiffUIR.","Two novel considerations make our DiffUIR non-trivial.","Firstly, we equip the model with strong condition guidance to obtain accurate generation direction of diffusion model (selective).","More importantly, DiffUIR integrates a flexible shared distribution term (SDT) into the diffusion algorithm elegantly and naturally, which gradually maps different distributions into a shared one.","In the reverse process, combined with SDT and strong condition guidance, DiffUIR iteratively guides the shared distribution to the task-specific distribution with high image quality (hourglass).","Without bells and whistles, by only modifying the mapping strategy, we achieve state-of-the-art performance on five image restoration tasks, 22 benchmarks in the universal setting and zero-shot generalization setting.","Surprisingly, by only using a lightweight model (only 0.89M), we could achieve outstanding performance.","The source code and pre-trained models are available at https://github.com/iSEE-Laboratory/DiffUIR"],"url":"http://arxiv.org/abs/2403.11157v1","category":"cs.CV"}
{"created":"2024-03-17 09:28:46","title":"Interactive $360^{\\circ}$ Video Streaming Using FoV-Adaptive Coding with Temporal Prediction","abstract":"For $360^{\\circ}$ video streaming, FoV-adaptive coding that allocates more bits for the predicted user's field of view (FoV) is an effective way to maximize the rendered video quality under the limited bandwidth. We develop a low-latency FoV-adaptive coding and streaming system for interactive applications that is robust to bandwidth variations and FoV prediction errors. To minimize the end-to-end delay and yet maximize the coding efficiency, we propose a frame-level FoV-adaptive inter-coding structure. In each frame, regions that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only. This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors. The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint. Integrating such frame-level FoV adaptation with temporal prediction is challenging due to the temporal variations of the FoV. We propose novel ways for modeling the influence of FoV dynamics on the quality-rate performance of temporal predictive coding.We further develop LSTM-based machine learning models to predict the user's FoV and network bandwidth.The proposed system is compared with three benchmark systems, using real-world network bandwidth traces and FoV traces, and is shown to significantly improve the rendered video quality, while achieving very low end-to-end delay and low frame-freeze probability.","sentences":["For $360^{\\circ}$ video streaming, FoV-adaptive coding that allocates more bits for the predicted user's field of view (FoV) is an effective way to maximize the rendered video quality under the limited bandwidth.","We develop a low-latency FoV-adaptive coding and streaming system for interactive applications that is robust to bandwidth variations and FoV prediction errors.","To minimize the end-to-end delay and yet maximize the coding efficiency, we propose a frame-level FoV-adaptive inter-coding structure.","In each frame, regions that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only.","This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors.","The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint.","Integrating such frame-level FoV adaptation with temporal prediction is challenging due to the temporal variations of the FoV. We propose novel ways for modeling the influence of FoV dynamics on the quality-rate performance of temporal predictive coding.","We further develop LSTM-based machine learning models to predict the user's FoV and network bandwidth.","The proposed system is compared with three benchmark systems, using real-world network bandwidth traces and FoV traces, and is shown to significantly improve the rendered video quality, while achieving very low end-to-end delay and low frame-freeze probability."],"url":"http://arxiv.org/abs/2403.11155v1","category":"eess.IV"}
{"created":"2024-03-17 09:19:19","title":"Linkage and Essential $p$-Dimension","abstract":"We prove that two cyclically linked $p$-algebras of prime degree become inseparably linked under a prime to $p$ extension if and only if the essential $p$-dimension of the pair is 2. We conclude that the essential $p$-dimension of pairs of cyclically linked $p$-algebras is 3 by constructing an example of a pair that does not become inseparably linked under any prime to $p$ extension.","sentences":["We prove that two cyclically linked $p$-algebras of prime degree become inseparably linked under a prime to $p$ extension if and only if the essential $p$-dimension of the pair is 2.","We conclude that the essential $p$-dimension of pairs of cyclically linked $p$-algebras is 3 by constructing an example of a pair that does not become inseparably linked under any prime to $p$ extension."],"url":"http://arxiv.org/abs/2403.11154v1","category":"math.RA"}
{"created":"2024-03-17 08:48:30","title":"Dynamics and Resonance Fluorescence from a Superconducting Artificial Atom Doubly Driven by Quantized and Classical Fields","abstract":"We report an experimental demonstration of resonance fluorescence in a two-level superconducting artificial atom under two driving fields coupled to a detuned cavity. One of the fields is classical and the other is varied from quantum (vacuum fluctuations) to classical one by controlling the photon number inside the cavity. The device consists of a transmon qubit strongly coupled to a one-dimensional transmission line and a coplanar waveguide resonator. We observe a sideband anti-crossing and asymmetry in the emission spectra of the system through a one-dimensional transmission line, which is fundamentally different from the weak coupling case. By changing the photon number inside the cavity, the emission spectrum of our doubly driven system approaches to the case when the atom is driven by two classical bichromatic fields. We also measure the dynamical evolution of the system through the transmission line and study the properties of the first-order correlation function, Rabi oscillations and energy relaxation in the system. The study of resonance fluorescence from an atom driven by two fields promotes understanding decoherence in superconducting quantum circuits and may find applications in superconducting quantum computing and quantum networks.","sentences":["We report an experimental demonstration of resonance fluorescence in a two-level superconducting artificial atom under two driving fields coupled to a detuned cavity.","One of the fields is classical and the other is varied from quantum (vacuum fluctuations) to classical one by controlling the photon number inside the cavity.","The device consists of a transmon qubit strongly coupled to a one-dimensional transmission line and a coplanar waveguide resonator.","We observe a sideband anti-crossing and asymmetry in the emission spectra of the system through a one-dimensional transmission line, which is fundamentally different from the weak coupling case.","By changing the photon number inside the cavity, the emission spectrum of our doubly driven system approaches to the case when the atom is driven by two classical bichromatic fields.","We also measure the dynamical evolution of the system through the transmission line and study the properties of the first-order correlation function, Rabi oscillations and energy relaxation in the system.","The study of resonance fluorescence from an atom driven by two fields promotes understanding decoherence in superconducting quantum circuits and may find applications in superconducting quantum computing and quantum networks."],"url":"http://arxiv.org/abs/2403.11142v1","category":"quant-ph"}
{"created":"2024-03-17 08:34:21","title":"Electrically controlled nonvolatile switching of single-atom magnetism in a Dy@C84 single-molecule transistor","abstract":"Single-atom magnetism switching is a key technique towards the ultimate data storage density of computer hard disks and has been conceptually realized by leveraging the spin bistability of a magnetic atom under a scanning tunnelling microscope. However, it has rarely been applied to solid-state transistors, an advancement that would be highly desirable for enabling various applications. Here, we demonstrate realization of the electrically controlled Zeeman effect in Dy@C84 single-molecule transistors, thus revealing a transition in the magnetic moment from 3.8 {\\mu}B to 5.1 {\\mu}B for the ground-state GN at an electric field strength of 3-10 MV/cm. The consequent magnetoresistance significantly increases from 600% to 1100% at the resonant tunneling point. Density functional theory calculations further corroborate our realization of nonvolatile switching of single-atom magnetism, and the switching stability emanates from an energy barrier of 92 meV for atomic relaxation. These results highlight the potential of using endohedral metallofullerenes for high-temperature, high-stability, high-speed, and compact single-atom magnetic data storage.","sentences":["Single-atom magnetism switching is a key technique towards the ultimate data storage density of computer hard disks and has been conceptually realized by leveraging the spin bistability of a magnetic atom under a scanning tunnelling microscope.","However, it has rarely been applied to solid-state transistors, an advancement that would be highly desirable for enabling various applications.","Here, we demonstrate realization of the electrically controlled Zeeman effect in Dy@C84 single-molecule transistors, thus revealing a transition in the magnetic moment from 3.8 {\\mu}B to 5.1 {\\mu}B for the ground-state GN at an electric field strength of 3-10 MV/cm.","The consequent magnetoresistance significantly increases from 600% to 1100% at the resonant tunneling point.","Density functional theory calculations further corroborate our realization of nonvolatile switching of single-atom magnetism, and the switching stability emanates from an energy barrier of 92 meV for atomic relaxation.","These results highlight the potential of using endohedral metallofullerenes for high-temperature, high-stability, high-speed, and compact single-atom magnetic data storage."],"url":"http://arxiv.org/abs/2403.11137v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-17 08:33:57","title":"Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation","abstract":"Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec.","sentences":["Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data.","Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS.","In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar.","Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention.","This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation?","To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings.","Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead.","We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques.","Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec."],"url":"http://arxiv.org/abs/2403.11136v1","category":"cs.IR"}
{"created":"2024-03-17 07:04:09","title":"A Versatile Framework for Multi-scene Person Re-identification","abstract":"Person Re-identification (ReID) has been extensively developed for a decade in order to learn the association of images of the same person across non-overlapping camera views. To overcome significant variations between images across camera views, mountains of variants of ReID models were developed for solving a number of challenges, such as resolution change, clothing change, occlusion, modality change, and so on. Despite the impressive performance of many ReID variants, these variants typically function distinctly and cannot be applied to other challenges. To our best knowledge, there is no versatile ReID model that can handle various ReID challenges at the same time. This work contributes to the first attempt at learning a versatile ReID model to solve such a problem. Our main idea is to form a two-stage prompt-based twin modeling framework called VersReID. Our VersReID firstly leverages the scene label to train a ReID Bank that contains abundant knowledge for handling various scenes, where several groups of scene-specific prompts are used to encode different scene-specific knowledge. In the second stage, we distill a V-Branch model with versatile prompts from the ReID Bank for adaptively solving the ReID of different scenes, eliminating the demand for scene labels during the inference stage. To facilitate training VersReID, we further introduce the multi-scene properties into self-supervised learning of ReID via a multi-scene prioris data augmentation (MPDA) strategy. Through extensive experiments, we demonstrate the success of learning an effective and versatile ReID model for handling ReID tasks under multi-scene conditions without manual assignment of scene labels in the inference stage, including general, low-resolution, clothing change, occlusion, and cross-modality scenes. Codes and models are available at https://github.com/iSEE-Laboratory/VersReID.","sentences":["Person Re-identification (ReID) has been extensively developed for a decade in order to learn the association of images of the same person across non-overlapping camera views.","To overcome significant variations between images across camera views, mountains of variants of ReID models were developed for solving a number of challenges, such as resolution change, clothing change, occlusion, modality change, and so on.","Despite the impressive performance of many ReID variants, these variants typically function distinctly and cannot be applied to other challenges.","To our best knowledge, there is no versatile ReID model that can handle various ReID challenges at the same time.","This work contributes to the first attempt at learning a versatile ReID model to solve such a problem.","Our main idea is to form a two-stage prompt-based twin modeling framework called VersReID.","Our VersReID firstly leverages the scene label to train a ReID Bank that contains abundant knowledge for handling various scenes, where several groups of scene-specific prompts are used to encode different scene-specific knowledge.","In the second stage, we distill a V-Branch model with versatile prompts from the ReID Bank for adaptively solving the ReID of different scenes, eliminating the demand for scene labels during the inference stage.","To facilitate training VersReID, we further introduce the multi-scene properties into self-supervised learning of ReID via a multi-scene prioris data augmentation (MPDA) strategy.","Through extensive experiments, we demonstrate the success of learning an effective and versatile ReID model for handling ReID tasks under multi-scene conditions without manual assignment of scene labels in the inference stage, including general, low-resolution, clothing change, occlusion, and cross-modality scenes.","Codes and models are available at https://github.com/iSEE-Laboratory/VersReID."],"url":"http://arxiv.org/abs/2403.11121v1","category":"cs.CV"}
{"created":"2024-03-17 07:02:22","title":"Electrical reversal of the sign for magnon thermal Hall coefficient in van der Waals bilayer antiferromagnet","abstract":"With spin-layer locking, the manipulation of spin degree of freedom via perpendicular electric field can be realized in a typical antiferromagnetically coupled bilayer. In analogy to the electric control of the anomalous layer Hall effect of electron within such bilayer system, we propose here its magnon counterpart i.e., thermal Hall effect controlled by a perpendicular electric field. Unlike electrons, magnon is charged neutral and its transport in solids can be driven by a thermal gradient. It also exhibits Hall response due to the intrinsic Berry curvature of magnon, analogous to the achievement in electron system. Taking bilayer 2H-VSe2 with both H-type stacking interlayer antiferromagnetic coupling as a platform, we perform first-principles calculations towards the magnetic exchange coupling parameters under applied electric field perpendicular to the plane. Based on linear spin wave approximation, we then fit the magnon band structures accordingly and calculate the corresponding Berry curvature. The thermal Hall coefficient dependence on the temperature under thermal gradient can be calculated correspondingly in linear response regime. It is shown that electric field reversal is able to reverse the sign of the coefficient. These findings provide a platform for the realization of all-electric magnon spintronics.","sentences":["With spin-layer locking, the manipulation of spin degree of freedom via perpendicular electric field can be realized in a typical antiferromagnetically coupled bilayer.","In analogy to the electric control of the anomalous layer Hall effect of electron within such bilayer system, we propose here its magnon counterpart i.e., thermal Hall effect controlled by a perpendicular electric field.","Unlike electrons, magnon is charged neutral and its transport in solids can be driven by a thermal gradient.","It also exhibits Hall response due to the intrinsic Berry curvature of magnon, analogous to the achievement in electron system.","Taking bilayer 2H-VSe2 with both H-type stacking interlayer antiferromagnetic coupling as a platform, we perform first-principles calculations towards the magnetic exchange coupling parameters under applied electric field perpendicular to the plane.","Based on linear spin wave approximation, we then fit the magnon band structures accordingly and calculate the corresponding Berry curvature.","The thermal Hall coefficient dependence on the temperature under thermal gradient can be calculated correspondingly in linear response regime.","It is shown that electric field reversal is able to reverse the sign of the coefficient.","These findings provide a platform for the realization of all-electric magnon spintronics."],"url":"http://arxiv.org/abs/2403.11119v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-17 06:40:50","title":"Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis","abstract":"Rotation invariance is an important requirement for point shape analysis. To achieve this, current state-of-the-art methods attempt to construct the local rotation-invariant representation through learning or defining the local reference frame (LRF). Although efficient, these LRF-based methods suffer from perturbation of local geometric relations, resulting in suboptimal local rotation invariance. To alleviate this issue, we propose a Local-consistent Transformation (LocoTrans) learning strategy. Specifically, we first construct the local-consistent reference frame (LCRF) by considering the symmetry of the two axes in LRF. In comparison with previous LRFs, our LCRF is able to preserve local geometric relationships better through performing local-consistent transformation. However, as the consistency only exists in local regions, the relative pose information is still lost in the intermediate layers of the network. We mitigate such a relative pose issue by developing a relative pose recovery (RPR) module. RPR aims to restore the relative pose between adjacent transformed patches. Equipped with LCRF and RPR, our LocoTrans is capable of learning local-consistent transformation and preserving local geometry, which benefits rotation invariance learning. Competitive performance under arbitrary rotations on both shape classification and part segmentation tasks and ablations can demonstrate the effectiveness of our method. Code will be available publicly at https://github.com/wdttt/LocoTrans.","sentences":["Rotation invariance is an important requirement for point shape analysis.","To achieve this, current state-of-the-art methods attempt to construct the local rotation-invariant representation through learning or defining the local reference frame (LRF).","Although efficient, these LRF-based methods suffer from perturbation of local geometric relations, resulting in suboptimal local rotation invariance.","To alleviate this issue, we propose a Local-consistent Transformation (LocoTrans) learning strategy.","Specifically, we first construct the local-consistent reference frame (LCRF) by considering the symmetry of the two axes in LRF.","In comparison with previous LRFs, our LCRF is able to preserve local geometric relationships better through performing local-consistent transformation.","However, as the consistency only exists in local regions, the relative pose information is still lost in the intermediate layers of the network.","We mitigate such a relative pose issue by developing a relative pose recovery (RPR) module.","RPR aims to restore the relative pose between adjacent transformed patches.","Equipped with LCRF and RPR, our LocoTrans is capable of learning local-consistent transformation and preserving local geometry, which benefits rotation invariance learning.","Competitive performance under arbitrary rotations on both shape classification and part segmentation tasks and ablations can demonstrate the effectiveness of our method.","Code will be available publicly at https://github.com/wdttt/LocoTrans."],"url":"http://arxiv.org/abs/2403.11113v1","category":"cs.CV"}
{"created":"2024-03-17 06:19:30","title":"Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models","abstract":"Text-driven diffusion models have significantly advanced the image editing performance by using text prompts as inputs. One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source prompt. While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source prompt, limiting the image editability by target text prompts. To address this issue, we propose a novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models. To make the inverted noise code be independent of the given source prompt as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint. Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained diffusion models to facilitate the searching process. The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing prompt and the source prompt, leading to a significant decrease in editing artifacts. In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance. The source code are available at https://github.com/leeruibin/SPDInv.","sentences":["Text-driven diffusion models have significantly advanced the image editing performance by using text prompts as inputs.","One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source prompt.","While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source prompt, limiting the image editability by target text prompts.","To address this issue, we propose a novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models.","To make the inverted noise code be independent of the given source prompt as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint.","Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained diffusion models to facilitate the searching process.","The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing prompt and the source prompt, leading to a significant decrease in editing artifacts.","In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance.","The source code are available at https://github.com/leeruibin/SPDInv."],"url":"http://arxiv.org/abs/2403.11105v1","category":"cs.CV"}
{"created":"2024-03-17 06:12:43","title":"ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models","abstract":"Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives.","sentences":["Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER).","This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets.","Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data.","Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures.","Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives."],"url":"http://arxiv.org/abs/2403.11103v1","category":"cs.CL"}
{"created":"2024-03-17 06:07:23","title":"Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing","abstract":"Ridesharing services, such as Uber or Didi, have attracted considerable attention in recent years due to their positive impact on environmental protection and the economy. Existing studies require quick responses to orders, which lack the flexibility to accommodate longer wait times for better grouping opportunities. In this paper, we address a NP-hard ridesharing problem, called Minimal Extra Time RideSharing (METRS), which balances waiting time and group quality (i.e., detour time) to improve riders' satisfaction. To tackle this problem, we propose a novel approach called WATTER (WAit To be fasTER), which leverages an order pooling management algorithm allowing orders to wait until they can be matched with suitable groups. The key challenge is to customize the extra time threshold for each order by reducing the original optimization objective into a convex function of threshold, thus offering a theoretical guarantee to be optimized efficiently. We model the dispatch process using a Markov Decision Process (MDP) with a carefully designed value function to learn the threshold. Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed approaches.","sentences":["Ridesharing services, such as Uber or Didi, have attracted considerable attention in recent years due to their positive impact on environmental protection and the economy.","Existing studies require quick responses to orders, which lack the flexibility to accommodate longer wait times for better grouping opportunities.","In this paper, we address a NP-hard ridesharing problem, called Minimal Extra Time RideSharing (METRS), which balances waiting time and group quality (i.e., detour time) to improve riders' satisfaction.","To tackle this problem, we propose a novel approach called WATTER (WAit To be fasTER), which leverages an order pooling management algorithm allowing orders to wait until they can be matched with suitable groups.","The key challenge is to customize the extra time threshold for each order by reducing the original optimization objective into a convex function of threshold, thus offering a theoretical guarantee to be optimized efficiently.","We model the dispatch process using a Markov Decision Process (MDP) with a carefully designed value function to learn the threshold.","Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed approaches."],"url":"http://arxiv.org/abs/2403.11099v1","category":"cs.DB"}
{"created":"2024-03-17 05:23:43","title":"PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for Aerial Wildfire Monitoring in Partially Observable Environments","abstract":"Motivated by agility, 3D mobility, and low-risk operation compared to human-operated management systems of autonomous unmanned aerial vehicles (UAVs), this work studies UAV-based active wildfire monitoring where a UAV detects fire incidents in remote areas and tracks the fire frontline. A UAV path planning solution is proposed considering realistic wildfire management missions, where a single low-altitude drone with limited power and flight time is available. Noting the limited field of view of commercial low-altitude UAVs, the problem formulates as a partially observable Markov decision process (POMDP), in which wildfire progression outside the field of view causes inaccurate state representation that prevents the UAV from finding the optimal path to track the fire front in limited time. Common deep reinforcement learning (DRL)-based trajectory planning solutions require diverse drone-recorded wildfire data to generalize pre-trained models to real-time systems, which is not currently available at a diverse and standard scale. To narrow down the gap caused by partial observability in the space of possible policies, a belief-based state representation with broad, extensive simulated data is proposed where the beliefs (i.e., ignition probabilities of different grid areas) are updated using a Bayesian framework for the cells within the field of view. The performance of the proposed solution in terms of the ratio of detected fire cells and monitored ignited area (MIA) is evaluated in a complex fire scenario with multiple rapidly growing fire batches, indicating that the belief state representation outperforms the observation state representation both in fire coverage and the distance to fire frontline.","sentences":["Motivated by agility, 3D mobility, and low-risk operation compared to human-operated management systems of autonomous unmanned aerial vehicles (UAVs), this work studies UAV-based active wildfire monitoring where a UAV detects fire incidents in remote areas and tracks the fire frontline.","A UAV path planning solution is proposed considering realistic wildfire management missions, where a single low-altitude drone with limited power and flight time is available.","Noting the limited field of view of commercial low-altitude UAVs, the problem formulates as a partially observable Markov decision process (POMDP), in which wildfire progression outside the field of view causes inaccurate state representation that prevents the UAV from finding the optimal path to track the fire front in limited time.","Common deep reinforcement learning (DRL)-based trajectory planning solutions require diverse drone-recorded wildfire data to generalize pre-trained models to real-time systems, which is not currently available at a diverse and standard scale.","To narrow down the gap caused by partial observability in the space of possible policies, a belief-based state representation with broad, extensive simulated data is proposed where the beliefs (i.e., ignition probabilities of different grid areas) are updated using a Bayesian framework for the cells within the field of view.","The performance of the proposed solution in terms of the ratio of detected fire cells and monitored ignited area (MIA) is evaluated in a complex fire scenario with multiple rapidly growing fire batches, indicating that the belief state representation outperforms the observation state representation both in fire coverage and the distance to fire frontline."],"url":"http://arxiv.org/abs/2403.11095v1","category":"cs.RO"}
{"created":"2024-03-17 05:07:04","title":"Learning-Based Pricing and Matching for Two-Sided Queues","abstract":"We consider a dynamic system with multiple types of customers and servers. Each type of waiting customer or server joins a separate queue, forming a bipartite graph with customer-side queues and server-side queues. The platform can match the servers and customers if their types are compatible. The matched pairs then leave the system. The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type. The arrival rate of each queue is determined by the price according to some unknown demand or supply functions. Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold. This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers. The difficulties of the problem include simultaneous learning and decision making, and the tradeoff between maximizing profit and minimizing queue length. We use a longest-queue-first matching algorithm and propose a learning-based pricing algorithm, which combines gradient-free stochastic projected gradient ascent with bisection search. We prove that our proposed algorithm yields a sublinear regret $\\tilde{O}(T^{5/6})$ and queue-length bound $\\tilde{O}(T^{2/3})$, where $T$ is the time horizon. We further establish a tradeoff between the regret bound and the queue-length bound: $\\tilde{O}(T^{1-\\gamma/4})$ versus $\\tilde{O}(T^{\\gamma})$ for $\\gamma \\in (0, 2/3].$","sentences":["We consider a dynamic system with multiple types of customers and servers.","Each type of waiting customer or server joins a separate queue, forming a bipartite graph with customer-side queues and server-side queues.","The platform can match the servers and customers if their types are compatible.","The matched pairs then leave the system.","The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type.","The arrival rate of each queue is determined by the price according to some unknown demand or supply functions.","Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold.","This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers.","The difficulties of the problem include simultaneous learning and decision making, and the tradeoff between maximizing profit and minimizing queue length.","We use a longest-queue-first matching algorithm and propose a learning-based pricing algorithm, which combines gradient-free stochastic projected gradient ascent with bisection search.","We prove that our proposed algorithm yields a sublinear regret $\\tilde{O}(T^{5/6})$ and queue-length bound $\\tilde{O}(T^{2/3})$, where $T$ is the time horizon.","We further establish a tradeoff between the regret bound and the queue-length bound: $\\tilde{O}(T^{1-\\gamma/4})$ versus $\\tilde{O}(T^{\\gamma})$ for $\\gamma \\in (0, 2/3].$"],"url":"http://arxiv.org/abs/2403.11093v1","category":"math.OC"}
{"created":"2024-03-17 04:55:36","title":"Stress relaxation and thermo-visco-elastic effects in fluid-filled slits and fluid-loaded plates","abstract":"In this paper, we theoretically analyse wave propagation in two canonical problems of interest: fluid-filled thermo-visco-elastic slits and fluid-loaded thermo-visco-elastic plates. We show that these two configurations can be studied via the same pair of dispersion equations with the aid of the framework developed in [https://doi.org/10.1098/rspa.2022.0193], which incoporates thermal effects. These two problems are further interrelated, since in the short wavelength limit (relative to the slit/plate width) the respective modes are governed by the same dispersion equation, commonly known as the Scholte--Stoneley equation. It is the Scholte-type modes that are mainly analyzed in this paper. We illustrate results when the fluid is water, although the theory is valid for any Newtonian fluid. Both `hard' and `soft' solids are compared, with the emphasis being placed on the importance of thermo-viscoelastic effects, particularly when stress relaxation is considered. Two main recent works are discussed extensively, namely [https://doi.org/10.1121/1.5078528] for slits and [https://doi.org/10.1103/PhysRevE.103.063002] for loaded plates, both of which do not incorporate viscoelastic mechanisms. We show how the consideration of viscoelasticity can extend the results discussed therein, and explain the circumstances under which they arise.","sentences":["In this paper, we theoretically analyse wave propagation in two canonical problems of interest: fluid-filled thermo-visco-elastic slits and fluid-loaded thermo-visco-elastic plates.","We show that these two configurations can be studied via the same pair of dispersion equations with the aid of the framework developed in [https://doi.org/10.1098/rspa.2022.0193], which incoporates thermal effects.","These two problems are further interrelated, since in the short wavelength limit (relative to the slit/plate width)","the respective modes are governed by the same dispersion equation, commonly known as the Scholte--Stoneley equation.","It is the Scholte-type modes that are mainly analyzed in this paper.","We illustrate results when the fluid is water, although the theory is valid for any Newtonian fluid.","Both `hard' and `soft' solids are compared, with the emphasis being placed on the importance of thermo-viscoelastic effects, particularly when stress relaxation is considered.","Two main recent works are discussed extensively, namely [https://doi.org/10.1121/1.5078528] for slits and [https://doi.org/10.1103/PhysRevE.103.063002] for loaded plates, both of which do not incorporate viscoelastic mechanisms.","We show how the consideration of viscoelasticity can extend the results discussed therein, and explain the circumstances under which they arise."],"url":"http://arxiv.org/abs/2403.11089v1","category":"physics.flu-dyn"}
{"created":"2024-03-17 04:42:41","title":"Incorporating Higher-order Structural Information for Graph Clustering","abstract":"Clustering holds profound significance in data mining. In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes. However, most existing methods ignore the higher-order structural information of the graph. Evidently, nodes within the same cluster can establish distant connections. Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure. In this paper, we propose a novel graph clustering network to make full use of graph structural information. To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includes modularity as a structural constraint. Our proposed model outperforms many state-of-the-art methods on various datasets, demonstrating its superiority.","sentences":["Clustering holds profound significance in data mining.","In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes.","However, most existing methods ignore the higher-order structural information of the graph.","Evidently, nodes within the same cluster can establish distant connections.","Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure.","In this paper, we propose a novel graph clustering network to make full use of graph structural information.","To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includes modularity as a structural constraint.","Our proposed model outperforms many state-of-the-art methods on various datasets, demonstrating its superiority."],"url":"http://arxiv.org/abs/2403.11087v1","category":"cs.LG"}
{"created":"2024-03-17 04:36:18","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","abstract":"Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realistic toolset. We further provide a high-quality subset of 1,565 task plans that are human-verified and correctly executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies (multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3 types of feedback (parsing/verification/execution). Finally, we summarize takeaways from our extensive experiments. Our dataset and code are available on HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github (https://github.com/RAIVNLab/mnms).","sentences":["Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models.","Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans.","However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions.","Should LLMs generate a full plan in a single shot or step-by-step?","Should they invoke tools directly with Python code or through structured data formats like JSON?","Does feedback improve planning?","To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules.","For each of these task queries, we provide automatically generated plans using this realistic toolset.","We further provide a high-quality subset of 1,565 task plans that are human-verified and correctly executable.","With m&m's, we evaluate 6 popular LLMs with 2 planning strategies (multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3 types of feedback (parsing/verification/execution).","Finally, we summarize takeaways from our extensive experiments.","Our dataset and code are available on HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github (https://github.com/RAIVNLab/mnms)."],"url":"http://arxiv.org/abs/2403.11085v1","category":"cs.CV"}
{"created":"2024-03-17 04:20:38","title":"Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time Defect Prediction","abstract":"Just-In-Time (JIT) defect prediction aims to automatically predict whether a commit is defective or not, and has been widely studied in recent years. In general, most studies can be classified into two categories: 1) simple models using traditional machine learning classifiers with hand-crafted features, and 2) complex models using deep learning techniques to automatically extract features from commit contents. Hand-crafted features used by simple models are based on expert knowledge but may not fully represent the semantic meaning of the commits. On the other hand, deep learning-based features used by complex models represent the semantic meaning of commits but may not reflect useful expert knowledge. Simple models and complex models seem complementary to each other to some extent. To utilize the advantages of both simple and complex models, we propose a model fusion framework that adopts both early fusions on the feature level and late fusions on the decision level. We propose SimCom++ by adopting the best early and late fusion strategies. The experimental results show that SimCom++ can significantly outperform the baselines by 5.7--26.9\\%. In addition, our experimental results confirm that the simple model and complex model are complementary to each other.","sentences":["Just-In-Time (JIT) defect prediction aims to automatically predict whether a commit is defective or not, and has been widely studied in recent years.","In general, most studies can be classified into two categories: 1) simple models using traditional machine learning classifiers with hand-crafted features, and 2) complex models using deep learning techniques to automatically extract features from commit contents.","Hand-crafted features used by simple models are based on expert knowledge but may not fully represent the semantic meaning of the commits.","On the other hand, deep learning-based features used by complex models represent the semantic meaning of commits but may not reflect useful expert knowledge.","Simple models and complex models seem complementary to each other to some extent.","To utilize the advantages of both simple and complex models, we propose a model fusion framework that adopts both early fusions on the feature level and late fusions on the decision level.","We propose SimCom++ by adopting the best early and late fusion strategies.","The experimental results show that SimCom++ can significantly outperform the baselines by 5.7--26.9\\%.","In addition, our experimental results confirm that the simple model and complex model are complementary to each other."],"url":"http://arxiv.org/abs/2403.11079v1","category":"cs.SE"}
{"created":"2024-03-17 04:02:39","title":"Zippo: Zipping Color and Transparency Distributions into a Single Diffusion Model","abstract":"Beyond the superiority of the text-to-image diffusion model in generating high-quality images, recent studies have attempted to uncover its potential for adapting the learned semantic knowledge to visual perception tasks. In this work, instead of translating a generative diffusion model into a visual perception model, we explore to retain the generative ability with the perceptive adaptation. To accomplish this, we present Zippo, a unified framework for zipping the color and transparency distributions into a single diffusion model by expanding the diffusion latent into a joint representation of RGB images and alpha mattes. By alternatively selecting one modality as the condition and then applying the diffusion process to the counterpart modality, Zippo is capable of generating RGB images from alpha mattes and predicting transparency from input images. In addition to single-modality prediction, we propose a modality-aware noise reassignment strategy to further empower Zippo with jointly generating RGB images and its corresponding alpha mattes under the text guidance. Our experiments showcase Zippo's ability of efficient text-conditioned transparent image generation and present plausible results of Matte-to-RGB and RGB-to-Matte translation.","sentences":["Beyond the superiority of the text-to-image diffusion model in generating high-quality images, recent studies have attempted to uncover its potential for adapting the learned semantic knowledge to visual perception tasks.","In this work, instead of translating a generative diffusion model into a visual perception model, we explore to retain the generative ability with the perceptive adaptation.","To accomplish this, we present Zippo, a unified framework for zipping the color and transparency distributions into a single diffusion model by expanding the diffusion latent into a joint representation of RGB images and alpha mattes.","By alternatively selecting one modality as the condition and then applying the diffusion process to the counterpart modality, Zippo is capable of generating RGB images from alpha mattes and predicting transparency from input images.","In addition to single-modality prediction, we propose a modality-aware noise reassignment strategy to further empower Zippo with jointly generating RGB images and its corresponding alpha mattes under the text guidance.","Our experiments showcase Zippo's ability of efficient text-conditioned transparent image generation and present plausible results of Matte-to-RGB and RGB-to-Matte translation."],"url":"http://arxiv.org/abs/2403.11077v1","category":"cs.CV"}
{"created":"2024-03-17 03:16:59","title":"Controllable Relation Disentanglement for Few-Shot Class-Incremental Learning","abstract":"In this paper, we propose to tackle Few-Shot Class-Incremental Learning (FSCIL) from a new perspective, i.e., relation disentanglement, which means enhancing FSCIL via disentangling spurious relation between categories. The challenge of disentangling spurious correlations lies in the poor controllability of FSCIL. On one hand, an FSCIL model is required to be trained in an incremental manner and thus it is very hard to directly control relationships between categories of different sessions. On the other hand, training samples per novel category are only in the few-shot setting, which increases the difficulty of alleviating spurious relation issues as well. To overcome this challenge, in this paper, we propose a new simple-yet-effective method, called ConTrollable Relation-disentangLed Few-Shot Class-Incremental Learning (CTRL-FSCIL). Specifically, during the base session, we propose to anchor base category embeddings in feature space and construct disentanglement proxies to bridge gaps between the learning for category representations in different sessions, thereby making category relation controllable. During incremental learning, the parameters of the backbone network are frozen in order to relieve the negative impact of data scarcity. Moreover, a disentanglement loss is designed to effectively guide a relation disentanglement controller to disentangle spurious correlations between the embeddings encoded by the backbone. In this way, the spurious correlation issue in FSCIL can be suppressed. Extensive experiments on CIFAR-100, mini-ImageNet, and CUB-200 datasets demonstrate the effectiveness of our CTRL-FSCIL method.","sentences":["In this paper, we propose to tackle Few-Shot Class-Incremental Learning (FSCIL) from a new perspective, i.e., relation disentanglement, which means enhancing FSCIL via disentangling spurious relation between categories.","The challenge of disentangling spurious correlations lies in the poor controllability of FSCIL.","On one hand, an FSCIL model is required to be trained in an incremental manner and thus it is very hard to directly control relationships between categories of different sessions.","On the other hand, training samples per novel category are only in the few-shot setting, which increases the difficulty of alleviating spurious relation issues as well.","To overcome this challenge, in this paper, we propose a new simple-yet-effective method, called ConTrollable Relation-disentangLed Few-Shot Class-Incremental Learning (CTRL-FSCIL).","Specifically, during the base session, we propose to anchor base category embeddings in feature space and construct disentanglement proxies to bridge gaps between the learning for category representations in different sessions, thereby making category relation controllable.","During incremental learning, the parameters of the backbone network are frozen in order to relieve the negative impact of data scarcity.","Moreover, a disentanglement loss is designed to effectively guide a relation disentanglement controller to disentangle spurious correlations between the embeddings encoded by the backbone.","In this way, the spurious correlation issue in FSCIL can be suppressed.","Extensive experiments on CIFAR-100, mini-ImageNet, and CUB-200 datasets demonstrate the effectiveness of our CTRL-FSCIL method."],"url":"http://arxiv.org/abs/2403.11070v1","category":"cs.CV"}
{"created":"2024-03-17 02:06:03","title":"Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration","abstract":"The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering. However, 3DGS is not alias-free, and its rendering at varying resolutions could produce severe blurring or jaggies. This is because 3DGS treats each pixel as an isolated, single point rather than as an area, causing insensitivity to changes in the footprints of pixels. Consequently, this discrete sampling scheme inevitably results in aliasing, owing to the restricted sampling bandwidth. In this paper, we derive an analytical solution to address this issue. More specifically, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral by subtracting the CDFs. We then introduce this approximation in the two-dimensional pixel shading, and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel. Moreover, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions. Experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity.","sentences":["The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering.","However, 3DGS is not alias-free, and its rendering at varying resolutions could produce severe blurring or jaggies.","This is because 3DGS treats each pixel as an isolated, single point rather than as an area, causing insensitivity to changes in the footprints of pixels.","Consequently, this discrete sampling scheme inevitably results in aliasing, owing to the restricted sampling bandwidth.","In this paper, we derive an analytical solution to address this issue.","More specifically, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral by subtracting the CDFs.","We then introduce this approximation in the two-dimensional pixel shading, and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel.","Moreover, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions.","Experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity."],"url":"http://arxiv.org/abs/2403.11056v1","category":"cs.CV"}
{"created":"2024-03-17 00:29:42","title":"JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks","abstract":"Despite the success of Quantum Neural Networks (QNNs) in decision-making systems, their fairness remains unexplored, as the focus primarily lies on accuracy. This work conducts a design space exploration, unveiling QNN unfairness, and highlighting the significant influence of QNN deployment and quantum noise on accuracy and fairness. To effectively navigate the vast QNN deployment design space, we propose JustQ, a framework for deploying fair and accurate QNNs on NISQ computers. It includes a complete NISQ error model, reinforcement learning-based deployment, and a flexible optimization objective incorporating both fairness and accuracy. Experimental results show JustQ outperforms previous methods, achieving superior accuracy and fairness. This work pioneers fair QNN design on NISQ computers, paving the way for future investigations.","sentences":["Despite the success of Quantum Neural Networks (QNNs) in decision-making systems, their fairness remains unexplored, as the focus primarily lies on accuracy.","This work conducts a design space exploration, unveiling QNN unfairness, and highlighting the significant influence of QNN deployment and quantum noise on accuracy and fairness.","To effectively navigate the vast QNN deployment design space, we propose JustQ, a framework for deploying fair and accurate QNNs on NISQ computers.","It includes a complete NISQ error model, reinforcement learning-based deployment, and a flexible optimization objective incorporating both fairness and accuracy.","Experimental results show JustQ outperforms previous methods, achieving superior accuracy and fairness.","This work pioneers fair QNN design on NISQ computers, paving the way for future investigations."],"url":"http://arxiv.org/abs/2403.11048v1","category":"quant-ph"}
{"created":"2024-03-16 23:52:25","title":"Advancing multivariate time series similarity assessment: an integrated computational approach","abstract":"Data mining, particularly the analysis of multivariate time series data, plays a crucial role in extracting insights from complex systems and supporting informed decision-making across diverse domains. However, assessing the similarity of multivariate time series data presents several challenges, including dealing with large datasets, addressing temporal misalignments, and the need for efficient and comprehensive analytical frameworks. To address all these challenges, we propose a novel integrated computational approach known as Multivariate Time series Alignment and Similarity Assessment (MTASA). MTASA is built upon a hybrid methodology designed to optimize time series alignment, complemented by a multiprocessing engine that enhances the utilization of computational resources. This integrated approach comprises four key components, each addressing essential aspects of time series similarity assessment, thereby offering a comprehensive framework for analysis. MTASA is implemented as an open-source Python library with a user-friendly interface, making it accessible to researchers and practitioners. To evaluate the effectiveness of MTASA, we conducted an empirical study focused on assessing agroecosystem similarity using real-world environmental data. The results from this study highlight MTASA's superiority, achieving approximately 1.5 times greater accuracy and twice the speed compared to existing state-of-the-art integrated frameworks for multivariate time series similarity assessment. It is hoped that MTASA will significantly enhance the efficiency and accessibility of multivariate time series analysis, benefitting researchers and practitioners across various domains. Its capabilities in handling large datasets, addressing temporal misalignments, and delivering accurate results make MTASA a valuable tool for deriving insights and aiding decision-making processes in complex systems.","sentences":["Data mining, particularly the analysis of multivariate time series data, plays a crucial role in extracting insights from complex systems and supporting informed decision-making across diverse domains.","However, assessing the similarity of multivariate time series data presents several challenges, including dealing with large datasets, addressing temporal misalignments, and the need for efficient and comprehensive analytical frameworks.","To address all these challenges, we propose a novel integrated computational approach known as Multivariate Time series Alignment and Similarity Assessment (MTASA).","MTASA is built upon a hybrid methodology designed to optimize time series alignment, complemented by a multiprocessing engine that enhances the utilization of computational resources.","This integrated approach comprises four key components, each addressing essential aspects of time series similarity assessment, thereby offering a comprehensive framework for analysis.","MTASA is implemented as an open-source Python library with a user-friendly interface, making it accessible to researchers and practitioners.","To evaluate the effectiveness of MTASA, we conducted an empirical study focused on assessing agroecosystem similarity using real-world environmental data.","The results from this study highlight MTASA's superiority, achieving approximately 1.5 times greater accuracy and twice the speed compared to existing state-of-the-art integrated frameworks for multivariate time series similarity assessment.","It is hoped that MTASA will significantly enhance the efficiency and accessibility of multivariate time series analysis, benefitting researchers and practitioners across various domains.","Its capabilities in handling large datasets, addressing temporal misalignments, and delivering accurate results make MTASA a valuable tool for deriving insights and aiding decision-making processes in complex systems."],"url":"http://arxiv.org/abs/2403.11044v1","category":"cs.LG"}
{"created":"2024-03-16 23:45:21","title":"Stochastic Lp string stability analysis in predecessor-following platoons under packet losses","abstract":"In this paper, we study (homogeneous) predecessor-following platoons in which the vehicle-to-vehicle (V2V) communications are affected by random packet losses. We model the overall platoon as a stochastic hybrid system and analyse its string stability via a small-gain approach. For nonlinear platoons, we illustrate how the different elements of the platoon have an impact on string stability, such as platoon topology and vehicle scheduling. For linear time-invariant platoons, we provide an explicit string stability condition that illustrates the interplay between the channel success probability, transmission rate, and time headway constant. Lastly, we illustrate our results by numerical simulations.","sentences":["In this paper, we study (homogeneous) predecessor-following platoons in which the vehicle-to-vehicle (V2V) communications are affected by random packet losses.","We model the overall platoon as a stochastic hybrid system and analyse its string stability via a small-gain approach.","For nonlinear platoons, we illustrate how the different elements of the platoon have an impact on string stability, such as platoon topology and vehicle scheduling.","For linear time-invariant platoons, we provide an explicit string stability condition that illustrates the interplay between the channel success probability, transmission rate, and time headway constant.","Lastly, we illustrate our results by numerical simulations."],"url":"http://arxiv.org/abs/2403.11043v1","category":"eess.SY"}
{"created":"2024-03-16 22:50:16","title":"A Spectrum-based Image Denoising Method with Edge Feature Enhancement","abstract":"Image denoising stands as a critical challenge in image processing and computer vision, aiming to restore the original image from noise-affected versions caused by various intrinsic and extrinsic factors. This process is essential for applications that rely on the high quality and clarity of visual information, such as image restoration, visual tracking, and image registration, where the original content is vital for performance. Despite the development of numerous denoising algorithms, effectively suppressing noise, particularly under poor capture conditions with high noise levels, remains a challenge. Image denoising's practical importance spans multiple domains, notably medical imaging for enhanced diagnostic precision, as well as surveillance and satellite imagery where it improves image quality and usability. Techniques like the Fourier transform, which excels in noise reduction and edge preservation, along with phase congruency-based methods, offer promising results for enhancing noisy and low-contrast images common in modern imaging scenarios.","sentences":["Image denoising stands as a critical challenge in image processing and computer vision, aiming to restore the original image from noise-affected versions caused by various intrinsic and extrinsic factors.","This process is essential for applications that rely on the high quality and clarity of visual information, such as image restoration, visual tracking, and image registration, where the original content is vital for performance.","Despite the development of numerous denoising algorithms, effectively suppressing noise, particularly under poor capture conditions with high noise levels, remains a challenge.","Image denoising's practical importance spans multiple domains, notably medical imaging for enhanced diagnostic precision, as well as surveillance and satellite imagery where it improves image quality and usability.","Techniques like the Fourier transform, which excels in noise reduction and edge preservation, along with phase congruency-based methods, offer promising results for enhancing noisy and low-contrast images common in modern imaging scenarios."],"url":"http://arxiv.org/abs/2403.11036v1","category":"eess.IV"}
{"created":"2024-03-16 22:46:12","title":"Resilient Fleet Management for Energy-Aware Intra-Factory Logistics","abstract":"This paper presents a novel fleet management strategy for battery-powered robot fleets tasked with intra-factory logistics in an autonomous manufacturing facility. In this environment, repetitive material handling operations are subject to real-world uncertainties such as blocked passages, and equipment or robot malfunctions. In such cases, centralized approaches enhance resilience by immediately adjusting the task allocation between the robots. To overcome the computational expense, a two-step methodology is proposed where the nominal problem is solved a priori using a Monte Carlo Tree Search algorithm for task allocation, resulting in a nominal search tree. When a disruption occurs, the nominal search tree is rapidly updated a posteriori with costs to the new problem while simultaneously generating feasible solutions. Computational experiments prove the real-time capability of the proposed algorithm for various scenarios and compare it with the case where the search tree is not used and the decentralized approach that does not attempt task reassignment.","sentences":["This paper presents a novel fleet management strategy for battery-powered robot fleets tasked with intra-factory logistics in an autonomous manufacturing facility.","In this environment, repetitive material handling operations are subject to real-world uncertainties such as blocked passages, and equipment or robot malfunctions.","In such cases, centralized approaches enhance resilience by immediately adjusting the task allocation between the robots.","To overcome the computational expense, a two-step methodology is proposed where the nominal problem is solved a priori using a Monte Carlo Tree Search algorithm for task allocation, resulting in a nominal search tree.","When a disruption occurs, the nominal search tree is rapidly updated a posteriori with costs to the new problem while simultaneously generating feasible solutions.","Computational experiments prove the real-time capability of the proposed algorithm for various scenarios and compare it with the case where the search tree is not used and the decentralized approach that does not attempt task reassignment."],"url":"http://arxiv.org/abs/2403.11034v1","category":"cs.RO"}
{"created":"2024-03-16 22:27:52","title":"A-upper motives","abstract":"Let $E/F$ be a finite field extension with every intermediate field being Galois over $F$. Given a reductive group $G$ over $F$ such that $G_E$ is of inner type, we define its A-upper motives. These motives are indecomposable and naturally related with the indecomposable summands in the motives of spectra of intermediate fields in $E/F$. We show that motives of projective homogeneous varieties under $G$ are isomorphic to direct sums of Tate shifts of A-upper motives. Based on that, we get a classification of the motives of projective homogeneous varieties under absolutely simple groups of type not $^6\\!D_4$, by means of their higher Artin-Tate traces. We also show how Tits indexes over suitable field extensions determine motivic equivalence classes for these algebraic groups.","sentences":["Let $E/F$ be a finite field extension with every intermediate field being Galois over $F$. Given a reductive group $G$ over $F$ such that $G_E$ is of inner type, we define its A-upper motives.","These motives are indecomposable and naturally related with the indecomposable summands in the motives of spectra of intermediate fields in $E/F$. We show that motives of projective homogeneous varieties under $G$ are isomorphic to direct sums of Tate shifts of A-upper motives.","Based on that, we get a classification of the motives of projective homogeneous varieties under absolutely simple groups of type not $^6\\!D_4$, by means of their higher Artin-Tate traces.","We also show how Tits indexes over suitable field extensions determine motivic equivalence classes for these algebraic groups."],"url":"http://arxiv.org/abs/2403.11030v1","category":"math.AG"}
{"created":"2024-03-16 22:19:08","title":"What Makes Systemic Discrimination, \"Systemic?\" Exposing the Amplifiers of Inequity","abstract":"Drawing on work spanning economics, public health, education, sociology, and law, I formalize theoretically what makes systemic discrimination \"systemic.\" Injustices do not occur in isolation, but within a complex system of interdependent factors; and their effects may amplify as a consequence. I develop a taxonomy of these amplification mechanisms, connecting them to well-understood concepts in economics that are precise, testable and policy-oriented. This framework reveals that these amplification mechanisms can either be directly disrupted, or exploited to amplify the effects of equity-focused interventions instead. In other words, it shows how to use the machinery of systemic discrimination against itself. Real-world examples discussed include but are not limited to reparations for slavery and Jim Crow, vouchers or place-based neighborhood interventions, police shootings, affirmative action, and Covid-19.","sentences":["Drawing on work spanning economics, public health, education, sociology, and law, I formalize theoretically what makes systemic discrimination \"systemic.\"","Injustices do not occur in isolation, but within a complex system of interdependent factors; and their effects may amplify as a consequence.","I develop a taxonomy of these amplification mechanisms, connecting them to well-understood concepts in economics that are precise, testable and policy-oriented.","This framework reveals that these amplification mechanisms can either be directly disrupted, or exploited to amplify the effects of equity-focused interventions instead.","In other words, it shows how to use the machinery of systemic discrimination against itself.","Real-world examples discussed include but are not limited to reparations for slavery and Jim Crow, vouchers or place-based neighborhood interventions, police shootings, affirmative action, and Covid-19."],"url":"http://arxiv.org/abs/2403.11028v1","category":"econ.GN"}
{"created":"2024-03-16 22:00:16","title":"Fast Sparse View Guided NeRF Update for Object Reconfigurations","abstract":"Neural Radiance Field (NeRF), as an implicit 3D scene representation, lacks inherent ability to accommodate changes made to the initial static scene. If objects are reconfigured, it is difficult to update the NeRF to reflect the new state of the scene without time-consuming data re-capturing and NeRF re-training. To address this limitation, we develop the first update method for NeRFs to physical changes. Our method takes only sparse new images (e.g. 4) of the altered scene as extra inputs and update the pre-trained NeRF in around 1 to 2 minutes. Particularly, we develop a pipeline to identify scene changes and update the NeRF accordingly. Our core idea is the use of a second helper NeRF to learn the local geometry and appearance changes, which sidesteps the optimization difficulties in direct NeRF fine-tuning. The interpolation power of the helper NeRF is the key to accurately reconstruct the un-occluded objects regions under sparse view supervision. Our method imposes no constraints on NeRF pre-training, and requires no extra user input or explicit semantic priors. It is an order of magnitude faster than re-training NeRF from scratch while maintaining on-par and even superior performance.","sentences":["Neural Radiance Field (NeRF), as an implicit 3D scene representation, lacks inherent ability to accommodate changes made to the initial static scene.","If objects are reconfigured, it is difficult to update the NeRF to reflect the new state of the scene without time-consuming data re-capturing and NeRF re-training.","To address this limitation, we develop the first update method for NeRFs to physical changes.","Our method takes only sparse new images (e.g. 4) of the altered scene as extra inputs and update the pre-trained NeRF in around 1 to 2 minutes.","Particularly, we develop a pipeline to identify scene changes and update the NeRF accordingly.","Our core idea is the use of a second helper NeRF to learn the local geometry and appearance changes, which sidesteps the optimization difficulties in direct NeRF fine-tuning.","The interpolation power of the helper NeRF is the key to accurately reconstruct the un-occluded objects regions under sparse view supervision.","Our method imposes no constraints on NeRF pre-training, and requires no extra user input or explicit semantic priors.","It is an order of magnitude faster than re-training NeRF from scratch while maintaining on-par and even superior performance."],"url":"http://arxiv.org/abs/2403.11024v1","category":"cs.CV"}
{"created":"2024-03-16 22:00:06","title":"On the boundary behavior of unclosed mappings with the inverse Poletsky inequality","abstract":"The manuscript is devoted to the boundary behavior of mappings with bounded and finite distortion, which has been actively studied recently. We consider mappings of domains of the Euclidean space that satisfy the inverse Poletsky inequality with an integrable majorant, are open, and discrete. Assume that the image of the boundary of the original domain is finitely connected relative to the mapped domain, and the preimage of the boundary of the latter is nowhere a dense set. Then, under certain conditions on the geometry of these domains, it is proved that the specified mappings have a continuous boundary extension. The result is valid even in a more general form, when the majorant in the inverse Poletsky inequality is integrable over almost all concentric spheres centered at each point. In particular, the obtained results are valid for homeomorphisms as well as for open discrete closed mappings with the appropriate modulus condition.","sentences":["The manuscript is devoted to the boundary behavior of mappings with bounded and finite distortion, which has been actively studied recently.","We consider mappings of domains of the Euclidean space that satisfy the inverse Poletsky inequality with an integrable majorant, are open, and discrete.","Assume that the image of the boundary of the original domain is finitely connected relative to the mapped domain, and the preimage of the boundary of the latter is nowhere a dense set.","Then, under certain conditions on the geometry of these domains, it is proved that the specified mappings have a continuous boundary extension.","The result is valid even in a more general form, when the majorant in the inverse Poletsky inequality is integrable over almost all concentric spheres centered at each point.","In particular, the obtained results are valid for homeomorphisms as well as for open discrete closed mappings with the appropriate modulus condition."],"url":"http://arxiv.org/abs/2403.11023v1","category":"math.CV"}
{"created":"2024-03-16 21:54:13","title":"Auctions with Dynamic Scoring","abstract":"We study the design of auctions with dynamic scoring, which allocate a single item according to a given scoring rule. We are motivated by online advertising auctions when users interact with a platform over the course of a session. The platform ranks ads based on a combination of bids and quality scores, and updates the quality scores throughout the session based on the user's online activity. The platform must decide when to show an ad during the session. By delaying the auction, the auctioneer acquires information about an ad's quality, improving her chances of selecting a high quality ad. However information is costly, because delay reduces market thickness and in turn revenue. When should the auctioneer allocate the impression to balance these forces?   We develop a theoretical model to study the effect of market design on the trade-off between market thickness and information. In particular, we focus on first- and second-price auctions. The auctioneer can commit to the auction format, but not to its timing: her decision can thus be cast as a real options problem. We show that under optimal stopping the first-price auction allocates efficiently but with delay. Instead, the second-price auction generates more revenue by avoiding delay. The auctioneer benefits from introducing reserve prices, more so in a first-price auction.","sentences":["We study the design of auctions with dynamic scoring, which allocate a single item according to a given scoring rule.","We are motivated by online advertising auctions when users interact with a platform over the course of a session.","The platform ranks ads based on a combination of bids and quality scores, and updates the quality scores throughout the session based on the user's online activity.","The platform must decide when to show an ad during the session.","By delaying the auction, the auctioneer acquires information about an ad's quality, improving her chances of selecting a high quality ad.","However information is costly, because delay reduces market thickness and in turn revenue.","When should the auctioneer allocate the impression to balance these forces?   ","We develop a theoretical model to study the effect of market design on the trade-off between market thickness and information.","In particular, we focus on first- and second-price auctions.","The auctioneer can commit to the auction format, but not to its timing: her decision can thus be cast as a real options problem.","We show that under optimal stopping the first-price auction allocates efficiently but with delay.","Instead, the second-price auction generates more revenue by avoiding delay.","The auctioneer benefits from introducing reserve prices, more so in a first-price auction."],"url":"http://arxiv.org/abs/2403.11022v1","category":"econ.TH"}
{"created":"2024-03-16 20:59:49","title":"Comprehensive OOS Evaluation of Predictive Algorithms with Statistical Decision Theory","abstract":"We argue that comprehensive out-of-sample (OOS) evaluation using statistical decision theory (SDT) should replace the current practice of K-fold and Common Task Framework validation in machine learning (ML) research. SDT provides a formal framework for performing comprehensive OOS evaluation across all possible (1) training samples, (2) populations that may generate training data, and (3) populations of prediction interest. Regarding feature (3), we emphasize that SDT requires the practitioner to directly confront the possibility that the future may not look like the past and to account for a possible need to extrapolate from one population to another when building a predictive algorithm. SDT is simple in abstraction, but it is often computationally demanding to implement. We discuss progress in tractable implementation of SDT when prediction accuracy is measured by mean square error or by misclassification rate. We summarize research studying settings in which the training data will be generated from a subpopulation of the population of prediction interest. We also consider conditional prediction with alternative restrictions on the state space of possible populations that may generate training data. We conclude by calling on ML researchers to join with econometricians and statisticians in expanding the domain within which implementation of SDT is tractable.","sentences":["We argue that comprehensive out-of-sample (OOS) evaluation using statistical decision theory (SDT) should replace the current practice of K-fold and Common Task Framework validation in machine learning (ML) research.","SDT provides a formal framework for performing comprehensive OOS evaluation across all possible (1) training samples, (2) populations that may generate training data, and (3) populations of prediction interest.","Regarding feature (3), we emphasize that SDT requires the practitioner to directly confront the possibility that the future may not look like the past and to account for a possible need to extrapolate from one population to another when building a predictive algorithm.","SDT is simple in abstraction, but it is often computationally demanding to implement.","We discuss progress in tractable implementation of SDT when prediction accuracy is measured by mean square error or by misclassification rate.","We summarize research studying settings in which the training data will be generated from a subpopulation of the population of prediction interest.","We also consider conditional prediction with alternative restrictions on the state space of possible populations that may generate training data.","We conclude by calling on ML researchers to join with econometricians and statisticians in expanding the domain within which implementation of SDT is tractable."],"url":"http://arxiv.org/abs/2403.11016v1","category":"econ.EM"}
{"created":"2024-03-16 20:42:27","title":"Improved Algorithm and Bounds for Successive Projection","abstract":"Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$ points on the simplex with noise (hence, some of the observed points fall outside the simplex). Vertex hunting is the problem of estimating the $K$ vertices of the simplex. A popular vertex hunting algorithm is successive projection algorithm (SPA). However, SPA is observed to perform unsatisfactorily under strong noise or outliers. We propose pseudo-point SPA (pp-SPA). It uses a projection step and a denoise step to generate pseudo-points and feed them into SPA for vertex hunting. We derive error bounds for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional random vectors. The results suggest that pp-SPA has faster rates and better numerical performances than SPA. Our analysis includes an improved non-asymptotic bound for the original SPA, which is of independent interest.","sentences":["Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$ points on the simplex with noise (hence, some of the observed points fall outside the simplex).","Vertex hunting is the problem of estimating the $K$ vertices of the simplex.","A popular vertex hunting algorithm is successive projection algorithm (SPA).","However, SPA is observed to perform unsatisfactorily under strong noise or outliers.","We propose pseudo-point SPA (pp-SPA).","It uses a projection step and a denoise step to generate pseudo-points and feed them into SPA for vertex hunting.","We derive error bounds for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional random vectors.","The results suggest that pp-SPA has faster rates and better numerical performances than SPA.","Our analysis includes an improved non-asymptotic bound for the original SPA, which is of independent interest."],"url":"http://arxiv.org/abs/2403.11013v1","category":"cs.LG"}
{"created":"2024-03-16 20:24:17","title":"Frizzle: Combining spectra or images by forward modeling","abstract":"When there are many observations of an astronomical source - many images with different dithers, or many spectra taken at different barycentric velocities - it is standard practice to shift and stack the data, to (for example) make a high signal-to-noise average image or mean spectrum. Bound-saturating measurements are made by manipulating a likelihood function, where the data are treated as fixed, and model parameters are modified to fit the data. Traditional shifting and stacking of data can be converted into a model-fitting procedure, such that the data are not modified, and yet the output is the shift-adjusted mean. The key component of this conversion is a spectral model that is completely flexible but also a continuous function of wavelength (or position in the case of imaging) that can represent any signal being measured by the device after any reasonable translation (or rotation or field distortion). The benefits of a modeling approach are myriad: The sacred data never are modified. Noise maps, data gaps, and bad-data masks don't require interpolation. The output can take the form of an image or spectrum evaluated on a pixel grid, as is traditional. In addition to shifts, the model can account for line-spread or point-spread function variations, world-coordinate-system variations, and calibration or normalization variations. The noise in the output becomes uncorrelated across neighboring pixels as the shifts deliver good coverage in some sense. The only cost is a small increase in computational complexity over that of traditional methods. We demonstrate the method with a small data example and we provide open source sample code for re-use.","sentences":["When there are many observations of an astronomical source - many images with different dithers, or many spectra taken at different barycentric velocities - it is standard practice to shift and stack the data, to (for example) make a high signal-to-noise average image or mean spectrum.","Bound-saturating measurements are made by manipulating a likelihood function, where the data are treated as fixed, and model parameters are modified to fit the data.","Traditional shifting and stacking of data can be converted into a model-fitting procedure, such that the data are not modified, and yet the output is the shift-adjusted mean.","The key component of this conversion is a spectral model that is completely flexible but also a continuous function of wavelength (or position in the case of imaging) that can represent any signal being measured by the device after any reasonable translation (or rotation or field distortion).","The benefits of a modeling approach are myriad: The sacred data never are modified.","Noise maps, data gaps, and bad-data masks don't require interpolation.","The output can take the form of an image or spectrum evaluated on a pixel grid, as is traditional.","In addition to shifts, the model can account for line-spread or point-spread function variations, world-coordinate-system variations, and calibration or normalization variations.","The noise in the output becomes uncorrelated across neighboring pixels as the shifts deliver good coverage in some sense.","The only cost is a small increase in computational complexity over that of traditional methods.","We demonstrate the method with a small data example and we provide open source sample code for re-use."],"url":"http://arxiv.org/abs/2403.11011v1","category":"astro-ph.IM"}
{"created":"2024-03-16 19:40:35","title":"Forward Learning of Graph Neural Networks","abstract":"Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward graph learning framework. We release our code at https://github.com/facebookresearch/forwardgnn.","sentences":["Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering.","Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs).","However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs.","Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals.","To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data.","Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training.","ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward).","Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors.","Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward graph learning framework.","We release our code at https://github.com/facebookresearch/forwardgnn."],"url":"http://arxiv.org/abs/2403.11004v1","category":"cs.LG"}
{"created":"2024-03-16 19:11:57","title":"Topologically faithful multi-class segmentation in medical images","abstract":"Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting. Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation. However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common. We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes. We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible. We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristics. Our loss formulation significantly enhances topological correctness in cardiac, cell, artery-vein, and Circle of Willis segmentation.","sentences":["Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting.","Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation.","However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common.","We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes.","We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible.","We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristics.","Our loss formulation significantly enhances topological correctness in cardiac, cell, artery-vein, and Circle of Willis segmentation."],"url":"http://arxiv.org/abs/2403.11001v1","category":"eess.IV"}
{"created":"2024-03-16 18:23:15","title":"Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control","abstract":"We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination.   Forward combinatorial optimization, defined as the process of solving a combinatorial problem given the reward (cost)-related parameters, is widely used in multi-robot coordination. In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts first and then these parameters are utilized for coordinating robots online. What if we need to change these parameters by non-expert human supervisors who watch over the robots during tasks to adapt to some new requirements? We are interested in the case where human supervisors can suggest what actions to take, and the robots need to change the internal parameters based on such suggestions. We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters and can make the greedy algorithm output actions the same as those suggested by humans. We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP). However, MIQP involves exponentially many binary variables, making it intractable for the existing solver when the problem size is large. We propose a new algorithm under the Branch $\\&$ Bound paradigm to solve such problems. In numerical simulations, we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to directly using an existing solver.","sentences":["We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination.   ","Forward combinatorial optimization, defined as the process of solving a combinatorial problem given the reward (cost)-related parameters, is widely used in multi-robot coordination.","In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts first and then these parameters are utilized for coordinating robots online.","What if we need to change these parameters by non-expert human supervisors who watch over the robots during tasks to adapt to some new requirements?","We are interested in the case where human supervisors can suggest what actions to take, and the robots need to change the internal parameters based on such suggestions.","We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem.","Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters and can make the greedy algorithm output actions the same as those suggested by humans.","We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP).","However, MIQP involves exponentially many binary variables, making it intractable for the existing solver when the problem size is large.","We propose a new algorithm under the Branch $\\&$ Bound paradigm to solve such problems.","In numerical simulations, we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to directly using an existing solver."],"url":"http://arxiv.org/abs/2403.10991v1","category":"cs.RO"}
{"created":"2024-03-16 17:56:31","title":"Risk Quadrangle and Robust Optimization Based on $\\varphi$-Divergence","abstract":"This paper studies robust and distributionally robust optimization based on the extended $\\varphi$-divergence under the Fundamental Risk Quadrangle framework. We present the primal and dual representations of the quadrangle elements: risk, deviation, regret, error, and statistic. The framework provides an interpretation of portfolio optimization, classification and regression as robust optimization. We furnish illustrative examples demonstrating that many common problems are included in this framework. The $\\varphi$-divergence risk measure used in distributionally robust optimization is a special case. We conduct a case study to visualize the risk envelope.","sentences":["This paper studies robust and distributionally robust optimization based on the extended $\\varphi$-divergence under the Fundamental Risk Quadrangle framework.","We present the primal and dual representations of the quadrangle elements: risk, deviation, regret, error, and statistic.","The framework provides an interpretation of portfolio optimization, classification and regression as robust optimization.","We furnish illustrative examples demonstrating that many common problems are included in this framework.","The $\\varphi$-divergence risk measure used in distributionally robust optimization is a special case.","We conduct a case study to visualize the risk envelope."],"url":"http://arxiv.org/abs/2403.10987v1","category":"math.OC"}
{"created":"2024-03-16 17:41:01","title":"Bounding the Graph Capacity with Quantum Mechanics and Finite Automata","abstract":"The zero-error capacity of a channel (or Shannon capacity of a graph) quantifies how much information can be transmitted with no risk of error. In contrast to the Shannon capacity of a channel, the zero-error capacity has not even been shown to be computable: we have no convergent upper bounds. In this work, we present a new quantity, the zero-error {\\em unitary} capacity, and show that it can be succinctly represented as the tensor product value of a quantum game. By studying the structure of finite automata, we show that the unitary capacity is within a controllable factor of the zero-error capacity. This allows new upper bounds through the sum-of-squares hierarchy, which converges to the commuting operator value of the game. Under the conjecture that the commuting operator and tensor product value of this game are equal, this would yield an algorithm for computing the zero-error capacity.","sentences":["The zero-error capacity of a channel (or Shannon capacity of a graph) quantifies how much information can be transmitted with no risk of error.","In contrast to the Shannon capacity of a channel, the zero-error capacity has not even been shown to be computable: we have no convergent upper bounds.","In this work, we present a new quantity, the zero-error {\\em unitary} capacity, and show that it can be succinctly represented as the tensor product value of a quantum game.","By studying the structure of finite automata, we show that the unitary capacity is within a controllable factor of the zero-error capacity.","This allows new upper bounds through the sum-of-squares hierarchy, which converges to the commuting operator value of the game.","Under the conjecture that the commuting operator and tensor product value of this game are equal, this would yield an algorithm for computing the zero-error capacity."],"url":"http://arxiv.org/abs/2403.10985v1","category":"cs.IT"}
{"created":"2024-03-16 17:23:20","title":"Inverse learning of black-box aggregator for robust Nash equilibrium","abstract":"In this note, we investigate the robustness of Nash equilibria (NE) in multi-player aggregative games with coupling constraints. There are many algorithms for computing an NE of an aggregative game given a known aggregator. When the coupling parameters are affected by uncertainty, robust NE need to be computed. We consider a scenario where players' weight in the aggregator is unknown, making the aggregator kind of \"a black box\". We pursue a suitable learning approach to estimate the unknown aggregator by proposing an inverse variational inequality-based relationship. We then utilize the counterpart to reconstruct the game and obtain first-order conditions for robust NE in the worst case. Furthermore, we characterize the generalization property of the learning methodology via an upper bound on the violation probability. Simulation experiments show the effectiveness of the proposed inverse learning approach.","sentences":["In this note, we investigate the robustness of Nash equilibria (NE) in multi-player aggregative games with coupling constraints.","There are many algorithms for computing an NE of an aggregative game given a known aggregator.","When the coupling parameters are affected by uncertainty, robust NE need to be computed.","We consider a scenario where players' weight in the aggregator is unknown, making the aggregator kind of \"a black box\".","We pursue a suitable learning approach to estimate the unknown aggregator by proposing an inverse variational inequality-based relationship.","We then utilize the counterpart to reconstruct the game and obtain first-order conditions for robust NE in the worst case.","Furthermore, we characterize the generalization property of the learning methodology via an upper bound on the violation probability.","Simulation experiments show the effectiveness of the proposed inverse learning approach."],"url":"http://arxiv.org/abs/2403.10980v1","category":"cs.GT"}
{"created":"2024-03-16 17:13:39","title":"Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video Streaming","abstract":"Traditional per-title encoding schemes aim to optimize encoding resolutions to deliver the highest perceptual quality for each representation. XPSNR is observed to correlate better with the subjective quality of VVC-coded bitstreams. Towards this realization, we predict the average XPSNR of VVC-coded bitstreams using spatiotemporal complexity features of the video and the target encoding configuration using an XGBoost-based model. Based on the predicted XPSNR scores, we introduce a Quality-A ware Dynamic Resolution Adaptation (QADRA) framework for adaptive video streaming applications, where we determine the convex-hull online. Furthermore, keeping the encoding and decoding times within an acceptable threshold is mandatory for smooth and energy-efficient streaming. Hence, QADRA determines the encoding resolution and quantization parameter (QP) for each target bitrate by maximizing XPSNR while constraining the maximum encoding and/ or decoding time below a threshold. QADRA implements a JND-based representation elimination algorithm to remove perceptually redundant representations from the bitrate ladder. QADRA is an open-source Python-based framework published under the GNU GPLv3 license. Github: https://github.com/PhoenixVideo/QADRA Online documentation: https://phoenixvideo.github.io/QADRA/","sentences":["Traditional per-title encoding schemes aim to optimize encoding resolutions to deliver the highest perceptual quality for each representation.","XPSNR is observed to correlate better with the subjective quality of VVC-coded bitstreams.","Towards this realization, we predict the average XPSNR of VVC-coded bitstreams using spatiotemporal complexity features of the video and the target encoding configuration using an XGBoost-based model.","Based on the predicted XPSNR scores, we introduce a Quality-A ware Dynamic Resolution Adaptation (QADRA) framework for adaptive video streaming applications, where we determine the convex-hull online.","Furthermore, keeping the encoding and decoding times within an acceptable threshold is mandatory for smooth and energy-efficient streaming.","Hence, QADRA determines the encoding resolution and quantization parameter (QP) for each target bitrate by maximizing XPSNR while constraining the maximum encoding and/ or decoding time below a threshold.","QADRA implements a JND-based representation elimination algorithm to remove perceptually redundant representations from the bitrate ladder.","QADRA is an open-source Python-based framework published under the GNU GPLv3 license.","Github: https://github.com/PhoenixVideo/QADRA","Online documentation: https://phoenixvideo.github.io/QADRA/"],"url":"http://arxiv.org/abs/2403.10976v1","category":"cs.MM"}
{"created":"2024-03-16 15:56:24","title":"Bootstrap percolation and $P_3$-hull number in direct products of graphs","abstract":"The $r$-neighbor bootstrap percolation is a graph infection process based on the update rule by which a vertex with $r$ infected neighbors becomes infected. We say that an initial set of infected vertices propagates if all vertices of a graph $G$ are eventually infected, and the minimum cardinality of such a set in $G$ is called the $r$-bootstrap percolation number, $m(G,r)$, of $G$. In this paper, we study percolating sets in direct products of graphs. While in general graphs there is no non-trivial upper bound on $m(G\\times H,r)$, we prove several upper bounds under the assumption $\\delta(G)\\ge r$. We also characterize the connected graphs $G$ and $H$ with minimum degree $2$ that satisfy $m(G \\times H, 2) = \\frac{|V(G \\times H)|}{2}$. In addition, we determine the exact values of $m(P_n \\times P_m, 2)$, which are $m+n-1$ if $m$ and $n$ are of different parities, and $m+n$ otherwise.","sentences":["The $r$-neighbor bootstrap percolation is a graph infection process based on the update rule by which a vertex with $r$ infected neighbors becomes infected.","We say that an initial set of infected vertices propagates if all vertices of a graph $G$ are eventually infected, and the minimum cardinality of such a set in $G$ is called the $r$-bootstrap percolation number, $m(G,r)$, of $G$. In this paper, we study percolating sets in direct products of graphs.","While in general graphs there is no non-trivial upper bound on $m(G\\times H,r)$, we prove several upper bounds under the assumption $\\delta(G)\\ge r$.","We also characterize the connected graphs $G$ and $H$ with minimum degree $2$ that satisfy $m(G \\times H, 2) = \\frac{|V(G \\times H)|}{2}$.","In addition, we determine the exact values of $m(P_n \\times P_m, 2)$, which are $m+n-1$ if $m$ and $n$ are of different parities, and $m+n$ otherwise."],"url":"http://arxiv.org/abs/2403.10957v1","category":"math.CO"}
{"created":"2024-03-16 15:54:07","title":"Stability and Dynamics of f(Q,B) Gravity","abstract":"This study explores the cosmological implications of a modified $f(Q, B)$ gravity model, incorporating both the nonmetricity scalar ($Q$) and the boundary term ($B$). A generic connection ($\\Gamma_{\\mu \\nu}^{\\alpha}$) has been used to define the four-dimensional metric tensor and the covariant derivative ($\\nabla_{\\mu}$). Statistical analysis using Markov Chain Monte Carlo (MCMC) techniques constrains the $H(z)$ model free parameters based on observational data from the Cosmic Chronometers (CC) sample, the extended Pantheon$^+$ dataset, and Baryonic Acoustic Oscillation (BAO) measurements. This analysis clarifies the deceleration and Equation of State (EoS) parameters and reveals a smooth transition from a deceleration to an accelerating expansion phase on the evolution history of the Universe. The most intriguing finding is identifying a stable critical point within the dynamical system of the model. This critical point corresponds to the de Sitter phase, a well-known era of accelerated expansion. The stability of the critical point suggests that, under specific initial conditions, the trajectory of the Universe will inherently be drawn towards and remain within the de Sitter phase. This aligns with the current observations, indicating Universe dominated by dark energy (DE) and undergoing late-time accelerated expansion.","sentences":["This study explores the cosmological implications of a modified $f(Q, B)$ gravity model, incorporating both the nonmetricity scalar ($Q$) and the boundary term ($B$).","A generic connection ($\\Gamma_{\\mu \\nu}^{\\alpha}$) has been used to define the four-dimensional metric tensor and the covariant derivative ($\\nabla_{\\mu}$).","Statistical analysis using Markov Chain Monte Carlo (MCMC) techniques constrains the $H(z)$ model free parameters based on observational data from the Cosmic Chronometers (CC) sample, the extended Pantheon$^+$ dataset, and Baryonic Acoustic Oscillation (BAO) measurements.","This analysis clarifies the deceleration and Equation of State (EoS) parameters and reveals a smooth transition from a deceleration to an accelerating expansion phase on the evolution history of the Universe.","The most intriguing finding is identifying a stable critical point within the dynamical system of the model.","This critical point corresponds to the de Sitter phase, a well-known era of accelerated expansion.","The stability of the critical point suggests that, under specific initial conditions, the trajectory of the Universe will inherently be drawn towards and remain within the de Sitter phase.","This aligns with the current observations, indicating Universe dominated by dark energy (DE) and undergoing late-time accelerated expansion."],"url":"http://arxiv.org/abs/2403.10956v1","category":"gr-qc"}
{"created":"2024-03-16 15:35:56","title":"TVIM: Thermo-Active Variable Impedance Module: Evaluating Shear-Mode Capabilities of Polycaprolactone","abstract":"In this work, we introduce an advanced thermo-active variable impedance module which builds upon our previous innovation in thermal-based impedance adjustment for actuation systems. Our initial design harnessed the temperature-responsive, viscoelastic properties of Polycaprolactone (PCL) to modulate stiffness and damping, facilitated by integrated flexible Peltier elements. While effective, the reliance on compressing and the inherent stress relaxation characteristics of PCL led to suboptimal response times in impedance adjustments. Addressing these limitations, the current iteration of our module pivots to a novel 'shear-mode' operation. By conducting comprehensive shear rheology analyses on PCL, we have identified a configuration that eliminates the viscoelastic delay, offering a faster response with improved heat transfer efficiency. A key advantage of our module lies in its scalability and elimination of additional mechanical actuators for impedance adjustment. The compactness and efficiency of thermal actuation through Peltier elements allow for significant downsizing, making these thermal, variable impedance modules exceptionally well-suited for applications where space constraints and actuator weight are critical considerations. This development represents a significant leap forward in the design of variable impedance actuators, offering a more versatile, responsive, and compact solution for a wide range of robotic and biomechanical applications.","sentences":["In this work, we introduce an advanced thermo-active variable impedance module which builds upon our previous innovation in thermal-based impedance adjustment for actuation systems.","Our initial design harnessed the temperature-responsive, viscoelastic properties of Polycaprolactone (PCL) to modulate stiffness and damping, facilitated by integrated flexible Peltier elements.","While effective, the reliance on compressing and the inherent stress relaxation characteristics of PCL led to suboptimal response times in impedance adjustments.","Addressing these limitations, the current iteration of our module pivots to a novel 'shear-mode' operation.","By conducting comprehensive shear rheology analyses on PCL, we have identified a configuration that eliminates the viscoelastic delay, offering a faster response with improved heat transfer efficiency.","A key advantage of our module lies in its scalability and elimination of additional mechanical actuators for impedance adjustment.","The compactness and efficiency of thermal actuation through Peltier elements allow for significant downsizing, making these thermal, variable impedance modules exceptionally well-suited for applications where space constraints and actuator weight are critical considerations.","This development represents a significant leap forward in the design of variable impedance actuators, offering a more versatile, responsive, and compact solution for a wide range of robotic and biomechanical applications."],"url":"http://arxiv.org/abs/2403.10951v1","category":"cs.RO"}
{"created":"2024-03-16 14:56:05","title":"Photo-induced charge state dynamics of the neutral and negatively charged silicon vacancy centers in room-temperature diamond","abstract":"The silicon vacancy (SiV) center in diamond is drawing much attention due to its optical and spin properties, attractive for quantum information processing and sensing. Comparatively little is known, however, about the dynamics governing SiV charge state interconversion mainly due to challenges associated with generating, stabilizing, and characterizing all possible charge states, particularly at room temperature. Here, we use multi-color confocal microscopy and density functional theory to examine photo-induced SiV recombination - from neutral, to single-, to double-negatively charged - over a broad spectral window in chemical-vapor-deposition diamond under ambient conditions. For the SiV0 to SiV- transition, we find a linear growth of the photo-recombination rate with laser power at all observed wavelengths, a hallmark of single photon dynamics. Laser excitation of SiV-, on the other hand, yields only fractional recombination into SiV2-, a finding we interpret in terms of a photo-activated electron tunneling process from proximal nitrogen atoms.","sentences":["The silicon vacancy (SiV) center in diamond is drawing much attention due to its optical and spin properties, attractive for quantum information processing and sensing.","Comparatively little is known, however, about the dynamics governing SiV charge state interconversion mainly due to challenges associated with generating, stabilizing, and characterizing all possible charge states, particularly at room temperature.","Here, we use multi-color confocal microscopy and density functional theory to examine photo-induced SiV recombination - from neutral, to single-, to double-negatively charged - over a broad spectral window in chemical-vapor-deposition diamond under ambient conditions.","For the SiV0 to SiV- transition, we find a linear growth of the photo-recombination rate with laser power at all observed wavelengths, a hallmark of single photon dynamics.","Laser excitation of SiV-, on the other hand, yields only fractional recombination into SiV2-, a finding we interpret in terms of a photo-activated electron tunneling process from proximal nitrogen atoms."],"url":"http://arxiv.org/abs/2403.10941v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-16 14:43:30","title":"Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning","abstract":"Dense retrieval has become the new paradigm in passage retrieval. Despite its effectiveness on typo-free queries, it is not robust when dealing with queries that contain typos. Current works on improving the typo-robustness of dense retrievers combine (i) data augmentation to obtain the typoed queries during training time with (ii) additional robustifying subtasks that aim to align the original, typo-free queries with their typoed variants. Even though multiple typoed variants are available as positive samples per query, some methods assume a single positive sample and a set of negative ones per anchor and tackle the robustifying subtask with contrastive learning; therefore, making insufficient use of the multiple positives (typoed queries). In contrast, in this work, we argue that all available positives can be used at the same time and employ contrastive learning that supports multiple positives (multi-positive). Experimental results on two datasets show that our proposed approach of leveraging all positives simultaneously and employing multi-positive contrastive learning on the robustifying subtask yields improvements in robustness against using contrastive learning with a single positive.","sentences":["Dense retrieval has become the new paradigm in passage retrieval.","Despite its effectiveness on typo-free queries, it is not robust when dealing with queries that contain typos.","Current works on improving the typo-robustness of dense retrievers combine (i) data augmentation to obtain the typoed queries during training time with (ii) additional robustifying subtasks that aim to align the original, typo-free queries with their typoed variants.","Even though multiple typoed variants are available as positive samples per query, some methods assume a single positive sample and a set of negative ones per anchor and tackle the robustifying subtask with contrastive learning; therefore, making insufficient use of the multiple positives (typoed queries).","In contrast, in this work, we argue that all available positives can be used at the same time and employ contrastive learning that supports multiple positives (multi-positive).","Experimental results on two datasets show that our proposed approach of leveraging all positives simultaneously and employing multi-positive contrastive learning on the robustifying subtask yields improvements in robustness against using contrastive learning with a single positive."],"url":"http://arxiv.org/abs/2403.10939v1","category":"cs.IR"}
{"created":"2024-03-16 14:38:31","title":"Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems","abstract":"Games with environmental feedback have become a crucial area of study across various scientific domains, modelling the dynamic interplay between human decisions and environmental changes, and highlighting the consequences of our choices on natural resources and biodiversity. In this work, we propose a co-evolutionary model for human-environment systems that incorporates the effects of knowledge feedback and social interaction on the sustainability of common pool resources. The model represents consumers as agents who adjust their resource extraction based on the resource's state. These agents are connected through social networks, where links symbolize either affinity or aversion among them. The interplay between social dynamics and resource dynamics is explored, with the system's evolution analyzed across various network topologies and initial conditions. We find that knowledge feedback can independently sustain common pool resources. However, the impact of social interactions on sustainability is dual-faceted: it can either support or impede sustainability, influenced by the network's connectivity and heterogeneity. A notable finding is the identification of a critical network mean degree, beyond which a depletion/repletion transition parallels an absorbing/active state transition in social dynamics, i.e., individual agents and their connections are/are not prone to being frozen in their social states. Furthermore, the study examines the evolution of the social network, revealing the emergence of two polarized groups where agents within each community have the same affinity. Comparative analyses using Monte-Carlo simulations and rate equations are employed, along with analytical arguments, to reinforce the study's findings. The model successfully captures how information spread and social dynamics may impact the sustanebility of common pool resource.","sentences":["Games with environmental feedback have become a crucial area of study across various scientific domains, modelling the dynamic interplay between human decisions and environmental changes, and highlighting the consequences of our choices on natural resources and biodiversity.","In this work, we propose a co-evolutionary model for human-environment systems that incorporates the effects of knowledge feedback and social interaction on the sustainability of common pool resources.","The model represents consumers as agents who adjust their resource extraction based on the resource's state.","These agents are connected through social networks, where links symbolize either affinity or aversion among them.","The interplay between social dynamics and resource dynamics is explored, with the system's evolution analyzed across various network topologies and initial conditions.","We find that knowledge feedback can independently sustain common pool resources.","However, the impact of social interactions on sustainability is dual-faceted: it can either support or impede sustainability, influenced by the network's connectivity and heterogeneity.","A notable finding is the identification of a critical network mean degree, beyond which a depletion/repletion transition parallels an absorbing/active state transition in social dynamics, i.e., individual agents and their connections are/are not prone to being frozen in their social states.","Furthermore, the study examines the evolution of the social network, revealing the emergence of two polarized groups where agents within each community have the same affinity.","Comparative analyses using Monte-Carlo simulations and rate equations are employed, along with analytical arguments, to reinforce the study's findings.","The model successfully captures how information spread and social dynamics may impact the sustanebility of common pool resource."],"url":"http://arxiv.org/abs/2403.10938v1","category":"physics.soc-ph"}
{"created":"2024-03-16 14:34:31","title":"Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR","abstract":"This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent for data sets of different sizes. Our approach is applicable for training speech recognition systems under low resource conditions where speech data and compute resources are insufficient, while there is a large text corpus that is available in the target language. Our research involves addressing the issue of out-of-vocabulary words of the baseline in general and does not focus on resolving the absence of named entities. Our proposed method is simple and yet computationally less expensive.","sentences":["This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices.","We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline.","The lattices generated after decoding with such an augmented baseline language model are more comprehensive.","We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method.","This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory.","We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent for data sets of different sizes.","Our approach is applicable for training speech recognition systems under low resource conditions where speech data and compute resources are insufficient, while there is a large text corpus that is available in the target language.","Our research involves addressing the issue of out-of-vocabulary words of the baseline in general and does not focus on resolving the absence of named entities.","Our proposed method is simple and yet computationally less expensive."],"url":"http://arxiv.org/abs/2403.10937v1","category":"eess.AS"}
{"created":"2024-03-16 14:20:46","title":"Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors","abstract":"Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches suffer from certain limitations. Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers. Quaternion-based SMC approaches have unwinding issues and complex architecture. Coordinate-free methods are slow and only almost globally stable. This paper presents a new six degrees of freedom SMC flight controller to address the above limitations. We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop. The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach. The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues. We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances. Our controller consistently outperforms the benchmark approaches with less control effort and actuator saturation, offering highly effective and efficient flight control.","sentences":["Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches suffer from certain limitations.","Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers.","Quaternion-based SMC approaches have unwinding issues and complex architecture.","Coordinate-free methods are slow and only almost globally stable.","This paper presents a new six degrees of freedom SMC flight controller to address the above limitations.","We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop.","The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach.","The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues.","We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances.","Our controller consistently outperforms the benchmark approaches with less control effort and actuator saturation, offering highly effective and efficient flight control."],"url":"http://arxiv.org/abs/2403.10934v1","category":"cs.RO"}
{"created":"2024-03-16 14:13:29","title":"Reduced Basis Method for the Elastic Scattering by Multiple Shape-Parametric Open Arcs in Two Dimensions","abstract":"We consider the elastic scattering problem by multiple disjoint arcs or \\emph{cracks} in two spatial dimensions. A key aspect of our approach lies in the parametric description of each arc's shape, which is controlled by a potentially high-dimensional, possibly countably infinite, set of parameters. We are interested in the efficient approximation of the parameter-to-solution map employing model order reduction techniques, specifically the reduced basis method.   Initially, we utilize boundary potentials to transform the boundary value problem, originally posed in an unbounded domain, into a system of boundary integral equations set on the parametrically defined open arcs. Our aim is to construct a rapid surrogate for solving this problem. To achieve this, we adopt the two-phase paradigm of the reduced basis method. In the offline phase, we compute solutions for this problem under the assumption of complete decoupling among arcs for various shapes. Leveraging these high-fidelity solutions and Proper Orthogonal Decomposition (POD), we construct a reduced-order basis tailored to the single arc problem. Subsequently, in the online phase, when computing solutions for the multiple arc problem with a new parametric input, we utilize the aforementioned basis for each individual arc. To expedite the offline phase, we employ a modified version of the Empirical Interpolation Method (EIM) to compute a precise and cost-effective affine representation of the interaction terms between arcs. Finally, we present a series of numerical experiments demonstrating the advantages of our proposed method in terms of both accuracy and computational efficiency.","sentences":["We consider the elastic scattering problem by multiple disjoint arcs or \\emph{cracks} in two spatial dimensions.","A key aspect of our approach lies in the parametric description of each arc's shape, which is controlled by a potentially high-dimensional, possibly countably infinite, set of parameters.","We are interested in the efficient approximation of the parameter-to-solution map employing model order reduction techniques, specifically the reduced basis method.   ","Initially, we utilize boundary potentials to transform the boundary value problem, originally posed in an unbounded domain, into a system of boundary integral equations set on the parametrically defined open arcs.","Our aim is to construct a rapid surrogate for solving this problem.","To achieve this, we adopt the two-phase paradigm of the reduced basis method.","In the offline phase, we compute solutions for this problem under the assumption of complete decoupling among arcs for various shapes.","Leveraging these high-fidelity solutions and Proper Orthogonal Decomposition (POD), we construct a reduced-order basis tailored to the single arc problem.","Subsequently, in the online phase, when computing solutions for the multiple arc problem with a new parametric input, we utilize the aforementioned basis for each individual arc.","To expedite the offline phase, we employ a modified version of the Empirical Interpolation Method (EIM) to compute a precise and cost-effective affine representation of the interaction terms between arcs.","Finally, we present a series of numerical experiments demonstrating the advantages of our proposed method in terms of both accuracy and computational efficiency."],"url":"http://arxiv.org/abs/2403.10933v1","category":"math.NA"}
{"created":"2024-03-16 14:11:54","title":"Uncertainty-Aware Adapter: Adapting Segment Anything Model (SAM) for Ambiguous Medical Image Segmentation","abstract":"The Segment Anything Model (SAM) gained significant success in natural image segmentation, and many methods have tried to fine-tune it to medical image segmentation. An efficient way to do so is by using Adapters, specialized modules that learn just a few parameters to tailor SAM specifically for medical images. However, unlike natural images, many tissues and lesions in medical images have blurry boundaries and may be ambiguous. Previous efforts to adapt SAM ignore this challenge and can only predict distinct segmentation.It may mislead clinicians or cause misdiagnosis, especially when encountering rare variants or situations with low model confidence. In this work, we propose a novel module called the Uncertainty-aware Adapter, which efficiently fine-tuning SAM for uncertainty-aware medical image segmentation. Utilizing a conditional variational autoencoder, we encoded stochastic samples to effectively represent the inherent uncertainty in medical imaging. We designed a new module on a standard adapter that utilizes a condition-based strategy to interact with samples to help SAM integrate uncertainty. We evaluated our method on two multi-annotated datasets with different modalities: LIDC-IDRI (lung abnormalities segmentation) and REFUGE2 (optic-cup segmentation). The experimental results show that the proposed model outperforms all the previous methods and achieves the new state-of-the-art (SOTA) on both benchmarks. We also demonstrated that our method can generate diverse segmentation hypotheses that are more realistic as well as heterogeneous.","sentences":["The Segment Anything Model (SAM) gained significant success in natural image segmentation, and many methods have tried to fine-tune it to medical image segmentation.","An efficient way to do so is by using Adapters, specialized modules that learn just a few parameters to tailor SAM specifically for medical images.","However, unlike natural images, many tissues and lesions in medical images have blurry boundaries and may be ambiguous.","Previous efforts to adapt SAM ignore this challenge and can only predict distinct segmentation.","It may mislead clinicians or cause misdiagnosis, especially when encountering rare variants or situations with low model confidence.","In this work, we propose a novel module called the Uncertainty-aware Adapter, which efficiently fine-tuning SAM for uncertainty-aware medical image segmentation.","Utilizing a conditional variational autoencoder, we encoded stochastic samples to effectively represent the inherent uncertainty in medical imaging.","We designed a new module on a standard adapter that utilizes a condition-based strategy to interact with samples to help SAM integrate uncertainty.","We evaluated our method on two multi-annotated datasets with different modalities: LIDC-IDRI (lung abnormalities segmentation) and REFUGE2 (optic-cup segmentation).","The experimental results show that the proposed model outperforms all the previous methods and achieves the new state-of-the-art (SOTA) on both benchmarks.","We also demonstrated that our method can generate diverse segmentation hypotheses that are more realistic as well as heterogeneous."],"url":"http://arxiv.org/abs/2403.10931v1","category":"eess.IV"}
{"created":"2024-03-16 14:00:04","title":"Function-space Parameterization of Neural Networks for Sequential Learning","abstract":"Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the project website.","sentences":["Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge.","While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images.","To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization.","Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining.","Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently.","We further show its strengths in uncertainty quantification and guiding exploration in model-based RL.","Further information and code is available on the project website."],"url":"http://arxiv.org/abs/2403.10929v1","category":"stat.ML"}
{"created":"2024-03-16 13:50:31","title":"Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC","abstract":"Utilizing unmanned aerial vehicles (UAVs) with edge server to assist terrestrial mobile edge computing (MEC) has attracted tremendous attention. Nevertheless, state-of-the-art schemes based on deterministic optimizations or single-objective reinforcement learning (RL) cannot reduce the backlog of task bits and simultaneously improve energy efficiency in highly dynamic network environments, where the design problem amounts to a sequential decision-making problem. In order to address the aforementioned problems, as well as the curses of dimensionality introduced by the growing number of terrestrial terrestrial users, this paper proposes a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme, integrated with MORL and the kernel method. The design of n-step return is also applied to average fluctuations in the backlog. Numerical results reveal that the n-step return can benefit the proposed kernel-based approach, achieving significant improvement in the long-term average backlog performance, compared to the conventional 1-step return design. Due to such design and the kernel-based neural network, to which decision-making features can be continuously added, the kernel-based approach can outperform the approach based on fully-connected deep neural network, yielding improvement in energy consumption and the backlog performance, as well as a significant reduction in decision-making and online learning time.","sentences":["Utilizing unmanned aerial vehicles (UAVs) with edge server to assist terrestrial mobile edge computing (MEC) has attracted tremendous attention.","Nevertheless, state-of-the-art schemes based on deterministic optimizations or single-objective reinforcement learning (RL) cannot reduce the backlog of task bits and simultaneously improve energy efficiency in highly dynamic network environments, where the design problem amounts to a sequential decision-making problem.","In order to address the aforementioned problems, as well as the curses of dimensionality introduced by the growing number of terrestrial terrestrial users, this paper proposes a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme, integrated with MORL and the kernel method.","The design of n-step return is also applied to average fluctuations in the backlog.","Numerical results reveal that the n-step return can benefit the proposed kernel-based approach, achieving significant improvement in the long-term average backlog performance, compared to the conventional 1-step return design.","Due to such design and the kernel-based neural network, to which decision-making features can be continuously added, the kernel-based approach can outperform the approach based on fully-connected deep neural network, yielding improvement in energy consumption and the backlog performance, as well as a significant reduction in decision-making and online learning time."],"url":"http://arxiv.org/abs/2403.10927v1","category":"cs.IT"}
{"created":"2024-03-16 13:44:00","title":"PAAMP: Polytopic Action-Set And Motion Planning For Long Horizon Dynamic Motion Planning via Mixed Integer Linear Programming","abstract":"Optimization methods for long-horizon, dynamically feasible motion planning in robotics tackle challenging non-convex and discontinuous optimization problems. Traditional methods often falter due to the nonlinear characteristics of these problems. We introduce a technique that utilizes learned representations of the system, known as Polytopic Action Sets, to efficiently compute long-horizon trajectories. By employing a suitable sequence of Polytopic Action Sets, we transform the long-horizon dynamically feasible motion planning problem into a Linear Program. This reformulation enables us to address motion planning as a Mixed Integer Linear Program (MILP). We demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning (PAAMP) approach by identifying swing-up motions for a torque-constrained pendulum within approximately 0.75 milliseconds. This approach is well-suited for solving complex motion planning and long-horizon Constraint Satisfaction Problems (CSPs) in dynamic and underactuated systems such as legged and aerial robots.","sentences":["Optimization methods for long-horizon, dynamically feasible motion planning in robotics tackle challenging non-convex and discontinuous optimization problems.","Traditional methods often falter due to the nonlinear characteristics of these problems.","We introduce a technique that utilizes learned representations of the system, known as Polytopic Action Sets, to efficiently compute long-horizon trajectories.","By employing a suitable sequence of Polytopic Action Sets, we transform the long-horizon dynamically feasible motion planning problem into a Linear Program.","This reformulation enables us to address motion planning as a Mixed Integer Linear Program (MILP).","We demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning (PAAMP) approach by identifying swing-up motions for a torque-constrained pendulum within approximately 0.75 milliseconds.","This approach is well-suited for solving complex motion planning and long-horizon Constraint Satisfaction Problems (CSPs) in dynamic and underactuated systems such as legged and aerial robots."],"url":"http://arxiv.org/abs/2403.10924v1","category":"cs.RO"}
{"created":"2024-03-18 12:06:31","title":"Sharp phase transitions in high-dimensional changepoint detection","abstract":"We study a hypothesis testing problem in the context of high-dimensional changepoint detection. Given a matrix $X \\in \\mathbb{R}^{p \\times n}$ with independent Gaussian entries, the goal is to determine whether or not a sparse, non-null fraction of rows in $X$ exhibits a shift in mean at a common index between $1$ and $n$. We focus on three aspects of this problem: the sparsity of non-null rows, the presence of a single, common changepoint in the non-null rows, and the signal strength associated with the changepoint. Within an asymptotic regime relating the data dimensions $n$ and $p$ to the signal sparsity and strength, we characterize the information-theoretic limits of the testing problem by a formula that determines whether the sum of Type I and II errors tends to zero or is bounded away from zero. The formula, called the \\emph{detection boundary}, is a curve that separates the parameter space into a detectable region and an undetectable region. We show that a Berk--Jones type test statistic can detect the presence of a sparse non-null fraction of rows, and does so adaptively throughout the detectable region. Conversely, within the undetectable region, no test is able to consistently distinguish the signal from noise.","sentences":["We study a hypothesis testing problem in the context of high-dimensional changepoint detection.","Given a matrix $X \\in \\mathbb{R}^{p \\times n}$ with independent Gaussian entries, the goal is to determine whether or not a sparse, non-null fraction of rows in $X$ exhibits a shift in mean at a common index between $1$ and $n$. We focus on three aspects of this problem: the sparsity of non-null rows, the presence of a single, common changepoint in the non-null rows, and the signal strength associated with the changepoint.","Within an asymptotic regime relating the data dimensions $n$ and $p$ to the signal sparsity and strength, we characterize the information-theoretic limits of the testing problem by a formula that determines whether the sum of Type I and II errors tends to zero or is bounded away from zero.","The formula, called the \\emph{detection boundary}, is a curve that separates the parameter space into a detectable region and an undetectable region.","We show that a Berk--Jones type test statistic can detect the presence of a sparse non-null fraction of rows, and does so adaptively throughout the detectable region.","Conversely, within the undetectable region, no test is able to consistently distinguish the signal from noise."],"url":"http://arxiv.org/abs/2403.11704v1","category":"math.ST"}
{"created":"2024-03-18 11:56:32","title":"A Spatial-Temporal Progressive Fusion Network for Breast Lesion Segmentation in Ultrasound Videos","abstract":"Ultrasound video-based breast lesion segmentation provides a valuable assistance in early breast lesion detection and treatment. However, existing works mainly focus on lesion segmentation based on ultrasound breast images which usually can not be adapted well to obtain desirable results on ultrasound videos. The main challenge for ultrasound video-based breast lesion segmentation is how to exploit the lesion cues of both intra-frame and inter-frame simultaneously. To address this problem, we propose a novel Spatial-Temporal Progressive Fusion Network (STPFNet) for video based breast lesion segmentation problem. The main aspects of the proposed STPFNet are threefold. First, we propose to adopt a unified network architecture to capture both spatial dependences within each ultrasound frame and temporal correlations between different frames together for ultrasound data representation. Second, we propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to fuse spatial and temporal cues together for lesion detection. MSFF can help to determine the boundary contour of lesion region to overcome the issue of lesion boundary blurring. Third, we propose to exploit the segmentation result of previous frame as the prior knowledge to suppress the noisy background and learn more robust representation. In particular, we introduce a new publicly available ultrasound video breast lesion segmentation dataset, termed UVBLS200, which is specifically dedicated to breast lesion segmentation. It contains 200 videos, including 80 videos of benign lesions and 120 videos of malignant lesions. Experiments on the proposed dataset demonstrate that the proposed STPFNet achieves better breast lesion detection performance than state-of-the-art methods.","sentences":["Ultrasound video-based breast lesion segmentation provides a valuable assistance in early breast lesion detection and treatment.","However, existing works mainly focus on lesion segmentation based on ultrasound breast images which usually can not be adapted well to obtain desirable results on ultrasound videos.","The main challenge for ultrasound video-based breast lesion segmentation is how to exploit the lesion cues of both intra-frame and inter-frame simultaneously.","To address this problem, we propose a novel Spatial-Temporal Progressive Fusion Network (STPFNet) for video based breast lesion segmentation problem.","The main aspects of the proposed STPFNet are threefold.","First, we propose to adopt a unified network architecture to capture both spatial dependences within each ultrasound frame and temporal correlations between different frames together for ultrasound data representation.","Second, we propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to fuse spatial and temporal cues together for lesion detection.","MSFF can help to determine the boundary contour of lesion region to overcome the issue of lesion boundary blurring.","Third, we propose to exploit the segmentation result of previous frame as the prior knowledge to suppress the noisy background and learn more robust representation.","In particular, we introduce a new publicly available ultrasound video breast lesion segmentation dataset, termed UVBLS200, which is specifically dedicated to breast lesion segmentation.","It contains 200 videos, including 80 videos of benign lesions and 120 videos of malignant lesions.","Experiments on the proposed dataset demonstrate that the proposed STPFNet achieves better breast lesion detection performance than state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.11699v1","category":"eess.IV"}
{"created":"2024-03-18 11:41:55","title":"TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models","abstract":"Test-Time Training (TTT) proposes to adapt a pre-trained network to changing data distributions on-the-fly. In this work, we propose the first TTT method for 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD) from foundation models (e.g. DINOv2) as a self-supervised objective for adaptation to distribution shifts at test-time. Given access to paired image-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for the main task of semantic segmentation using the pointclouds and the task of 2D $\\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model. At test-time, our TTT-KD updates the 3D segmentation backbone for each test sample, by using the self-supervised task of knowledge distillation, before performing the final prediction. Extensive evaluations on multiple indoor and outdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves performance for both in-distribution (ID) and out-of-distribution (ODO) test datasets. We achieve a gain of up to 13% mIoU (7% on average) when the train and test distributions are similar and up to 45% (20% on average) when adapting to OOD test samples.","sentences":["Test-Time Training (TTT) proposes to adapt a pre-trained network to changing data distributions on-the-fly.","In this work, we propose the first TTT method for 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD) from foundation models (e.g. DINOv2) as a self-supervised objective for adaptation to distribution shifts at test-time.","Given access to paired image-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for the main task of semantic segmentation using the pointclouds and the task of 2D $\\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model.","At test-time, our TTT-KD updates the 3D segmentation backbone for each test sample, by using the self-supervised task of knowledge distillation, before performing the final prediction.","Extensive evaluations on multiple indoor and outdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves performance for both in-distribution (ID) and out-of-distribution (ODO) test datasets.","We achieve a gain of up to 13% mIoU (7% on average) when the train and test distributions are similar and up to 45% (20% on average) when adapting to OOD test samples."],"url":"http://arxiv.org/abs/2403.11691v1","category":"cs.CV"}
{"created":"2024-03-18 08:55:48","title":"OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation","abstract":"Multi-target domain adaptation (MTDA) for semantic segmentation poses a significant challenge, as it involves multiple target domains with varying distributions. The goal of MTDA is to minimize the domain discrepancies among a single source and multi-target domains, aiming to train a single model that excels across all target domains. Previous MTDA approaches typically employ multiple teacher architectures, where each teacher specializes in one target domain to simplify the task. However, these architectures hinder the student model from fully assimilating comprehensive knowledge from all target-specific teachers and escalate training costs with increasing target domains. In this paper, we propose an ouroboric domain bridging (OurDB) framework, offering an efficient solution to the MTDA problem using a single teacher architecture. This framework dynamically cycles through multiple target domains, aligning each domain individually to restrain the biased alignment problem, and utilizes Fisher information to minimize the forgetting of knowledge from previous target domains. We also propose a context-guided class-wise mixup (CGMix) that leverages contextual information tailored to diverse target contexts in MTDA. Experimental evaluations conducted on four urban driving datasets (i.e., GTA5, Cityscapes, IDD, and Mapillary) demonstrate the superiority of our method over existing state-of-the-art approaches.","sentences":["Multi-target domain adaptation (MTDA) for semantic segmentation poses a significant challenge, as it involves multiple target domains with varying distributions.","The goal of MTDA is to minimize the domain discrepancies among a single source and multi-target domains, aiming to train a single model that excels across all target domains.","Previous MTDA approaches typically employ multiple teacher architectures, where each teacher specializes in one target domain to simplify the task.","However, these architectures hinder the student model from fully assimilating comprehensive knowledge from all target-specific teachers and escalate training costs with increasing target domains.","In this paper, we propose an ouroboric domain bridging (OurDB) framework, offering an efficient solution to the MTDA problem using a single teacher architecture.","This framework dynamically cycles through multiple target domains, aligning each domain individually to restrain the biased alignment problem, and utilizes Fisher information to minimize the forgetting of knowledge from previous target domains.","We also propose a context-guided class-wise mixup (CGMix) that leverages contextual information tailored to diverse target contexts in MTDA.","Experimental evaluations conducted on four urban driving datasets (i.e., GTA5, Cityscapes, IDD, and Mapillary) demonstrate the superiority of our method over existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.11582v1","category":"cs.CV"}
{"created":"2024-03-18 08:41:36","title":"R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement","abstract":"We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks. Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals. Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs. Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation.","sentences":["We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks.","Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet.","This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals.","Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs.","Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation."],"url":"http://arxiv.org/abs/2403.11567v1","category":"cs.RO"}
{"created":"2024-03-18 08:18:33","title":"Distributed Adaptive Gradient Algorithm with Gradient Tracking for Stochastic Non-Convex Optimization","abstract":"This paper considers a distributed stochastic non-convex optimization problem, where the nodes in a network cooperatively minimize a sum of $L$-smooth local cost functions with sparse gradients. By adaptively adjusting the stepsizes according to the historical (possibly sparse) gradients, a distributed adaptive gradient algorithm is proposed, in which a gradient tracking estimator is used to handle the heterogeneity between different local cost functions. We establish an upper bound on the optimality gap, which indicates that our proposed algorithm can reach a first-order stationary solution dependent on the upper bound on the variance of the stochastic gradients. Finally, numerical examples are presented to illustrate the effectiveness of the algorithm.","sentences":["This paper considers a distributed stochastic non-convex optimization problem, where the nodes in a network cooperatively minimize a sum of $L$-smooth local cost functions with sparse gradients.","By adaptively adjusting the stepsizes according to the historical (possibly sparse) gradients, a distributed adaptive gradient algorithm is proposed, in which a gradient tracking estimator is used to handle the heterogeneity between different local cost functions.","We establish an upper bound on the optimality gap, which indicates that our proposed algorithm can reach a first-order stationary solution dependent on the upper bound on the variance of the stochastic gradients.","Finally, numerical examples are presented to illustrate the effectiveness of the algorithm."],"url":"http://arxiv.org/abs/2403.11557v1","category":"math.OC"}
{"created":"2024-03-18 08:00:23","title":"Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters","abstract":"Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models. Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the zero-shot recognition capability of vision-language models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP, respectively. Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%. Our code locates at https://github.com/JiazuoYu/MoE-Adapters4CL","sentences":["Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset.","However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning.","In this work, we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models.","Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks.","To preserve the zero-shot recognition capability of vision-language models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP, respectively.","Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%.","Our code locates at https://github.com/JiazuoYu/MoE-Adapters4CL"],"url":"http://arxiv.org/abs/2403.11549v1","category":"cs.CV"}
{"created":"2024-03-18 06:54:38","title":"Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation","abstract":"In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user's preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: \\href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}","sentences":["In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape.","These properties play a crucial role in operating a robot to interact with objects and align with human intention.","In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI).","To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method.","CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's preference.","This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks.","Furthermore, we incorporate such texts along with a sequence of images to infer the user's preferences.","Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments.","Code and videos are available at: \\href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}"],"url":"http://arxiv.org/abs/2403.11513v1","category":"cs.RO"}
{"created":"2024-03-18 06:42:38","title":"Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation","abstract":"This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a domain adaptation problem. In this case, we present a global-to-local method to address hybrid domain gaps in RGB and depth data and insufficient multi-modal feature alignment. First, a self-supervised rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks. We then propose a global-to-local alignment pipeline with individual global domain classifiers for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities. In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the simulation and real-world scenarios throughout the training process. Due to such designs, the proposed method substantially reduces the domain shift and thus leads to consistent performance improvements. Extensive experiments are conducted on the GraspNet-Planar benchmark and physical environment, and superior results are achieved which demonstrate the effectiveness of our method.","sentences":["This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a domain adaptation problem.","In this case, we present a global-to-local method to address hybrid domain gaps in RGB and depth data and insufficient multi-modal feature alignment.","First, a self-supervised rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks.","We then propose a global-to-local alignment pipeline with individual global domain classifiers for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities.","In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the simulation and real-world scenarios throughout the training process.","Due to such designs, the proposed method substantially reduces the domain shift and thus leads to consistent performance improvements.","Extensive experiments are conducted on the GraspNet-Planar benchmark and physical environment, and superior results are achieved which demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.11511v1","category":"cs.RO"}
{"created":"2024-03-18 06:20:49","title":"Covid-19 detection from CT scans using EfficientNet and Attention mechanism","abstract":"Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year's teams on the validation set of the competition dataset.","sentences":["Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient.","We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs.","The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images.","The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step.","Our pipeline outperforms last year's teams on the validation set of the competition dataset."],"url":"http://arxiv.org/abs/2403.11505v1","category":"eess.IV"}
{"created":"2024-03-18 06:12:17","title":"The role of Ohmic dissipation of internal currents on Hot Jupiter radii","abstract":"The inflated radii observed in hundreds of Hot Jupiters represent a long-standing open issue. The observed correlation between radii and irradiation strength, and the occasional extreme cases, nearly double the size of Jupiter, remain without a comprehensive quantitative explanation. In this investigation, we delve into this issue within the framework of Ohmic dissipation, one of the most promising mechanisms for explaining the radius anomaly. Using the evolutionary code MESA, we simulate the evolution of irradiated giant planets, spanning the range 1 to 8 Jupiter masses, incorporating an internal source of Ohmic dissipation located beneath the radiative-convective boundary. Our modeling is based on physical parameters, and accounts for the approximated conductivity and the evolution of the magnetic fields, utilizing widely-used scaling laws. We compute the radius evolution across a spectrum of masses and equilibrium temperatures, considering varying amounts of Ohmic dissipation, calculated with the internal conductivity profile and an effective parametrization of the currents, based on the typical radius of curvature of the field lines. Our analysis reveals that this internal Ohmic dissipation can broadly reproduce the range of observed radii using values of radius of curvature up to about one order of magnitude lower than what we estimate from the Juno measurements of the Jovian magnetosphere and from MHD dynamo simulations presented herein. The observed trend with equilibrium temperature can be explained if the highly-irradiated planets have more intense and more small-scale magnetic fields. This suggests the possibility of an interplay between atmospherically induced currents and the interior, via turbulence, in agreement with recent box simulations of turbulent MHD in atmospheric columns.","sentences":["The inflated radii observed in hundreds of Hot Jupiters represent a long-standing open issue.","The observed correlation between radii and irradiation strength, and the occasional extreme cases, nearly double the size of Jupiter, remain without a comprehensive quantitative explanation.","In this investigation, we delve into this issue within the framework of Ohmic dissipation, one of the most promising mechanisms for explaining the radius anomaly.","Using the evolutionary code MESA, we simulate the evolution of irradiated giant planets, spanning the range 1 to 8 Jupiter masses, incorporating an internal source of Ohmic dissipation located beneath the radiative-convective boundary.","Our modeling is based on physical parameters, and accounts for the approximated conductivity and the evolution of the magnetic fields, utilizing widely-used scaling laws.","We compute the radius evolution across a spectrum of masses and equilibrium temperatures, considering varying amounts of Ohmic dissipation, calculated with the internal conductivity profile and an effective parametrization of the currents, based on the typical radius of curvature of the field lines.","Our analysis reveals that this internal Ohmic dissipation can broadly reproduce the range of observed radii using values of radius of curvature up to about one order of magnitude lower than what we estimate from the Juno measurements of the Jovian magnetosphere and from MHD dynamo simulations presented herein.","The observed trend with equilibrium temperature can be explained if the highly-irradiated planets have more intense and more small-scale magnetic fields.","This suggests the possibility of an interplay between atmospherically induced currents and the interior, via turbulence, in agreement with recent box simulations of turbulent MHD in atmospheric columns."],"url":"http://arxiv.org/abs/2403.11501v1","category":"astro-ph.EP"}
{"created":"2024-03-18 04:42:24","title":"Tight minimum degree conditions for apex-outerplanar minors and subdivisions in graphs and digraphs","abstract":"Motivated by Hadwiger's conjecture and related problems for list-coloring, we study graphs $H$ for which every graph with minimum degree at least $|V(H)|-1$ contains $H$ as a minor. We prove that a large class of apex-outerplanar graphs satisfies this property. Our result gives the first examples of such graphs whose vertex cover numbers are significantly larger than half of the number of its vertices, which breaks a barrier for attacking related coloring problems via extremal functions, and recovers all known such graphs that have arbitrarily large maximum degree. Our proof can be adapted to directed graphs to show that if $\\vec H$ is the digraph obtained from a directed cycle or an in-arborescence by adding an apex source, then every digraph with minimum out-degree $|V(\\vec H)|-1$ contains $\\vec H$ as a subdivision or a butterfly minor respectively. These results provide the optimal upper bound for the chromatic number and dichromatic number of graphs and digraphs that do not contain the aforementioned graphs or digraphs as a minor, butterfly minor and a subdivision, respectively. Special cases of our results solve an open problem of Aboulker, Cohen, Havet, Lochet, Moura and Thomass\\'{e} and strengthen results of Gishboliner, Steiner and Szab\\'{o}.","sentences":["Motivated by Hadwiger's conjecture and related problems for list-coloring, we study graphs $H$ for which every graph with minimum degree at least $|V(H)|-1$ contains $H$ as a minor.","We prove that a large class of apex-outerplanar graphs satisfies this property.","Our result gives the first examples of such graphs whose vertex cover numbers are significantly larger than half of the number of its vertices, which breaks a barrier for attacking related coloring problems via extremal functions, and recovers all known such graphs that have arbitrarily large maximum degree.","Our proof can be adapted to directed graphs to show that if $\\vec H$ is the digraph obtained from a directed cycle or an in-arborescence by adding an apex source, then every digraph with minimum out-degree $|V(\\vec H)|-1$ contains $\\vec H$ as a subdivision or a butterfly minor respectively.","These results provide the optimal upper bound for the chromatic number and dichromatic number of graphs and digraphs that do not contain the aforementioned graphs or digraphs as a minor, butterfly minor and a subdivision, respectively.","Special cases of our results solve an open problem of Aboulker, Cohen, Havet, Lochet, Moura and Thomass\\'{e} and strengthen results of Gishboliner, Steiner and Szab\\'{o}."],"url":"http://arxiv.org/abs/2403.11470v1","category":"math.CO"}
{"created":"2024-03-18 02:19:15","title":"Solar chromospheric heating by magnetohydrodynamic waves: dependence on magnetic field inclination","abstract":"A proposed mechanism for solar chromospheric heating is that magnetohydrodynamic waves propagate upward along magnetic field lines and dissipate their energy in the chromosphere. In particular, compressible magneto-acoustic waves may contribute to the heating. Theoretically, the components below the cutoff frequency cannot propagate into the chromosphere; however, the cutoff frequency depends on the inclination of the magnetic field lines. In this study, using high temporal cadence spectral data of IRIS and Hinode SOT spectropolarimeter (SP) in plages, we investigated the dependence of the low-frequency waves on magnetic-field properties and quantitatively estimated the amount of energy dissipation in the chromosphere. The following results were obtained: (a) The amount of energy dissipated by the low-frequency component (3--6 mHz) increases with the field inclination below 40 degrees, whereas it is decreased as a function of the field inclination above 40 degrees. (b) The amount of the energy is enhanced toward $10^4 W/m^2$, which is the energy required for heating in the chromospheric plage regions, when the magnetic field is higher than 600 G and inclined more than 40 degree. (c) In the photosphere, the low-frequency component has much more power in the magnetic field inclined more and weaker than 400 G. The results suggest that the observed low-frequency components can bring the energy along the magnetic field lines and that only a specific range of the field inclination angles and field strength may allow the low-frequency component to bring the sufficient amount of the energy into the chromosphere.","sentences":["A proposed mechanism for solar chromospheric heating is that magnetohydrodynamic waves propagate upward along magnetic field lines and dissipate their energy in the chromosphere.","In particular, compressible magneto-acoustic waves may contribute to the heating.","Theoretically, the components below the cutoff frequency cannot propagate into the chromosphere; however, the cutoff frequency depends on the inclination of the magnetic field lines.","In this study, using high temporal cadence spectral data of IRIS and Hinode SOT spectropolarimeter (SP) in plages, we investigated the dependence of the low-frequency waves on magnetic-field properties and quantitatively estimated the amount of energy dissipation in the chromosphere.","The following results were obtained: (a) The amount of energy dissipated by the low-frequency component (3--6 mHz) increases with the field inclination below 40 degrees, whereas it is decreased as a function of the field inclination above 40 degrees.","(b) The amount of the energy is enhanced toward $10^4 W/m^2$, which is the energy required for heating in the chromospheric plage regions, when the magnetic field is higher than 600 G and inclined more than 40 degree.","(c) In the photosphere, the low-frequency component has much more power in the magnetic field inclined more and weaker than 400 G.","The results suggest that the observed low-frequency components can bring the energy along the magnetic field lines and that only a specific range of the field inclination angles and field strength may allow the low-frequency component to bring the sufficient amount of the energy into the chromosphere."],"url":"http://arxiv.org/abs/2403.11419v1","category":"astro-ph.SR"}
{"created":"2024-03-18 01:13:39","title":"Cloaking Transition of Droplets on Lubricated Brushes","abstract":"We study the equilibrium properties and the wetting behavior of a simple liquid on a polymer brush, with and without presence of lubricant by multibody Dissipative Particle Dynamics simulations. The lubricant is modelled as a polymeric liquid consisting of short chains that are chemically identical to the brush polymers. We investigate the behavior of the brush in terms of the grafting density and the amount of lubricant present. Regarding the wetting behavior, we study a sessile droplet on top of the brush. The droplet consists of non-bonded particles that form a dense phase. Our model and choice of parameters result in the formation of a wetting ridge and in the cloaking of the droplet by the lubricant, i.e. the lubricant chains creep up onto the droplet and eventually cover its surface completely. Cloaking is a phenomenon that is observed experimentally and is of integral importance to the dynamics of sliding droplets. We quantify the cloaking in terms of its thickness, which increases with the amount of lubricant present. The analysis reveals a well-defined transition point where the cloaking sets in. We propose a thermodynamic theory to explain this behavior. In addition we investigate the dependence of the contact angles on the size of the droplet and the possible effect of line tension. We quantify the variation of the contact angle with the curvature of the contact line on a lubricant free brush and find a negative value for the line tension. Finally we investigate the effect of cloaking/lubrication on the contact angles and the wetting ridge. We find that lubrication and cloaking reduce the contact angles by a couple of degrees. The effect on the wetting ridge is a reduction in the extension of the brush chains near the three phase contact line, an effect that was also observed in experiments of droplets on crosslinked gels.","sentences":["We study the equilibrium properties and the wetting behavior of a simple liquid on a polymer brush, with and without presence of lubricant by multibody Dissipative Particle Dynamics simulations.","The lubricant is modelled as a polymeric liquid consisting of short chains that are chemically identical to the brush polymers.","We investigate the behavior of the brush in terms of the grafting density and the amount of lubricant present.","Regarding the wetting behavior, we study a sessile droplet on top of the brush.","The droplet consists of non-bonded particles that form a dense phase.","Our model and choice of parameters result in the formation of a wetting ridge and in the cloaking of the droplet by the lubricant, i.e. the lubricant chains creep up onto the droplet and eventually cover its surface completely.","Cloaking is a phenomenon that is observed experimentally and is of integral importance to the dynamics of sliding droplets.","We quantify the cloaking in terms of its thickness, which increases with the amount of lubricant present.","The analysis reveals a well-defined transition point where the cloaking sets in.","We propose a thermodynamic theory to explain this behavior.","In addition we investigate the dependence of the contact angles on the size of the droplet and the possible effect of line tension.","We quantify the variation of the contact angle with the curvature of the contact line on a lubricant free brush and find a negative value for the line tension.","Finally we investigate the effect of cloaking/lubrication on the contact angles and the wetting ridge.","We find that lubrication and cloaking reduce the contact angles by a couple of degrees.","The effect on the wetting ridge is a reduction in the extension of the brush chains near the three phase contact line, an effect that was also observed in experiments of droplets on crosslinked gels."],"url":"http://arxiv.org/abs/2403.11398v1","category":"cond-mat.soft"}
{"created":"2024-03-17 14:43:47","title":"Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias","abstract":"Domain adaptation is a critical task in machine learning that aims to improve model performance on a target domain by leveraging knowledge from a related source domain. In this work, we introduce Universal Semi-Supervised Domain Adaptation (UniSSDA), a practical yet challenging setting where the target domain is partially labeled, and the source and target label space may not strictly match. UniSSDA is at the intersection of Universal Domain Adaptation (UniDA) and Semi-Supervised Domain Adaptation (SSDA): the UniDA setting does not allow for fine-grained categorization of target private classes not represented in the source domain, while SSDA focuses on the restricted closed-set setting where source and target label spaces match exactly. Existing UniDA and SSDA methods are susceptible to common-class bias in UniSSDA settings, where models overfit to data distributions of classes common to both domains at the expense of private classes. We propose a new prior-guided pseudo-label refinement strategy to reduce the reinforcement of common-class bias due to pseudo-labeling, a common label propagation strategy in domain adaptation. We demonstrate the effectiveness of the proposed strategy on benchmark datasets Office-Home, DomainNet, and VisDA. The proposed strategy attains the best performance across UniSSDA adaptation settings and establishes a new baseline for UniSSDA.","sentences":["Domain adaptation is a critical task in machine learning that aims to improve model performance on a target domain by leveraging knowledge from a related source domain.","In this work, we introduce Universal Semi-Supervised Domain Adaptation (UniSSDA), a practical yet challenging setting where the target domain is partially labeled, and the source and target label space may not strictly match.","UniSSDA is at the intersection of Universal Domain Adaptation (UniDA) and Semi-Supervised Domain Adaptation (SSDA): the UniDA setting does not allow for fine-grained categorization of target private classes not represented in the source domain, while SSDA focuses on the restricted closed-set setting where source and target label spaces match exactly.","Existing UniDA and SSDA methods are susceptible to common-class bias in UniSSDA settings, where models overfit to data distributions of classes common to both domains at the expense of private classes.","We propose a new prior-guided pseudo-label refinement strategy to reduce the reinforcement of common-class bias due to pseudo-labeling, a common label propagation strategy in domain adaptation.","We demonstrate the effectiveness of the proposed strategy on benchmark datasets Office-Home, DomainNet, and VisDA.","The proposed strategy attains the best performance across UniSSDA adaptation settings and establishes a new baseline for UniSSDA."],"url":"http://arxiv.org/abs/2403.11234v1","category":"cs.CV"}
{"created":"2024-03-17 14:16:00","title":"Multiple Teachers-Meticulous Student: A Domain Adaptive Meta-Knowledge Distillation Model for Medical Image Classification","abstract":"Background: Image classification can be considered one of the key pillars of medical image analysis. Deep learning (DL) faces challenges that prevent its practical applications despite the remarkable improvement in medical image classification. The data distribution differences can lead to a drop in the efficiency of DL, known as the domain shift problem. Besides, requiring bulk annotated data for model training, the large size of models, and the privacy-preserving of patients are other challenges of using DL in medical image classification. This study presents a strategy that can address the mentioned issues simultaneously. Method: The proposed domain adaptive model based on knowledge distillation can classify images by receiving limited annotated data of different distributions. The designed multiple teachers-meticulous student model trains a student network that tries to solve the challenges by receiving the parameters of several teacher networks. The proposed model was evaluated using six available datasets of different distributions by defining the respiratory motion artefact detection task. Results: The results of extensive experiments using several datasets show the superiority of the proposed model in addressing the domain shift problem and lack of access to bulk annotated data. Besides, the privacy preservation of patients by receiving only the teacher network parameters instead of the original data and consolidating the knowledge of several DL models into a model with almost similar performance are other advantages of the proposed model. Conclusions: The proposed model can pave the way for practical clinical applications of deep classification methods by achieving the mentioned objectives simultaneously.","sentences":["Background: Image classification can be considered one of the key pillars of medical image analysis.","Deep learning (DL) faces challenges that prevent its practical applications despite the remarkable improvement in medical image classification.","The data distribution differences can lead to a drop in the efficiency of DL, known as the domain shift problem.","Besides, requiring bulk annotated data for model training, the large size of models, and the privacy-preserving of patients are other challenges of using DL in medical image classification.","This study presents a strategy that can address the mentioned issues simultaneously.","Method: The proposed domain adaptive model based on knowledge distillation can classify images by receiving limited annotated data of different distributions.","The designed multiple teachers-meticulous student model trains a student network that tries to solve the challenges by receiving the parameters of several teacher networks.","The proposed model was evaluated using six available datasets of different distributions by defining the respiratory motion artefact detection task.","Results: The results of extensive experiments using several datasets show the superiority of the proposed model in addressing the domain shift problem and lack of access to bulk annotated data.","Besides, the privacy preservation of patients by receiving only the teacher network parameters instead of the original data and consolidating the knowledge of several DL models into a model with almost similar performance are other advantages of the proposed model.","Conclusions: The proposed model can pave the way for practical clinical applications of deep classification methods by achieving the mentioned objectives simultaneously."],"url":"http://arxiv.org/abs/2403.11226v1","category":"eess.IV"}
{"created":"2024-03-17 13:45:28","title":"Lion: Minimizing Distributed Transactions through Adaptive Replica Provision (Extended Version)","abstract":"Distributed transaction processing often involves multiple rounds of cross-node communications, and therefore tends to be slow. To improve performance, existing approaches convert distributed transactions into single-node transactions by either migrating co-accessed partitions onto the same nodes or establishing a super node housing replicas of the entire database. However, migration-based methods might cause transactions to be blocked due to waiting for data migration, while the super node can become a bottleneck. In this paper, we present Lion, a novel transaction processing protocol that utilizes partition-based replication to reduce the occurrence of distributed transactions. Lion aims to assign a node with one replica from each partition involved in a given transaction's read or write operations. To ensure such a node is available, we propose an adaptive replica provision mechanism, enhanced with an LSTM-based workload prediction algorithm, to determine the appropriate node for locating replicas of co-accessed partitions. The adaptation of replica placement is conducted preemptively and asynchronously, thereby minimizing its impact on performance. By employing this adaptive replica placement strategy, we ensure that the majority of transactions can be efficiently processed on a single node without additional overhead. Only a small fraction of transactions will need to be treated as regular distributed transactions when such a node is unavailable. Consequently, Lion effectively minimizes distributed transactions while avoiding any disruption caused by data migration or the creation of a super node. We conduct extensive experiments to compare Lion against various transaction processing protocols. The results show that Lion achieves up to 2.7x higher throughput and 76.4% better scalability against these state-of-the-art approaches.","sentences":["Distributed transaction processing often involves multiple rounds of cross-node communications, and therefore tends to be slow.","To improve performance, existing approaches convert distributed transactions into single-node transactions by either migrating co-accessed partitions onto the same nodes or establishing a super node housing replicas of the entire database.","However, migration-based methods might cause transactions to be blocked due to waiting for data migration, while the super node can become a bottleneck.","In this paper, we present Lion, a novel transaction processing protocol that utilizes partition-based replication to reduce the occurrence of distributed transactions.","Lion aims to assign a node with one replica from each partition involved in a given transaction's read or write operations.","To ensure such a node is available, we propose an adaptive replica provision mechanism, enhanced with an LSTM-based workload prediction algorithm, to determine the appropriate node for locating replicas of co-accessed partitions.","The adaptation of replica placement is conducted preemptively and asynchronously, thereby minimizing its impact on performance.","By employing this adaptive replica placement strategy, we ensure that the majority of transactions can be efficiently processed on a single node without additional overhead.","Only a small fraction of transactions will need to be treated as regular distributed transactions when such a node is unavailable.","Consequently, Lion effectively minimizes distributed transactions while avoiding any disruption caused by data migration or the creation of a super node.","We conduct extensive experiments to compare Lion against various transaction processing protocols.","The results show that Lion achieves up to 2.7x higher throughput and 76.4% better scalability against these state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.11221v1","category":"cs.DC"}
{"created":"2024-03-17 13:38:20","title":"Non-Fermi liquid behavior of scattering rate in three-orbital Emery model","abstract":"Motivated by the recent findings on the $T$-linear electronic scattering rate in the two-dimensional Hubbard model, we have investigated the three-orbital Emery model and its temperature-dependent electronic and quasiparticle scattering rates by adopting dynamical cluster quantum Monte Carlo simulations. By focusing on two characteristic site energies $\\epsilon_p$ of O-2$p$ orbital relevant to cuprates and nickelates separately, our exploration discovered that, for $\\epsilon_p=3.24$ relevant to cuprates, the scattering rate can exhibit a linear-$T$ dependence at low temperature for a range of intermediate densities. In contrast, for larger $\\epsilon_p=6.0$ presumably relevant to nickelates, a wide range of densities support a downturn of the scattering rate below the temperature scale $T\\sim0.1$ with possibly two consecutive nearly linear-$T$ regimes connected via a smooth crossover around $T\\sim0.1$. Furthermore, the temperature dependent quasiparticle scattering rate generically departs from the unity slope as predicted by the Planckian dissipation theory. Our presented work provides valuable insights on the extensively studied three-orbital Emery model, particularly on the quantitative examination of non-Fermi liquid features of scattering rates.","sentences":["Motivated by the recent findings on the $T$-linear electronic scattering rate in the two-dimensional Hubbard model, we have investigated the three-orbital Emery model and its temperature-dependent electronic and quasiparticle scattering rates by adopting dynamical cluster quantum Monte Carlo simulations.","By focusing on two characteristic site energies $\\epsilon_p$ of O-2$p$ orbital relevant to cuprates and nickelates separately, our exploration discovered that, for $\\epsilon_p=3.24$ relevant to cuprates, the scattering rate can exhibit a linear-$T$ dependence at low temperature for a range of intermediate densities.","In contrast, for larger $\\epsilon_p=6.0$ presumably relevant to nickelates, a wide range of densities support a downturn of the scattering rate below the temperature scale $T\\sim0.1$ with possibly two consecutive nearly linear-$T$ regimes connected via a smooth crossover around $T\\sim0.1$. Furthermore, the temperature dependent quasiparticle scattering rate generically departs from the unity slope as predicted by the Planckian dissipation theory.","Our presented work provides valuable insights on the extensively studied three-orbital Emery model, particularly on the quantitative examination of non-Fermi liquid features of scattering rates."],"url":"http://arxiv.org/abs/2403.11218v1","category":"cond-mat.str-el"}
{"created":"2024-03-17 13:14:09","title":"CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution","abstract":"Encrypted network traffic Classification tackles the problem from different approaches and with different goals. One of the common approaches is using Machine learning or Deep Learning-based solutions on a fixed number of classes, leading to misclassification when an unknown class is given as input. One of the solutions for handling unknown classes is to retrain the model, however, retraining models every time they become obsolete is both resource and time-consuming. Therefore, there is a growing need to allow classification models to detect and adapt to new classes dynamically, without retraining, but instead able to detect new classes using few shots learning [1]. In this paper, we introduce Adaptive Classification By Retrieval CBR, a novel approach for encrypted network traffic classification. Our new approach is based on an ANN-based method, which allows us to effectively identify new and existing classes without retraining the model. The novel approach is simple, yet effective and achieved similar results to RF with up to 5% difference (usually less than that) in the classification tasks while having a slight decrease in the case of new samples (from new classes) without retraining. To summarize, the new method is a real-time classification, which can classify new classes without retraining. Furthermore, our solution can be used as a complementary solution alongside RF or any other machine/deep learning classification method, as an aggregated solution.","sentences":["Encrypted network traffic Classification tackles the problem from different approaches and with different goals.","One of the common approaches is using Machine learning or Deep Learning-based solutions on a fixed number of classes, leading to misclassification when an unknown class is given as input.","One of the solutions for handling unknown classes is to retrain the model, however, retraining models every time they become obsolete is both resource and time-consuming.","Therefore, there is a growing need to allow classification models to detect and adapt to new classes dynamically, without retraining, but instead able to detect new classes using few shots learning [1].","In this paper, we introduce Adaptive Classification By Retrieval CBR, a novel approach for encrypted network traffic classification.","Our new approach is based on an ANN-based method, which allows us to effectively identify new and existing classes without retraining the model.","The novel approach is simple, yet effective and achieved similar results to RF with up to 5% difference (usually less than that) in the classification tasks while having a slight decrease in the case of new samples (from new classes) without retraining.","To summarize, the new method is a real-time classification, which can classify new classes without retraining.","Furthermore, our solution can be used as a complementary solution alongside RF or any other machine/deep learning classification method, as an aggregated solution."],"url":"http://arxiv.org/abs/2403.11206v1","category":"cs.LG"}
{"created":"2024-03-17 12:49:02","title":"TAG: Guidance-free Open-Vocabulary Semantic Segmentation","abstract":"Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category. However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training. Furthermore, because supervised learning uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones. Unsupervised and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance. In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations. It retrieves class labels from an external database, providing flexibility to adapt to new scenarios. Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC. All code and data will be released at https://github.com/Valkyrja3607/TAG.","sentences":["Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category.","However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training.","Furthermore, because supervised learning uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones.","Unsupervised and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance.","In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation.","TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations.","It retrieves class labels from an external database, providing flexibility to adapt to new scenarios.","Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC.","All code and data will be released at https://github.com/Valkyrja3607/TAG."],"url":"http://arxiv.org/abs/2403.11197v1","category":"cs.CV"}
{"created":"2024-03-17 12:26:23","title":"Boosting Semi-Supervised Temporal Action Localization by Learning from Non-Target Classes","abstract":"The crux of semi-supervised temporal action localization (SS-TAL) lies in excavating valuable information from abundant unlabeled videos. However, current approaches predominantly focus on building models that are robust to the error-prone target class (i.e, the predicted class with the highest confidence) while ignoring informative semantics within non-target classes. This paper approaches SS-TAL from a novel perspective by advocating for learning from non-target classes, transcending the conventional focus solely on the target class. The proposed approach involves partitioning the label space of the predicted class distribution into distinct subspaces: target class, positive classes, negative classes, and ambiguous classes, aiming to mine both positive and negative semantics that are absent in the target class, while excluding ambiguous classes. To this end, we first devise innovative strategies to adaptively select high-quality positive and negative classes from the label space, by modeling both the confidence and rank of a class in relation to those of the target class. Then, we introduce novel positive and negative losses designed to guide the learning process, pushing predictions closer to positive classes and away from negative classes. Finally, the positive and negative processes are integrated into a hybrid positive-negative learning framework, facilitating the utilization of non-target classes in both labeled and unlabeled videos. Experimental results on THUMOS14 and ActivityNet v1.3 demonstrate the superiority of the proposed method over prior state-of-the-art approaches.","sentences":["The crux of semi-supervised temporal action localization (SS-TAL) lies in excavating valuable information from abundant unlabeled videos.","However, current approaches predominantly focus on building models that are robust to the error-prone target class (i.e, the predicted class with the highest confidence) while ignoring informative semantics within non-target classes.","This paper approaches SS-TAL from a novel perspective by advocating for learning from non-target classes, transcending the conventional focus solely on the target class.","The proposed approach involves partitioning the label space of the predicted class distribution into distinct subspaces: target class, positive classes, negative classes, and ambiguous classes, aiming to mine both positive and negative semantics that are absent in the target class, while excluding ambiguous classes.","To this end, we first devise innovative strategies to adaptively select high-quality positive and negative classes from the label space, by modeling both the confidence and rank of a class in relation to those of the target class.","Then, we introduce novel positive and negative losses designed to guide the learning process, pushing predictions closer to positive classes and away from negative classes.","Finally, the positive and negative processes are integrated into a hybrid positive-negative learning framework, facilitating the utilization of non-target classes in both labeled and unlabeled videos.","Experimental results on THUMOS14 and ActivityNet v1.3 demonstrate the superiority of the proposed method over prior state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.11189v1","category":"cs.CV"}
{"created":"2024-03-17 12:14:34","title":"DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation","abstract":"Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.","sentences":["Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart.","Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance.","Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models.","To this end, we propose a dual student framework with trustworthy progressive learning (DuPL).","Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net.","The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels.","In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy.","Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS.","Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel.","Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets.","Code is available at https://github.com/Wu0409/DuPL."],"url":"http://arxiv.org/abs/2403.11184v1","category":"cs.CV"}
{"created":"2024-03-17 10:26:41","title":"Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption","abstract":"The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities. Our community has made substantial contributions to addressing this challenge, proposing various approaches such as federated learning (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC). However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider). While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem.   In this paper, we present Pencil, the first private training framework for collaborative learning that simultaneously offers data privacy, model privacy, and extensibility to multiple data providers, without relying on the non-colluding assumption. Our fundamental design principle is to construct the n-party collaborative training protocol based on an efficient two-party protocol, and meanwhile ensuring that switching to different data providers during model training introduces no extra cost. We introduce several novel cryptographic protocols to realize this design principle and conduct a rigorous security and privacy analysis. Our comprehensive evaluations of Pencil demonstrate that (i) models trained in plaintext and models trained privately using Pencil exhibit nearly identical test accuracies; (ii) The training overhead of Pencil is greatly reduced: Pencil achieves 10 ~ 260x higher throughput and 2 orders of magnitude less communication than prior art; (iii) Pencil is resilient against both existing and adaptive (white-box) attacks.","sentences":["The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities.","Our community has made substantial contributions to addressing this challenge, proposing various approaches such as federated learning (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC).","However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider).","While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem.   ","In this paper, we present Pencil, the first private training framework for collaborative learning that simultaneously offers data privacy, model privacy, and extensibility to multiple data providers, without relying on the non-colluding assumption.","Our fundamental design principle is to construct the n-party collaborative training protocol based on an efficient two-party protocol, and meanwhile ensuring that switching to different data providers during model training introduces no extra cost.","We introduce several novel cryptographic protocols to realize this design principle and conduct a rigorous security and privacy analysis.","Our comprehensive evaluations of Pencil demonstrate that (i) models trained in plaintext and models trained privately using Pencil exhibit nearly identical test accuracies; (ii) The training overhead of Pencil is greatly reduced: Pencil achieves 10 ~ 260x higher throughput and 2 orders of magnitude less communication than prior art; (iii) Pencil is resilient against both existing and adaptive (white-box) attacks."],"url":"http://arxiv.org/abs/2403.11166v1","category":"cs.CR"}
{"created":"2024-03-17 08:51:17","title":"Toward Adaptive Cooperation: Model-Based Shared Control Using LQ-Differential Games","abstract":"This paper introduces a novel model-based adaptive shared control to allow for the identification and design challenge for shared-control systems, in which humans and automation share control tasks. The main challenge is the adaptive behavior of the human in such shared control interactions. Consequently, merely identifying human behavior without considering automation is insufficient and often leads to inadequate automation design. Therefore, this paper proposes a novel solution involving online identification of the human and the adaptation of shared control using Linear-Quadratic differential games. The effectiveness of the proposed online adaptation is analyzed in simulations and compared with a non-adaptive shared control from the state of the art. Finally, the proposed approach is tested through human-in-the-loop experiments, highlighting its suitability for real-time applications.","sentences":["This paper introduces a novel model-based adaptive shared control to allow for the identification and design challenge for shared-control systems, in which humans and automation share control tasks.","The main challenge is the adaptive behavior of the human in such shared control interactions.","Consequently, merely identifying human behavior without considering automation is insufficient and often leads to inadequate automation design.","Therefore, this paper proposes a novel solution involving online identification of the human and the adaptation of shared control using Linear-Quadratic differential games.","The effectiveness of the proposed online adaptation is analyzed in simulations and compared with a non-adaptive shared control from the state of the art.","Finally, the proposed approach is tested through human-in-the-loop experiments, highlighting its suitability for real-time applications."],"url":"http://arxiv.org/abs/2403.11146v1","category":"eess.SY"}
{"created":"2024-03-17 08:51:01","title":"A Challenge Dataset and Effective Models for Conversational Stance Detection","abstract":"Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called \\textbf{MT-CSD}), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (\\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of only 50.47\\%, highlighting the persistent challenges in conversational stance detection. Furthermore, our MT-CSD dataset serves as a valuable resource to catalyze advancements in cross-domain stance detection, where a classifier is adapted from a different yet related target. We believe that MT-CSD will contribute to advancing real-world applications of stance detection research. Our source code, data, and models are available at \\url{https://github.com/nfq729/MT-CSD}.","sentences":["Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions.","This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection.","In this paper, we introduce a new multi-turn conversation stance detection dataset (called \\textbf{MT-CSD}), which encompasses multiple targets for conversational stance detection.","To derive stances from this challenging dataset, we propose a global-local attention network (\\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data.","Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of only 50.47\\%, highlighting the persistent challenges in conversational stance detection.","Furthermore, our MT-CSD dataset serves as a valuable resource to catalyze advancements in cross-domain stance detection, where a classifier is adapted from a different yet related target.","We believe that MT-CSD will contribute to advancing real-world applications of stance detection research.","Our source code, data, and models are available at \\url{https://github.com/nfq729/MT-CSD}."],"url":"http://arxiv.org/abs/2403.11145v1","category":"cs.CL"}
{"created":"2024-03-17 07:47:26","title":"Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications","abstract":"Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing.","sentences":["Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications.","However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments.","Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks.","We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing.","Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner.","Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views.","This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding.","In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing."],"url":"http://arxiv.org/abs/2403.11131v1","category":"cs.CV"}
{"created":"2024-03-17 07:29:32","title":"GRA: Detecting Oriented Objects through Group-wise Rotating and Attention","abstract":"Oriented object detection, an emerging task in recent years, aims to identify and locate objects across varied orientations. This requires the detector to accurately capture the orientation information, which varies significantly within and across images. Despite the existing substantial efforts, simultaneously ensuring model effectiveness and parameter efficiency remains challenging in this scenario. In this paper, we propose a lightweight yet effective \\textbf{G}roup-wise \\textbf{R}otating and \\textbf{A}ttention (GRA) module to replace the convolution operations in backbone networks for oriented object detection. GRA can adaptively capture fine-grained features of objects with diverse orientations, comprising two key components: Group-wise Rotating and Group-wise Attention. Group-wise Rotating first divides the convolution kernel into groups, where each group extracts different object features by rotating at a specific angle according to the object orientation. Subsequently, Group-wise Attention is employed to adaptively enhance the object-related regions in the feature. The collaborative effort of these components enables GRA to effectively capture the various orientation information while maintaining parameter efficiency. Extensive experimental results demonstrate the superiority of our method. For example, GRA achieves a new state-of-the-art (SOTA) on the DOTA-v2.0 benchmark, while saving the parameters by nearly 50\\% compared to the previous SOTA method. Code will be released.","sentences":["Oriented object detection, an emerging task in recent years, aims to identify and locate objects across varied orientations.","This requires the detector to accurately capture the orientation information, which varies significantly within and across images.","Despite the existing substantial efforts, simultaneously ensuring model effectiveness and parameter efficiency remains challenging in this scenario.","In this paper, we propose a lightweight yet effective \\textbf{G}roup-wise \\textbf{R}otating and \\textbf{A}ttention (GRA) module to replace the convolution operations in backbone networks for oriented object detection.","GRA can adaptively capture fine-grained features of objects with diverse orientations, comprising two key components: Group-wise Rotating and Group-wise Attention.","Group-wise Rotating first divides the convolution kernel into groups, where each group extracts different object features by rotating at a specific angle according to the object orientation.","Subsequently, Group-wise Attention is employed to adaptively enhance the object-related regions in the feature.","The collaborative effort of these components enables GRA to effectively capture the various orientation information while maintaining parameter efficiency.","Extensive experimental results demonstrate the superiority of our method.","For example, GRA achieves a new state-of-the-art (SOTA) on the DOTA-v2.0 benchmark, while saving the parameters by nearly 50\\% compared to the previous SOTA method.","Code will be released."],"url":"http://arxiv.org/abs/2403.11127v1","category":"cs.CV"}
{"created":"2024-03-17 06:31:14","title":"Smart structural health monitoring (SHM) system for on-board localization of defects in pipes using torsional ultrasonic guided waves","abstract":"Most reported research for monitoring health of pipelines using ultrasonic guided waves (GW) typically utilize bulky piezoelectric transducer rings and laboratory-grade ultrasonic non-destructive testing (NDT) equipment. Consequently, the translation of these approaches from laboratory settings to field-deployable systems for real-time structural health monitoring (SHM) becomes challenging. In this work, we present an innovative algorithm for damage identification and localization in pipes, implemented on a compact FPGA-based smart GW-SHM system. The custom-designed board, featuring a Xilinx Artix-7 FPGA and front-end electronics, is capable of actuating the PZT thickness shear mode transducers, data acquisition and recording from PZT sensors and generating a damage index (DI) map for localizing the damage on the structure. The algorithm is a variation of the common source method adapted for cylindrical geometry. The utility of the algorithm is demonstrated for detection and localization of defects such as notch and mass loading on a steel pipe, through extensive finite element (FE) method simulations. Experimental results obtained using a C-clamp for applying mass loading on the pipe show good agreement with the FE simulations. The localization error values for experimental data analyzed using C code on a processor implemented on the FPGA are consistent with algorithm results generated on a computer running MATLAB code. The system presented in this study is suitable for a wide range of GW-SHM applications, especially in cost-sensitive scenarios that benefit from on-node signal processing over cloud-based solutions.","sentences":["Most reported research for monitoring health of pipelines using ultrasonic guided waves (GW) typically utilize bulky piezoelectric transducer rings and laboratory-grade ultrasonic non-destructive testing (NDT) equipment.","Consequently, the translation of these approaches from laboratory settings to field-deployable systems for real-time structural health monitoring (SHM) becomes challenging.","In this work, we present an innovative algorithm for damage identification and localization in pipes, implemented on a compact FPGA-based smart GW-SHM system.","The custom-designed board, featuring a Xilinx Artix-7 FPGA and front-end electronics, is capable of actuating the PZT thickness shear mode transducers, data acquisition and recording from PZT sensors and generating a damage index (DI) map for localizing the damage on the structure.","The algorithm is a variation of the common source method adapted for cylindrical geometry.","The utility of the algorithm is demonstrated for detection and localization of defects such as notch and mass loading on a steel pipe, through extensive finite element (FE) method simulations.","Experimental results obtained using a C-clamp for applying mass loading on the pipe show good agreement with the FE simulations.","The localization error values for experimental data analyzed using C code on a processor implemented on the FPGA are consistent with algorithm results generated on a computer running MATLAB code.","The system presented in this study is suitable for a wide range of GW-SHM applications, especially in cost-sensitive scenarios that benefit from on-node signal processing over cloud-based solutions."],"url":"http://arxiv.org/abs/2403.11110v1","category":"eess.SP"}
{"created":"2024-03-17 06:21:21","title":"Self-supervised co-salient object detection via feature correspondence at multiple scales","abstract":"Our paper introduces a novel two-stage self-supervised approach for detecting co-occurring salient objects (CoSOD) in image groups without requiring segmentation annotations. Unlike existing unsupervised methods that rely solely on patch-level information (e.g. clustering patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance. In the first stage, we train a self-supervised network that detects co-salient regions by computing local patch-level feature correspondences across images. We obtain the segmentation predictions using confidence-based adaptive thresholding. In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the cross-attention maps (from the previous stage). Extensive experiments on three CoSOD benchmark datasets show that our self-supervised model outperforms the corresponding state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably, our self-supervised model also outperforms several recent fully supervised CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model has a 4.6% F-measure gain over a recent supervised CoSOD model).","sentences":["Our paper introduces a novel two-stage self-supervised approach for detecting co-occurring salient objects (CoSOD) in image groups without requiring segmentation annotations.","Unlike existing unsupervised methods that rely solely on patch-level information (e.g. clustering patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance.","In the first stage, we train a self-supervised network that detects co-salient regions by computing local patch-level feature correspondences across images.","We obtain the segmentation predictions using confidence-based adaptive thresholding.","In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the cross-attention maps (from the previous stage).","Extensive experiments on three CoSOD benchmark datasets show that our self-supervised model outperforms the corresponding state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model).","Notably, our self-supervised model also outperforms several recent fully supervised CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model has a 4.6% F-measure gain over a recent supervised CoSOD model)."],"url":"http://arxiv.org/abs/2403.11107v1","category":"cs.CV"}
{"created":"2024-03-17 05:22:01","title":"Nonlinear Self-Interference Cancellation With Learnable Orthonormal Polynomials for Full-Duplex Wireless Systems","abstract":"Nonlinear self-interference cancellation (SIC) is essential for full-duplex communication systems, which can offer twice the spectral efficiency of traditional half-duplex systems. The challenge of nonlinear SIC is similar to the classic problem of system identification in adaptive filter theory, whose crux lies in identifying the optimal nonlinear basis functions for a nonlinear system. This becomes especially difficult when the system input has a non-stationary distribution. In this paper, we propose a novel algorithm for nonlinear digital SIC that adaptively constructs orthonormal polynomial basis functions according to the non-stationary moments of the transmit signal. By combining these basis functions with the least mean squares (LMS) algorithm, we introduce a new SIC technique, called as the adaptive orthonormal polynomial LMS (AOP-LMS) algorithm. To reduce computational complexity for practical systems, we augment our approach with a precomputed look-up table, which maps a given modulation and coding scheme to its corresponding basis functions. Numerical simulation indicates that our proposed method surpasses existing state-of-the-art SIC algorithms in terms of convergence speed and mean squared error when the transmit signal is non-stationary, such as with adaptive modulation and coding. Experimental evaluation with a wireless testbed confirms that our proposed approach outperforms existing digital SIC algorithms.","sentences":["Nonlinear self-interference cancellation (SIC) is essential for full-duplex communication systems, which can offer twice the spectral efficiency of traditional half-duplex systems.","The challenge of nonlinear SIC is similar to the classic problem of system identification in adaptive filter theory, whose crux lies in identifying the optimal nonlinear basis functions for a nonlinear system.","This becomes especially difficult when the system input has a non-stationary distribution.","In this paper, we propose a novel algorithm for nonlinear digital SIC that adaptively constructs orthonormal polynomial basis functions according to the non-stationary moments of the transmit signal.","By combining these basis functions with the least mean squares (LMS) algorithm, we introduce a new SIC technique, called as the adaptive orthonormal polynomial LMS (AOP-LMS) algorithm.","To reduce computational complexity for practical systems, we augment our approach with a precomputed look-up table, which maps a given modulation and coding scheme to its corresponding basis functions.","Numerical simulation indicates that our proposed method surpasses existing state-of-the-art SIC algorithms in terms of convergence speed and mean squared error when the transmit signal is non-stationary, such as with adaptive modulation and coding.","Experimental evaluation with a wireless testbed confirms that our proposed approach outperforms existing digital SIC algorithms."],"url":"http://arxiv.org/abs/2403.11094v1","category":"eess.SP"}
{"created":"2024-03-17 05:00:40","title":"Multitask frame-level learning for few-shot sound event detection","abstract":"This paper focuses on few-shot Sound Event Detection (SED), which aims to automatically recognize and classify sound events with limited samples. However, prevailing methods methods in few-shot SED predominantly rely on segment-level predictions, which often providing detailed, fine-grained predictions, particularly for events of brief duration. Although frame-level prediction strategies have been proposed to overcome these limitations, these strategies commonly face difficulties with prediction truncation caused by background noise. To alleviate this issue, we introduces an innovative multitask frame-level SED framework. In addition, we introduce TimeFilterAug, a linear timing mask for data augmentation, to increase the model's robustness and adaptability to diverse acoustic environments. The proposed method achieves a F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event detection category of the Detection and Classification of Acoustic Scenes and Events Challenge 2023.","sentences":["This paper focuses on few-shot Sound Event Detection (SED), which aims to automatically recognize and classify sound events with limited samples.","However, prevailing methods methods in few-shot SED predominantly rely on segment-level predictions, which often providing detailed, fine-grained predictions, particularly for events of brief duration.","Although frame-level prediction strategies have been proposed to overcome these limitations, these strategies commonly face difficulties with prediction truncation caused by background noise.","To alleviate this issue, we introduces an innovative multitask frame-level SED framework.","In addition, we introduce TimeFilterAug, a linear timing mask for data augmentation, to increase the model's robustness and adaptability to diverse acoustic environments.","The proposed method achieves a F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event detection category of the Detection and Classification of Acoustic Scenes and Events Challenge 2023."],"url":"http://arxiv.org/abs/2403.11091v1","category":"cs.SD"}
{"created":"2024-03-17 04:33:00","title":"High Performance Graphene Integrated Photonics Platform Enabled by Gold-assisted Transfer","abstract":"Graphene is promising for nanoscale, efficient, ultra-fast photo- and opto-electronic devices because of its remarkable electrical and optical properties, such as fast electron relaxation and heat dissipation. Here, we realize high-performance graphene integrated photonics platform enabled by gold-assisted transfer. Thanks to our optimized transfer technique, we fabricate and demonstrate (1) a microscale thermo-optic modulator with a tuning efficiency of 0.037 nm/mW and high heating performance of 67.4 K${\\mu}m^{3}mW^{-1}$ on a small active area of 7.54 ${\\mu}m^{2}$ and (2) a graphene electro-absorption modulator featuring an high modulation bandwidth up to 26.8 GHz and a high-speed data rate reaching 48 Gb/s, and (3) a graphene Mach-Zehnder interferometer modulator with a high normalized modulation efficiency of 0.027 dBV$^{-1}{\\mu}m^{-1}$. Our graphene integrated photonics platform has far superior performances compared to state of the art in terms of efficiency, low process complexity, and compact device footage. Thus, our approach and results provide the background for the realization of high-performance integrated photonic circuits with CMOS compatibility.","sentences":["Graphene is promising for nanoscale, efficient, ultra-fast photo- and opto-electronic devices because of its remarkable electrical and optical properties, such as fast electron relaxation and heat dissipation.","Here, we realize high-performance graphene integrated photonics platform enabled by gold-assisted transfer.","Thanks to our optimized transfer technique, we fabricate and demonstrate (1) a microscale thermo-optic modulator with a tuning efficiency of 0.037 nm/mW and high heating performance of 67.4 K${\\mu}m^{3}mW^{-1}$ on a small active area of 7.54 ${\\mu}m^{2}$ and (2) a graphene electro-absorption modulator featuring an high modulation bandwidth up to 26.8 GHz and a high-speed data rate reaching 48 Gb/s, and (3) a graphene Mach-Zehnder interferometer modulator with a high normalized modulation efficiency of 0.027 dBV$^{-1}{\\mu}m^{-1}$.","Our graphene integrated photonics platform has far superior performances compared to state of the art in terms of efficiency, low process complexity, and compact device footage.","Thus, our approach and results provide the background for the realization of high-performance integrated photonic circuits with CMOS compatibility."],"url":"http://arxiv.org/abs/2403.11084v1","category":"physics.optics"}
{"created":"2024-03-17 04:08:58","title":"Adaptive Semantic-Enhanced Denoising Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution","abstract":"Remote sensing image super-resolution (SR) is a crucial task to restore high-resolution (HR) images from low-resolution (LR) observations. Recently, the Denoising Diffusion Probabilistic Model (DDPM) has shown promising performance in image reconstructions by overcoming problems inherent in generative models, such as over-smoothing and mode collapse. However, the high-frequency details generated by DDPM often suffer from misalignment with HR images due to the model's tendency to overlook long-range semantic contexts. This is attributed to the widely used U-Net decoder in the conditional noise predictor, which tends to overemphasize local information, leading to the generation of noises with significant variances during the prediction process. To address these issues, an adaptive semantic-enhanced DDPM (ASDDPM) is proposed to enhance the detail-preserving capability of the DDPM by incorporating low-frequency semantic information provided by the Transformer. Specifically, a novel adaptive diffusion Transformer decoder (ADTD) is developed to bridge the semantic gap between the encoder and decoder through regulating the noise prediction with the global contextual relationships and long-range dependencies in the diffusion process. Additionally, a residual feature fusion strategy establishes information exchange between the two decoders at multiple levels. As a result, the predicted noise generated by our approach closely approximates that of the real noise distribution.Extensive experiments on two SR and two semantic segmentation datasets confirm the superior performance of the proposed ASDDPM in both SR and the subsequent downstream applications. The source code will be available at https://github.com/littlebeen/ASDDPM-Adaptive-Semantic-Enhanced-DDPM.","sentences":["Remote sensing image super-resolution (SR) is a crucial task to restore high-resolution (HR) images from low-resolution (LR) observations.","Recently, the Denoising Diffusion Probabilistic Model (DDPM) has shown promising performance in image reconstructions by overcoming problems inherent in generative models, such as over-smoothing and mode collapse.","However, the high-frequency details generated by DDPM often suffer from misalignment with HR images due to the model's tendency to overlook long-range semantic contexts.","This is attributed to the widely used U-Net decoder in the conditional noise predictor, which tends to overemphasize local information, leading to the generation of noises with significant variances during the prediction process.","To address these issues, an adaptive semantic-enhanced DDPM (ASDDPM) is proposed to enhance the detail-preserving capability of the DDPM by incorporating low-frequency semantic information provided by the Transformer.","Specifically, a novel adaptive diffusion Transformer decoder (ADTD) is developed to bridge the semantic gap between the encoder and decoder through regulating the noise prediction with the global contextual relationships and long-range dependencies in the diffusion process.","Additionally, a residual feature fusion strategy establishes information exchange between the two decoders at multiple levels.","As a result, the predicted noise generated by our approach closely approximates that of the real noise distribution.","Extensive experiments on two SR and two semantic segmentation datasets confirm the superior performance of the proposed ASDDPM in both SR and the subsequent downstream applications.","The source code will be available at https://github.com/littlebeen/ASDDPM-Adaptive-Semantic-Enhanced-DDPM."],"url":"http://arxiv.org/abs/2403.11078v1","category":"eess.IV"}
{"created":"2024-03-17 03:56:15","title":"Unbiased Parameter Estimation via DREM with Annihilators","abstract":"In the adaptive control theory, the dynamic regressor extension and mixing (DREM) procedure has become widespread as it allows one to describe a variety of adaptive control problems in unified terms of the parameter estimation problem of a regression equation with a scalar regressor. However, being applied when the system/parameterization is affected by perturbations, in general, it asymptotically provides only biased estimates. In this paper, based on the bias-eliminated least-squares (BELS) approach, a modification of the DREM procedure is proposed that allows one to annihilate perturbations asymptotically and, consequently, asymptotically obtain unbiased estimates. The theoretical results are supported with mathematical modelling and can be used to design adaptive observers and control systems.","sentences":["In the adaptive control theory, the dynamic regressor extension and mixing (DREM) procedure has become widespread as it allows one to describe a variety of adaptive control problems in unified terms of the parameter estimation problem of a regression equation with a scalar regressor.","However, being applied when the system/parameterization is affected by perturbations, in general, it asymptotically provides only biased estimates.","In this paper, based on the bias-eliminated least-squares (BELS) approach, a modification of the DREM procedure is proposed that allows one to annihilate perturbations asymptotically and, consequently, asymptotically obtain unbiased estimates.","The theoretical results are supported with mathematical modelling and can be used to design adaptive observers and control systems."],"url":"http://arxiv.org/abs/2403.11076v1","category":"eess.SY"}
{"created":"2024-03-17 00:51:59","title":"Endora: Video Generation Models as Endoscopy Simulators","abstract":"Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for machine learning. Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.This paper introduces \\model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes. We present a novel generative model design that integrates a meticulously crafted spatial-temporal video transformer with advanced 2D vision foundation model priors, explicitly modeling spatial-temporal dynamics during video generation. We also pioneer the first public benchmark for endoscopy simulation with video generation models, adapting existing state-of-the-art methods for this endeavor.Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing. Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency. In a nutshell, Endora marks a notable breakthrough in the deployment of generative AI for clinical endoscopy research, setting a substantial stage for further advances in medical content generation. For more details, please visit our project page: https://endora-medvidgen.github.io/.","sentences":["Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for machine learning.","Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.","This paper introduces \\model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes.","We present a novel generative model design that integrates a meticulously crafted spatial-temporal video transformer with advanced 2D vision foundation model priors, explicitly modeling spatial-temporal dynamics during video generation.","We also pioneer the first public benchmark for endoscopy simulation with video generation models, adapting existing state-of-the-art methods for this endeavor.","Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing.","Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency.","In a nutshell, Endora marks a notable breakthrough in the deployment of generative AI for clinical endoscopy research, setting a substantial stage for further advances in medical content generation.","For more details, please visit our project page: https://endora-medvidgen.github.io/."],"url":"http://arxiv.org/abs/2403.11050v1","category":"cs.CV"}
{"created":"2024-03-16 17:02:50","title":"Task-Aware Low-Rank Adaptation of Segment Anything Model","abstract":"The Segment Anything Model (SAM), with its remarkable zero-shot capability, has been proven to be a powerful foundation model for image segmentation tasks, which is an important task in computer vision. However, the transfer of its rich semantic information to multiple different downstream tasks remains unexplored. In this paper, we propose the Task-Aware Low-Rank Adaptation (TA-LoRA) method, which enables SAM to work as a foundation model for multi-task learning. Specifically, TA-LoRA injects an update parameter tensor into each layer of the encoder in SAM and leverages a low-rank tensor decomposition method to incorporate both task-shared and task-specific information. Furthermore, we introduce modified SAM (mSAM) for multi-task learning where we remove the prompt encoder of SAM and use task-specific no mask embeddings and mask decoder for each task. Extensive experiments conducted on benchmark datasets substantiate the efficacy of TA-LoRA in enhancing the performance of mSAM across multiple downstream tasks.","sentences":["The Segment Anything Model (SAM), with its remarkable zero-shot capability, has been proven to be a powerful foundation model for image segmentation tasks, which is an important task in computer vision.","However, the transfer of its rich semantic information to multiple different downstream tasks remains unexplored.","In this paper, we propose the Task-Aware Low-Rank Adaptation (TA-LoRA) method, which enables SAM to work as a foundation model for multi-task learning.","Specifically, TA-LoRA injects an update parameter tensor into each layer of the encoder in SAM and leverages a low-rank tensor decomposition method to incorporate both task-shared and task-specific information.","Furthermore, we introduce modified SAM (mSAM) for multi-task learning where we remove the prompt encoder of SAM and use task-specific no mask embeddings and mask decoder for each task.","Extensive experiments conducted on benchmark datasets substantiate the efficacy of TA-LoRA in enhancing the performance of mSAM across multiple downstream tasks."],"url":"http://arxiv.org/abs/2403.10971v1","category":"cs.CV"}
{"created":"2024-03-16 15:29:40","title":"Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics","abstract":"Model-based controllers using a linearized model around the system's equilibrium point is a common approach in the control of a wheeled humanoid due to their less computational load and ease of stability analysis. However, controlling a wheeled humanoid robot while it lifts an unknown object presents significant challenges, primarily due to the lack of knowledge in object dynamics. This paper presents a framework designed for predicting the new equilibrium point explicitly to control a wheeled-legged robot with unknown dynamics. We estimated the total mass and center of mass of the system from its response to initially unknown dynamics, then calculated the new equilibrium point accordingly. To avoid using additional sensors (e.g., force torque sensor) and reduce the effort of obtaining expensive real data, a data-driven approach is utilized with a novel real-to-sim adaptation. A more accurate nonlinear dynamics model, offering a closer representation of real-world physics, is injected into a rigid-body simulation for real-to-sim adaptation. The nonlinear dynamics model parameters were optimized using Particle Swarm Optimization. The efficacy of this framework was validated on a physical wheeled inverted pendulum, a simplified model of a wheeled-legged robot. The experimental results indicate that employing a more precise analytical model with optimized parameters significantly reduces the gap between simulation and reality, thus improving the efficiency of a model-based controller in controlling a wheeled robot with unknown dynamics.","sentences":["Model-based controllers using a linearized model around the system's equilibrium point is a common approach in the control of a wheeled humanoid due to their less computational load and ease of stability analysis.","However, controlling a wheeled humanoid robot while it lifts an unknown object presents significant challenges, primarily due to the lack of knowledge in object dynamics.","This paper presents a framework designed for predicting the new equilibrium point explicitly to control a wheeled-legged robot with unknown dynamics.","We estimated the total mass and center of mass of the system from its response to initially unknown dynamics, then calculated the new equilibrium point accordingly.","To avoid using additional sensors (e.g., force torque sensor) and reduce the effort of obtaining expensive real data, a data-driven approach is utilized with a novel real-to-sim adaptation.","A more accurate nonlinear dynamics model, offering a closer representation of real-world physics, is injected into a rigid-body simulation for real-to-sim adaptation.","The nonlinear dynamics model parameters were optimized using Particle Swarm Optimization.","The efficacy of this framework was validated on a physical wheeled inverted pendulum, a simplified model of a wheeled-legged robot.","The experimental results indicate that employing a more precise analytical model with optimized parameters significantly reduces the gap between simulation and reality, thus improving the efficiency of a model-based controller in controlling a wheeled robot with unknown dynamics."],"url":"http://arxiv.org/abs/2403.10948v1","category":"cs.RO"}
{"created":"2024-03-16 15:29:22","title":"The Fallacy of Minimizing Local Regret in the Sequential Task Setting","abstract":"In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret rates in the early tasks may lead to worse rates in the subsequent ones, even when the outcome distributions stay the same. To realize the optimal cumulative regret bound across all the tasks, the algorithm has to overly explore in the earlier tasks. This theoretical insight is practically significant, suggesting that due to unanticipated changes (e.g., rapid technological development or human-in-the-loop involvement) between tasks, the algorithm needs to explore more than it would in the usual stationary setting within each task. Such implication resonates with the common practice of using clipped policies in mobile health clinical trials and maintaining a fixed rate of $\\epsilon$-greedy exploration in robotic learning.","sentences":["In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret.","In a stationary setting, strong theoretical guarantees, like a sublinear ($\\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration.","However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks.","We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces.","Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret rates in the early tasks may lead to worse rates in the subsequent ones, even when the outcome distributions stay the same.","To realize the optimal cumulative regret bound across all the tasks, the algorithm has to overly explore in the earlier tasks.","This theoretical insight is practically significant, suggesting that due to unanticipated changes (e.g., rapid technological development or human-in-the-loop involvement) between tasks, the algorithm needs to explore more than it would in the usual stationary setting within each task.","Such implication resonates with the common practice of using clipped policies in mobile health clinical trials and maintaining a fixed rate of $\\epsilon$-greedy exploration in robotic learning."],"url":"http://arxiv.org/abs/2403.10946v1","category":"stat.ML"}
{"created":"2024-03-16 14:58:58","title":"ScanTalk: 3D Talking Heads from Unregistered Scans","abstract":"Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model will be made available.","sentences":["Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges.","Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate.","In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data.","Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations.","By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads.","Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques.","While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations.","Code for reproducing our results, and the pre-trained model will be made available."],"url":"http://arxiv.org/abs/2403.10942v1","category":"cs.CV"}
{"created":"2024-03-16 14:13:08","title":"Learning-Based Design of Off-Policy Gaussian Controllers: Integrating Model Predictive Control and Gaussian Process Regression","abstract":"This paper presents an off-policy Gaussian Predictive Control (GPC) framework aimed at solving optimal control problems with a smaller computational footprint, thereby facilitating real-time applicability while ensuring critical safety considerations. The proposed controller imitates classical control methodologies by modeling the optimization process through a Gaussian process and employs Gaussian Process Regression to learn from the Model Predictive Control (MPC) algorithm. Notably, the Gaussian Process setup does not incorporate a built-in model, enhancing its applicability to a broad range of control problems. We applied this framework experimentally to a differential drive mobile robot, tasking it with trajectory tracking and obstacle avoidance. Leveraging the off-policy aspect, the controller demonstrated adaptability to diverse trajectories and obstacle behaviors. Simulation experiments confirmed the effectiveness of the proposed GPC method, emphasizing its ability to learn the dynamics of optimal control strategies. Consequently, our findings highlight the significant potential of off-policy Gaussian Predictive Control in achieving real-time optimal control for handling of robotic systems in safety-critical scenarios.","sentences":["This paper presents an off-policy Gaussian Predictive Control (GPC) framework aimed at solving optimal control problems with a smaller computational footprint, thereby facilitating real-time applicability while ensuring critical safety considerations.","The proposed controller imitates classical control methodologies by modeling the optimization process through a Gaussian process and employs Gaussian Process Regression to learn from the Model Predictive Control (MPC) algorithm.","Notably, the Gaussian Process setup does not incorporate a built-in model, enhancing its applicability to a broad range of control problems.","We applied this framework experimentally to a differential drive mobile robot, tasking it with trajectory tracking and obstacle avoidance.","Leveraging the off-policy aspect, the controller demonstrated adaptability to diverse trajectories and obstacle behaviors.","Simulation experiments confirmed the effectiveness of the proposed GPC method, emphasizing its ability to learn the dynamics of optimal control strategies.","Consequently, our findings highlight the significant potential of off-policy Gaussian Predictive Control in achieving real-time optimal control for handling of robotic systems in safety-critical scenarios."],"url":"http://arxiv.org/abs/2403.10932v1","category":"cs.RO"}
{"created":"2024-03-16 13:26:33","title":"Batch-oriented Element-wise Approximate Activation for Privacy-Preserving Neural Networks","abstract":"Privacy-Preserving Neural Networks (PPNN) are advanced to perform inference without breaching user privacy, which can serve as an essential tool for medical diagnosis to simultaneously achieve big data utility and privacy protection. As one of the key techniques to enable PPNN, Fully Homomorphic Encryption (FHE) is facing a great challenge that homomorphic operations cannot be easily adapted for non-linear activation calculations. In this paper, batch-oriented element-wise data packing and approximate activation are proposed, which train linear low-degree polynomials to approximate the non-linear activation function - ReLU. Compared with other approximate activation methods, the proposed fine-grained, trainable approximation scheme can effectively reduce the accuracy loss caused by approximation errors. Meanwhile, due to element-wise data packing, a large batch of images can be packed and inferred concurrently, leading to a much higher utility ratio of ciphertext slots. Therefore, although the total inference time increases sharply, the amortized time for each image actually decreases, especially when the batch size increases. Furthermore, knowledge distillation is adopted in the training process to further enhance the inference accuracy. Experiment results show that when ciphertext inference is performed on 4096 input images, compared with the current most efficient channel-wise method, the inference accuracy is improved by 1.65%, and the amortized inference time is reduced by 99.5%.","sentences":["Privacy-Preserving Neural Networks (PPNN) are advanced to perform inference without breaching user privacy, which can serve as an essential tool for medical diagnosis to simultaneously achieve big data utility and privacy protection.","As one of the key techniques to enable PPNN, Fully Homomorphic Encryption (FHE) is facing a great challenge that homomorphic operations cannot be easily adapted for non-linear activation calculations.","In this paper, batch-oriented element-wise data packing and approximate activation are proposed, which train linear low-degree polynomials to approximate the non-linear activation function - ReLU.","Compared with other approximate activation methods, the proposed fine-grained, trainable approximation scheme can effectively reduce the accuracy loss caused by approximation errors.","Meanwhile, due to element-wise data packing, a large batch of images can be packed and inferred concurrently, leading to a much higher utility ratio of ciphertext slots.","Therefore, although the total inference time increases sharply, the amortized time for each image actually decreases, especially when the batch size increases.","Furthermore, knowledge distillation is adopted in the training process to further enhance the inference accuracy.","Experiment results show that when ciphertext inference is performed on 4096 input images, compared with the current most efficient channel-wise method, the inference accuracy is improved by 1.65%, and the amortized inference time is reduced by 99.5%."],"url":"http://arxiv.org/abs/2403.10920v1","category":"cs.CR"}
{"created":"2024-03-16 13:09:38","title":"Computationally feasible bounds for the free energy of nonequilibrium steady states, applied to simple models of heat conduction","abstract":"In this paper we study computationally feasible bounds for relative free energies between two many-particle systems. Specifically, we consider systems out of equilibrium that do not necessarily satisfy a fluctuation-dissipation relation, but that nevertheless admit a nonequilibrium steady state that is reached asymptotically in the long-time limit. The bounds that we suggest are based on the well-known Bogoliubov inequality and variants of Gibbs' and Donsker-Varadhan variational principles. As a general paradigm, we consider systems of oscillators coupled to heat baths at different temperatures. For such systems, we define the free energy of the system relative to any given reference system (that may or may not be in thermal equilibrium) in terms of the Kullback-Leibler divergence between steady states. By employing a two-sided Bogoliubov inequality and a mean-variance approximation of the free energy (or cumulant generating function, we can efficiently estimate the free energy cost needed in passing from the reference system to the system out of equilibrium (characterised by a temperature gradient). A specific test case to validate our bounds are harmonic oscillator chains with ends that are coupled to Langevin thermostats at different temperatures; such a system is simple enough to allow for analytic calculations and general enough to be used as a prototype to estimate, e.g., heat fluxes or interface effects in a larger class of nonequilibrium particle systems.","sentences":["In this paper we study computationally feasible bounds for relative free energies between two many-particle systems.","Specifically, we consider systems out of equilibrium that do not necessarily satisfy a fluctuation-dissipation relation, but that nevertheless admit a nonequilibrium steady state that is reached asymptotically in the long-time limit.","The bounds that we suggest are based on the well-known Bogoliubov inequality and variants of Gibbs' and Donsker-Varadhan variational principles.","As a general paradigm, we consider systems of oscillators coupled to heat baths at different temperatures.","For such systems, we define the free energy of the system relative to any given reference system (that may or may not be in thermal equilibrium) in terms of the Kullback-Leibler divergence between steady states.","By employing a two-sided Bogoliubov inequality and a mean-variance approximation of the free energy (or cumulant generating function, we can efficiently estimate the free energy cost needed in passing from the reference system to the system out of equilibrium (characterised by a temperature gradient).","A specific test case to validate our bounds are harmonic oscillator chains with ends that are coupled to Langevin thermostats at different temperatures; such a system is simple enough to allow for analytic calculations and general enough to be used as a prototype to estimate, e.g., heat fluxes or interface effects in a larger class of nonequilibrium particle systems."],"url":"http://arxiv.org/abs/2403.10918v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-16 12:56:06","title":"High order Well-Balanced Arbitrary-Lagrangian-Eulerian ADER discontinuous Galerkin schemes on general polygonal moving meshes","abstract":"In this work, we present a novel family of high order accurate numerical schemes for the solution of hyperbolic partial differential equations (PDEs) which combines several geometrical and physical structure preserving properties. First, we settle our methods in the Lagrangian framework, where each element of the mesh evolves following as close as possible the local fluid flow, so to reduce the numerical dissipation at contact waves and moving interfaces and to satisfy the Galilean and rotational invariance properties of the studied PDEs system. In particular, we choose the direct Arbitrary-Lagrangian-Eulerian (ALE) approach which, in order to always guarantee the high quality of the moving mesh, allows to combine the Lagrangian motion with mesh optimization techniques. The employed polygonal tessellation is thus regenerated at each time step, the previous one is connected with the new one by space-time control volumes, including hole-like sliver elements in correspondence of topology changes, over which we integrate a space-time divergence form of the original PDEs through a high order accurate ADER discontinuous Galerkin (DG) scheme. Mass conservation and adherence to the GCL condition are guaranteed by construction thanks to the integration over closed control volumes, and robustness over shock discontinuities is ensured by the use of an a posteriori subcell finite volume (FV) limiting technique.","sentences":["In this work, we present a novel family of high order accurate numerical schemes for the solution of hyperbolic partial differential equations (PDEs) which combines several geometrical and physical structure preserving properties.","First, we settle our methods in the Lagrangian framework, where each element of the mesh evolves following as close as possible the local fluid flow, so to reduce the numerical dissipation at contact waves and moving interfaces and to satisfy the Galilean and rotational invariance properties of the studied PDEs system.","In particular, we choose the direct Arbitrary-Lagrangian-Eulerian (ALE) approach which, in order to always guarantee the high quality of the moving mesh, allows to combine the Lagrangian motion with mesh optimization techniques.","The employed polygonal tessellation is thus regenerated at each time step, the previous one is connected with the new one by space-time control volumes, including hole-like sliver elements in correspondence of topology changes, over which we integrate a space-time divergence form of the original PDEs through a high order accurate ADER discontinuous Galerkin (DG) scheme.","Mass conservation and adherence to the GCL condition are guaranteed by construction thanks to the integration over closed control volumes, and robustness over shock discontinuities is ensured by the use of an a posteriori subcell finite volume (FV) limiting technique."],"url":"http://arxiv.org/abs/2403.10917v1","category":"math.NA"}
{"created":"2024-03-16 12:18:20","title":"Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation","abstract":"Test-time adaptation (TTA) addresses the unforeseen distribution shifts occurring during test time. In TTA, both performance and, memory and time consumption serve as crucial considerations. A recent diffusion-based TTA approach for restoring corrupted images involves image-level updates. However, using pixel space diffusion significantly increases resource requirements compared to conventional model updating TTA approaches, revealing limitations as a TTA method. To address this, we propose a novel TTA method by leveraging a latent diffusion model (LDM) based image editing model and fine-tuning it with our newly introduced corruption modeling scheme. This scheme enhances the robustness of the diffusion model against distribution shifts by creating (clean, corrupted) image pairs and fine-tuning the model to edit corrupted images into clean ones. Moreover, we introduce a distilled variant to accelerate the model for corruption editing using only 4 network function evaluations (NFEs). We extensively validated our method across various architectures and datasets including image and video domains. Our model achieves the best performance with a 100 times faster runtime than that of a diffusion-based baseline. Furthermore, it outpaces the speed of the model updating TTA method based on data augmentation threefold, rendering an image-level updating approach more practical.","sentences":["Test-time adaptation (TTA) addresses the unforeseen distribution shifts occurring during test time.","In TTA, both performance and, memory and time consumption serve as crucial considerations.","A recent diffusion-based TTA approach for restoring corrupted images involves image-level updates.","However, using pixel space diffusion significantly increases resource requirements compared to conventional model updating TTA approaches, revealing limitations as a TTA method.","To address this, we propose a novel TTA method by leveraging a latent diffusion model (LDM) based image editing model and fine-tuning it with our newly introduced corruption modeling scheme.","This scheme enhances the robustness of the diffusion model against distribution shifts by creating (clean, corrupted) image pairs and fine-tuning the model to edit corrupted images into clean ones.","Moreover, we introduce a distilled variant to accelerate the model for corruption editing using only 4 network function evaluations (NFEs).","We extensively validated our method across various architectures and datasets including image and video domains.","Our model achieves the best performance with a 100 times faster runtime than that of a diffusion-based baseline.","Furthermore, it outpaces the speed of the model updating TTA method based on data augmentation threefold, rendering an image-level updating approach more practical."],"url":"http://arxiv.org/abs/2403.10911v1","category":"cs.CV"}
{"created":"2024-03-16 10:54:51","title":"A simple and accurate method to determine fluid-crystal phase boundaries from direct coexistence simulations","abstract":"One method for computationally determining phase boundaries is to explicitly simulate a direct coexistence between the two phases of interest. Although this approach works very well for fluid-fluid coexistences, it is often considered to be less useful for fluid-crystal transitions, as additional care must be taken to prevent the simulation boundaries from imposing unwanted strains on the crystal phase. Here, we present a simple adaptation to the direct coexistence method that nonetheless allows us to obtain highly accurate predictions of fluid-crystal coexistence conditions. We test our approach on hard spheres, the screened Coulomb potential, and a 2D patchy-particle model. In all cases, we find excellent agreement between the direct coexistence approach and (much more cumbersome) free-energy calculation methods. Moreover, the method is sufficiently accurate to resolve the (tiny) free-energy difference between the face-centered cubic and hexagonally close-packed crystal of hard spheres in the thermodynamic limit. The simplicity of this method also ensures that it can be trivially implemented in essentially any simulation method or package. Hence, this approach provides an excellent alternative to free-energy based methods for the precise determination of phase boundaries.","sentences":["One method for computationally determining phase boundaries is to explicitly simulate a direct coexistence between the two phases of interest.","Although this approach works very well for fluid-fluid coexistences, it is often considered to be less useful for fluid-crystal transitions, as additional care must be taken to prevent the simulation boundaries from imposing unwanted strains on the crystal phase.","Here, we present a simple adaptation to the direct coexistence method that nonetheless allows us to obtain highly accurate predictions of fluid-crystal coexistence conditions.","We test our approach on hard spheres, the screened Coulomb potential, and a 2D patchy-particle model.","In all cases, we find excellent agreement between the direct coexistence approach and (much more cumbersome) free-energy calculation methods.","Moreover, the method is sufficiently accurate to resolve the (tiny) free-energy difference between the face-centered cubic and hexagonally close-packed crystal of hard spheres in the thermodynamic limit.","The simplicity of this method also ensures that it can be trivially implemented in essentially any simulation method or package.","Hence, this approach provides an excellent alternative to free-energy based methods for the precise determination of phase boundaries."],"url":"http://arxiv.org/abs/2403.10891v1","category":"cond-mat.soft"}
{"created":"2024-03-16 10:46:14","title":"LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival","abstract":"Image-text retrieval (ITR) plays a significant role in making informed decisions for various remote sensing (RS) applications. Nonetheless, creating ITR datasets containing vision and language modalities not only requires significant geo-spatial sampling area but also varing categories and detailed descriptions. To this end, we introduce an image caption dataset LuojiaHOG, which is geospatial-aware, label-extension-friendly and comprehensive-captioned. LuojiaHOG involves the hierarchical spatial sampling, extensible classification system to Open Geospatial Consortium (OGC) standards, and detailed caption generation. In addition, we propose a CLIP-based Image Semantic Enhancement Network (CISEN) to promote sophisticated ITR. CISEN consists of two components, namely dual-path knowledge transfer and progressive cross-modal feature fusion. Comprehensive statistics on LuojiaHOG reveal the richness in sampling diversity, labels quantity and descriptions granularity. The evaluation on LuojiaHOG is conducted across various state-of-the-art ITR models, including ALBEF, ALIGN, CLIP, FILIP, Wukong, GeoRSCLIP and CISEN. We use second- and third-level labels to evaluate these vision-language models through adapter-tuning and CISEN demonstrates superior performance. For instance, it achieves the highest scores with WMAP@5 of 88.47\\% and 87.28\\% on third-level ITR tasks, respectively. In particular, CISEN exhibits an improvement of approximately 1.3\\% and 0.9\\% in terms of WMAP@5 compared to its baseline. These findings highlight CISEN advancements accurately retrieving pertinent information across image and text. LuojiaHOG and CISEN can serve as a foundational resource for future RS image-text alignment research, facilitating a wide range of vision-language applications.","sentences":["Image-text retrieval (ITR) plays a significant role in making informed decisions for various remote sensing (RS) applications.","Nonetheless, creating ITR datasets containing vision and language modalities not only requires significant geo-spatial sampling area but also varing categories and detailed descriptions.","To this end, we introduce an image caption dataset LuojiaHOG, which is geospatial-aware, label-extension-friendly and comprehensive-captioned.","LuojiaHOG involves the hierarchical spatial sampling, extensible classification system to Open Geospatial Consortium (OGC) standards, and detailed caption generation.","In addition, we propose a CLIP-based Image Semantic Enhancement Network (CISEN) to promote sophisticated ITR. CISEN consists of two components, namely dual-path knowledge transfer and progressive cross-modal feature fusion.","Comprehensive statistics on LuojiaHOG reveal the richness in sampling diversity, labels quantity and descriptions granularity.","The evaluation on LuojiaHOG is conducted across various state-of-the-art ITR models, including ALBEF, ALIGN, CLIP, FILIP, Wukong, GeoRSCLIP and CISEN.","We use second- and third-level labels to evaluate these vision-language models through adapter-tuning and CISEN demonstrates superior performance.","For instance, it achieves the highest scores with WMAP@5 of 88.47\\% and 87.28\\% on third-level ITR tasks, respectively.","In particular, CISEN exhibits an improvement of approximately 1.3\\% and 0.9\\% in terms of WMAP@5 compared to its baseline.","These findings highlight CISEN advancements accurately retrieving pertinent information across image and text.","LuojiaHOG and CISEN can serve as a foundational resource for future RS image-text alignment research, facilitating a wide range of vision-language applications."],"url":"http://arxiv.org/abs/2403.10887v1","category":"cs.CV"}
{"created":"2024-03-18 12:22:11","title":"Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation","abstract":"We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and in a semi supervised, contrastive learning setting. Both times, we were able to outperform real valued counterparts as well as two baseline models: one with the uncompressed time-series as the input and the other with a regular downsampling using the mean. Further, we could improve the classification benchmark set by SimCLR-TS from 81.43% to 83.90%.","sentences":["We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series.","This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product.","To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space.","Furthermore, we investigate the connection between the derived update rules and automatic differentiation.","We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and in a semi supervised, contrastive learning setting.","Both times, we were able to outperform real valued counterparts as well as two baseline models: one with the uncompressed time-series as the input and the other with a regular downsampling using the mean.","Further, we could improve the classification benchmark set by SimCLR-TS from 81.43% to 83.90%."],"url":"http://arxiv.org/abs/2403.11722v1","category":"cs.LG"}
{"created":"2024-03-18 12:19:02","title":"Variational calculations of symmetric nuclear matter and pure neutron matter with the tensor-optimized Fermi Sphere (TOFS) method: many-body effects and short-range correlation","abstract":"The equations of state for symmetric nuclear matter and pure neutron matter are investigated with the tensor-optimized Fermi Sphere method (TOFS) up to the density $\\rho=0.5$~fm$^{-3}$. This method is based on a linked-cluster expansion theorem, and the energy per particle of nuclear matter ($E/A$) is calculated variationally with respect to the correlated nuclear matter wave function. We can study the density dependence of the many-body terms arising from the operator products, which contribute to $E/A$. In order to clarify the relation between the many-body effects and short-range correlation, we take the spin-isospin dependent central {\\it NN} interaction with a few GeV repulsion in the inner region. The EOS obtained by the TOFS method is reasonably reproduced, compared with other \\textit{ab initio} many-body methods. We found that the many-body terms from the 2-body to 6-body ones) give sizable effects on $E/A$ at higher density, and they play an important role in nuclear matter.","sentences":["The equations of state for symmetric nuclear matter and pure neutron matter are investigated with the tensor-optimized Fermi Sphere method (TOFS) up to the density $\\rho=0.5$~fm$^{-3}$. This method is based on a linked-cluster expansion theorem, and the energy per particle of nuclear matter ($E/A$) is calculated variationally with respect to the correlated nuclear matter wave function.","We can study the density dependence of the many-body terms arising from the operator products, which contribute to $E/A$. In order to clarify the relation between the many-body effects and short-range correlation, we take the spin-isospin dependent central {\\it NN} interaction with a few GeV repulsion in the inner region.","The EOS obtained by the TOFS method is reasonably reproduced, compared with other \\textit{ab initio} many-body methods.","We found that the many-body terms from the 2-body to 6-body ones) give sizable effects on $E/A$ at higher density, and they play an important role in nuclear matter."],"url":"http://arxiv.org/abs/2403.11717v1","category":"nucl-th"}
{"created":"2024-03-18 11:37:53","title":"Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates","abstract":"We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID to the true derivative, encompassing the best available rates in the smooth setting. We present illustrative experiments confirming our analysis.","sentences":["We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map.","This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks.","We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID).","A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore.","Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case.","We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator.","We establish rates for the convergence of NSID to the true derivative, encompassing the best available rates in the smooth setting.","We present illustrative experiments confirming our analysis."],"url":"http://arxiv.org/abs/2403.11687v1","category":"stat.ML"}
{"created":"2024-03-18 11:37:42","title":"Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding","abstract":"Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, with minimal modifications to the original Transformer architecture. Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets.","sentences":["Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science.","In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful.","However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention.","In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space.","We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer.","Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, with minimal modifications to the original Transformer architecture.","Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets."],"url":"http://arxiv.org/abs/2403.11686v1","category":"cs.LG"}
{"created":"2024-03-18 10:45:27","title":"Gridless 2D Recovery of Lines using the Sliding Frank-Wolfe Algorithm","abstract":"We present a new approach leveraging the Sliding Frank--Wolfe algorithm to address the challenge of line recovery in degraded images. Building upon advances in conditional gradient methods for sparse inverse problems with differentiable measurement models, we propose two distinct models tailored for line detection tasks within the realm of blurred line deconvolution and ridge detection of linear chirps in spectrogram images.","sentences":["We present a new approach leveraging the Sliding Frank--Wolfe algorithm to address the challenge of line recovery in degraded images.","Building upon advances in conditional gradient methods for sparse inverse problems with differentiable measurement models, we propose two distinct models tailored for line detection tasks within the realm of blurred line deconvolution and ridge detection of linear chirps in spectrogram images."],"url":"http://arxiv.org/abs/2403.11649v1","category":"cs.CV"}
{"created":"2024-03-18 10:44:27","title":"Vehicle single track modeling using physics guided neural differential equations","abstract":"In this paper, we follow the physics guided modeling approach and integrate a neural differential equation network into the physical structure of a vehicle single track model. By relying on the kinematic relations of the single track ordinary differential equations (ODE), a small neural network and few training samples are sufficient to substantially improve the model accuracy compared with a pure physics based vehicle single track model. To be more precise, the sum of squared error is reduced by 68% in the considered scenario. In addition, it is demonstrated that the prediction capabilities of the physics guided neural ODE model are superior compared with a pure black box neural differential equation approach.","sentences":["In this paper, we follow the physics guided modeling approach and integrate a neural differential equation network into the physical structure of a vehicle single track model.","By relying on the kinematic relations of the single track ordinary differential equations (ODE), a small neural network and few training samples are sufficient to substantially improve the model accuracy compared with a pure physics based vehicle single track model.","To be more precise, the sum of squared error is reduced by 68% in the considered scenario.","In addition, it is demonstrated that the prediction capabilities of the physics guided neural ODE model are superior compared with a pure black box neural differential equation approach."],"url":"http://arxiv.org/abs/2403.11648v1","category":"cs.CE"}
{"created":"2024-03-18 09:29:17","title":"Citerion of Internal Stability of Linear Formations","abstract":"Necessary and sufficient conditions for the internal stability of formations whose dynamics are obtained is determined by linear differential equations.","sentences":["Necessary and sufficient conditions for the internal stability of formations whose dynamics are obtained is determined by linear differential equations."],"url":"http://arxiv.org/abs/2403.11605v1","category":"math.OC"}
{"created":"2024-03-18 09:24:01","title":"Collapsing regular Riemannian foliations with flat leaves","abstract":"We present how to collapse a manifold equipped with a closed flat regular Riemannian foliation with leaves of positive dimension on a compact manifold, while keeping the sectional curvature uniformly bounded from above and below. From this deformation, we show that a closed flat regular Riemannian foliation with leaves of positive dimension on a compact simply-connected manifold is given by torus actions. This gives a geometric characterization of aspherical regular Riemannian foliations given by torus actions.","sentences":["We present how to collapse a manifold equipped with a closed flat regular Riemannian foliation with leaves of positive dimension on a compact manifold, while keeping the sectional curvature uniformly bounded from above and below.","From this deformation, we show that a closed flat regular Riemannian foliation with leaves of positive dimension on a compact simply-connected manifold is given by torus actions.","This gives a geometric characterization of aspherical regular Riemannian foliations given by torus actions."],"url":"http://arxiv.org/abs/2403.11602v1","category":"math.DG"}
{"created":"2024-03-18 09:16:30","title":"Semi-Analytical Methods for Population Balance models involving Aggregation and Breakage processes: A comparative study","abstract":"Population balance models often integrate fundamental kernels, including sum, gelling and Brownian aggregation kernels. These kernels have demonstrated extensive utility across various disciplines such as aerosol physics, chemical engineering, astrophysics, pharmaceutical sciences and mathematical biology for the purpose of elucidating particle dynamics. The objective of this study is to refine the semi-analytical solutions derived from current methodologies in addressing the nonlinear aggregation and coupled aggregation-breakage population balance equation. This work presents a unique semi-analytical approach based on the homotopy analysis method (HAM) to solve pure aggregation and couple aggregation-fragmentation population balance equations, which is an integro-partial differentia equation. By decomposing the non-linear operator, we investigate how to utilize the convergence control parameter to expedite the convergence of the HAM solution towards its precise values in the proposed method.","sentences":["Population balance models often integrate fundamental kernels, including sum, gelling and Brownian aggregation kernels.","These kernels have demonstrated extensive utility across various disciplines such as aerosol physics, chemical engineering, astrophysics, pharmaceutical sciences and mathematical biology for the purpose of elucidating particle dynamics.","The objective of this study is to refine the semi-analytical solutions derived from current methodologies in addressing the nonlinear aggregation and coupled aggregation-breakage population balance equation.","This work presents a unique semi-analytical approach based on the homotopy analysis method (HAM) to solve pure aggregation and couple aggregation-fragmentation population balance equations, which is an integro-partial differentia equation.","By decomposing the non-linear operator, we investigate how to utilize the convergence control parameter to expedite the convergence of the HAM solution towards its precise values in the proposed method."],"url":"http://arxiv.org/abs/2403.11595v1","category":"math.NA"}
{"created":"2024-03-18 09:08:41","title":"HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction","abstract":"This article presents our results for the sixth Affective Behavior Analysis in-the-wild (ABAW) competition. To improve the trustworthiness of facial analysis, we study the possibility of using pre-trained deep models that extract reliable emotional features without the need to fine-tune the neural networks for a downstream task. In particular, we introduce several lightweight models based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN architectures trained in multi-task scenarios to recognize facial expressions, valence, and arousal on static photos. These neural networks extract frame-level features fed into a simple classifier, e.g., linear feed-forward neural network, to predict emotion intensity, compound expressions, action units, facial expressions, and valence/arousal. Experimental results for five tasks from the sixth ABAW challenge demonstrate that our approach lets us significantly improve quality metrics on validation sets compared to existing non-ensemble techniques.","sentences":["This article presents our results for the sixth Affective Behavior Analysis in-the-wild (ABAW) competition.","To improve the trustworthiness of facial analysis, we study the possibility of using pre-trained deep models that extract reliable emotional features without the need to fine-tune the neural networks for a downstream task.","In particular, we introduce several lightweight models based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN architectures trained in multi-task scenarios to recognize facial expressions, valence, and arousal on static photos.","These neural networks extract frame-level features fed into a simple classifier, e.g., linear feed-forward neural network, to predict emotion intensity, compound expressions, action units, facial expressions, and valence/arousal.","Experimental results for five tasks from the sixth ABAW challenge demonstrate that our approach lets us significantly improve quality metrics on validation sets compared to existing non-ensemble techniques."],"url":"http://arxiv.org/abs/2403.11590v1","category":"cs.CV"}
{"created":"2024-03-18 08:58:48","title":"DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction","abstract":"This paper explores the problem of reconstructing temporally consistent surfaces from a 3D point cloud sequence without correspondence. To address this challenging task, we propose DynoSurf, an unsupervised learning framework integrating a template surface representation with a learnable deformation field. Specifically, we design a coarse-to-fine strategy for learning the template surface based on the deformable tetrahedron representation. Furthermore, we propose a learnable deformation representation based on the learnable control points and blending weights, which can deform the template surface non-rigidly while maintaining the consistency of the local shape. Experimental results demonstrate the significant superiority of DynoSurf over current state-of-the-art approaches, showcasing its potential as a powerful tool for dynamic mesh reconstruction. The code is publicly available at https://github.com/yaoyx689/DynoSurf.","sentences":["This paper explores the problem of reconstructing temporally consistent surfaces from a 3D point cloud sequence without correspondence.","To address this challenging task, we propose DynoSurf, an unsupervised learning framework integrating a template surface representation with a learnable deformation field.","Specifically, we design a coarse-to-fine strategy for learning the template surface based on the deformable tetrahedron representation.","Furthermore, we propose a learnable deformation representation based on the learnable control points and blending weights, which can deform the template surface non-rigidly while maintaining the consistency of the local shape.","Experimental results demonstrate the significant superiority of DynoSurf over current state-of-the-art approaches, showcasing its potential as a powerful tool for dynamic mesh reconstruction.","The code is publicly available at https://github.com/yaoyx689/DynoSurf."],"url":"http://arxiv.org/abs/2403.11586v1","category":"cs.CV"}
{"created":"2024-03-18 08:57:34","title":"Long time behaviour of solutions to non-local and non-linear dispersion problems","abstract":"This paper explores a non-linear, non-local model describing the evolution of a single species. We investigate scenarios where the spatial domain is either an arbitrary bounded and open subset of the $n$-dimensional Euclidean space or a periodic environment modeled by $n$-dimensional torus. The analysis includes the study of spectrum of the linear, bounded operator in the considered equation, which is a scaled, non-local analogue of classical Laplacian with Neumann boundaries. In particular we show the explicit formulas for eigenvalues and eigenfunctions. Moreover we show the asymptotic behaviour of eigenvalues. Within the context of the non-linear evolution problem, we establish the existence of an invariant region, give a criterion for convergence to the mean mass, and construct spatially heterogeneous steady states.","sentences":["This paper explores a non-linear, non-local model describing the evolution of a single species.","We investigate scenarios where the spatial domain is either an arbitrary bounded and open subset of the $n$-dimensional Euclidean space or a periodic environment modeled by $n$-dimensional torus.","The analysis includes the study of spectrum of the linear, bounded operator in the considered equation, which is a scaled, non-local analogue of classical Laplacian with Neumann boundaries.","In particular we show the explicit formulas for eigenvalues and eigenfunctions.","Moreover we show the asymptotic behaviour of eigenvalues.","Within the context of the non-linear evolution problem, we establish the existence of an invariant region, give a criterion for convergence to the mean mass, and construct spatially heterogeneous steady states."],"url":"http://arxiv.org/abs/2403.11584v1","category":"math.AP"}
{"created":"2024-03-18 08:43:44","title":"Logistic regression to boost exoplanet detection performances","abstract":"Direct imaging of exoplanets requires to separate the background noise from the exoplanet signals. Statistical methods have been recently proposed to avoid subtracting any signal of interest as opposed to initial self-subtracting methods based on Angular Differential Imaging (ADI). However, unless conservative thresholds are chosen to claim for a detection, such approaches tend to produce a list of candidates that include many false positives. Choosing high, conservative, thresholds leads to miss the faintest planets. We extend a statistical framework with a logistic regression to filter the list of candidates. Features with physical/optical meaning (in two wavelengths) are used, leading to a very fast and pragmatic approach. The overall method requires a simple edge detection (image processing) and clustering algorithm to work with sub-images. To estimate its efficiency, we apply our approach to targets observed with the ESO/SPHERE high contrast imager, that were previously used as tests for blind surveys. Experimental results with injected signals show that either the number of false detections is considerably reduced or faint exoplanets that would otherwise not be detected can be sometimes found. Typically, on the blind tests performed, we are now able to detect around 50% more of the injected planets with an SNR below 5, and with a very low number of additional candidates.","sentences":["Direct imaging of exoplanets requires to separate the background noise from the exoplanet signals.","Statistical methods have been recently proposed to avoid subtracting any signal of interest as opposed to initial self-subtracting methods based on Angular Differential Imaging (ADI).","However, unless conservative thresholds are chosen to claim for a detection, such approaches tend to produce a list of candidates that include many false positives.","Choosing high, conservative, thresholds leads to miss the faintest planets.","We extend a statistical framework with a logistic regression to filter the list of candidates.","Features with physical/optical meaning (in two wavelengths) are used, leading to a very fast and pragmatic approach.","The overall method requires a simple edge detection (image processing) and clustering algorithm to work with sub-images.","To estimate its efficiency, we apply our approach to targets observed with the ESO/SPHERE high contrast imager, that were previously used as tests for blind surveys.","Experimental results with injected signals show that either the number of false detections is considerably reduced or faint exoplanets that would otherwise not be detected can be sometimes found.","Typically, on the blind tests performed, we are now able to detect around 50% more of the injected planets with an SNR below 5, and with a very low number of additional candidates."],"url":"http://arxiv.org/abs/2403.11571v1","category":"astro-ph.EP"}
{"created":"2024-03-18 08:13:26","title":"Hierarchical Frequency-based Upsampling and Refining for Compressed Video Quality Enhancement","abstract":"Video compression artifacts arise due to the quantization operation in the frequency domain. The goal of video quality enhancement is to reduce compression artifacts and reconstruct a visually-pleasant result. In this work, we propose a hierarchical frequency-based upsampling and refining neural network (HFUR) for compressed video quality enhancement. HFUR consists of two modules: implicit frequency upsampling module (ImpFreqUp) and hierarchical and iterative refinement module (HIR). ImpFreqUp exploits DCT-domain prior derived through implicit DCT transform, and accurately reconstructs the DCT-domain loss via a coarse-to-fine transfer. Consequently, HIR is introduced to facilitate cross-collaboration and information compensation between the scales, thus further refine the feature maps and promote the visual quality of the final output. We demonstrate the effectiveness of the proposed modules via ablation experiments and visualized results. Extensive experiments on public benchmarks show that HFUR achieves state-of-the-art performance for both constant bit rate and constant QP modes.","sentences":["Video compression artifacts arise due to the quantization operation in the frequency domain.","The goal of video quality enhancement is to reduce compression artifacts and reconstruct a visually-pleasant result.","In this work, we propose a hierarchical frequency-based upsampling and refining neural network (HFUR) for compressed video quality enhancement.","HFUR consists of two modules: implicit frequency upsampling module (ImpFreqUp) and hierarchical and iterative refinement module (HIR).","ImpFreqUp exploits DCT-domain prior derived through implicit DCT transform, and accurately reconstructs the DCT-domain loss via a coarse-to-fine transfer.","Consequently, HIR is introduced to facilitate cross-collaboration and information compensation between the scales, thus further refine the feature maps and promote the visual quality of the final output.","We demonstrate the effectiveness of the proposed modules via ablation experiments and visualized results.","Extensive experiments on public benchmarks show that HFUR achieves state-of-the-art performance for both constant bit rate and constant QP modes."],"url":"http://arxiv.org/abs/2403.11556v1","category":"eess.IV"}
{"created":"2024-03-18 04:55:00","title":"A priori estimates for parabolic Monge-Amp\u00e8re type equations","abstract":"We prove the existence and regularity of convex solutions to the first initial-boundary value problem of the parabolic Monge-Amp\\`ere equation $$   \\left\\{\\begin{eqnarray}   &u_t=\\det D^2u\\quad\\text{ in } Q_T, \\\\   &u=\\phi\\quad\\text{ on }\\partial_pQ_T,   \\end{eqnarray}\\right. $$ where $\\phi$ is a smooth function, $Q_T=\\Omega\\times(0,T]$, $\\partial_p Q_T$ is the parabolic boundary of $Q_T$, and $\\Omega$ is a uniformly convex domain in $\\mathbb{R}^n$ with smooth boundary. Our approach can also be used to prove similar results for $\\gamma$-Gauss curvature flow with any $0<\\gamma\\le 1$.","sentences":["We prove the existence and regularity of convex solutions to the first initial-boundary value problem of the parabolic Monge-Amp\\`ere equation $$   \\left\\{\\begin{eqnarray}   &u_t=\\det D^2u\\quad\\text{ in } Q_T, \\\\   &u=\\phi\\quad\\text{ on }\\partial_pQ_T,   \\end{eqnarray}\\right.","$$ where $\\phi$ is a smooth function, $Q_T=\\Omega\\times(0,T]$, $\\partial_p Q_T$ is the parabolic boundary of $Q_T$, and $\\Omega$ is a uniformly convex domain in $\\mathbb{R}^n$ with smooth boundary.","Our approach can also be used to prove similar results for $\\gamma$-Gauss curvature flow with any $0<\\gamma\\le 1$."],"url":"http://arxiv.org/abs/2403.11479v1","category":"math.AP"}
{"created":"2024-03-18 04:43:19","title":"Self-similar imploding solutions of the relativistic Euler equations","abstract":"Motivated by recent breakthrough on smooth imploding solutions of compressible Euler, we construct self-similar smooth imploding solutions of isentropic relativistic Euler equations with isothermal equation of state $p=\\frac1\\ell\\varrho$ for \\textit{all} $\\ell>1$ in physical space dimension $d=2,3$ and for $\\ell>1$ close to 1 in higher dimensions. This work is a crucial step toward solving the long-standing problem: finite time blow-up of the supercritical defocusing nonlinear wave equation.","sentences":["Motivated by recent breakthrough on smooth imploding solutions of compressible Euler, we construct self-similar smooth imploding solutions of isentropic relativistic Euler equations with isothermal equation of state $p=\\frac1\\ell\\varrho$ for \\textit{all} $\\ell>1$ in physical space dimension $d=2,3$ and for $\\ell>1$ close to 1 in higher dimensions.","This work is a crucial step toward solving the long-standing problem: finite time blow-up of the supercritical defocusing nonlinear wave equation."],"url":"http://arxiv.org/abs/2403.11471v1","category":"math.AP"}
{"created":"2024-03-18 01:52:03","title":"Well-balanced path-conservative discontinuous Galerkin methods with equilibrium preserving space for two-layer shallow water equations","abstract":"This paper introduces well-balanced path-conservative discontinuous Galerkin (DG) methods for two-layer shallow water equations, ensuring exactness for both still water and moving water equilibrium steady states. The approach involves approximating the equilibrium variables within the DG piecewise polynomial space, while expressing the DG scheme in the form of path-conservative schemes. To robustly handle the nonconservative products governing momentum exchange between the layers, we incorporate the theory of Dal Maso, LeFloch, and Murat (DLM) within the DG method. Additionally, linear segment paths connecting the equilibrium functions are chosen to guarantee the well-balanced property of the resulting scheme. The simple ``lake-at-rest\" steady state is naturally satisfied without any modification, while a specialized treatment of the numerical flux is crucial for preserving the moving water steady state. Extensive numerical examples in one and two dimensions validate the exact equilibrium preservation of the steady state solutions and demonstrate its high-order accuracy. The performance of the method and high-resolution results further underscore its potential as a robust approach for nonconservative hyperbolic balance laws.","sentences":["This paper introduces well-balanced path-conservative discontinuous Galerkin (DG) methods for two-layer shallow water equations, ensuring exactness for both still water and moving water equilibrium steady states.","The approach involves approximating the equilibrium variables within the DG piecewise polynomial space, while expressing the DG scheme in the form of path-conservative schemes.","To robustly handle the nonconservative products governing momentum exchange between the layers, we incorporate the theory of Dal Maso, LeFloch, and Murat (DLM) within the DG method.","Additionally, linear segment paths connecting the equilibrium functions are chosen to guarantee the well-balanced property of the resulting scheme.","The simple ``lake-at-rest\" steady state is naturally satisfied without any modification, while a specialized treatment of the numerical flux is crucial for preserving the moving water steady state.","Extensive numerical examples in one and two dimensions validate the exact equilibrium preservation of the steady state solutions and demonstrate its high-order accuracy.","The performance of the method and high-resolution results further underscore its potential as a robust approach for nonconservative hyperbolic balance laws."],"url":"http://arxiv.org/abs/2403.11409v1","category":"math.NA"}
{"created":"2024-03-18 00:35:32","title":"An efficient biosensor based on magnetoelastic waves for detection of antibodies in human plasma for COVID-19 serodiagnosis","abstract":"The study proposes a new efficient wireless biosensor based on magnetoelastic waves for the detection of antibodies in human plasma, aiming at the serological diagnosis of COVID-19. The biosensor was functionalized with the N antigen - nucleocapsid phosphoprotein of the SARS-CoV-2 virus. Validation analyses, by sodium dodecyl-sulfate polyacrylamide gel electrophoresis (SDS-PAGE), Western blotting, atomic force microscopy (AFM), scanning electron microscopy (SEM), micro-Raman spectroscopy, confirmed the selectivity and effective surface functionalization of the biosensor. The research successfully obtained, expressed and purified the recombinant antigen, while plasma samples from COVID-19 positive and negative patients were used to test the performance of the biosensor. A comparison of performance with the ELISA method revealed equivalent diagnostic power. These results indicate the robustness of the biosensor in reliably differentiating between positive and negative samples, highlighting its potential as an efficient and low-cost tool for the serological diagnosis of COVID-19. In addition to being fast to execute and having the potential for automation in large-scale diagnostic studies, the biosensor fills a significant gap in existing SARS-CoV-2 detection approaches.","sentences":["The study proposes a new efficient wireless biosensor based on magnetoelastic waves for the detection of antibodies in human plasma, aiming at the serological diagnosis of COVID-19.","The biosensor was functionalized with the N antigen - nucleocapsid phosphoprotein of the SARS-CoV-2 virus.","Validation analyses, by sodium dodecyl-sulfate polyacrylamide gel electrophoresis (SDS-PAGE), Western blotting, atomic force microscopy (AFM), scanning electron microscopy (SEM), micro-Raman spectroscopy, confirmed the selectivity and effective surface functionalization of the biosensor.","The research successfully obtained, expressed and purified the recombinant antigen, while plasma samples from COVID-19 positive and negative patients were used to test the performance of the biosensor.","A comparison of performance with the ELISA method revealed equivalent diagnostic power.","These results indicate the robustness of the biosensor in reliably differentiating between positive and negative samples, highlighting its potential as an efficient and low-cost tool for the serological diagnosis of COVID-19.","In addition to being fast to execute and having the potential for automation in large-scale diagnostic studies, the biosensor fills a significant gap in existing SARS-CoV-2 detection approaches."],"url":"http://arxiv.org/abs/2403.11389v1","category":"physics.bio-ph"}
{"created":"2024-03-18 00:22:33","title":"Stochastic approach for elliptic problems in perforated domains","abstract":"A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters. Solving such perforated domain problems suffers from computational challenges related to resolving the scale imposed by the geometries of perforations. We propose a neural network-based mesh-free approach for perforated domain problems. The method is robust and efficient in capturing various configuration scales, including the averaged macroscopic behavior of the solution that involves a multiscale nature induced by small perforations. The new approach incorporates the derivative-free loss method that uses a stochastic representation or the Feynman-Kac formulation. In particular, we implement the Neumann boundary condition for the derivative-free loss method to handle the interface between the domain and perforations. A suite of stringent numerical tests is provided to support the proposed method's efficacy in handling various perforation scales.","sentences":["A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters.","Solving such perforated domain problems suffers from computational challenges related to resolving the scale imposed by the geometries of perforations.","We propose a neural network-based mesh-free approach for perforated domain problems.","The method is robust and efficient in capturing various configuration scales, including the averaged macroscopic behavior of the solution that involves a multiscale nature induced by small perforations.","The new approach incorporates the derivative-free loss method that uses a stochastic representation or the Feynman-Kac formulation.","In particular, we implement the Neumann boundary condition for the derivative-free loss method to handle the interface between the domain and perforations.","A suite of stringent numerical tests is provided to support the proposed method's efficacy in handling various perforation scales."],"url":"http://arxiv.org/abs/2403.11385v1","category":"math.NA"}
{"created":"2024-03-18 00:13:41","title":"Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach","abstract":"Supernet is a core component in many recent Neural Architecture Search (NAS) methods. It not only helps embody the search space but also provides a (relative) estimation of the final performance of candidate architectures. Thus, it is critical that the top architectures ranked by a supernet should be consistent with those ranked by true performance, which is known as the order-preserving ability. In this work, we analyze the order-preserving ability on the whole search space (global) and a sub-space of top architectures (local), and empirically show that the local order-preserving for current two-stage NAS methods still need to be improved. To rectify this, we propose a novel concept of Supernet Shifting, a refined search strategy combining architecture searching with supernet fine-tuning. Specifically, apart from evaluating, the training loss is also accumulated in searching and the supernet is updated every iteration. Since superior architectures are sampled more frequently in evolutionary searching, the supernet is encouraged to focus on top architectures, thus improving local order-preserving. Besides, a pre-trained supernet is often un-reusable for one-shot methods. We show that Supernet Shifting can fulfill transferring supernet to a new dataset. Specifically, the last classifier layer will be unset and trained through evolutionary searching. Comprehensive experiments show that our method has better order-preserving ability and can find a dominating architecture. Moreover, the pre-trained supernet can be easily transferred into a new dataset with no loss of performance.","sentences":["Supernet is a core component in many recent Neural Architecture Search (NAS) methods.","It not only helps embody the search space but also provides a (relative) estimation of the final performance of candidate architectures.","Thus, it is critical that the top architectures ranked by a supernet should be consistent with those ranked by true performance, which is known as the order-preserving ability.","In this work, we analyze the order-preserving ability on the whole search space (global) and a sub-space of top architectures (local), and empirically show that the local order-preserving for current two-stage NAS methods still need to be improved.","To rectify this, we propose a novel concept of Supernet Shifting, a refined search strategy combining architecture searching with supernet fine-tuning.","Specifically, apart from evaluating, the training loss is also accumulated in searching and the supernet is updated every iteration.","Since superior architectures are sampled more frequently in evolutionary searching, the supernet is encouraged to focus on top architectures, thus improving local order-preserving.","Besides, a pre-trained supernet is often un-reusable for one-shot methods.","We show that Supernet Shifting can fulfill transferring supernet to a new dataset.","Specifically, the last classifier layer will be unset and trained through evolutionary searching.","Comprehensive experiments show that our method has better order-preserving ability and can find a dominating architecture.","Moreover, the pre-trained supernet can be easily transferred into a new dataset with no loss of performance."],"url":"http://arxiv.org/abs/2403.11380v1","category":"cs.CV"}
{"created":"2024-03-17 22:37:15","title":"On the area of ordinary hyperbolic reduced polygons","abstract":"A convex body $R$ in the hyperbolic plane is reduced if any convex body $K\\subset R$ has a smaller minimal width than $R$. We examine the area of a family of hyperbolic reduced $n$-gons, and prove that, within this family, regular $n$-gons have minimal area.","sentences":["A convex body $R$ in the hyperbolic plane is reduced if any convex body $K\\subset R$ has a smaller minimal width than $R$. We examine the area of a family of hyperbolic reduced $n$-gons, and prove that, within this family, regular $n$-gons have minimal area."],"url":"http://arxiv.org/abs/2403.11360v1","category":"math.MG"}
{"created":"2024-03-17 21:35:45","title":"Robustness of the data-driven approach in limited angle tomography","abstract":"The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness. In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods.","sentences":["The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness.","In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods."],"url":"http://arxiv.org/abs/2403.11350v1","category":"math.NA"}
{"created":"2024-03-17 20:13:41","title":"Quantum dynamics in the self-consistent quadratic approximation","abstract":"A self-consistent quadratic theory is presented to account for nonlinear contributions in quantum dynamics. Evolution equations are shown to depend on higher-order gradients of the Hamiltonian, which are incorporated via their equations of motion or through perturbative calculations. The dynamics is proven trace-preserving, with the Hamiltonian acting as a constant of motion for initial Gaussian states. Nonlinear response functions are calculated perturbatively, and sufficient conditions are provided for the existence of their classical limit.","sentences":["A self-consistent quadratic theory is presented to account for nonlinear contributions in quantum dynamics.","Evolution equations are shown to depend on higher-order gradients of the Hamiltonian, which are incorporated via their equations of motion or through perturbative calculations.","The dynamics is proven trace-preserving, with the Hamiltonian acting as a constant of motion for initial Gaussian states.","Nonlinear response functions are calculated perturbatively, and sufficient conditions are provided for the existence of their classical limit."],"url":"http://arxiv.org/abs/2403.11327v1","category":"quant-ph"}
{"created":"2024-03-17 19:35:21","title":"The asymptotic behaviour of the solutions to widely degenerate p-Laplace equations","abstract":"In this paper, we consider the Dirichlet problems with a widely degenerate equation. Through a well-known result by Talenti, we explicitly express the gradient of the solution $u_p$ outside the ball with a radius of $1$, if the datum $f$ is a non-negative radially decreasing function. This allows us to analyze the behaviour of $u_p$ as $p \\to 1^+$ and to establish some higher regularity results, assuming the datum $f$ in a suitable Lorentz space.","sentences":["In this paper, we consider the Dirichlet problems with a widely degenerate equation.","Through a well-known result by Talenti, we explicitly express the gradient of the solution $u_p$ outside the ball with a radius of $1$, if the datum $f$ is a non-negative radially decreasing function.","This allows us to analyze the behaviour of $u_p$ as $p \\to 1^+$ and to establish some higher regularity results, assuming the datum $f$ in a suitable Lorentz space."],"url":"http://arxiv.org/abs/2403.11315v1","category":"math.AP"}
{"created":"2024-03-17 17:43:45","title":"Atomic polarization and Stark-shift in relativistic strong field ionization","abstract":"A relativistic analytical theory of strong field ionization applicable across the regimes of the deep-tunneling up to the over-barrier ionization (OTBI) is developed, incorporating the effects of the polarization of the atomic bound state and the Stark-shift in an ultrastrong laser field. The theory, in particular, addresses the order of magnitude discrepancy of the ionization yield in the relativistic regime calculated via the numerical solution of the Klein-Gordon equation [B. Hafizi \\textit{et al}., Phys. Rev. Lett. 118, 133201 (2017)] with respect to the state-of-the-art quasiclassical theory of Perelomov-Popov-Terent'ev (PPT) for strong field ionization. The developed theory employs a Keldysh-like approach describing the ionization as an adiabatic quantum jump from the bound state to the continuum at a specific transition time, where the improved performance is achieved by accounting for the bound state distortion in the laser field. In the nonrelativistic limit, our theory reproduces the well-known fit to the numerical calculations for the OTBI rate via the Tong-Lin factor. Realistic conditions for an experimental confirmation of the prediction of the present relativistic model versus PPT-theory are also presented.","sentences":["A relativistic analytical theory of strong field ionization applicable across the regimes of the deep-tunneling up to the over-barrier ionization (OTBI) is developed, incorporating the effects of the polarization of the atomic bound state and the Stark-shift in an ultrastrong laser field.","The theory, in particular, addresses the order of magnitude discrepancy of the ionization yield in the relativistic regime calculated via the numerical solution of the Klein-Gordon equation [B. Hafizi \\textit{et al}., Phys.","Rev. Lett.","118, 133201 (2017)] with respect to the state-of-the-art quasiclassical theory of Perelomov-Popov-Terent'ev (PPT) for strong field ionization.","The developed theory employs a Keldysh-like approach describing the ionization as an adiabatic quantum jump from the bound state to the continuum at a specific transition time, where the improved performance is achieved by accounting for the bound state distortion in the laser field.","In the nonrelativistic limit, our theory reproduces the well-known fit to the numerical calculations for the OTBI rate via the Tong-Lin factor.","Realistic conditions for an experimental confirmation of the prediction of the present relativistic model versus PPT-theory are also presented."],"url":"http://arxiv.org/abs/2403.11285v1","category":"physics.atom-ph"}
{"created":"2024-03-17 16:41:17","title":"A Dynamically Weighted Loss Function for Unsupervised Image Segmentation","abstract":"Image segmentation is the foundation of several computer vision tasks, where pixel-wise knowledge is a prerequisite for achieving the desired target. Deep learning has shown promising performance in supervised image segmentation. However, supervised segmentation algorithms require a massive amount of data annotated at a pixel level, thus limiting their applicability and scalability. Therefore, there is a need to invest in unsupervised learning for segmentation. This work presents an improved version of an unsupervised Convolutional Neural Network (CNN) based algorithm that uses a constant weight factor to balance between the segmentation criteria of feature similarity and spatial continuity, and it requires continuous manual adjustment of parameters depending on the degree of detail in the image and the dataset. In contrast, we propose a novel dynamic weighting scheme that leads to a flexible update of the parameters and an automatic tuning of the balancing weight between the two criteria above to bring out the details in the images in a genuinely unsupervised manner. We present quantitative and qualitative results on four datasets, which show that the proposed scheme outperforms the current unsupervised segmentation approaches without requiring manual adjustment.","sentences":["Image segmentation is the foundation of several computer vision tasks, where pixel-wise knowledge is a prerequisite for achieving the desired target.","Deep learning has shown promising performance in supervised image segmentation.","However, supervised segmentation algorithms require a massive amount of data annotated at a pixel level, thus limiting their applicability and scalability.","Therefore, there is a need to invest in unsupervised learning for segmentation.","This work presents an improved version of an unsupervised Convolutional Neural Network (CNN) based algorithm that uses a constant weight factor to balance between the segmentation criteria of feature similarity and spatial continuity, and it requires continuous manual adjustment of parameters depending on the degree of detail in the image and the dataset.","In contrast, we propose a novel dynamic weighting scheme that leads to a flexible update of the parameters and an automatic tuning of the balancing weight between the two criteria above to bring out the details in the images in a genuinely unsupervised manner.","We present quantitative and qualitative results on four datasets, which show that the proposed scheme outperforms the current unsupervised segmentation approaches without requiring manual adjustment."],"url":"http://arxiv.org/abs/2403.11266v1","category":"eess.IV"}
{"created":"2024-03-17 15:48:41","title":"Cytoplasmic Viscosity is a Potential Biomarker for Metastatic Breast Cancer Cells","abstract":"Cellular microrheology has shown that cancer cells with high metastatic potential are softer compared to non-tumorigenic normal cells. These findings rely on measuring the apparent Young modulus of whole cells using primarily atomic force microscopy. This study aims to explore whether alternative mechanical parameters have discriminating features with regard to metastatic potential. Magnetic rotational spectroscopy (MRS) is employed in the examination of mammary epithelial cell lines: MCF-7 and MDA-MB-231, representing low and high meta-static potential, alongside normal-like MCF-10A cells. MRS utilizes active micron-sized magnetic wires in a rotating magnetic field to measure the viscosity and elastic modulus of the cy-toplasm. All three cell lines display viscoelastic behavior, with cytoplasmic viscosities ranging from 10-70 Pa s and elastic moduli from 30-80 Pa. It is found that the tumorigenic MCF-7 and MDA-MB-231 cells are softer than the MCF-10A cells, with a twofold decrease in elastic modulus. To differentiate cells with low and high malignancy however, viscosity emerges as the more discriminating parameter, as MCF-7 exhibits a 5 times higher viscosity as compared to MDA-MB-231. These findings highlight the sensitivity of cytoplasmic viscosity to metastatic potential, suggesting its potential utility as a mechanical marker for malignant cancer cells.","sentences":["Cellular microrheology has shown that cancer cells with high metastatic potential are softer compared to non-tumorigenic normal cells.","These findings rely on measuring the apparent Young modulus of whole cells using primarily atomic force microscopy.","This study aims to explore whether alternative mechanical parameters have discriminating features with regard to metastatic potential.","Magnetic rotational spectroscopy (MRS) is employed in the examination of mammary epithelial cell lines:","MCF-7 and MDA-MB-231, representing low and high meta-static potential, alongside normal-like MCF-10A cells.","MRS utilizes active micron-sized magnetic wires in a rotating magnetic field to measure the viscosity and elastic modulus of the cy-toplasm.","All three cell lines display viscoelastic behavior, with cytoplasmic viscosities ranging from 10-70 Pa s and elastic moduli from 30-80 Pa.","It is found that the tumorigenic MCF-7 and MDA-MB-231 cells are softer than the MCF-10A cells, with a twofold decrease in elastic modulus.","To differentiate cells with low and high malignancy however, viscosity emerges as the more discriminating parameter, as MCF-7 exhibits a 5 times higher viscosity as compared to MDA-MB-231.","These findings highlight the sensitivity of cytoplasmic viscosity to metastatic potential, suggesting its potential utility as a mechanical marker for malignant cancer cells."],"url":"http://arxiv.org/abs/2403.11250v1","category":"physics.bio-ph"}
{"created":"2024-03-17 15:14:56","title":"On the refined Herglotz-Zagier function","abstract":"We give a functional equation for the refined Herglotz-Zagier function. It is analogous to a result in the theory of modular forms.","sentences":["We give a functional equation for the refined Herglotz-Zagier function.","It is analogous to a result in the theory of modular forms."],"url":"http://arxiv.org/abs/2403.11243v1","category":"math.NT"}
{"created":"2024-03-17 15:08:48","title":"Fidelity-preserving Learning-Based Image Compression: Loss Function and Subjective Evaluation Methodology","abstract":"Learning-based image compression methods have emerged as state-of-the-art, showcasing higher performance compared to conventional compression solutions. These data-driven approaches aim to learn the parameters of a neural network model through iterative training on large amounts of data. The optimization process typically involves minimizing the distortion between the decoded and the original ground truth images. This paper focuses on perceptual optimization of learning-based image compression solutions and proposes: i) novel loss function to be used during training and ii) novel subjective test methodology that aims to evaluate the decoded image fidelity. According to experimental results from the subjective test taken with the new methodology, the optimization procedure can enhance image quality for low-rates while offering no advantage for high-rates.","sentences":["Learning-based image compression methods have emerged as state-of-the-art, showcasing higher performance compared to conventional compression solutions.","These data-driven approaches aim to learn the parameters of a neural network model through iterative training on large amounts of data.","The optimization process typically involves minimizing the distortion between the decoded and the original ground truth images.","This paper focuses on perceptual optimization of learning-based image compression solutions and proposes: i) novel loss function to be used during training and ii) novel subjective test methodology that aims to evaluate the decoded image fidelity.","According to experimental results from the subjective test taken with the new methodology, the optimization procedure can enhance image quality for low-rates while offering no advantage for high-rates."],"url":"http://arxiv.org/abs/2403.11241v1","category":"cs.MM"}
{"created":"2024-03-17 14:34:51","title":"Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection","abstract":"This study explores the use of deep learning techniques for analyzing lung Computed Tomography (CT) images. Classic deep learning approaches face challenges with varying slice counts and resolutions in CT images, a diversity arising from the utilization of assorted scanning equipment. Typically, predictions are made on single slices which are then combined for a comprehensive outcome. Yet, this method does not incorporate learning features specific to each slice, leading to a compromise in effectiveness. To address these challenges, we propose an advanced Spatial-Slice Feature Learning (SSFL++) framework specifically tailored for CT scans. It aims to filter out out-of-distribution (OOD) data within the entire CT scan, allowing us to select essential spatial-slice features for analysis by reducing data redundancy by 70\\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS) method to enhance stability during training and inference phases, thereby accelerating convergence and enhancing overall performance. Remarkably, our experiments reveal that our model achieves promising results with a simple EfficientNet-2D (E2D) model. The effectiveness of our approach is confirmed on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop.","sentences":["This study explores the use of deep learning techniques for analyzing lung Computed Tomography (CT) images.","Classic deep learning approaches face challenges with varying slice counts and resolutions in CT images, a diversity arising from the utilization of assorted scanning equipment.","Typically, predictions are made on single slices which are then combined for a comprehensive outcome.","Yet, this method does not incorporate learning features specific to each slice, leading to a compromise in effectiveness.","To address these challenges, we propose an advanced Spatial-Slice Feature Learning (SSFL++) framework specifically tailored for CT scans.","It aims to filter out out-of-distribution (OOD) data within the entire CT scan, allowing us to select essential spatial-slice features for analysis by reducing data redundancy by 70\\%.","Additionally, we introduce a Kernel-Density-based slice Sampling (KDS) method to enhance stability during training and inference phases, thereby accelerating convergence and enhancing overall performance.","Remarkably, our experiments reveal that our model achieves promising results with a simple EfficientNet-2D (E2D) model.","The effectiveness of our approach is confirmed on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop."],"url":"http://arxiv.org/abs/2403.11230v1","category":"eess.IV"}
{"created":"2024-03-17 13:51:25","title":"SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream","abstract":"Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRF's multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at https://github.com/BIT-Vision/SpikeNeRF.","sentences":["Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras.","However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios.","To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data.","Our approach leverages NeRF's multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios.","The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions.","We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes.","Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology.","The dataset and source code are released at https://github.com/BIT-Vision/SpikeNeRF."],"url":"http://arxiv.org/abs/2403.11222v1","category":"cs.CV"}
{"created":"2024-03-17 13:25:01","title":"Carroll-Schr\u00f6dinger Equation","abstract":"The Poincar\\'e symmetry can be contracted in two ways to yield the Galilei symmetry and the Carroll symmetry. The Schr\\\"odinger equation exhibits the Galilei symmetry and is a fundamental equation in Galilean quantum mechanics. However, the question remains: what is the quantum equation that corresponds to the Carroll symmetry? In this paper, we derive a novel equation, called the Carroll-Schr\\\"odinger equation, which describes the quantum dynamics in the Carrollian framework. We also introduce the Carroll-Schr\\\"odinger algebra, which is a conformal extension of the centrally extended Carroll algebra, and show that it is the symmetry algebra of the Carroll-Schr\\\"odinger equation in two dimensions. We generalize our results to arbitrary dimensions and discuss some possible applications and extensions of our work.","sentences":["The Poincar\\'e symmetry can be contracted in two ways to yield the Galilei symmetry and the Carroll symmetry.","The Schr\\\"odinger equation exhibits the Galilei symmetry and is a fundamental equation in Galilean quantum mechanics.","However, the question remains: what is the quantum equation that corresponds to the Carroll symmetry?","In this paper, we derive a novel equation, called the Carroll-Schr\\\"odinger equation, which describes the quantum dynamics in the Carrollian framework.","We also introduce the Carroll-Schr\\\"odinger algebra, which is a conformal extension of the centrally extended Carroll algebra, and show that it is the symmetry algebra of the Carroll-Schr\\\"odinger equation in two dimensions.","We generalize our results to arbitrary dimensions and discuss some possible applications and extensions of our work."],"url":"http://arxiv.org/abs/2403.11212v1","category":"hep-th"}
{"created":"2024-03-17 12:35:15","title":"Generalised core partitions and Diophantine equations","abstract":"We study generalised core partitions arising from affine Grassmannian elements in arbitrary Dynkin type. The corresponding notion of size is given by the atomic length in the sense of [CLG22]. In this paper, we first develop the theory for extended affine Weyl groups. In a series of applications, we give some remarkable parametrisations of the solutions of certain Diophantine equations resembling Pell's equation, by refining the results of [BN22] and [Alp14], and generalising them to further types.","sentences":["We study generalised core partitions arising from affine Grassmannian elements in arbitrary Dynkin type.","The corresponding notion of size is given by the atomic length in the sense of [CLG22].","In this paper, we first develop the theory for extended affine Weyl groups.","In a series of applications, we give some remarkable parametrisations of the solutions of certain Diophantine equations resembling Pell's equation, by refining the results of [BN22] and [Alp14], and generalising them to further types."],"url":"http://arxiv.org/abs/2403.11191v1","category":"math.CO"}
{"created":"2024-03-17 11:55:35","title":"Solving Maxwell's Equations","abstract":"This paper discusses the use of the Riemann-Silberstein vector to solve the source-free Maxwell's equations and obtains novel analytical solutions. The solving process naturally leads to the spinor form of the source-free Maxwell's equations. Several powerful theorems are established to solve this spinor form equation. The Waveguide Solution Theorem provides an elegant way to solve waveguide problems, while The Schrodinger Solution Theorem connects the Maxwell's equations with the two-dimensional Schrodinger equation. By utilizing The Schrodinger Solution Theorem, a precise formula for spatiotemporal diffraction of the Maxwell's equations is derived, which allows for the reconstruction of electromagnetic waves throughout space and time based on the field distribution on the diffraction screen.","sentences":["This paper discusses the use of the Riemann-Silberstein vector to solve the source-free Maxwell's equations and obtains novel analytical solutions.","The solving process naturally leads to the spinor form of the source-free Maxwell's equations.","Several powerful theorems are established to solve this spinor form equation.","The Waveguide Solution Theorem provides an elegant way to solve waveguide problems, while The Schrodinger Solution Theorem connects the Maxwell's equations with the two-dimensional Schrodinger equation.","By utilizing The Schrodinger Solution Theorem, a precise formula for spatiotemporal diffraction of the Maxwell's equations is derived, which allows for the reconstruction of electromagnetic waves throughout space and time based on the field distribution on the diffraction screen."],"url":"http://arxiv.org/abs/2403.11181v1","category":"physics.gen-ph"}
{"created":"2024-03-17 11:19:45","title":"Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks","abstract":"Artificial neural network (NN) architecture design is a nontrivial and time-consuming task that often requires a high level of human expertise. Neural architecture search (NAS) serves to automate the design of NN architectures and has proven to be successful in automatically finding NN architectures that outperform those manually designed by human experts. NN architecture performance can be quantified based on multiple objectives, which include model accuracy and some NN architecture complexity objectives, among others. The majority of modern NAS methods that consider multiple objectives for NN architecture performance evaluation are concerned with automated feed forward NN architecture design, which leaves multi-objective automated recurrent neural network (RNN) architecture design unexplored. RNNs are important for modeling sequential datasets, and prominent within the natural language processing domain. It is often the case in real world implementations of machine learning and NNs that a reasonable trade-off is accepted for marginally reduced model accuracy in favour of lower computational resources demanded by the model. This paper proposes a multi-objective evolutionary algorithm-based RNN architecture search method. The proposed method relies on approximate network morphisms for RNN architecture complexity optimisation during evolution. The results show that the proposed method is capable of finding novel RNN architectures with comparable performance to state-of-the-art manually designed RNN architectures, but with reduced computational demand.","sentences":["Artificial neural network (NN) architecture design is a nontrivial and time-consuming task that often requires a high level of human expertise.","Neural architecture search (NAS) serves to automate the design of NN architectures and has proven to be successful in automatically finding NN architectures that outperform those manually designed by human experts.","NN architecture performance can be quantified based on multiple objectives, which include model accuracy and some NN architecture complexity objectives, among others.","The majority of modern NAS methods that consider multiple objectives for NN architecture performance evaluation are concerned with automated feed forward NN architecture design, which leaves multi-objective automated recurrent neural network (RNN) architecture design unexplored.","RNNs are important for modeling sequential datasets, and prominent within the natural language processing domain.","It is often the case in real world implementations of machine learning and NNs that a reasonable trade-off is accepted for marginally reduced model accuracy in favour of lower computational resources demanded by the model.","This paper proposes a multi-objective evolutionary algorithm-based RNN architecture search method.","The proposed method relies on approximate network morphisms for RNN architecture complexity optimisation during evolution.","The results show that the proposed method is capable of finding novel RNN architectures with comparable performance to state-of-the-art manually designed RNN architectures, but with reduced computational demand."],"url":"http://arxiv.org/abs/2403.11173v1","category":"cs.NE"}
{"created":"2024-03-17 10:34:31","title":"Theoretical and numerical comparison between the pseudopotential and the free energy lattice Boltzmann methods","abstract":"The pseudopotential and free energy models are two popular extensions of the lattice Boltzmann method for multiphase flows. Until now, they have been developed apart from each other in the literature. However, important questions about whether each method performs better needs to be solved. In this work, we perform a theoretical and numerical comparison between both methods. This comparison is only possible because we developed a novel approach for controlling the interface thickness in the pseudopotential method independently on the equation of state. In this way, it is possible to compare both methods maintaining the same equilibrium densities, interface thickness, surface tension and equation of state parameters. The well-balanced approach was selected to represent the free energy. We found that the free energy one is more practical to use, as it is not necessary to carry out previous simulations to determine simulation parameters (interface thickness, surface tension, etc). In addition, the tests proofed that the free energy model is more accurate than the pseudopotential model. Furthermore, the pseudopotential method suffers from a lack of thermodynamic consistency even when applying the corrections proposed in the literature. On the other hand, for both static and dynamic tests we verified that the pseudopotential method is more stable than the free energy one, allowing simulations with lower reduced temperatures. We hope that these results will guide authors in the use of each method.","sentences":["The pseudopotential and free energy models are two popular extensions of the lattice Boltzmann method for multiphase flows.","Until now, they have been developed apart from each other in the literature.","However, important questions about whether each method performs better needs to be solved.","In this work, we perform a theoretical and numerical comparison between both methods.","This comparison is only possible because we developed a novel approach for controlling the interface thickness in the pseudopotential method independently on the equation of state.","In this way, it is possible to compare both methods maintaining the same equilibrium densities, interface thickness, surface tension and equation of state parameters.","The well-balanced approach was selected to represent the free energy.","We found that the free energy one is more practical to use, as it is not necessary to carry out previous simulations to determine simulation parameters (interface thickness, surface tension, etc).","In addition, the tests proofed that the free energy model is more accurate than the pseudopotential model.","Furthermore, the pseudopotential method suffers from a lack of thermodynamic consistency even when applying the corrections proposed in the literature.","On the other hand, for both static and dynamic tests we verified that the pseudopotential method is more stable than the free energy one, allowing simulations with lower reduced temperatures.","We hope that these results will guide authors in the use of each method."],"url":"http://arxiv.org/abs/2403.11167v1","category":"physics.comp-ph"}
{"created":"2024-03-17 10:18:05","title":"4-Dimensional Isoparametric Hypersurfaces of Index 2 in the Pseudo-Riemannian Space Forms","abstract":"We study isoparametric hypersurfaces, whose principal curvatures are all constant, in the pseudo-Riemannian space forms. In this paper, we investigate three topics.Firstly, according to Petrov's classification theorem, we give a classification of hypersurfaces of index 2 with respect to a pair of a shape operator and a metric. Therefore, we can define types of isoparametric hypersurfaces of index 2 concerning the classification. Secondly, we give several examples of certain types. Thirdly, we show that there exist no isoparametric hypersurfaces of index 2 whose shape operators have complex principal curvatures in certain cases.","sentences":["We study isoparametric hypersurfaces, whose principal curvatures are all constant, in the pseudo-Riemannian space forms.","In this paper, we investigate three topics.","Firstly, according to Petrov's classification theorem, we give a classification of hypersurfaces of index 2 with respect to a pair of a shape operator and a metric.","Therefore, we can define types of isoparametric hypersurfaces of index 2 concerning the classification.","Secondly, we give several examples of certain types.","Thirdly, we show that there exist no isoparametric hypersurfaces of index 2 whose shape operators have complex principal curvatures in certain cases."],"url":"http://arxiv.org/abs/2403.11165v1","category":"math.DG"}
{"created":"2024-03-17 09:59:52","title":"Floquet-Bloch functions on non-simply connected manifolds, the Aharonov-Bohm fluxes, and conformal invariants of immersed surfaces","abstract":"Spectral (Bloch) varieties of multidimensional differential operators on non-simply connected manifolds are defined. In their terms it is given a description of the analytical dependence of the spectra of magnetic Laplacians on non-simply connected manifolds on the values of the Aharonov-Bohm fluxes and a construction of analogues of spectral curves for two-dimensional Dirac operators on Riemann surfaces and, thereby, new conformal invariants of immersions of surfaces into 3- and 4-dimensional Euclidean spaces.","sentences":["Spectral (Bloch) varieties of multidimensional differential operators on non-simply connected manifolds are defined.","In their terms it is given a description of the analytical dependence of the spectra of magnetic Laplacians on non-simply connected manifolds on the values of the Aharonov-Bohm fluxes and a construction of analogues of spectral curves for two-dimensional Dirac operators on Riemann surfaces and, thereby, new conformal invariants of immersions of surfaces into 3- and 4-dimensional Euclidean spaces."],"url":"http://arxiv.org/abs/2403.11161v1","category":"math.DG"}
{"created":"2024-03-17 09:50:20","title":"Deep Neural Crossover","abstract":"We present a novel multi-parent crossover operator in genetic algorithms (GAs) called ``Deep Neural Crossover'' (DNC). Unlike conventional GA crossover operators that rely on a random selection of parental genes, DNC leverages the capabilities of deep reinforcement learning (DRL) and an encoder-decoder architecture to select the genes. Specifically, we use DRL to learn a policy for selecting promising genes. The policy is stochastic, to maintain the stochastic nature of GAs, representing a distribution for selecting genes with a higher probability of improving fitness. Our architecture features a recurrent neural network (RNN) to encode the parental genomes into latent memory states, and a decoder RNN that utilizes an attention-based pointing mechanism to generate a distribution over the next selected gene in the offspring. To improve the training time, we present a pre-training approach, wherein the architecture is initially trained on a single problem within a specific domain and then applied to solving other problems of the same domain. We compare DNC to known operators from the literature over two benchmark domains -- bin packing and graph coloring. We compare with both two- and three-parent crossover, outperforming all baselines. DNC is domain-independent and can be easily applied to other problem domains.","sentences":["We present a novel multi-parent crossover operator in genetic algorithms (GAs) called ``Deep Neural Crossover'' (DNC).","Unlike conventional GA crossover operators that rely on a random selection of parental genes, DNC leverages the capabilities of deep reinforcement learning (DRL) and an encoder-decoder architecture to select the genes.","Specifically, we use DRL to learn a policy for selecting promising genes.","The policy is stochastic, to maintain the stochastic nature of GAs, representing a distribution for selecting genes with a higher probability of improving fitness.","Our architecture features a recurrent neural network (RNN) to encode the parental genomes into latent memory states, and a decoder RNN that utilizes an attention-based pointing mechanism to generate a distribution over the next selected gene in the offspring.","To improve the training time, we present a pre-training approach, wherein the architecture is initially trained on a single problem within a specific domain and then applied to solving other problems of the same domain.","We compare DNC to known operators from the literature over two benchmark domains -- bin packing and graph coloring.","We compare with both two- and three-parent crossover, outperforming all baselines.","DNC is domain-independent and can be easily applied to other problem domains."],"url":"http://arxiv.org/abs/2403.11159v1","category":"cs.NE"}
{"created":"2024-03-17 08:53:59","title":"Primordial black hole formation from a nonspherical density profile with a misaligned deformation tensor","abstract":"We perform the numerical simulation of primordial black hole formation from a nonspherical profile of the initial curvature perturbation $\\zeta$. We consider the background expanding universe filled with the perfect fluid with the linear equation of state $p=w\\rho$ ($w=1/3$ or $1/5$), where $p$ and $\\rho$ are the pressure and the energy density, respectively. The initial condition is set in a way such that the principal directions of the second derivatives of $\\zeta$ and $\\triangle \\zeta$ at the central peak are misaligned, where $\\triangle$ is the Laplacian. In this setting, since the linearized density is proportional to $\\triangle \\zeta$, the inertia tensor and deformation tensor $\\partial_i\\partial_j \\zeta$ are misaligned. Thus tidal torque may act and the spin of a resultant primordial black hole would be non-zero in general, although it is estimated to be very small from previous perturbative analyses.As a result, we do not find a finite value of the spin within our numerical precision, giving support for the negligibly small value of the black hole spin for $1/5\\lesssim w \\lesssim 1/3$. More specifically, our results suggest that the dimensionless PBH spin $s$ is typically so small that $s\\ll0.1$ for $w\\gtrsim0.2$.","sentences":["We perform the numerical simulation of primordial black hole formation from a nonspherical profile of the initial curvature perturbation $\\zeta$. We consider the background expanding universe filled with the perfect fluid with the linear equation of state $p=w\\rho$ ($w=1/3$ or $1/5$), where $p$ and $\\rho$ are the pressure and the energy density, respectively.","The initial condition is set in a way such that the principal directions of the second derivatives of $\\zeta$ and $\\triangle \\zeta$ at the central peak are misaligned, where $\\triangle$ is the Laplacian.","In this setting, since the linearized density is proportional to $\\triangle \\zeta$, the inertia tensor and deformation tensor $\\partial_i\\partial_j \\zeta$ are misaligned.","Thus tidal torque may act and the spin of a resultant primordial black hole would be non-zero in general, although it is estimated to be very small from previous perturbative analyses.","As a result, we do not find a finite value of the spin within our numerical precision, giving support for the negligibly small value of the black hole spin for $1/5\\lesssim w \\lesssim 1/3$. More specifically, our results suggest that the dimensionless PBH spin $s$ is typically so small that $s\\ll0.1$ for $w\\gtrsim0.2$."],"url":"http://arxiv.org/abs/2403.11147v1","category":"gr-qc"}
{"created":"2024-03-17 08:43:19","title":"Understanding the PDHG Algorithm via High-Resolution Differential Equations","abstract":"The least absolute shrinkage and selection operator (Lasso) is widely recognized across various fields of mathematics and engineering. Its variant, the generalized Lasso, finds extensive application in the fields of statistics, machine learning, image science, and related areas. Among the optimization techniques used to tackle this issue, saddle-point methods stand out, with the primal-dual hybrid gradient (PDHG) algorithm emerging as a particularly popular choice. However, the iterative behavior of PDHG remains poorly understood. In this paper, we employ dimensional analysis to derive a system of high-resolution ordinary differential equations (ODEs) tailored for PDHG. This system effectively captures a key feature of PDHG, the coupled $x$-correction and $y$-correction, distinguishing it from the proximal Arrow-Hurwicz algorithm. The small but essential perturbation ensures that PDHG consistently converges, bypassing the periodic behavior observed in the proximal Arrow-Hurwicz algorithm. Through Lyapunov analysis, We investigate the convergence behavior of the system of high-resolution ODEs and extend our insights to the discrete PDHG algorithm. Our analysis indicates that numerical errors resulting from the implicit scheme serve as a crucial factor affecting the convergence rate and monotonicity of PDHG, showcasing a noteworthy pattern also observed for the Alternating Direction Method of Multipliers (ADMM), as identified in [Li and Shi, 2024]. In addition, we further discover that when one component of the objective function is strongly convex, the iterative average of PDHG converges strongly at a rate $O(1/N)$, where $N$ is the number of iterations.","sentences":["The least absolute shrinkage and selection operator (Lasso) is widely recognized across various fields of mathematics and engineering.","Its variant, the generalized Lasso, finds extensive application in the fields of statistics, machine learning, image science, and related areas.","Among the optimization techniques used to tackle this issue, saddle-point methods stand out, with the primal-dual hybrid gradient (PDHG) algorithm emerging as a particularly popular choice.","However, the iterative behavior of PDHG remains poorly understood.","In this paper, we employ dimensional analysis to derive a system of high-resolution ordinary differential equations (ODEs) tailored for PDHG.","This system effectively captures a key feature of PDHG, the coupled $x$-correction and $y$-correction, distinguishing it from the proximal Arrow-Hurwicz algorithm.","The small but essential perturbation ensures that PDHG consistently converges, bypassing the periodic behavior observed in the proximal Arrow-Hurwicz algorithm.","Through Lyapunov analysis, We investigate the convergence behavior of the system of high-resolution ODEs and extend our insights to the discrete PDHG algorithm.","Our analysis indicates that numerical errors resulting from the implicit scheme serve as a crucial factor affecting the convergence rate and monotonicity of PDHG, showcasing a noteworthy pattern also observed for the Alternating Direction Method of Multipliers (ADMM), as identified in [Li and Shi, 2024].","In addition, we further discover that when one component of the objective function is strongly convex, the iterative average of PDHG converges strongly at a rate $O(1/N)$, where $N$ is the number of iterations."],"url":"http://arxiv.org/abs/2403.11139v1","category":"math.OC"}
{"created":"2024-03-17 08:41:48","title":"Spiking Wavelet Transformer","abstract":"Spiking neural networks (SNNs) offer an energy-efficient alternative to conventional deep learning by mimicking the event-driven processing of the brain. Incorporating the Transformers with SNNs has shown promise for accuracy, yet it is incompetent to capture high-frequency patterns like moving edge and pixel-level brightness changes due to their reliance on global self-attention operations. Porting frequency representations in SNN is challenging yet crucial for event-driven vision. To address this issue, we propose the Spiking Wavelet Transformer (SWformer), an attention-free architecture that effectively learns comprehensive spatial-frequency features in a spike-driven manner by leveraging the sparse wavelet transform. The critical component is a Frequency-Aware Token Mixer (FATM) with three branches: 1) spiking wavelet learner for spatial-frequency domain learning, 2) convolution-based learner for spatial feature extraction, and 3) spiking pointwise convolution for cross-channel information aggregation. We also adopt negative spike dynamics to strengthen the frequency representation further. This enables the SWformer to outperform vanilla Spiking Transformers in capturing high-frequency visual components, as evidenced by our empirical results. Experiments on both static and neuromorphic datasets demonstrate SWformer's effectiveness in capturing spatial-frequency patterns in a multiplication-free, event-driven fashion, outperforming state-of-the-art SNNs. SWformer achieves an over 50% reduction in energy consumption, a 21.1% reduction in parameter count, and a 2.40% performance improvement on the ImageNet dataset compared to vanilla Spiking Transformers.","sentences":["Spiking neural networks (SNNs) offer an energy-efficient alternative to conventional deep learning by mimicking the event-driven processing of the brain.","Incorporating the Transformers with SNNs has shown promise for accuracy, yet it is incompetent to capture high-frequency patterns like moving edge and pixel-level brightness changes due to their reliance on global self-attention operations.","Porting frequency representations in SNN is challenging yet crucial for event-driven vision.","To address this issue, we propose the Spiking Wavelet Transformer (SWformer), an attention-free architecture that effectively learns comprehensive spatial-frequency features in a spike-driven manner by leveraging the sparse wavelet transform.","The critical component is a Frequency-Aware Token Mixer (FATM) with three branches: 1) spiking wavelet learner for spatial-frequency domain learning, 2) convolution-based learner for spatial feature extraction, and 3) spiking pointwise convolution for cross-channel information aggregation.","We also adopt negative spike dynamics to strengthen the frequency representation further.","This enables the SWformer to outperform vanilla Spiking Transformers in capturing high-frequency visual components, as evidenced by our empirical results.","Experiments on both static and neuromorphic datasets demonstrate SWformer's effectiveness in capturing spatial-frequency patterns in a multiplication-free, event-driven fashion, outperforming state-of-the-art SNNs.","SWformer achieves an over 50% reduction in energy consumption, a 21.1% reduction in parameter count, and a 2.40% performance improvement on the ImageNet dataset compared to vanilla Spiking Transformers."],"url":"http://arxiv.org/abs/2403.11138v1","category":"cs.NE"}
{"created":"2024-03-17 07:57:08","title":"Recent Advances in 3D Gaussian Splatting","abstract":"The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis. Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique. This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation.","sentences":["The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis.","Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images.","Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, geometry editing, and physical simulation.","Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality.","Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique.","This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation."],"url":"http://arxiv.org/abs/2403.11134v1","category":"cs.CV"}
{"created":"2024-03-17 07:55:16","title":"Angular Momentum Memory Effect","abstract":"Utilizing recent mathematical advances in proving stability of Minkowski spacetime with minimal decay rates and nonlinear stability of Kerr black holes with small angular momentum, we investigate the detailed asymptotic behaviors of gravitational waves generated in these spacetimes. Here we report and propose a new angular momentum memory effect along future null infinity. This accompanies Christodoulou's nonlinear displacement memory effect and the spin memory effect. The connections and differences to these effects are also addressed.","sentences":["Utilizing recent mathematical advances in proving stability of Minkowski spacetime with minimal decay rates and nonlinear stability of Kerr black holes with small angular momentum, we investigate the detailed asymptotic behaviors of gravitational waves generated in these spacetimes.","Here we report and propose a new angular momentum memory effect along future null infinity.","This accompanies Christodoulou's nonlinear displacement memory effect and the spin memory effect.","The connections and differences to these effects are also addressed."],"url":"http://arxiv.org/abs/2403.11133v1","category":"gr-qc"}
{"created":"2024-03-17 06:11:48","title":"Jointly Optimizing Terahertz based Sensing and Communications in Vehicular Networks: A Dynamic Graph Neural Network Approach","abstract":"In this paper, the problem of vehicle service mode selection (sensing, communication, or both) and vehicle connections within terahertz (THz) enabled joint sensing and communications over vehicular networks is studied. The considered network consists of several service provider vehicles (SPVs) that can provide: 1) only sensing service, 2) only communication service, and 3) both services, sensing service request vehicles, and communication service request vehicles. Based on the vehicle network topology and their service accessibility, SPVs strategically select service request vehicles to provide sensing, communication, or both services. This problem is formulated as an optimization problem, aiming to maximize the number of successfully served vehicles by jointly determining the service mode of each SPV and its associated vehicles. To solve this problem, we propose a dynamic graph neural network (GNN) model that selects appropriate graph information aggregation functions according to the vehicle network topology, thus extracting more vehicle network information compared to traditional static GNNs that use fixed aggregation functions for different vehicle network topologies. Using the extracted vehicle network information, the service mode of each SPV and its served service request vehicles will be determined. Simulation results show that the proposed dynamic GNN based method can improve the number of successfully served vehicles by up to 17% and 28% compared to a GNN based algorithm with a fixed neural network model and a conventional optimization algorithm without using GNNs.","sentences":["In this paper, the problem of vehicle service mode selection (sensing, communication, or both) and vehicle connections within terahertz (THz) enabled joint sensing and communications over vehicular networks is studied.","The considered network consists of several service provider vehicles (SPVs) that can provide: 1) only sensing service, 2) only communication service, and 3) both services, sensing service request vehicles, and communication service request vehicles.","Based on the vehicle network topology and their service accessibility, SPVs strategically select service request vehicles to provide sensing, communication, or both services.","This problem is formulated as an optimization problem, aiming to maximize the number of successfully served vehicles by jointly determining the service mode of each SPV and its associated vehicles.","To solve this problem, we propose a dynamic graph neural network (GNN) model that selects appropriate graph information aggregation functions according to the vehicle network topology, thus extracting more vehicle network information compared to traditional static GNNs that use fixed aggregation functions for different vehicle network topologies.","Using the extracted vehicle network information, the service mode of each SPV and its served service request vehicles will be determined.","Simulation results show that the proposed dynamic GNN based method can improve the number of successfully served vehicles by up to 17% and 28% compared to a GNN based algorithm with a fixed neural network model and a conventional optimization algorithm without using GNNs."],"url":"http://arxiv.org/abs/2403.11102v1","category":"cs.NI"}
{"created":"2024-03-17 06:08:08","title":"Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance","abstract":"Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.","sentences":["Expansion property of a graph refers to its strong connectivity as well as sparseness.","It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance.","Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms.","We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties.","We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers.","Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM."],"url":"http://arxiv.org/abs/2403.11100v1","category":"cs.LG"}
{"created":"2024-03-17 02:44:36","title":"The masses and decay widths of the $S$-wave $\u039b_c\\bar\u039b_c$ bound states","abstract":"In this work, we investigate possible bound states of the $\\Lambda_c\\bar{\\Lambda}_c$ system in the Bethe-Salpeter formalism in the ladder and instantaneous approximations. By numerically solving the Bethe-Salpeter equation, we confirm the existence of $\\Lambda_c\\bar{\\Lambda}_c$ bound states with quantum numbers $J^{PC}=0^{-+}$ and $J^{PC}=1^{--}$. We further investigate the partial decay widths of the $\\Lambda_c\\bar{\\Lambda}_c$ bound states into $N\\bar{N}$, $D\\bar{D}$, $D\\bar{D}^\\ast$, $D^\\ast\\bar{D}^\\ast$, $\\pi\\bar{\\pi}$, and $K\\bar{K}$. Our results indicate that the decay width of the $\\Lambda_c\\bar{\\Lambda}_c$ bound state with $J^{PC}=1^{--}$ is much larger than that with $J^{PC}=0^{-+}$, and among their decay channels, the $D\\bar{D}^\\ast$ final state is the main decay mode. We suggest experiments to search for the $\\Lambda_c\\bar{\\Lambda}_c$ bound states in the $D\\bar{D}^\\ast$ final state.","sentences":["In this work, we investigate possible bound states of the $\\Lambda_c\\bar{\\Lambda}_c$ system in the Bethe-Salpeter formalism in the ladder and instantaneous approximations.","By numerically solving the Bethe-Salpeter equation, we confirm the existence of $\\Lambda_c\\bar{\\Lambda}_c$ bound states with quantum numbers $J^{PC}=0^{-+}$ and $J^{PC}=1^{--}$. We further investigate the partial decay widths of the $\\Lambda_c\\bar{\\Lambda}_c$ bound states into $N\\bar{N}$, $D\\bar{D}$, $D\\bar{D}^\\ast$, $D^\\ast\\bar{D}^\\ast$, $\\pi\\bar{\\pi}$, and $K\\bar{K}$. Our results indicate that the decay width of the $\\Lambda_c\\bar{\\Lambda}_c$ bound state with $J^{PC}=1^{--}$ is much larger than that with $J^{PC}=0^{-+}$, and among their decay channels, the $D\\bar{D}^\\ast$ final state is the main decay mode.","We suggest experiments to search for the $\\Lambda_c\\bar{\\Lambda}_c$ bound states in the $D\\bar{D}^\\ast$ final state."],"url":"http://arxiv.org/abs/2403.11066v1","category":"hep-ph"}
{"created":"2024-03-17 02:41:53","title":"Double-Private Distributed Estimation Algorithm Using Differential Privacy and a Key-Like Proportionate Matrix with Its Performance Analysis","abstract":"In this brief, we present an enhanced privacy-preserving distributed estimation algorithm, referred to as the ``Double-Private Algorithm,\" which combines the principles of both differential privacy (DP) and cryptography. The proposed algorithm enhances privacy by introducing DP noise into the intermediate estimations of neighboring nodes. Additionally, we employ an inverse of a closed-form reproducible proportionate gain matrix as the cryptographic key matrix to fortify the privacy protection within the proposed double private algorithm. \\textcolor{blue}{We improve the algorithm by transmitting alternative variable vectors instead of raw measurements, resulting in enhanced key matrix reconstruction performance. This innovative approach mitigate noise impact, enhancing overall algorithm effectiveness.} We also establish an upper bound for the norm of the error between the non-private Diffusion Least Mean Square (DLMS) algorithm and our double private algorithm. Further, we determine a sufficient condition for the step-size to ensure the mean convergence of the proposed algorithm. Simulation results demonstrate the effectiveness of the proposed algorithm, particularly its ability to attain the final Mean Square Deviation (MSD) comparable to that of the non-private DLMS.","sentences":["In this brief, we present an enhanced privacy-preserving distributed estimation algorithm, referred to as the ``Double-Private Algorithm,\" which combines the principles of both differential privacy (DP) and cryptography.","The proposed algorithm enhances privacy by introducing DP noise into the intermediate estimations of neighboring nodes.","Additionally, we employ an inverse of a closed-form reproducible proportionate gain matrix as the cryptographic key matrix to fortify the privacy protection within the proposed double private algorithm.","\\textcolor{blue}{We improve the algorithm by transmitting alternative variable vectors instead of raw measurements, resulting in enhanced key matrix reconstruction performance.","This innovative approach mitigate noise impact, enhancing overall algorithm effectiveness.","}","We also establish an upper bound for the norm of the error between the non-private Diffusion Least Mean Square (DLMS) algorithm and our double private algorithm.","Further, we determine a sufficient condition for the step-size to ensure the mean convergence of the proposed algorithm.","Simulation results demonstrate the effectiveness of the proposed algorithm, particularly its ability to attain the final Mean Square Deviation (MSD) comparable to that of the non-private DLMS."],"url":"http://arxiv.org/abs/2403.11064v1","category":"eess.SP"}
{"created":"2024-03-17 02:11:40","title":"Formal derivations from Boltzmann equation to three stationary equations","abstract":"In this paper, we concentrate on the connection between Boltzmann equation and stationary equations. To our knowledge, the stationary Navier-Stokes-Fourier system, the stationary Euler equations and the stationary Stokes equations are formally derived by moment estimate in the first time and extend the results of Bardos, Golse, and Levermore in J. Statist. Phys. 63(1-2), 323-344, 1991.","sentences":["In this paper, we concentrate on the connection between Boltzmann equation and stationary equations.","To our knowledge, the stationary Navier-Stokes-Fourier system, the stationary Euler equations and the stationary Stokes equations are formally derived by moment estimate in the first time and extend the results of Bardos, Golse, and Levermore in J. Statist.","Phys.","63(1-2), 323-344, 1991."],"url":"http://arxiv.org/abs/2403.11058v1","category":"math.AP"}
{"created":"2024-03-16 23:24:03","title":"FAGH: Accelerating Federated Learning with Approximated Global Hessian","abstract":"In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communication rounds and thus the shortened training time. Experimental results verify FAGH's effectiveness in decreasing the number of communication rounds and the time required to achieve the pre-specified objectives of the global model performance in terms of training and test losses as well as test accuracy. Notably, FAGH outperforms several state-of-the-art FL training methods.","sentences":["In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge.","Specifically, a large number of communication rounds are required to achieve the convergence in FL.","One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate.","However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server.","To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training.","FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model.","By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communication rounds and thus the shortened training time.","Experimental results verify FAGH's effectiveness in decreasing the number of communication rounds and the time required to achieve the pre-specified objectives of the global model performance in terms of training and test losses as well as test accuracy.","Notably, FAGH outperforms several state-of-the-art FL training methods."],"url":"http://arxiv.org/abs/2403.11041v1","category":"cs.LG"}
{"created":"2024-03-16 22:49:47","title":"Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed Diffractive Optical Processor","abstract":"Quantitative phase imaging (QPI) is a label-free technique that provides optical path length information for transparent specimens, finding utility in biology, materials science, and engineering. Here, we present quantitative phase imaging of a 3D stack of phase-only objects using a wavelength-multiplexed diffractive optical processor. Utilizing multiple spatially engineered diffractive layers trained through deep learning, this diffractive processor can transform the phase distributions of multiple 2D objects at various axial positions into intensity patterns, each encoded at a unique wavelength channel. These wavelength-multiplexed patterns are projected onto a single field-of-view (FOV) at the output plane of the diffractive processor, enabling the capture of quantitative phase distributions of input objects located at different axial planes using an intensity-only image sensor. Based on numerical simulations, we show that our diffractive processor could simultaneously achieve all-optical quantitative phase imaging across several distinct axial planes at the input by scanning the illumination wavelength. A proof-of-concept experiment with a 3D-fabricated diffractive processor further validated our approach, showcasing successful imaging of two distinct phase objects at different axial positions by scanning the illumination wavelength in the terahertz spectrum. Diffractive network-based multiplane QPI designs can open up new avenues for compact on-chip phase imaging and sensing devices.","sentences":["Quantitative phase imaging (QPI) is a label-free technique that provides optical path length information for transparent specimens, finding utility in biology, materials science, and engineering.","Here, we present quantitative phase imaging of a 3D stack of phase-only objects using a wavelength-multiplexed diffractive optical processor.","Utilizing multiple spatially engineered diffractive layers trained through deep learning, this diffractive processor can transform the phase distributions of multiple 2D objects at various axial positions into intensity patterns, each encoded at a unique wavelength channel.","These wavelength-multiplexed patterns are projected onto a single field-of-view (FOV) at the output plane of the diffractive processor, enabling the capture of quantitative phase distributions of input objects located at different axial planes using an intensity-only image sensor.","Based on numerical simulations, we show that our diffractive processor could simultaneously achieve all-optical quantitative phase imaging across several distinct axial planes at the input by scanning the illumination wavelength.","A proof-of-concept experiment with a 3D-fabricated diffractive processor further validated our approach, showcasing successful imaging of two distinct phase objects at different axial positions by scanning the illumination wavelength in the terahertz spectrum.","Diffractive network-based multiplane QPI designs can open up new avenues for compact on-chip phase imaging and sensing devices."],"url":"http://arxiv.org/abs/2403.11035v1","category":"physics.optics"}
{"created":"2024-03-16 22:20:03","title":"Cohesion and Shear Strength of Compacted Lunar and Martian Regolith Simulants","abstract":"Shear strength and cohesion of granular materials are important geotechnical properties that play a crucial role in the stability and behavior of lunar and Martian regolith, as well as their terrestrial analog materials. To characterize and predict shear strength and cohesion for future space missions, it is also important to understand the effects of particle size distribution and density on these fundamental geotechnical properties. Generalized equations have been established using empirical data from direct shear measurements of lunar and Martian regolith simulants to quantify the effects of particle size distribution and density on cohesion and shear strength. Preliminary results are also presented highlighting the effects of atmospheric absorbed water on shear strength and cohesion when conducting experiments in atmospheric conditions on Earth. The results of this study show that cohesion increases exponentially with bulk density, while the exponential growth constant is also dependent on particle size distribution.","sentences":["Shear strength and cohesion of granular materials are important geotechnical properties that play a crucial role in the stability and behavior of lunar and Martian regolith, as well as their terrestrial analog materials.","To characterize and predict shear strength and cohesion for future space missions, it is also important to understand the effects of particle size distribution and density on these fundamental geotechnical properties.","Generalized equations have been established using empirical data from direct shear measurements of lunar and Martian regolith simulants to quantify the effects of particle size distribution and density on cohesion and shear strength.","Preliminary results are also presented highlighting the effects of atmospheric absorbed water on shear strength and cohesion when conducting experiments in atmospheric conditions on Earth.","The results of this study show that cohesion increases exponentially with bulk density, while the exponential growth constant is also dependent on particle size distribution."],"url":"http://arxiv.org/abs/2403.11029v1","category":"physics.space-ph"}
{"created":"2024-03-16 22:01:55","title":"EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration","abstract":"Transformers have emerged as the state-of-the-art architecture in medical image registration, outperforming convolutional neural networks (CNNs) by addressing their limited receptive fields and overcoming gradient instability in deeper models. Despite their success, transformer-based models require substantial resources for training, including data, memory, and computational power, which may restrict their applicability for end users with limited resources. In particular, existing transformer-based 3D image registration architectures face three critical gaps that challenge their efficiency and effectiveness. Firstly, while mitigating the quadratic complexity of full attention by focusing on local regions, window-based attention mechanisms often fail to adequately integrate local and global information. Secondly, feature similarities across attention heads that were recently found in multi-head attention architectures indicate a significant computational redundancy, suggesting that the capacity of the network could be better utilized to enhance performance. Lastly, the granularity of tokenization, a key factor in registration accuracy, presents a trade-off; smaller tokens improve detail capture at the cost of higher computational complexity, increased memory demands, and a risk of overfitting. Here, we propose EfficientMorph, a transformer-based architecture for unsupervised 3D image registration. It optimizes the balance between local and global attention through a plane-based attention mechanism, reduces computational redundancy via cascaded group attention, and captures fine details without compromising computational efficiency, thanks to a Hi-Res tokenization strategy complemented by merging operations. Notably, EfficientMorph sets a new benchmark for performance on the OASIS dataset with 16-27x fewer parameters.","sentences":["Transformers have emerged as the state-of-the-art architecture in medical image registration, outperforming convolutional neural networks (CNNs) by addressing their limited receptive fields and overcoming gradient instability in deeper models.","Despite their success, transformer-based models require substantial resources for training, including data, memory, and computational power, which may restrict their applicability for end users with limited resources.","In particular, existing transformer-based 3D image registration architectures face three critical gaps that challenge their efficiency and effectiveness.","Firstly, while mitigating the quadratic complexity of full attention by focusing on local regions, window-based attention mechanisms often fail to adequately integrate local and global information.","Secondly, feature similarities across attention heads that were recently found in multi-head attention architectures indicate a significant computational redundancy, suggesting that the capacity of the network could be better utilized to enhance performance.","Lastly, the granularity of tokenization, a key factor in registration accuracy, presents a trade-off; smaller tokens improve detail capture at the cost of higher computational complexity, increased memory demands, and a risk of overfitting.","Here, we propose EfficientMorph, a transformer-based architecture for unsupervised 3D image registration.","It optimizes the balance between local and global attention through a plane-based attention mechanism, reduces computational redundancy via cascaded group attention, and captures fine details without compromising computational efficiency, thanks to a Hi-Res tokenization strategy complemented by merging operations.","Notably, EfficientMorph sets a new benchmark for performance on the OASIS dataset with 16-27x fewer parameters."],"url":"http://arxiv.org/abs/2403.11026v1","category":"cs.CV"}
{"created":"2024-03-16 21:14:35","title":"Carrier confinement and alloy disorder exacerbate Auger-Meitner recombination in AlGaN ultraviolet light-emitting diodes","abstract":"The quantum efficiency of AlGaN ultraviolet light-emitting diodes (LEDs) declines (droops) at increasing operating powers due to Auger-Meitner recombination (AMR). Using first-principles density-functional theory, we show that indirect AMR mediated by electron-phonon coupling and alloy disorder can induce bulk $C$ coefficients as large as $\\sim10^{-31}$ cm$^6$/s. Furthermore, we find that the confinement of carriers by polarization fields within quantum wells severely relaxes crystal-momentum conservation, which exacerbates the rate of AMR over radiative recombination by an order of magnitude relative to the bulk. This results in a striking decrease in quantum efficiency at high power. Suppressing polarization fields and jointly increasing the well width would greatly mitigate AMR and efficiency droop.","sentences":["The quantum efficiency of AlGaN ultraviolet light-emitting diodes (LEDs) declines (droops) at increasing operating powers due to Auger-Meitner recombination (AMR).","Using first-principles density-functional theory, we show that indirect AMR mediated by electron-phonon coupling and alloy disorder can induce bulk $C$ coefficients as large as $\\sim10^{-31}$ cm$^6$/s.","Furthermore, we find that the confinement of carriers by polarization fields within quantum wells severely relaxes crystal-momentum conservation, which exacerbates the rate of AMR over radiative recombination by an order of magnitude relative to the bulk.","This results in a striking decrease in quantum efficiency at high power.","Suppressing polarization fields and jointly increasing the well width would greatly mitigate AMR and efficiency droop."],"url":"http://arxiv.org/abs/2403.11019v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-16 21:05:05","title":"Continuous-time mediation analysis for repeatedlymeasured mediators and outcomes","abstract":"Mediation analysis aims to decipher the underlying causal mechanisms between an exposure, an outcome, and intermediate variables called mediators. Initially developed for fixed-time mediator and outcome, it has been extended to the framework of longitudinal data by discretizing the assessment times of mediator and outcome. Yet, processes in play in longitudinal studies are usually defined in continuous time and measured at irregular and subject-specific visits. This is the case in dementia research when cerebral and cognitive changes measured at planned visits in cohorts are of interest. We thus propose a methodology to estimate the causal mechanisms between a time-fixed exposure ($X$), a mediator process ($\\mathcal{M}_t$) and an outcome process ($\\mathcal{Y}_t$) both measured repeatedly over time in the presence of a time-dependent confounding process ($\\mathcal{L}_t$). We consider three types of causal estimands, the natural effects, path-specific effects and randomized interventional analogues to natural effects, and provide identifiability assumptions. We employ a dynamic multivariate model based on differential equations for their estimation. The performance of the methods are explored in simulations, and we illustrate the method in two real-world examples motivated by the 3C cerebral aging study to assess: (1) the effect of educational level on functional dependency through depressive symptomatology and cognitive functioning, and (2) the effect of a genetic factor on cognitive functioning potentially mediated by vascular brain lesions and confounded by neurodegeneration.","sentences":["Mediation analysis aims to decipher the underlying causal mechanisms between an exposure, an outcome, and intermediate variables called mediators.","Initially developed for fixed-time mediator and outcome, it has been extended to the framework of longitudinal data by discretizing the assessment times of mediator and outcome.","Yet, processes in play in longitudinal studies are usually defined in continuous time and measured at irregular and subject-specific visits.","This is the case in dementia research when cerebral and cognitive changes measured at planned visits in cohorts are of interest.","We thus propose a methodology to estimate the causal mechanisms between a time-fixed exposure ($X$), a mediator process ($\\mathcal{M}_t$) and an outcome process ($\\mathcal{Y}_t$) both measured repeatedly over time in the presence of a time-dependent confounding process ($\\mathcal{L}_t$).","We consider three types of causal estimands, the natural effects, path-specific effects and randomized interventional analogues to natural effects, and provide identifiability assumptions.","We employ a dynamic multivariate model based on differential equations for their estimation.","The performance of the methods are explored in simulations, and we illustrate the method in two real-world examples motivated by the 3C cerebral aging study to assess: (1) the effect of educational level on functional dependency through depressive symptomatology and cognitive functioning, and (2) the effect of a genetic factor on cognitive functioning potentially mediated by vascular brain lesions and confounded by neurodegeneration."],"url":"http://arxiv.org/abs/2403.11017v1","category":"stat.ME"}
{"created":"2024-03-16 20:43:34","title":"Beyond May: Complexity-stability relationships in disordered dynamical systems","abstract":"Robert May famously used random matrix theory to predict that large, complex systems cannot admit stable fixed points. However, this general conclusion is not supported by empirical observation: from cells to biomes, biological systems are large, complex -- and, by and large, stable. In this paper, we revisit May's argument in the light of recent developments in both ecology and random matrix theory. Using a non-linear generalization of the Lotka-Volterra model, we show that there are in fact two kinds of complexity-stability relationships in disordered dynamical systems: if self-interactions grow faster with density than cross-interactions, complexity is destabilizing; but if cross-interactions grow faster than self-interactions, complexity is stabilizing. Our result shows that May's principle that \"complexity begets instability\" is not a general property of complex systems; instead, it is a property of a subclass of weakly cross-regulated disordered systems.","sentences":["Robert May famously used random matrix theory to predict that large, complex systems cannot admit stable fixed points.","However, this general conclusion is not supported by empirical observation: from cells to biomes, biological systems are large, complex -- and, by and large, stable.","In this paper, we revisit May's argument in the light of recent developments in both ecology and random matrix theory.","Using a non-linear generalization of the Lotka-Volterra model, we show that there are in fact two kinds of complexity-stability relationships in disordered dynamical systems: if self-interactions grow faster with density than cross-interactions, complexity is destabilizing; but if cross-interactions grow faster than self-interactions, complexity is stabilizing.","Our result shows that May's principle that \"complexity begets instability\" is not a general property of complex systems; instead, it is a property of a subclass of weakly cross-regulated disordered systems."],"url":"http://arxiv.org/abs/2403.11014v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-16 18:08:09","title":"Coherent Acoustic Control of Defect Orbital States in the Strong-Driving Limit","abstract":"We use a bulk acoustic wave resonator to demonstrate coherent control of the excited orbital states in a diamond nitrogen-vacancy (NV) center at cryogenic temperature. Coherent quantum control is an essential tool for understanding and mitigating decoherence. Moreover, characterizing and controlling orbital states is a central challenge for quantum networking, where optical coherence is tied to orbital coherence. We study resonant multi-phonon orbital Rabi oscillations in both the frequency and time domain, extracting the strength of the orbital-phonon interactions and the coherence of the acoustically driven orbital states. We reach the strong-driving limit, where the physics is dominated by the coupling induced by the acoustic waves. We find agreement between our measurements, quantum master equation simulations, and a Landau-Zener transition model in the strong-driving limit. Using perturbation theory, we derive an expression for the orbital Rabi frequency versus acoustic drive strength that is non-perturbative in the drive strength and agrees well with our measurements for all acoustic powers. Motivated by continuous wave spin resonance-based decoherence protection schemes, we model the orbital decoherence and find good agreement between our model and our measured few-to-several nanoseconds orbital decoherence times. We discuss the outlook for orbital decoherence protection.","sentences":["We use a bulk acoustic wave resonator to demonstrate coherent control of the excited orbital states in a diamond nitrogen-vacancy (NV) center at cryogenic temperature.","Coherent quantum control is an essential tool for understanding and mitigating decoherence.","Moreover, characterizing and controlling orbital states is a central challenge for quantum networking, where optical coherence is tied to orbital coherence.","We study resonant multi-phonon orbital Rabi oscillations in both the frequency and time domain, extracting the strength of the orbital-phonon interactions and the coherence of the acoustically driven orbital states.","We reach the strong-driving limit, where the physics is dominated by the coupling induced by the acoustic waves.","We find agreement between our measurements, quantum master equation simulations, and a Landau-Zener transition model in the strong-driving limit.","Using perturbation theory, we derive an expression for the orbital Rabi frequency versus acoustic drive strength that is non-perturbative in the drive strength and agrees well with our measurements for all acoustic powers.","Motivated by continuous wave spin resonance-based decoherence protection schemes, we model the orbital decoherence and find good agreement between our model and our measured few-to-several nanoseconds orbital decoherence times.","We discuss the outlook for orbital decoherence protection."],"url":"http://arxiv.org/abs/2403.10989v1","category":"quant-ph"}
{"created":"2024-03-16 17:52:33","title":"MPC for Tracking applied to rendezvous with non-cooperative tumbling targets ensuring stability and feasibility","abstract":"A Model Predictive Controller for Tracking is introduced for rendezvous with non-cooperative tumbling targets in active debris removal applications. The target's three-dimensional non-periodic rotational dynamics as well as other state and control constraints are considered. The approach is based on applying an intermediate coordinate transformation that eliminates the time-dependency due to rotations in the constraints. The control law is then found as the solution to a QP problem with linear constraints and dynamics, as derived from the HCW equations, that provides feasibility and stability guarantees by means of a terminal LQR and dead-beat region. The proposed control algorithm performs well in a realistic simulation scenario, namely a near rendezvous with the Envisat spacecraft.","sentences":["A Model Predictive Controller for Tracking is introduced for rendezvous with non-cooperative tumbling targets in active debris removal applications.","The target's three-dimensional non-periodic rotational dynamics as well as other state and control constraints are considered.","The approach is based on applying an intermediate coordinate transformation that eliminates the time-dependency due to rotations in the constraints.","The control law is then found as the solution to a QP problem with linear constraints and dynamics, as derived from the HCW equations, that provides feasibility and stability guarantees by means of a terminal LQR and dead-beat region.","The proposed control algorithm performs well in a realistic simulation scenario, namely a near rendezvous with the Envisat spacecraft."],"url":"http://arxiv.org/abs/2403.10986v1","category":"eess.SY"}
{"created":"2024-03-16 16:17:47","title":"Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!","abstract":"While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural \"shortcuts\", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour highlights several general challenges for LR NMT, such as modern tokenization strategies, noisy real-world conditions, and linguistic complexities. We call for better scrutiny of linguistically motivated improvements to NMT given the blackbox nature of Transformer models, as well as for a focus on the above problems in the field.","sentences":["While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it.","In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural \"shortcuts\", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings.","We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings.","However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords.","Our discussion of the reasons for this behaviour highlights several general challenges for LR NMT, such as modern tokenization strategies, noisy real-world conditions, and linguistic complexities.","We call for better scrutiny of linguistically motivated improvements to NMT given the blackbox nature of Transformer models, as well as for a focus on the above problems in the field."],"url":"http://arxiv.org/abs/2403.10963v1","category":"cs.CL"}
{"created":"2024-03-16 16:16:31","title":"Energy-Based Models with Applications to Speech and Language Processing","abstract":"Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in speech and language processing. First, the basics of EBMs are introduced, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones. Then, the application of EBMs in three different scenarios is presented, i.e., for modeling marginal, conditional and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where the main focus is on the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in speech recognition, sequence labeling and text generation; 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural language understanding.","sentences":["Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models.","EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs).","Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress.","The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images).","Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in speech and language processing.","First, the basics of EBMs are introduced, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones.","Then, the application of EBMs in three different scenarios is presented, i.e., for modeling marginal, conditional and joint distributions, respectively.","1) EBMs for sequential data with applications in language modeling, where the main focus is on the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in speech recognition, sequence labeling and text generation; 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural language understanding."],"url":"http://arxiv.org/abs/2403.10961v1","category":"cs.LG"}
{"created":"2024-03-16 16:16:08","title":"Infinitely many normalized solutions of $L^2$-supercritical NLS equations on noncompact metric graphs with localized nonlinearities","abstract":"We consider the existence of solutions for nonlinear Schr\\\"odinger equations on noncompact metric graphs with localized nonlinearities. In an $L^2$-supercritical regime, we establish the existence of infinitely many solutions for any prescribed mass.","sentences":["We consider the existence of solutions for nonlinear Schr\\\"odinger equations on noncompact metric graphs with localized nonlinearities.","In an $L^2$-supercritical regime, we establish the existence of infinitely many solutions for any prescribed mass."],"url":"http://arxiv.org/abs/2403.10959v1","category":"math.AP"}
{"created":"2024-03-16 14:30:25","title":"Channel-wise Feature Decorrelation for Enhanced Learned Image Compression","abstract":"The emerging Learned Compression (LC) replaces the traditional codec modules with Deep Neural Networks (DNN), which are trained end-to-end for rate-distortion performance. This approach is considered as the future of image/video compression, and major efforts have been dedicated to improving its compression efficiency. However, most proposed works target compression efficiency by employing more complex DNNS, which contributes to higher computational complexity. Alternatively, this paper proposes to improve compression by fully exploiting the existing DNN capacity. To do so, the latent features are guided to learn a richer and more diverse set of features, which corresponds to better reconstruction. A channel-wise feature decorrelation loss is designed and is integrated into the LC optimization. Three strategies are proposed and evaluated, which optimize (1) the transformation network, (2) the context model, and (3) both networks. Experimental results on two established LC methods show that the proposed method improves the compression with a BD-Rate of up to 8.06%, with no added complexity. The proposed solution can be applied as a plug-and-play solution to optimize any similar LC method.","sentences":["The emerging Learned Compression (LC) replaces the traditional codec modules with Deep Neural Networks (DNN), which are trained end-to-end for rate-distortion performance.","This approach is considered as the future of image/video compression, and major efforts have been dedicated to improving its compression efficiency.","However, most proposed works target compression efficiency by employing more complex DNNS, which contributes to higher computational complexity.","Alternatively, this paper proposes to improve compression by fully exploiting the existing DNN capacity.","To do so, the latent features are guided to learn a richer and more diverse set of features, which corresponds to better reconstruction.","A channel-wise feature decorrelation loss is designed and is integrated into the LC optimization.","Three strategies are proposed and evaluated, which optimize (1) the transformation network, (2) the context model, and (3) both networks.","Experimental results on two established LC methods show that the proposed method improves the compression with a BD-Rate of up to 8.06%, with no added complexity.","The proposed solution can be applied as a plug-and-play solution to optimize any similar LC method."],"url":"http://arxiv.org/abs/2403.10936v1","category":"eess.IV"}
{"created":"2024-03-16 14:23:17","title":"Understanding Robustness of Visual State Space Models for Image Classification","abstract":"Visual State Space Model (VMamba) has recently emerged as a promising architecture, exhibiting remarkable performance in various computer vision tasks. However, its robustness has not yet been thoroughly studied. In this paper, we delve into the robustness of this architecture through comprehensive investigations from multiple perspectives. Firstly, we investigate its robustness to adversarial attacks, employing both whole-image and patch-specific adversarial attacks. Results demonstrate superior adversarial robustness compared to Transformer architectures while revealing scalability weaknesses. Secondly, the general robustness of VMamba is assessed against diverse scenarios, including natural adversarial examples, out-of-distribution data, and common corruptions. VMamba exhibits exceptional generalizability with out-of-distribution data but shows scalability weaknesses against natural adversarial examples and common corruptions. Additionally, we explore VMamba's gradients and back-propagation during white-box attacks, uncovering unique vulnerabilities and defensive capabilities of its novel components. Lastly, the sensitivity of VMamba to image structure variations is examined, highlighting vulnerabilities associated with the distribution of disturbance areas and spatial information, with increased susceptibility closer to the image center. Through these comprehensive studies, we contribute to a deeper understanding of VMamba's robustness, providing valuable insights for refining and advancing the capabilities of deep neural networks in computer vision applications.","sentences":["Visual State Space Model (VMamba) has recently emerged as a promising architecture, exhibiting remarkable performance in various computer vision tasks.","However, its robustness has not yet been thoroughly studied.","In this paper, we delve into the robustness of this architecture through comprehensive investigations from multiple perspectives.","Firstly, we investigate its robustness to adversarial attacks, employing both whole-image and patch-specific adversarial attacks.","Results demonstrate superior adversarial robustness compared to Transformer architectures while revealing scalability weaknesses.","Secondly, the general robustness of VMamba is assessed against diverse scenarios, including natural adversarial examples, out-of-distribution data, and common corruptions.","VMamba exhibits exceptional generalizability with out-of-distribution data but shows scalability weaknesses against natural adversarial examples and common corruptions.","Additionally, we explore VMamba's gradients and back-propagation during white-box attacks, uncovering unique vulnerabilities and defensive capabilities of its novel components.","Lastly, the sensitivity of VMamba to image structure variations is examined, highlighting vulnerabilities associated with the distribution of disturbance areas and spatial information, with increased susceptibility closer to the image center.","Through these comprehensive studies, we contribute to a deeper understanding of VMamba's robustness, providing valuable insights for refining and advancing the capabilities of deep neural networks in computer vision applications."],"url":"http://arxiv.org/abs/2403.10935v1","category":"cs.CV"}
{"created":"2024-03-16 13:55:44","title":"Liquid-Liquid Crossover in Water Model: Local Structure vs. Kinetics of Hydrogen Bonds","abstract":"In equilibrium and supercooled liquids, polymorphism is manifested by thermodynamic regions defined in the phase diagram, which are predominantly of different short- and medium-range order (local structure). It is found that on the phase diagram of the water model, the thermodynamic region corresponding to the equilibrium liquid phase is divided by a line of the smooth liquid-liquid crossover. In the case of the water model, this crossover is revealed by various local order parameters and corresponds to pressures of the order of $3\\,150 \\pm 350$ atm at ambient temperature. In the vicinity of the crossover, the dynamics of water molecules change significantly, which is reflected, in particular, in the fact that the self-diffusion coefficient reaches its maximum values. In addition, changes in the structure also manifest themselves in changes in the kinetics of hydrogen bonding, which is captured by values of such the quantities as the average lifetime of hydrogen bonding, the average lifetimes of different local coordination numbers, and the frequencies of changes in different local coordination numbers. An interpretation of the hydrogen bond kinetics in terms of the free energy landscape concept in the space of possible coordination numbers is proposed.","sentences":["In equilibrium and supercooled liquids, polymorphism is manifested by thermodynamic regions defined in the phase diagram, which are predominantly of different short- and medium-range order (local structure).","It is found that on the phase diagram of the water model, the thermodynamic region corresponding to the equilibrium liquid phase is divided by a line of the smooth liquid-liquid crossover.","In the case of the water model, this crossover is revealed by various local order parameters and corresponds to pressures of the order of $3\\,150 \\pm 350$ atm at ambient temperature.","In the vicinity of the crossover, the dynamics of water molecules change significantly, which is reflected, in particular, in the fact that the self-diffusion coefficient reaches its maximum values.","In addition, changes in the structure also manifest themselves in changes in the kinetics of hydrogen bonding, which is captured by values of such the quantities as the average lifetime of hydrogen bonding, the average lifetimes of different local coordination numbers, and the frequencies of changes in different local coordination numbers.","An interpretation of the hydrogen bond kinetics in terms of the free energy landscape concept in the space of possible coordination numbers is proposed."],"url":"http://arxiv.org/abs/2403.10928v1","category":"cond-mat.soft"}
{"created":"2024-03-16 13:34:39","title":"Radiative transitions of $\u03c7_{_{cJ}}\\to\u03c8\u03b3$ and $\u03c7_{_{bJ}}\\to\u03a5\u03b3$","abstract":"In the framework of instantaneous Bethe-Salpeter equation, according to the $J ^ {PC}$ of quarkonia, we find that their wave functions all contain multiple partial waves, rather than pure waves. In the radiative electromagnetic transitions $\\chi_{_{cJ}}$$\\rightarrow$$\\gamma\\psi$ and $\\chi_{_{bJ}}$$\\rightarrow$$\\gamma\\Upsilon$ ($J=0,1,2$), the main wave of quarkonium gives the non-relativistic contribution, while other waves provide the relativistic corrections. Our results indicate that the relativistic effect of charmonium, especially highly excited states, is significant. Such as the relativistic effects of $\\chi_{_{cJ}}(2P)\\to\\gamma\\psi(1S)$ ($J=0,1,2$) are $\\{49.7\\%,~30.9\\%,~37.5\\%\\}$, much larger than the corresponding $\\{17.8\\%,~7.08\\%,~12.9\\%\\}$ of $\\chi_{_{bJ}}(2P)\\rightarrow\\gamma\\Upsilon(1S)$. The decay of $\\chi_{_{cJ}}(2P)\\to\\gamma\\psi$ can be used to distinguish between $\\chi_{_{c0}}(3860)$ and $\\chi_{_{c0}}(3915)$, which particle is the charmonium $\\chi_{_{c0}}(2P)$. Although our result of $\\chi_{_{c1}}(3872)$$\\rightarrow$$\\gamma\\psi(2S)$ is consistent with data, but the one of $\\chi_{_{c1}}(3872)$$\\rightarrow$$\\gamma\\psi(1S)$ is much larger than data, so whether $\\chi_{_{c1}}(3872)$ is the conventional $\\chi_{_{c1}}(2P)$ remains an open question. The undiscovered $\\Upsilon(1D)$ and $\\Upsilon(2D)$ have large production rates in decays of $\\chi_{_{b0}}(2P)\\rightarrow\\gamma\\Upsilon(1D)$ and $\\chi_{_{bJ}}(3P)\\rightarrow\\gamma\\Upsilon(2D)$ ($J=0,1$), respectively. To search for $\\chi_{_{bJ}}(3P)$ $(J=0,1,2)$, the most competitive channels are the decays $\\chi_{_{bJ}}(3P)\\rightarrow\\gamma\\Upsilon(3S)$. And the best way to find $\\chi_{_{b2}}(1F)$ is to search for the decay of $\\chi_{_{b2}}(1F)\\rightarrow\\gamma\\Upsilon(1D)$.","sentences":["In the framework of instantaneous Bethe-Salpeter equation, according to the $J ^ {PC}$ of quarkonia, we find that their wave functions all contain multiple partial waves, rather than pure waves.","In the radiative electromagnetic transitions $\\chi_{_{cJ}}$$\\rightarrow$$\\gamma\\psi$ and $\\chi_{_{bJ}}$$\\rightarrow$$\\gamma\\Upsilon$ ($J=0,1,2$), the main wave of quarkonium gives the non-relativistic contribution, while other waves provide the relativistic corrections.","Our results indicate that the relativistic effect of charmonium, especially highly excited states, is significant.","Such as the relativistic effects of $\\chi_{_{cJ}}(2P)\\to\\gamma\\psi(1S)$ ($J=0,1,2$) are $\\{49.7\\%,~30.9\\%,~37.5\\%\\}$, much larger than the corresponding $\\{17.8\\%,~7.08\\%,~12.9\\%\\}$ of $\\chi_{_{bJ}}(2P)\\rightarrow\\gamma\\Upsilon(1S)$. The decay of $\\chi_{_{cJ}}(2P)\\to\\gamma\\psi$ can be used to distinguish between $\\chi_{_{c0}}(3860)$ and $\\chi_{_{c0}}(3915)$, which particle is the charmonium $\\chi_{_{c0}}(2P)$.","Although our result of $\\chi_{_{c1}}(3872)$$\\rightarrow$$\\gamma\\psi(2S)$ is consistent with data, but the one of $\\chi_{_{c1}}(3872)$$\\rightarrow$$\\gamma\\psi(1S)$ is much larger than data, so whether $\\chi_{_{c1}}(3872)$ is the conventional $\\chi_{_{c1}}(2P)$ remains an open question.","The undiscovered $\\Upsilon(1D)$ and $\\Upsilon(2D)$ have large production rates in decays of $\\chi_{_{b0}}(2P)\\rightarrow\\gamma\\Upsilon(1D)$ and $\\chi_{_{bJ}}(3P)\\rightarrow\\gamma\\Upsilon(2D)$ ($J=0,1$), respectively.","To search for $\\chi_{_{bJ}}(3P)$ $(J=0,1,2)$, the most competitive channels are the decays $\\chi_{_{bJ}}(3P)\\rightarrow\\gamma\\Upsilon(3S)$. And the best way to find $\\chi_{_{b2}}(1F)$ is to search for the decay of $\\chi_{_{b2}}(1F)\\rightarrow\\gamma\\Upsilon(1D)$."],"url":"http://arxiv.org/abs/2403.10922v1","category":"hep-ph"}
{"created":"2024-03-16 12:44:08","title":"FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation","abstract":"Fish stock assessment often involves manual fish counting by taxonomy specialists, which is both time-consuming and costly. We propose an automated computer vision system that performs both taxonomic classification and fish size estimation from images taken with a low-cost digital camera. The system first performs object detection and segmentation using a Mask R-CNN to identify individual fish from images containing multiple fish, possibly consisting of different species. Then each fish species is classified and the predicted length using separate machine learning models. These models are trained on a dataset of 50,000 hand-annotated images containing 163 different fish species, ranging in length from 10cm to 250cm. Evaluated on held-out test data, our system achieves a $92\\%$ intersection over union on the fish segmentation task, a $89\\%$ top-1 classification accuracy on single fish species classification, and a $2.3$~cm mean error on the fish length estimation task.","sentences":["Fish stock assessment often involves manual fish counting by taxonomy specialists, which is both time-consuming and costly.","We propose an automated computer vision system that performs both taxonomic classification and fish size estimation from images taken with a low-cost digital camera.","The system first performs object detection and segmentation using a Mask R-CNN to identify individual fish from images containing multiple fish, possibly consisting of different species.","Then each fish species is classified and the predicted length using separate machine learning models.","These models are trained on a dataset of 50,000 hand-annotated images containing 163 different fish species, ranging in length from 10cm to 250cm.","Evaluated on held-out test data, our system achieves a $92\\%$ intersection over union on the fish segmentation task, a $89\\%$ top-1 classification accuracy on single fish species classification, and a $2.3$~cm mean error on the fish length estimation task."],"url":"http://arxiv.org/abs/2403.10916v1","category":"cs.CV"}
{"created":"2024-03-16 12:36:22","title":"Semigroup of annuli in Liouville CFT","abstract":"In conformal field theory, the semigroup of annuli with boundary parametrization plays a special role, in that it generates the whole algebra of local conformal symmetries, the so-called Virasoro algebra. The subgroup of elements $\\mathbb{A}_f=\\mathbb{D}\\setminus f(\\mathbb{D}^\\circ)$ for contracting biholomorphisms $f:\\mathbb{D}\\to f(\\mathbb{D})\\subset \\mathbb{D}^\\circ$ with $f(0)=0$ is called the holomorphic semigroup of annuli. In this article, we construct a differentiable representation of the holomorphic semigroup into the space of bounded operators on the Hilbert space $\\mathcal{H}$ of Liouville Conformal Field Theory and show it generates under differentiation the positive Virasoro elements ${\\bf L}_n,\\tilde{{\\bf L}}_n$ for $n\\geq 0$. We also construct a projective representation of the semigroup of annuli in the space of bounded operators on $\\mathcal{H}$ in terms of Segal amplitudes and show that all Virasoro elements ${\\bf L}_n,\\tilde{{\\bf L}}_n$ for $n\\in \\mathbb{Z}$ are generated by differentiation of these annuli amplitudes. Finally, we use this to show that the Segal amplitudes for Liouville theory are differentiable with respetc to their boundary parametrizations, and the differential is computed in terms of Virasoro generators. This paper will serve, in a forthcoming work, as a fundamental tool in the construction of the conformal blocks as globally defined holomorphic sections of a holomorphic line bundle on Teichmuller space and satisfying the Ward identities.","sentences":["In conformal field theory, the semigroup of annuli with boundary parametrization plays a special role, in that it generates the whole algebra of local conformal symmetries, the so-called Virasoro algebra.","The subgroup of elements $\\mathbb{A}_f=\\mathbb{D}\\setminus f(\\mathbb{D}^\\circ)$ for contracting biholomorphisms $f:\\mathbb{D}\\to f(\\mathbb{D})\\subset \\mathbb{D}^\\circ$ with $f(0)=0$ is called the holomorphic semigroup of annuli.","In this article, we construct a differentiable representation of the holomorphic semigroup into the space of bounded operators on the Hilbert space $\\mathcal{H}$ of Liouville Conformal Field Theory and show it generates under differentiation the positive Virasoro elements ${\\bf L}_n,\\tilde{{\\bf L}}_n$ for $n\\geq 0$.","We also construct a projective representation of the semigroup of annuli in the space of bounded operators on $\\mathcal{H}$ in terms of Segal amplitudes and show that all Virasoro elements ${\\bf L}_n,\\tilde{{\\bf L}}_n$ for $n\\in \\mathbb{Z}$ are generated by differentiation of these annuli amplitudes.","Finally, we use this to show that the Segal amplitudes for Liouville theory are differentiable with respetc to their boundary parametrizations, and the differential is computed in terms of Virasoro generators.","This paper will serve, in a forthcoming work, as a fundamental tool in the construction of the conformal blocks as globally defined holomorphic sections of a holomorphic line bundle on Teichmuller space and satisfying the Ward identities."],"url":"http://arxiv.org/abs/2403.10914v1","category":"math.PR"}
{"created":"2024-03-16 12:25:30","title":"Automatic location detection based on deep learning","abstract":"The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification. Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities. Drawing from an extensive dataset, our model classifies images into five major Indian cities: Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state. To achieve high precision and recall rates, we adopted two approaches. The first, a vanilla Convolutional Neural Network (CNN) and then we explored the power of transfer learning by leveraging the VGG16 model. The vanilla CNN achieved commendable accuracy and the VGG16 model achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and potential areas of improvement, positioning our model as not only competitive but also scalable for broader applications. With an emphasis on open-source ethos, our work aims to contribute to the community, encouraging further development and diverse applications. Our findings demonstrate the potential applications in tourism, urban planning, and even real-time location identification systems, among others.","sentences":["The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification.","Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities.","Drawing from an extensive dataset, our model classifies images into five major Indian cities:","Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state.","To achieve high precision and recall rates, we adopted two approaches.","The first, a vanilla Convolutional Neural Network (CNN) and then we explored the power of transfer learning by leveraging the VGG16 model.","The vanilla CNN achieved commendable accuracy and the VGG16 model achieved a test accuracy of 63.6%.","Evaluations highlighted the strengths and potential areas of improvement, positioning our model as not only competitive but also scalable for broader applications.","With an emphasis on open-source ethos, our work aims to contribute to the community, encouraging further development and diverse applications.","Our findings demonstrate the potential applications in tourism, urban planning, and even real-time location identification systems, among others."],"url":"http://arxiv.org/abs/2403.10912v1","category":"cs.CV"}
{"created":"2024-03-16 11:38:58","title":"Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems","abstract":"Data-driven modeling of complex physical systems is receiving a growing amount of attention in the simulation and machine learning communities. Since most physical simulations are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of application areas. In this context, we present a novel benchmark for the evaluation of 1-step generative learning models in terms of speed and physical correctness. Our Urban Sound Propagation benchmark is based on the physically complex and practically relevant, yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in an urban environment. We provide a dataset with 100k samples, where each sample consists of pairs of real 2d building maps drawn from OpenStreetmap, a parameterized sound source, and a simulated ground truth sound propagation for the given scene. The dataset provides four different simulation tasks with increasing complexity regarding reflection, diffraction and source variance. A first baseline evaluation of common generative U-Net, GAN and Diffusion models shows, that while these models are very well capable of modeling sound propagations in simple cases, the approximation of sub-systems represented by higher order equations systematically fails. Information about the dataset, download instructions and source codes are provided on our anonymous website: https://www.urban-sound-data.org.","sentences":["Data-driven modeling of complex physical systems is receiving a growing amount of attention in the simulation and machine learning communities.","Since most physical simulations are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of application areas.","In this context, we present a novel benchmark for the evaluation of 1-step generative learning models in terms of speed and physical correctness.","Our Urban Sound Propagation benchmark is based on the physically complex and practically relevant, yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in an urban environment.","We provide a dataset with 100k samples, where each sample consists of pairs of real 2d building maps drawn from OpenStreetmap, a parameterized sound source, and a simulated ground truth sound propagation for the given scene.","The dataset provides four different simulation tasks with increasing complexity regarding reflection, diffraction and source variance.","A first baseline evaluation of common generative U-Net, GAN and Diffusion models shows, that while these models are very well capable of modeling sound propagations in simple cases, the approximation of sub-systems represented by higher order equations systematically fails.","Information about the dataset, download instructions and source codes are provided on our anonymous website: https://www.urban-sound-data.org."],"url":"http://arxiv.org/abs/2403.10904v1","category":"cs.SD"}
{"created":"2024-03-16 11:28:50","title":"$Herschel$ investigation of cores and filamentary structures in L1251 located in the Cepheus flare","abstract":"Context: Molecular clouds are the prime locations of star formation. These clouds contain filamentary structures and cores which are crucial in the formation of young stars. Aims: In this work, we aim to quantify the physical properties of structural characteristics within the molecular cloud L1251 to better understand the initial conditions for star formation. Methods: We applied the getsf algorithm to identify cores and filaments within the molecular cloud L1251 using the Herschel multiband dust continuum image, enabling us to measure their respective physical properties. Additionally, we utilized an enhanced differential term algorithm to produce high-resolution temperature maps and column density maps with a resolution of ${13.5}''$. Results: We identified 122 cores in the region. Out of them, 23 are protostellar cores, 13 are robust prestellar cores, 32 are candidate prestellar cores (including 13 robust prestellar cores and 19 strictly candidate prestellar cores), and 67 are unbound starless cores. getsf also found 147 filament structures in the region. Statistical analysis of the physical properties (mass (M), temperature (T), size, and core brightness (hereafter, we are using the word luminosity (L)) for the core brightness) of obtained cores shows a negative correlation between core mass and temperature and a positive correlation between (M/L) and (M/T). Analysis of the filaments gives a median width of 0.14 pc and no correlation between width and length. Out of those 122 cores, 92 are present in filaments (75.4%) and the remaining were outside them. Out of the cores present in filaments, 57 (62%) cores are present in supercritical filaments ($M_{\\rm line}>16M_{\\odot }/{\\rm pc}$).","sentences":["Context: Molecular clouds are the prime locations of star formation.","These clouds contain filamentary structures and cores which are crucial in the formation of young stars.","Aims:","In this work, we aim to quantify the physical properties of structural characteristics within the molecular cloud L1251 to better understand the initial conditions for star formation.","Methods: We applied the getsf algorithm to identify cores and filaments within the molecular cloud L1251 using the Herschel multiband dust continuum image, enabling us to measure their respective physical properties.","Additionally, we utilized an enhanced differential term algorithm to produce high-resolution temperature maps and column density maps with a resolution of ${13.5}''$. Results: We identified 122 cores in the region.","Out of them, 23 are protostellar cores, 13 are robust prestellar cores, 32 are candidate prestellar cores (including 13 robust prestellar cores and 19 strictly candidate prestellar cores), and 67 are unbound starless cores.","getsf also found 147 filament structures in the region.","Statistical analysis of the physical properties (mass (M), temperature (T), size, and core brightness (hereafter, we are using the word luminosity (L)) for the core brightness) of obtained cores shows a negative correlation between core mass and temperature and a positive correlation between (M/L) and (M/T).","Analysis of the filaments gives a median width of 0.14 pc and no correlation between width and length.","Out of those 122 cores, 92 are present in filaments (75.4%) and the remaining were outside them.","Out of the cores present in filaments, 57 (62%) cores are present in supercritical filaments ($M_{\\rm line}>16M_{\\odot }/{\\rm pc}$)."],"url":"http://arxiv.org/abs/2403.10901v1","category":"astro-ph.GA"}
{"created":"2024-03-16 11:24:53","title":"Role of Coulomb Interaction in Valley Photogalvanic Effect","abstract":"We develop a theory of Coulomb interaction-related contribution to the photogalvanic current of electrons in two-dimensional non-centrosymmetric Dirac materials possessing a nontrivial structure of valleys and exposed to an external electromagnetic field. The valley photogalvanic effect occurs here due to the trigonal warping of electrons and holes' dispersions in a given valley of the monolayer. We study the low-frequency limit of the external field: the field frequency is smaller than the temperature $T$, and the electron-electron and electron-hole scattering times are much larger than the electron-impurity scattering time. In this regime, we employ the Boltzmann transport equations and show that electron-hole scattering dominates electron-electron scattering in intrinsic semiconductors. Coulomb interaction-related contribution to the valley photogalvanic current can reduce the value of the bare photogalvanic current as these two currents flow in opposite directions.","sentences":["We develop a theory of Coulomb interaction-related contribution to the photogalvanic current of electrons in two-dimensional non-centrosymmetric Dirac materials possessing a nontrivial structure of valleys and exposed to an external electromagnetic field.","The valley photogalvanic effect occurs here due to the trigonal warping of electrons and holes' dispersions in a given valley of the monolayer.","We study the low-frequency limit of the external field: the field frequency is smaller than the temperature $T$, and the electron-electron and electron-hole scattering times are much larger than the electron-impurity scattering time.","In this regime, we employ the Boltzmann transport equations and show that electron-hole scattering dominates electron-electron scattering in intrinsic semiconductors.","Coulomb interaction-related contribution to the valley photogalvanic current can reduce the value of the bare photogalvanic current as these two currents flow in opposite directions."],"url":"http://arxiv.org/abs/2403.10898v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-16 10:43:12","title":"Could We Generate Cytology Images from Histopathology Images? An Empirical Study","abstract":"Automation in medical imaging is quite challenging due to the unavailability of annotated datasets and the scarcity of domain experts. In recent years, deep learning techniques have solved some complex medical imaging tasks like disease classification, important object localization, segmentation, etc. However, most of the task requires a large amount of annotated data for their successful implementation. To mitigate the shortage of data, different generative models are proposed for data augmentation purposes which can boost the classification performances. For this, different synthetic medical image data generation models are developed to increase the dataset. Unpaired image-to-image translation models here shift the source domain to the target domain. In the breast malignancy identification domain, FNAC is one of the low-cost low-invasive modalities normally used by medical practitioners. But availability of public datasets in this domain is very poor. Whereas, for automation of cytology images, we need a large amount of annotated data. Therefore synthetic cytology images are generated by translating breast histopathology samples which are publicly available. In this study, we have explored traditional image-to-image transfer models like CycleGAN, and Neural Style Transfer. Further, it is observed that the generated cytology images are quite similar to real breast cytology samples by measuring FID and KID scores.","sentences":["Automation in medical imaging is quite challenging due to the unavailability of annotated datasets and the scarcity of domain experts.","In recent years, deep learning techniques have solved some complex medical imaging tasks like disease classification, important object localization, segmentation, etc.","However, most of the task requires a large amount of annotated data for their successful implementation.","To mitigate the shortage of data, different generative models are proposed for data augmentation purposes which can boost the classification performances.","For this, different synthetic medical image data generation models are developed to increase the dataset.","Unpaired image-to-image translation models here shift the source domain to the target domain.","In the breast malignancy identification domain, FNAC is one of the low-cost low-invasive modalities normally used by medical practitioners.","But availability of public datasets in this domain is very poor.","Whereas, for automation of cytology images, we need a large amount of annotated data.","Therefore synthetic cytology images are generated by translating breast histopathology samples which are publicly available.","In this study, we have explored traditional image-to-image transfer models like CycleGAN, and Neural Style Transfer.","Further, it is observed that the generated cytology images are quite similar to real breast cytology samples by measuring FID and KID scores."],"url":"http://arxiv.org/abs/2403.10885v1","category":"eess.IV"}
{"created":"2024-03-16 10:05:15","title":"Kodaira vanishing theorems for Kahler Lie algebroids","abstract":"A Lie algebroid is a generalization of Lie algebra that provides a general framework to describe the symmetries of a manifold. In this paper, we generalize the Kodaira vanishing theorem, which is a basic result in complex geometry, to Kahler Lie algebroids. The generalization of the Kodaira vanishing theorem states that the kernel of the Lie algebroid Laplace operator on Lie algebroid positive line bundle-valued (p,q)-forms vanishes when p+q is sufficiently large. The most difficult part of the proof of the generalized Kodaira vanishing theorem is the generalization of the Kahler identities to Lie algebroids. In this paper, we provide an approach by using local coordinate calculation.","sentences":["A Lie algebroid is a generalization of Lie algebra that provides a general framework to describe the symmetries of a manifold.","In this paper, we generalize the Kodaira vanishing theorem, which is a basic result in complex geometry, to Kahler Lie algebroids.","The generalization of the Kodaira vanishing theorem states that the kernel of the Lie algebroid Laplace operator on Lie algebroid positive line bundle-valued (p,q)-forms vanishes when p+q is sufficiently large.","The most difficult part of the proof of the generalized Kodaira vanishing theorem is the generalization of the Kahler identities to Lie algebroids.","In this paper, we provide an approach by using local coordinate calculation."],"url":"http://arxiv.org/abs/2403.10876v1","category":"math.DG"}
{"created":"2024-03-16 09:52:21","title":"CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid Beamforming","abstract":"Hybrid beamforming is vital in modern wireless systems, especially for massive MIMO and millimeter-wave deployments, offering efficient directional transmission with reduced hardware complexity. However, effective beamforming in multi-user scenarios relies heavily on accurate channel state information, the acquisition of which often incurs excessive pilot overhead, degrading system performance. To address this and inspired by the spatial congruence between sub-6GHz (sub-6G) and mmWave channels, we propose a Sub-6G information Aided Multi-User Hybrid Beamforming (SA-MUHBF) framework, avoiding excessive use of pilots. SA-MUHBF employs a convolutional neural network to predict mmWave beamspace from sub-6G channel estimate, followed by a novel multi-layer graph neural network for analog beam selection and a linear minimum mean-square error algorithm for digital beamforming. Numerical results demonstrate that SA-MUHBF efficiently predicts the mmWave beamspace representation and achieves superior spectrum efficiency over state-of-the-art benchmarks. Moreover, SA-MUHBF demonstrates robust performance across varied sub-6G system configurations and exhibits strong generalization to unseen scenarios.","sentences":["Hybrid beamforming is vital in modern wireless systems, especially for massive MIMO and millimeter-wave deployments, offering efficient directional transmission with reduced hardware complexity.","However, effective beamforming in multi-user scenarios relies heavily on accurate channel state information, the acquisition of which often incurs excessive pilot overhead, degrading system performance.","To address this and inspired by the spatial congruence between sub-6GHz (sub-6G) and mmWave channels, we propose a Sub-6G information Aided Multi-User Hybrid Beamforming (SA-MUHBF) framework, avoiding excessive use of pilots.","SA-MUHBF employs a convolutional neural network to predict mmWave beamspace from sub-6G channel estimate, followed by a novel multi-layer graph neural network for analog beam selection and a linear minimum mean-square error algorithm for digital beamforming.","Numerical results demonstrate that SA-MUHBF efficiently predicts the mmWave beamspace representation and achieves superior spectrum efficiency over state-of-the-art benchmarks.","Moreover, SA-MUHBF demonstrates robust performance across varied sub-6G system configurations and exhibits strong generalization to unseen scenarios."],"url":"http://arxiv.org/abs/2403.10873v1","category":"cs.IT"}
{"created":"2024-03-16 09:43:29","title":"Spectrum conversion and pattern preservation of Airy beams in fractional systems with a dynamical harmonic-oscillator potential","abstract":"We investigate the dynamics of optical Airy beams in the one-dimensional fractional Schr\\\"odinger equation with a harmonic-oscillator (HO) potential subjected to modulation along the propagation distance. Deriving general solutions for propagating beams and particular solutions for Airy waves with/without chirp, we study analytically the spectrum conversion and pattern preservation for the chirp-free and chirped Airy beams in the fractional system including the HO potential with moir\\'e-lattice, hyperbolic-secant, and delta-functional modulation formats. For the HO-moir\\'e-lattice potential, it is found that the chirp-free Airy beam experiences multiple spectrum conversions between the Airy and Gaussian patterns in the momentum space, preserving the Airy pattern in the coordinate space. The chirp magnitude of the chirped Airy beam determines whether the spectrum conversion occurs in the momentum space, and the splitting and evolution direction of the beam in the coordinate space. For the HO-hyperbolic-secant potential, the chirp-free Airy beam undergoes spectrum conversion and tunneling, with the positions of the spectrum conversion and tunneling significantly depending on parameters of the hyperbolic-secant potential; however, the spectrum conversion and pattern preservation of the chirped Airy beam occurs only under a certain relation of the chirp and parameters of the potential. In the case of the HO-delta-functional potential, the chirp-free Airy beam experiences abrupt spectrum conversion and a two-step spectrum shift; however, for the chirped Airy beam, the spectrum conversion is affected by the relation between the chirp and height of the potential. Effects of the fractional L\\'evy index on the spectrum conversion and pattern preservation of the Airy beams under the action of the three modulation patterns considered here are also explored in detail.","sentences":["We investigate the dynamics of optical Airy beams in the one-dimensional fractional Schr\\\"odinger equation with a harmonic-oscillator (HO) potential subjected to modulation along the propagation distance.","Deriving general solutions for propagating beams and particular solutions for Airy waves with/without chirp, we study analytically the spectrum conversion and pattern preservation for the chirp-free and chirped Airy beams in the fractional system including the HO potential with moir\\'e-lattice, hyperbolic-secant, and delta-functional modulation formats.","For the HO-moir\\'e-lattice potential, it is found that the chirp-free Airy beam experiences multiple spectrum conversions between the Airy and Gaussian patterns in the momentum space, preserving the Airy pattern in the coordinate space.","The chirp magnitude of the chirped Airy beam determines whether the spectrum conversion occurs in the momentum space, and the splitting and evolution direction of the beam in the coordinate space.","For the HO-hyperbolic-secant potential, the chirp-free Airy beam undergoes spectrum conversion and tunneling, with the positions of the spectrum conversion and tunneling significantly depending on parameters of the hyperbolic-secant potential; however, the spectrum conversion and pattern preservation of the chirped Airy beam occurs only under a certain relation of the chirp and parameters of the potential.","In the case of the HO-delta-functional potential, the chirp-free Airy beam experiences abrupt spectrum conversion and a two-step spectrum shift; however, for the chirped Airy beam, the spectrum conversion is affected by the relation between the chirp and height of the potential.","Effects of the fractional L\\'evy index on the spectrum conversion and pattern preservation of the Airy beams under the action of the three modulation patterns considered here are also explored in detail."],"url":"http://arxiv.org/abs/2403.10871v1","category":"physics.optics"}
{"created":"2024-03-16 09:42:28","title":"Nonadiabatic quantum Vlasov equation in spinor QED","abstract":"The nonadiabatic quantum Vlasov equation in spinor QED is derived, and its relation to the well-known adiabatic one is established by three methods. One is by an explicitly analytical expression, the second is by the Dirac equation in the V gauge, and the last is by introducing a turn-off electric field. Wherein what the first two of them are given is an instantaneous relation. Moreover, the time evolution of the distribution function for a specific momentum and the momentum distribution of created particle pairs after turning off the electric field are calculated and compared with those in scalar QED. It is found that both the oscillation periods of the distribution functions in spinor and scalar QED equal pi divided by the total energy of a particle after the electric field is turned off. The momentum distributions in spinor and scalar QED show a novel oscillation and out-of-phase behavior that cannot be explained by the Stokes phenomenon. These findings will further deepen our understanding of the quantum Vlasov equation and its application in vacuum pair production.","sentences":["The nonadiabatic quantum Vlasov equation in spinor QED is derived, and its relation to the well-known adiabatic one is established by three methods.","One is by an explicitly analytical expression, the second is by the Dirac equation in the V gauge, and the last is by introducing a turn-off electric field.","Wherein what the first two of them are given is an instantaneous relation.","Moreover, the time evolution of the distribution function for a specific momentum and the momentum distribution of created particle pairs after turning off the electric field are calculated and compared with those in scalar QED.","It is found that both the oscillation periods of the distribution functions in spinor and scalar QED equal pi divided by the total energy of a particle after the electric field is turned off.","The momentum distributions in spinor and scalar QED show a novel oscillation and out-of-phase behavior that cannot be explained by the Stokes phenomenon.","These findings will further deepen our understanding of the quantum Vlasov equation and its application in vacuum pair production."],"url":"http://arxiv.org/abs/2403.10870v1","category":"hep-th"}
{"created":"2024-03-16 08:58:03","title":"FedQNN: Federated Learning using Quantum Neural Networks","abstract":"In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning. This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct datasets, proving its suitability for conducting various QML tasks. Our research not only identifies the limitations of classical paradigms but also presents a novel framework to propel the field of QML into a new era of secure and collaborative innovation.","sentences":["In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks.","Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information.","Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning.","This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing.","Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework.","The results consistently exceed 86% accuracy across three distinct datasets, proving its suitability for conducting various QML tasks.","Our research not only identifies the limitations of classical paradigms but also presents a novel framework to propel the field of QML into a new era of secure and collaborative innovation."],"url":"http://arxiv.org/abs/2403.10861v1","category":"quant-ph"}
{"created":"2024-03-16 08:51:02","title":"Neural-Kernel Conditional Mean Embeddings","abstract":"Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on Q-learning, our approach naturally leads to a new variant of distributional RL methods, which demonstrates consistent effectiveness across different environments.","sentences":["Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges.","In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges.","Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective.","This design circumvents the computationally expensive Gram matrix inversion required by current CME methods.","To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters.","In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods.","Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts.","Building on Q-learning, our approach naturally leads to a new variant of distributional RL methods, which demonstrates consistent effectiveness across different environments."],"url":"http://arxiv.org/abs/2403.10859v1","category":"stat.ML"}
{"created":"2024-03-18 12:12:45","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification","abstract":"Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal pedestrian retrieval task, due to significant intra-class variations and cross-modal discrepancies among different cameras. Existing works mainly focus on embedding images of different modalities into a unified space to mine modality-shared features. They only seek distinctive information within these shared features, while ignoring the identity-aware useful information that is implicit in the modality-specific features. To address this issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL) network to uncover and leverage the implicit discriminative information contained within the modality-specific. First, we extract modality-specific and modality-shared features using a novel dual-stream network. Then, the modality-specific features undergo purification to reduce their modality style discrepancies while preserving identity-aware discriminative knowledge. Subsequently, this kind of implicit knowledge is distilled into the modality-shared feature to enhance its distinctiveness. Finally, an alignment loss is proposed to minimize modality discrepancy on enhanced modality-shared features. Extensive experiments on multiple public datasets demonstrate the superiority of IDKL network over the state-of-the-art methods. Code is available at https://github.com/1KK077/IDKL.","sentences":["Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal pedestrian retrieval task, due to significant intra-class variations and cross-modal discrepancies among different cameras.","Existing works mainly focus on embedding images of different modalities into a unified space to mine modality-shared features.","They only seek distinctive information within these shared features, while ignoring the identity-aware useful information that is implicit in the modality-specific features.","To address this issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL) network to uncover and leverage the implicit discriminative information contained within the modality-specific.","First, we extract modality-specific and modality-shared features using a novel dual-stream network.","Then, the modality-specific features undergo purification to reduce their modality style discrepancies while preserving identity-aware discriminative knowledge.","Subsequently, this kind of implicit knowledge is distilled into the modality-shared feature to enhance its distinctiveness.","Finally, an alignment loss is proposed to minimize modality discrepancy on enhanced modality-shared features.","Extensive experiments on multiple public datasets demonstrate the superiority of IDKL network over the state-of-the-art methods.","Code is available at https://github.com/1KK077/IDKL."],"url":"http://arxiv.org/abs/2403.11708v1","category":"cs.CV"}
{"created":"2024-03-18 10:45:50","title":"Prioritized Semantic Learning for Zero-shot Instance Navigation","abstract":"We study zero-shot instance navigation, in which the agent navigates to a specific object without using object annotations for training. Previous object navigation approaches apply the image-goal navigation (ImageNav) task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a vision-language model. However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments. In this paper, we propose a Prioritized Semantic Learning (PSL) method to improve the semantic understanding ability of navigation agents. Specifically, a semantic-enhanced PSL agent is proposed and a prioritized semantic training strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching. At inference time, a semantic expansion inference scheme is designed to preserve the same granularity level of the goal-semantic as training. Furthermore, for the popular HM3D environment, we present an Instance Navigation (InstanceNav) task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation (ObjectNav) task where the goal is defined merely by the object category. Our PSL agent outperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in terms of success rate and is also superior on the new InstanceNav task. Code will be released at https://anonymous.4open. science/r/PSL/.","sentences":["We study zero-shot instance navigation, in which the agent navigates to a specific object without using object annotations for training.","Previous object navigation approaches apply the image-goal navigation (ImageNav) task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a vision-language model.","However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments.","In this paper, we propose a Prioritized Semantic Learning (PSL) method to improve the semantic understanding ability of navigation agents.","Specifically, a semantic-enhanced PSL agent is proposed and a prioritized semantic training strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching.","At inference time, a semantic expansion inference scheme is designed to preserve the same granularity level of the goal-semantic as training.","Furthermore, for the popular HM3D environment, we present an Instance Navigation (InstanceNav) task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation (ObjectNav) task where the goal is defined merely by the object category.","Our PSL agent outperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in terms of success rate and is also superior on the new InstanceNav task.","Code will be released at https://anonymous.4open.","science/r/PSL/."],"url":"http://arxiv.org/abs/2403.11650v1","category":"cs.CV"}
{"created":"2024-03-18 10:42:24","title":"MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks","abstract":"Transfer learning has become a powerful tool to initialize deep learning models to achieve faster convergence and higher performance. This is especially useful in the medical imaging analysis domain, where data scarcity limits possible performance gains for deep learning models. Some advancements have been made in boosting the transfer learning performance gain by merging models starting from the same initialization. However, in the medical imaging analysis domain, there is an opportunity in merging models starting from different initialisations, thus combining the features learnt from different tasks. In this work, we propose MedMerge, a method whereby the weights of different models can be merged, and their features can be effectively utilized to boost performance on a new task. With MedMerge, we learn kernel-level weights that can later be used to merge the models into a single model, even when starting from different initializations. Testing on various medical imaging analysis tasks, we show that our merged model can achieve significant performance gains, with up to 3% improvement on the F1 score. The code implementation of this work will be available at www.github.com/BioMedIA-MBZUAI/MedMerge.","sentences":["Transfer learning has become a powerful tool to initialize deep learning models to achieve faster convergence and higher performance.","This is especially useful in the medical imaging analysis domain, where data scarcity limits possible performance gains for deep learning models.","Some advancements have been made in boosting the transfer learning performance gain by merging models starting from the same initialization.","However, in the medical imaging analysis domain, there is an opportunity in merging models starting from different initialisations, thus combining the features learnt from different tasks.","In this work, we propose MedMerge, a method whereby the weights of different models can be merged, and their features can be effectively utilized to boost performance on a new task.","With MedMerge, we learn kernel-level weights that can later be used to merge the models into a single model, even when starting from different initializations.","Testing on various medical imaging analysis tasks, we show that our merged model can achieve significant performance gains, with up to 3% improvement on the F1 score.","The code implementation of this work will be available at www.github.com/BioMedIA-MBZUAI/MedMerge."],"url":"http://arxiv.org/abs/2403.11646v1","category":"cs.CV"}
{"created":"2024-03-18 10:19:52","title":"The Value of Reward Lookahead in Reinforcement Learning","abstract":"In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and reward-free exploration. We further provide tight bounds for the ratio given the worst-case dynamics. Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts.","sentences":["In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards.","Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward.","Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction.","In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis.","In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead.","We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations.","Surprisingly, the resulting ratios relate to known quantities in offline RL and reward-free exploration.","We further provide tight bounds for the ratio given the worst-case dynamics.","Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts."],"url":"http://arxiv.org/abs/2403.11637v1","category":"cs.LG"}
{"created":"2024-03-18 09:47:41","title":"Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception","abstract":"For training a video-based action recognition model that accepts multi-view video, annotating frame-level labels is tedious and difficult. However, it is relatively easy to annotate sequence-level labels. This kind of coarse annotations are called as weak labels. However, training a multi-view video-based action recognition model with weak labels for frame-level perception is challenging. In this paper, we propose a novel learning framework, where the weak labels are first used to train a multi-view video-based base model, which is subsequently used for downstream frame-level perception tasks. The base model is trained to obtain individual latent embeddings for each view in the multi-view input. For training the model using the weak labels, we propose a novel latent loss function. We also propose a model that uses the view-specific latent embeddings for downstream frame-level action recognition and detection tasks. The proposed framework is evaluated using the MM Office dataset by comparing several baseline algorithms. The results show that the proposed base model is effectively trained using weak labels and the latent embeddings help the downstream models improve accuracy.","sentences":["For training a video-based action recognition model that accepts multi-view video, annotating frame-level labels is tedious and difficult.","However, it is relatively easy to annotate sequence-level labels.","This kind of coarse annotations are called as weak labels.","However, training a multi-view video-based action recognition model with weak labels for frame-level perception is challenging.","In this paper, we propose a novel learning framework, where the weak labels are first used to train a multi-view video-based base model, which is subsequently used for downstream frame-level perception tasks.","The base model is trained to obtain individual latent embeddings for each view in the multi-view input.","For training the model using the weak labels, we propose a novel latent loss function.","We also propose a model that uses the view-specific latent embeddings for downstream frame-level action recognition and detection tasks.","The proposed framework is evaluated using the MM Office dataset by comparing several baseline algorithms.","The results show that the proposed base model is effectively trained using weak labels and the latent embeddings help the downstream models improve accuracy."],"url":"http://arxiv.org/abs/2403.11616v1","category":"cs.CV"}
{"created":"2024-03-18 08:50:30","title":"Offline Multitask Representation Learning for Reinforcement Learning","abstract":"We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.","sentences":["We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation.","We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning.","Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks.","Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model."],"url":"http://arxiv.org/abs/2403.11574v1","category":"cs.LG"}
{"created":"2024-03-18 07:43:14","title":"Semantic Prompting with Image-Token for Continual Learning","abstract":"Continual learning aims to refine model parameters for new tasks while retaining knowledge from previous tasks. Recently, prompt-based learning has emerged to leverage pre-trained models to be prompted to learn subsequent tasks without the reliance on the rehearsal buffer. Although this approach has demonstrated outstanding results, existing methods depend on preceding task-selection process to choose appropriate prompts. However, imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic approach focuses on the visual semantic information of image tokens to eliminate task prediction. Our method consists of semantic prompt matching, which determines prompts based on similarities between tokens, and image token-level prompting, which applies prompts directly to image tokens in the intermediate layers. Consequently, our method achieves competitive performance on four benchmarks while significantly reducing training time compared to state-of-the-art methods. Moreover, we demonstrate the superiority of our method across various scenarios through extensive experiments.","sentences":["Continual learning aims to refine model parameters for new tasks while retaining knowledge from previous tasks.","Recently, prompt-based learning has emerged to leverage pre-trained models to be prompted to learn subsequent tasks without the reliance on the rehearsal buffer.","Although this approach has demonstrated outstanding results, existing methods depend on preceding task-selection process to choose appropriate prompts.","However, imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced.","To address this issue, we introduce I-Prompt, a task-agnostic approach focuses on the visual semantic information of image tokens to eliminate task prediction.","Our method consists of semantic prompt matching, which determines prompts based on similarities between tokens, and image token-level prompting, which applies prompts directly to image tokens in the intermediate layers.","Consequently, our method achieves competitive performance on four benchmarks while significantly reducing training time compared to state-of-the-art methods.","Moreover, we demonstrate the superiority of our method across various scenarios through extensive experiments."],"url":"http://arxiv.org/abs/2403.11537v1","category":"cs.CV"}
{"created":"2024-03-18 07:35:25","title":"Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)","abstract":"Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of using OOD together with CP apply the other way around by using OOD scores as non-conformity scores, which results in improving upon current CP methods. One of the key messages of these contributions is that since OOD is concerned with designing scores and CP with interpreting these scores, the two fields may be inherently intertwined.","sentences":["Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data.","On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees.","In this work, we propose to use CP to better assess the efficiency of OOD scores.","Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset.","Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics.","We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022).","We also show that the benefits of using OOD together with CP apply the other way around by using OOD scores as non-conformity scores, which results in improving upon current CP methods.","One of the key messages of these contributions is that since OOD is concerned with designing scores and CP with interpreting these scores, the two fields may be inherently intertwined."],"url":"http://arxiv.org/abs/2403.11532v1","category":"stat.ML"}
{"created":"2024-03-18 07:13:09","title":"Efficient and Privacy-Preserving Federated Learning based on Full Homomorphic Encryption","abstract":"Since the first theoretically feasible full homomorphic encryption (FHE) scheme was proposed in 2009, great progress has been achieved. These improvements have made FHE schemes come off the paper and become quite useful in solving some practical problems. In this paper, we propose a set of novel Federated Learning Schemes by utilizing the latest homomorphic encryption technologies, so as to improve the security, functionality and practicality at the same time. Comparisons have been given in four practical data sets separately from medical, business, biometric and financial fields, covering both horizontal and vertical federated learning scenarios. The experiment results show that our scheme achieves significant improvements in security, efficiency and practicality, compared with classical horizontal and vertical federated learning schemes.","sentences":["Since the first theoretically feasible full homomorphic encryption (FHE) scheme was proposed in 2009, great progress has been achieved.","These improvements have made FHE schemes come off the paper and become quite useful in solving some practical problems.","In this paper, we propose a set of novel Federated Learning Schemes by utilizing the latest homomorphic encryption technologies, so as to improve the security, functionality and practicality at the same time.","Comparisons have been given in four practical data sets separately from medical, business, biometric and financial fields, covering both horizontal and vertical federated learning scenarios.","The experiment results show that our scheme achieves significant improvements in security, efficiency and practicality, compared with classical horizontal and vertical federated learning schemes."],"url":"http://arxiv.org/abs/2403.11519v1","category":"cs.CR"}
{"created":"2024-03-18 04:31:38","title":"FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update","abstract":"Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \\textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-models, FedSPU maintains the full model architecture on each device but randomly freezes a certain percentage of neurons in the local model during training while updating the remaining neurons. This approach ensures that a portion of the local model remains personalized, thereby enhancing the model's robustness against biased parameters from other clients. Experimental results demonstrate that FedSPU outperforms federated dropout by 7.57\\% on average in terms of accuracy. Furthermore, an introduced early stopping scheme leads to a significant reduction of the training time by \\(24.8\\%\\sim70.4\\%\\) while maintaining high accuracy.","sentences":["Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy.","However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL.","Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \\textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads.","Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data.","When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable.","In response, we propose federated learning with stochastic parameter update (FedSPU).","Unlike dropout that tailors the global model to small-size local sub-models, FedSPU maintains the full model architecture on each device but randomly freezes a certain percentage of neurons in the local model during training while updating the remaining neurons.","This approach ensures that a portion of the local model remains personalized, thereby enhancing the model's robustness against biased parameters from other clients.","Experimental results demonstrate that FedSPU outperforms federated dropout by 7.57\\% on average in terms of accuracy.","Furthermore, an introduced early stopping scheme leads to a significant reduction of the training time by \\(24.8\\%\\sim70.4\\%\\) while maintaining high accuracy."],"url":"http://arxiv.org/abs/2403.11464v1","category":"cs.LG"}
{"created":"2024-03-18 04:30:31","title":"Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding","abstract":"Video Paragraph Grounding (VPG) is an emerging task in video-language understanding, which aims at localizing multiple sentences with semantic relations and temporal order from an untrimmed video. However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire. In this work, we introduce and explore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the need of temporal annotations. Different from previous weakly-supervised grounding frameworks based on multiple instance learning or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG. Specifically, we devise a Siamese Grounding TRansformer (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision. An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing multiple sentences in a normal video. We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient weakly-supervised or semi-supervised learning, outperforming state-of-the-art methods trained with the same or stronger supervision.","sentences":["Video Paragraph Grounding (VPG) is an emerging task in video-language understanding, which aims at localizing multiple sentences with semantic relations and temporal order from an untrimmed video.","However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire.","In this work, we introduce and explore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the need of temporal annotations.","Different from previous weakly-supervised grounding frameworks based on multiple instance learning or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG.","Specifically, we devise a Siamese Grounding TRansformer (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision.","An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing multiple sentences in a normal video.","We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient weakly-supervised or semi-supervised learning, outperforming state-of-the-art methods trained with the same or stronger supervision."],"url":"http://arxiv.org/abs/2403.11463v1","category":"cs.CV"}
{"created":"2024-03-18 04:26:52","title":"VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation","abstract":"In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering. VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages. These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion. On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task. In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility. Videos and code implementation can be found at our project site: https://vihe-3d.github.io.","sentences":["In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering.","VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages.","These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion.","On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task.","In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility.","Videos and code implementation can be found at our project site: https://vihe-3d.github.io."],"url":"http://arxiv.org/abs/2403.11461v1","category":"cs.RO"}
{"created":"2024-03-18 02:44:46","title":"BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors","abstract":"Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation. Recently, some studies have successfully constructed animatable 3D models from monocular videos. However, these approaches require sufficient view coverage of the object within the input video and typically necessitate significant time and computational costs for training and rendering. This limitation restricts the practical applications. In this work, we propose a method to build animatable 3D Gaussian Splatting from monocular video with diffusion priors. The 3D Gaussian representations significantly accelerate the training and rendering process, and the diffusion priors allow the method to learn 3D models with limited viewpoints. We also present the rigid regularization to enhance the utilization of the priors. We perform an extensive evaluation across various real-world videos, demonstrating its superior performance compared to the current state-of-the-art methods.","sentences":["Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation.","Recently, some studies have successfully constructed animatable 3D models from monocular videos.","However, these approaches require sufficient view coverage of the object within the input video and typically necessitate significant time and computational costs for training and rendering.","This limitation restricts the practical applications.","In this work, we propose a method to build animatable 3D Gaussian Splatting from monocular video with diffusion priors.","The 3D Gaussian representations significantly accelerate the training and rendering process, and the diffusion priors allow the method to learn 3D models with limited viewpoints.","We also present the rigid regularization to enhance the utilization of the priors.","We perform an extensive evaluation across various real-world videos, demonstrating its superior performance compared to the current state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.11427v1","category":"cs.CV"}
{"created":"2024-03-18 02:42:01","title":"Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure","abstract":"Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysis shows that the proposed narrative features remarkably increased feature density and improved performance.","sentences":["Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship.","Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety.","This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes.","We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer.","The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%.","The analysis shows that the proposed narrative features remarkably increased feature density and improved performance."],"url":"http://arxiv.org/abs/2403.11425v1","category":"cs.LG"}
{"created":"2024-03-17 14:52:05","title":"FORCE: Dataset and Method for Intuitive Physics Guided Human-object Interaction","abstract":"Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction. They introduce important motion nuances that are essential for diversity and realism. Despite advancements in recent kinematics-based methods, this aspect has been overlooked. Generating nuanced human motion presents two challenges. First, it is non-trivial to learn from multi-modal human and object information derived from both the physical and non-physical attributes. Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development. This work addresses the gap by introducing the FORCE model, a kinematic approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes. Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance. Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance. Experiments also demonstrate incorporating human force facilitates learning multi-class motion. Accompanying our model, we contribute the FORCE dataset. It features diverse, different-styled motion through interactions with varying resistances.","sentences":["Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction.","They introduce important motion nuances that are essential for diversity and realism.","Despite advancements in recent kinematics-based methods, this aspect has been overlooked.","Generating nuanced human motion presents two challenges.","First, it is non-trivial to learn from multi-modal human and object information derived from both the physical and non-physical attributes.","Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development.","This work addresses the gap by introducing the FORCE model, a kinematic approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes.","Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance.","Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance.","Experiments also demonstrate incorporating human force facilitates learning multi-class motion.","Accompanying our model, we contribute the FORCE dataset.","It features diverse, different-styled motion through interactions with varying resistances."],"url":"http://arxiv.org/abs/2403.11237v1","category":"cs.CV"}
{"created":"2024-03-17 14:21:42","title":"Cheap Ways of Extracting Clinical Markers from Texts","abstract":"This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.","sentences":["This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level.","Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis).","Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient.","The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights.","The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers."],"url":"http://arxiv.org/abs/2403.11227v1","category":"cs.CL"}
{"created":"2024-03-17 13:34:31","title":"Tell machine learning potentials what they are needed for: Simulation-oriented training exemplified for glycine","abstract":"Machine learning potentials (MLPs) are widely applied as an efficient alternative way to represent potential energy surface (PES) in many chemical simulations, e.g., geometry optimizations, frequency calculations, molecular dynamics, and Monte Carlo computations. However, there is a growing realization that the evaluation of the reliability of MLPs cannot be reduced to the common metrics such as the mean absolute (MAE) or root-mean-square (RMSE) errors on the test set drawn from the same distribution as the training data. Here, we systematically investigate the relationship between such test errors and the accuracy of chemical simulations with MLPs on an example of a full-dimensional, global PES for the glycine amino acid, which is of great significance in both biology and astronomy. Our results show that, indeed, the errors in the test set do not unambiguously reflect the MLP performance in different simulation tasks such as relative conformer energies, barriers, vibrational levels, and zero-point vibrational energies. Importantly, we also offer a solution and an easily accessible tool to improve the fidelity of MLPs in a simulation-oriented manner. Guided by the loss function based on real simulation-oriented metrics, our solution ensures the quality of MLPs, ultimately yielding the most accurate one that can even obtain near-zero simulation errors in some tasks such as predicting relative conformer energies and barriers. Furthermore, this solution also passed the stringent test in the diffusion Monte Carlo simulations that confirmed the comprehensive description of the global PES by the resulting MLP.","sentences":["Machine learning potentials (MLPs) are widely applied as an efficient alternative way to represent potential energy surface (PES) in many chemical simulations, e.g., geometry optimizations, frequency calculations, molecular dynamics, and Monte Carlo computations.","However, there is a growing realization that the evaluation of the reliability of MLPs cannot be reduced to the common metrics such as the mean absolute (MAE) or root-mean-square (RMSE) errors on the test set drawn from the same distribution as the training data.","Here, we systematically investigate the relationship between such test errors and the accuracy of chemical simulations with MLPs on an example of a full-dimensional, global PES for the glycine amino acid, which is of great significance in both biology and astronomy.","Our results show that, indeed, the errors in the test set do not unambiguously reflect the MLP performance in different simulation tasks such as relative conformer energies, barriers, vibrational levels, and zero-point vibrational energies.","Importantly, we also offer a solution and an easily accessible tool to improve the fidelity of MLPs in a simulation-oriented manner.","Guided by the loss function based on real simulation-oriented metrics, our solution ensures the quality of MLPs, ultimately yielding the most accurate one that can even obtain near-zero simulation errors in some tasks such as predicting relative conformer energies and barriers.","Furthermore, this solution also passed the stringent test in the diffusion Monte Carlo simulations that confirmed the comprehensive description of the global PES by the resulting MLP."],"url":"http://arxiv.org/abs/2403.11216v1","category":"physics.chem-ph"}
{"created":"2024-03-17 13:04:35","title":"TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models","abstract":"KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Moreover, updating the entire set of parameters in KEPLMs is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that entities in text corpora usually follow the long-tail distribution, where the representations of some entities are suboptimally optimized and hinder the pre-training process for KEPLMs. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Furthermore, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual knowledge is both sufficient and efficient. Specifically, we utilize dynamic knowledge routing to identify knowledge paths in FFNs and selectively update parameters during pre-training. Experimental results show that TRELM reduces pre-training time by at least 50% and outperforms other KEPLMs in knowledge probing tasks and multiple knowledge-aware language understanding tasks.","sentences":["KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding.","Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs.","However, these models do not prioritize learning embeddings for entity-related tokens.","Moreover, updating the entire set of parameters in KEPLMs is computationally demanding.","This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models.","We observe that entities in text corpora usually follow the long-tail distribution, where the representations of some entities are suboptimally optimized and hinder the pre-training process for KEPLMs.","To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information.","Furthermore, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual knowledge is both sufficient and efficient.","Specifically, we utilize dynamic knowledge routing to identify knowledge paths in FFNs and selectively update parameters during pre-training.","Experimental results show that TRELM reduces pre-training time by at least 50% and outperforms other KEPLMs in knowledge probing tasks and multiple knowledge-aware language understanding tasks."],"url":"http://arxiv.org/abs/2403.11203v1","category":"cs.CL"}
{"created":"2024-03-17 12:40:49","title":"MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic Segmentation","abstract":"Semantic segmentation is essential in computer vision for various applications, yet traditional approaches face significant challenges, including the high cost of annotation and extensive training for supervised learning. Additionally, due to the limited predefined categories in supervised learning, models typically struggle with infrequent classes and are unable to predict novel classes. To address these limitations, we propose MaskDiffusion, an innovative approach that leverages pretrained frozen Stable Diffusion to achieve open-vocabulary semantic segmentation without the need for additional training or annotation, leading to improved performance compared to similar methods. We also demonstrate the superior performance of MaskDiffusion in handling open vocabularies, including fine-grained and proper noun-based categories, thus expanding the scope of segmentation applications. Overall, our MaskDiffusion shows significant qualitative and quantitative improvements in contrast to other comparable unsupervised segmentation methods, i.e. on the Potsdam dataset (+10.5 mIoU compared to GEM) and COCO-Stuff (+14.8 mIoU compared to DiffSeg). All code and data will be released at https://github.com/Valkyrja3607/MaskDiffusion.","sentences":["Semantic segmentation is essential in computer vision for various applications, yet traditional approaches face significant challenges, including the high cost of annotation and extensive training for supervised learning.","Additionally, due to the limited predefined categories in supervised learning, models typically struggle with infrequent classes and are unable to predict novel classes.","To address these limitations, we propose MaskDiffusion, an innovative approach that leverages pretrained frozen Stable Diffusion to achieve open-vocabulary semantic segmentation without the need for additional training or annotation, leading to improved performance compared to similar methods.","We also demonstrate the superior performance of MaskDiffusion in handling open vocabularies, including fine-grained and proper noun-based categories, thus expanding the scope of segmentation applications.","Overall, our MaskDiffusion shows significant qualitative and quantitative improvements in contrast to other comparable unsupervised segmentation methods, i.e. on the Potsdam dataset (+10.5 mIoU compared to GEM) and COCO-Stuff (+14.8 mIoU compared to DiffSeg).","All code and data will be released at https://github.com/Valkyrja3607/MaskDiffusion."],"url":"http://arxiv.org/abs/2403.11194v1","category":"cs.CV"}
{"created":"2024-03-17 10:09:54","title":"A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques","abstract":"This paper presents a selective review of statistical computation methods for massive data analysis. A huge amount of statistical methods for massive data computation have been rapidly developed in the past decades. In this work, we focus on three categories of statistical computation methods: (1) distributed computing, (2) subsampling methods, and (3) minibatch gradient techniques. The first class of literature is about distributed computing and focuses on the situation, where the dataset size is too huge to be comfortably handled by one single computer. In this case, a distributed computation system with multiple computers has to be utilized. The second class of literature is about subsampling methods and concerns about the situation, where the sample size of dataset is small enough to be placed on one single computer but too large to be easily processed by its memory as a whole. The last class of literature studies those minibatch gradient related optimization techniques, which have been extensively used for optimizing various deep learning models.","sentences":["This paper presents a selective review of statistical computation methods for massive data analysis.","A huge amount of statistical methods for massive data computation have been rapidly developed in the past decades.","In this work, we focus on three categories of statistical computation methods: (1) distributed computing, (2) subsampling methods, and (3) minibatch gradient techniques.","The first class of literature is about distributed computing and focuses on the situation, where the dataset size is too huge to be comfortably handled by one single computer.","In this case, a distributed computation system with multiple computers has to be utilized.","The second class of literature is about subsampling methods and concerns about the situation, where the sample size of dataset is small enough to be placed on one single computer but too large to be easily processed by its memory as a whole.","The last class of literature studies those minibatch gradient related optimization techniques, which have been extensively used for optimizing various deep learning models."],"url":"http://arxiv.org/abs/2403.11163v1","category":"stat.ME"}
{"created":"2024-03-17 08:50:44","title":"Is Mamba Effective for Time Series Forecasting?","abstract":"In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and D-Mamba, both employing the Mamba Block to extract variate correlations. Remarkably, S-Mamba and D-Mamba achieve superior performance while saving GPU memory and training time. Furthermore, we conduct extensive experiments to delve deeper into the potential of Mamba compared to the Transformer in the TSF, aiming to explore a new research direction for this field. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.","sentences":["In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables.","However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist.","Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity.","In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation.","This has piqued our interest in exploring SSM's potential in TSF tasks.","In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and D-Mamba, both employing the Mamba Block to extract variate correlations.","Remarkably, S-Mamba and D-Mamba achieve superior performance while saving GPU memory and training time.","Furthermore, we conduct extensive experiments to delve deeper into the potential of Mamba compared to the Transformer in the TSF, aiming to explore a new research direction for this field.","Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba."],"url":"http://arxiv.org/abs/2403.11144v1","category":"cs.LG"}
{"created":"2024-03-17 08:09:48","title":"A lightweight deep learning pipeline with DRDA-Net and MobileNet for breast cancer classification","abstract":"Accurate and early detection of breast cancer is essential for successful treatment. This paper introduces a novel deep-learning approach for improved breast cancer classification in histopathological images, a crucial step in diagnosis. Our method hinges on the Dense Residual Dual-Shuffle Attention Network (DRDA-Net), inspired by ShuffleNet's efficient architecture. DRDA-Net achieves exceptional accuracy across various magnification levels on the BreaKHis dataset, a breast cancer histopathology analysis benchmark. However, for real-world deployment, computational efficiency is paramount. We integrate a pre-trained MobileNet model renowned for its lightweight design to address computational. MobileNet ensures fast execution even on devices with limited resources without sacrificing performance. This combined approach offers a promising solution for accurate breast cancer diagnosis, paving the way for faster and more accessible screening procedures.","sentences":["Accurate and early detection of breast cancer is essential for successful treatment.","This paper introduces a novel deep-learning approach for improved breast cancer classification in histopathological images, a crucial step in diagnosis.","Our method hinges on the Dense Residual Dual-Shuffle Attention Network (DRDA-Net), inspired by ShuffleNet's efficient architecture.","DRDA-Net achieves exceptional accuracy across various magnification levels on the BreaKHis dataset, a breast cancer histopathology analysis benchmark.","However, for real-world deployment, computational efficiency is paramount.","We integrate a pre-trained MobileNet model renowned for its lightweight design to address computational.","MobileNet ensures fast execution even on devices with limited resources without sacrificing performance.","This combined approach offers a promising solution for accurate breast cancer diagnosis, paving the way for faster and more accessible screening procedures."],"url":"http://arxiv.org/abs/2403.11135v1","category":"eess.IV"}
{"created":"2024-03-17 07:41:58","title":"Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering","abstract":"Document-level Event Causality Identification (DECI) aims to identify causal relations between two events in documents. Recent research tends to use pre-trained language models to generate the event causal relations. Whereas, these methods are prone to the errors of sequential generation due to multiple events in a document. Moreover, the potential structures such as event coreference and related causal chain are neglected. In this paper, we propose a multi-task learning framework to enhance event causality identification with rationale and structure-aware causal question answering. Specifically, the DECI task is transformed into multiple-choice question answering, and the causes and effects of the questioned event are generated with large language models. In addition, we generate the rationales to explain why these events have causal relations. Moreover, we construct an event structure graph, which models the multi-hop potential relations for causal reasoning of the current event. Experiments on two benchmark datasets show the great advantages of our proposed approach compared to the state-of-the-art methods. Moreover, we conduct both quantitative and qualitative analyses, which shed light on why each component of our approach can lead to great improvements.","sentences":["Document-level Event Causality Identification (DECI) aims to identify causal relations between two events in documents.","Recent research tends to use pre-trained language models to generate the event causal relations.","Whereas, these methods are prone to the errors of sequential generation due to multiple events in a document.","Moreover, the potential structures such as event coreference and related causal chain are neglected.","In this paper, we propose a multi-task learning framework to enhance event causality identification with rationale and structure-aware causal question answering.","Specifically, the DECI task is transformed into multiple-choice question answering, and the causes and effects of the questioned event are generated with large language models.","In addition, we generate the rationales to explain why these events have causal relations.","Moreover, we construct an event structure graph, which models the multi-hop potential relations for causal reasoning of the current event.","Experiments on two benchmark datasets show the great advantages of our proposed approach compared to the state-of-the-art methods.","Moreover, we conduct both quantitative and qualitative analyses, which shed light on why each component of our approach can lead to great improvements."],"url":"http://arxiv.org/abs/2403.11129v1","category":"cs.CL"}
{"created":"2024-03-17 07:17:07","title":"Machine learning-based system reliability analysis with Gaussian Process Regression","abstract":"Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. Simulation results show that the optimal learning strategy considering the Kriging correlation works better than that neglecting the Kriging correlation and other state-of-the art learning functions from the literatures in terms of the reduction of number of evaluations of performance function. However, the implementation needs to investigate very large computational resource.","sentences":["Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy.","Recently, many efficient learning strategies have been proposed to enhance the computational performance.","However, few of them explores the theoretical optimal learning strategy.","In this article, we propose several theorems that facilitates such exploration.","Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated.","Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation.","In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions.","Simulation results show that the optimal learning strategy considering the Kriging correlation works better than that neglecting the Kriging correlation and other state-of-the art learning functions from the literatures in terms of the reduction of number of evaluations of performance function.","However, the implementation needs to investigate very large computational resource."],"url":"http://arxiv.org/abs/2403.11125v1","category":"stat.ML"}
{"created":"2024-03-17 07:07:12","title":"LERENet: Eliminating Intra-class Differences for Metal Surface Defect Few-shot Semantic Segmentation","abstract":"Few-shot segmentation models excel in metal defect detection due to their rapid generalization ability to new classes and pixel-level segmentation, rendering them ideal for addressing data scarcity issues and achieving refined object delineation in industrial applications. Existing works neglect the \\textit{Intra-Class Differences}, inherent in metal surface defect data, which hinders the model from learning sufficient knowledge from the support set to guide the query set segmentation. Specifically, it can be categorized into two types: the \\textit{Semantic Difference} induced by internal factors in metal samples and the \\textit{Distortion Difference} caused by external factors of surroundings. To address these differences, we introduce a \\textbf{L}ocal d\\textbf{E}scriptor based \\textbf{R}easoning and \\textbf{E}xcitation \\textbf{Net}work (\\textbf{LERENet}) to learn the two-view guidance, i.e., local and global information from the graph and feature space, and fuse them to segment precisely. Since the relation structure of local features embedded in graph space will help to eliminate \\textit{Semantic Difference}, we employ Multi-Prototype Reasoning (MPR) module, extracting local descriptors based prototypes and analyzing local-view feature relevance in support-query pairs. Besides, due to the global information that will assist in countering the \\textit{Distortion Difference} in observations, we utilize Multi-Prototype Excitation (MPE) module to capture the global-view relations in support-query pairs. Finally, we employ an Information Fusion Module (IFM) to fuse learned prototypes in local and global views to generate pixel-level masks. Our comprehensive experiments on defect datasets demonstrate that it outperforms existing benchmarks, establishing a new state-of-the-art.","sentences":["Few-shot segmentation models excel in metal defect detection due to their rapid generalization ability to new classes and pixel-level segmentation, rendering them ideal for addressing data scarcity issues and achieving refined object delineation in industrial applications.","Existing works neglect the \\textit{Intra-Class Differences}, inherent in metal surface defect data, which hinders the model from learning sufficient knowledge from the support set to guide the query set segmentation.","Specifically, it can be categorized into two types: the \\textit{Semantic Difference} induced by internal factors in metal samples and the \\textit{Distortion Difference} caused by external factors of surroundings.","To address these differences, we introduce a \\textbf{L}ocal d\\textbf{E}scriptor based \\textbf{R}easoning and \\textbf{E}xcitation \\textbf{Net}work (\\textbf{LERENet}) to learn the two-view guidance, i.e., local and global information from the graph and feature space, and fuse them to segment precisely.","Since the relation structure of local features embedded in graph space will help to eliminate \\textit{Semantic Difference}, we employ Multi-Prototype Reasoning (MPR) module, extracting local descriptors based prototypes and analyzing local-view feature relevance in support-query pairs.","Besides, due to the global information that will assist in countering the \\textit{Distortion Difference} in observations, we utilize Multi-Prototype Excitation (MPE) module to capture the global-view relations in support-query pairs.","Finally, we employ an Information Fusion Module (IFM) to fuse learned prototypes in local and global views to generate pixel-level masks.","Our comprehensive experiments on defect datasets demonstrate that it outperforms existing benchmarks, establishing a new state-of-the-art."],"url":"http://arxiv.org/abs/2403.11122v1","category":"cs.CV"}
{"created":"2024-03-17 03:15:29","title":"Deep Learning-based Sentiment Analysis in Persian Language","abstract":"Recently, there has been a growing interest in the use of deep learning techniques for tasks in natural language processing (NLP), with sentiment analysis being one of the most challenging areas, particularly in the Persian language. The vast amounts of content generated by Persian users on thousands of websites, blogs, and social networks such as Telegram, Instagram, and Twitter present a rich resource of information. Deep learning techniques have become increasingly favored for extracting insights from this extensive pool of raw data, although they face several challenges. In this study, we introduced and implemented a hybrid deep learning-based model for sentiment analysis, using customer review data from the Digikala Online Retailer website. We employed a variety of deep learning networks and regularization techniques as classifiers. Ultimately, our hybrid approach yielded an impressive performance, achieving an F1 score of 78.3 across three sentiment categories: positive, negative, and neutral.","sentences":["Recently, there has been a growing interest in the use of deep learning techniques for tasks in natural language processing (NLP), with sentiment analysis being one of the most challenging areas, particularly in the Persian language.","The vast amounts of content generated by Persian users on thousands of websites, blogs, and social networks such as Telegram, Instagram, and Twitter present a rich resource of information.","Deep learning techniques have become increasingly favored for extracting insights from this extensive pool of raw data, although they face several challenges.","In this study, we introduced and implemented a hybrid deep learning-based model for sentiment analysis, using customer review data from the Digikala Online Retailer website.","We employed a variety of deep learning networks and regularization techniques as classifiers.","Ultimately, our hybrid approach yielded an impressive performance, achieving an F1 score of 78.3 across three sentiment categories: positive, negative, and neutral."],"url":"http://arxiv.org/abs/2403.11069v1","category":"cs.CL"}
{"created":"2024-03-16 22:35:21","title":"FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning","abstract":"Familial Hypercholesterolemia (FH) is a genetic disorder characterized by elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated genes. Early-stage and accurate categorization of FH is of significance allowing for timely interventions to mitigate the risk of life-threatening conditions. Conventional diagnosis approach, however, is complex, costly, and a challenging interpretation task even for experienced clinicians resulting in high underdiagnosis rates. Although there has been a recent surge of interest in using Machine Learning (ML) models for early FH detection, existing solutions only consider a binary classification task solely using classical ML models. Despite its significance, application of Deep Learning (DL) for FH detection is in its infancy, possibly, due to categorical nature of the underlying clinical data. The paper addresses this gap by introducing the FH-TabNet, which is a multi-stage tabular DL network for multi-class (Definite, Probable, Possible, and Unlikely) FH detection. The FH-TabNet initially involves applying a deep tabular data learning architecture (TabNet) for primary categorization into healthy (Possible/Unlikely) and patient (Probable/Definite) classes. Subsequently, independent TabNet classifiers are applied to each subgroup, enabling refined classification. The model's performance is evaluated through 5-fold cross-validation illustrating superior performance in categorizing FH patients, particularly in the challenging low-prevalence subcategories.","sentences":["Familial Hypercholesterolemia (FH) is a genetic disorder characterized by elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated genes.","Early-stage and accurate categorization of FH is of significance allowing for timely interventions to mitigate the risk of life-threatening conditions.","Conventional diagnosis approach, however, is complex, costly, and a challenging interpretation task even for experienced clinicians resulting in high underdiagnosis rates.","Although there has been a recent surge of interest in using Machine Learning (ML) models for early FH detection, existing solutions only consider a binary classification task solely using classical ML models.","Despite its significance, application of Deep Learning (DL) for FH detection is in its infancy, possibly, due to categorical nature of the underlying clinical data.","The paper addresses this gap by introducing the FH-TabNet, which is a multi-stage tabular DL network for multi-class (Definite, Probable, Possible, and Unlikely) FH detection.","The FH-TabNet initially involves applying a deep tabular data learning architecture (TabNet) for primary categorization into healthy (Possible/Unlikely) and patient (Probable/Definite) classes.","Subsequently, independent TabNet classifiers are applied to each subgroup, enabling refined classification.","The model's performance is evaluated through 5-fold cross-validation illustrating superior performance in categorizing FH patients, particularly in the challenging low-prevalence subcategories."],"url":"http://arxiv.org/abs/2403.11032v1","category":"cs.LG"}
{"created":"2024-03-16 21:34:24","title":"Accelerating prototype selection with spatial abstraction","abstract":"The increasing digitalization in industry and society leads to a growing abundance of data available to be processed and exploited. However, the high volume of data requires considerable computational resources for applying machine learning approaches. Prototype selection techniques have been applied to reduce the requirements of computational resources that are needed by these techniques. In this paper, we propose an approach for speeding up existing prototype selection techniques. It builds an abstract representation of the dataset, using the notion of spatial partition. The second step uses this abstract representation to prune the search space efficiently and select a set of candidate prototypes. After, some conventional prototype selection algorithms can be applied to the candidates selected by our approach. Our approach was integrated with five conventional prototype selection algorithms and tested on 14 widely recognized datasets used in classification tasks. The performance of the modified algorithms was compared to that of their original versions in terms of accuracy and reduction rate. The experimental results demonstrate that, overall, our proposed approach maintains accuracy while enhancing the reduction rate of the original prototype selection algorithms and simultaneously reducing their execution times.","sentences":["The increasing digitalization in industry and society leads to a growing abundance of data available to be processed and exploited.","However, the high volume of data requires considerable computational resources for applying machine learning approaches.","Prototype selection techniques have been applied to reduce the requirements of computational resources that are needed by these techniques.","In this paper, we propose an approach for speeding up existing prototype selection techniques.","It builds an abstract representation of the dataset, using the notion of spatial partition.","The second step uses this abstract representation to prune the search space efficiently and select a set of candidate prototypes.","After, some conventional prototype selection algorithms can be applied to the candidates selected by our approach.","Our approach was integrated with five conventional prototype selection algorithms and tested on 14 widely recognized datasets used in classification tasks.","The performance of the modified algorithms was compared to that of their original versions in terms of accuracy and reduction rate.","The experimental results demonstrate that, overall, our proposed approach maintains accuracy while enhancing the reduction rate of the original prototype selection algorithms and simultaneously reducing their execution times."],"url":"http://arxiv.org/abs/2403.11020v1","category":"cs.LG"}
{"created":"2024-03-18 10:38:06","title":"A tree-approach Pauli decomposition algorithm with application to quantum computing","abstract":"The Pauli matrices are 2-by-2 matrices that are very useful in quantum computing. They can be used as elementary gates in quantum circuits but also to decompose any matrix of $\\mathbb{C}^{2^n \\times 2^n}$ as a linear combination of tensor products of the Pauli matrices. However, the computational cost of this decomposition is potentially very expensive since it can be exponential in $n$. In this paper, we propose an algorithm with a parallel implementation that optimizes this decomposition using a tree approach to avoid redundancy in the computation while using a limited memory footprint. We also explain how some particular matrix structures can be exploited to reduce the number of operations. We provide numerical experiments to evaluate the sequential and parallel performance of our decomposition algorithm and we illustrate how this algorithm can be applied to encode matrices in a quantum memory.","sentences":["The Pauli matrices are 2-by-2 matrices that are very useful in quantum computing.","They can be used as elementary gates in quantum circuits but also to decompose any matrix of $\\mathbb{C}^{2^n \\times 2^n}$ as a linear combination of tensor products of the Pauli matrices.","However, the computational cost of this decomposition is potentially very expensive since it can be exponential in $n$. In this paper, we propose an algorithm with a parallel implementation that optimizes this decomposition using a tree approach to avoid redundancy in the computation while using a limited memory footprint.","We also explain how some particular matrix structures can be exploited to reduce the number of operations.","We provide numerical experiments to evaluate the sequential and parallel performance of our decomposition algorithm","and we illustrate how this algorithm can be applied to encode matrices in a quantum memory."],"url":"http://arxiv.org/abs/2403.11644v1","category":"quant-ph"}
{"created":"2024-03-18 09:11:49","title":"Surface hopping molecular dynamics simulation of ultrafast methyl iodide photodissociation mapped by Coulomb explosion imaging","abstract":"We present a highly efficient method to directly simulate the photodissociation followed by Coulomb explosion of methyl iodide. In order to achieve statistical reliability, more than 40,000 trajectories are calculated on accurate potential energy surfaces of both the neutral molecule and the doubly charged cation. Non-adiabatic effects during photodissociation are treated using a Landau-Zener surface hopping algorithm. The simulation is performed analogous to a recent pump-probe experiment using coincident ion momentum imaging [Ziaee \\textit{et al., Phys. Chem. Chem. Phys.} 2023, \\textbf{25}, 9999]. At large pump-probe delays, the simulated delay-dependent kinetic energy release signals show overall good agreement with the experiment, with two major dissociation channels leading to $\\text{I}(^2\\text{P}_{3/2})$ and $\\text{I}^*(^2\\text{P}_{1/2})$ products. At short pump-probe delays, the simulated kinetic energy release shows a clear bifurcation near 12 fs, owing to non-adiabatic transitions through a conical intersection. The developed method is particularly suitable and efficient in simulating processes that highly rely on statistics or for identifying rare reaction channels.","sentences":["We present a highly efficient method to directly simulate the photodissociation followed by Coulomb explosion of methyl iodide.","In order to achieve statistical reliability, more than 40,000 trajectories are calculated on accurate potential energy surfaces of both the neutral molecule and the doubly charged cation.","Non-adiabatic effects during photodissociation are treated using a Landau-Zener surface hopping algorithm.","The simulation is performed analogous to a recent pump-probe experiment using coincident ion momentum imaging [Ziaee \\textit{et al., Phys.","Chem.","Chem.","Phys.} 2023, \\textbf{25}, 9999].","At large pump-probe delays, the simulated delay-dependent kinetic energy release signals show overall good agreement with the experiment, with two major dissociation channels leading to $\\text{I}(^2\\text{P}_{3/2})$ and $\\text{I}^*(^2\\text{P}_{1/2})$ products.","At short pump-probe delays, the simulated kinetic energy release shows a clear bifurcation near 12 fs, owing to non-adiabatic transitions through a conical intersection.","The developed method is particularly suitable and efficient in simulating processes that highly rely on statistics or for identifying rare reaction channels."],"url":"http://arxiv.org/abs/2403.11592v1","category":"physics.chem-ph"}
{"created":"2024-03-18 07:43:31","title":"On the Integration of Spectrum-Based Fault Localization Tools into IDEs","abstract":"Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected. SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness. Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available. Reasons for this can be multiple, including the algortihms' sub-optimal effectiveness and other technical weaknesses. But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs. In this paper, we attempt to provide such a list in form of recommendations, based on surveying the most popular SBFL tools and on our own researchers' and tool builders' experience.","sentences":["Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected.","SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness.","Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available.","Reasons for this can be multiple, including the algortihms' sub-optimal effectiveness and other technical weaknesses.","But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs.","In this paper, we attempt to provide such a list in form of recommendations, based on surveying the most popular SBFL tools and on our own researchers' and tool builders' experience."],"url":"http://arxiv.org/abs/2403.11538v1","category":"cs.SE"}
{"created":"2024-03-18 05:48:13","title":"New energy distances for statistical inference on infinite dimensional Hilbert spaces without moment conditions","abstract":"For statistical inference on an infinite-dimensional Hilbert space $\\H $ with no moment conditions we introduce a new class of energy distances on the space of probability measures on $\\H$. The proposed distances consist of the integrated squared modulus of the corresponding difference of the characteristic functionals with respect to a reference probability measure on the Hilbert space. Necessary and sufficient conditions are established for the reference probability measure to be {\\em characteristic}, the property that guarantees that the distance defines a metric on the space of probability measures on $\\H$. We also use these results to define new distance covariances, which can be used to measure the dependence between the marginals of a two dimensional distribution of $\\H^2$ without existing moments.   On the basis of the new distances we develop statistical inference for Hilbert space valued data, which does not require any moment assumptions. As a consequence, our methods are robust with respect to heavy tails in finite dimensional data. In particular, we consider the problem of comparing the distributions of two samples and the problem of testing for independence and construct new minimax optimal tests for the corresponding hypotheses. We also develop aggregated (with respect to the reference measure) procedures for power enhancement and investigate the finite-sample properties by means of a simulation study.","sentences":["For statistical inference on an infinite-dimensional Hilbert space $\\H $ with no moment conditions we introduce a new class of energy distances on the space of probability measures on $\\H$. The proposed distances consist of the integrated squared modulus of the corresponding difference of the characteristic functionals with respect to a reference probability measure on the Hilbert space.","Necessary and sufficient conditions are established for the reference probability measure to be {\\em characteristic}, the property that guarantees that the distance defines a metric on the space of probability measures on $\\H$. We also use these results to define new distance covariances, which can be used to measure the dependence between the marginals of a two dimensional distribution of $\\H^2$ without existing moments.   ","On the basis of the new distances we develop statistical inference for Hilbert space valued data, which does not require any moment assumptions.","As a consequence, our methods are robust with respect to heavy tails in finite dimensional data.","In particular, we consider the problem of comparing the distributions of two samples and the problem of testing for independence and construct new minimax optimal tests for the corresponding hypotheses.","We also develop aggregated (with respect to the reference measure) procedures for power enhancement and investigate the finite-sample properties by means of a simulation study."],"url":"http://arxiv.org/abs/2403.11489v1","category":"math.ST"}
{"created":"2024-03-18 02:43:01","title":"ETH-Tight Algorithm for Cycle Packing on Unit Disk Graphs","abstract":"In this paper, we consider the Cycle Packing problem on unit disk graphs defined as follows. Given a unit disk graph G with n vertices and an integer k, the goal is to find a set of $k$ vertex-disjoint cycles of G if it exists. Our algorithm runs in time $2^{O(\\sqrt k)}n^{O(1)}$. This improves the $2^{O(\\sqrt k\\log k)}n^{O(1)}$-time algorithm by Fomin et al. [SODA 2012, ICALP 2017]. Moreover, our algorithm is optimal assuming the exponential-time hypothesis.","sentences":["In this paper, we consider the Cycle Packing problem on unit disk graphs defined as follows.","Given a unit disk graph G with n vertices and an integer k, the goal is to find a set of $k$ vertex-disjoint cycles of G if it exists.","Our algorithm runs in time $2^{O(\\sqrt k)}n^{O(1)}$.","This improves the $2^{O(\\sqrt k\\log k)}n^{O(1)}$-time algorithm by Fomin et al.","[SODA 2012, ICALP 2017].","Moreover, our algorithm is optimal assuming the exponential-time hypothesis."],"url":"http://arxiv.org/abs/2403.11426v1","category":"cs.DS"}
{"created":"2024-03-18 01:11:53","title":"Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization","abstract":"The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information. NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance. However, these models are found to be vulnerable to adversarial attacks, which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores. In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the adversarial robustness of NR-IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\\ell_1$ norm of the model's gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on NR-IQA models. Our study offers valuable insights into the adversarial robustness of NR-IQA models and provides a foundation for future research in this area.","sentences":["The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information.","NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance.","However, these models are found to be vulnerable to adversarial attacks, which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores.","In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the adversarial robustness of NR-IQA models.","To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\\ell_1$ norm of the model's gradient with respect to the input image.","Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models.","Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks.","To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on NR-IQA models.","Our study offers valuable insights into the adversarial robustness of NR-IQA models and provides a foundation for future research in this area."],"url":"http://arxiv.org/abs/2403.11397v1","category":"cs.CV"}
{"created":"2024-03-17 23:51:31","title":"Quasi-Monte Carlo and importance sampling methods for Bayesian inverse problems","abstract":"Importance Sampling (IS), an effective variance reduction strategy in Monte Carlo (MC) simulation, is frequently utilized for Bayesian inference and other statistical challenges. Quasi-Monte Carlo (QMC) replaces the random samples in MC with low discrepancy points and has the potential to substantially enhance error rates. In this paper, we integrate IS with a randomly shifted rank-1 lattice rule, a widely used QMC method, to approximate posterior expectations arising from Bayesian Inverse Problems (BIPs) where the posterior density tends to concentrate as the intensity of noise diminishes. Within the framework of weighted Hilbert spaces, we first establish the convergence rate of the lattice rule for a large class of unbounded integrands. This method extends to the analysis of QMC combined with IS in BIPs. Furthermore, we explore the robustness of the IS-based randomly shifted rank-1 lattice rule by determining the quadrature error rate with respect to the noise level. The effects of using Gaussian distributions and $t$-distributions as the proposal distributions on the error rate of QMC are comprehensively investigated. We find that the error rate may deteriorate at low intensity of noise when using improper proposals, such as the prior distribution. To reclaim the effectiveness of QMC, we propose a new IS method such that the lattice rule with $N$ quadrature points achieves an optimal error rate close to $O(N^{-1})$, which is insensitive to the noise level. Numerical experiments are conducted to support the theoretical results.","sentences":["Importance Sampling (IS), an effective variance reduction strategy in Monte Carlo (MC) simulation, is frequently utilized for Bayesian inference and other statistical challenges.","Quasi-Monte Carlo (QMC) replaces the random samples in MC with low discrepancy points and has the potential to substantially enhance error rates.","In this paper, we integrate IS with a randomly shifted rank-1 lattice rule, a widely used QMC method, to approximate posterior expectations arising from Bayesian Inverse Problems (BIPs) where the posterior density tends to concentrate as the intensity of noise diminishes.","Within the framework of weighted Hilbert spaces, we first establish the convergence rate of the lattice rule for a large class of unbounded integrands.","This method extends to the analysis of QMC combined with IS in BIPs.","Furthermore, we explore the robustness of the IS-based randomly shifted rank-1 lattice rule by determining the quadrature error rate with respect to the noise level.","The effects of using Gaussian distributions and $t$-distributions as the proposal distributions on the error rate of QMC are comprehensively investigated.","We find that the error rate may deteriorate at low intensity of noise when using improper proposals, such as the prior distribution.","To reclaim the effectiveness of QMC, we propose a new IS method such that the lattice rule with $N$ quadrature points achieves an optimal error rate close to $O(N^{-1})$, which is insensitive to the noise level.","Numerical experiments are conducted to support the theoretical results."],"url":"http://arxiv.org/abs/2403.11374v1","category":"math.NA"}
{"created":"2024-03-17 22:51:26","title":"Determination of a Small Elliptical Anomaly in Electrical Impedance Tomography using Minimal Measurements","abstract":"We consider the problem of determining a small elliptical conductivity anomaly in a unit disc from boundary measurements. The conductivity of the anomaly is assumed to be a small perturbation from the constant background. A measurement of voltage across two point-electrodes on the boundary through which a constant current is passed. We further assume the limiting case when the distance between two electrodes go to zero, creating a dipole field. We show that three such measurements suffice to locate the anomaly size and location inside the disc. Two further measurements are needed to obtain the aspect ratio and the orientation of the ellipse. The investigation includes the studies of the stability of the inverse problem and optimal experiment design.","sentences":["We consider the problem of determining a small elliptical conductivity anomaly in a unit disc from boundary measurements.","The conductivity of the anomaly is assumed to be a small perturbation from the constant background.","A measurement of voltage across two point-electrodes on the boundary through which a constant current is passed.","We further assume the limiting case when the distance between two electrodes go to zero, creating a dipole field.","We show that three such measurements suffice to locate the anomaly size and location inside the disc.","Two further measurements are needed to obtain the aspect ratio and the orientation of the ellipse.","The investigation includes the studies of the stability of the inverse problem and optimal experiment design."],"url":"http://arxiv.org/abs/2403.11365v1","category":"math.NA"}
{"created":"2024-03-17 17:01:25","title":"Interference Cancellation for OTFS-Based Over-the-Air Computation","abstract":"This paper investigates over-the-air computation (AirComp) in the context of multiple-access time-varying multipath channels. We focus on a scenario where devices with high mobility transmit their sensing data to a fusion center (FC) for averaging. To combat the time-varying channel and Doppler effect, each device adopts orthogonal time frequency space (OTFS) modulation. After signals are received by the FC, the aggregated data undergoes demodulation and estimation within the delay-Doppler domain. We leverage the mean squared error (MSE) as a metric for the computational error of OTFS-based AirComp. We then derive the optimal transmit power at each device and signal scaling factor at FC for minimizing MSE. Notably, the performance of OTFS-based AirComp is not only affected by the noise but also by the inter-symbol interference and inter-link interference arising from the multipath channel. To counteract the interference-induced computational errors, we incorporate zero-padding (ZP)-assisted OTFS into AirComp and propose algorithms for interference cancellation. Numerical results underscore the enhanced performance of ZP-assisted OTFS-based AirComp over naive OTFS-based AirComp.","sentences":["This paper investigates over-the-air computation (AirComp) in the context of multiple-access time-varying multipath channels.","We focus on a scenario where devices with high mobility transmit their sensing data to a fusion center (FC) for averaging.","To combat the time-varying channel and Doppler effect, each device adopts orthogonal time frequency space (OTFS) modulation.","After signals are received by the FC, the aggregated data undergoes demodulation and estimation within the delay-Doppler domain.","We leverage the mean squared error (MSE) as a metric for the computational error of OTFS-based AirComp.","We then derive the optimal transmit power at each device and signal scaling factor at FC for minimizing MSE.","Notably, the performance of OTFS-based AirComp is not only affected by the noise but also by the inter-symbol interference and inter-link interference arising from the multipath channel.","To counteract the interference-induced computational errors, we incorporate zero-padding (ZP)-assisted OTFS into AirComp and propose algorithms for interference cancellation.","Numerical results underscore the enhanced performance of ZP-assisted OTFS-based AirComp over naive OTFS-based AirComp."],"url":"http://arxiv.org/abs/2403.11272v1","category":"cs.IT"}
{"created":"2024-03-17 15:41:37","title":"Lagrange duality on DC evenly convex optimization problems via a generalized conjugation scheme","abstract":"In this paper we study how Lagrange duality is connected to optimization problems whose objective function is the difference of two convex functions, briefly called DC problems. We present two Lagrange dual problems, each of them obtained via a different approach. While one of the duals corresponds to the standard formulation of the Lagrange dual problem, the other is written in terms of conjugate functions. When one of the involved functions in the objective is evenly convex, both problems are equivalent, but this relation is no longer true in the general setting. For this reason, we study conditions ensuring not only weak, but also zero duality and strong duality between the primal and one of the dual problems written using conjugate functions. For the other dual, and due to the fact that weak duality holds by construction, we just develop conditions for zero duality gap and strong duality between the primal DC problem and its (standard) Lagrange dual problem. Finally, we characterize weak and strong duality together with zero duality gap between the primal problem and its Fenchel-Lagrange dual following techniques used throughout the manuscript.","sentences":["In this paper we study how Lagrange duality is connected to optimization problems whose objective function is the difference of two convex functions, briefly called DC problems.","We present two Lagrange dual problems, each of them obtained via a different approach.","While one of the duals corresponds to the standard formulation of the Lagrange dual problem, the other is written in terms of conjugate functions.","When one of the involved functions in the objective is evenly convex, both problems are equivalent, but this relation is no longer true in the general setting.","For this reason, we study conditions ensuring not only weak, but also zero duality and strong duality between the primal and one of the dual problems written using conjugate functions.","For the other dual, and due to the fact that weak duality holds by construction, we just develop conditions for zero duality gap and strong duality between the primal DC problem and its (standard) Lagrange dual problem.","Finally, we characterize weak and strong duality together with zero duality gap between the primal problem and its Fenchel-Lagrange dual following techniques used throughout the manuscript."],"url":"http://arxiv.org/abs/2403.11248v1","category":"math.OC"}
{"created":"2024-03-17 14:49:09","title":"ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization","abstract":"Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datasets, our trained model consistently exhibits superior performance in chart summarization tasks, surpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and codes are publicly accessible.","sentences":["Data visualization serves as a critical means for presenting data and mining its valuable insights.","The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts.","However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches.","To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart.","Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data.","Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries.","Built upon the curated datasets, our trained model consistently exhibits superior performance in chart summarization tasks, surpassing 8 state-of-the-art models over 7 evaluation metrics.","Our dataset and codes are publicly accessible."],"url":"http://arxiv.org/abs/2403.11236v1","category":"cs.CL"}
{"created":"2024-03-17 14:28:53","title":"Routing Algorithms","abstract":"Routing algorithms play a crucial role in the efficient transmission of data within computer networks by determining the optimal paths for packet forwarding. This paper presents a comprehensive exploration of routing algorithms, focusing on their fundamental principles, classification, challenges, recent advancements, and practical applications. Beginning with an overview of the significance of routing in modern communication networks, the paper delves into the historical evolution of routing algorithms, tracing their development from early approaches to contemporary techniques. Key categories of routing algorithms, including distance vector, link-state, and path vector algorithms, are examined in detail, along with hybrid approaches that integrate multiple routing paradigms. Common challenges faced by routing algorithms, such as routing loops and scalability issues, are identified, and current research efforts aimed at addressing these challenges are discussed.","sentences":["Routing algorithms play a crucial role in the efficient transmission of data within computer networks by determining the optimal paths for packet forwarding.","This paper presents a comprehensive exploration of routing algorithms, focusing on their fundamental principles, classification, challenges, recent advancements, and practical applications.","Beginning with an overview of the significance of routing in modern communication networks, the paper delves into the historical evolution of routing algorithms, tracing their development from early approaches to contemporary techniques.","Key categories of routing algorithms, including distance vector, link-state, and path vector algorithms, are examined in detail, along with hybrid approaches that integrate multiple routing paradigms.","Common challenges faced by routing algorithms, such as routing loops and scalability issues, are identified, and current research efforts aimed at addressing these challenges are discussed."],"url":"http://arxiv.org/abs/2403.11228v1","category":"cs.NI"}
{"created":"2024-03-17 09:41:55","title":"An Empirical Study on JIT Defect Prediction Based on BERT-style Model","abstract":"Previous works on Just-In-Time (JIT) defect prediction tasks have primarily applied pre-trained models directly, neglecting the configurations of their fine-tuning process. In this study, we perform a systematic empirical study to understand the impact of the settings of the fine-tuning process on BERT-style pre-trained model for JIT defect prediction. Specifically, we explore the impact of different parameter freezing settings, parameter initialization settings, and optimizer strategies on the performance of BERT-style models for JIT defect prediction. Our findings reveal the crucial role of the first encoder layer in the BERT-style model and the project sensitivity to parameter initialization settings. Another notable finding is that the addition of a weight decay strategy in the Adam optimizer can slightly improve model performance. Additionally, we compare performance using different feature extractors (FCN, CNN, LSTM, transformer) and find that a simple network can achieve great performance. These results offer new insights for fine-tuning pre-trained models for JIT defect prediction. We combine these findings to find a cost-effective fine-tuning method based on LoRA, which achieve a comparable performance with only one-third memory consumption than original fine-tuning process.","sentences":["Previous works on Just-In-Time (JIT) defect prediction tasks have primarily applied pre-trained models directly, neglecting the configurations of their fine-tuning process.","In this study, we perform a systematic empirical study to understand the impact of the settings of the fine-tuning process on BERT-style pre-trained model for JIT defect prediction.","Specifically, we explore the impact of different parameter freezing settings, parameter initialization settings, and optimizer strategies on the performance of BERT-style models for JIT defect prediction.","Our findings reveal the crucial role of the first encoder layer in the BERT-style model and the project sensitivity to parameter initialization settings.","Another notable finding is that the addition of a weight decay strategy in the Adam optimizer can slightly improve model performance.","Additionally, we compare performance using different feature extractors (FCN, CNN, LSTM, transformer) and find that a simple network can achieve great performance.","These results offer new insights for fine-tuning pre-trained models for JIT defect prediction.","We combine these findings to find a cost-effective fine-tuning method based on LoRA, which achieve a comparable performance with only one-third memory consumption than original fine-tuning process."],"url":"http://arxiv.org/abs/2403.11158v1","category":"cs.SE"}
{"created":"2024-03-17 06:48:20","title":"Superlinear Optimization Algorithms","abstract":"This paper proposes several novel optimization algorithms for minimizing a nonlinear objective function. The algorithms are enlightened by the optimal state trajectory of an optimal control problem closely related to the minimized objective function. They are superlinear convergent when appropriate parameters are selected as required. Unlike Newton's method, all of them can be also applied in the case of a singular Hessian matrix. More importantly, by reduction, some of them avoid calculating the inverse of the Hessian matrix or an identical dimension matrix and some of them need only the diagonal elements of the Hessian matrix. In these cases, these algorithms still outperform the gradient descent method. The merits of the proposed optimization algorithm are illustrated by numerical experiments.","sentences":["This paper proposes several novel optimization algorithms for minimizing a nonlinear objective function.","The algorithms are enlightened by the optimal state trajectory of an optimal control problem closely related to the minimized objective function.","They are superlinear convergent when appropriate parameters are selected as required.","Unlike Newton's method, all of them can be also applied in the case of a singular Hessian matrix.","More importantly, by reduction, some of them avoid calculating the inverse of the Hessian matrix or an identical dimension matrix and some of them need only the diagonal elements of the Hessian matrix.","In these cases, these algorithms still outperform the gradient descent method.","The merits of the proposed optimization algorithm are illustrated by numerical experiments."],"url":"http://arxiv.org/abs/2403.11115v1","category":"math.OC"}
{"created":"2024-03-17 05:25:42","title":"Modeling and Coverage Analysis of K-Tier Integrated Satellite-Terrestrial Downlink Networks","abstract":"Integrated satellite-terrestrial networks (ISTNs) can significantly expand network coverage while diminishing reliance on terrestrial infrastructure. Despite the enticing potential of ISTNs, there is no comprehensive mathematical performance analysis framework for these emerging networks. In this paper, we introduce a tractable approach to analyze the downlink coverage performance of multi-tier ISTNs, where each network tier operates with orthogonal frequency bands. The proposed approach is to model the spatial distribution of cellular and satellite base stations using homogeneous Poisson point processes arranged on concentric spheres with varying radii. Central to our analysis is a displacement principle that transforms base station locations on different spheres into projected rings while preserving the distance distribution to the typical user. By incorporating the effects of Shadowed-Rician fading on satellite channels and employing orthogonal frequency bands, we derive analytical expressions for coverage in the integrated networks while keeping full generality. Our primary discovery is that network performance reaches its maximum when selecting the optimal density ratio of users associated with the network according to the density and the channel parameters of each network. Through simulations, we validate the precision of our derived expressions.","sentences":["Integrated satellite-terrestrial networks (ISTNs) can significantly expand network coverage while diminishing reliance on terrestrial infrastructure.","Despite the enticing potential of ISTNs, there is no comprehensive mathematical performance analysis framework for these emerging networks.","In this paper, we introduce a tractable approach to analyze the downlink coverage performance of multi-tier ISTNs, where each network tier operates with orthogonal frequency bands.","The proposed approach is to model the spatial distribution of cellular and satellite base stations using homogeneous Poisson point processes arranged on concentric spheres with varying radii.","Central to our analysis is a displacement principle that transforms base station locations on different spheres into projected rings while preserving the distance distribution to the typical user.","By incorporating the effects of Shadowed-Rician fading on satellite channels and employing orthogonal frequency bands, we derive analytical expressions for coverage in the integrated networks while keeping full generality.","Our primary discovery is that network performance reaches its maximum when selecting the optimal density ratio of users associated with the network according to the density and the channel parameters of each network.","Through simulations, we validate the precision of our derived expressions."],"url":"http://arxiv.org/abs/2403.11096v1","category":"eess.SP"}
{"created":"2024-03-17 02:06:49","title":"Large Language Models Powered Context-aware Motion Prediction","abstract":"Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving.","sentences":["Motion prediction is among the most fundamental tasks in autonomous driving.","Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks.","In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks.","We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts.","Through this approach, we obtained rich traffic context information from the LLM.","By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions.","Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\\% LLM-augmented datasets.","Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving."],"url":"http://arxiv.org/abs/2403.11057v1","category":"cs.CV"}
{"created":"2024-03-16 22:39:02","title":"Stellarator Optimization with Constraints","abstract":"In this work we consider the problem of optimizing a stellarator subject to hard constraints on the design variables and physics properties of the equilibrium. We survey current numerical methods for handling these constraints, and summarize a number of methods from the wider optimization community that have not been used extensively for stellarator optimization thus far. We demonstrate the utility of new methods of constrained optimization by optimizing a QA stellarator for favorable physics properties while preventing strong shaping of the plasma boundary which can be difficult to create with external current sources.","sentences":["In this work we consider the problem of optimizing a stellarator subject to hard constraints on the design variables and physics properties of the equilibrium.","We survey current numerical methods for handling these constraints, and summarize a number of methods from the wider optimization community that have not been used extensively for stellarator optimization thus far.","We demonstrate the utility of new methods of constrained optimization by optimizing a QA stellarator for favorable physics properties while preventing strong shaping of the plasma boundary which can be difficult to create with external current sources."],"url":"http://arxiv.org/abs/2403.11033v1","category":"physics.plasm-ph"}
{"created":"2024-03-16 20:40:58","title":"Towards stochastic realization theory for Generalized Linear Switched Systems with inputs: decomposition into stochastic and deterministic components and existence and uniqueness of innovation form","abstract":"In this paper, we study a class of stochastic Generalized Linear Switched System (GLSS), which includes subclasses of jump-Markov, piecewide-linear and Linear Parameter-Varying (LPV) systems. We prove that the output of such systems can be decomposed into deterministic and stochastic components. Using this decomposition, we show existence of state-space representation in innovation form, and we provide sufficient conditions for such representations to be minimal and unique up to isomorphism.","sentences":["In this paper, we study a class of stochastic Generalized Linear Switched System (GLSS), which includes subclasses of jump-Markov, piecewide-linear and Linear Parameter-Varying (LPV) systems.","We prove that the output of such systems can be decomposed into deterministic and stochastic components.","Using this decomposition, we show existence of state-space representation in innovation form, and we provide sufficient conditions for such representations to be minimal and unique up to isomorphism."],"url":"http://arxiv.org/abs/2403.11012v1","category":"math.OC"}
{"created":"2024-03-16 16:24:19","title":"Circle Packing Problem Using Nature-Inspired Optimization Techniques","abstract":"This paper deals with the problem of circle packing, in which the largest radii circle is to be fit in a confined space filled with arbitrary circles of different radii and centers. A circle packing problem is one of a variety of cutting and packing problems. We suggest four different nature-inspired Meta-heuristic algorithms to solve this problem. Algorithms are based on the social behavior of other biology species such as birds, wolves, fireflies, and bats. Moreover, recent advancements in these algorithms are also considered for problem-solving. The circle packing problem is one of the NP-hard problems. It is challenging to solve NP-hard problems exactly, so the proposed algorithms provide an approximate solution within the allotted time. Standard statistical parameters are used for comparison, and simulation and results indicate that the problem is highly non-linear and sensitive.","sentences":["This paper deals with the problem of circle packing, in which the largest radii circle is to be fit in a confined space filled with arbitrary circles of different radii and centers.","A circle packing problem is one of a variety of cutting and packing problems.","We suggest four different nature-inspired Meta-heuristic algorithms to solve this problem.","Algorithms are based on the social behavior of other biology species such as birds, wolves, fireflies, and bats.","Moreover, recent advancements in these algorithms are also considered for problem-solving.","The circle packing problem is one of the NP-hard problems.","It is challenging to solve NP-hard problems exactly, so the proposed algorithms provide an approximate solution within the allotted time.","Standard statistical parameters are used for comparison, and simulation and results indicate that the problem is highly non-linear and sensitive."],"url":"http://arxiv.org/abs/2403.10965v1","category":"math.OC"}
{"created":"2024-03-16 16:18:43","title":"Procedurally Optimised ZX-Diagram Cutting for Efficient T-Decomposition in Classical Simulation","abstract":"A quantum circuit may be strongly classically simulated with the aid of ZX-calculus by decomposing its $t$ T-gates into a sum of $2^{\\alpha t}$ classically computable stabiliser terms. In this paper, we introduce a general procedure to find an optimal pattern of vertex cuts in a ZX-diagram to maximise its T-count reduction at the cost of the fewest cuts. Rather than reducing a Clifford+T diagram based on a fixed routine of decomposing its T-gates directly (as is the conventional approach), we focus instead on taking advantage of certain patterns and structures common to such circuits to, in effect, design by automatic procedure an arrangement of spider decompositions that is optimised for the particular circuit. In short, this works by assigning weights to vertices based on how many T-like gates they are blocking from fusing/cancelling and then appropriately propagating these weights through any neighbours which are then blocking weighted vertices from fusing, and so on. Ultimately, this then provides a set of weightings on relevant nodes, which can then each be cut, starting from the highest weighted down. While this is a heuristic approach, we show that, for circuits small enough to verify, this method achieves the most optimal set of cuts possible $71\\%$ of the time. Furthermore, there is no upper bound for the efficiency achieved by this method, allowing, in principle, an effective decomposition efficiency $\\alpha\\rightarrow0$ for highly structured circuits. Even applied to random pseudo-structured circuits (produced from CNOTs, phase gates, and Toffolis), we record the number of stabiliser terms required to reduce all T-gates, via our method as compared to that of the more conventional T-decomposition approaches (with $\\alpha\\approx0.47$), and show consistent improvements of orders of magnitude, with an effective efficiency $0.1\\lesssim\\alpha\\lesssim0.2$.","sentences":["A quantum circuit may be strongly classically simulated with the aid of ZX-calculus by decomposing its $t$ T-gates into a sum of $2^{\\alpha t}$ classically computable stabiliser terms.","In this paper, we introduce a general procedure to find an optimal pattern of vertex cuts in a ZX-diagram to maximise its T-count reduction at the cost of the fewest cuts.","Rather than reducing a Clifford+T diagram based on a fixed routine of decomposing its T-gates directly (as is the conventional approach), we focus instead on taking advantage of certain patterns and structures common to such circuits to, in effect, design by automatic procedure an arrangement of spider decompositions that is optimised for the particular circuit.","In short, this works by assigning weights to vertices based on how many T-like gates they are blocking from fusing/cancelling and then appropriately propagating these weights through any neighbours which are then blocking weighted vertices from fusing, and so on.","Ultimately, this then provides a set of weightings on relevant nodes, which can then each be cut, starting from the highest weighted down.","While this is a heuristic approach, we show that, for circuits small enough to verify, this method achieves the most optimal set of cuts possible $71\\%$ of the time.","Furthermore, there is no upper bound for the efficiency achieved by this method, allowing, in principle, an effective decomposition efficiency $\\alpha\\rightarrow0$ for highly structured circuits.","Even applied to random pseudo-structured circuits (produced from CNOTs, phase gates, and Toffolis), we record the number of stabiliser terms required to reduce all T-gates, via our method as compared to that of the more conventional T-decomposition approaches (with $\\alpha\\approx0.47$), and show consistent improvements of orders of magnitude, with an effective efficiency $0.1\\lesssim\\alpha\\lesssim0.2$."],"url":"http://arxiv.org/abs/2403.10964v1","category":"quant-ph"}
{"created":"2024-03-16 15:37:08","title":"Information Geometry and Universal Bounds on Non-stationary Responsiveness of Markov Dynamics","abstract":"Markov dynamics can effectively describe a wide range of thermodynamic and biological processes. Understanding how such systems respond to changes in environmental variables or external inputs is crucial for predicting and controlling their behaviors. This work presents a set of universal thermodynamic bounds on the responsiveness of any Markov system toward environmental changes. The systems of interest can be arbitrarily far from stationary state. The central element of this work is the introduction of information geometry on the manifold of probability distributions of stochastic trajectories. This work lays the foundation for understanding biological processes, engineering artificial systems, and exploring the fundamental principles governing complex systems far from equilibrium.","sentences":["Markov dynamics can effectively describe a wide range of thermodynamic and biological processes.","Understanding how such systems respond to changes in environmental variables or external inputs is crucial for predicting and controlling their behaviors.","This work presents a set of universal thermodynamic bounds on the responsiveness of any Markov system toward environmental changes.","The systems of interest can be arbitrarily far from stationary state.","The central element of this work is the introduction of information geometry on the manifold of probability distributions of stochastic trajectories.","This work lays the foundation for understanding biological processes, engineering artificial systems, and exploring the fundamental principles governing complex systems far from equilibrium."],"url":"http://arxiv.org/abs/2403.10952v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-16 11:15:40","title":"Solving the Multiobjective Quasi-Clique Problem","abstract":"Given a simple undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 < \\gamma \\leq 1)$. Finding a maximum quasi-clique has been addressed from two different perspectives: $i)$ maximizing vertex cardinality for a given edge density; and $ii)$ maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique Problem (MOQC), which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a biobjective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using $\\varepsilon$-constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an $\\varepsilon$-constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an $\\varepsilon$-constraint in a final stage. Experimental results on real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in terms of running time and ability to produce new efficient quasi-cliques.","sentences":["Given a simple undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 < \\gamma \\leq 1)$. Finding a maximum quasi-clique has been addressed from two different perspectives: $i)$ maximizing vertex cardinality for a given edge density; and $ii)$ maximizing edge density for a given vertex cardinality.","However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective.","We introduce the Multiobjective Quasi-clique Problem (MOQC), which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density.","To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a biobjective optimization problem, along with several properties of the MOQC problem and quasi-cliques.","We propose a baseline approach using $\\varepsilon$-constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an $\\varepsilon$-constraint methodology in the second phase.","Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an $\\varepsilon$-constraint in a final stage.","Experimental results on real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in terms of running time and ability to produce new efficient quasi-cliques."],"url":"http://arxiv.org/abs/2403.10896v1","category":"cs.DM"}
{"created":"2024-03-16 09:45:07","title":"Integration of 5G and Motion Sensors for Vehicular Positioning: A Loosely-Coupled Approach","abstract":"Autonomous vehicles (AVs) are poised to revolutionize the transportation industry by enhancing traffic efficiency and road safety. However, achieving optimal vehicular autonomy demands an uninterrupted and precise positioning solution, especially in deep urban environments. 5G mmWave holds immense potential to provide such a service due to its accurate range and angle measurements. Yet, as mmWave signals are prone to signal blockage, severe positioning errors will occur. Most of the 5G positioning literature relies on constant motion models to bridge such 5G outages, which do not capture the true dynamics of the vehicle. Few proposed methodologies rely on inertial measurement units (IMUs) to bridge such gaps, where they predominantly use tightly coupled (TC) integration schemes, introducing a nonlinear 5G measurement model. Such approaches, which rely on Kalman filtering, necessitate the linearization of the measurement model, leading to pronounced positioning errors. In this paper, however, we propose a loosely coupled (LC) sensor fusion scheme to integrate 5G, IMUs, and odometers to mitigate linearization errors. Additionally, we propose a novel method to design the process covariance matrix of the extended Kalman filter (EKF). Moreover, we propose enhancements to the mechanization of the IMU data to enhance the standalone IMU solution. The proposed methodologies were tested using a novel setup comprising 5G measurements from Siradel's S_5G simulation tool and real IMU and odometer measurements from an hour-long trajectory. The proposed method resulted in 14 cm of error for 95% of the time compared to 1 m provided by the traditional constant velocity model approach.","sentences":["Autonomous vehicles (AVs) are poised to revolutionize the transportation industry by enhancing traffic efficiency and road safety.","However, achieving optimal vehicular autonomy demands an uninterrupted and precise positioning solution, especially in deep urban environments.","5G mmWave holds immense potential to provide such a service due to its accurate range and angle measurements.","Yet, as mmWave signals are prone to signal blockage, severe positioning errors will occur.","Most of the 5G positioning literature relies on constant motion models to bridge such 5G outages, which do not capture the true dynamics of the vehicle.","Few proposed methodologies rely on inertial measurement units (IMUs) to bridge such gaps, where they predominantly use tightly coupled (TC) integration schemes, introducing a nonlinear 5G measurement model.","Such approaches, which rely on Kalman filtering, necessitate the linearization of the measurement model, leading to pronounced positioning errors.","In this paper, however, we propose a loosely coupled (LC) sensor fusion scheme to integrate 5G, IMUs, and odometers to mitigate linearization errors.","Additionally, we propose a novel method to design the process covariance matrix of the extended Kalman filter (EKF).","Moreover, we propose enhancements to the mechanization of the IMU data to enhance the standalone IMU solution.","The proposed methodologies were tested using a novel setup comprising 5G measurements from Siradel's S_5G simulation tool and real IMU and odometer measurements from an hour-long trajectory.","The proposed method resulted in 14 cm of error for 95% of the time compared to 1 m provided by the traditional constant velocity model approach."],"url":"http://arxiv.org/abs/2403.10872v1","category":"eess.SP"}
{"created":"2024-03-16 08:30:45","title":"A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment","abstract":"While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.","sentences":["While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored.","In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA.","Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting).","We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems.","We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios.","Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly."],"url":"http://arxiv.org/abs/2403.10854v1","category":"cs.CV"}
{"created":"2024-03-18 12:16:48","title":"Combination and Reinterpretation of LHC SUSY Searches","abstract":"To maximise the information obtained from various independent new physics searches conducted at the LHC, it is imperative to consider the combination of multiple analyses. To showcase the exclusion power gained by combining signal regions from different searches, we consider a simplified scenario inspired by supersymmetry, with all particles but one squark flavour and a bino-like neutralino decoupled. The corresponding signal therefore comprises strong squark pair production, associated squark-neutralino production, as well as weak neutralino pair production. We find that considering the associated and strong production mechanisms together significantly impacts mass limits, while contributions from the weak production are insignificant in the context of current exclusion limits. In addition, we demonstrate that the combination of uncorrelated signal regions as assessed from the recent TACO approach substantially pushes exclusion limits towards higher masses, relative to the bounds derived from the most sensitive individual analyses.","sentences":["To maximise the information obtained from various independent new physics searches conducted at the LHC, it is imperative to consider the combination of multiple analyses.","To showcase the exclusion power gained by combining signal regions from different searches, we consider a simplified scenario inspired by supersymmetry, with all particles but one squark flavour and a bino-like neutralino decoupled.","The corresponding signal therefore comprises strong squark pair production, associated squark-neutralino production, as well as weak neutralino pair production.","We find that considering the associated and strong production mechanisms together significantly impacts mass limits, while contributions from the weak production are insignificant in the context of current exclusion limits.","In addition, we demonstrate that the combination of uncorrelated signal regions as assessed from the recent TACO approach substantially pushes exclusion limits towards higher masses, relative to the bounds derived from the most sensitive individual analyses."],"url":"http://arxiv.org/abs/2403.11715v1","category":"hep-ph"}
{"created":"2024-03-18 11:59:07","title":"Gravitational Form Factors and Mechanical Properties of Quarks in Protons: A Basis Light-Front Quantization Approach","abstract":"We compute the gravitational form factors (GFFs) and study their applications for the description of the mechanical properties such as the pressure, shear force distributions, and the mechanical radius of the proton from its light-front wave functions (LFWFs) based on basis light-front quantization (BLFQ). The LFWFs of the proton are given by the lowest eigenvector of a light-front effective Hamiltonian that incorporates a three-dimensional confining potential and a one-gluon exchange interaction with fixed coupling between the constituent quarks solved in the valence Fock sector. We find acceptable agreement between our BLFQ computations and the lattice QCD for the GFFs. Our $D$-term form factor also agrees well with the extracted data from the deeply virtual Compton scattering experiments at Jefferson Lab, and the results of different phenomenological models. The distributions of pressures and shear forces are similar to those from different models.","sentences":["We compute the gravitational form factors (GFFs) and study their applications for the description of the mechanical properties such as the pressure, shear force distributions, and the mechanical radius of the proton from its light-front wave functions (LFWFs) based on basis light-front quantization (BLFQ).","The LFWFs of the proton are given by the lowest eigenvector of a light-front effective Hamiltonian that incorporates a three-dimensional confining potential and a one-gluon exchange interaction with fixed coupling between the constituent quarks solved in the valence Fock sector.","We find acceptable agreement between our BLFQ computations and the lattice QCD for the GFFs.","Our $D$-term form factor also agrees well with the extracted data from the deeply virtual Compton scattering experiments at Jefferson Lab, and the results of different phenomenological models.","The distributions of pressures and shear forces are similar to those from different models."],"url":"http://arxiv.org/abs/2403.11702v1","category":"hep-ph"}
{"created":"2024-03-18 11:24:46","title":"Narrow absorption lines from intervening material in supernovae I. Measurements and temporal evolution","abstract":"Narrow absorption features in nearby supernova (SN) spectra are a powerful diagnostic of the slow-moving material in the line of sight: they are extensively used to infer dust extinction from the host galaxies, and they can also serve in the detection of circumstellar material originating from the SN progenitor and present in the vicinity of the explosion. Despite their wide use, very few studies have examined the biases of the methods to characterize narrow lines, and not many statistical analyses exist. This is the first paper of a series in which we present a statistical analysis of narrow lines of SN spectra of various resolutions. We develop a robust automated methodology to measure the equivalent width (EW) and velocity of narrow absorption lines from intervening material in the line of sight of SNe, including Na I D , Ca II H&K, K i and diffuse interstellar bands (DIBs). We carefully study systematic biases in heterogeneous spectra from the literature by simulating different signal-to-noise, spectral resolution, slit size and orientation and present the real capabilities and limitations of using low- and mid-resolution spectra to study these lines. In particular, we find that the measurement of the equivalent width of the narrow lines in low-resolution spectra is highly affected by the evolving broad P-Cygni profiles of the SN ejecta, both for core-collapse and type Ia SNe, inducing a conspicuous apparent evolution. We present thus an easy way to detect and exclude those cases to obtain more robust and reliable measurements. Finally, after considering all possible effects, we analyse the temporal evolution of the narrow features in a large sample of nearby SNe to detect any possible variation in their EWs over time. We find no time evolution of the narrow line features in our large sample for all SN types","sentences":["Narrow absorption features in nearby supernova (SN) spectra are a powerful diagnostic of the slow-moving material in the line of sight: they are extensively used to infer dust extinction from the host galaxies, and they can also serve in the detection of circumstellar material originating from the SN progenitor and present in the vicinity of the explosion.","Despite their wide use, very few studies have examined the biases of the methods to characterize narrow lines, and not many statistical analyses exist.","This is the first paper of a series in which we present a statistical analysis of narrow lines of SN spectra of various resolutions.","We develop a robust automated methodology to measure the equivalent width (EW) and velocity of narrow absorption lines from intervening material in the line of sight of SNe, including Na I D , Ca II H&K, K i and diffuse interstellar bands (DIBs).","We carefully study systematic biases in heterogeneous spectra from the literature by simulating different signal-to-noise, spectral resolution, slit size and orientation and present the real capabilities and limitations of using low- and mid-resolution spectra to study these lines.","In particular, we find that the measurement of the equivalent width of the narrow lines in low-resolution spectra is highly affected by the evolving broad P-Cygni profiles of the SN ejecta, both for core-collapse and type Ia SNe, inducing a conspicuous apparent evolution.","We present thus an easy way to detect and exclude those cases to obtain more robust and reliable measurements.","Finally, after considering all possible effects, we analyse the temporal evolution of the narrow features in a large sample of nearby SNe to detect any possible variation in their EWs over time.","We find no time evolution of the narrow line features in our large sample for all SN types"],"url":"http://arxiv.org/abs/2403.11677v1","category":"astro-ph.IM"}
{"created":"2024-03-18 10:50:19","title":"SU(3) gauge field of magnons in antiferromagnetic skyrmion crystals","abstract":"Quasiparticle excitations in material solids often experience a fictitious gauge field, which can be a potential source of intriguing transport phenomena. Here, we show that low-energy excitations in insulating antiferromagnetic skyrmion crystals on the triangular lattice are effectively described by magnons with an SU(3) gauge field. The three-sublattice structure in the antiferromagnetic skyrmion crystals is inherited as three internal degrees of freedom for the magnons, which are coupled with their kinetic motion via the SU(3) gauge field that arises from the topologically nontrivial spin texture in real space. We also demonstrate that the non-commutativity of the SU(3) gauge field breaks an effective time-reversal symmetry and contributes to a magnon thermal Hall effect.","sentences":["Quasiparticle excitations in material solids often experience a fictitious gauge field, which can be a potential source of intriguing transport phenomena.","Here, we show that low-energy excitations in insulating antiferromagnetic skyrmion crystals on the triangular lattice are effectively described by magnons with an SU(3) gauge field.","The three-sublattice structure in the antiferromagnetic skyrmion crystals is inherited as three internal degrees of freedom for the magnons, which are coupled with their kinetic motion via the SU(3) gauge field that arises from the topologically nontrivial spin texture in real space.","We also demonstrate that the non-commutativity of the SU(3) gauge field breaks an effective time-reversal symmetry and contributes to a magnon thermal Hall effect."],"url":"http://arxiv.org/abs/2403.11655v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 09:55:01","title":"Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model","abstract":"Large Language Models (LLMs) are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as models scale. Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in model pruning and training efficiency. Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary. In contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling more precise and computationally efficient model updates. The experimental results show that NeFT not only exceeded the performance of full-parameter fine-tuning and PEFT but also provided insights into the analysis of neurons.","sentences":["Large Language Models (LLMs) are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as models scale.","Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in model pruning and training efficiency.","Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary.","In contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level).","We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling more precise and computationally efficient model updates.","The experimental results show that NeFT not only exceeded the performance of full-parameter fine-tuning and PEFT but also provided insights into the analysis of neurons."],"url":"http://arxiv.org/abs/2403.11621v1","category":"cs.CL"}
{"created":"2024-03-18 09:17:32","title":"Rare Charm Decays at BESIII","abstract":"The rare and forbidden processes within the Standard Model offer an opportunity to explore potential new physics beyond the SM. We summarize the research method and the recent results of rare charm decays at BESIII based on the extensive data samples in the $\\tau-c$ energy region, many of which impose stringent constraints on the new physics.","sentences":["The rare and forbidden processes within the Standard Model offer an opportunity to explore potential new physics beyond the SM.","We summarize the research method and the recent results of rare charm decays at BESIII based on the extensive data samples in the $\\tau-c$ energy region, many of which impose stringent constraints on the new physics."],"url":"http://arxiv.org/abs/2403.11597v1","category":"hep-ex"}
{"created":"2024-03-18 09:01:28","title":"Numerical observation of $\\mathrm{SU}(N)$ Nagaoka ferromagnetism","abstract":"We provide numerical evidence of the Nagaoka's theorem in the $\\mathrm{SU}(N)$ Fermi-Hubbard model on various cluster geometries, such as the square, the honeycomb and the triangular lattices. In particular, by diagonalizing several finite-size clusters, we show that for one hole away from filling $1/N$, the itinerant ferromagnetism arises for $U$ (the positive on-site interaction) larger than $U_c$ (the value at the transition), which strongly depends on the coordination number $z$ and on $N$, the number of degenerate orbitals, that we vary from $N=2$ to $N=6$ in our simulations. We prove that $U_c$ is a non decreasing function of $N$. In addition, we find that the lattice dependency is rooted in the kinetic energy of the hole. We find that large coordination numbers $z$ lower the value of $U_c$. Complementary, we explore the effect of long-range hopping on the appearance of itinerant ferromagnetism and demonstrate that it acts as an increased coordination number, protecting the ferromagnetic phase at small $U$. Finally, both the effects of the presence of some additional holes and of the finite size of the clusters are briefly discussed.","sentences":["We provide numerical evidence of the Nagaoka's theorem in the $\\mathrm{SU}(N)$ Fermi-Hubbard model on various cluster geometries, such as the square, the honeycomb and the triangular lattices.","In particular, by diagonalizing several finite-size clusters, we show that for one hole away from filling $1/N$, the itinerant ferromagnetism arises for $U$ (the positive on-site interaction) larger than $U_c$ (the value at the transition), which strongly depends on the coordination number $z$ and on $N$, the number of degenerate orbitals, that we vary from $N=2$ to $N=6$ in our simulations.","We prove that $U_c$ is a non decreasing function of $N$. In addition, we find that the lattice dependency is rooted in the kinetic energy of the hole.","We find that large coordination numbers $z$ lower the value of $U_c$. Complementary, we explore the effect of long-range hopping on the appearance of itinerant ferromagnetism and demonstrate that it acts as an increased coordination number, protecting the ferromagnetic phase at small $U$. Finally, both the effects of the presence of some additional holes and of the finite size of the clusters are briefly discussed."],"url":"http://arxiv.org/abs/2403.11588v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 08:55:37","title":"Giant CP violation in charmless three-body $B$ meson decays at LHCb: all order formalism for meson-meson final state interactions","abstract":"LHCb has observed giant CP violation in localized regions of the Dalitz plots of B to three charmless light mesons. This has been interpreted as an enhancement due to strong two-body final state interactions. In this talk, we show how such interactions, described with dispersive analyses of data, can be implemented beyond the leading order expansion in the two-body re-scattering amplitude.","sentences":["LHCb has observed giant CP violation in localized regions of the Dalitz plots of B to three charmless light mesons.","This has been interpreted as an enhancement due to strong two-body final state interactions.","In this talk, we show how such interactions, described with dispersive analyses of data, can be implemented beyond the leading order expansion in the two-body re-scattering amplitude."],"url":"http://arxiv.org/abs/2403.11581v1","category":"hep-ph"}
{"created":"2024-03-18 08:54:21","title":"Fractional Dimensional Approach to Dielectric Tuning Effects on Excitonic Parameters in 2D semiconductor materials","abstract":"We demonstrated the potential of the fractional dimensional approach to understand exciton parameters in the exemplary atomically thin semiconductor material, a monolayer of WS$_2$. This approach has proved to be successful in finding the exciton binding energy and quasiparticle bandgap for the WS$_2$ monolayer in varying dielectric environments. A tuning of the quasiparticle bandgap and binding energy by 141 meV and 188 meV, respectively, has been achieved by varying the dielectric of the environment from 1.52 to 8.1. The approach is justified by comparing the changes in the binding energy with the computational results from the Quantum Electrostatic Heterostructures model. The fractional dimension found through the excitonic Rydberg series is close to 2.8 for WS$_2$ monolayer in all different dielectric surroundings. Thus, this approach provides a rapid and robust method for determining the binding energy of excitons in 2D semiconductors independent of the particular dielectric environment.","sentences":["We demonstrated the potential of the fractional dimensional approach to understand exciton parameters in the exemplary atomically thin semiconductor material, a monolayer of WS$_2$. This approach has proved to be successful in finding the exciton binding energy and quasiparticle bandgap for the WS$_2$ monolayer in varying dielectric environments.","A tuning of the quasiparticle bandgap and binding energy by 141 meV and 188 meV, respectively, has been achieved by varying the dielectric of the environment from 1.52 to 8.1.","The approach is justified by comparing the changes in the binding energy with the computational results from the Quantum Electrostatic Heterostructures model.","The fractional dimension found through the excitonic Rydberg series is close to 2.8 for WS$_2$ monolayer in all different dielectric surroundings.","Thus, this approach provides a rapid and robust method for determining the binding energy of excitons in 2D semiconductors independent of the particular dielectric environment."],"url":"http://arxiv.org/abs/2403.11579v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 08:41:34","title":"The effect of Coulomb assisted hopping on STM signal: extended two site Hubbard model analysis","abstract":"In this work we study STM signal in the presence of Coulomb assisted hopping. We perform an extended two site Hubbard model analysis between the atom on the tip and the atom in the sample nearest to each other. We show that in the presence of Coulomb assisted hopping the STM signal depends on several spectral functions thereby complicating its interpretation. Furthermore in the broadband tip limit there are now three different competing rates for the total current (instead of one for the usual two site Hubbard model analysis). We find an exact (within the Fermi golden rule - that is in the limit of weak coupling between tip and sample) an expression for the current as a function of the bias voltage. As an example we apply our calculations to the case of free fermions with a uniform density of states. Even in this simple case there are non-trivial corrections where the dI/dV (the rate of change of the current with respect to bias voltage) is not uniform as a two site (non-extended) Hubbard model analysis would predict.","sentences":["In this work we study STM signal in the presence of Coulomb assisted hopping.","We perform an extended two site Hubbard model analysis between the atom on the tip and the atom in the sample nearest to each other.","We show that in the presence of Coulomb assisted hopping the STM signal depends on several spectral functions thereby complicating its interpretation.","Furthermore in the broadband tip limit there are now three different competing rates for the total current (instead of one for the usual two site Hubbard model analysis).","We find an exact (within the Fermi golden rule - that is in the limit of weak coupling between tip and sample) an expression for the current as a function of the bias voltage.","As an example we apply our calculations to the case of free fermions with a uniform density of states.","Even in this simple case there are non-trivial corrections where the dI/dV (the rate of change of the current with respect to bias voltage) is not uniform as a two site (non-extended) Hubbard model analysis would predict."],"url":"http://arxiv.org/abs/2403.11566v1","category":"cond-mat.str-el"}
{"created":"2024-03-18 07:52:35","title":"Giant graviton expansions for line operator index","abstract":"We discuss giant graviton expansions for the Schur index of ${\\cal N}=4$ $U(N)$ SYM with the insertion of Wilson lines of the fundamental and the anti-fundamental representations. We first propose a double-sum giant graviton expansion and numerically confirm that it correctly reproduces the line-operator index. We also find that it reduces to a simple-sum expansion when we treat the index as a Taylor series with respect to a specific fugacity.","sentences":["We discuss giant graviton expansions for the Schur index of ${\\cal N}=4$ $U(N)$ SYM with the insertion of Wilson lines of the fundamental and the anti-fundamental representations.","We first propose a double-sum giant graviton expansion and numerically confirm that it correctly reproduces the line-operator index.","We also find that it reduces to a simple-sum expansion when we treat the index as a Taylor series with respect to a specific fugacity."],"url":"http://arxiv.org/abs/2403.11543v1","category":"hep-th"}
{"created":"2024-03-18 07:11:04","title":"Optical manipulation of the topological phase in ZrTe5 revealed by time- and angle-resolved photoemission","abstract":"High-resolution time- and angle-resolved photoemission measurements were conducted on the topological insulator ZrTe5. With strong femtosecond photoexcitation, a possible ultrafast phase transition from a weak to a strong topological insulating phase was experimentally realized by recovering the energy gap inversion in a time scale that was shorter than 0.15 ps. This photoinduced transient strong topological phase can last longer than 2 ps at the highest excitation fluence studied, and it cannot be attributed to the photoinduced heating of electrons or modification of the conduction band filling. Additionally, the measured unoccupied electronic states are consistent with the first-principles calculation based on experimental crystal lattice constants, which favor a strong topological insulating phase. These findings provide new insights into the longstanding controversy about the strong and weak topological properties in ZrTe5, and they suggest that many-body effects including electron-electron interactions must be taken into account to understand the equilibrium weak topological insulating phase in ZrTe5.","sentences":["High-resolution time- and angle-resolved photoemission measurements were conducted on the topological insulator ZrTe5.","With strong femtosecond photoexcitation, a possible ultrafast phase transition from a weak to a strong topological insulating phase was experimentally realized by recovering the energy gap inversion in a time scale that was shorter than 0.15 ps.","This photoinduced transient strong topological phase can last longer than 2 ps at the highest excitation fluence studied, and it cannot be attributed to the photoinduced heating of electrons or modification of the conduction band filling.","Additionally, the measured unoccupied electronic states are consistent with the first-principles calculation based on experimental crystal lattice constants, which favor a strong topological insulating phase.","These findings provide new insights into the longstanding controversy about the strong and weak topological properties in ZrTe5, and they suggest that many-body effects including electron-electron interactions must be taken into account to understand the equilibrium weak topological insulating phase in ZrTe5."],"url":"http://arxiv.org/abs/2403.11518v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 06:07:46","title":"Resolving the $H_0$ and $S_8$ tensions with neutrino mass and chemical potential","abstract":"A simple and natural extension of the standard $\\Lambda$CDM model is to allow relic neutrinos to have non-zero degeneracy. We confront this $\\Lambda$CDM$\\xi$ model, $\\Lambda$CDM with neutrino mass $M_\\nu$ and degeneracy $\\xi_3$ as additional parameters, with the \\textit{Planck} TT, lowT, plik--lensing, BAO, and DES datasets, and we observe a strong preference (Bayes factor $\\log_{10}B=1.9$) for it over the standard $\\Lambda$CDM model. Both the $H_0$ and $S_8$ tensions are resolved to within 1$\\sigma$ with the same set of neutrino parameters, along with 3$\\sigma$ evidence for nonzero neutrino mass ($M_\\nu=0.58^{+0.17}_{-0.13}\\ \\mathrm{eV}$) and degeneracy ($\\xi_3=1.27^{+0.42}_{-0.22}$). Furthermore, our analysis favors the scalar index $n_s$ to be slightly larger than 1, compatible with some hybrid inflation models, as well as a significantly larger optical depth $\\tau$ than the standard Planck value, indicating an earlier onset of reionization.","sentences":["A simple and natural extension of the standard $\\Lambda$CDM model is to allow relic neutrinos to have non-zero degeneracy.","We confront this $\\Lambda$CDM$\\xi$ model, $\\Lambda$CDM with neutrino mass $M_\\nu$ and degeneracy $\\xi_3$ as additional parameters, with the \\textit{Planck} TT, lowT, plik--lensing, BAO, and DES datasets, and we observe a strong preference (Bayes factor $\\log_{10}B=1.9$) for it over the standard $\\Lambda$CDM model.","Both the $H_0$ and $S_8$ tensions are resolved to within 1$\\sigma$ with the same set of neutrino parameters, along with 3$\\sigma$ evidence for nonzero neutrino mass ($M_\\nu=0.58^{+0.17}_{-0.13}\\ \\mathrm{eV}$) and degeneracy ($\\xi_3=1.27^{+0.42}_{-0.22}$).","Furthermore, our analysis favors the scalar index $n_s$ to be slightly larger than 1, compatible with some hybrid inflation models, as well as a significantly larger optical depth $\\tau$ than the standard Planck value, indicating an earlier onset of reionization."],"url":"http://arxiv.org/abs/2403.11499v1","category":"hep-ph"}
{"created":"2024-03-18 02:09:58","title":"Surface region band enhancement in noble gas adsorption assisted ARPES on kagome superconductor RbV3Sb5","abstract":"Electronic states near surface regions can be distinct from bulk states, which are paramount in understanding various physical phenomena occurring at surfaces and in applications in semiconductors, energy, and catalysis. Here, we report an abnormal surface region band enhancement effect in angle-resolved photoemission spectroscopy on kagome superconductor RbV3Sb5, by depositing noble gases with fine control. In contrast to conventional surface contamination, the intensity of surface region Sb band can be enhanced more than three times with noble gas adsorption. In the meantime, a hole-dope effect is observed for the enhanced surface region band, with other bands hardly changing. The doping effect is more pronounced with heavier noble gases. We propose that noble gas atoms selectively fill into alkali metal vacancy sites on the surface, which improves the surface condition, boosts surface region bands, and effectively dopes it with the Pauli repulsion mechanism. Our results provide a novel and reversible way to improve surface conditions and tune surface region bands by controlled surface noble gas deposition.","sentences":["Electronic states near surface regions can be distinct from bulk states, which are paramount in understanding various physical phenomena occurring at surfaces and in applications in semiconductors, energy, and catalysis.","Here, we report an abnormal surface region band enhancement effect in angle-resolved photoemission spectroscopy on kagome superconductor RbV3Sb5, by depositing noble gases with fine control.","In contrast to conventional surface contamination, the intensity of surface region Sb band can be enhanced more than three times with noble gas adsorption.","In the meantime, a hole-dope effect is observed for the enhanced surface region band, with other bands hardly changing.","The doping effect is more pronounced with heavier noble gases.","We propose that noble gas atoms selectively fill into alkali metal vacancy sites on the surface, which improves the surface condition, boosts surface region bands, and effectively dopes it with the Pauli repulsion mechanism.","Our results provide a novel and reversible way to improve surface conditions and tune surface region bands by controlled surface noble gas deposition."],"url":"http://arxiv.org/abs/2403.11416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 01:18:34","title":"Spatially Randomized Designs Can Enhance Policy Evaluation","abstract":"This article studies the benefits of using spatially randomized experimental designs which partition the experimental area into distinct, non-overlapping units with treatments assigned randomly. Such designs offer improved policy evaluation in online experiments by providing more precise policy value estimators and more effective A/B testing algorithms than traditional global designs, which apply the same treatment across all units simultaneously. We examine both parametric and nonparametric methods for estimating and inferring policy values based on this randomized approach. Our analysis includes evaluating the mean squared error of the treatment effect estimator and the statistical power of the associated tests. Additionally, we extend our findings to experiments with spatio-temporal dependencies, where treatments are allocated sequentially over time, and account for potential temporal carryover effects. Our theoretical insights are supported by comprehensive numerical experiments.","sentences":["This article studies the benefits of using spatially randomized experimental designs which partition the experimental area into distinct, non-overlapping units with treatments assigned randomly.","Such designs offer improved policy evaluation in online experiments by providing more precise policy value estimators and more effective A/B testing algorithms than traditional global designs, which apply the same treatment across all units simultaneously.","We examine both parametric and nonparametric methods for estimating and inferring policy values based on this randomized approach.","Our analysis includes evaluating the mean squared error of the treatment effect estimator and the statistical power of the associated tests.","Additionally, we extend our findings to experiments with spatio-temporal dependencies, where treatments are allocated sequentially over time, and account for potential temporal carryover effects.","Our theoretical insights are supported by comprehensive numerical experiments."],"url":"http://arxiv.org/abs/2403.11400v1","category":"math.ST"}
{"created":"2024-03-18 00:29:20","title":"Gamma-Ray Burst Pulses and Lateral Jet Motion","abstract":"We propose that gamma-ray burst pulses are produced when highly-relativistic jets sweep across an observer's line-of-sight. We hypothesize that axisymmetric jet profiles, coupled with special relativistic effects, produce the time-reversed properties of gamma-ray burst pulses. Curvature resulting from rapid jet expansion is responsible for much of the observed pulse asymmetry and hard-to-soft evolution. The relative obliqueness with which the jet crosses the line-of-sight explains the known GRB pulse morphological types. We explore two scenarios: one in which a rigid/semi-rigid jet moves laterally, and the other in which a ballistic jet sprays material from a laterally-moving nozzle. The ballistic jet model is favored based upon its consistency with standard emission mechanisms.","sentences":["We propose that gamma-ray burst pulses are produced when highly-relativistic jets sweep across an observer's line-of-sight.","We hypothesize that axisymmetric jet profiles, coupled with special relativistic effects, produce the time-reversed properties of gamma-ray burst pulses.","Curvature resulting from rapid jet expansion is responsible for much of the observed pulse asymmetry and hard-to-soft evolution.","The relative obliqueness with which the jet crosses the line-of-sight explains the known GRB pulse morphological types.","We explore two scenarios: one in which a rigid/semi-rigid jet moves laterally, and the other in which a ballistic jet sprays material from a laterally-moving nozzle.","The ballistic jet model is favored based upon its consistency with standard emission mechanisms."],"url":"http://arxiv.org/abs/2403.11387v1","category":"astro-ph.HE"}
{"created":"2024-03-18 00:15:31","title":"Exclusive interplay between topological quasiparticles and strongly correlated fermions","abstract":"The low-energy excitations of topological quantum matter have been increasingly comprehended. However, the impact of these excitations on strongly correlated fermions remains rarely explored. Here, we report the discovery of an incompatible dynamic of 4$f$ quasiparticle and topological quasiparticle. The former forms through screening processes, while the latter suppresses the screening channel. By employing ab initio many-body perturbation theory combined with dynamical mean field theory, we show that this effect prompts two types of topological semimetals. The type-I semimetal PrPtBi exhibits a topological quasiparticle away from the Fermi level due to the formation of a 4$f$ quasiparticle. The type-II semimetals HoPtBi and PrAlGe feature a robust topological quasiparticle at the Fermi level, which pushes the 4$f$ quasiparticle away. Our work provides an avenue to harness semimetals' anomalous quantum effects from topological quasiparticle to heavy-fermion behavior by the incorporation of 4$f$ quasiparticles.","sentences":["The low-energy excitations of topological quantum matter have been increasingly comprehended.","However, the impact of these excitations on strongly correlated fermions remains rarely explored.","Here, we report the discovery of an incompatible dynamic of 4$f$ quasiparticle and topological quasiparticle.","The former forms through screening processes, while the latter suppresses the screening channel.","By employing ab initio many-body perturbation theory combined with dynamical mean field theory, we show that this effect prompts two types of topological semimetals.","The type-I semimetal PrPtBi exhibits a topological quasiparticle away from the Fermi level due to the formation of a 4$f$ quasiparticle.","The type-II semimetals HoPtBi and PrAlGe feature a robust topological quasiparticle at the Fermi level, which pushes the 4$f$ quasiparticle away.","Our work provides an avenue to harness semimetals' anomalous quantum effects from topological quasiparticle to heavy-fermion behavior by the incorporation of 4$f$ quasiparticles."],"url":"http://arxiv.org/abs/2403.11382v1","category":"cond-mat.str-el"}
{"created":"2024-03-17 20:57:06","title":"Resistive and ballistic phonon transport in $\u03b2$-Ga$_2$O$_3$","abstract":"The anisotropic thermal conductivity and the phonon mean free path (mfp) in monoclinic $\\beta$-Ga$_2$O$_3$ single crystals and homoepitaxial films of several $\\mu$m were determined using the 3$\\omega$-method in the temperature range from 10K-300 K. The measured effective thermal conductivity of both, single crystal and homoepitaxial films are in the order of 20 W/(mK) at room temperature, below 30 K it increases with a maximum of 1000 to 2000 W/(mK) and decreases with T$^3$ below 25 K. Analysis of the phonon mfp shows a dominance of phonon-phonon-Umklapp scattering above 80 K, below which the influence of point-defect scattering is observed. Below 30 K the phonon mfp increases until it is limited by the total $\\beta$-Ga$_2$O$_3$ sample size. A crossover from resistive to ballistic phonon transport is observed below 20 K and boundary effects of the total sample size become dominant. This reveals that the homoepitaxial film-substrate interface is highly phonon-transparent. The resistive and ballistic phonon transport regimes in $\\beta$-Ga$_2$O$_3$ are discussed corresponding to the models of Callaway and Majumdar, respectively.","sentences":["The anisotropic thermal conductivity and the phonon mean free path (mfp) in monoclinic $\\beta$-Ga$_2$O$_3$ single crystals and homoepitaxial films of several $\\mu$m were determined using the 3$\\omega$-method in the temperature range from 10K-300 K. The measured effective thermal conductivity of both, single crystal and homoepitaxial films are in the order of 20 W/(mK) at room temperature, below 30 K it increases with a maximum of 1000 to 2000 W/(mK) and decreases with T$^3$ below 25 K. Analysis of the phonon mfp shows a dominance of phonon-phonon-Umklapp scattering above 80 K, below which the influence of point-defect scattering is observed.","Below 30 K the phonon mfp increases until it is limited by the total $\\beta$-Ga$_2$O$_3$ sample size.","A crossover from resistive to ballistic phonon transport is observed below 20 K and boundary effects of the total sample size become dominant.","This reveals that the homoepitaxial film-substrate interface is highly phonon-transparent.","The resistive and ballistic phonon transport regimes in $\\beta$-Ga$_2$O$_3$ are discussed corresponding to the models of Callaway and Majumdar, respectively."],"url":"http://arxiv.org/abs/2403.11341v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-17 19:50:16","title":"SDSS~J075217.84+193542.2: X-ray weighing of a secondary BH","abstract":"Precise measurements of black hole (BHs) masses are necessary to understand the coevolution of these sources and their host galaxies. Sometimes in the center of a galaxy there is not one, but two BHs. The BH duality of the quasar nucleus SDSS~J075217.84+193542.2 (herein SDSS~J0752) was recently proposed based on the observed strict periodicity of optical emission from the source. We tested this assumption using X-ray observations with Swift/XRT (2008--2010). We fitted the SDSS~J075217 spectrum using a Comptonization model and discovered soft X-ray variability in the 0.3--10~keV energy range. We pursued a scenario in which two supermassive BHs at the center of SDSS~J0752 form a pair; and the less massive (secondary) BH periodically crosses/punctures the disk around the more massive (primary) BH. We associate these periodic crossings with tidal disruptions of the disk and, as a consequence, with an increase in X-rays seen as a flare in SDSS~J0752. During such an X-ray flare event (2008--2010), we discovered a change in the source spectral states and the photon index saturation at the $\\Gamma\\sim3$ level with mass accretion rate $\\dot M$. For BH mass scaling we used sources: OJ~287, M101 ULX--1 and HLX--1 ESO~243--49, as a reference ones, and found that M$_{SDSS}=9\\times 10^7$ solar masses, assuming $d_{SDSS}= 500$ Mpc. Thus, we obtained a lower limit to a BH mass due the unknown inclination. In addition, we used the virial mass of the secondary BH based on $H_\\alpha$-line measurements and we estimated the binary's inclination at SDSS~J0752, $i=80^{\\circ}$, using a scaling technique.","sentences":["Precise measurements of black hole (BHs) masses are necessary to understand the coevolution of these sources and their host galaxies.","Sometimes in the center of a galaxy there is not one, but two BHs.","The BH duality of the quasar nucleus SDSS~J075217.84+193542.2 (herein SDSS~J0752) was recently proposed based on the observed strict periodicity of optical emission from the source.","We tested this assumption using X-ray observations with Swift/XRT (2008--2010).","We fitted the SDSS~J075217 spectrum using a Comptonization model and discovered soft X-ray variability in the 0.3--10~keV energy range.","We pursued a scenario in which two supermassive BHs at the center of SDSS~J0752 form a pair; and the less massive (secondary) BH periodically crosses/punctures the disk around the more massive (primary) BH.","We associate these periodic crossings with tidal disruptions of the disk and, as a consequence, with an increase in X-rays seen as a flare in SDSS~J0752.","During such an X-ray flare event (2008--2010), we discovered a change in the source spectral states and the photon index saturation at the $\\Gamma\\sim3$ level with mass accretion rate $\\dot M$. For BH mass scaling we used sources: OJ~287, M101 ULX--1 and HLX--1 ESO~243--49, as a reference ones, and found that M$_{SDSS}=9\\times 10^7$ solar masses, assuming $d_{SDSS}= 500$ Mpc.","Thus, we obtained a lower limit to a BH mass due the unknown inclination.","In addition, we used the virial mass of the secondary BH based on $H_\\alpha$-line measurements and we estimated the binary's inclination at SDSS~J0752, $i=80^{\\circ}$, using a scaling technique."],"url":"http://arxiv.org/abs/2403.11319v1","category":"astro-ph.HE"}
{"created":"2024-03-17 17:46:34","title":"On the Ginzburg-Landau Energy of Corners","abstract":"It is a well known fact that the geometry of a superconducting sample influences the distribution of the surface superconductivity for strong applied magnetic fields. For instance, the presence of corners induces geometric terms described through effective models in sector-like regions. We study the connection between two effective models for the offset of superconductivity and for surface superconductivity introduced in \\cite{BNF} and \\cite{CG2}, respectively. We prove that the transition between the two models is continuous with respect to the magnetic field strength, and, as a byproduct, we deduce the existence of a minimizer at the threshold for both effective problems. Furthermore, as a consequence, we disprove a conjecture stated in \\cite{CG2} concerning the dependence of the corner energy on the angle close to the threshold.","sentences":["It is a well known fact that the geometry of a superconducting sample influences the distribution of the surface superconductivity for strong applied magnetic fields.","For instance, the presence of corners induces geometric terms described through effective models in sector-like regions.","We study the connection between two effective models for the offset of superconductivity and for surface superconductivity introduced in \\cite{BNF} and \\cite{CG2}, respectively.","We prove that the transition between the two models is continuous with respect to the magnetic field strength, and, as a byproduct, we deduce the existence of a minimizer at the threshold for both effective problems.","Furthermore, as a consequence, we disprove a conjecture stated in \\cite{CG2} concerning the dependence of the corner energy on the angle close to the threshold."],"url":"http://arxiv.org/abs/2403.11286v1","category":"math-ph"}
{"created":"2024-03-17 16:21:42","title":"Next-to-Leading-Order Weak Annihilation Correction to Rare $B \\to \\left \\{K, \u03c0\\right \\} \\ell^{+} \\ell^{-}$ Decays","abstract":"We accomplish for the first time the next-to-leading-order computation of the weak annihilation contribution to the exclusive electroweak penguin decays $B \\to \\left \\{K, \\pi \\right \\} \\ell^{+} \\ell^{-}$ with an energetic light-flavour meson, which is an essential missing piece of the complete QCD correction to the matrix elements of hadronic operators in the weak effective Hamiltonian. Both the hard functions and the jet functions in the perturbative factorization formulae from the short-distance fluctuations at the two distinct scales $m_b$ and $\\sqrt{m_b \\, \\Lambda}$ are determined at ${\\cal O}(\\alpha_s)$ with the soft-collinear factorization technique. We then demonstrate that the one-loop weak annihilation correction can bring about the noticeable impacts on theory predictions for the CP asymmetries and the isospin asymmetry in the $B \\to \\pi \\ell^{+} \\ell^{-}$ decays.","sentences":["We accomplish for the first time the next-to-leading-order computation of the weak annihilation contribution to the exclusive electroweak penguin decays $B \\to \\left \\{K, \\pi \\right \\} \\ell^{+} \\ell^{-}$ with an energetic light-flavour meson, which is an essential missing piece of the complete QCD correction to the matrix elements of hadronic operators in the weak effective Hamiltonian.","Both the hard functions and the jet functions in the perturbative factorization formulae from the short-distance fluctuations at the two distinct scales $m_b$ and $\\sqrt{m_b \\, \\Lambda}$ are determined at ${\\cal O}(\\alpha_s)$ with the soft-collinear factorization technique.","We then demonstrate that the one-loop weak annihilation correction can bring about the noticeable impacts on theory predictions for the CP asymmetries and the isospin asymmetry in the $B \\to \\pi \\ell^{+} \\ell^{-}$ decays."],"url":"http://arxiv.org/abs/2403.11258v1","category":"hep-ph"}
{"created":"2024-03-17 11:32:45","title":"$T_7$ Flavor Symmetry gym: The Key to Unlocking the Neutrino Mass Puzzle","abstract":"Recent research has indicated that the Standard Model (SM), while historically highly effective, is found to be insufficient due to its prediction of zero mass for neutrinos. With the exception of a few, the majority of the parameters related to neutrinos have been determined by neutrino oscillation experiments with excellent precision. Experiments on neutrino oscillation and neutrino mixing have shown that neutrinos are massive. To fill in gaps, discrete symmetries are becoming more common alongside continuous symmetries while describing the observed pattern of neutrino mixing. Here, we present a $T_7$ flavor symmetry to explain the masses of charged leptons and neutrinos. The light neutrino mass matrix is derived using seesaw mechanism of type I, which involves the Dirac neutrino mass matrix as well as the right-handed neutrino mass matrix. We estimate the Pontecorvo-Maki-Nakagawa-Sakata matrix ($U_{PMNS}$), three mixing angles, $\\theta_{12}$, $\\theta_{23}$ and $\\theta_{13}$, which are strongly correlated with the recent experimental results. The extent of $CP$ violation in neutrino oscillations is obtained by calculating Jarskog invariant $(J_{CP})$ on the behalf of $U_{PMNS}$. We also find the masses of three neutrinos and Effective Majorana neutrino mass parameter $\\langle m_{ee} \\rangle$ which is $1.0960244138965946$ $meV$ and $10.92168920244218$ $meV$ for normal and inverted hierarchy, respectively.","sentences":["Recent research has indicated that the Standard Model (SM), while historically highly effective, is found to be insufficient due to its prediction of zero mass for neutrinos.","With the exception of a few, the majority of the parameters related to neutrinos have been determined by neutrino oscillation experiments with excellent precision.","Experiments on neutrino oscillation and neutrino mixing have shown that neutrinos are massive.","To fill in gaps, discrete symmetries are becoming more common alongside continuous symmetries while describing the observed pattern of neutrino mixing.","Here, we present a $T_7$ flavor symmetry to explain the masses of charged leptons and neutrinos.","The light neutrino mass matrix is derived using seesaw mechanism of type I, which involves the Dirac neutrino mass matrix as well as the right-handed neutrino mass matrix.","We estimate the Pontecorvo-Maki-Nakagawa-Sakata matrix ($U_{PMNS}$), three mixing angles, $\\theta_{12}$, $\\theta_{23}$ and $\\theta_{13}$, which are strongly correlated with the recent experimental results.","The extent of $CP$ violation in neutrino oscillations is obtained by calculating Jarskog invariant $(J_{CP})$ on the behalf of $U_{PMNS}$. We also find the masses of three neutrinos and Effective Majorana neutrino mass parameter $\\langle m_{ee} \\rangle$ which is $1.0960244138965946$ $meV$ and $10.92168920244218$ $meV$ for normal and inverted hierarchy, respectively."],"url":"http://arxiv.org/abs/2403.11177v1","category":"hep-ph"}
{"created":"2024-03-17 09:03:15","title":"A note on quantum subgroups of free quantum groups","abstract":"In this short note, quantum subgroups in finite free products of the Pontryagin duals of free unitary quantum groups are classified. They correspond to pairs of a subgroup $\\Gamma$ and a subset $S$ of the free group $\\mathbb{F}_n$ such that $S$ is $\\Gamma$-invariant, containing $\\Gamma$, and connected in the Cayley graph of $\\mathbb{F}_n$","sentences":["In this short note, quantum subgroups in finite free products of the Pontryagin duals of free unitary quantum groups are classified.","They correspond to pairs of a subgroup $\\Gamma$ and a subset $S$ of the free group $\\mathbb{F}_n$ such that $S$ is $\\Gamma$-invariant, containing $\\Gamma$, and connected in the Cayley graph of $\\mathbb{F}_n$"],"url":"http://arxiv.org/abs/2403.11151v1","category":"math.OA"}
{"created":"2024-03-17 08:48:31","title":"Metal-semiconductor behavior along the line of stacking order change in gated multilayer graphene","abstract":"We investigate gated multilayer graphene with stacking order change along the armchair direction. We consider some layers cracked to release shear strain at the stacking domain wall. The energy cones of graphene overlap along the corresponding direction in the k-space, so the topological gapless states from different valleys also overlap. However, these states strongly interact and split due to atomic-scale defects caused by the broken layers, yielding an effective energy gap. We find that for some gate voltages, the gap states cross and the metallic behavior along the stacking domain wall can be restored. In particular cases, a flat band appears at the Fermi energy. We show that for small variations of the gate voltage the charge occupying this band oscillates between the outer layers.","sentences":["We investigate gated multilayer graphene with stacking order change along the armchair direction.","We consider some layers cracked to release shear strain at the stacking domain wall.","The energy cones of graphene overlap along the corresponding direction in the k-space, so the topological gapless states from different valleys also overlap.","However, these states strongly interact and split due to atomic-scale defects caused by the broken layers, yielding an effective energy gap.","We find that for some gate voltages, the gap states cross and the metallic behavior along the stacking domain wall can be restored.","In particular cases, a flat band appears at the Fermi energy.","We show that for small variations of the gate voltage the charge occupying this band oscillates between the outer layers."],"url":"http://arxiv.org/abs/2403.11143v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-17 08:43:19","title":"Theoretical investigation of the vertical dielectric screening dependence on defects for few-layered van der Waals materials","abstract":"First-principle calculations were employed to analyze the effects induced by vacancies of molybdenum (Mo) and sulfur (S) on the dielectric properties of few-layered MoS2. We explored the combined effects of vacancies and dipole interactions on the dielectric properties of few-layered MoS2. In the presence of dielectric screening, we investigated uniformly distributed Mo and S vacancies, and then considered the case of concentrated vacancies. Our results show that the dielectric screening remarkably depends on the distribution of vacancies owing to the polarization induced by the vacancies and on the interlayer distances. This conclusion was validated for a wide range of wide-gap semiconductors with different positions and distributions of vacancies, providing an effective and reliable method for calculating and predicting electrostatic screening of dimensionally reduced materials. We further provided a method for engineering the dielectric constant by changing the interlayer distance, tuning the number of vacancies and the distribution of vacancies in few-layered van der Waals materials for their application in nanodevices and supercapacitors.","sentences":["First-principle calculations were employed to analyze the effects induced by vacancies of molybdenum (Mo) and sulfur (S) on the dielectric properties of few-layered MoS2.","We explored the combined effects of vacancies and dipole interactions on the dielectric properties of few-layered MoS2.","In the presence of dielectric screening, we investigated uniformly distributed Mo and S vacancies, and then considered the case of concentrated vacancies.","Our results show that the dielectric screening remarkably depends on the distribution of vacancies owing to the polarization induced by the vacancies and on the interlayer distances.","This conclusion was validated for a wide range of wide-gap semiconductors with different positions and distributions of vacancies, providing an effective and reliable method for calculating and predicting electrostatic screening of dimensionally reduced materials.","We further provided a method for engineering the dielectric constant by changing the interlayer distance, tuning the number of vacancies and the distribution of vacancies in few-layered van der Waals materials for their application in nanodevices and supercapacitors."],"url":"http://arxiv.org/abs/2403.11140v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-17 07:44:44","title":"Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models","abstract":"This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language.","sentences":["This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks.","Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference.","Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance.","The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language.","However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency.","Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability.","Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged.","This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets.","Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic.","This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language."],"url":"http://arxiv.org/abs/2403.11130v1","category":"cs.CL"}
{"created":"2024-03-17 07:02:55","title":"Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence","abstract":"This paper introduces a Transformer-based integrative feature and cost aggregation network designed for dense matching tasks. In the context of dense matching, many works benefit from one of two forms of aggregation: feature aggregation, which pertains to the alignment of similar features, or cost aggregation, a procedure aimed at instilling coherence in the flow estimates across neighboring pixels. In this work, we first show that feature aggregation and cost aggregation exhibit distinct characteristics and reveal the potential for substantial benefits stemming from the judicious use of both aggregation processes. We then introduce a simple yet effective architecture that harnesses self- and cross-attention mechanisms to show that our approach unifies feature aggregation and cost aggregation and effectively harnesses the strengths of both techniques. Within the proposed attention layers, the features and cost volume both complement each other, and the attention layers are interleaved through a coarse-to-fine design to further promote accurate correspondence estimation. Finally at inference, our network produces multi-scale predictions, computes their confidence scores, and selects the most confident flow for final prediction. Our framework is evaluated on standard benchmarks for semantic matching, and also applied to geometric matching, where we show that our approach achieves significant improvements compared to existing methods.","sentences":["This paper introduces a Transformer-based integrative feature and cost aggregation network designed for dense matching tasks.","In the context of dense matching, many works benefit from one of two forms of aggregation: feature aggregation, which pertains to the alignment of similar features, or cost aggregation, a procedure aimed at instilling coherence in the flow estimates across neighboring pixels.","In this work, we first show that feature aggregation and cost aggregation exhibit distinct characteristics and reveal the potential for substantial benefits stemming from the judicious use of both aggregation processes.","We then introduce a simple yet effective architecture that harnesses self- and cross-attention mechanisms to show that our approach unifies feature aggregation and cost aggregation and effectively harnesses the strengths of both techniques.","Within the proposed attention layers, the features and cost volume both complement each other, and the attention layers are interleaved through a coarse-to-fine design to further promote accurate correspondence estimation.","Finally at inference, our network produces multi-scale predictions, computes their confidence scores, and selects the most confident flow for final prediction.","Our framework is evaluated on standard benchmarks for semantic matching, and also applied to geometric matching, where we show that our approach achieves significant improvements compared to existing methods."],"url":"http://arxiv.org/abs/2403.11120v1","category":"cs.CV"}
{"created":"2024-03-17 06:34:19","title":"A note on the dimensional regularization and the on-mass-shell renormalization in the two-loop order","abstract":"The use of the dimensional regularization in the on-mass-shell renormalization scheme sometimes fails to locally cancel the ultraviolet divergence for a class of diagrams in the two-loop order. The mechanism is discussed based on an example with explicit computation.","sentences":["The use of the dimensional regularization in the on-mass-shell renormalization scheme sometimes fails to locally cancel the ultraviolet divergence for a class of diagrams in the two-loop order.","The mechanism is discussed based on an example with explicit computation."],"url":"http://arxiv.org/abs/2403.11112v1","category":"hep-ph"}
{"created":"2024-03-17 04:25:39","title":"Enhanced Index Modulation Aided Non-Orthogonal Multiple Access via Constellation Rotation","abstract":"Non-orthogonal multiple access (NOMA) has been widely nominated as an emerging spectral efficiency (SE) multiple access technique for the next generation of wireless communication network. To meet the growing demands in massive connectivity and huge data in transmission, a novel index modulation aided NOMA with the rotation of signal constellation of low power users (IM-NOMA-RC) is developed to the downlink transmission. In the proposed IM-NOMA-RC system, the users are classified into far-user group and near-user group according to their channel conditions, where the rotation constellation based IM operation is performed only on the users who belong to the near-user group that are allocated lower power compared with the far ones to transmit extra information. In the proposed IM-NOMA-RC, all the subcarriers are activated to transmit information to multiple users to achieve higher SE. With the aid of the multiple dimension modulation in IM-NOMA-RC, more users can be supported over an orthogonal resource block. Then, both maximum likelihood (ML) detector and successive interference cancellation (SIC) detector are studied for all the user. Numerical simulation results of the proposed IM-NOMARC scheme are investigate for the ML detector and the SIC detector for each users, which shows that proposed scheme can outperform conventional NOMA.","sentences":["Non-orthogonal multiple access (NOMA) has been widely nominated as an emerging spectral efficiency (SE) multiple access technique for the next generation of wireless communication network.","To meet the growing demands in massive connectivity and huge data in transmission, a novel index modulation aided NOMA with the rotation of signal constellation of low power users (IM-NOMA-RC) is developed to the downlink transmission.","In the proposed IM-NOMA-RC system, the users are classified into far-user group and near-user group according to their channel conditions, where the rotation constellation based IM operation is performed only on the users who belong to the near-user group that are allocated lower power compared with the far ones to transmit extra information.","In the proposed IM-NOMA-RC, all the subcarriers are activated to transmit information to multiple users to achieve higher SE.","With the aid of the multiple dimension modulation in IM-NOMA-RC, more users can be supported over an orthogonal resource block.","Then, both maximum likelihood (ML) detector and successive interference cancellation (SIC) detector are studied for all the user.","Numerical simulation results of the proposed IM-NOMARC scheme are investigate for the ML detector and the SIC detector for each users, which shows that proposed scheme can outperform conventional NOMA."],"url":"http://arxiv.org/abs/2403.11081v1","category":"cs.IT"}
{"created":"2024-03-17 01:51:09","title":"A Novel Mutual Insurance Model for Hedging Against Cyber Risks in Power Systems Deploying Smart Technologies","abstract":"In this paper, a novel cyber-insurance model design is proposed based on system risk evaluation with smart technology applications. The cyber insurance policy for power systems is tailored via cyber risk modeling, reliability impact analysis, and insurance premium calculation. A stochastic Epidemic Network Model is developed to evaluate the cyber risk by propagating cyberattacks among graphical vulnerabilities. Smart technologies deployed in risk modeling include smart monitoring and job thread assignment. Smart monitoring boosts the substation availability against cyberattacks with preventive and corrective measures. The job thread assignment solution reduces the execution failures by distributing the control and monitoring tasks to multiple threads. Reliability assessment is deployed to estimate load losses convertible to monetary losses. These monetary losses would be shared through a mutual insurance plan. To ensure a fair distribution of indemnity, a new Shapley mutual insurance principle is devised. Effectiveness of the proposed Shapley mutual insurance design is validated via case studies. The Shapley premium is compared with existent premium designs. It is shown that the Shapley premium has high indemnity levels closer to those of Tail Conditional Expectation premium. Meanwhile, the Shapley premium is nearly as affordable as the coalitional premium and keeps a relatively low insolvency probability.","sentences":["In this paper, a novel cyber-insurance model design is proposed based on system risk evaluation with smart technology applications.","The cyber insurance policy for power systems is tailored via cyber risk modeling, reliability impact analysis, and insurance premium calculation.","A stochastic Epidemic Network Model is developed to evaluate the cyber risk by propagating cyberattacks among graphical vulnerabilities.","Smart technologies deployed in risk modeling include smart monitoring and job thread assignment.","Smart monitoring boosts the substation availability against cyberattacks with preventive and corrective measures.","The job thread assignment solution reduces the execution failures by distributing the control and monitoring tasks to multiple threads.","Reliability assessment is deployed to estimate load losses convertible to monetary losses.","These monetary losses would be shared through a mutual insurance plan.","To ensure a fair distribution of indemnity, a new Shapley mutual insurance principle is devised.","Effectiveness of the proposed Shapley mutual insurance design is validated via case studies.","The Shapley premium is compared with existent premium designs.","It is shown that the Shapley premium has high indemnity levels closer to those of Tail Conditional Expectation premium.","Meanwhile, the Shapley premium is nearly as affordable as the coalitional premium and keeps a relatively low insolvency probability."],"url":"http://arxiv.org/abs/2403.11054v1","category":"cs.GT"}
{"created":"2024-03-17 01:27:00","title":"Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention","abstract":"Recent advancements in text-to-image diffusion models have demonstrated their remarkable capability to generate high-quality images from textual prompts. However, increasing research indicates that these models memorize and replicate images from their training data, raising tremendous concerns about potential copyright infringement and privacy risks. In our study, we provide a novel perspective to understand this memorization phenomenon by examining its relationship with cross-attention mechanisms. We reveal that during memorization, the cross-attention tends to focus disproportionately on the embeddings of specific tokens. The diffusion model is overfitted to these token embeddings, memorizing corresponding training images. To elucidate this phenomenon, we further identify and discuss various intrinsic findings of cross-attention that contribute to memorization. Building on these insights, we introduce an innovative approach to detect and mitigate memorization in diffusion models. The advantage of our proposed method is that it will not compromise the speed of either the training or the inference processes in these models while preserving the quality of generated images. Our code is available at https://github.com/renjie3/MemAttn .","sentences":["Recent advancements in text-to-image diffusion models have demonstrated their remarkable capability to generate high-quality images from textual prompts.","However, increasing research indicates that these models memorize and replicate images from their training data, raising tremendous concerns about potential copyright infringement and privacy risks.","In our study, we provide a novel perspective to understand this memorization phenomenon by examining its relationship with cross-attention mechanisms.","We reveal that during memorization, the cross-attention tends to focus disproportionately on the embeddings of specific tokens.","The diffusion model is overfitted to these token embeddings, memorizing corresponding training images.","To elucidate this phenomenon, we further identify and discuss various intrinsic findings of cross-attention that contribute to memorization.","Building on these insights, we introduce an innovative approach to detect and mitigate memorization in diffusion models.","The advantage of our proposed method is that it will not compromise the speed of either the training or the inference processes in these models while preserving the quality of generated images.","Our code is available at https://github.com/renjie3/MemAttn ."],"url":"http://arxiv.org/abs/2403.11052v1","category":"cs.CV"}
{"created":"2024-03-16 23:15:00","title":"On the continuity of intertwining operators over generalized convolution algebras","abstract":"Let ${\\sf G}$ be a locally compact group, $\\mathscr C\\overset{q}{\\to}{\\sf G}$ a Fell bundle and $\\mathfrak B=L^1({\\sf G}\\,\\vert\\,\\mathscr C)$ the algebra of integrable cross-sections associated to the bundle. We give conditions that guarantee the automatic continuity of intertwining operators $\\theta:\\mathcal X_1\\to\\mathcal X_2$, where $\\mathcal X_1$ is a Banach $\\mathfrak B$-bimodule and $\\mathcal X_2$ is a weak Banach $\\mathfrak B$-bimodule, in terms of the continuity ideal of $\\theta$. We provide examples of algebras where this conditions are met, both in the case of derivations and algebra morphisms. In particular, we show that, if ${\\sf G}$ is infinite, finitely-generated, has polynomial growth and $\\alpha$ is a topologically free (partial) action of ${\\sf G}$ on the compact space $X$, then every homomorphism of $\\ell^1_\\alpha({\\sf G},C(X))$ into a Banach algebra is automatically continuous.","sentences":["Let ${\\sf G}$ be a locally compact group, $\\mathscr C\\overset{q}{\\to}{\\sf G}$ a Fell bundle and $\\mathfrak B=L^1({\\sf G}\\,\\vert\\,\\mathscr C)$ the algebra of integrable cross-sections associated to the bundle.","We give conditions that guarantee the automatic continuity of intertwining operators $\\theta:\\mathcal X_1\\to\\mathcal X_2$, where $\\mathcal X_1$ is a Banach $\\mathfrak B$-bimodule and $\\mathcal X_2$ is a weak Banach $\\mathfrak B$-bimodule, in terms of the continuity ideal of $\\theta$. We provide examples of algebras where this conditions are met, both in the case of derivations and algebra morphisms.","In particular, we show that, if ${\\sf G}$ is infinite, finitely-generated, has polynomial growth and $\\alpha$ is a topologically free (partial) action of ${\\sf G}$ on the compact space $X$, then every homomorphism of $\\ell^1_\\alpha({\\sf G},C(X))$ into a Banach algebra is automatically continuous."],"url":"http://arxiv.org/abs/2403.11039v1","category":"math.FA"}
{"created":"2024-03-16 23:01:51","title":"Texture Edge detection by Patch consensus (TEP)","abstract":"We propose Texture Edge detection using Patch consensus (TEP) which is a training-free method to detect the boundary of texture. We propose a new simple way to identify the texture edge location, using the consensus of segmented local patch information. While on the boundary, even using local patch information, the distinction between textures are typically not clear, but using neighbor consensus give a clear idea of the boundary. We utilize local patch, and its response against neighboring regions, to emphasize the similarities and the differences across different textures. The step of segmentation of response further emphasizes the edge location, and the neighborhood voting gives consensus and stabilize the edge detection. We analyze texture as a stationary process to give insight into the patch width parameter verses the quality of edge detection. We derive the necessary condition for textures to be distinguished, and analyze the patch width with respect to the scale of textures. Various experiments are presented to validate the proposed model.","sentences":["We propose Texture Edge detection using Patch consensus (TEP) which is a training-free method to detect the boundary of texture.","We propose a new simple way to identify the texture edge location, using the consensus of segmented local patch information.","While on the boundary, even using local patch information, the distinction between textures are typically not clear, but using neighbor consensus give a clear idea of the boundary.","We utilize local patch, and its response against neighboring regions, to emphasize the similarities and the differences across different textures.","The step of segmentation of response further emphasizes the edge location, and the neighborhood voting gives consensus and stabilize the edge detection.","We analyze texture as a stationary process to give insight into the patch width parameter verses the quality of edge detection.","We derive the necessary condition for textures to be distinguished, and analyze the patch width with respect to the scale of textures.","Various experiments are presented to validate the proposed model."],"url":"http://arxiv.org/abs/2403.11038v1","category":"cs.CV"}
{"created":"2024-03-16 20:16:37","title":"MASSM: An End-to-End Deep Learning Framework for Multi-Anatomy Statistical Shape Modeling Directly From Images","abstract":"Statistical Shape Modeling (SSM) is an effective method for quantitatively analyzing anatomical variations within populations. However, its utility is limited by the need for manual segmentations of anatomies, a task that relies on the scarce expertise of medical professionals. Recent advances in deep learning have provided a promising approach that automatically generates statistical representations from unsegmented images. Once trained, these deep learning-based models eliminate the need for manual segmentation for new subjects. Nonetheless, most current methods still require manual pre-alignment of image volumes and specifying a bounding box around the target anatomy prior for inference, resulting in a partially manual inference process. Recent approaches facilitate anatomy localization but only estimate statistical representations at the population level. However, they cannot delineate anatomy directly in images and are limited to modeling a single anatomy. Here, we introduce MASSM, a novel end-to-end deep learning framework that simultaneously localizes multiple anatomies in an image, estimates population-level statistical representations, and delineates each anatomy. Our findings emphasize the crucial role of local correspondences, showcasing their indispensability in providing superior shape information for medical imaging tasks.","sentences":["Statistical Shape Modeling (SSM) is an effective method for quantitatively analyzing anatomical variations within populations.","However, its utility is limited by the need for manual segmentations of anatomies, a task that relies on the scarce expertise of medical professionals.","Recent advances in deep learning have provided a promising approach that automatically generates statistical representations from unsegmented images.","Once trained, these deep learning-based models eliminate the need for manual segmentation for new subjects.","Nonetheless, most current methods still require manual pre-alignment of image volumes and specifying a bounding box around the target anatomy prior for inference, resulting in a partially manual inference process.","Recent approaches facilitate anatomy localization but only estimate statistical representations at the population level.","However, they cannot delineate anatomy directly in images and are limited to modeling a single anatomy.","Here, we introduce MASSM, a novel end-to-end deep learning framework that simultaneously localizes multiple anatomies in an image, estimates population-level statistical representations, and delineates each anatomy.","Our findings emphasize the crucial role of local correspondences, showcasing their indispensability in providing superior shape information for medical imaging tasks."],"url":"http://arxiv.org/abs/2403.11008v1","category":"cs.CV"}
{"created":"2024-03-16 19:57:38","title":"Synthesizing extreme-ultraviolet vector beams in a chip","abstract":"Structured light has gained significant attention in recent years, especially in the generation and application of vector beams. These beams, characterized by a spatially varying polarization state, are a powerful tool to enhance our capacity to control light-matter interactions. In this study, we demonstrate the synthesis of extreme-ultraviolet (EUV) vector beams in a chip through high-order harmonic generation (HHG). Our findings showcase the chip's ability to transfer the laser polarization state into the EUV beam despite the extended interaction length. This approach not only outperforms conventional free-space methods but also paves the way for a multitude of on-chip investigations in the realms of EUV and soft-X-ray spectroscopy.","sentences":["Structured light has gained significant attention in recent years, especially in the generation and application of vector beams.","These beams, characterized by a spatially varying polarization state, are a powerful tool to enhance our capacity to control light-matter interactions.","In this study, we demonstrate the synthesis of extreme-ultraviolet (EUV) vector beams in a chip through high-order harmonic generation (HHG).","Our findings showcase the chip's ability to transfer the laser polarization state into the EUV beam despite the extended interaction length.","This approach not only outperforms conventional free-space methods but also paves the way for a multitude of on-chip investigations in the realms of EUV and soft-X-ray spectroscopy."],"url":"http://arxiv.org/abs/2403.11006v1","category":"physics.optics"}
