{"created":"2024-04-09 17:59:47","title":"Superpolynomial Lower Bounds for Smooth 3-LCCs and Sharp Bounds for Designs","abstract":"We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$. A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings. Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq 2^{\\sqrt{8 k}}$. Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   (2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23]. Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs [KM23].","sentences":["We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$.","A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings.","Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq","2^{\\sqrt{8 k}}$.","Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   ","(2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   ","Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23].","Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs","[KM23]."],"url":"http://arxiv.org/abs/2404.06513v1","category":"cs.CC"}
{"created":"2024-04-09 17:59:31","title":"MoReVQA: Exploring Modular Reasoning Models for Video Question Answering","abstract":"This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework. Previous modular methods have shown promise with a single planning stage ungrounded in visual content. However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings. Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory. All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage. By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning).","sentences":["This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework.","Previous modular methods have shown promise with a single planning stage ungrounded in visual content.","However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings.","Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory.","All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage.","By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning)."],"url":"http://arxiv.org/abs/2404.06511v1","category":"cs.CV"}
{"created":"2024-04-09 17:59:04","title":"Horizon-penetrating form of parametrized metrics for static and stationary black holes","abstract":"The Rezzolla-Zhidenko (RZ) and Konoplya-Rezzolla-Zhidenko (KRZ) frameworks provide an efficient approach to characterize agnostically spherically symmetric or stationary black-hole spacetimes in arbitrary metric theories. In their original construction, these metrics were defined only in the spacetime region outside of the event horizon, where they can reproduce any black-hole metric with with percent precision and a few parameters only. At the same time, numerical simulations of accreting black holes often require metric functions that are regular across the horizon, so that the inner boundary of the computational domain can be placed in a region that is causally disconnected from the exterior. We present a novel formulation of the RZ/KRZ parametrized metrics in coordinate systems that are regular at the horizon and defined everywhere in the interior. We compare the horizon-penetrating form of the KRZ and RZ metrics with the corresponding forms of the Kerr metric in Kerr-Schild coordinates and of the Schwarzschild metric in Eddington-Finkelstein coordinates, remarking the similarities and differences. We expect the horizon-penetrating formulations of the RZ/KRZ metrics to represent new tools to study via simulations the physical processes that occur near the horizon of an arbitrary black hole.","sentences":["The Rezzolla-Zhidenko (RZ) and Konoplya-Rezzolla-Zhidenko (KRZ) frameworks provide an efficient approach to characterize agnostically spherically symmetric or stationary black-hole spacetimes in arbitrary metric theories.","In their original construction, these metrics were defined only in the spacetime region outside of the event horizon, where they can reproduce any black-hole metric with with percent precision and a few parameters only.","At the same time, numerical simulations of accreting black holes often require metric functions that are regular across the horizon, so that the inner boundary of the computational domain can be placed in a region that is causally disconnected from the exterior.","We present a novel formulation of the RZ/KRZ parametrized metrics in coordinate systems that are regular at the horizon and defined everywhere in the interior.","We compare the horizon-penetrating form of the KRZ and RZ metrics with the corresponding forms of the Kerr metric in Kerr-Schild coordinates and of the Schwarzschild metric in Eddington-Finkelstein coordinates, remarking the similarities and differences.","We expect the horizon-penetrating formulations of the RZ/KRZ metrics to represent new tools to study via simulations the physical processes that occur near the horizon of an arbitrary black hole."],"url":"http://arxiv.org/abs/2404.06509v1","category":"gr-qc"}
{"created":"2024-04-09 17:54:42","title":"Primordial black holes and curvature perturbations from false-vacuum islands","abstract":"Recently, much attention has been focused on the false-vacuum islands that are flooded by an expanding ocean of true-vacuum bubbles slightly later than most of the other parts of the world. These delayed decay regions will accumulate locally larger vacuum energy density by staying in the false vacuum longer than those already transited into the true vacuum. A false-vacuum island with thus acquired density contrast of a super-horizon size will evolve locally from radiation dominance to vacuum dominance, creating a local baby Universe that can be regarded effectively as a local closed Universe. If such density contrasts of super-horizon sizes can ever grow large enough to exceed the threshold of gravitational collapse, primordial black holes will form similar to those collapsing curvature perturbations on super-horizon scales induced by small-scale enhancements during inflation. If not, such density contrasts can still induce curvature perturbations potentially observable today. In this paper, we revisit and elaborate on the generations of primordial black holes and curvature perturbations from delayed-decayed false-vacuum islands during asynchronous first-order phase transitions with fitting formulas convenient for future model-independent studies.","sentences":["Recently, much attention has been focused on the false-vacuum islands that are flooded by an expanding ocean of true-vacuum bubbles slightly later than most of the other parts of the world.","These delayed decay regions will accumulate locally larger vacuum energy density by staying in the false vacuum longer than those already transited into the true vacuum.","A false-vacuum island with thus acquired density contrast of a super-horizon size will evolve locally from radiation dominance to vacuum dominance, creating a local baby Universe that can be regarded effectively as a local closed Universe.","If such density contrasts of super-horizon sizes can ever grow large enough to exceed the threshold of gravitational collapse, primordial black holes will form similar to those collapsing curvature perturbations on super-horizon scales induced by small-scale enhancements during inflation.","If not, such density contrasts can still induce curvature perturbations potentially observable today.","In this paper, we revisit and elaborate on the generations of primordial black holes and curvature perturbations from delayed-decayed false-vacuum islands during asynchronous first-order phase transitions with fitting formulas convenient for future model-independent studies."],"url":"http://arxiv.org/abs/2404.06506v1","category":"astro-ph.CO"}
{"created":"2024-04-09 17:54:27","title":"Characterizing visual cortical magnification with topological smoothing and optimal transportation","abstract":"Human vision has different concentration on visual fields. Cortical magnification factor (CMF) is a popular measurement on visual acuity and cortex concentration. In order to achieve thorough measurement of CMF across the whole visual field, we propose a method to measure planar CMF upon retinotopic maps generated by pRF decoding, with help of our proposed methods: optimal transportation and topological smoothing. The optimal transportation re-calculates vertex location in retinotopic mapping, and topological smoothing guarantees topological conditions in retinotopic maps, which allow us to calculate planar CMF with the proposed 1-ring patch method. The pipeline was applied to the HCP 7T dataset, giving new planar results on CMF measurement across all 181 subjects, which illustrate novel concentration behavior on visual fields and their individual difference.","sentences":["Human vision has different concentration on visual fields.","Cortical magnification factor (CMF) is a popular measurement on visual acuity and cortex concentration.","In order to achieve thorough measurement of CMF across the whole visual field, we propose a method to measure planar CMF upon retinotopic maps generated by pRF decoding, with help of our proposed methods: optimal transportation and topological smoothing.","The optimal transportation re-calculates vertex location in retinotopic mapping, and topological smoothing guarantees topological conditions in retinotopic maps, which allow us to calculate planar CMF with the proposed 1-ring patch method.","The pipeline was applied to the HCP 7T dataset, giving new planar results on CMF measurement across all 181 subjects, which illustrate novel concentration behavior on visual fields and their individual difference."],"url":"http://arxiv.org/abs/2404.06505v1","category":"q-bio.NC"}
{"created":"2024-04-09 17:54:10","title":"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?","abstract":"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.","sentences":["Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note.","A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology.","The relevant information can then be extracted and organized according to the structure of the SOAP note.","In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency.","The first approach generates the sections independently, while the second method generates them all together.","In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric.","We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators.","Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively.","With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics.","This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections."],"url":"http://arxiv.org/abs/2404.06503v1","category":"cs.CL"}
{"created":"2024-04-09 17:51:21","title":"A Machine Learning Framework for the Prediction of Grain Boundary Segregation in Chemically Complex Environments","abstract":"The discovery of complex concentrated alloys has unveiled materials with diverse atomic environments, prompting the exploration of solute segregation beyond dilute alloys. Data-driven methods offer promising for modeling segregation in such chemically complex environments, and are employed in this study to understand segregation behavior of a refractory complex concentrated alloy, NbMoTaW. A flexible methodology is developed that uses composable computational modules, with different arrangements of these modules employed to obtain site availabilities at absolute zero and the corresponding density of states beyond the dilute limit, resulting in an extremely large dataset containing 10 million data points. The artificial neural network developed here can rely solely on descriptions of local atomic environments to predict behavior at the dilute limit with very small errors, while the addition of negative segregation instance classification allows any solute concentration from zero up to the equiatomic concentration for ternary or quaternary alloys to be modeled at room temperature. The machine learning model thus achieves a significant speed advantage over traditional atomistic simulations, being four orders of magnitude faster, while only experiencing a minimal reduction in accuracy. This efficiency presents a powerful tool for rapid microstructural and interfacial design in unseen domains. Scientifically, our approach reveals a transition in the segregation behavior of Mo from unfavorable in simple systems to favorable in complex environments. Additionally, increasing solute concentration was observed to cause anti-segregation sites to begin to fill, challenging conventional understanding and highlighting the complexity of segregation dynamics in chemically complex environments.","sentences":["The discovery of complex concentrated alloys has unveiled materials with diverse atomic environments, prompting the exploration of solute segregation beyond dilute alloys.","Data-driven methods offer promising for modeling segregation in such chemically complex environments, and are employed in this study to understand segregation behavior of a refractory complex concentrated alloy, NbMoTaW. A flexible methodology is developed that uses composable computational modules, with different arrangements of these modules employed to obtain site availabilities at absolute zero and the corresponding density of states beyond the dilute limit, resulting in an extremely large dataset containing 10 million data points.","The artificial neural network developed here can rely solely on descriptions of local atomic environments to predict behavior at the dilute limit with very small errors, while the addition of negative segregation instance classification allows any solute concentration from zero up to the equiatomic concentration for ternary or quaternary alloys to be modeled at room temperature.","The machine learning model thus achieves a significant speed advantage over traditional atomistic simulations, being four orders of magnitude faster, while only experiencing a minimal reduction in accuracy.","This efficiency presents a powerful tool for rapid microstructural and interfacial design in unseen domains.","Scientifically, our approach reveals a transition in the segregation behavior of Mo from unfavorable in simple systems to favorable in complex environments.","Additionally, increasing solute concentration was observed to cause anti-segregation sites to begin to fill, challenging conventional understanding and highlighting the complexity of segregation dynamics in chemically complex environments."],"url":"http://arxiv.org/abs/2404.06499v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 17:50:37","title":"Banach lattices of positively homogeneous functions induced by a Banach space","abstract":"Motivated by the construction of the free Banach lattice generated by a Banach space, we introduce and study several vector and Banach lattices of positively homogeneous functions defined on the dual of a Banach space $E$. The relations between these lattices allow us to give multiple characterizations of when the underlying Banach space $E$ is finite-dimensional and when it is reflexive. Furthermore, we show that lattice homomorphisms between free Banach lattices are always composition operators, and study how these operators behave on the scale of lattices of positively homogeneous functions.","sentences":["Motivated by the construction of the free Banach lattice generated by a Banach space, we introduce and study several vector and Banach lattices of positively homogeneous functions defined on the dual of a Banach space $E$.","The relations between these lattices allow us to give multiple characterizations of when the underlying Banach space $E$ is finite-dimensional and when it is reflexive.","Furthermore, we show that lattice homomorphisms between free Banach lattices are always composition operators, and study how these operators behave on the scale of lattices of positively homogeneous functions."],"url":"http://arxiv.org/abs/2404.06497v1","category":"math.FA"}
{"created":"2024-04-09 17:49:18","title":"Mechanism Design for ZK-Rollup Prover Markets","abstract":"In ZK-Rollups, provers spend significant computational resources to generate validity proofs. Their costs should be compensated properly, so a sustainable prover market can form over time. Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin. In this paper, we explore the design of transaction fee mechanisms for prover markets. The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists). This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties. In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected. We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements. Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks. We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets. This work is in progress, and this manuscript will be updated.","sentences":["In ZK-Rollups, provers spend significant computational resources to generate validity proofs.","Their costs should be compensated properly, so a sustainable prover market can form over time.","Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin.","In this paper, we explore the design of transaction fee mechanisms for prover markets.","The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists).","This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties.","In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected.","We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements.","Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks.","We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets.","This work is in progress, and this manuscript will be updated."],"url":"http://arxiv.org/abs/2404.06495v1","category":"cs.GT"}
{"created":"2024-04-09 17:45:25","title":"Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective","abstract":"Graphs are a natural representation for systems based on relations between connected entities. Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space. The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics. Despite the fact that they arose in markedly different fields, these techniques share significant commonalities. Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems. After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure. Finally, we discuss the common challenges facing the field and open research questions. In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions.","sentences":["Graphs are a natural representation for systems based on relations between connected entities.","Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space.","The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics.","Despite the fact that they arose in markedly different fields, these techniques share significant commonalities.","Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems.","After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure.","Finally, we discuss the common challenges facing the field and open research questions.","In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions."],"url":"http://arxiv.org/abs/2404.06492v1","category":"cs.LG"}
{"created":"2024-04-09 17:43:28","title":"Finding Stable Price Zones in European Electricity Markets: Aiming to Square the Circle?","abstract":"The European day-ahead electricity market is split into multiple bidding zones. Within these zones, a uniform energy price is computed for each hour. Large bidding zones have been under scrutiny. The fact that zonal clearing ignores the transmission capacities within zones and the increase in renewables lead to a growing number of interventions in the generation of energy sources and large redispatch costs. The European Union Agency for the Cooperation of Energy Regulators (ACER) proposed alternative bidding zone configurations that should be analyzed as part of the Bidding Zone Review. For Germany, four alternative configurations were suggested. Bidding zones shall be stable and based on long-term, structural congestion in the grid. We analyzed the proposed configurations considering different clustering algorithms and periods. We found that the configurations do not reduce the price standard deviations within zones considerably, while the average prices across zones are similar. Other configurations identified based on clustering prices lead to lower price standard deviations but are not geographically coherent. Importantly, different configurations emerge depending on clustering features, algorithm, and period considered. Given the substantial changes in energy supply and demand that can be expected in the future, defining stable configurations appears to be a moving target.","sentences":["The European day-ahead electricity market is split into multiple bidding zones.","Within these zones, a uniform energy price is computed for each hour.","Large bidding zones have been under scrutiny.","The fact that zonal clearing ignores the transmission capacities within zones and the increase in renewables lead to a growing number of interventions in the generation of energy sources and large redispatch costs.","The European Union Agency for the Cooperation of Energy Regulators (ACER) proposed alternative bidding zone configurations that should be analyzed as part of the Bidding Zone Review.","For Germany, four alternative configurations were suggested.","Bidding zones shall be stable and based on long-term, structural congestion in the grid.","We analyzed the proposed configurations considering different clustering algorithms and periods.","We found that the configurations do not reduce the price standard deviations within zones considerably, while the average prices across zones are similar.","Other configurations identified based on clustering prices lead to lower price standard deviations but are not geographically coherent.","Importantly, different configurations emerge depending on clustering features, algorithm, and period considered.","Given the substantial changes in energy supply and demand that can be expected in the future, defining stable configurations appears to be a moving target."],"url":"http://arxiv.org/abs/2404.06489v1","category":"econ.GN"}
{"created":"2024-04-09 17:42:59","title":"Pitfalls of Conversational LLMs on News Debiasing","abstract":"This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.","sentences":["This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task.","We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist.","Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs.","Our findings indicate that none of the LLMs are perfect in debiasing.","Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation.","Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs."],"url":"http://arxiv.org/abs/2404.06488v1","category":"cs.CL"}
{"created":"2024-04-09 17:35:11","title":"Public-private funding models in open source software development: A case study on scikit-learn","abstract":"Governments are increasingly allocating funding for open source software (OSS) development in order to address concerns related to software security, digital sovereignty, and the competitiveness of domestic software markets, amongst others. While such funding is generally welcomed by OSS practitioners, how OSS developers perceive the relative benefits and drawbacks of governmental funding remains an open question. This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million EUR grant from the French government's artificial intelligence strategy. Through 25 interviews with scikit-learn maintainers and funders, this study makes two key contributions with implications for research and practice. First, it provides novel insights into the role of a public-private funding model in a successful, community-led OSS project and how maintainers evaluate their funding model. Furthermore, it highlights the governance mechanisms employed by maintainers to safeguard the community ethos of the project. Second, it offers practical implications for OSS developer communities, companies, and governments. For OSS communities, the study illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources. For companies, it serves as a reminder that sponsoring developers or directly funding OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads. For governments, the findings emphasise the importance of funding the maintenance of existing OSS projects in addition to or exclusively funding new innovations. The paper concludes with suggestions for future research on OSS funding models.","sentences":["Governments are increasingly allocating funding for open source software (OSS) development in order to address concerns related to software security, digital sovereignty, and the competitiveness of domestic software markets, amongst others.","While such funding is generally welcomed by OSS practitioners, how OSS developers perceive the relative benefits and drawbacks of governmental funding remains an open question.","This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million EUR grant from the French government's artificial intelligence strategy.","Through 25 interviews with scikit-learn maintainers and funders, this study makes two key contributions with implications for research and practice.","First, it provides novel insights into the role of a public-private funding model in a successful, community-led OSS project and how maintainers evaluate their funding model.","Furthermore, it highlights the governance mechanisms employed by maintainers to safeguard the community ethos of the project.","Second, it offers practical implications for OSS developer communities, companies, and governments.","For OSS communities, the study illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources.","For companies, it serves as a reminder that sponsoring developers or directly funding OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads.","For governments, the findings emphasise the importance of funding the maintenance of existing OSS projects in addition to or exclusively funding new innovations.","The paper concludes with suggestions for future research on OSS funding models."],"url":"http://arxiv.org/abs/2404.06484v1","category":"cs.SE"}
{"created":"2024-04-09 17:31:18","title":"GeoDirDock: Guiding Docking Along Geodesic Paths","abstract":"This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions. GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom. Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions. We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism. Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions. Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately.","sentences":["This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions.","GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom.","Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions.","We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism.","Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions.","Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately."],"url":"http://arxiv.org/abs/2404.06481v1","category":"q-bio.BM"}
{"created":"2024-04-09 17:30:48","title":"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks","abstract":"Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.","sentences":["Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents.","As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important.","Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks.","These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges.","Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.","In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs.","Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities.","These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens.","We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval.","The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.","Our code is available at https://github.com/open-compass/Ada-LEval."],"url":"http://arxiv.org/abs/2404.06480v1","category":"cs.CL"}
{"created":"2024-04-09 17:30:18","title":"Text-Based Reasoning About Vector Graphics","abstract":"While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks. Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics. We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/","sentences":["While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes.","In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes.","To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics.","VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding.","Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values.","PVD is task-agnostic and represents visual primitives that are universal across all vector graphics.","It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks.","By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks.","Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics.","We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes.","Project page: https://mikewangwzhl.github.io/VDLM/"],"url":"http://arxiv.org/abs/2404.06479v1","category":"cs.CL"}
{"created":"2024-04-09 17:25:47","title":"Autonomous Evaluation and Refinement of Digital Agents","abstract":"We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario.","sentences":["We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control.","We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy.","We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics.","Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance.","Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario."],"url":"http://arxiv.org/abs/2404.06474v1","category":"cs.AI"}
{"created":"2024-04-09 17:25:30","title":"Generalized cubic partitions","abstract":"A cubic partition consists of partition pairs $(\\lambda,\\mu)$ such that $\\vert\\lambda\\vert+\\vert\\mu\\vert=n$ where $\\mu$ involves only even integers but no restriction is placed on $\\lambda$. This paper initiates the notion of generalized cubic partitions and will prove a number of new congruences akin to the classical Ramanujan-type. The tools emphasize three methods of proofs. The paper concludes with a conjecture on the rarity of the aforementioned Ramanujan-type congruences.","sentences":["A cubic partition consists of partition pairs $(\\lambda,\\mu)$ such that $\\vert\\lambda\\vert+\\vert\\mu\\vert=n$ where $\\mu$ involves only even integers but no restriction is placed on $\\lambda$. This paper initiates the notion of generalized cubic partitions and will prove a number of new congruences akin to the classical Ramanujan-type.","The tools emphasize three methods of proofs.","The paper concludes with a conjecture on the rarity of the aforementioned Ramanujan-type congruences."],"url":"http://arxiv.org/abs/2404.06473v1","category":"math.NT"}
{"created":"2024-04-09 17:24:25","title":"High-skilled Human Workers in Non-Routine Jobs are Susceptible to AI Automation but Wage Benefits Differ between Occupations","abstract":"Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages. By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI. Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI. Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict.","sentences":["Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages.","By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI.","Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI.","Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict."],"url":"http://arxiv.org/abs/2404.06472v1","category":"econ.GN"}
{"created":"2024-04-09 17:17:48","title":"Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes","abstract":"We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval. By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor. Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities. To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints. We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes. The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc. To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process. The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state. We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI.","sentences":["We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval.","By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor.","Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities.","To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints.","We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes.","The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc.","To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process.","The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state.","We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI."],"url":"http://arxiv.org/abs/2404.06470v1","category":"cs.CV"}
{"created":"2024-04-09 17:17:23","title":"Neuromorphic In-Context Learning for Energy-Efficient MIMO Symbol Detection","abstract":"In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context. ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output. Prior work has proved the usefulness of ICL for detection in MIMO channels. In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals. However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices. This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection. This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting. When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations.","sentences":["In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context.","ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output.","Prior work has proved the usefulness of ICL for detection in MIMO channels.","In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals.","However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices.","This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection.","This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting.","When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations."],"url":"http://arxiv.org/abs/2404.06469v1","category":"eess.SP"}
{"created":"2024-04-09 16:57:50","title":"Critical non-linearity for some evolution equations with Fujita-type critical exponent","abstract":"We consider the Cauchy problem for a class of non-linear evolution equations in the form \\[L(\\partial_t,\\partial_x) u=F(\\partial_t^\\ell u), \\quad (t,x)\\in [0,\\infty)\\times \\mathbb{R}^n;\\] here, $L(\\partial_t,\\partial_x)$ is a linear partial differential operator with constant coefficients, of order $m\\geq 1$ with respect to the time variable $t$, and $\\ell$ is a natural number satisfying $0\\leq \\ell\\leq m-1$. For several different choices of $L$, many authors have investigated the existence of global (in time) solutions to this problem when $F(s)=|s|^p$ is a power non-linearity, looking for a \\textit{critical exponent} $p_c>1$ such that global small data solutions exist in the supercritical case $p>p_c$, whereas no global weak solutions exist, under suitable sign assumptions on the data, in the subcritical case $1<p<p_c$. In the present paper we consider a more general non-linear term in the form $F(s)=|s|^p\\mu(|s|)$; for a large class of models, we provide an integral condition on $\\mu$ which allows to distinguish more precisely the region of existence of a global (in time) small data solution from that in which the problem admits no global (in time) weak solutions, refining the existing results about the critical exponents for power type non-linearities.","sentences":["We consider the Cauchy problem for a class of non-linear evolution equations in the form \\[L(\\partial_t,\\partial_x) u=F(\\partial_t^\\ell u), \\quad (t,x)\\in","[0,\\infty)\\times \\mathbb{R}^n;\\] here, $L(\\partial_t,\\partial_x)$ is a linear partial differential operator with constant coefficients, of order $m\\geq 1$ with respect to the time variable $t$, and $\\ell$ is a natural number satisfying $0\\leq \\ell\\leq m-1$. For several different choices of $L$, many authors have investigated the existence of global (in time) solutions to this problem when $F(s)=|s|^p$ is a power non-linearity, looking for a \\textit{critical exponent} $p_c>1$ such that global small data solutions exist in the supercritical case $p>p_c$, whereas no global weak solutions exist, under suitable sign assumptions on the data, in the subcritical case $1<p<p_c$. In the present paper we consider a more general non-linear term in the form $F(s)=|s|^p\\mu(|s|)$; for a large class of models, we provide an integral condition on $\\mu$ which allows to distinguish more precisely the region of existence of a global (in time) small data solution from that in which the problem admits no global (in time) weak solutions, refining the existing results about the critical exponents for power type non-linearities."],"url":"http://arxiv.org/abs/2404.06458v1","category":"math.AP"}
{"created":"2024-04-09 16:55:23","title":"A comparative analysis of deep learning models for lung segmentation on X-ray images","abstract":"Robust and highly accurate lung segmentation in X-rays is crucial in medical imaging. This study evaluates deep learning solutions for this task, ranking existing methods and analyzing their performance under diverse image modifications. Out of 61 analyzed papers, only nine offered implementation or pre-trained models, enabling assessment of three prominent methods: Lung VAE, TransResUNet, and CE-Net. The analysis revealed that CE-Net performs best, demonstrating the highest values in dice similarity coefficient and intersection over union metric.","sentences":["Robust and highly accurate lung segmentation in X-rays is crucial in medical imaging.","This study evaluates deep learning solutions for this task, ranking existing methods and analyzing their performance under diverse image modifications.","Out of 61 analyzed papers, only nine offered implementation or pre-trained models, enabling assessment of three prominent methods: Lung VAE, TransResUNet, and CE-Net.","The analysis revealed that CE-Net performs best, demonstrating the highest values in dice similarity coefficient and intersection over union metric."],"url":"http://arxiv.org/abs/2404.06455v1","category":"eess.IV"}
{"created":"2024-04-09 16:54:34","title":"Superdense Coding and Stabiliser Codes with Ising-coupled Entanglement","abstract":"A new class of quantum states is introduced by demanding that the computational measurement statistics approach the Boltzmann distribution of higher-order strongly coupled Ising models. The states, referred to as $n$-coupled states, are superpositions of even or odd parity $n$-qubit states, generalize Bell states, and form an orthonormal basis for the $n$-qubit Hilbert space. For any $n$, the states are maximally connected and locally maximally entangled. It is proven that the $n$-qubit W and GHZ multipartite entanglement classes have vanishing hyperdeterminant for all $n\\geq 3$ and $n\\geq 4$, respectively, and that the $n$-coupled states fall in the latter. Still, multiple novel protocols for multi-party secure dense coding and stabiliser code construction are presented, which rely on the structure of $n$-coupled states as well as symmetry-breaking phase perturbations.","sentences":["A new class of quantum states is introduced by demanding that the computational measurement statistics approach the Boltzmann distribution of higher-order strongly coupled Ising models.","The states, referred to as $n$-coupled states, are superpositions of even or odd parity $n$-qubit states, generalize Bell states, and form an orthonormal basis for the $n$-qubit Hilbert space.","For any $n$, the states are maximally connected and locally maximally entangled.","It is proven that the $n$-qubit W and GHZ multipartite entanglement classes have vanishing hyperdeterminant for all $n\\geq 3$ and $n\\geq 4$, respectively, and that the $n$-coupled states fall in the latter.","Still, multiple novel protocols for multi-party secure dense coding and stabiliser code construction are presented, which rely on the structure of $n$-coupled states as well as symmetry-breaking phase perturbations."],"url":"http://arxiv.org/abs/2404.06454v1","category":"quant-ph"}
{"created":"2024-04-09 16:54:19","title":"PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits","abstract":"The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks. Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult. We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons. This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature. We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet. While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations. Our code is available at https://github.com/maxdreyer/PURE.","sentences":["The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks.","Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult.","We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons.","This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature.","We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet.","While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations.","Our code is available at https://github.com/maxdreyer/PURE."],"url":"http://arxiv.org/abs/2404.06453v1","category":"cs.CV"}
{"created":"2024-04-09 16:53:43","title":"SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions","abstract":"Human visual imagination usually begins with analogies or rough sketches. For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt. Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts. To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt. The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts. In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP. It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects. Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts. Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl.","sentences":["Human visual imagination usually begins with analogies or rough sketches.","For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt.","Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts.","To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt.","The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts.","In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP.","It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects.","Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts.","Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl."],"url":"http://arxiv.org/abs/2404.06451v1","category":"cs.CV"}
{"created":"2024-04-09 16:52:27","title":"Distorted static black holes with a bubble","abstract":"We construct a family of local static, vacuum five-dimensional solutions with two commuting spatial isometries describing a black hole with a $S^3$ horizon and a 2-cycle `bubble' in the domain of outer communications. The solutions are obtained by adding distortions to an asymptotically flat seed solution. We show that the conical singularities in the undistorted geometry can be removed by an appropriate choice of the distortion.","sentences":["We construct a family of local static, vacuum five-dimensional solutions with two commuting spatial isometries describing a black hole with a $S^3$ horizon and a 2-cycle `bubble' in the domain of outer communications.","The solutions are obtained by adding distortions to an asymptotically flat seed solution.","We show that the conical singularities in the undistorted geometry can be removed by an appropriate choice of the distortion."],"url":"http://arxiv.org/abs/2404.06450v1","category":"gr-qc"}
{"created":"2024-04-09 16:50:30","title":"Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.","sentences":["Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs).","However, for many downstream tasks, it is necessary to fine-tune LLMs using private data.","While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks.","More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning.","To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency.","FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training.","It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM.","Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers.","Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06448v1","category":"cs.LG"}
{"created":"2024-04-09 16:49:27","title":"An effective version of Fekete's theorem","abstract":"A classical result of Fekete gives necessary conditions on a compact set in the complex plane so that it contains infinitely many sets of conjugate algebraic integers. We prove an effective version of Fekete's theorem in terms of a height function. As an application, we give a lower bound on the growth of the leading coefficient of certain polynomial sequences, generalizing a result by Schur. Lastly, we give an upper bound on minimal asymptotics of height over sequences of algebraic numbers.","sentences":["A classical result of Fekete gives necessary conditions on a compact set in the complex plane so that it contains infinitely many sets of conjugate algebraic integers.","We prove an effective version of Fekete's theorem in terms of a height function.","As an application, we give a lower bound on the growth of the leading coefficient of certain polynomial sequences, generalizing a result by Schur.","Lastly, we give an upper bound on minimal asymptotics of height over sequences of algebraic numbers."],"url":"http://arxiv.org/abs/2404.06446v1","category":"math.CV"}
{"created":"2024-04-09 16:47:13","title":"Cosmic Clues: DESI, Dark Energy, and the Cosmological Constant Problem","abstract":"Several attempts to solve the cosmological constant problem, which concerns the value of the cosmological constant being extremely smaller than the Standard Model mass scales, have introduced a scalar field with a very flat potential that can be approximated as linear around any given position. The scalar field scans the cosmological constant in such a way that the current small value is explained. Recently, Dark Energy Spectroscopic Instrument (DESI) reported the results of the first year. Combining the data with CMB, Pantheon, Union3, and/or DES-SN5YR, there is a preference or anomaly, indicating that the dark energy in the current Universe slightly deviates from that in the $\\Lambda$CDM model and varies over time. In this paper, I show that the simple linear potential of a scalar field that may explain the small cosmological constant can explain the DESI anomaly. In particular, the model proposed by the present author in 2108.04246, which relaxes the cosmological constant by the condition that inflation ends, predicts a time-dependence of the dark energy close to the one favored by the data.","sentences":["Several attempts to solve the cosmological constant problem, which concerns the value of the cosmological constant being extremely smaller than the Standard Model mass scales, have introduced a scalar field with a very flat potential that can be approximated as linear around any given position.","The scalar field scans the cosmological constant in such a way that the current small value is explained.","Recently, Dark Energy Spectroscopic Instrument (DESI) reported the results of the first year.","Combining the data with CMB, Pantheon, Union3, and/or DES-SN5YR, there is a preference or anomaly, indicating that the dark energy in the current Universe slightly deviates from that in the $\\Lambda$CDM model and varies over time.","In this paper, I show that the simple linear potential of a scalar field that may explain the small cosmological constant can explain the DESI anomaly.","In particular, the model proposed by the present author in 2108.04246, which relaxes the cosmological constant by the condition that inflation ends, predicts a time-dependence of the dark energy close to the one favored by the data."],"url":"http://arxiv.org/abs/2404.06444v1","category":"hep-ph"}
{"created":"2024-04-09 16:42:54","title":"QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding","abstract":"Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction. Robotic tasks such as planning and navigation require a semantic understanding of the scene as well. This is typically achieved via object-level semantic segmentation. However, such methods struggle to segment out topological regions like \"kitchen\" in the scene. In this work, we introduce a two-step pipeline. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer. Our language-topology alignment supports natural language querying, e.g., a \"place to cook\" locates the \"kitchen\". We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding.","sentences":["Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction.","Robotic tasks such as planning and navigation require a semantic understanding of the scene as well.","This is typically achieved via object-level semantic segmentation.","However, such methods struggle to segment out topological regions like \"kitchen\" in the scene.","In this work, we introduce a two-step pipeline.","First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation.","Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer.","Our language-topology alignment supports natural language querying, e.g., a \"place to cook\" locates the \"kitchen\".","We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%.","Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding."],"url":"http://arxiv.org/abs/2404.06442v1","category":"cs.CV"}
{"created":"2024-04-09 16:42:45","title":"A minimal solution to the axion isocurvature problem from a non-minimal coupling","abstract":"The main limitation for pre-inflationary breaking of Peccei-Quinn (PQ) symmetry is the upper bound on the Hubble rate during inflation from axion isocurvature fluctuations. This leads to a tension between high scale inflation and QCD axions with Grand Unified Theory (GUT) scale decay constants, which reduces the potential for a detection of tensor modes at next generation CMB experiments. We propose a mechanism that excplicitly breaks PQ symmetry via non-minimal coupling to gravity, that lifts the axion mass above the Hubble scale during inflation and has negligible impact on today's axion potential. The initially heavy axion gets trapped at an intermediate minimum during inflation given by the phase of the non-minimal coupling, before it moves to its true CP-conserving minimum after inflation. During this stage it undergoes coherent oscillations around an adiabatically decreasing minimum, which slightly dilutes the axion energy density, while still being able to explain the observed dark matter relic abundance. This scenario can be tested by the combination of next generation CMB surveys like CMB-S4 and LiteBIRD with haloscopes such as ABRACADABRA or CASPEr-Electric.","sentences":["The main limitation for pre-inflationary breaking of Peccei-Quinn (PQ) symmetry is the upper bound on the Hubble rate during inflation from axion isocurvature fluctuations.","This leads to a tension between high scale inflation and QCD axions with Grand Unified Theory (GUT) scale decay constants, which reduces the potential for a detection of tensor modes at next generation CMB experiments.","We propose a mechanism that excplicitly breaks PQ symmetry via non-minimal coupling to gravity, that lifts the axion mass above the Hubble scale during inflation and has negligible impact on today's axion potential.","The initially heavy axion gets trapped at an intermediate minimum during inflation given by the phase of the non-minimal coupling, before it moves to its true CP-conserving minimum after inflation.","During this stage it undergoes coherent oscillations around an adiabatically decreasing minimum, which slightly dilutes the axion energy density, while still being able to explain the observed dark matter relic abundance.","This scenario can be tested by the combination of next generation CMB surveys like CMB-S4 and LiteBIRD with haloscopes such as ABRACADABRA or CASPEr-Electric."],"url":"http://arxiv.org/abs/2404.06441v1","category":"hep-ph"}
{"created":"2024-04-09 16:38:17","title":"ClassiPyGRB: Machine Learning-Based Classification and Visualization of Gamma Ray Bursts using t-SNE","abstract":"Gamma-ray burst (GRBs) are the brightest events in the universe. For decades, astrophysicists have known about their cosmological nature. Every year, space missions such as Fermi and SWIFT detect hundreds of them. In spite of this large sample, GRBs show a complex taxonomy in the first seconds after their appearance, which makes it very difficult to find similarities between them using conventional techniques. It is known that GRBs originate from the death of a massive star or from the merger of two compact objects. GRB classification is typically based on the duration of the burst (Kouveliotou et al., 1993). Nevertheless, events such as GRB 211211A (Yang et al., 2022), whose duration of about 50 seconds lies in the group of long GRBs, has challenged this categorization by the evidence of features related with the short GRB population (the kilonova emission and the properties of its host galaxy). Therefore, a classification based only on their gamma-ray duration does not provide a completely reliable determination of the progenitor. Motivated by this problem, Jespersen et al. (2020) and Steinhardt et al. (2023) carried out analysis of GRB light curves by using the t-SNE algorithm, showing that Swift/BAT GRBs database, consisting of light curves in four energy bands (15-25 keV, 25-50 keV, 50-100 keV, 100-350 keV), clusters into two groups corresponding with the typical long/short classification. However, in this case, this classification is based on the information provided by their gamma-ray emission light curves. ClassiPyGRB is a Python 3 package to download, process, visualize and classify GRBs database from the Swift/BAT Instrument (up to July 2022). It is distributed over the GNU General Public License Version 2 (1991). We also included a noise-reduction and an interpolation tools for achieving a deeper analysis of the data.","sentences":["Gamma-ray burst (GRBs) are the brightest events in the universe.","For decades, astrophysicists have known about their cosmological nature.","Every year, space missions such as Fermi and SWIFT detect hundreds of them.","In spite of this large sample, GRBs show a complex taxonomy in the first seconds after their appearance, which makes it very difficult to find similarities between them using conventional techniques.","It is known that GRBs originate from the death of a massive star or from the merger of two compact objects.","GRB classification is typically based on the duration of the burst (Kouveliotou et al., 1993).","Nevertheless, events such as GRB 211211A (Yang et al., 2022), whose duration of about 50 seconds lies in the group of long GRBs, has challenged this categorization by the evidence of features related with the short GRB population (the kilonova emission and the properties of its host galaxy).","Therefore, a classification based only on their gamma-ray duration does not provide a completely reliable determination of the progenitor.","Motivated by this problem, Jespersen et al. (2020) and Steinhardt et al.","(2023) carried out analysis of GRB light curves by using the t-SNE algorithm, showing that Swift/BAT GRBs database, consisting of light curves in four energy bands (15-25 keV, 25-50 keV, 50-100 keV, 100-350 keV), clusters into two groups corresponding with the typical long/short classification.","However, in this case, this classification is based on the information provided by their gamma-ray emission light curves.","ClassiPyGRB is a Python 3 package to download, process, visualize and classify GRBs database from the Swift/BAT Instrument (up to July 2022).","It is distributed over the GNU General Public License Version 2 (1991).","We also included a noise-reduction and an interpolation tools for achieving a deeper analysis of the data."],"url":"http://arxiv.org/abs/2404.06439v1","category":"astro-ph.HE"}
{"created":"2024-04-09 16:25:20","title":"Perspective on Physical Interpretations of R\u00e9nyi Entropy in Statistical Mechanics","abstract":"R\\'enyi entropy is a one-parameter generalization of Shannon entropy, which has been used in various fields of physics. Despite its wide applicability, the physical interpretations of the R\\'enyi entropy are not widely known. In this paper, we discuss some basic properties of the R\\'enyi entropy relevant to physics, in particular statistical mechanics, and its physical interpretations using free energy, replicas, work, and large deviation.","sentences":["R\\'enyi entropy is a one-parameter generalization of Shannon entropy, which has been used in various fields of physics.","Despite its wide applicability, the physical interpretations of the R\\'enyi entropy are not widely known.","In this paper, we discuss some basic properties of the R\\'enyi entropy relevant to physics, in particular statistical mechanics, and its physical interpretations using free energy, replicas, work, and large deviation."],"url":"http://arxiv.org/abs/2404.06436v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 16:25:02","title":"Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids -- A Gig Driver Case Study","abstract":"Decision aids based on artificial intelligence (AI) are becoming increasingly common. When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes. In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes. More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions? We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool. Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low. Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users.","sentences":["Decision aids based on artificial intelligence (AI) are becoming increasingly common.","When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes.","In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes.","More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions?","We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool.","Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low.","Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users."],"url":"http://arxiv.org/abs/2404.06432v1","category":"cs.HC"}
{"created":"2024-04-09 16:23:01","title":"pfl-research: simulation framework for accelerating research in Private Federated Learning","abstract":"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.","sentences":["Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants.","Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas.","However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets.","We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL.","It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms.","We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups.","Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive.","We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios.","The code is available on GitHub at https://github.com/apple/pfl-research."],"url":"http://arxiv.org/abs/2404.06430v1","category":"cs.LG"}
{"created":"2024-04-09 16:20:03","title":"Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion","abstract":"Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)","sentences":["Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently.","One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models.","However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries.","To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min).","Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images.","It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results.","Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details.","(Project Page: https://magic-research.github.io/magic-boost/)"],"url":"http://arxiv.org/abs/2404.06429v1","category":"cs.CV"}
{"created":"2024-04-09 16:18:13","title":"A universal sequence of tensors for the asymptotic rank conjecture","abstract":"The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers. Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture [Progr. Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank.","sentences":["The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers.","Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   ","Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture","[Progr.","Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank."],"url":"http://arxiv.org/abs/2404.06427v1","category":"cs.CC"}
{"created":"2024-04-09 16:14:45","title":"Detection of Contact Binary Candidates Observed By TESS Using Autoencoder Neural Network","abstract":"Contact binary may be the progenitor of a red nova that eventually produces a merger event and have a cut-off period around 0.2 days. Therefore, a large number of contact binaries is needed to search for the progenitor of red novae and to study the characteristics of short-period contact binaries. In this paper, we employ the Phoebe program to generate a large number of light curves based on the fundamental parameters of contact binaries. Using these light curves as samples, an autoencoder model is trained, which can reconstruct the light curves of contact binaries very well. When the error between the output light curve from the model and the input light curve is large, it may be due to other types of variable stars. The goodness of fit (R2) between the output light curve from the model and the input light curve is calculated. Based on the thresholds for global goodness of fit (R2), period, range magnitude, and local goodness of fit (R2), a total of 1322 target candidates were obtained.","sentences":["Contact binary may be the progenitor of a red nova that eventually produces a merger event and have a cut-off period around 0.2 days.","Therefore, a large number of contact binaries is needed to search for the progenitor of red novae and to study the characteristics of short-period contact binaries.","In this paper, we employ the Phoebe program to generate a large number of light curves based on the fundamental parameters of contact binaries.","Using these light curves as samples, an autoencoder model is trained, which can reconstruct the light curves of contact binaries very well.","When the error between the output light curve from the model and the input light curve is large, it may be due to other types of variable stars.","The goodness of fit (R2) between the output light curve from the model and the input light curve is calculated.","Based on the thresholds for global goodness of fit (R2), period, range magnitude, and local goodness of fit (R2), a total of 1322 target candidates were obtained."],"url":"http://arxiv.org/abs/2404.06424v1","category":"astro-ph.SR"}
{"created":"2024-04-09 16:14:03","title":"Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints","abstract":"This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority. Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot. The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge. We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics.","sentences":["This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority.","Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot.","The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge.","We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics."],"url":"http://arxiv.org/abs/2404.06423v1","category":"cs.RO"}
{"created":"2024-04-09 16:10:31","title":"Reconfigurable Multiple-Valued Logic Function and Sequential Circuit Realizations via Threshold Logic Gates","abstract":"In this paper, we present a general reconfigurable multiple-valued logic circuit. The proposed architecture is based on threshold logic gate and is compatible with binary logic, which allows a designer to easily integrate multiple valued logic with binary logic. We also present a methodology to design sequential circuits.","sentences":["In this paper, we present a general reconfigurable multiple-valued logic circuit.","The proposed architecture is based on threshold logic gate and is compatible with binary logic, which allows a designer to easily integrate multiple valued logic with binary logic.","We also present a methodology to design sequential circuits."],"url":"http://arxiv.org/abs/2404.06420v1","category":"eess.SP"}
{"created":"2024-04-09 16:09:19","title":"Signatures of Four Fermion Contact Couplings of a Dark Fermion and an Electron at Hadron Collider","abstract":"Both the collider searches and direct detections are promising approaches to probe a fermionic dark matter. In this paper we study signatures of the four fermion contact operators involving a dark fermion, an electron and a quark pair. We show that the mono-electron production channel at hadron collider can provide strong constraints. Associated productions of a charged electron with a photon/jet with missing energy are also studied. Using the current LHC data at $\\sqrt{s} = 13$\\,TeV, the lower bound on the energy scale of the (axial-)vector operator can reach to $12$\\,TeV for a massless dark fermion. It can be further improved to about $24$\\,TeV at the HE-LHC with $\\sqrt{s} = 25$\\,TeV and a total luminosity $20\\,{\\rm ab}^{-1}$. For the direct detections, the signal operators can generate induced $\\beta^\\pm$ decays. For the induced $\\beta^-$ decay, we show that the constraints are weaker than the ones from the collider searches in almost all of the parameter space, and the accessible parameter space is already excluded by the current LHC data. In case of a relative heavy dark fermion (a few MeV), the induced $\\beta^+$ decay is more sensitive than the collider search. Despite the advantage of the collider search that a much wider range of the dark fermion mass can be investigated, it can also provide a complementarity to the detect detections.","sentences":["Both the collider searches and direct detections are promising approaches to probe a fermionic dark matter.","In this paper we study signatures of the four fermion contact operators involving a dark fermion, an electron and a quark pair.","We show that the mono-electron production channel at hadron collider can provide strong constraints.","Associated productions of a charged electron with a photon/jet with missing energy are also studied.","Using the current LHC data at $\\sqrt{s} = 13$\\,TeV, the lower bound on the energy scale of the (axial-)vector operator can reach to $12$\\,TeV for a massless dark fermion.","It can be further improved to about $24$\\,TeV at the HE-LHC with $\\sqrt{s} = 25$\\,TeV and a total luminosity $20\\,{\\rm ab}^{-1}$.","For the direct detections, the signal operators can generate induced $\\beta^\\pm$ decays.","For the induced $\\beta^-$ decay, we show that the constraints are weaker than the ones from the collider searches in almost all of the parameter space, and the accessible parameter space is already excluded by the current LHC data.","In case of a relative heavy dark fermion (a few MeV), the induced $\\beta^+$ decay is more sensitive than the collider search.","Despite the advantage of the collider search that a much wider range of the dark fermion mass can be investigated, it can also provide a complementarity to the detect detections."],"url":"http://arxiv.org/abs/2404.06419v1","category":"hep-ph"}
{"created":"2024-04-09 16:07:35","title":"Studying the Impact of Latent Representations in Implicit Neural Networks for Scientific Continuous Field Reconstruction","abstract":"Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines. In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks. In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model. The adopted methods are general enough to be leveraged for any latent space inspection. Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance. As a work in progress, we will continue to verify our findings and develop novel explainability approaches.","sentences":["Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines.","In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks.","In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model.","The adopted methods are general enough to be leveraged for any latent space inspection.","Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance.","As a work in progress, we will continue to verify our findings and develop novel explainability approaches."],"url":"http://arxiv.org/abs/2404.06418v1","category":"cs.LG"}
{"created":"2024-04-09 16:03:42","title":"Constraints on the black-hole charges of M87* and Sagittarius A* by lensing rings","abstract":"From Event Horizon Telescope (EHT) and other observations on M87* and Sagittarius A* (Sgr A*), which are supermassive objects in M87 and our galaxy, respectively, we can put a constrain on the electrical or alternative charges of M87* and Sgr A*. The constraints on the charges by the EHT collaboration are based on the change rate of the radius of a photon sphere while their general relativistic magnetohydrodynamic simulations of black holes implies that the observed rings are formed by the gravitational lensing of synchrotron radiations from a hot plasma near supermassive black holes. On this paper, we investigate the constraints on the charges of M87* and Sgr A* by using the change rate of size of not the photon sphere but the lensing ring, which is the image of a disk edge at the innermost stable circular orbit of a particle, lensed nearly outside of the photon sphere as the simplest model. We show that the bounds of the charge by using the change rate of size of the lensing ring is more relaxed than the ones by the photon sphere obtained by the EHT collaboration. We concentrate on the Reissner-Nordstr\\\"{o}m black hole spacetime in this paper, but our result implies the relaxation of the bound of the charge parameters on other black hole spacetimes.","sentences":["From Event Horizon Telescope (EHT) and other observations on M87* and Sagittarius A* (Sgr A*), which are supermassive objects in M87 and our galaxy, respectively, we can put a constrain on the electrical or alternative charges of M87* and Sgr A*.","The constraints on the charges by the EHT collaboration are based on the change rate of the radius of a photon sphere while their general relativistic magnetohydrodynamic simulations of black holes implies that the observed rings are formed by the gravitational lensing of synchrotron radiations from a hot plasma near supermassive black holes.","On this paper, we investigate the constraints on the charges of M87* and Sgr A* by using the change rate of size of not the photon sphere but the lensing ring, which is the image of a disk edge at the innermost stable circular orbit of a particle, lensed nearly outside of the photon sphere as the simplest model.","We show that the bounds of the charge by using the change rate of size of the lensing ring is more relaxed than the ones by the photon sphere obtained by the EHT collaboration.","We concentrate on the Reissner-Nordstr\\\"{o}m black hole spacetime in this paper, but our result implies the relaxation of the bound of the charge parameters on other black hole spacetimes."],"url":"http://arxiv.org/abs/2404.06414v1","category":"gr-qc"}
{"created":"2024-04-09 16:02:48","title":"Gravitational wave seismology of charged strange stars in the Cowling approximation: the fluid pulsation modes","abstract":"In this work we study, within the framework of Cowling approximation, the effect of the electric charge on the gravitational wave frequency of fluid oscillation modes of strange quark stars. For this purpose, the dense matter of the stellar fluid is described by the MIT bag model equation of state (EoS), while for the electric charge profile, we consider that the electric charge density is proportional to the energy density. We find that the gravitational wave frequencies change with the increment of electric charge; these effects are more noticeable at higher total mass values. We obtain that the $f$-mode is very sensitive to the change in the electric charge of the star. Furthermore, in the case of the $p_1$ mode, the effect of the electric charge is not very significant. Our results reveal that the study of the fundamental pulsation mode of an electrically charged compact star is very important to distinguish whether compact stars could contain electric charge.","sentences":["In this work we study, within the framework of Cowling approximation, the effect of the electric charge on the gravitational wave frequency of fluid oscillation modes of strange quark stars.","For this purpose, the dense matter of the stellar fluid is described by the MIT bag model equation of state (EoS), while for the electric charge profile, we consider that the electric charge density is proportional to the energy density.","We find that the gravitational wave frequencies change with the increment of electric charge; these effects are more noticeable at higher total mass values.","We obtain that the $f$-mode is very sensitive to the change in the electric charge of the star.","Furthermore, in the case of the $p_1$ mode, the effect of the electric charge is not very significant.","Our results reveal that the study of the fundamental pulsation mode of an electrically charged compact star is very important to distinguish whether compact stars could contain electric charge."],"url":"http://arxiv.org/abs/2404.06412v1","category":"astro-ph.SR"}
{"created":"2024-04-09 16:01:24","title":"AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents","abstract":"The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.","sentences":["The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks.","As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress.","However, existing benchmarks are often narrow and simply compute overall task success.","To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task.","We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase.","Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest."],"url":"http://arxiv.org/abs/2404.06411v1","category":"cs.AI"}
{"created":"2024-04-09 15:55:54","title":"Module Categories As Spans","abstract":"We realize module functors and module natural transforms as spans of monoidal categories. We also discuss the generalizations to algebras and modules within an arbitrary monoidal 2-category, including $\\mathbf{2Vect}$, $\\mathbf{2Rep}(G)$, $\\mathbf{2Vect}^\\pi_G$, $\\mathbf{Mod}(\\mathcal{B})$, $\\mathbf{MCat}$ and $\\mathbf{BrCat}$.","sentences":["We realize module functors and module natural transforms as spans of monoidal categories.","We also discuss the generalizations to algebras and modules within an arbitrary monoidal 2-category, including $\\mathbf{2Vect}$, $\\mathbf{2Rep}(G)$, $\\mathbf{2Vect}^\\pi_G$, $\\mathbf{Mod}(\\mathcal{B})$, $\\mathbf{MCat}$ and $\\mathbf{BrCat}$."],"url":"http://arxiv.org/abs/2404.06408v1","category":"math.CT"}
{"created":"2024-04-09 15:54:16","title":"Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak","abstract":"Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not.   In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.","sentences":["Large language models (LLMs) have become increasingly integrated with various applications.","To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted.","However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak.","Different systems have been proposed to perform the jailbreak automatically.","These systems rely on evaluation methods to determine whether a jailbreak attempt is successful.","However, our analysis reveals that current jailbreak evaluation methods have two limitations.","(1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses.","(2) They oversimplify the jailbreak result as a binary outcome, successful or not.   ","In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak.","Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors.","To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response.","We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems.","The benchmark dataset is labeled by three annotators.","We compare our multifaceted approach with three existing jailbreak evaluation methods.","Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines.","Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model."],"url":"http://arxiv.org/abs/2404.06407v1","category":"cs.CL"}
{"created":"2024-04-09 15:54:00","title":"Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry","abstract":"Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills. Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning. The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten. In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong. Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods. This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem. Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist. (ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve. Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist.","sentences":["Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills.","Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning.","The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough.","It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten.","In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong.","Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods.","This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem.","Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist.","(ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve.","Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist."],"url":"http://arxiv.org/abs/2404.06405v1","category":"cs.AI"}
{"created":"2024-04-09 15:53:06","title":"Apprentices to Research Assistants: Advancing Research with Large Language Models","abstract":"Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in various research domains.","This article examines their potential through a literature review and firsthand experimentation.","While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed.","The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations.","Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise.","This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically.","By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research."],"url":"http://arxiv.org/abs/2404.06404v1","category":"cs.HC"}
{"created":"2024-04-09 15:53:02","title":"Online Learning of Decision Trees with Thompson Sampling","abstract":"Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting.","sentences":["Decision Trees are prominent prediction models for interpretable Machine Learning.","They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART.","Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees.","Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream.","To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting.","We analyse our algorithm and prove its almost sure convergence to the optimal tree.","Furthermore, we conduct extensive experiments to validate our findings empirically.","The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting."],"url":"http://arxiv.org/abs/2404.06403v1","category":"cs.LG"}
{"created":"2024-04-09 15:46:00","title":"Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations","abstract":"Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation. After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.","sentences":["Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution.","The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h.","Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow.","We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation.","After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh.","While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy."],"url":"http://arxiv.org/abs/2404.06400v1","category":"cs.LG"}
{"created":"2024-04-09 15:39:16","title":"Reconciling $S_8$: Insights from Interacting Dark Sectors","abstract":"We do a careful investigation of the prospects of dark energy (DE) interacting with cold dark matter (CDM) in alleviating the $S_8$ clustering tension. To this end, we consider various well-known parametrizations of the DE equation of state (EoS), and consider perturbations in both the dark sectors, along with an interaction term. Moreover, we perform a separate study for the phantom and non-phantom regimes. Using CMB, BAO and SNIa datasets, the constraints on the model parameters for each case have been obtained and a generic reduction in the $H_0-\\sigma_{8,0}$ correlation has been observed, both for constant and dynamical DE EoS. This reduction, coupled with a significant negative correlation between the interaction term and $\\sigma_{8,0}$, contributes to easing the clustering tension by lowering $\\sigma_{8,0}$ to somewhere in between the early CMB and late-time clustering measurements for the phantom regime, for almost all the models under consideration. In addition, this is achieved without exacerbating the Hubble tension. In this regard, the CPL and JBP models perform the best in relaxing the $S_8$ tension to $<1\\sigma$. However, for the non-phantom regime the $\\sigma_{8,0}$ tension tends to have worsened, which reassures the merits of phantom dark energy from latest data. We further do an investigation of the role of RSD datasets and find an overall reduction in tension, with a value of $\\sigma_{8,0}$ relatively closer to the CMB value. We finally check if further extensions of this scenario, like the inclusion of the sound speed of dark energy and warm dark matter interacting with DE, can have some effects.","sentences":["We do a careful investigation of the prospects of dark energy (DE) interacting with cold dark matter (CDM) in alleviating the $S_8$ clustering tension.","To this end, we consider various well-known parametrizations of the DE equation of state (EoS), and consider perturbations in both the dark sectors, along with an interaction term.","Moreover, we perform a separate study for the phantom and non-phantom regimes.","Using CMB, BAO and SNIa datasets, the constraints on the model parameters for each case have been obtained and a generic reduction in the $H_0-\\sigma_{8,0}$ correlation has been observed, both for constant and dynamical DE","EoS.","This reduction, coupled with a significant negative correlation between the interaction term and $\\sigma_{8,0}$, contributes to easing the clustering tension by lowering $\\sigma_{8,0}$ to somewhere in between the early CMB and late-time clustering measurements for the phantom regime, for almost all the models under consideration.","In addition, this is achieved without exacerbating the Hubble tension.","In this regard, the CPL and JBP models perform the best in relaxing the $S_8$ tension to $<1\\sigma$. However, for the non-phantom regime the $\\sigma_{8,0}$ tension tends to have worsened, which reassures the merits of phantom dark energy from latest data.","We further do an investigation of the role of RSD datasets and find an overall reduction in tension, with a value of $\\sigma_{8,0}$ relatively closer to the CMB value.","We finally check if further extensions of this scenario, like the inclusion of the sound speed of dark energy and warm dark matter interacting with DE, can have some effects."],"url":"http://arxiv.org/abs/2404.06396v1","category":"astro-ph.CO"}
{"created":"2024-04-09 15:36:16","title":"On the minimal memory set of cellular automata","abstract":"For a group $G$ and a finite set $A$, a cellular automaton (CA) is a transformation $\\tau : A^G \\to A^G$ defined via a finite memory set $S \\subseteq G$ and a local map $\\mu : A^S \\to A$. Although memory sets are not unique, every CA admits a unique minimal memory set, which consists on all the essential elements of $S$ that affect the behavior of the local map. In this paper, we study the links between the minimal memory set and the generating patterns $\\mathcal{P}$ of $\\mu$; these are the patterns in $A^S$ that are not fixed when the cellular automaton is applied. In particular, we show that when $\\vert \\mathcal{P} \\vert$ is not a multiple of $\\vert A \\vert$, then the minimal memory set is $S$ or $S \\setminus \\{e\\}$. Moreover, when $\\vert \\mathcal{P} \\vert = \\vert A \\vert$, and the action of $\\mu$ on these patterns is well-behaved, then the minimal memory set is $S$ or $S \\setminus \\{s\\}$, for some $s \\in S \\setminus \\{e\\}$. These are some of the first general theoretical results on the minimal memory set of a cellular automaton.","sentences":["For a group $G$ and a finite set $A$, a cellular automaton (CA) is a transformation $\\tau : A^G \\to A^G$ defined via a finite memory set $S \\subseteq G$ and a local map $\\mu : A^S \\to A$.","Although memory sets are not unique, every CA admits a unique minimal memory set, which consists on all the essential elements of $S$ that affect the behavior of the local map.","In this paper, we study the links between the minimal memory set and the generating patterns $\\mathcal{P}$ of $\\mu$; these are the patterns in $A^S$ that are not fixed when the cellular automaton is applied.","In particular, we show that when $\\vert \\mathcal{P} \\vert$ is not a multiple of $\\vert A \\vert$, then the minimal memory set is $S$ or $S \\setminus \\{e\\}$. Moreover, when $\\vert \\mathcal{P} \\vert = \\vert A \\vert$, and the action of $\\mu$ on these patterns is well-behaved, then the minimal memory set is $S$ or $S \\setminus \\{s\\}$, for some $s \\in S \\setminus \\{e\\}$. These are some of the first general theoretical results on the minimal memory set of a cellular automaton."],"url":"http://arxiv.org/abs/2404.06394v1","category":"nlin.CG"}
{"created":"2024-04-09 15:35:52","title":"MuPT: A Generative Symbolic Music Pretrained Transformer","abstract":"In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set. Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.","sentences":["In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music.","While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition.","To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks.","Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set.","Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance.","The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions."],"url":"http://arxiv.org/abs/2404.06393v1","category":"cs.SD"}
{"created":"2024-04-09 15:35:41","title":"Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis","abstract":"Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.","sentences":["Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language.","This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic.","We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages.","Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality.","Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer.","In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature.","In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting.","To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE).","The dataset and code are publicly available."],"url":"http://arxiv.org/abs/2404.06392v1","category":"cs.CL"}
{"created":"2024-04-09 15:33:09","title":"Latent Distance Guided Alignment Training for Large Language Models","abstract":"Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.","sentences":["Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs).","Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy.","The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods.","In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align).","This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space.","The latent space is generated through sample reconstruction, akin to auto-encoding.","Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training.","Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment."],"url":"http://arxiv.org/abs/2404.06390v1","category":"cs.CL"}
{"created":"2024-04-09 15:31:48","title":"Raster Forge: Interactive Raster Manipulation Library and GUI for Python","abstract":"Raster Forge is a Python library and graphical user interface for raster data manipulation and analysis. The tool is focused on remote sensing applications, particularly in wildfire management. It allows users to import, visualize, and process raster layers for tasks such as image compositing or topographical analysis. For wildfire management, it generates fuel maps using predefined models. Its impact extends from disaster management to hydrological modeling, agriculture, and environmental monitoring. Raster Forge can be a valuable asset for geoscientists and researchers who rely on raster data analysis, enhancing geospatial data processing and visualization across various disciplines.","sentences":["Raster Forge is a Python library and graphical user interface for raster data manipulation and analysis.","The tool is focused on remote sensing applications, particularly in wildfire management.","It allows users to import, visualize, and process raster layers for tasks such as image compositing or topographical analysis.","For wildfire management, it generates fuel maps using predefined models.","Its impact extends from disaster management to hydrological modeling, agriculture, and environmental monitoring.","Raster Forge can be a valuable asset for geoscientists and researchers who rely on raster data analysis, enhancing geospatial data processing and visualization across various disciplines."],"url":"http://arxiv.org/abs/2404.06389v1","category":"eess.IV"}
{"created":"2024-04-09 15:20:32","title":"Rainbow ortho-convex 4-sets in k-colored point sets","abstract":"Let $P$ be a $k$-colored set of $n$ points in the plane, $4 \\leq k \\leq n$. We study the problem of deciding if $P$ contains a subset of four points of different colors such that its Rectilinear Convex Hull has positive area. We provide an $O(n \\log n)$-time algorithm for this problem, where the hidden constant does not depend on $k$; then, we prove that this problem has time complexity $\\Omega(n \\log n)$ in the algebraic computation tree model. No general position assumptions for $P$ are required.","sentences":["Let $P$ be a $k$-colored set of $n$ points in the plane, $4 \\leq k \\leq n$. We study the problem of deciding if $P$ contains a subset of four points of different colors such that its Rectilinear Convex Hull has positive area.","We provide an $O(n \\log n)$-time algorithm for this problem, where the hidden constant does not depend on $k$; then, we prove that this problem has time complexity $\\Omega(n \\log n)$ in the algebraic computation tree model.","No general position assumptions for $P$ are required."],"url":"http://arxiv.org/abs/2404.06376v1","category":"cs.CG"}
{"created":"2024-04-09 15:18:50","title":"Emergent Modified Gravity","abstract":"A complete canonical formulation of general covariance makes it possible to construct new modified theories of gravity that are not of higher-curvature form, as shown here in a spherically symmetric setting. The usual uniqueness theorems are evaded by using a crucial and novel ingredient, allowing for fundamental fields of gravity distinct from an emergent space-time metric that provides a geometrical structure to all solutions. As specific examples, there are new expansion-shear couplings in cosmological models, a form of modified Newtonian dynamics (MOND) can appear in a space-time covariant theory without introducing extra fields, and related effects help to make effective models of canonical quantum gravity fully consistent with general covariance.","sentences":["A complete canonical formulation of general covariance makes it possible to construct new modified theories of gravity that are not of higher-curvature form, as shown here in a spherically symmetric setting.","The usual uniqueness theorems are evaded by using a crucial and novel ingredient, allowing for fundamental fields of gravity distinct from an emergent space-time metric that provides a geometrical structure to all solutions.","As specific examples, there are new expansion-shear couplings in cosmological models, a form of modified Newtonian dynamics (MOND) can appear in a space-time covariant theory without introducing extra fields, and related effects help to make effective models of canonical quantum gravity fully consistent with general covariance."],"url":"http://arxiv.org/abs/2404.06375v1","category":"gr-qc"}
{"created":"2024-04-09 15:17:48","title":"Synchronization in Modern Heterogeneous Power Networks with Inverter-Based Resources","abstract":"Synchronized operation of generators in a power network is paramount to the stability and reliability of energy delivery. In this paper, we address the synchronization problem in a heterogeneous power grid, consisting of both synchronous generators and inverter-based generators, and derive the necessary and sufficient condition guaranteeing the existence of a unique locally stable synchronized mode. Further, our results have implications for utilizing grid-following inverters versus grid-forming inverters in order to enhance network stability. This work is of particular importance as power grids around the world are undergoing a transition from being predominantly composed of synchronous generators towards a grid consisting of a preponderance of inverter based generators.","sentences":["Synchronized operation of generators in a power network is paramount to the stability and reliability of energy delivery.","In this paper, we address the synchronization problem in a heterogeneous power grid, consisting of both synchronous generators and inverter-based generators, and derive the necessary and sufficient condition guaranteeing the existence of a unique locally stable synchronized mode.","Further, our results have implications for utilizing grid-following inverters versus grid-forming inverters in order to enhance network stability.","This work is of particular importance as power grids around the world are undergoing a transition from being predominantly composed of synchronous generators towards a grid consisting of a preponderance of inverter based generators."],"url":"http://arxiv.org/abs/2404.06374v1","category":"eess.SY"}
{"created":"2024-04-09 15:07:25","title":"Model Generation from Requirements with LLMs: an Exploratory Study","abstract":"Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.","sentences":["Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design.","However, creating models from requirements involves manual effort.","The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation.","This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements.","We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains.","Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis.","Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges.","This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency.","The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation."],"url":"http://arxiv.org/abs/2404.06371v1","category":"cs.SE"}
{"created":"2024-04-09 15:06:25","title":"Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python","abstract":"Purpose: Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments. In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation. In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions. Findings: Large Language Models are undeniably potent but can sometimes be a double-edged sword. Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise. It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field. Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods.","sentences":["Purpose:","Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments.","In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods.","Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families.","Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation.","In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions.","Findings:","Large Language Models are undeniably potent but can sometimes be a double-edged sword.","Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise.","It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field.","Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods."],"url":"http://arxiv.org/abs/2404.06370v1","category":"cs.AI"}
{"created":"2024-04-09 15:05:48","title":"VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs","abstract":"Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams. Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks. Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility. Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation. To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation. Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset. In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances. Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code. The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui.","sentences":["Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams.","Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks.","Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility.","Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation.","To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation.","Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset.","In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances.","Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code.","The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui."],"url":"http://arxiv.org/abs/2404.06369v1","category":"cs.CV"}
{"created":"2024-04-09 15:05:16","title":"Replacing bar-like resolutions in a simplicial setting","abstract":"It is well known that the bar resolution can be replaced with any projective resolution of the corresponding algebra when computing the Hochschild (co)homology of that algebra. This is, in fact, a feature of its construction via derived functors. For generalizations and extensions of the Hochschild (co)homology, one uses a bar-like resolution in a simplicial setting in order to accommodate the changing module structures in every dimension. In this note, we present a method in order to replace these bar-like resolutions.","sentences":["It is well known that the bar resolution can be replaced with any projective resolution of the corresponding algebra when computing the Hochschild (co)homology of that algebra.","This is, in fact, a feature of its construction via derived functors.","For generalizations and extensions of the Hochschild (co)homology, one uses a bar-like resolution in a simplicial setting in order to accommodate the changing module structures in every dimension.","In this note, we present a method in order to replace these bar-like resolutions."],"url":"http://arxiv.org/abs/2404.06368v1","category":"math.RA"}
{"created":"2024-04-09 15:04:27","title":"ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish","abstract":"Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.","sentences":["Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis.","This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish.","This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose.","This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data.","Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes.","These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records.","The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest."],"url":"http://arxiv.org/abs/2404.06367v1","category":"cs.CL"}
{"created":"2024-04-09 14:59:13","title":"Minimizing the determinant of the graph Laplacian","abstract":"In this paper, we study extremal values for the determinant of the weighted graph Laplacian under simple nondegeneracy conditions on the weights. We derive necessary and sufficient conditions for the determinant of the Laplacian to be bounded away from zero and for the existence of a minimizing set of weights. These conditions are given both in terms of properties of random spanning trees and in terms of a type of density on graphs. These results generalize and extend the work of [7].","sentences":["In this paper, we study extremal values for the determinant of the weighted graph Laplacian under simple nondegeneracy conditions on the weights.","We derive necessary and sufficient conditions for the determinant of the Laplacian to be bounded away from zero and for the existence of a minimizing set of weights.","These conditions are given both in terms of properties of random spanning trees and in terms of a type of density on graphs.","These results generalize and extend the work of [7]."],"url":"http://arxiv.org/abs/2404.06363v1","category":"math.CO"}
{"created":"2024-04-09 14:56:34","title":"Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation","abstract":"The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM. Code and text prompts will be available online.","sentences":["The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs).","SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities.","However, their unified potential has not yet been explored in medical image segmentation.","To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available.","This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation.","Specifically, we propose a simple unified framework, SaLIP, for organ segmentation.","Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks.","Finally, SAM is prompted by the retrieved ROI to segment a specific organ.","Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering.","Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM.","Code and text prompts will be available online."],"url":"http://arxiv.org/abs/2404.06362v1","category":"cs.CV"}
{"created":"2024-04-09 14:55:48","title":"Meaningfulness and Genericity in a Subsuming Framework","abstract":"This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV). We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature. We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms. The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension. Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus.","sentences":["This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV).","We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature.","We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms.","The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension.","Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus."],"url":"http://arxiv.org/abs/2404.06361v1","category":"cs.LO"}
{"created":"2024-04-09 14:53:59","title":"Towards Practical Meshlet Compression","abstract":"We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader. Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP). Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference. The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX.","sentences":["We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader.","Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP).","Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference.","The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX."],"url":"http://arxiv.org/abs/2404.06359v1","category":"cs.GR"}
{"created":"2024-04-09 14:48:49","title":"Elements with unique length factorization of a numerical semigroup generated by three consecutive numbers","abstract":"Let $S$ be the numerical semigroup generated by three consecutive numbers $a,a+1,a+2$, where $a\\in\\mathbb{N}$, $a\\geq 3$. We describe the elements of $S$ whose factorizations have all the same length, as well as the set of factorizations of each of these elements. We give natural partitions of this subset of $S$ in terms of the length and the denumerant. By using Ap\\'ery sets and Betti elements we are able to extend some results, first obtained by elementary means.","sentences":["Let $S$ be the numerical semigroup generated by three consecutive numbers $a,a+1,a+2$, where $a\\in\\mathbb{N}$, $a\\geq 3$.","We describe the elements of $S$ whose factorizations have all the same length, as well as the set of factorizations of each of these elements.","We give natural partitions of this subset of $S$ in terms of the length and the denumerant.","By using Ap\\'ery sets and Betti elements we are able to extend some results, first obtained by elementary means."],"url":"http://arxiv.org/abs/2404.06358v1","category":"math.CO"}
{"created":"2024-04-09 14:48:32","title":"Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!","abstract":"We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking). We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset). For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels. For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains. Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets. With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles. We argue that future sarcasm research should take the broad scope of sarcasm into account.","sentences":["We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking).","We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset).","For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels.","For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains.","Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets.","With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles.","We argue that future sarcasm research should take the broad scope of sarcasm into account."],"url":"http://arxiv.org/abs/2404.06357v1","category":"cs.CL"}
{"created":"2024-04-09 14:46:48","title":"Policy-Guided Diffusion","abstract":"In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.","sentences":["In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy.","Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias.","Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience.","However, in practice, model rollouts must be severely truncated to avoid compounding error.","As an alternative, we propose policy-guided diffusion.","Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy.","We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline.","Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments.","Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data."],"url":"http://arxiv.org/abs/2404.06356v1","category":"cs.LG"}
{"created":"2024-04-09 14:44:12","title":"High Noise Scheduling is a Must","abstract":"Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques. Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training. Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum. In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability. This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm. Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising. To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution. The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique. The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps. Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48.","sentences":["Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques.","Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training.","Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum.","In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability.","This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm.","Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising.","To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution.","The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique.","The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps.","Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48."],"url":"http://arxiv.org/abs/2404.06353v1","category":"cs.LG"}
{"created":"2024-04-09 14:43:19","title":"DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View Segmentation with Occlusion Reasoning","abstract":"Semantic segmentation is an effective way to perform scene understanding. Recently, segmentation in 3D Bird's Eye View (BEV) space has become popular as its directly used by drive policy. However, there is limited work on BEV segmentation for surround-view fisheye cameras, commonly used in commercial vehicles. As this task has no real-world public dataset and existing synthetic datasets do not handle amodal regions due to occlusion, we create a synthetic dataset using the Cognata simulator comprising diverse road types, weather, and lighting conditions. We generalize the BEV segmentation to work with any camera model; this is useful for mixing diverse cameras. We implement a baseline by applying cylindrical rectification on the fisheye images and using a standard LSS-based BEV segmentation model. We demonstrate that we can achieve better performance without undistortion, which has the adverse effects of increased runtime due to pre-processing, reduced field-of-view, and resampling artifacts. Further, we introduce a distortion-aware learnable BEV pooling strategy that is more effective for the fisheye cameras. We extend the model with an occlusion reasoning module, which is critical for estimating in BEV space. Qualitative performance of DaF-BEVSeg is showcased in the video at https://streamable.com/ge4v51.","sentences":["Semantic segmentation is an effective way to perform scene understanding.","Recently, segmentation in 3D Bird's Eye View (BEV) space has become popular as its directly used by drive policy.","However, there is limited work on BEV segmentation for surround-view fisheye cameras, commonly used in commercial vehicles.","As this task has no real-world public dataset and existing synthetic datasets do not handle amodal regions due to occlusion, we create a synthetic dataset using the Cognata simulator comprising diverse road types, weather, and lighting conditions.","We generalize the BEV segmentation to work with any camera model; this is useful for mixing diverse cameras.","We implement a baseline by applying cylindrical rectification on the fisheye images and using a standard LSS-based BEV segmentation model.","We demonstrate that we can achieve better performance without undistortion, which has the adverse effects of increased runtime due to pre-processing, reduced field-of-view, and resampling artifacts.","Further, we introduce a distortion-aware learnable BEV pooling strategy that is more effective for the fisheye cameras.","We extend the model with an occlusion reasoning module, which is critical for estimating in BEV space.","Qualitative performance of DaF-BEVSeg is showcased in the video at https://streamable.com/ge4v51."],"url":"http://arxiv.org/abs/2404.06352v1","category":"cs.CV"}
{"created":"2024-04-09 14:42:31","title":"HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention","abstract":"Predicting the trajectories of road agents is essential for autonomous driving systems. The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames. These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency. As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation. Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method. Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions. Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions. Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions. The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories. Our code are available at https://github.com/XiaolongTang23/HPNet.","sentences":["Predicting the trajectories of road agents is essential for autonomous driving systems.","The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames.","These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency.","As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation.","Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method.","Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions.","Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions.","Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions.","The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.","Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories.","Our code are available at https://github.com/XiaolongTang23/HPNet."],"url":"http://arxiv.org/abs/2404.06351v1","category":"cs.CV"}
{"created":"2024-04-09 14:40:08","title":"CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models","abstract":"Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals. With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention. However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous. To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs. Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms. Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty. Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization. Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects. Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios. Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures. Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains.","sentences":["Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals.","With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention.","However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous.","To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs.","Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms.","Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty.","Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization.","Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects.","Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios.","Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures.","Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains."],"url":"http://arxiv.org/abs/2404.06349v1","category":"cs.LG"}
{"created":"2024-04-09 14:34:48","title":"RAR-b: Reasoning as Retrieval Benchmark","abstract":"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.","sentences":["Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years.","Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them.","Addressing this, we pose the question: Can retrievers solve reasoning problems?","By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks.","Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align.","However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding.","We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model.","We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models.","RAR-b is available at https://github.com/gowitheflow-1998/RAR-b."],"url":"http://arxiv.org/abs/2404.06347v1","category":"cs.CL"}
{"created":"2024-04-09 14:33:16","title":"AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning","abstract":"Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.","sentences":["Connected and autonomous driving is developing rapidly in recent years.","However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities.","In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems.","In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.","AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module.","It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning.","In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments.","Extensive experiments are conducted and show the superiority of AgentsCoDriver."],"url":"http://arxiv.org/abs/2404.06345v1","category":"cs.AI"}
{"created":"2024-04-09 14:33:03","title":"Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design","abstract":"We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices. To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability. Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models.","sentences":["We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices.","To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability.","Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models."],"url":"http://arxiv.org/abs/2404.06344v1","category":"cs.NE"}
{"created":"2024-04-09 14:24:49","title":"Statistical Estimation of Mean Lorentzian Line Width in Spectra by Gaussian Processes","abstract":"We propose a statistical approach for estimating the mean line width in spectra comprising Lorentzian, Gaussian, or Voigt line shapes. Our approach uses Gaussian processes in two stages to jointly model a spectrum and its Fourier transform. We generate statistical samples for the mean line width by drawing realizations for the Fourier transform and its derivative using Markov chain Monte Carlo methods. In addition to being fully automated, our method enables well-calibrated uncertainty quantification of the mean line width estimate through Bayesian inference. We validate our method using a simulation study and apply it to an experimental Raman spectrum of $\\beta$-carotene.","sentences":["We propose a statistical approach for estimating the mean line width in spectra comprising Lorentzian, Gaussian, or Voigt line shapes.","Our approach uses Gaussian processes in two stages to jointly model a spectrum and its Fourier transform.","We generate statistical samples for the mean line width by drawing realizations for the Fourier transform and its derivative using Markov chain Monte Carlo methods.","In addition to being fully automated, our method enables well-calibrated uncertainty quantification of the mean line width estimate through Bayesian inference.","We validate our method using a simulation study and apply it to an experimental Raman spectrum of $\\beta$-carotene."],"url":"http://arxiv.org/abs/2404.06338v1","category":"stat.AP"}
{"created":"2024-04-09 14:21:51","title":"Quantum State Generation with Structure-Preserving Diffusion Model","abstract":"This article considers the generative modeling of the states of quantum systems, and an approach based on denoising diffusion model is proposed. The key contribution is an algorithmic innovation that respects the physical nature of quantum states. More precisely, the commonly used density matrix representation of mixed-state has to be complex-valued Hermitian, positive semi-definite, and trace one. Generic diffusion models, or other generative methods, may not be able to generate data that strictly satisfy these structural constraints, even if all training data do. To develop a machine learning algorithm that has physics hard-wired in, we leverage the recent development of Mirror Diffusion Model and design a previously unconsidered mirror map, to enable strict structure-preserving generation. Both unconditional generation and conditional generation via classifier-free guidance are experimentally demonstrated efficacious, the latter even enabling the design of new quantum states when generated on unseen labels.","sentences":["This article considers the generative modeling of the states of quantum systems, and an approach based on denoising diffusion model is proposed.","The key contribution is an algorithmic innovation that respects the physical nature of quantum states.","More precisely, the commonly used density matrix representation of mixed-state has to be complex-valued Hermitian, positive semi-definite, and trace one.","Generic diffusion models, or other generative methods, may not be able to generate data that strictly satisfy these structural constraints, even if all training data do.","To develop a machine learning algorithm that has physics hard-wired in, we leverage the recent development of Mirror Diffusion Model and design a previously unconsidered mirror map, to enable strict structure-preserving generation.","Both unconditional generation and conditional generation via classifier-free guidance are experimentally demonstrated efficacious, the latter even enabling the design of new quantum states when generated on unseen labels."],"url":"http://arxiv.org/abs/2404.06336v1","category":"quant-ph"}
{"created":"2024-04-09 14:08:47","title":"Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning","abstract":"The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data. After training, the SR algorithm based on reinforcement learning is distilled into a Transformer. When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.","sentences":["The mathematical formula is the human language to describe nature and is the essence of scientific research.","Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence.","This area is called symbolic regression.","Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms.","These two kinds of algorithms have strong noise robustness ability and good Versatility.","However, inference time usually takes a long time, so the search efficiency is relatively low.","Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT).","Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast.","However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods.","So, can we combine the advantages of the above two categories of SR algorithms?","In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data.","After training, the SR algorithm based on reinforcement learning is distilled into a Transformer.","When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context.","Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines.","In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency."],"url":"http://arxiv.org/abs/2404.06330v1","category":"cs.LG"}
{"created":"2024-04-09 14:04:33","title":"Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs","abstract":"CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment. CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification. A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables). In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications. Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants. Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability. We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer.","sentences":["CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment.","CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification.","A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables).","In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   ","Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications.","Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants.","Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability.","We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer."],"url":"http://arxiv.org/abs/2404.06327v1","category":"cs.DS"}
{"created":"2024-04-09 14:04:26","title":"What is the $\\textit{intrinsic}$ dimension of your binary data? -- and how to compute it quickly","abstract":"Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension. In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets. To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value. We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions.","sentences":["Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data.","In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension.","In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets.","To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value.","We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions."],"url":"http://arxiv.org/abs/2404.06326v1","category":"cs.LG"}
{"created":"2024-04-09 14:03:38","title":"Automatically Learning HTN Methods from Landmarks","abstract":"Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem. Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn. We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process. It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex. This eliminates the need for manual input, resolving a core issue with HTN-MAKER. We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER.","sentences":["Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem.","Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn.","We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process.","It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex.","This eliminates the need for manual input, resolving a core issue with HTN-MAKER.","We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER."],"url":"http://arxiv.org/abs/2404.06325v1","category":"cs.AI"}
{"created":"2024-04-09 14:03:04","title":"Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection","abstract":"Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation). However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets. In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users. The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy. We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN). DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection. We provide extensive theoretical analysis to study the convergence of DCLM. We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem. We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems. We show the efficiency of DCLM via numerical simulations and provide a series of future directions.","sentences":["Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation).","However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets.","In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users.","The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy.","We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN).","DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection.","We provide extensive theoretical analysis to study the convergence of DCLM.","We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem.","We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems.","We show the efficiency of DCLM via numerical simulations and provide a series of future directions."],"url":"http://arxiv.org/abs/2404.06324v1","category":"cs.NI"}
{"created":"2024-04-09 13:59:01","title":"Commutative Properties of Schubert Puzzles with Convex Polygonal Boundary Shapes","abstract":"We generalize classical triangular Schubert puzzles to puzzles with convex polygonal boundary. We give these puzzles a geometric Schubert calculus interpretation and derive novel combinatorial commutativity statements, using purely geometric arguments, for puzzles with four, five, and six sides, having various types of symmetry in their boundary conditions. We also present formulas for the associated structure constants in terms of Littlewood-Richardson numbers, and we prove an analogue of commutativity for parallelogram-shaped equivariant puzzles.","sentences":["We generalize classical triangular Schubert puzzles to puzzles with convex polygonal boundary.","We give these puzzles a geometric Schubert calculus interpretation and derive novel combinatorial commutativity statements, using purely geometric arguments, for puzzles with four, five, and six sides, having various types of symmetry in their boundary conditions.","We also present formulas for the associated structure constants in terms of Littlewood-Richardson numbers, and we prove an analogue of commutativity for parallelogram-shaped equivariant puzzles."],"url":"http://arxiv.org/abs/2404.06320v1","category":"math.CO"}
{"created":"2024-04-09 13:44:16","title":"DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level","abstract":"Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.However, the lack of transparency in presenting recommendations can lead to user confusion. In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews. Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item.","sentences":["Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.","However, the lack of transparency in presenting recommendations can lead to user confusion.","In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.","Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.","We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.","Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews.","Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item."],"url":"http://arxiv.org/abs/2404.06311v1","category":"cs.IR"}
{"created":"2024-04-09 13:39:37","title":"Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models","abstract":"Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models. However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP. In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features. Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system. We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features. Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features. Code and data available at: https://github.com/dkurzend/ClipClap-GZSL.","sentences":["Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models.","However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP.","In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features.","Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system.","We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features.","Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features.","Code and data available at: https://github.com/dkurzend/ClipClap-GZSL."],"url":"http://arxiv.org/abs/2404.06309v1","category":"cs.CV"}
{"created":"2024-04-09 13:39:34","title":"A generalization of concise words","abstract":"The study of verbal subgroups within a group is well-known for being an effective tool to obtain structural information about a group. Therefore, conditions that allow the classification of words in a free group are of paramount importance. One of the most studied problems is to establish which words are concise, where a word $w$ is said to be concise if the verbal subgroup $w(G)$ is finite in each group $G$ in which $w$ takes only a finite number of values.   The purpose of this article is to present some results, in which a hierarchy among words is introduced, generalizing the concept of concise word.","sentences":["The study of verbal subgroups within a group is well-known for being an effective tool to obtain structural information about a group.","Therefore, conditions that allow the classification of words in a free group are of paramount importance.","One of the most studied problems is to establish which words are concise, where a word $w$ is said to be concise if the verbal subgroup $w(G)$ is finite in each group $G$ in which $w$ takes only a finite number of values.   ","The purpose of this article is to present some results, in which a hierarchy among words is introduced, generalizing the concept of concise word."],"url":"http://arxiv.org/abs/2404.06308v1","category":"math.GR"}
{"created":"2024-04-09 13:35:15","title":"Disproof of the Riemann Hypothesis","abstract":"The Riemann Hypothesis is a conjecture that all non-trivial zeros of Riemann Zeta function are located on the critical line in the complex plane. Hundreds of propositions in function theory and analytic number theory rely on this hypothesis. However, the problem has been unresolved for over a century. Here we show that at least one set of quadruplet-zeros exists outside the critical line through expanding the infinite product of the Riemann Xi zero function. We found that assuming there are no zeros outside the critical line will result in a contradiction with the known result that the reciprocal sum of all zeros of the xi-function is a constant, thereby refuting the Riemann Hypothesis.","sentences":["The Riemann Hypothesis is a conjecture that all non-trivial zeros of Riemann Zeta function are located on the critical line in the complex plane.","Hundreds of propositions in function theory and analytic number theory rely on this hypothesis.","However, the problem has been unresolved for over a century.","Here we show that at least one set of quadruplet-zeros exists outside the critical line through expanding the infinite product of the Riemann Xi zero function.","We found that assuming there are no zeros outside the critical line will result in a contradiction with the known result that the reciprocal sum of all zeros of the xi-function is a constant, thereby refuting the Riemann Hypothesis."],"url":"http://arxiv.org/abs/2404.06306v1","category":"math.GM"}
{"created":"2024-04-09 13:30:10","title":"Recovering a Magnitude-Symmetric Matrix from its Principal Minors","abstract":"We consider the inverse problem of finding a magnitude-symmetric matrix (matrix with opposing off-diagonal entries equal in magnitude) with a prescribed set of principal minors. This problem is closely related to the theory of recognizing and learning signed determinantal point processes in machine learning, as kernels of these point processes are magnitude-symmetric matrices. In this work, we prove a number of properties regarding sparse and generic magnitude-symmetric matrices. We show that principal minors of order at most $\\ell$, for some invariant $\\ell$ depending only on principal minors of order at most two, uniquely determines principal minors of all orders. In addition, we produce a polynomial-time algorithm that, given access to principal minors, recovers a matrix with those principal minors using only a quadratic number of queries. Furthermore, when principal minors are known only approximately, we present an algorithm that approximately recovers a matrix, and show that the approximation guarantee of this algorithm cannot be improved in general.","sentences":["We consider the inverse problem of finding a magnitude-symmetric matrix (matrix with opposing off-diagonal entries equal in magnitude) with a prescribed set of principal minors.","This problem is closely related to the theory of recognizing and learning signed determinantal point processes in machine learning, as kernels of these point processes are magnitude-symmetric matrices.","In this work, we prove a number of properties regarding sparse and generic magnitude-symmetric matrices.","We show that principal minors of order at most $\\ell$, for some invariant $\\ell$ depending only on principal minors of order at most two, uniquely determines principal minors of all orders.","In addition, we produce a polynomial-time algorithm that, given access to principal minors, recovers a matrix with those principal minors using only a quadratic number of queries.","Furthermore, when principal minors are known only approximately, we present an algorithm that approximately recovers a matrix, and show that the approximation guarantee of this algorithm cannot be improved in general."],"url":"http://arxiv.org/abs/2404.06302v1","category":"math.CO"}
{"created":"2024-04-09 13:25:48","title":"Inertia emulation contribution of Frades 2 variable speed pump-turbine to power network stability","abstract":"This paper is addressing the quantification and the comparison of pumped storage power plants, PSPP, contribution to synchronous inertia and synthetic inertia when fixed speed and variable speed motor-generators technologies are considered, respectively. Therefore, a grid stability study was conducted by means of 1D SIMSEN simulation for the 2 x 395 MW PSPP Frades 2 in Portugal with both fixed speed and variable speed technologies in case of operation connected to an infinite power network or to an islanded 4.4 GW synchronous power network.","sentences":["This paper is addressing the quantification and the comparison of pumped storage power plants, PSPP, contribution to synchronous inertia and synthetic inertia when fixed speed and variable speed motor-generators technologies are considered, respectively.","Therefore, a grid stability study was conducted by means of 1D SIMSEN simulation for the 2 x 395 MW PSPP Frades 2 in Portugal with both fixed speed and variable speed technologies in case of operation connected to an infinite power network or to an islanded 4.4 GW synchronous power network."],"url":"http://arxiv.org/abs/2404.06299v1","category":"eess.SY"}
{"created":"2024-04-09 13:23:12","title":"Complete stability for spherically symmetric backgrounds in beyond Horndeski theory","abstract":"We consider a general static, spherically symmetric background in the quadratic beyond Horndeski theory and analyse the behaviour of linear perturbations in both parity odd and parity even sectors. We derive a full set of stability conditions for an arbitrary static, spherically symmetric solution which guarantees absence of ghosts, gradient instabilities, tachyons and superluminal modes in both sectors.","sentences":["We consider a general static, spherically symmetric background in the quadratic beyond Horndeski theory and analyse the behaviour of linear perturbations in both parity odd and parity even sectors.","We derive a full set of stability conditions for an arbitrary static, spherically symmetric solution which guarantees absence of ghosts, gradient instabilities, tachyons and superluminal modes in both sectors."],"url":"http://arxiv.org/abs/2404.06297v1","category":"gr-qc"}
{"created":"2024-04-09 13:22:55","title":"Calculation of toroidal Alfv\u00e9n eigenmode mode structure in general axisymmetric toroidal geometry","abstract":"A workflow is developed based on the ideal MHD model to investigate the linear physics of various Alfv\\'en eigenmodes in general axisymmetric toroidal geometry, by solving the coupled shear Alfv\\'en wave (SAW) and ion sound wave (ISW) equations in ballooning space. The model equations are solved by the FALCON code in the singular layer, and the corresponding solutions are then taken as the boundary conditions for calculating parallel mode structures in the whole ballooning space. As an application of the code, the frequencies and mode structures of toroidal Alfv\\'en eigenmode (TAE) are calculated in the reference equilibria of the Divertor Tokamak Test facility (DTT) with positive and negative triangularities, respectively. By properly handling the boundary conditions, we demonstrate finite TAE damping due to coupling with the local acoustic continuum, and find that the damping rate is small for typical plasma parameters.","sentences":["A workflow is developed based on the ideal MHD model to investigate the linear physics of various Alfv\\'en eigenmodes in general axisymmetric toroidal geometry, by solving the coupled shear Alfv\\'en wave (SAW) and ion sound wave (ISW) equations in ballooning space.","The model equations are solved by the FALCON code in the singular layer, and the corresponding solutions are then taken as the boundary conditions for calculating parallel mode structures in the whole ballooning space.","As an application of the code, the frequencies and mode structures of toroidal Alfv\\'en eigenmode (TAE) are calculated in the reference equilibria of the Divertor Tokamak Test facility (DTT) with positive and negative triangularities, respectively.","By properly handling the boundary conditions, we demonstrate finite TAE damping due to coupling with the local acoustic continuum, and find that the damping rate is small for typical plasma parameters."],"url":"http://arxiv.org/abs/2404.06296v1","category":"physics.plasm-ph"}
{"created":"2024-04-09 13:19:43","title":"Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures","abstract":"Super-Resolution (SR) is a time-hallowed image processing problem that aims to improve the quality of a Low-Resolution (LR) sample up to the standard of its High-Resolution (HR) counterpart. We aim to address this by introducing Super-Resolution Generator (SuRGe), a fully-convolutional Generative Adversarial Network (GAN)-based architecture for SR. We show that distinct convolutional features obtained at increasing depths of a GAN generator can be optimally combined by a set of learnable convex weights to improve the quality of generated SR samples. In the process, we employ the Jensen-Shannon and the Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of distributions to further aid the generator of SuRGe to better exploit the available information in an attempt to improve SR. Moreover, we train the discriminator of SuRGe with the Wasserstein loss with gradient penalty, to primarily prevent mode collapse. The proposed SuRGe, as an end-to-end GAN workflow tailor-made for super-resolution, offers improved performance while maintaining low inference time. The efficacy of SuRGe is substantiated by its superior performance compared to 18 state-of-the-art contenders on 10 benchmark datasets.","sentences":["Super-Resolution (SR) is a time-hallowed image processing problem that aims to improve the quality of a Low-Resolution (LR) sample up to the standard of its High-Resolution (HR) counterpart.","We aim to address this by introducing Super-Resolution Generator (SuRGe), a fully-convolutional Generative Adversarial Network (GAN)-based architecture for SR.","We show that distinct convolutional features obtained at increasing depths of a GAN generator can be optimally combined by a set of learnable convex weights to improve the quality of generated SR samples.","In the process, we employ the Jensen-Shannon and the Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of distributions to further aid the generator of SuRGe to better exploit the available information in an attempt to improve SR.","Moreover, we train the discriminator of SuRGe with the Wasserstein loss with gradient penalty, to primarily prevent mode collapse.","The proposed SuRGe, as an end-to-end GAN workflow tailor-made for super-resolution, offers improved performance while maintaining low inference time.","The efficacy of SuRGe is substantiated by its superior performance compared to 18 state-of-the-art contenders on 10 benchmark datasets."],"url":"http://arxiv.org/abs/2404.06294v1","category":"eess.IV"}
{"created":"2024-04-09 13:18:44","title":"Computer-Assisted Global Analysis for Vibro-Impact Dynamics: A Reduced Smooth Maps Approach","abstract":"We present a novel approach for studying the global dynamics of a vibro-impact pair, that is, a ball moving in a harmonically forced capsule. Motivated by a specific context of vibro-impact energy harvesting, we develop the method with broader non-smooth systems in mind. The seeming complications of the impacts of the ball with the capsule are exploited as useful non-smooth features in selecting appropriate return maps. This choice yields a computationally efficient framework for constructing return maps on short-time realizations from the state space of possible initial conditions rather than via long-time simulations often used to generate more traditional maps. The different dynamics in sub-regions in the state space yield a small collection of reduced polynomial approximations. Combined into a piecewise composite map, these capture transient and attracting behaviors and reproduce bifurcation sequences of the full system. Further ``separable'' reductions of the composite map provide insight into both transient and global dynamics. This composite map is valuable for cobweb analysis, which opens the door to computer-assisted global analysis and is realized via conservative auxiliary maps based on the extreme bounds of the maps in each subregion. We study the global dynamics of energetically favorable states and illustrate the potential of this approach in broader classes of dynamics.","sentences":["We present a novel approach for studying the global dynamics of a vibro-impact pair, that is, a ball moving in a harmonically forced capsule.","Motivated by a specific context of vibro-impact energy harvesting, we develop the method with broader non-smooth systems in mind.","The seeming complications of the impacts of the ball with the capsule are exploited as useful non-smooth features in selecting appropriate return maps.","This choice yields a computationally efficient framework for constructing return maps on short-time realizations from the state space of possible initial conditions rather than via long-time simulations often used to generate more traditional maps.","The different dynamics in sub-regions in the state space yield a small collection of reduced polynomial approximations.","Combined into a piecewise composite map, these capture transient and attracting behaviors and reproduce bifurcation sequences of the full system.","Further ``separable'' reductions of the composite map provide insight into both transient and global dynamics.","This composite map is valuable for cobweb analysis, which opens the door to computer-assisted global analysis and is realized via conservative auxiliary maps based on the extreme bounds of the maps in each subregion.","We study the global dynamics of energetically favorable states and illustrate the potential of this approach in broader classes of dynamics."],"url":"http://arxiv.org/abs/2404.06291v1","category":"math.DS"}
{"created":"2024-04-09 13:17:28","title":"Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models","abstract":"Large language models (LLMs) have gained widespread popularity and demonstrated exceptional performance not only in natural language processing (NLP) tasks but also in non-linguistic domains. Their potential as artificial general intelligence extends beyond NLP, showcasing promising capabilities in diverse optimization scenarios. Despite this rising trend, whether the integration of LLMs into these black-box optimization problems is genuinely beneficial remains unexplored. This paper endeavors to tackle this issue by offering deeper insights into the potential of LLMs in optimization tasks through a comprehensive investigation. Our approach involves a comprehensive evaluation, covering both discrete and continuous optimization problems, aiming to assess the efficacy and distinctive characteristics that LLMs bring to the realm of optimization. Our findings reveal both the limitations and advantages of LLMs in optimization. On one hand, despite consuming the significant power required to run the model, LLMs exhibit subpar performance and lack desirable properties in pure numerical tasks, primarily due to a mismatch between the problem domain and their processing capabilities. On the other hand, although LLMs may not be ideal for traditional numerical optimization, their potential in broader optimization contexts remains promising. LLMs exhibit the ability to solve problems in non-numerical domains and can leverage heuristics from the prompt to enhance their performance. To the best of our knowledge, this work presents the first systematic evaluation of LLMs for numerical optimization, offering a progressive, wide-coverage, and behavioral analysis. Our findings pave the way for a deeper understanding of LLMs' role in optimization and guide future application in diverse scenarios for LLMs.","sentences":["Large language models (LLMs) have gained widespread popularity and demonstrated exceptional performance not only in natural language processing (NLP) tasks but also in non-linguistic domains.","Their potential as artificial general intelligence extends beyond NLP, showcasing promising capabilities in diverse optimization scenarios.","Despite this rising trend, whether the integration of LLMs into these black-box optimization problems is genuinely beneficial remains unexplored.","This paper endeavors to tackle this issue by offering deeper insights into the potential of LLMs in optimization tasks through a comprehensive investigation.","Our approach involves a comprehensive evaluation, covering both discrete and continuous optimization problems, aiming to assess the efficacy and distinctive characteristics that LLMs bring to the realm of optimization.","Our findings reveal both the limitations and advantages of LLMs in optimization.","On one hand, despite consuming the significant power required to run the model, LLMs exhibit subpar performance and lack desirable properties in pure numerical tasks, primarily due to a mismatch between the problem domain and their processing capabilities.","On the other hand, although LLMs may not be ideal for traditional numerical optimization, their potential in broader optimization contexts remains promising.","LLMs exhibit the ability to solve problems in non-numerical domains and can leverage heuristics from the prompt to enhance their performance.","To the best of our knowledge, this work presents the first systematic evaluation of LLMs for numerical optimization, offering a progressive, wide-coverage, and behavioral analysis.","Our findings pave the way for a deeper understanding of LLMs' role in optimization and guide future application in diverse scenarios for LLMs."],"url":"http://arxiv.org/abs/2404.06290v1","category":"cs.NE"}
{"created":"2024-04-09 13:12:44","title":"NR-V2X Quality of Service Prediction Through Machine Learning with Nested Cross-Validation Scheme","abstract":"The proliferation of connected vehicles and the advent of New Radio (NR) technologies have ushered in a new era of intelligent transportation systems. Ensuring reliable and lowlatency communication between vehicles and their surrounding environment is of utmost importance for the success of these systems. This paper presents a novel approach to predict Quality of Service (QoS) in Vehicle-to-Everything (V2X) communications through nested cross-validation. Our methodology employs several machine learning (ML) methods to predict some QoS metrics, such as packet delivery ratio (PDR), and throughput, in NR-based V2X scenarios. In ML employment, nested cross-validation approach, unlike conventional cross-validation approach, prevents information leakage from parameter selection into hyperparameter selection, and this results in getting more robust results in terms of overfitting. The study utilizes real-world NR-V2X datasets to train and validate the proposed ML methods. Through extensive experiments, we demonstrate the efficacy of our approach in accurately predicting QoS parameters, even in dynamic and challenging vehicular environments. In summary, our research contributes to the advancement of NR-based V2X communication systems by introducing employment of ML methods with a novel approach for QoS prediction. The combination of accurate predictions through nested cross-validation not only enhances the reliability of communication in connected vehicles' landscape but also has a supportive role for stakeholders to make informed decisions for the optimization and management of vehicular networks.","sentences":["The proliferation of connected vehicles and the advent of New Radio (NR) technologies have ushered in a new era of intelligent transportation systems.","Ensuring reliable and lowlatency communication between vehicles and their surrounding environment is of utmost importance for the success of these systems.","This paper presents a novel approach to predict Quality of Service (QoS) in Vehicle-to-Everything (V2X) communications through nested cross-validation.","Our methodology employs several machine learning (ML) methods to predict some QoS metrics, such as packet delivery ratio (PDR), and throughput, in NR-based V2X scenarios.","In ML employment, nested cross-validation approach, unlike conventional cross-validation approach, prevents information leakage from parameter selection into hyperparameter selection, and this results in getting more robust results in terms of overfitting.","The study utilizes real-world NR-V2X datasets to train and validate the proposed ML methods.","Through extensive experiments, we demonstrate the efficacy of our approach in accurately predicting QoS parameters, even in dynamic and challenging vehicular environments.","In summary, our research contributes to the advancement of NR-based V2X communication systems by introducing employment of ML methods with a novel approach for QoS prediction.","The combination of accurate predictions through nested cross-validation not only enhances the reliability of communication in connected vehicles' landscape but also has a supportive role for stakeholders to make informed decisions for the optimization and management of vehicular networks."],"url":"http://arxiv.org/abs/2404.06286v1","category":"cs.IT"}
{"created":"2024-04-09 13:11:27","title":"Coarse-grained quantum state tomography with optimal POVM construction","abstract":"Constructing an integrated large-scale qubit system of realistic size requires addressing the challenge of physical crowding among qubits. This constraint poses an issue of coarse-grained (CG) measurement, wherein information from the multi-qubit system is collectively gathered. In this work, we introduce a novel approach to reconstruct the target density matrix from a comprehensive set of Positive Operator-Valued Measures (POVM) using a Parameterized Quantum Circuit (PQC) under the constraint of CG measurement. We improve the robustness and stability of CG quantum state tomography (QST) by optimizing the POVM set to achieve a generalized symmetric informationally complete (GSIC) POVM through maximization of the von Neumann entropy. This optimized construction of CG-POVMs is scalable to an N-qubit system. We further discuss a more efficient construction of N-qubit CG-QST without exponential increases in two-qubit gates or circuit depth per measurement. Our scheme offers a viable pathway towards a detector-efficient large-scale solid-state embedded qubit platform by reconstructing crucial quantum information from collective measurements.","sentences":["Constructing an integrated large-scale qubit system of realistic size requires addressing the challenge of physical crowding among qubits.","This constraint poses an issue of coarse-grained (CG) measurement, wherein information from the multi-qubit system is collectively gathered.","In this work, we introduce a novel approach to reconstruct the target density matrix from a comprehensive set of Positive Operator-Valued Measures (POVM) using a Parameterized Quantum Circuit (PQC) under the constraint of CG measurement.","We improve the robustness and stability of CG quantum state tomography (QST) by optimizing the POVM set to achieve a generalized symmetric informationally complete (GSIC) POVM through maximization of the von Neumann entropy.","This optimized construction of CG-POVMs is scalable to an N-qubit system.","We further discuss a more efficient construction of N-qubit CG-QST without exponential increases in two-qubit gates or circuit depth per measurement.","Our scheme offers a viable pathway towards a detector-efficient large-scale solid-state embedded qubit platform by reconstructing crucial quantum information from collective measurements."],"url":"http://arxiv.org/abs/2404.06285v1","category":"quant-ph"}
{"created":"2024-04-09 13:02:40","title":"Algorithms for Caching and MTS with reduced number of predictions","abstract":"ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation -- this motivated Im et al. '22 to introduce the study of algorithms which use predictions parsimoniously. We design parsimonious algorithms for caching and MTS with action predictions, proposed by Antoniadis et al. '20, focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error). Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions. We propose an algorithm for general MTS whose consistency and smoothness both scale linearly with the decreasing number of predictions. Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. '20.","sentences":["ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds.","Producing these predictions might be a costly operation -- this motivated Im et al.","'22 to introduce the study of algorithms which use predictions parsimoniously.","We design parsimonious algorithms for caching and MTS with action predictions, proposed by Antoniadis et al. '20, focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error).","Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions.","We propose an algorithm for general MTS whose consistency and smoothness both scale linearly with the decreasing number of predictions.","Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. '20."],"url":"http://arxiv.org/abs/2404.06280v1","category":"cs.LG"}
{"created":"2024-04-09 13:02:33","title":"NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural Cellular Automata","abstract":"Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent. In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems. To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE. However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question. We study NCA models at the limit where space-time discretization approaches continuity. We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called \"seed\". To address this, we propose a solution that utilizes uniform noise as the initial condition. We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities. Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns. We demonstrate this new NCA feature in our interactive online demo. Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective.","sentences":["Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent.","In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems.","To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE.","However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question.","We study NCA models at the limit where space-time discretization approaches continuity.","We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called \"seed\".","To address this, we propose a solution that utilizes uniform noise as the initial condition.","We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities.","Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns.","We demonstrate this new NCA feature in our interactive online demo.","Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective."],"url":"http://arxiv.org/abs/2404.06279v1","category":"cs.CV"}
{"created":"2024-04-09 13:02:22","title":"Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform","abstract":"Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.","sentences":["Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance.","This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality.","We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context.","By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions.","The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency.","This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications.","Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input."],"url":"http://arxiv.org/abs/2404.06278v1","category":"cs.DB"}
{"created":"2024-04-09 13:01:26","title":"Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping","abstract":"Foundation models are a strong trend in deep learning and computer vision. These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications. Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information. When they are followed in a pipeline by an object identification model, they can perform object detection without training. Here, we focus on training such an object identification model. A crucial practical aspect for an object identification model is to be flexible in input size. As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers). The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids. CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible. In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model. We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection. There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data.","sentences":["Foundation models are a strong trend in deep learning and computer vision.","These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications.","Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information.","When they are followed in a pipeline by an object identification model, they can perform object detection without training.","Here, we focus on training such an object identification model.","A crucial practical aspect for an object identification model is to be flexible in input size.","As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers).","The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids.","CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible.","In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model.","We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection.","There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data."],"url":"http://arxiv.org/abs/2404.06277v1","category":"cs.CV"}
{"created":"2024-04-09 12:55:46","title":"Extending the Defect Tolerance of Halide Perovskite Nanocrystals to Hot Carrier Cooling Dynamics","abstract":"Defect tolerance is a critical enabling factor for efficient lead-halide perovskite materials, but the current understanding is primarily on band-edge (cold) carriers, with significant debate over whether hot carriers (HCs) can also exhibit defect tolerance. Here, this important gap in the field is addressed by investigating how internationally-introduced traps affect HC relaxation in CsPbX3 nanocrystals (X = Br, I, or mixture). Using femtosecond interband and intraband spectroscopy, along with energy-dependent photoluminescence measurements and kinetic modelling, it is found that HCs are not universally defect tolerant in CsPbX3, but are strongly correlated to the defect tolerance of cold carriers, requiring shallow traps to be present (as in CsPbI3). It is found that HCs are directly captured by traps, instead of going through an intermediate cold carrier, and deeper traps cause faster HC cooling, reducing the effects of the hot phonon bottleneck and Auger reheating. This work provides important insights into how defects influence HCs, which will be important for designing materials for hot carrier solar cells, multiexciton generation, and optical gain media.","sentences":["Defect tolerance is a critical enabling factor for efficient lead-halide perovskite materials, but the current understanding is primarily on band-edge (cold) carriers, with significant debate over whether hot carriers (HCs) can also exhibit defect tolerance.","Here, this important gap in the field is addressed by investigating how internationally-introduced traps affect HC relaxation in CsPbX3 nanocrystals (X = Br, I, or mixture).","Using femtosecond interband and intraband spectroscopy, along with energy-dependent photoluminescence measurements and kinetic modelling, it is found that HCs are not universally defect tolerant in CsPbX3, but are strongly correlated to the defect tolerance of cold carriers, requiring shallow traps to be present (as in CsPbI3).","It is found that HCs are directly captured by traps, instead of going through an intermediate cold carrier, and deeper traps cause faster HC cooling, reducing the effects of the hot phonon bottleneck and Auger reheating.","This work provides important insights into how defects influence HCs, which will be important for designing materials for hot carrier solar cells, multiexciton generation, and optical gain media."],"url":"http://arxiv.org/abs/2404.06276v1","category":"physics.app-ph"}
{"created":"2024-04-09 12:51:00","title":"Blow-up of classical solutions of quasilinear wave equations in one space dimension","abstract":"This paper studies the upper bound of the lifespan of classical solutions of the initial value problems for one dimensional wave equations with quasilinear terms of space-, or time-derivatives of the unknown function. The results are same as those of the semilinear case. But it is quite meaningful to consider this kind of problems for the purpose to cover the optimality of the general theory for nonlinear wave equations by many model equations as far as possible.","sentences":["This paper studies the upper bound of the lifespan of classical solutions of the initial value problems for one dimensional wave equations with quasilinear terms of space-, or time-derivatives of the unknown function.","The results are same as those of the semilinear case.","But it is quite meaningful to consider this kind of problems for the purpose to cover the optimality of the general theory for nonlinear wave equations by many model equations as far as possible."],"url":"http://arxiv.org/abs/2404.06274v1","category":"math.AP"}
{"created":"2024-04-09 12:45:17","title":"PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances","abstract":"We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances. PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs. Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies. PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process.","sentences":["We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances.","PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs.","Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies.","PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process."],"url":"http://arxiv.org/abs/2404.06267v1","category":"cs.LG"}
{"created":"2024-04-09 12:35:16","title":"The walled Brauer category and stable cohomology of $\\mathrm{IA}_n$","abstract":"The IA-automorphism group is the group of automorphisms of the free group $F_n$ that act trivially on the abelianization $F_n^{\\mathrm{ab}}$. This group is in many ways analoguous to Torelli groups of surfaces and their higher dimensional analogues. In recent work, the stable rational cohomology of such groups was studied by Kupers and Randal-Williams, using the machinery of so-called Brauer categories. In this paper, we adapt their methods to study the stable rational cohomology of the IA-automorphism group. We obtain a conjectural description of the algebraic part of the stable rational cohomology and prove that it holds up to degree $Q+1$, given the assumption that the stable cohomology groups are stably finite dimensional in degrees up to $Q$. In particular, this allows us to compute the algebraic part of the stable cohomology in degree 2, which we show agrees with the part generated by the first cohomology group via the cup product map and which has previously been computed by Pettet.   In the appendix, written by Mai Katada, it is shown how the results of the paper can be applied to compute the stable Albanese (co)homology of the IA-automorphism group.","sentences":["The IA-automorphism group is the group of automorphisms of the free group $F_n$ that act trivially on the abelianization $F_n^{\\mathrm{ab}}$. This group is in many ways analoguous to Torelli groups of surfaces and their higher dimensional analogues.","In recent work, the stable rational cohomology of such groups was studied by Kupers and Randal-Williams, using the machinery of so-called Brauer categories.","In this paper, we adapt their methods to study the stable rational cohomology of the IA-automorphism group.","We obtain a conjectural description of the algebraic part of the stable rational cohomology and prove that it holds up to degree $Q+1$, given the assumption that the stable cohomology groups are stably finite dimensional in degrees up to $Q$. In particular, this allows us to compute the algebraic part of the stable cohomology in degree 2, which we show agrees with the part generated by the first cohomology group via the cup product map and which has previously been computed by Pettet.   ","In the appendix, written by Mai Katada, it is shown how the results of the paper can be applied to compute the stable Albanese (co)homology of the IA-automorphism group."],"url":"http://arxiv.org/abs/2404.06263v1","category":"math.AT"}
{"created":"2024-04-09 12:34:28","title":"Playing to Vision Foundation Model's Strengths in Stereo Matching","abstract":"Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks. This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.","sentences":["Stereo matching has become a key technique for 3D environment perception in intelligent vehicles.","For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain.","Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets.","While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks.","This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching.","Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention.","The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels.","Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches.","We believe this new paradigm will pave the way for the next generation of stereo matching networks."],"url":"http://arxiv.org/abs/2404.06261v1","category":"cs.CV"}
{"created":"2024-04-09 12:29:16","title":"Label-Efficient 3D Object Detection For Road-Side Units","abstract":"Occlusion presents a significant challenge for safety-critical applications such as autonomous driving. Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion. While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data. Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds. We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery. Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement. Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models. Extensive experiments are carried out in simulated and real-world datasets to evaluate our method.","sentences":["Occlusion presents a significant challenge for safety-critical applications such as autonomous driving.","Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion.","While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data.","Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds.","We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery.","Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement.","Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models.","Extensive experiments are carried out in simulated and real-world datasets to evaluate our method."],"url":"http://arxiv.org/abs/2404.06256v1","category":"cs.CV"}
{"created":"2024-04-09 12:26:11","title":"Modularity of special 0-cycles on toroidal compactifications of Shimura Varieties","abstract":"We consider the generating series of appropriately completed 0-dimensional special cycles on a toroidal compactification of an orthogonal or unitary Shimura variety with values in the Chow group. We prove that it is a holomorphic Siegel, respectively Hermitian, modular form","sentences":["We consider the generating series of appropriately completed 0-dimensional special cycles on a toroidal compactification of an orthogonal or unitary Shimura variety with values in the Chow group.","We prove that it is a holomorphic Siegel, respectively Hermitian, modular form"],"url":"http://arxiv.org/abs/2404.06254v1","category":"math.NT"}
{"created":"2024-04-09 12:24:43","title":"Design and Characterization of Strategy-Proof Mechanisms for Two-Facility Game on a Line","abstract":"We focus on the problem of placing two facilities along a linear space to serve a group of agents. Each agent is committed to minimizing the distance between her location and the closest facility. A mechanism is an algorithm that maps the reported agent locations to the facility locations. We are interested in mechanisms without money that are deterministic, strategy-proof, and provide a bounded approximation ratio for social cost.   It is a fundamental problem to characterize the family of strategy-proof mechanisms with a bounded approximation ratio. Fotakis and Tzamos already demonstrated that the deterministic strategy-proof mechanisms for the 2-facility game problem are mechanisms with a unique dictator and the leftmost-rightmost mechanism. In this paper, we first present a more refined characterization of the first family.   We then reveal three new classes of strategy-proof mechanisms that show the intricacy of structure within this family. This helps us get a more complete picture of the characterization of the 2-facility game problem, and may also have value in understanding and solving more general facility allocation game problems.   Besides, based on our refined characterization, we surprisingly find that prediction cannot effectively improve the performance of the mechanism in the two-facility game problem, while this methodology to overcome bad approximation ratio works in many other mechanism design problems. We show that if we require that the mechanism admits a bounded approximation ratio when the prediction is arbitrarily bad, then at the same time, the mechanism can never achieve sublinear approximation ratios even with perfect prediction.","sentences":["We focus on the problem of placing two facilities along a linear space to serve a group of agents.","Each agent is committed to minimizing the distance between her location and the closest facility.","A mechanism is an algorithm that maps the reported agent locations to the facility locations.","We are interested in mechanisms without money that are deterministic, strategy-proof, and provide a bounded approximation ratio for social cost.   ","It is a fundamental problem to characterize the family of strategy-proof mechanisms with a bounded approximation ratio.","Fotakis and Tzamos already demonstrated that the deterministic strategy-proof mechanisms for the 2-facility game problem are mechanisms with a unique dictator and the leftmost-rightmost mechanism.","In this paper, we first present a more refined characterization of the first family.   ","We then reveal three new classes of strategy-proof mechanisms that show the intricacy of structure within this family.","This helps us get a more complete picture of the characterization of the 2-facility game problem, and may also have value in understanding and solving more general facility allocation game problems.   ","Besides, based on our refined characterization, we surprisingly find that prediction cannot effectively improve the performance of the mechanism in the two-facility game problem, while this methodology to overcome bad approximation ratio works in many other mechanism design problems.","We show that if we require that the mechanism admits a bounded approximation ratio when the prediction is arbitrarily bad, then at the same time, the mechanism can never achieve sublinear approximation ratios even with perfect prediction."],"url":"http://arxiv.org/abs/2404.06252v1","category":"cs.GT"}
{"created":"2024-04-09 12:20:46","title":"On checking $\\mathrm{L}^p$-admissibility for parabolic control systems","abstract":"In this note we discuss the difficulty of verifying $\\mathrm{L}^p$-admissibility for $p\\neq 2$ -- that even manifests in the presence of a self-adjoint semigroup generator on a Hilbert space -- and survey tests for $\\mathrm{L}^p$-admissibility of given control operators. These tests are obtained by virtue of either mapping properties of boundary trace operators, yielding a characterization of admissibility via abstract interpolation spaces; or through Laplace--Carleson embeddings, slightly extending results from Jacob, Partington and Pott to a class of systems which are not necessarily diagonal with respect to sequence spaces. Special focus is laid on illustrating the theory by means of examples based on the heat equation on various domains.","sentences":["In this note we discuss the difficulty of verifying $\\mathrm{L}^p$-admissibility for $p\\neq 2$ -- that even manifests in the presence of a self-adjoint semigroup generator on a Hilbert space -- and survey tests for $\\mathrm{L}^p$-admissibility of given control operators.","These tests are obtained by virtue of either mapping properties of boundary trace operators, yielding a characterization of admissibility via abstract interpolation spaces; or through Laplace--Carleson embeddings, slightly extending results from Jacob, Partington and Pott to a class of systems which are not necessarily diagonal with respect to sequence spaces.","Special focus is laid on illustrating the theory by means of examples based on the heat equation on various domains."],"url":"http://arxiv.org/abs/2404.06250v1","category":"math.OC"}
{"created":"2024-04-09 12:11:25","title":"GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields","abstract":"Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time.","sentences":["Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations.","However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games.","In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation.","GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features.","This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture.","To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms.","Our results show that GHNeRF can achieve state-of-the-art results in near real-time."],"url":"http://arxiv.org/abs/2404.06246v1","category":"cs.CV"}
{"created":"2024-04-09 12:10:54","title":"Anchor-based Robust Finetuning of Vision-Language Models","abstract":"We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization. We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data. Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as ``a photo of a [CLASS]''. This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information. Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization. Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks.","sentences":["We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization.","We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data.","Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as ``a photo of a [CLASS]''.","This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information.","Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization.","Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics.","Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities.","Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks."],"url":"http://arxiv.org/abs/2404.06244v1","category":"cs.CV"}
{"created":"2024-04-09 12:09:56","title":"ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos","abstract":"Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer. The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: https://github.com/rana2149/ActNetFormer.","sentences":["Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more.","Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire.","This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition.","Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples.","We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer.","The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames.","By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action.","This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures.","Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data.","The official website of this work is available at: https://github.com/rana2149/ActNetFormer."],"url":"http://arxiv.org/abs/2404.06243v1","category":"cs.CV"}
{"created":"2024-04-09 12:08:51","title":"Redshift drift in a universe with structure III: Numerical relativity","abstract":"Measurements of the cosmic redshift drift - the change in redshift of a source over time - will enable independent detection of cosmological expansion thanks to the immense precision soon reached by new facilities such as the Square Kilometer Array Observatory and the Extremely Large Telescope. We conduct the first ever redshift drift computation in fully relativistic cosmological simulations, with the simulations performed with the Einstein Toolkit. We compute the redshift drift over the full skies of 50 synthetic observers in the simulation. We compare all-sky averages for each observer - and across all observers - to the Einstein-de Sitter (EdS) model which represents the large-scale spatially-averaged spacetime of the simulation. We find that at $z\\approx0.2$ the mean redshift drift across the sky for all observers deviates from the EdS prediction at the percent level, reducing to $\\sim0.1\\%$ by $z\\approx 1$. However, fluctuations in the redshift drift across the sky are $\\sim 10-30\\%$ at $z\\approx 0.1$ and a few percent at $z\\approx 0.5$. Such fluctuations are large enough to potentially exceed the expected precision of upcoming redshift drift measurements. Additionally, we find that along 0.48% of the light rays the redshift drift becomes temporarily positive at very low redshift of $z\\lesssim 0.02$. This occurs despite our simulation data being based on a matter-dominated model universe. By including a cosmological constant, we expect a slower growth of structures than in the leading-order EdS space-time, and this may reduce the anisotropy over the observers' skies, although we generally expect our results to hold as order-of-magnitude estimates. Redshift drift is arguably one of the most important measurements to be made by next-generation telescopes. Our results collectively serve as preparation for interpreting such a measurement in the presence of realistic cosmic structures.","sentences":["Measurements of the cosmic redshift drift - the change in redshift of a source over time - will enable independent detection of cosmological expansion thanks to the immense precision soon reached by new facilities such as the Square Kilometer Array Observatory and the Extremely Large Telescope.","We conduct the first ever redshift drift computation in fully relativistic cosmological simulations, with the simulations performed with the Einstein Toolkit.","We compute the redshift drift over the full skies of 50 synthetic observers in the simulation.","We compare all-sky averages for each observer - and across all observers - to the Einstein-de Sitter (EdS) model which represents the large-scale spatially-averaged spacetime of the simulation.","We find that at $z\\approx0.2$ the mean redshift drift across the sky for all observers deviates from the EdS prediction at the percent level, reducing to $\\sim0.1\\%$ by $z\\approx 1$.","However, fluctuations in the redshift drift across the sky are $\\sim 10-30\\%$ at $z\\approx 0.1$ and a few percent at $z\\approx 0.5$.","Such fluctuations are large enough to potentially exceed the expected precision of upcoming redshift drift measurements.","Additionally, we find that along 0.48% of the light rays the redshift drift becomes temporarily positive at very low redshift of $z\\lesssim 0.02$.","This occurs despite our simulation data being based on a matter-dominated model universe.","By including a cosmological constant, we expect a slower growth of structures than in the leading-order EdS space-time, and this may reduce the anisotropy over the observers' skies, although we generally expect our results to hold as order-of-magnitude estimates.","Redshift drift is arguably one of the most important measurements to be made by next-generation telescopes.","Our results collectively serve as preparation for interpreting such a measurement in the presence of realistic cosmic structures."],"url":"http://arxiv.org/abs/2404.06242v1","category":"astro-ph.CO"}
{"created":"2024-04-09 11:58:53","title":"Permutation Testing for Monotone Trend","abstract":"In this paper, we consider the fundamental problem of testing for monotone trend in a time series. While the term \"trend\" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context. A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples. On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations. We also introduce \"local\" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series. Similar properties of permutation tests are obtained for these tests as well.","sentences":["In this paper, we consider the fundamental problem of testing for monotone trend in a time series.","While the term \"trend\" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context.","A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples.","On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations.","We also introduce \"local\" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series.","Similar properties of permutation tests are obtained for these tests as well."],"url":"http://arxiv.org/abs/2404.06239v1","category":"math.ST"}
{"created":"2024-04-09 11:57:22","title":"Intra- and intercycle analysis of intraband high-order harmonic generation","abstract":"We study intraband high-order harmonic generation arising from a band-gap material driven by a linearly polarized laser field. We factorize the intraband high-order harmonic-generation signal into intracycle and intercycle terms. The intracycle term uniquely determines the spectral characteristics whereas the intercycle term merely modulates the spectral features by imposing energy conservation in the long-pulse limit. Through analysis of the intracycle interference, the cutoff is identified, and the origin of the harmonic selection rules is revealed. Further, it is outlined how different components of the bandstructure contribute to different regions of the harmonic spectrum, giving rise to non-trivial intensity scaling of individual harmonics in the plateau region.","sentences":["We study intraband high-order harmonic generation arising from a band-gap material driven by a linearly polarized laser field.","We factorize the intraband high-order harmonic-generation signal into intracycle and intercycle terms.","The intracycle term uniquely determines the spectral characteristics whereas the intercycle term merely modulates the spectral features by imposing energy conservation in the long-pulse limit.","Through analysis of the intracycle interference, the cutoff is identified, and the origin of the harmonic selection rules is revealed.","Further, it is outlined how different components of the bandstructure contribute to different regions of the harmonic spectrum, giving rise to non-trivial intensity scaling of individual harmonics in the plateau region."],"url":"http://arxiv.org/abs/2404.06237v1","category":"physics.optics"}
{"created":"2024-04-09 11:56:29","title":"Towards Robust Domain Generation Algorithm Classification","abstract":"In this work, we conduct a comprehensive study on the robustness of domain generation algorithm (DGA) classifiers. We implement 32 white-box attacks, 19 of which are very effective and induce a false-negative rate (FNR) of $\\approx$ 100\\% on unhardened classifiers. To defend the classifiers, we evaluate different hardening approaches and propose a novel training scheme that leverages adversarial latent space vectors and discretized adversarial domains to significantly improve robustness. In our study, we highlight a pitfall to avoid when hardening classifiers and uncover training biases that can be easily exploited by attackers to bypass detection, but which can be mitigated by adversarial training (AT). In our study, we do not observe any trade-off between robustness and performance, on the contrary, hardening improves a classifier's detection performance for known and unknown DGAs. We implement all attacks and defenses discussed in this paper as a standalone library, which we make publicly available to facilitate hardening of DGA classifiers: https://gitlab.com/rwth-itsec/robust-dga-detection","sentences":["In this work, we conduct a comprehensive study on the robustness of domain generation algorithm (DGA) classifiers.","We implement 32 white-box attacks, 19 of which are very effective and induce a false-negative rate (FNR) of $\\approx$ 100\\% on unhardened classifiers.","To defend the classifiers, we evaluate different hardening approaches and propose a novel training scheme that leverages adversarial latent space vectors and discretized adversarial domains to significantly improve robustness.","In our study, we highlight a pitfall to avoid when hardening classifiers and uncover training biases that can be easily exploited by attackers to bypass detection, but which can be mitigated by adversarial training (AT).","In our study, we do not observe any trade-off between robustness and performance, on the contrary, hardening improves a classifier's detection performance for known and unknown DGAs.","We implement all attacks and defenses discussed in this paper as a standalone library, which we make publicly available to facilitate hardening of DGA classifiers: https://gitlab.com/rwth-itsec/robust-dga-detection"],"url":"http://arxiv.org/abs/2404.06236v1","category":"cs.CR"}
{"created":"2024-04-09 11:54:07","title":"Universal Deformations and Inhomogeneities in Isotropic Cauchy Elasticity","abstract":"For a given class of materials, \\emph{universal deformations} are those deformations that can be maintained in the absence of body forces and by applying solely boundary tractions. For inhomogeneous bodies, in addition to the universality constraints that determine the universal deformations, there are extra constraints on the form of the material inhomogeneities -- \\emph{universal inhomogeneity constraints}. Those inhomogeneities compatible with the universal inhomogeneity constraints are called \\emph{universal inhomogeneities}. In a Cauchy elastic solid, stress at a given point and at an instance of time is a function of strain at that point and that exact moment in time, without any dependence on prior history. A Cauchy elastic solid does not necessarily have an energy function, i.e., Cauchy elastic solids are, in general, non-hyperelastic (or non-Green elastic). In this paper we characterize universal deformations in both compressible and incompressible inhomogeneous isotropic Cauchy elasticity. As Cauchy elasticity includes hyperelasticity, one expects the universal deformations of Cauchy elasticity to be a subset of those of hyperelasticity both in the compressible and incompressible cases. It is also expected that the universal inhomogeneity constraints to be more strict than those of hyperelasticity, and hence, the set of universal inhomogeneities to be smaller than that of hyperelasticity. We prove the unexpected result that the sets of universal deformations of isotropic Cauchy elasticity and isotropic hyperelasticity are identical, in both the compressible and incompressible cases. We also prove that their corresponding universal inhomogeneities are identical as well.","sentences":["For a given class of materials, \\emph{universal deformations} are those deformations that can be maintained in the absence of body forces and by applying solely boundary tractions.","For inhomogeneous bodies, in addition to the universality constraints that determine the universal deformations, there are extra constraints on the form of the material inhomogeneities -- \\emph{universal inhomogeneity constraints}.","Those inhomogeneities compatible with the universal inhomogeneity constraints are called \\emph{universal inhomogeneities}.","In a Cauchy elastic solid, stress at a given point and at an instance of time is a function of strain at that point and that exact moment in time, without any dependence on prior history.","A Cauchy elastic solid does not necessarily have an energy function, i.e., Cauchy elastic solids are, in general, non-hyperelastic (or non-Green elastic).","In this paper we characterize universal deformations in both compressible and incompressible inhomogeneous isotropic Cauchy elasticity.","As Cauchy elasticity includes hyperelasticity, one expects the universal deformations of Cauchy elasticity to be a subset of those of hyperelasticity both in the compressible and incompressible cases.","It is also expected that the universal inhomogeneity constraints to be more strict than those of hyperelasticity, and hence, the set of universal inhomogeneities to be smaller than that of hyperelasticity.","We prove the unexpected result that the sets of universal deformations of isotropic Cauchy elasticity and isotropic hyperelasticity are identical, in both the compressible and incompressible cases.","We also prove that their corresponding universal inhomogeneities are identical as well."],"url":"http://arxiv.org/abs/2404.06235v1","category":"physics.class-ph"}
{"created":"2024-04-09 11:53:47","title":"The Rubin Observatory's Legacy Survey of Space and Time DP0.2 processing campaign at CC-IN2P3","abstract":"The Vera C. Rubin Observatory, currently in construction in Chile, will start performing the Legacy Survey of Space and Time (LSST) in 2025 for 10 years. Its 8.4-meter telescope will survey the southern sky in less than 4 nights in six optical bands, and repeatedly generate about 2 000 exposures per night, corresponding to a data volume of about 20 TiB every night. Three data facilities are preparing to contribute to the production of the annual data releases: the US Data Facility will process 35% of the raw data, the UK data facility will process 25% of the raw data and the French data facility, operated by CC-IN2P3, will locally process the remaining 40% of the raw data. In the context of the Data Preview 0.2 (DP0.2), the Data Release Production pipelines have been executed on the DC-2 simulated dataset (generated by the Dark Energy Science Collaboration, DESC). This dataset includes 20 000 simulated exposures, representing 300 square degrees of Rubin images with a typical depth of 5 years. DP0.2 ran at the Interim Data Facility (based on Google cloud), and the full exercise was independently replicated at CC-IN2P3. During this exercise, 3 PiB of data and more than 200 million files were produced. In this contribution we will present a detailed description of the system that we set up to perform this processing campaign using CC-IN2P3's computing and storage infrastructure. Several topics will be addressed: workflow generation and execution, batch job submission, memory and I/O requirements, etc. We will focus on the issues that arose during this campaign and how we addressed them and will present some perspectives after this exercise.","sentences":["The Vera C. Rubin Observatory, currently in construction in Chile, will start performing the Legacy Survey of Space and Time (LSST) in 2025 for 10 years.","Its 8.4-meter telescope will survey the southern sky in less than 4 nights in six optical bands, and repeatedly generate about 2 000 exposures per night, corresponding to a data volume of about 20 TiB every night.","Three data facilities are preparing to contribute to the production of the annual data releases: the US Data Facility will process 35% of the raw data, the UK data facility will process 25% of the raw data and the French data facility, operated by CC-IN2P3, will locally process the remaining 40% of the raw data.","In the context of the Data Preview 0.2 (DP0.2), the Data Release Production pipelines have been executed on the DC-2 simulated dataset (generated by the Dark Energy Science Collaboration, DESC).","This dataset includes 20 000 simulated exposures, representing 300 square degrees of Rubin images with a typical depth of 5 years.","DP0.2 ran at the Interim Data Facility (based on Google cloud), and the full exercise was independently replicated at CC-IN2P3.","During this exercise, 3 PiB of data and more than 200 million files were produced.","In this contribution we will present a detailed description of the system that we set up to perform this processing campaign using CC-IN2P3's computing and storage infrastructure.","Several topics will be addressed: workflow generation and execution, batch job submission, memory and I/O requirements, etc.","We will focus on the issues that arose during this campaign and how we addressed them and will present some perspectives after this exercise."],"url":"http://arxiv.org/abs/2404.06234v1","category":"astro-ph.IM"}
{"created":"2024-04-09 11:42:32","title":"Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning","abstract":"Federated learning (FL) has been introduced to enable a large number of clients, possibly mobile devices, to collaborate on generating a generalized machine learning model thanks to utilizing a larger number of local samples without sharing to offer certain privacy to collaborating clients. However, due to the participation of a large number of clients, it is often difficult to profile and verify each client, which leads to a security threat that malicious participants may hamper the accuracy of the trained model by conveying poisoned models during the training. Hence, the aggregation framework at the parameter server also needs to minimize the detrimental effects of these malicious clients. A plethora of attack and defence strategies have been analyzed in the literature. However, often the Byzantine problem is analyzed solely from the outlier detection perspective, being oblivious to the topology of neural networks (NNs).   In the scope of this work, we argue that by extracting certain side information specific to the NN topology, one can design stronger attacks. Hence, inspired by the sparse neural networks, we introduce a hybrid sparse Byzantine attack that is composed of two parts: one exhibiting a sparse nature and attacking only certain NN locations with higher sensitivity, and the other being more silent but accumulating over time, where each ideally targets a different type of defence mechanism, and together they form a strong but imperceptible attack. Finally, we show through extensive simulations that the proposed hybrid Byzantine attack is effective against 8 different defence methods.","sentences":["Federated learning (FL) has been introduced to enable a large number of clients, possibly mobile devices, to collaborate on generating a generalized machine learning model thanks to utilizing a larger number of local samples without sharing to offer certain privacy to collaborating clients.","However, due to the participation of a large number of clients, it is often difficult to profile and verify each client, which leads to a security threat that malicious participants may hamper the accuracy of the trained model by conveying poisoned models during the training.","Hence, the aggregation framework at the parameter server also needs to minimize the detrimental effects of these malicious clients.","A plethora of attack and defence strategies have been analyzed in the literature.","However, often the Byzantine problem is analyzed solely from the outlier detection perspective, being oblivious to the topology of neural networks (NNs).   ","In the scope of this work, we argue that by extracting certain side information specific to the NN topology, one can design stronger attacks.","Hence, inspired by the sparse neural networks, we introduce a hybrid sparse Byzantine attack that is composed of two parts: one exhibiting a sparse nature and attacking only certain NN locations with higher sensitivity, and the other being more silent but accumulating over time, where each ideally targets a different type of defence mechanism, and together they form a strong but imperceptible attack.","Finally, we show through extensive simulations that the proposed hybrid Byzantine attack is effective against 8 different defence methods."],"url":"http://arxiv.org/abs/2404.06230v1","category":"cs.LG"}
{"created":"2024-04-09 11:40:37","title":"Towards Autonomous Driving with Small-Scale Cars: A Survey of Recent Development","abstract":"While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend? While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of small-scale car platforms offers a compelling alternative. These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology. This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage. The conclusion provides proposals for promising future directions in the field.","sentences":["While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend?","While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of small-scale car platforms offers a compelling alternative.","These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology.","This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage.","The conclusion provides proposals for promising future directions in the field."],"url":"http://arxiv.org/abs/2404.06229v1","category":"cs.RO"}
{"created":"2024-04-09 11:36:08","title":"Multimodal Road Network Generation Based on Large Language Model","abstract":"With the increasing popularity of ChatGPT, large language models (LLMs) have demonstrated their capabilities in communication and reasoning, promising for transportation sector intelligentization. However, they still face challenges in domain-specific knowledge. This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an \"intelligent operating system\" for transportation simulation software, exploring their potential with transportation modeling and simulation. We introduce Network Generation AI (NGAI), integrating LLMs with road network modeling plugins, validated through experiments for accuracy and robustness. NGAI's effective use has reduced modeling costs, revolutionized transportation simulations, optimized user steps, and proposed a novel approach for LLM integration in the transportation field.","sentences":["With the increasing popularity of ChatGPT, large language models (LLMs) have demonstrated their capabilities in communication and reasoning, promising for transportation sector intelligentization.","However, they still face challenges in domain-specific knowledge.","This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an \"intelligent operating system\" for transportation simulation software, exploring their potential with transportation modeling and simulation.","We introduce Network Generation AI (NGAI), integrating LLMs with road network modeling plugins, validated through experiments for accuracy and robustness.","NGAI's effective use has reduced modeling costs, revolutionized transportation simulations, optimized user steps, and proposed a novel approach for LLM integration in the transportation field."],"url":"http://arxiv.org/abs/2404.06227v1","category":"cs.HC"}
{"created":"2024-04-09 11:26:59","title":"Low-Cost Generation and Evaluation of Dictionary Example Sentences","abstract":"Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.","sentences":["Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging.","Prior works have demonstrated that language models can be trained to generate example sentences.","However, they relied on costly customized models and word sense datasets for generation and evaluation of their work.","Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences.","We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences.","OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation.","We experiment with various LLMs and configurations to generate dictionary sentences across word classes.","We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning.","The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences."],"url":"http://arxiv.org/abs/2404.06224v1","category":"cs.CL"}
{"created":"2024-04-09 11:26:23","title":"Dark sterile neutrinos on a linear seesaw of neutrino masses","abstract":"Sterile neutrinos as source of the mass and flavor mixing of active neutrinos as well as genesis of the dark matter (DM) and matter-antimatter asymmetry have gained special interest. Here we study the case of the Standard Model (SM) extended with three right-handed (RH) neutrinos and a dark sector with two extra sterile neutrinos, odd under a discrete $Z_2$ symmetry. The RH neutrinos are responsible for producing the baryon asymmetry via the high-scale unflavored leptogenesis. They are superheavy and their abundance at the electroweak broken stage is vanishingly small, so that they have no impact on the phenomenology at low energies. The two dark neutrinos generate the tiny mass of two active neutrinos through a mechanism similar to the minimal linear seesaw, and saturate the relic abundance as freeze-in DM via decay of the heavy SM bosons. The absence of the dark Majorana mass terms in the dark linear seesaw is explained by invoking a hidden symmetry, the so-called presymmetry, and the DM candidate appears in the form of a quasi-Dirac neutrino. The $Z_2$ symmetry is broken in the dark neutrino sector, but exact in the realm of RH neutrinos. The required coupling weakness for the freeze-in DM neutrino is related to a very small breach of unitarity in the active neutrino mixing matrix. We show how phenomenological constraints on the production and decay of the DM neutrino imply an upper bound around 1 MeV for its mass and unitarity up to $\\mathcal{O}(10^{-7})$ for the mixing matrix.","sentences":["Sterile neutrinos as source of the mass and flavor mixing of active neutrinos as well as genesis of the dark matter (DM) and matter-antimatter asymmetry have gained special interest.","Here we study the case of the Standard Model (SM) extended with three right-handed (RH) neutrinos and a dark sector with two extra sterile neutrinos, odd under a discrete $Z_2$ symmetry.","The RH neutrinos are responsible for producing the baryon asymmetry via the high-scale unflavored leptogenesis.","They are superheavy and their abundance at the electroweak broken stage is vanishingly small, so that they have no impact on the phenomenology at low energies.","The two dark neutrinos generate the tiny mass of two active neutrinos through a mechanism similar to the minimal linear seesaw, and saturate the relic abundance as freeze-in DM via decay of the heavy SM bosons.","The absence of the dark Majorana mass terms in the dark linear seesaw is explained by invoking a hidden symmetry, the so-called presymmetry, and the DM candidate appears in the form of a quasi-Dirac neutrino.","The $Z_2$ symmetry is broken in the dark neutrino sector, but exact in the realm of RH neutrinos.","The required coupling weakness for the freeze-in DM neutrino is related to a very small breach of unitarity in the active neutrino mixing matrix.","We show how phenomenological constraints on the production and decay of the DM neutrino imply an upper bound around 1 MeV for its mass and unitarity up to $\\mathcal{O}(10^{-7})$ for the mixing matrix."],"url":"http://arxiv.org/abs/2404.06223v1","category":"hep-ph"}
{"created":"2024-04-09 11:14:45","title":"Zero-Shot Relational Learning for Multimodal Knowledge Graphs","abstract":"Relational learning is an essential task in the domain of knowledge representation, particularly in knowledge graph completion (KGC).While relational learning in traditional single-modal settings has been extensively studied, exploring it within a multimodal KGC context presents distinct challenges and opportunities. One of the major challenges is inference on newly discovered relations without any associated training data. This zero-shot relational learning scenario poses unique requirements for multimodal KGC, i.e., utilizing multimodality to facilitate relational learning. However, existing works fail to support the leverage of multimodal information and leave the problem unexplored. In this paper, we propose a novel end-to-end framework, consisting of three components, i.e., multimodal learner, structure consolidator, and relation embedding generator, to integrate diverse multimodal information and knowledge graph structures to facilitate the zero-shot relational learning. Evaluation results on two multimodal knowledge graphs demonstrate the superior performance of our proposed method.","sentences":["Relational learning is an essential task in the domain of knowledge representation, particularly in knowledge graph completion (KGC).While relational learning in traditional single-modal settings has been extensively studied, exploring it within a multimodal KGC context presents distinct challenges and opportunities.","One of the major challenges is inference on newly discovered relations without any associated training data.","This zero-shot relational learning scenario poses unique requirements for multimodal KGC, i.e., utilizing multimodality to facilitate relational learning.","However, existing works fail to support the leverage of multimodal information and leave the problem unexplored.","In this paper, we propose a novel end-to-end framework, consisting of three components, i.e., multimodal learner, structure consolidator, and relation embedding generator, to integrate diverse multimodal information and knowledge graph structures to facilitate the zero-shot relational learning.","Evaluation results on two multimodal knowledge graphs demonstrate the superior performance of our proposed method."],"url":"http://arxiv.org/abs/2404.06220v1","category":"cs.LG"}
{"created":"2024-04-09 11:13:36","title":"Automatic Defect Detection in Sewer Network Using Deep Learning Based Object Detector","abstract":"Maintaining sewer systems in large cities is important, but also time and effort consuming, because visual inspections are currently done manually. To reduce the amount of aforementioned manual work, defects within sewer pipes should be located and classified automatically. In the past, multiple works have attempted solving this problem using classical image processing, machine learning, or a combination of those. However, each provided solution only focus on detecting a limited set of defect/structure types, such as fissure, root, and/or connection. Furthermore, due to the use of hand-crafted features and small training datasets, generalization is also problematic. In order to overcome these deficits, a sizable dataset with 14.7 km of various sewer pipes were annotated by sewer maintenance experts in the scope of this work. On top of that, an object detector (EfficientDet-D0) was trained for automatic defect detection. From the result of several expermients, peculiar natures of defects in the context of object detection, which greatly effect annotation and training process, are found and discussed. At the end, the final detector was able to detect 83% of defects in the test set; out of the missing 17%, only 0.77% are very severe defects. This work provides an example of applying deep learning-based object detection into an important but quiet engineering field. It also gives some practical pointers on how to annotate peculiar \"object\", such as defects.","sentences":["Maintaining sewer systems in large cities is important, but also time and effort consuming, because visual inspections are currently done manually.","To reduce the amount of aforementioned manual work, defects within sewer pipes should be located and classified automatically.","In the past, multiple works have attempted solving this problem using classical image processing, machine learning, or a combination of those.","However, each provided solution only focus on detecting a limited set of defect/structure types, such as fissure, root, and/or connection.","Furthermore, due to the use of hand-crafted features and small training datasets, generalization is also problematic.","In order to overcome these deficits, a sizable dataset with 14.7 km of various sewer pipes were annotated by sewer maintenance experts in the scope of this work.","On top of that, an object detector (EfficientDet-D0) was trained for automatic defect detection.","From the result of several expermients, peculiar natures of defects in the context of object detection, which greatly effect annotation and training process, are found and discussed.","At the end, the final detector was able to detect 83% of defects in the test set; out of the missing 17%, only 0.77% are very severe defects.","This work provides an example of applying deep learning-based object detection into an important but quiet engineering field.","It also gives some practical pointers on how to annotate peculiar \"object\", such as defects."],"url":"http://arxiv.org/abs/2404.06219v1","category":"cs.CV"}
{"created":"2024-04-09 11:12:39","title":"Quantum Circuit $C^*$-algebra Net","abstract":"This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits. Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network. By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates. This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks. As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms. Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks.","sentences":["This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits.","Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network.","By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates.","This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks.","As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms.","Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks."],"url":"http://arxiv.org/abs/2404.06218v1","category":"cs.LG"}
{"created":"2024-04-09 11:10:00","title":"VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection","abstract":"Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}.","sentences":["Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications.","While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention.","Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data.","In this paper, we delve into textual OOD detection with Transformers.","We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance.","We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers.","Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability.","Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}."],"url":"http://arxiv.org/abs/2404.06217v1","category":"cs.CL"}
{"created":"2024-04-09 11:07:57","title":"Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking","abstract":"As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated. While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths. We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm. Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed. Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs. This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process. We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible.","sentences":["As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated.","While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths.","We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm.","Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed.","Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs.","This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process.","We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible."],"url":"http://arxiv.org/abs/2404.06216v1","category":"cs.CR"}
{"created":"2024-04-09 11:03:09","title":"Quantum Approach to Bound States in Field Theory","abstract":"It is well known that (possibly non-unique) suitable field dynamics can be prescribed in spacetimes with timelike boundaries by means of appropriate boundary conditions. In Ref. [J. Math. Phys. {\\bf 21}, 2802 (1980)], Wald derived a conserved energy functional for each prescribed dynamics. This conserved energy is related to the positive self-adjoint extensions of the spatial part $A$ of the wave equation $\\partial^2\\Phi/\\partial t^2=-A\\Phi$ ($A$ may not be, in principle, essentially self-adjoint). This is quite surprising since the canonical energy is not conserved in these cases. In this paper, we rederive this energy functional from an action principle (with appropriate boundary terms) following Ref. [Phys. Rev. D, {\\bf 69}, 085005, (2004)] and consider field dynamics arising from non-positive self-adjoint extensions of $A$. The spectrum of the resulting theory fails to be positive and unstable mode solutions for classical fields come to light. By studying fields in half-Minkowski spacetime, we illustrate that these unstable classical solutions come as a consequence of an inverted parabolic potential governing their dynamics. From the quantum mechanical point of view, this leads to an effective inverted harmonic oscillator at the boundary. We then explore these unstable modes behavior, as well as their instabilities, at the quantum level.","sentences":["It is well known that (possibly non-unique) suitable field dynamics can be prescribed in spacetimes with timelike boundaries by means of appropriate boundary conditions.","In Ref.","[J. Math. Phys. {\\bf 21}, 2802 (1980)], Wald derived a conserved energy functional for each prescribed dynamics.","This conserved energy is related to the positive self-adjoint extensions of the spatial part $A$ of the wave equation $\\partial^2\\Phi/\\partial t^2=-A\\Phi$ ($A$ may not be, in principle, essentially self-adjoint).","This is quite surprising since the canonical energy is not conserved in these cases.","In this paper, we rederive this energy functional from an action principle (with appropriate boundary terms) following Ref.","[Phys. Rev. D, {\\bf 69}, 085005, (2004)] and consider field dynamics arising from non-positive self-adjoint extensions of $A$.","The spectrum of the resulting theory fails to be positive and unstable mode solutions for classical fields come to light.","By studying fields in half-Minkowski spacetime, we illustrate that these unstable classical solutions come as a consequence of an inverted parabolic potential governing their dynamics.","From the quantum mechanical point of view, this leads to an effective inverted harmonic oscillator at the boundary.","We then explore these unstable modes behavior, as well as their instabilities, at the quantum level."],"url":"http://arxiv.org/abs/2404.06213v1","category":"hep-th"}
{"created":"2024-04-09 11:00:19","title":"OmniFusion Technical Report","abstract":"Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM). We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral). Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion.","sentences":["Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM).","We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality.","We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral).","Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU.","We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion."],"url":"http://arxiv.org/abs/2404.06212v1","category":"cs.CV"}
{"created":"2024-04-09 10:58:21","title":"Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models","abstract":"While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that LLMs have memorized many popular tabular datasets verbatim. We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training. We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the in-context statistical learning abilities of LLMs. Without fine-tuning, we find them to be limited. This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge. Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training. We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker","sentences":["While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over.","In this work, we address this concern for tabular data.","Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training.","This investigation reveals that LLMs have memorized many popular tabular datasets verbatim.","We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training.","We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting.","At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations.","We then investigate the in-context statistical learning abilities of LLMs.","Without fine-tuning, we find them to be limited.","This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge.","Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training.","We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker"],"url":"http://arxiv.org/abs/2404.06209v1","category":"cs.LG"}
{"created":"2024-04-09 10:56:46","title":"Leveraging edge detection and neural networks for better UAV localization","abstract":"We propose a novel method for geolocalizing Unmanned Aerial Vehicles (UAVs) in environments lacking Global Navigation Satellite Systems (GNSS). Current state-of-the-art techniques employ an offline-trained encoder to generate a vector representation (embedding) of the UAV's current view, which is then compared with pre-computed embeddings of geo-referenced images to determine the UAV's position. Here, we demonstrate that the performance of these methods can be significantly enhanced by preprocessing the images to extract their edges, which exhibit robustness to seasonal and illumination variations. Furthermore, we establish that utilizing edges enhances resilience to orientation and altitude inaccuracies. Additionally, we introduce a confidence criterion for localization. Our findings are substantiated through synthetic experiments.","sentences":["We propose a novel method for geolocalizing Unmanned Aerial Vehicles (UAVs) in environments lacking Global Navigation Satellite Systems (GNSS).","Current state-of-the-art techniques employ an offline-trained encoder to generate a vector representation (embedding) of the UAV's current view, which is then compared with pre-computed embeddings of geo-referenced images to determine the UAV's position.","Here, we demonstrate that the performance of these methods can be significantly enhanced by preprocessing the images to extract their edges, which exhibit robustness to seasonal and illumination variations.","Furthermore, we establish that utilizing edges enhances resilience to orientation and altitude inaccuracies.","Additionally, we introduce a confidence criterion for localization.","Our findings are substantiated through synthetic experiments."],"url":"http://arxiv.org/abs/2404.06207v1","category":"cs.CV"}
{"created":"2024-04-09 10:53:29","title":"Deep Learning Method for Computing Committor Functions with Adaptive Sampling","abstract":"The committor function is a central object for quantifying the transitions between metastable states of dynamical systems. Recently, a number of computational methods based on deep neural networks have been developed for computing the high-dimensional committor function. The success of the methods relies on sampling adequate data for the transition, which still is a challenging task for complex systems at low temperatures. In this work, we propose a deep learning method with two novel adaptive sampling schemes (I and II). In the two schemes, the data are generated actively with a modified potential where the bias potential is constructed from the learned committor function. We theoretically demonstrate the advantages of the sampling schemes and show that the data in sampling scheme II are uniformly distributed along the transition tube. This makes a promising method for studying the transition of complex systems. The efficiency of the method is illustrated in high-dimensional systems including the alanine dipeptide and a solvated dimer system.","sentences":["The committor function is a central object for quantifying the transitions between metastable states of dynamical systems.","Recently, a number of computational methods based on deep neural networks have been developed for computing the high-dimensional committor function.","The success of the methods relies on sampling adequate data for the transition, which still is a challenging task for complex systems at low temperatures.","In this work, we propose a deep learning method with two novel adaptive sampling schemes (I and II).","In the two schemes, the data are generated actively with a modified potential where the bias potential is constructed from the learned committor function.","We theoretically demonstrate the advantages of the sampling schemes and show that the data in sampling scheme II are uniformly distributed along the transition tube.","This makes a promising method for studying the transition of complex systems.","The efficiency of the method is illustrated in high-dimensional systems including the alanine dipeptide and a solvated dimer system."],"url":"http://arxiv.org/abs/2404.06206v1","category":"physics.comp-ph"}
{"created":"2024-04-09 10:52:35","title":"Precise measurements of $W$- and $Z$-boson transverse momentum spectra with the ATLAS detector using $pp$ collisions at $\\sqrt{s} = 5.02$ TeV and $13$ TeV","abstract":"This paper describes measurements of the transverse momentum spectra of $W$ and $Z$ bosons produced in proton-proton collisions at centre-of-mass energies of $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV with the ATLAS experiment at the Large Hadron Collider. Measurements are performed in the electron and muon channels, $W \\to \\ell\\nu$ and $Z \\to \\ell\\ell$ ($\\ell=e$ or $\\mu$), and for $W$ events further separated by charge. The data were collected in 2017 and 2018, in dedicated runs with reduced instantaneous luminosity, and correspond to $255$ pb$^{-1}$ and $338$ pb$^{-1}$ at $\\sqrt{s}=5.02$ TeV and $13$ TeV, respectively. These conditions optimise the reconstruction of the $W$-boson transverse momentum. The distributions observed in the electron and muon channels are unfolded, combined, and compared to QCD calculations based on parton shower Monte Carlo event generators and analytical resummation. The description of the transverse momentum distributions by Monte Carlo event generators is imperfect and shows significant differences largely common to $W^-$, $W^+$ and $Z$ production. The agreement is better at $\\sqrt{s}=5.02$ TeV, especially for predictions that were tuned to $Z$ production data at $\\sqrt{s}=7$ TeV. Higher-order, resummed predictions based on DYTurbo generally match the data best across the spectra. Distribution ratios are also presented and test the understanding of differences between the production processes.","sentences":["This paper describes measurements of the transverse momentum spectra of $W$ and $Z$ bosons produced in proton-proton collisions at centre-of-mass energies of $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV with the ATLAS experiment at the Large Hadron Collider.","Measurements are performed in the electron and muon channels, $W \\to \\ell\\nu$ and $Z \\to \\ell\\ell$ ($\\ell=e$ or $\\mu$), and for $W$ events further separated by charge.","The data were collected in 2017 and 2018, in dedicated runs with reduced instantaneous luminosity, and correspond to $255$ pb$^{-1}$ and $338$ pb$^{-1}$ at $\\sqrt{s}=5.02$ TeV and $13$ TeV, respectively.","These conditions optimise the reconstruction of the $W$-boson transverse momentum.","The distributions observed in the electron and muon channels are unfolded, combined, and compared to QCD calculations based on parton shower Monte Carlo event generators and analytical resummation.","The description of the transverse momentum distributions by Monte Carlo event generators is imperfect and shows significant differences largely common to $W^-$, $W^+$ and $Z$ production.","The agreement is better at $\\sqrt{s}=5.02$ TeV, especially for predictions that were tuned to $Z$ production data at $\\sqrt{s}=7$ TeV. Higher-order, resummed predictions based on DYTurbo generally match the data best across the spectra.","Distribution ratios are also presented and test the understanding of differences between the production processes."],"url":"http://arxiv.org/abs/2404.06204v1","category":"hep-ex"}
{"created":"2024-04-09 10:49:23","title":"A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks","abstract":"Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real time. Stream processing frameworks facilitate scalable computing by distributing the application's execution across multiple machines. Despite performance being extensively studied, the measurement of fault tolerance-a key and most appealing feature offered by stream processing frameworks-has still not been measured properly with updated and comprehensive testbeds. Moreover, the impact that fault recovery can have on performance is mostly ignored. This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming. Our benchmarking analysis is inspired by chaos engineering to inject failures. Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing. In particular, the results indicate that Flink can be the fastest and stablest under failures. Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current repartitioning strategy that can be suboptimal in terms of load balancing. Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency. Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our benchmark; (iii) identify, prevent, and assist in solving potential issues in production deployments.","sentences":["Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real time.","Stream processing frameworks facilitate scalable computing by distributing the application's execution across multiple machines.","Despite performance being extensively studied, the measurement of fault tolerance-a key and most appealing feature offered by stream processing frameworks-has still not been measured properly with updated and comprehensive testbeds.","Moreover, the impact that fault recovery can have on performance is mostly ignored.","This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming.","Our benchmarking analysis is inspired by chaos engineering to inject failures.","Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing.","In particular, the results indicate that Flink can be the fastest and stablest under failures.","Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current repartitioning strategy that can be suboptimal in terms of load balancing.","Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency.","Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our benchmark; (iii) identify, prevent, and assist in solving potential issues in production deployments."],"url":"http://arxiv.org/abs/2404.06203v1","category":"cs.DC"}
{"created":"2024-04-09 10:47:43","title":"Automated National Urban Map Extraction","abstract":"Developing countries usually lack the proper governance means to generate and regularly update a national rooftop map. Using traditional photogrammetry and surveying methods to produce a building map at the federal level is costly and time consuming. Using earth observation and deep learning methods, we can bridge this gap and propose an automated pipeline to fetch such national urban maps. This paper aims to exploit the power of fully convolutional neural networks for multi-class buildings' instance segmentation to leverage high object-wise accuracy results. Buildings' instance segmentation from sub-meter high-resolution satellite images can be achieved with relatively high pixel-wise metric scores. We detail all engineering steps to replicate this work and ensure highly accurate results in dense and slum areas witnessed in regions that lack proper urban planning in the Global South. We applied a case study of the proposed pipeline to Lebanon and successfully produced the first comprehensive national building footprint map with approximately 1 Million units with an 84% accuracy. The proposed architecture relies on advanced augmentation techniques to overcome dataset scarcity, which is often the case in developing countries.","sentences":["Developing countries usually lack the proper governance means to generate and regularly update a national rooftop map.","Using traditional photogrammetry and surveying methods to produce a building map at the federal level is costly and time consuming.","Using earth observation and deep learning methods, we can bridge this gap and propose an automated pipeline to fetch such national urban maps.","This paper aims to exploit the power of fully convolutional neural networks for multi-class buildings' instance segmentation to leverage high object-wise accuracy results.","Buildings' instance segmentation from sub-meter high-resolution satellite images can be achieved with relatively high pixel-wise metric scores.","We detail all engineering steps to replicate this work and ensure highly accurate results in dense and slum areas witnessed in regions that lack proper urban planning in the Global South.","We applied a case study of the proposed pipeline to Lebanon and successfully produced the first comprehensive national building footprint map with approximately 1 Million units with an 84% accuracy.","The proposed architecture relies on advanced augmentation techniques to overcome dataset scarcity, which is often the case in developing countries."],"url":"http://arxiv.org/abs/2404.06202v1","category":"cs.CV"}
{"created":"2024-04-09 10:47:02","title":"Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning","abstract":"Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.","sentences":["Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond.","Like traditional SE tools, open-source collaboration is key in realising the excellent products.","However, with AI models, the essential need is in data.","The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data.","However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects.","This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community.","Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations.","Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected.","We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security.","Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control.","Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance."],"url":"http://arxiv.org/abs/2404.06201v1","category":"cs.SE"}
{"created":"2024-04-09 10:41:59","title":"The impact of data set similarity and diversity on transfer learning success in time series forecasting","abstract":"Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning. While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to transfer learning success. Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data. We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias.","sentences":["Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning.","While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to transfer learning success.","Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation.","We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data.","We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias."],"url":"http://arxiv.org/abs/2404.06198v1","category":"cs.LG"}
{"created":"2024-04-09 10:38:10","title":"Permissible extensions of classical to quantum games combining three strategies","abstract":"We study the extension of classical games to the quantum domain, generated by the addition of one unitary strategy to two classical strategies of each player. The conditions that need to be met by unitary operations to ensure that the extended game is invariant with respect to the isomorphic transformations of the input game are determined. It has been shown that there are three types of these extensions, two of them are purely quantum. On the other hand, it has been demonstrated that the extensions of two versions of the same classical game by a unitary operator that does not meet these conditions may result in quantum games that are non-equivalent, e.g. having different Nash equilibria. We use the obtained results to extend the classical Prisoner's Dilemma game to a quantum game that has a unique Nash equilibrium closer to Pareto-optimal solutions than the original one.","sentences":["We study the extension of classical games to the quantum domain, generated by the addition of one unitary strategy to two classical strategies of each player.","The conditions that need to be met by unitary operations to ensure that the extended game is invariant with respect to the isomorphic transformations of the input game are determined.","It has been shown that there are three types of these extensions, two of them are purely quantum.","On the other hand, it has been demonstrated that the extensions of two versions of the same classical game by a unitary operator that does not meet these conditions may result in quantum games that are non-equivalent, e.g. having different Nash equilibria.","We use the obtained results to extend the classical Prisoner's Dilemma game to a quantum game that has a unique Nash equilibrium closer to Pareto-optimal solutions than the original one."],"url":"http://arxiv.org/abs/2404.06196v1","category":"quant-ph"}
{"created":"2024-04-09 10:28:09","title":"Uniform approximation on certain polynomial polyhedra in $\\mathbb{C}^2$","abstract":"In this paper we extend the dichotomy given by Samuelsson and Wold that can be thought of as an analogue of the Wermer maximality theorem in $\\mathbb{C}^2$ for certain polynomial polyhedra. We consider complex non-degenerate simply connected polynomial polyhedra of the form $\\Omega:=\\{z\\in\\mathbb{C}^2: |p_1(z)|<1, |p_2(z)|<1\\}$ such that $\\overline{\\Omega}$ is compact. Under a mild condition of the polynomials $p_1$ and $p_2$, we prove that either the uniform algebra, generated by polynomials and some continuous functions $f_1,\\dots, f_N$ on the distinguished boundary that extends as pluriharmonic functions on $\\Omega$, is all continuous functions on the distinguished boundary or there exists an algebraic variety in $\\overline{\\Omega}$ on which each $f_j$ is holomorphic. We also compute the polynomial hull of the graph of pluriharmonic functions in some cases where the pluriharmonic functions are conjugates of holomorphic polynomials. We also give a couple of general theorem about uniform approximation on the domains with low boundary regularity.","sentences":["In this paper we extend the dichotomy given by Samuelsson and Wold that can be thought of as an analogue of the Wermer maximality theorem in $\\mathbb{C}^2$ for certain polynomial polyhedra.","We consider complex non-degenerate simply connected polynomial polyhedra of the form $\\Omega:=\\{z\\in\\mathbb{C}^2: |p_1(z)|<1, |p_2(z)|<1\\}$ such that $\\overline{\\Omega}$ is compact.","Under a mild condition of the polynomials $p_1$ and $p_2$, we prove that either the uniform algebra, generated by polynomials and some continuous functions $f_1,\\dots, f_N$ on the distinguished boundary that extends as pluriharmonic functions on $\\Omega$, is all continuous functions on the distinguished boundary or there exists an algebraic variety in $\\overline{\\Omega}$ on which each $f_j$ is holomorphic.","We also compute the polynomial hull of the graph of pluriharmonic functions in some cases where the pluriharmonic functions are conjugates of holomorphic polynomials.","We also give a couple of general theorem about uniform approximation on the domains with low boundary regularity."],"url":"http://arxiv.org/abs/2404.06195v1","category":"math.CV"}
{"created":"2024-04-09 10:27:22","title":"Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection","abstract":"Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and overlook the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.","sentences":["Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes.","However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances.","In addition, these detectors primarily rely on category names and overlook the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone.","In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs).","Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process.","Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions.","Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition.","Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection.","The code and models are available at https://github.com/ltttpku/CMD-SE-release."],"url":"http://arxiv.org/abs/2404.06194v1","category":"cs.CV"}
{"created":"2024-04-09 10:23:01","title":"Study of the timelike electromagnetic form factors of the $\u039b_c$","abstract":"In this paper, the reaction of electron-positron annihilation into $\\Lambda_c^+\\bar{\\Lambda}_c^-$ is investigated. The $\\Lambda_c^+\\bar{\\Lambda}_c^-$ scattering amplitudes are obtained by solving the Lippmann-Schwinger equation. The contact, annihilation, and two pseudoscalar-exchange potentials are taken into account in the spirit of the chiral effective field theory. The amplitudes of $e^+e^-\\to \\Lambda_c^+\\bar{\\Lambda}_c^-$ are constructed by the distorted wave Born approximation method, with the final state interactions of the $\\Lambda_c^+\\bar{\\Lambda}_c^-$ re-scattering implemented. By fitting to the experimental data, the unknown couplings are fixed, and high-quality solutions are obtained. With these amplitudes, the individual electromagnetic form factors in the timelike region, $G_E^{\\Lambda_c}$, $G_M^{\\Lambda_c}$, and their ratio, $G_E^{\\Lambda_c}/G_M^{\\Lambda_c}$, are extracted. Both modulus and phases are predicted. These individual electromagnetic form factors reveal new insights into the properties of the $\\Lambda_c$. The separated contributions of the Born term, contact, annihilation, as well as the two pseudoscalar exchange potentials to the electromagnetic form factors are isolated. It is found that the Born term dominates the whole energy region. The contact term plays a crucial role in the enhancement near the threshold, and the annihilation term is essential in generating the fluctuation of the electromagnetic form factors.","sentences":["In this paper, the reaction of electron-positron annihilation into $\\Lambda_c^+\\bar{\\Lambda}_c^-$ is investigated.","The $\\Lambda_c^+\\bar{\\Lambda}_c^-$ scattering amplitudes are obtained by solving the Lippmann-Schwinger equation.","The contact, annihilation, and two pseudoscalar-exchange potentials are taken into account in the spirit of the chiral effective field theory.","The amplitudes of $e^+e^-\\to \\Lambda_c^+\\bar{\\Lambda}_c^-$ are constructed by the distorted wave Born approximation method, with the final state interactions of the $\\Lambda_c^+\\bar{\\Lambda}_c^-$ re-scattering implemented.","By fitting to the experimental data, the unknown couplings are fixed, and high-quality solutions are obtained.","With these amplitudes, the individual electromagnetic form factors in the timelike region, $G_E^{\\Lambda_c}$, $G_M^{\\Lambda_c}$, and their ratio, $G_E^{\\Lambda_c}/G_M^{\\Lambda_c}$, are extracted.","Both modulus and phases are predicted.","These individual electromagnetic form factors reveal new insights into the properties of the $\\Lambda_c$. The separated contributions of the Born term, contact, annihilation, as well as the two pseudoscalar exchange potentials to the electromagnetic form factors are isolated.","It is found that the Born term dominates the whole energy region.","The contact term plays a crucial role in the enhancement near the threshold, and the annihilation term is essential in generating the fluctuation of the electromagnetic form factors."],"url":"http://arxiv.org/abs/2404.06191v1","category":"hep-ph"}
{"created":"2024-04-09 10:22:13","title":"Transport resistance strikes back: unveiling its impact on fill factor losses in organic solar cells","abstract":"The fill factor ($FF$) is a critical parameter for solar cell efficiency, yet its analytical description is challenging due to the interplay between recombination and charge extraction processes. An often overlooked yet significant factor contributing to $FF$ losses, beyond recombination, is the influence of charge transport. In most state-of-the-art organic solar cells, the primary limitation of the $FF$ arises not from recombination but rather from low conductivity, highlighting the need for refined models to predict the $FF$ accurately. Here, we extend the analytical model for transport resistance to a more general case. Drawing from a large set of experimental current-voltage and light intensity-dependent open-circuit voltage data, we systematically incorporate crucial details previously omitted in the model. Consequently, we introduce a straightforward set of equations to predict the $FF$ of a solar cell, enabling the differentiation of losses attributed to recombination and transport resistance. Our study provides valuable insights into strategies for mitigating $FF$ losses based on the experimentally validated analytical model, guiding the development of more efficient solar cell designs and optimization strategies.","sentences":["The fill factor ($FF$) is a critical parameter for solar cell efficiency, yet its analytical description is challenging due to the interplay between recombination and charge extraction processes.","An often overlooked yet significant factor contributing to $FF$ losses, beyond recombination, is the influence of charge transport.","In most state-of-the-art organic solar cells, the primary limitation of the $FF$ arises not from recombination but rather from low conductivity, highlighting the need for refined models to predict the $FF$ accurately.","Here, we extend the analytical model for transport resistance to a more general case.","Drawing from a large set of experimental current-voltage and light intensity-dependent open-circuit voltage data, we systematically incorporate crucial details previously omitted in the model.","Consequently, we introduce a straightforward set of equations to predict the $FF$ of a solar cell, enabling the differentiation of losses attributed to recombination and transport resistance.","Our study provides valuable insights into strategies for mitigating $FF$ losses based on the experimentally validated analytical model, guiding the development of more efficient solar cell designs and optimization strategies."],"url":"http://arxiv.org/abs/2404.06190v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 10:15:18","title":"Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions. To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes. In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values. It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values. By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach. We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks. These modules lead to reliable value estimation and efficient policy learning from offline data. Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions. Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency.","sentences":["Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions.","To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes.","In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values.","It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values.","By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach.","We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks.","These modules lead to reliable value estimation and efficient policy learning from offline data.","Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions.","Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency."],"url":"http://arxiv.org/abs/2404.06188v1","category":"cs.LG"}
{"created":"2024-04-09 10:12:34","title":"Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles","abstract":"Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach.","sentences":["Crossword puzzles are popular linguistic games often used as tools to engage students in learning.","Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles.","Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing.","In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs).","By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context.","With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues.","We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword.","Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.06186v1","category":"cs.CL"}
{"created":"2024-04-09 10:08:03","title":"Shock wave generation and propagation in dissipative and nonlocal nonlinear Rydberg media","abstract":"We investigate the generation of optical shock waves in strongly interacting Rydberg atomic gases with a spatially homogeneous dissipative potential. The Rydberg atom interaction induces an optical nonlocal nonlinarity. We focus on local nonlinear ($R_b\\ll R_0$) and nonlocal nonlinear ($R_b\\sim R_0$) regimes, where $R_b$ and $R_0$ are the characteristic length of the Rydberg nonlinearity and beam width, respectively. In the local regime, we show spatial width and contrast of the shock wave change monotonically when increasing strength of the dissipative potential and optical intensity. In the nonlocal regime, the characteristic quantity of the shock wave depend on $R_b/R_0$ and dissipative potential nontrivially and on the intensity monotonically. We find that formation of shock waves dominantly takes place when $R_b$ is smaller than $R_0$, while the propagation dynamics is largely linear when $R_b$ is comparable to or larger than $R_0$. Our results reveal nontrivial roles played by dissipation and nonlocality in the generation of shock waves, and provide a route to manipulate their profiles and stability. Our study furthermore opens new avenues to explore non-Hermitian physics, and nonlinear wave generation and propagation by controlling dissipation and nonlocality in the Rydberg media.","sentences":["We investigate the generation of optical shock waves in strongly interacting Rydberg atomic gases with a spatially homogeneous dissipative potential.","The Rydberg atom interaction induces an optical nonlocal nonlinarity.","We focus on local nonlinear ($R_b\\ll R_0$) and nonlocal nonlinear ($R_b\\sim R_0$) regimes, where $R_b$ and $R_0$ are the characteristic length of the Rydberg nonlinearity and beam width, respectively.","In the local regime, we show spatial width and contrast of the shock wave change monotonically when increasing strength of the dissipative potential and optical intensity.","In the nonlocal regime, the characteristic quantity of the shock wave depend on $R_b/R_0$ and dissipative potential nontrivially and on the intensity monotonically.","We find that formation of shock waves dominantly takes place when $R_b$ is smaller than $R_0$, while the propagation dynamics is largely linear when $R_b$ is comparable to or larger than $R_0$. Our results reveal nontrivial roles played by dissipation and nonlocality in the generation of shock waves, and provide a route to manipulate their profiles and stability.","Our study furthermore opens new avenues to explore non-Hermitian physics, and nonlinear wave generation and propagation by controlling dissipation and nonlocality in the Rydberg media."],"url":"http://arxiv.org/abs/2404.06183v1","category":"physics.optics"}
{"created":"2024-04-09 10:06:43","title":"Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI","abstract":"In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements. For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.","sentences":["In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest.","Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections.","In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges.","We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit.","Particularly, the framework comprises three stages: data collection, data analysis, and data delivery.","In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements.","For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme.","To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI.","Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach.","The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study."],"url":"http://arxiv.org/abs/2404.06182v1","category":"cs.NI"}
{"created":"2024-04-09 10:04:06","title":"EPL: Evidential Prototype Learning for Semi-supervised Medical Image Segmentation","abstract":"Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously. To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking. The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features. The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy.","sentences":["Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously.","To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking.","The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features.","The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy."],"url":"http://arxiv.org/abs/2404.06181v1","category":"cs.CV"}
{"created":"2024-04-09 10:03:44","title":"YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images","abstract":"Detecting objects from aerial images poses significant challenges due to the following factors: 1) Aerial images typically have very large sizes, generally with millions or even hundreds of millions of pixels, while computational resources are limited. 2) Small object size leads to insufficient information for effective detection. 3) Non-uniform object distribution leads to computational resource wastage. To address these issues, we propose YOLC (You Only Look Clusters), an efficient and effective framework that builds on an anchor-free object detector, CenterNet. To overcome the challenges posed by large-scale images and non-uniform object distribution, we introduce a Local Scale Module (LSM) that adaptively searches cluster regions for zooming in for accurate detection. Additionally, we modify the regression loss using Gaussian Wasserstein distance (GWD) to obtain high-quality bounding boxes. Deformable convolution and refinement methods are employed in the detection head to enhance the detection of small objects. We perform extensive experiments on two aerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the effectiveness and superiority of our proposed approach.","sentences":["Detecting objects from aerial images poses significant challenges due to the following factors: 1) Aerial images typically have very large sizes, generally with millions or even hundreds of millions of pixels, while computational resources are limited.","2) Small object size leads to insufficient information for effective detection.","3) Non-uniform object distribution leads to computational resource wastage.","To address these issues, we propose YOLC (You Only Look Clusters), an efficient and effective framework that builds on an anchor-free object detector, CenterNet.","To overcome the challenges posed by large-scale images and non-uniform object distribution, we introduce a Local Scale Module (LSM) that adaptively searches cluster regions for zooming in for accurate detection.","Additionally, we modify the regression loss using Gaussian Wasserstein distance (GWD) to obtain high-quality bounding boxes.","Deformable convolution and refinement methods are employed in the detection head to enhance the detection of small objects.","We perform extensive experiments on two aerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the effectiveness and superiority of our proposed approach."],"url":"http://arxiv.org/abs/2404.06180v1","category":"cs.CV"}
{"created":"2024-04-09 09:58:10","title":"Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised Medical Image Segmentation","abstract":"Although the existing uncertainty-based semi-supervised medical segmentation methods have achieved excellent performance, they usually only consider a single uncertainty evaluation, which often fails to solve the problem related to credibility completely. Therefore, based on the framework of evidential deep learning, this paper integrates the evidential predictive results in the cross-region of mixed and original samples to reallocate the confidence degree and uncertainty measure of each voxel, which is realized by emphasizing uncertain information of probability assignments fusion rule of traditional evidence theory. Furthermore, we design a voxel-level asymptotic learning strategy by introducing information entropy to combine with the fused uncertainty measure to estimate voxel prediction more precisely. The model will gradually pay attention to the prediction results with high uncertainty in the learning process, to learn the features that are difficult to master. The experimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the superior performance of our proposed method in comparison with the existing state of the arts.","sentences":["Although the existing uncertainty-based semi-supervised medical segmentation methods have achieved excellent performance, they usually only consider a single uncertainty evaluation, which often fails to solve the problem related to credibility completely.","Therefore, based on the framework of evidential deep learning, this paper integrates the evidential predictive results in the cross-region of mixed and original samples to reallocate the confidence degree and uncertainty measure of each voxel, which is realized by emphasizing uncertain information of probability assignments fusion rule of traditional evidence theory.","Furthermore, we design a voxel-level asymptotic learning strategy by introducing information entropy to combine with the fused uncertainty measure to estimate voxel prediction more precisely.","The model will gradually pay attention to the prediction results with high uncertainty in the learning process, to learn the features that are difficult to master.","The experimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the superior performance of our proposed method in comparison with the existing state of the arts."],"url":"http://arxiv.org/abs/2404.06177v1","category":"cs.CV"}
{"created":"2024-04-09 09:54:21","title":"Improving Interpretable Embeddings for Ad-hoc Video Search with Generative Captions and Multi-word Concept Bank","abstract":"Aligning a user query and video clips in cross-modal latent space and that with semantic concepts are two mainstream approaches for ad-hoc video search (AVS). However, the effectiveness of existing approaches is bottlenecked by the small sizes of available video-text datasets and the low quality of concept banks, which results in the failures of unseen queries and the out-of-vocabulary problem. This paper addresses these two problems by constructing a new dataset and developing a multi-word concept bank. Specifically, capitalizing on a generative model, we construct a new dataset consisting of 7 million generated text and video pairs for pre-training. To tackle the out-of-vocabulary problem, we develop a multi-word concept bank based on syntax analysis to enhance the capability of a state-of-the-art interpretable AVS method in modeling relationships between query words. We also study the impact of current advanced features on the method. Experimental results show that the integration of the above-proposed elements doubles the R@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAP on the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2% to 77%, with an average about 20%.","sentences":["Aligning a user query and video clips in cross-modal latent space and that with semantic concepts are two mainstream approaches for ad-hoc video search (AVS).","However, the effectiveness of existing approaches is bottlenecked by the small sizes of available video-text datasets and the low quality of concept banks, which results in the failures of unseen queries and the out-of-vocabulary problem.","This paper addresses these two problems by constructing a new dataset and developing a multi-word concept bank.","Specifically, capitalizing on a generative model, we construct a new dataset consisting of 7 million generated text and video pairs for pre-training.","To tackle the out-of-vocabulary problem, we develop a multi-word concept bank based on syntax analysis to enhance the capability of a state-of-the-art interpretable AVS method in modeling relationships between query words.","We also study the impact of current advanced features on the method.","Experimental results show that the integration of the above-proposed elements doubles the R@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAP on the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2% to 77%, with an average about 20%."],"url":"http://arxiv.org/abs/2404.06173v1","category":"cs.CV"}
{"created":"2024-04-09 09:50:34","title":"Full description of Benjamin-Feir instability for generalized Korteweg-de Vries equations","abstract":"In this paper we consider a family of generalized Korteweg-de Vries equations and study the linear modulational instability of small amplitude traveling waves solutions. Under explicit non-degeneracy conditions on the dispersion relation, we completely describe the spectrum near the origin of the linearized operator at such solutions and prove that the unstable spectrum (when present) is composed by branches depicting always a closed figure ''8''. We apply our abstract theorem to several equations such as the Whitham, the gravity-capillary Whitham and the Kawahara equations, confirming that the unstable spectrum of the corresponding linearized operators exhibits a figure ''8'' instability, as it was observed before only numerically. Our method of proof uses a symplectic version of Kato's theory of similarity transformation to reduce the problem to determine the eigenvalues of a $3 \\times 3$ complex Hamiltonian and reversible matrix. Then, via a block-diagonalization procedure, we conjugate such matrix into a block-diagonal one composed by a $2\\times 2$ Hamiltonian and reversible matrix, describing the unstable spectrum, and a single purely imaginary element describing the stable eigenvalue.","sentences":["In this paper we consider a family of generalized Korteweg-de Vries equations and study the linear modulational instability of small amplitude traveling waves solutions.","Under explicit non-degeneracy conditions on the dispersion relation, we completely describe the spectrum near the origin of the linearized operator at such solutions and prove that the unstable spectrum (when present) is composed by branches depicting always a closed figure ''8''.","We apply our abstract theorem to several equations such as the Whitham, the gravity-capillary Whitham and the Kawahara equations, confirming that the unstable spectrum of the corresponding linearized operators exhibits a figure ''8'' instability, as it was observed before only numerically.","Our method of proof uses a symplectic version of Kato's theory of similarity transformation to reduce the problem to determine the eigenvalues of a $3 \\times 3$ complex Hamiltonian and reversible matrix.","Then, via a block-diagonalization procedure, we conjugate such matrix into a block-diagonal one composed by a $2\\times 2$ Hamiltonian and reversible matrix, describing the unstable spectrum, and a single purely imaginary element describing the stable eigenvalue."],"url":"http://arxiv.org/abs/2404.06172v1","category":"math.AP"}
{"created":"2024-04-09 09:50:02","title":"Intelligence and Motion Models of Continuum Robots: an Overview","abstract":"Many technical solutions are bio-inspired. Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess. Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication. In case of a lost connection, robot autonomy is required. Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience. However these methods are not well investigated yet. Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet. Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems. We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research. The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself. For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated. In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications. We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots.","sentences":["Many technical solutions are bio-inspired.","Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess.","Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication.","In case of a lost connection, robot autonomy is required.","Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience.","However these methods are not well investigated yet.","Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet.","Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems.","We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research.","The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself.","For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated.","In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications.","We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots."],"url":"http://arxiv.org/abs/2404.06171v1","category":"cs.RO"}
{"created":"2024-04-09 09:49:57","title":"CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers","abstract":"Contrastive Language-Image Pre-training (CLIP) has been shown to improve zero-shot generalization capabilities of language and vision models. In this paper, we extend CLIP for efficient knowledge distillation, by utilizing embeddings as teachers. Typical knowledge distillation frameworks require running forward passes through a teacher model, which is often prohibitive in the case of billion or trillion parameter teachers. In these cases, using only the embeddings of the teacher models to guide the distillation can yield significant computational savings. Our preliminary findings show that CLIP-based knowledge distillation with embeddings can outperform full scale knowledge distillation using $9\\times$ less memory and $8\\times$ less training time. Code available at: https://github.com/lnairGT/CLIP-Distillation/","sentences":["Contrastive Language-Image Pre-training (CLIP) has been shown to improve zero-shot generalization capabilities of language and vision models.","In this paper, we extend CLIP for efficient knowledge distillation, by utilizing embeddings as teachers.","Typical knowledge distillation frameworks require running forward passes through a teacher model, which is often prohibitive in the case of billion or trillion parameter teachers.","In these cases, using only the embeddings of the teacher models to guide the distillation can yield significant computational savings.","Our preliminary findings show that CLIP-based knowledge distillation with embeddings can outperform full scale knowledge distillation using $9\\times$ less memory and $8\\times$ less training time.","Code available at: https://github.com/lnairGT/CLIP-Distillation/"],"url":"http://arxiv.org/abs/2404.06170v1","category":"cs.LG"}
{"created":"2024-04-09 09:49:44","title":"Turbulent cascade arrests and the formation of intermediate-scale condensates","abstract":"Energy cascades lie at the heart of the dynamics of turbulent flows. In a recent study of turbulence in fluids with odd-viscosity [de Wit \\textit{et al.}, Nature \\textbf{627}, 515 (2024)], the two-dimensionalization of the flow at small scales leads to the arrest of the energy cascade and selection of an intermediate scale, between the forcing and the viscous scales. To investigate the generality of this phenomenon, we study a shell model that is carefully constructed to have three-dimensional turbulent dynamics at small wavenumbers and two-dimensional turbulent dynamics at large wavenumbers. The large scale separation that we can achieve in our shell model allows us to examine clearly the interplay between these dynamics, which leads to an arrest of the energy cascade at a transitional wavenumber and an associated accumulation of energy at the same scale. Such pile-up of energy around the transitional wavenumber is reminiscent of the formation of condensates in two-dimensional turbulence, \\textit{but, in contrast, it occurs at intermediate wavenumbers instead of the smallest wavenumber","sentences":["Energy cascades lie at the heart of the dynamics of turbulent flows.","In a recent study of turbulence in fluids with odd-viscosity [de Wit \\textit{et al.}, Nature \\textbf{627}, 515 (2024)], the two-dimensionalization of the flow at small scales leads to the arrest of the energy cascade and selection of an intermediate scale, between the forcing and the viscous scales.","To investigate the generality of this phenomenon, we study a shell model that is carefully constructed to have three-dimensional turbulent dynamics at small wavenumbers and two-dimensional turbulent dynamics at large wavenumbers.","The large scale separation that we can achieve in our shell model allows us to examine clearly the interplay between these dynamics, which leads to an arrest of the energy cascade at a transitional wavenumber and an associated accumulation of energy at the same scale.","Such pile-up of energy around the transitional wavenumber is reminiscent of the formation of condensates in two-dimensional turbulence, \\textit{but, in contrast, it occurs at intermediate wavenumbers instead of the smallest wavenumber"],"url":"http://arxiv.org/abs/2404.06169v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 09:49:08","title":"Protection of Guizhou Miao Batik Culture Based on Knowledge Graph and Deep Learning","abstract":"In the globalization trend, China's cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their subjects can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 99.0%, 99.0%, 98.9%, and 99.0%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.","sentences":["In the globalization trend, China's cultural heritage is in danger of gradually disappearing.","The protection and inheritance of these precious cultural resources has become a critical task.","This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture.","We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations.","First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph.","Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns.","Second, for the batik pattern classification, we propose an improved ResNet34 model.","By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced.","By inputting pattern images into this model, their subjects can be accurately identified, and then the underlying cultural connotations can be understood.","Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 99.0%, 99.0%, 98.9%, and 99.0%, respectively.","This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection."],"url":"http://arxiv.org/abs/2404.06168v1","category":"stat.AP"}
{"created":"2024-04-09 09:46:17","title":"scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding","abstract":"Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements. Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies. Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity. Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information. scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods. (ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity. (iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction. Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis. Our code is available at: https://github.com/XPgogogo/scCDCG.","sentences":["Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements.","Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies.","Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity.","Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information.","scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods.","(ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity.","(iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction.","Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis.","Our code is available at: https://github.com/XPgogogo/scCDCG."],"url":"http://arxiv.org/abs/2404.06167v1","category":"cs.LG"}
{"created":"2024-04-09 09:43:39","title":"Classical and quantum field theory in a box with moving boundaries: A numerical study of the Dynamical Casimir Effect","abstract":"We present a detailed description of a quantum scalar field theory within a flat spacetime confined to a cavity with perfectly reflecting moving boundaries. Moreover, we establish an equivalence between this time-dependent setting and a field theory on an acoustic metric with static Dirichlet boundary conditions. We discuss the classical and quantum aspects of the theory from the latter perspective, accompanied by the introduction of novel numerical techniques designed for the (nonperturbative) computation of particle production attributed to the Dynamical Casimir effect, applicable to arbitrary boundary trajectories. As an illustrative example of these methodologies, we compute the particle production for a massless field in 1+1 dimensions. Notably, our approaches readily extend to encompass scenarios involving massive fields and higher dimensions","sentences":["We present a detailed description of a quantum scalar field theory within a flat spacetime confined to a cavity with perfectly reflecting moving boundaries.","Moreover, we establish an equivalence between this time-dependent setting and a field theory on an acoustic metric with static Dirichlet boundary conditions.","We discuss the classical and quantum aspects of the theory from the latter perspective, accompanied by the introduction of novel numerical techniques designed for the (nonperturbative) computation of particle production attributed to the Dynamical Casimir effect, applicable to arbitrary boundary trajectories.","As an illustrative example of these methodologies, we compute the particle production for a massless field in 1+1 dimensions.","Notably, our approaches readily extend to encompass scenarios involving massive fields and higher dimensions"],"url":"http://arxiv.org/abs/2404.06166v1","category":"quant-ph"}
{"created":"2024-04-09 09:42:18","title":"Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications","abstract":"Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors. The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance. This work introduces a learning-based approach to infer the height of radar points associated with 3D objects. A novel robust regression loss is introduced to address the sparse target challenge. In addition, a multi-task training strategy is employed, emphasizing important features. The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method. The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks. Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks.","sentences":["Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors.","The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance.","This work introduces a learning-based approach to infer the height of radar points associated with 3D objects.","A novel robust regression loss is introduced to address the sparse target challenge.","In addition, a multi-task training strategy is employed, emphasizing important features.","The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method.","The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks.","Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks."],"url":"http://arxiv.org/abs/2404.06165v1","category":"cs.CV"}
{"created":"2024-04-09 09:34:25","title":"Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports","abstract":"As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization). In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively. We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command. We find that GPT-3.5 and Command fail to perform this summarization task meaningfully. For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs. This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination. We employ prompt engineering to improve GPT-4's use of numbers with limited success. Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4.","sentences":["As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior.","A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization).","In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively.","We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command.","We find that GPT-3.5 and Command fail to perform this summarization task meaningfully.","For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs.","This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information.","We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination.","We employ prompt engineering to improve GPT-4's use of numbers with limited success.","Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4."],"url":"http://arxiv.org/abs/2404.06162v1","category":"cs.CL"}
{"created":"2024-04-09 09:34:15","title":"Second order Sobolev regularity results for the generalized $p$-parabolic equation","abstract":"We study a general class of parabolic equations $$ u_t-|Du|^\\gamma\\big(\\Delta u+(p-2) \\Delta_\\infty^N u\\big)=0, $$ which can be highly degenerate or singular. This class contains as special cases the standard parabolic $p$-Laplace equation and the normalized version that arises from stochastic game theory. Utilizing the systematic approach developed in our previous work we establish second order Sobolev regularity together with a priori estimates and improved range of parameters. In addition we derive second order Sobolev estimate for a nonlinear quantity. This quantity contains many useful special cases. As a corollary we also obtain that a viscosity solution has locally $L^2$-integrable Sobolev time derivative.","sentences":["We study a general class of parabolic equations $$ u_t-|Du|^\\gamma\\big(\\Delta u+(p-2) \\Delta_\\infty^N u\\big)=0, $$ which can be highly degenerate or singular.","This class contains as special cases the standard parabolic $p$-Laplace equation and the normalized version that arises from stochastic game theory.","Utilizing the systematic approach developed in our previous work we establish second order Sobolev regularity together with a priori estimates and improved range of parameters.","In addition we derive second order Sobolev estimate for a nonlinear quantity.","This quantity contains many useful special cases.","As a corollary we also obtain that a viscosity solution has locally $L^2$-integrable Sobolev time derivative."],"url":"http://arxiv.org/abs/2404.06161v1","category":"math.AP"}
{"created":"2024-04-09 09:32:00","title":"Distributed Artificial Intelligence as a Means to Achieve Self-X-Functions for Increasing Resilience: the First Steps","abstract":"Using sensors as a means to achieve self-awareness and artificial intelligence for decision-making, may be a way to make complex systems self-adaptive, autonomous and resilient. Investigating the combination of distributed artificial intelligence methods and bio-inspired robotics can provide results that will be helpful for implementing autonomy of such robots and other complex systems. In this paper, we describe Distributed Artificial Intelligence application area, the most common examples of continuum robots and provide a description of our first steps towards implementing distributed control.","sentences":["Using sensors as a means to achieve self-awareness and artificial intelligence for decision-making, may be a way to make complex systems self-adaptive, autonomous and resilient.","Investigating the combination of distributed artificial intelligence methods and bio-inspired robotics can provide results that will be helpful for implementing autonomy of such robots and other complex systems.","In this paper, we describe Distributed Artificial Intelligence application area, the most common examples of continuum robots and provide a description of our first steps towards implementing distributed control."],"url":"http://arxiv.org/abs/2404.06159v1","category":"cs.RO"}
{"created":"2024-04-09 09:29:42","title":"A data-driven approach to UIO-based fault diagnosis","abstract":"In this paper we propose a data-driven approach to the design of a residual generator, based on a dead-beat unknown-input observer, for linear time-invariant discrete-time state-space models, whose state equation is affected both by disturbances and by actuator faults. We first review the modelbased conditions for the existence of such a residual generator, and then prove that under suitable assumptions on the collected historical data, we are both able to determine if the problem is solvable and to identify the matrices of a possible residual generator. We propose an algorithm that, based only on the collected data (and not on the system description), is able to perform both tasks. An illustrating example and some remarks on limitations and possible extensions of the current results conclude the paper.","sentences":["In this paper we propose a data-driven approach to the design of a residual generator, based on a dead-beat unknown-input observer, for linear time-invariant discrete-time state-space models, whose state equation is affected both by disturbances and by actuator faults.","We first review the modelbased conditions for the existence of such a residual generator, and then prove that under suitable assumptions on the collected historical data, we are both able to determine if the problem is solvable and to identify the matrices of a possible residual generator.","We propose an algorithm that, based only on the collected data (and not on the system description), is able to perform both tasks.","An illustrating example and some remarks on limitations and possible extensions of the current results conclude the paper."],"url":"http://arxiv.org/abs/2404.06158v1","category":"eess.SY"}
{"created":"2024-04-09 09:28:05","title":"Efficient and Robust Point Cloud Registration via Heuristics-guided Parameter Search","abstract":"Estimating the rigid transformation with 6 degrees of freedom based on a putative 3D correspondence set is a crucial procedure in point cloud registration. Existing correspondence identification methods usually lead to large outlier ratios ($>$ 95 $\\%$ is common), underscoring the significance of robust registration methods. Many researchers turn to parameter search-based strategies (e.g., Branch-and-Bround) for robust registration. Although related methods show high robustness, their efficiency is limited to the high-dimensional search space. This paper proposes a heuristics-guided parameter search strategy to accelerate the search while maintaining high robustness. We first sample some correspondences (i.e., heuristics) and then just need to sequentially search the feasible regions that make each sample an inlier. Our strategy largely reduces the search space and can guarantee accuracy with only a few inlier samples, therefore enjoying an excellent trade-off between efficiency and robustness. Since directly parameterizing the 6-dimensional nonlinear feasible region for efficient search is intractable, we construct a three-stage decomposition pipeline to reparameterize the feasible region, resulting in three lower-dimensional sub-problems that are easily solvable via our strategy. Besides reducing the searching dimension, our decomposition enables the leverage of 1-dimensional interval stabbing at all three stages for searching acceleration. Moreover, we propose a valid sampling strategy to guarantee our sampling effectiveness, and a compatibility verification setup to further accelerate our search. Extensive experiments on both simulated and real-world datasets demonstrate that our approach exhibits comparable robustness with state-of-the-art methods while achieving a significant efficiency boost.","sentences":["Estimating the rigid transformation with 6 degrees of freedom based on a putative 3D correspondence set is a crucial procedure in point cloud registration.","Existing correspondence identification methods usually lead to large outlier ratios ($>$ 95 $\\%$ is common), underscoring the significance of robust registration methods.","Many researchers turn to parameter search-based strategies (e.g., Branch-and-Bround) for robust registration.","Although related methods show high robustness, their efficiency is limited to the high-dimensional search space.","This paper proposes a heuristics-guided parameter search strategy to accelerate the search while maintaining high robustness.","We first sample some correspondences (i.e., heuristics) and then just need to sequentially search the feasible regions that make each sample an inlier.","Our strategy largely reduces the search space and can guarantee accuracy with only a few inlier samples, therefore enjoying an excellent trade-off between efficiency and robustness.","Since directly parameterizing the 6-dimensional nonlinear feasible region for efficient search is intractable, we construct a three-stage decomposition pipeline to reparameterize the feasible region, resulting in three lower-dimensional sub-problems that are easily solvable via our strategy.","Besides reducing the searching dimension, our decomposition enables the leverage of 1-dimensional interval stabbing at all three stages for searching acceleration.","Moreover, we propose a valid sampling strategy to guarantee our sampling effectiveness, and a compatibility verification setup to further accelerate our search.","Extensive experiments on both simulated and real-world datasets demonstrate that our approach exhibits comparable robustness with state-of-the-art methods while achieving a significant efficiency boost."],"url":"http://arxiv.org/abs/2404.06155v1","category":"cs.CV"}
{"created":"2024-04-09 09:25:16","title":"scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling","abstract":"Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample. While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties. Results: Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT). This method generates virtual scRNA-seq data by leveraging a real dataset. The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs). This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples. This scheme allows us to learn data features from actual scRNA-seq samples during model training. Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance. Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples. Availability and implementation: https://github.com/DongShengze/scRDiT","sentences":["Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample.","While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties.","Results:","Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT).","This method generates virtual scRNA-seq data by leveraging a real dataset.","The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs).","This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples.","This scheme allows us to learn data features from actual scRNA-seq samples during model training.","Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance.","Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM).","scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples.","Availability and implementation: https://github.com/DongShengze/scRDiT"],"url":"http://arxiv.org/abs/2404.06153v1","category":"cs.LG"}
{"created":"2024-04-09 09:23:04","title":"HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields","abstract":"In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.","sentences":["In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images.","However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances.","Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder.","While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR).","HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps.","We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features.","The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner.","This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF."],"url":"http://arxiv.org/abs/2404.06152v1","category":"cs.CV"}
{"created":"2024-04-09 09:22:47","title":"Non-Gaussianities in primordial black hole formation and induced gravitational waves","abstract":"The most promising mechanism of generating primordial black holes (PBHs) is by the enhancement of power spectrum of the primordial curvature perturbation, which is usually accompanied by the the enhancement of non-Gaussianity that crucially changes the abundance of PBHs. In this review I will discuss how non-Gaussianity is generated in single field inflation as well as in the curvaton scenario, and then discuss how to calculate PBH mass function with such non-Gaussianities. I also show non-Gaussianity only has mild effects on the induced gravitational waves (GWs), which gives robust predictions in the mHz and nHz GW experiments.","sentences":["The most promising mechanism of generating primordial black holes (PBHs) is by the enhancement of power spectrum of the primordial curvature perturbation, which is usually accompanied by the the enhancement of non-Gaussianity that crucially changes the abundance of PBHs.","In this review I will discuss how non-Gaussianity is generated in single field inflation as well as in the curvaton scenario, and then discuss how to calculate PBH mass function with such non-Gaussianities.","I also show non-Gaussianity only has mild effects on the induced gravitational waves (GWs), which gives robust predictions in the mHz and nHz GW experiments."],"url":"http://arxiv.org/abs/2404.06151v1","category":"astro-ph.CO"}
{"created":"2024-04-09 09:09:36","title":"Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability","abstract":"Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data. Such a process finds wide application in various fields, such as finance and healthcare. While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount. The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties. In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability. Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model. We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm.","sentences":["Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data.","Such a process finds wide application in various fields, such as finance and healthcare.","While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount.","The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties.","In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP).","We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability.","Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model.","We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm."],"url":"http://arxiv.org/abs/2404.06144v1","category":"cs.LG"}
{"created":"2024-04-09 09:06:48","title":"On the shrinking solitons of generalized Ricci flow","abstract":"We show that every gradient shrinking soliton of the generalized Ricci flow on compact manifold is a Ricci soliton. And we prove that the pluriclosed soliton is gradient Kahler-Ricci soliton under a broad cohomological condition. Moreover, we construct the first example of non-trivial shrinking generalized soliton, which can serve as a singularity model of the generalized Ricci flow.","sentences":["We show that every gradient shrinking soliton of the generalized Ricci flow on compact manifold is a Ricci soliton.","And we prove that the pluriclosed soliton is gradient Kahler-Ricci soliton under a broad cohomological condition.","Moreover, we construct the first example of non-trivial shrinking generalized soliton, which can serve as a singularity model of the generalized Ricci flow."],"url":"http://arxiv.org/abs/2404.06141v1","category":"math.DG"}
{"created":"2024-04-09 09:06:36","title":"Thin Accretion disks in GR-MHD simulations","abstract":"We review some recent results of general relativistic magnetohydrodynamic (GR-MHD) simulations considering the evolution of geometrically thin disks around a central black hole. Thin disk GR-MHD simulations complement the widely used MAD (Magnetically Arrested Disk) or SANE (Standard And Normal Evolution) approaches of evolving from an initial disk torus. In particular, we discuss the dynamical evolution of the disk, its role in the formation of disk winds or jets, the impact of disk resistivity, and its potential role in generating magnetic flux by an internal disk dynamo. The main characteristics of a thin disk in our approach are the Keplerian rotation of the disk material, which allows to launch disk outflows by the Blandford-Payne magneto-centrifugal effect, in addition to the Blandford-Znajek-driven spine jet from the black hole ergosphere. Thus, for this approach, we neglect disk thermodynamics and radiative effects, concentrating predominantly on the dynamical evolution of the system. Resistive MHD further allows the investigation of physical reconnection and also dynamo action. Magnetic reconnection may generate magnetic islands of plasmoids that are ejected from the disk along with the outflow. We also discussed potential applications of thin disk in explaining the decaying phase of an outburst in black hole X-ray binaries (BH-XRBs). Post-processing of radiation using the simulated dynamical data allows to derive spectra or fluxes, e.g., in the X-ray band, and to derive potential variability characteristics.","sentences":["We review some recent results of general relativistic magnetohydrodynamic (GR-MHD) simulations considering the evolution of geometrically thin disks around a central black hole.","Thin disk GR-MHD simulations complement the widely used MAD (Magnetically Arrested Disk) or SANE (Standard And Normal Evolution) approaches of evolving from an initial disk torus.","In particular, we discuss the dynamical evolution of the disk, its role in the formation of disk winds or jets, the impact of disk resistivity, and its potential role in generating magnetic flux by an internal disk dynamo.","The main characteristics of a thin disk in our approach are the Keplerian rotation of the disk material, which allows to launch disk outflows by the Blandford-Payne magneto-centrifugal effect, in addition to the Blandford-Znajek-driven spine jet from the black hole ergosphere.","Thus, for this approach, we neglect disk thermodynamics and radiative effects, concentrating predominantly on the dynamical evolution of the system.","Resistive MHD further allows the investigation of physical reconnection and also dynamo action.","Magnetic reconnection may generate magnetic islands of plasmoids that are ejected from the disk along with the outflow.","We also discussed potential applications of thin disk in explaining the decaying phase of an outburst in black hole X-ray binaries (BH-XRBs).","Post-processing of radiation using the simulated dynamical data allows to derive spectra or fluxes, e.g., in the X-ray band, and to derive potential variability characteristics."],"url":"http://arxiv.org/abs/2404.06140v1","category":"astro-ph.HE"}
{"created":"2024-04-09 09:05:23","title":"DiffHarmony: Latent Diffusion Model Meets Image Harmonization","abstract":"Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. Diffusion models have recently promoted the rapid development of image-to-image translation tasks . However, training diffusion models from scratch is computationally intensive. Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony .","sentences":["Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task.","Diffusion models have recently promoted the rapid development of image-to-image translation tasks .","However, training diffusion models from scratch is computationally intensive.","Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics.","To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images.","Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images.","Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method.","The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony ."],"url":"http://arxiv.org/abs/2404.06139v1","category":"cs.CV"}
{"created":"2024-04-09 09:04:30","title":"Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages","abstract":"Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.","sentences":["Large language models (LLMs) show remarkable human-like capability in various domains and languages.","However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts.","To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes.","We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia.","Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia.","In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation.","Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency.","Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning."],"url":"http://arxiv.org/abs/2404.06138v1","category":"cs.CL"}
{"created":"2024-04-09 09:03:44","title":"SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection","abstract":"In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task. Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models. Through these explorations, we introduce three distinct methods that exhibit strong performance metrics. To amplify our training data, we generate additional training samples from unlabelled training subset. Furthermore, we provide a detailed comparative analysis of our approaches. Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential.","sentences":["In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task.","Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models.","Through these explorations, we introduce three distinct methods that exhibit strong performance metrics.","To amplify our training data, we generate additional training samples from unlabelled training subset.","Furthermore, we provide a detailed comparative analysis of our approaches.","Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential."],"url":"http://arxiv.org/abs/2404.06137v1","category":"cs.CL"}
{"created":"2024-04-09 09:03:06","title":"Inexact Policy Iteration Methods for Large-Scale Markov Decision Processes","abstract":"We consider inexact policy iteration methods for large-scale infinite-horizon discounted MDPs with finite spaces, a variant of policy iteration where the policy evaluation step is implemented inexactly using an iterative solver for linear systems. In the classical dynamic programming literature, a similar principle is deployed in optimistic policy iteration, where an a-priori fixed-number of iterations of value iteration is used to inexactly solve the policy evaluation step. Inspired by the connection between policy iteration and semismooth Newton's method, we investigate a class of iPI methods that mimic the inexact variants of semismooth Newton's method by adopting a parametric stopping condition to regulate the level of inexactness of the policy evaluation step. For this class of methods we discuss local and global convergence properties and derive a practical range of values for the stopping-condition parameter that provide contraction guarantees. Our analysis is general and therefore encompasses a variety of iterative solvers for policy evaluation, including the standard value iteration as well as more sophisticated ones such as GMRES. As underlined by our analysis, the selection of the inner solver is of fundamental importance for the performance of the overall method. We therefore consider different iterative methods to solve the policy evaluation step and analyze their applicability and contraction properties when used for policy evaluation. We show that the contraction properties of these methods tend to be enhanced by the specific structure of policy evaluation and that there is margin for substantial improvement in terms of convergence rate. Finally, we study the numerical performance of different instances of inexact policy iteration on large-scale MDPs for the design of health policies to control the spread of infectious diseases in epidemiology.","sentences":["We consider inexact policy iteration methods for large-scale infinite-horizon discounted MDPs with finite spaces, a variant of policy iteration where the policy evaluation step is implemented inexactly using an iterative solver for linear systems.","In the classical dynamic programming literature, a similar principle is deployed in optimistic policy iteration, where an a-priori fixed-number of iterations of value iteration is used to inexactly solve the policy evaluation step.","Inspired by the connection between policy iteration and semismooth Newton's method, we investigate a class of iPI methods that mimic the inexact variants of semismooth Newton's method by adopting a parametric stopping condition to regulate the level of inexactness of the policy evaluation step.","For this class of methods we discuss local and global convergence properties and derive a practical range of values for the stopping-condition parameter that provide contraction guarantees.","Our analysis is general and therefore encompasses a variety of iterative solvers for policy evaluation, including the standard value iteration as well as more sophisticated ones such as GMRES.","As underlined by our analysis, the selection of the inner solver is of fundamental importance for the performance of the overall method.","We therefore consider different iterative methods to solve the policy evaluation step and analyze their applicability and contraction properties when used for policy evaluation.","We show that the contraction properties of these methods tend to be enhanced by the specific structure of policy evaluation and that there is margin for substantial improvement in terms of convergence rate.","Finally, we study the numerical performance of different instances of inexact policy iteration on large-scale MDPs for the design of health policies to control the spread of infectious diseases in epidemiology."],"url":"http://arxiv.org/abs/2404.06136v1","category":"math.OC"}
{"created":"2024-04-09 08:56:48","title":"A multi-phase thermo-mechanical model for rock-ice avalanche","abstract":"We propose a novel physically-based multi-phase thermo-mechanical model for rock-ice avalanche. The model is built on a multi-phase mass flow model and extends a two-phase rock-ice avalanche model. It considers rock, ice and fluid; includes the mechanism of ice-melting and a rigorously derived dynamically changing general temperature equation for avalanching bulk mass, the first of its kind. It explains advection-diffusion of heat including the heat exchange across the rock-ice avalanche body, basal heat conduction, production and loss of heat due to frictional shearing and changing temperature, a general formulation of the ice melting rate and enhancement of temperature due to basal entrainment. The temperature equation includes a composite term containing coupled dynamics: rate of change of thermal conductivity and temperature. Ice melt intensity determines these rates as mixture conductivity evolves, characterizing distinctive thermo-mechanical processes. The model highlights essential aspects of rock-ice avalanches. Lateral heat productions play an important role in temperature evolution. Fast moving avalanches produce higher amount of heat. Fast ice melting results in substantial change in temperature. We formally derive the melting efficiency dependent general fluid production rate. The model includes internal mass and momentum exchanges between the phases and mass and momentum productions due to entrainment. The latter significantly changes the state of temperature; yet, the former exclusively characterizes rock-ice avalanche. Temperature changes are rapid when heat entrainment across the avalanche boundary is substantial. It also applies to basal heat conduction. A strong coupling exists between phase mass and momentum balances and the temperature equation. The new model offers the first-ever complete dynamical solution for simulating rock-ice avalanche with changing temperature.","sentences":["We propose a novel physically-based multi-phase thermo-mechanical model for rock-ice avalanche.","The model is built on a multi-phase mass flow model and extends a two-phase rock-ice avalanche model.","It considers rock, ice and fluid; includes the mechanism of ice-melting and a rigorously derived dynamically changing general temperature equation for avalanching bulk mass, the first of its kind.","It explains advection-diffusion of heat including the heat exchange across the rock-ice avalanche body, basal heat conduction, production and loss of heat due to frictional shearing and changing temperature, a general formulation of the ice melting rate and enhancement of temperature due to basal entrainment.","The temperature equation includes a composite term containing coupled dynamics: rate of change of thermal conductivity and temperature.","Ice melt intensity determines these rates as mixture conductivity evolves, characterizing distinctive thermo-mechanical processes.","The model highlights essential aspects of rock-ice avalanches.","Lateral heat productions play an important role in temperature evolution.","Fast moving avalanches produce higher amount of heat.","Fast ice melting results in substantial change in temperature.","We formally derive the melting efficiency dependent general fluid production rate.","The model includes internal mass and momentum exchanges between the phases and mass and momentum productions due to entrainment.","The latter significantly changes the state of temperature; yet, the former exclusively characterizes rock-ice avalanche.","Temperature changes are rapid when heat entrainment across the avalanche boundary is substantial.","It also applies to basal heat conduction.","A strong coupling exists between phase mass and momentum balances and the temperature equation.","The new model offers the first-ever complete dynamical solution for simulating rock-ice avalanche with changing temperature."],"url":"http://arxiv.org/abs/2404.06130v1","category":"physics.geo-ph"}
{"created":"2024-04-09 08:56:43","title":"Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management","abstract":"In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies. Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures. Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation. This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal human intervention. We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach's effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings. We validate our approach using a dual-arm KUKA robot.","sentences":["In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies.","Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures.","Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation.","This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal human intervention.","We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach's effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings.","We validate our approach using a dual-arm KUKA robot."],"url":"http://arxiv.org/abs/2404.06129v1","category":"cs.RO"}
{"created":"2024-04-09 08:51:05","title":"FLEX: FLEXible Federated Learning Framework","abstract":"In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount. As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection. Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy. This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments. By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques. The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains. Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications.","sentences":["In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount.","As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection.","Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy.","This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments.","By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques.","The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains.","Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications."],"url":"http://arxiv.org/abs/2404.06127v1","category":"cs.CR"}
{"created":"2024-04-09 08:49:01","title":"Hierarchical Insights: Exploiting Structural Similarities for Reliable 3D Semantic Segmentation","abstract":"Safety-critical applications like autonomous driving call for robust 3D environment perception algorithms which can withstand highly diverse and ambiguous surroundings. The predictive performance of any classification model strongly depends on the underlying dataset and the prior knowledge conveyed by the annotated labels. While the labels provide a basis for the learning process, they usually fail to represent inherent relations between the classes - representations, which are a natural element of the human perception system. We propose a training strategy which enables a 3D LiDAR semantic segmentation model to learn structural relationships between the different classes through abstraction. We achieve this by implicitly modeling those relationships through a learning rule for hierarchical multi-label classification (HMC). With a detailed analysis we show, how this training strategy not only improves the model's confidence calibration, but also preserves additional information for downstream tasks like fusion, prediction and planning.","sentences":["Safety-critical applications like autonomous driving call for robust 3D environment perception algorithms which can withstand highly diverse and ambiguous surroundings.","The predictive performance of any classification model strongly depends on the underlying dataset and the prior knowledge conveyed by the annotated labels.","While the labels provide a basis for the learning process, they usually fail to represent inherent relations between the classes - representations, which are a natural element of the human perception system.","We propose a training strategy which enables a 3D LiDAR semantic segmentation model to learn structural relationships between the different classes through abstraction.","We achieve this by implicitly modeling those relationships through a learning rule for hierarchical multi-label classification (HMC).","With a detailed analysis we show, how this training strategy not only improves the model's confidence calibration, but also preserves additional information for downstream tasks like fusion, prediction and planning."],"url":"http://arxiv.org/abs/2404.06124v1","category":"cs.CV"}
{"created":"2024-04-09 08:43:24","title":"Predicting the future applications of any stoichiometric inorganic material through learning from past literature","abstract":"Through learning from past literature, artificial intelligence models have been able to predict the future applications of various stoichiometric inorganic materials in a variety of subfields of materials science. This capacity offers exciting opportunities for boosting the research and development (R&D) of new functional materials. Unfortunately, the previous models can only provide the prediction for existing materials in past literature, but cannot predict the applications of new materials. Here, we construct a model that can predict the applications of any stoichiometric inorganic material (regardless of whether it is a new material). Historical validation confirms the high reliability of our model. Key to our model is that it allows the generation of the word embedding of any stoichiometric inorganic material, which cannot be achieved by the previous models. This work constructs a powerful model, which can predict the future applications of any stoichiometric inorganic material using only a laptop, potentially revolutionizing the R&D paradigm for new functional materials","sentences":["Through learning from past literature, artificial intelligence models have been able to predict the future applications of various stoichiometric inorganic materials in a variety of subfields of materials science.","This capacity offers exciting opportunities for boosting the research and development (R&D) of new functional materials.","Unfortunately, the previous models can only provide the prediction for existing materials in past literature, but cannot predict the applications of new materials.","Here, we construct a model that can predict the applications of any stoichiometric inorganic material (regardless of whether it is a new material).","Historical validation confirms the high reliability of our model.","Key to our model is that it allows the generation of the word embedding of any stoichiometric inorganic material, which cannot be achieved by the previous models.","This work constructs a powerful model, which can predict the future applications of any stoichiometric inorganic material using only a laptop, potentially revolutionizing the R&D paradigm for new functional materials"],"url":"http://arxiv.org/abs/2404.06120v1","category":"physics.app-ph"}
{"created":"2024-04-09 08:41:13","title":"DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation","abstract":"Text-to-3D generation, which synthesizes 3D assets according to an overall text description, has significantly progressed. However, a challenge arises when the specific appearances need customizing at designated viewpoints but referring solely to the overall description for generating 3D objects. For instance, ambiguity easily occurs when producing a T-shirt with distinct patterns on its front and back using a single overall text guidance. In this work, we propose DreamView, a text-to-image approach enabling multi-view customization while maintaining overall consistency by adaptively injecting the view-specific and overall text guidance through a collaborative text guidance injection module, which can also be lifted to 3D generation via score distillation sampling. DreamView is trained with large-scale rendered multi-view images and their corresponding view-specific texts to learn to balance the separate content manipulation in each view and the global consistency of the overall object, resulting in a dual achievement of customization and consistency. Consequently, DreamView empowers artists to design 3D objects creatively, fostering the creation of more innovative and diverse 3D assets. Code and model will be released at https://github.com/iSEE-Laboratory/DreamView.","sentences":["Text-to-3D generation, which synthesizes 3D assets according to an overall text description, has significantly progressed.","However, a challenge arises when the specific appearances need customizing at designated viewpoints but referring solely to the overall description for generating 3D objects.","For instance, ambiguity easily occurs when producing a T-shirt with distinct patterns on its front and back using a single overall text guidance.","In this work, we propose DreamView, a text-to-image approach enabling multi-view customization while maintaining overall consistency by adaptively injecting the view-specific and overall text guidance through a collaborative text guidance injection module, which can also be lifted to 3D generation via score distillation sampling.","DreamView is trained with large-scale rendered multi-view images and their corresponding view-specific texts to learn to balance the separate content manipulation in each view and the global consistency of the overall object, resulting in a dual achievement of customization and consistency.","Consequently, DreamView empowers artists to design 3D objects creatively, fostering the creation of more innovative and diverse 3D assets.","Code and model will be released at https://github.com/iSEE-Laboratory/DreamView."],"url":"http://arxiv.org/abs/2404.06119v1","category":"cs.CV"}
{"created":"2024-04-09 08:35:04","title":"Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey","abstract":"With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning. In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale. This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training. Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting. Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope.","sentences":["With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning.","In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources.","Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale.","This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures.","Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training.","Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference.","After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting.","Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases.","This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope."],"url":"http://arxiv.org/abs/2404.06114v1","category":"cs.DC"}
{"created":"2024-04-09 08:20:46","title":"Generalized Positive Energy Representations of the Group of Compactly Supported Diffeomorphisms","abstract":"Motivated by asymptotic symmetry groups in general relativity, we consider projective unitary representations $(\\overline{\\rho}, \\mathcal{H})$ of the Lie group $\\mathrm{Diff}_c(M)$ of compactly supported diffeomorphisms of a smooth manifold $M$ that satisfy a so-called generalized positive energy condition. In particular, this captures representations that are in a suitable sense compatible with a KMS state on the von Neumann algebra generated by $\\overline{\\rho}$. We show that if $M$ is connected and $\\dim(M) > 1$, then any such representation is necessarily trivial on the identity component $\\mathrm{Diff}_c(M)_0$. As an intermediate step towards this result, we determine the continuous second Lie algebra cohomology $H^2_{\\mathrm{ct}}(\\mathcal{X}_c(M), \\mathbb{R})$ of the Lie algebra of compactly supported vector fields (which is subtly different from Gelfand--Fuks cohomology).","sentences":["Motivated by asymptotic symmetry groups in general relativity, we consider projective unitary representations $(\\overline{\\rho}, \\mathcal{H})$ of the Lie group $\\mathrm{Diff}_c(M)$ of compactly supported diffeomorphisms of a smooth manifold $M$ that satisfy a so-called generalized positive energy condition.","In particular, this captures representations that are in a suitable sense compatible with a KMS state on the von Neumann algebra generated by $\\overline{\\rho}$. We show that if $M$ is connected and $\\dim(M) > 1$, then any such representation is necessarily trivial on the identity component $\\mathrm{Diff}_c(M)_0$. As an intermediate step towards this result, we determine the continuous second Lie algebra cohomology $H^2_{\\mathrm{ct}}(\\mathcal{X}_c(M), \\mathbb{R})$ of the Lie algebra of compactly supported vector fields (which is subtly different from Gelfand--Fuks cohomology)."],"url":"http://arxiv.org/abs/2404.06110v1","category":"math-ph"}
{"created":"2024-04-09 08:20:37","title":"Revising Densification in Gaussian Splatting","abstract":"In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency.","sentences":["In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis.","ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic.","Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification.","We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations.","Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency."],"url":"http://arxiv.org/abs/2404.06109v1","category":"cs.CV"}
{"created":"2024-04-09 08:19:33","title":"Symmetry-guided gradient descent for quantum neural networks","abstract":"Many supervised learning tasks have intrinsic symmetries, such as translational and rotational symmetry in image classifications. These symmetries can be exploited to enhance performance. We formulate the symmetry constraints into a concise mathematical form. We design two ways to adopt the constraints into the cost function, thereby shaping the cost landscape in favour of parameter choices which respect the given symmetry. Unlike methods that alter the neural network circuit ansatz to impose symmetry, our method only changes the classical post-processing of gradient descent, which is simpler to implement. We call the method symmetry-guided gradient descent (SGGD). We illustrate SGGD in entanglement classification of Werner states and in a binary classification task in a 2-D feature space. In both cases, the results show that SGGD can accelerate the training, improve the generalization ability, and remove vanishing gradients, especially when the training data is biased.","sentences":["Many supervised learning tasks have intrinsic symmetries, such as translational and rotational symmetry in image classifications.","These symmetries can be exploited to enhance performance.","We formulate the symmetry constraints into a concise mathematical form.","We design two ways to adopt the constraints into the cost function, thereby shaping the cost landscape in favour of parameter choices which respect the given symmetry.","Unlike methods that alter the neural network circuit ansatz to impose symmetry, our method only changes the classical post-processing of gradient descent, which is simpler to implement.","We call the method symmetry-guided gradient descent (SGGD).","We illustrate SGGD in entanglement classification of Werner states and in a binary classification task in a 2-D feature space.","In both cases, the results show that SGGD can accelerate the training, improve the generalization ability, and remove vanishing gradients, especially when the training data is biased."],"url":"http://arxiv.org/abs/2404.06108v1","category":"quant-ph"}
{"created":"2024-04-09 08:17:32","title":"Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model","abstract":"Modern deep neural networks have achieved high performance across various tasks. Recently, researchers have noted occurrences of low-dimensional structure in the weights, Hessian's, gradients, and feature vectors of these networks, spanning different datasets and architectures when trained to convergence. In this analysis, we theoretically demonstrate these observations arising, and show how they can be unified within a generalized unconstrained feature model that can be considered analytically. Specifically, we consider a previously described structure called Neural Collapse, and its multi-layer counterpart, Deep Neural Collapse, which emerges when the network approaches global optima. This phenomenon explains the other observed low-dimensional behaviours on a layer-wise level, such as the bulk and outlier structure seen in Hessian spectra, and the alignment of gradient descent with the outlier eigenspace of the Hessian. Empirical results in both the deep linear unconstrained feature model and its non-linear equivalent support these predicted observations.","sentences":["Modern deep neural networks have achieved high performance across various tasks.","Recently, researchers have noted occurrences of low-dimensional structure in the weights, Hessian's, gradients, and feature vectors of these networks, spanning different datasets and architectures when trained to convergence.","In this analysis, we theoretically demonstrate these observations arising, and show how they can be unified within a generalized unconstrained feature model that can be considered analytically.","Specifically, we consider a previously described structure called Neural Collapse, and its multi-layer counterpart, Deep Neural Collapse, which emerges when the network approaches global optima.","This phenomenon explains the other observed low-dimensional behaviours on a layer-wise level, such as the bulk and outlier structure seen in Hessian spectra, and the alignment of gradient descent with the outlier eigenspace of the Hessian.","Empirical results in both the deep linear unconstrained feature model and its non-linear equivalent support these predicted observations."],"url":"http://arxiv.org/abs/2404.06106v1","category":"cs.LG"}
{"created":"2024-04-09 08:17:00","title":"Bicolored point sets admitting non-crossing alternating Hamiltonian paths","abstract":"Consider a bicolored point set $P$ in general position in the plane consisting of $n$ blue and $n$ red points. We show that if a subset of the red points forms the vertices of a convex polygon separating the blue points, lying inside the polygon, from the remaining red points, lying outside the polygon, then the points of $P$ can be connected by non-crossing straight-line segments so that the resulting graph is a properly colored closed Hamiltonian path.","sentences":["Consider a bicolored point set $P$ in general position in the plane consisting of $n$ blue and $n$ red points.","We show that if a subset of the red points forms the vertices of a convex polygon separating the blue points, lying inside the polygon, from the remaining red points, lying outside the polygon, then the points of $P$ can be connected by non-crossing straight-line segments so that the resulting graph is a properly colored closed Hamiltonian path."],"url":"http://arxiv.org/abs/2404.06105v1","category":"math.CO"}
{"created":"2024-04-09 08:11:46","title":"A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes","abstract":"Neural networks are playing a crucial role in everyday life, with the most modern generative models able to achieve impressive results. Nonetheless, their functioning is still not very clear, and several strategies have been adopted to study how and why these model reach their outputs. A common approach is to consider the data in an Euclidean settings: recent years has witnessed instead a shift from this paradigm, moving thus to more general framework, namely Riemannian Geometry. Two recent works introduced a geometric framework to study neural networks making use of singular Riemannian metrics. In this paper we extend these results to convolutional, residual and recursive neural networks, studying also the case of non-differentiable activation functions, such as ReLU. We illustrate our findings with some numerical experiments on classification of images and thermodynamic problems.","sentences":["Neural networks are playing a crucial role in everyday life, with the most modern generative models able to achieve impressive results.","Nonetheless, their functioning is still not very clear, and several strategies have been adopted to study how and why these model reach their outputs.","A common approach is to consider the data in an Euclidean settings: recent years has witnessed instead a shift from this paradigm, moving thus to more general framework, namely Riemannian Geometry.","Two recent works introduced a geometric framework to study neural networks making use of singular Riemannian metrics.","In this paper we extend these results to convolutional, residual and recursive neural networks, studying also the case of non-differentiable activation functions, such as ReLU.","We illustrate our findings with some numerical experiments on classification of images and thermodynamic problems."],"url":"http://arxiv.org/abs/2404.06104v1","category":"math.DG"}
{"created":"2024-04-09 08:01:52","title":"Weak lensing combined with the kinetic Sunyaev Zel'dovich effect: A study of baryonic feedback","abstract":"Extracting precise cosmology from weak lensing surveys requires modelling the non-linear matter power spectrum, which is suppressed at small scales due to baryonic feedback processes. However, hydrodynamical galaxy formation simulations make widely varying predictions for the amplitude and extent of this effect. We use measurements of Dark Energy Survey Year 3 weak lensing (WL) and Atacama Cosmology Telescope DR5 kinematic Sunyaev-Zel'dovich (kSZ) to jointly constrain cosmological and astrophysical baryonic feedback parameters using a flexible analytical model, `baryonification'. First, using WL only, we compare the $S_8$ constraints using baryonification to a simulation-calibrated halo model, a simulation-based emulator model and the approach of discarding WL measurements on small angular scales. We find that model flexibility can shift the value of $S_8$ and degrade the uncertainty. The kSZ provides additional constraints on the astrophysical parameters and shifts $S_8$ to $S_8=0.823^{+0.019}_{-0.020}$, a higher value than attained using the WL-only analysis. We measure the suppression of the non-linear matter power spectrum using WL + kSZ and constrain a mean feedback scenario that is more extreme than the predictions from most hydrodynamical simulations. We constrain the baryon fractions and the gas mass fractions and find them to be generally lower than inferred from X-ray observations and simulation predictions. We conclude that the WL + kSZ measurements provide a new and complementary benchmark for building a coherent picture of the impact of gas around galaxies across observations.","sentences":["Extracting precise cosmology from weak lensing surveys requires modelling the non-linear matter power spectrum, which is suppressed at small scales due to baryonic feedback processes.","However, hydrodynamical galaxy formation simulations make widely varying predictions for the amplitude and extent of this effect.","We use measurements of Dark Energy Survey Year 3 weak lensing (WL) and Atacama Cosmology Telescope DR5 kinematic Sunyaev-Zel'dovich (kSZ) to jointly constrain cosmological and astrophysical baryonic feedback parameters using a flexible analytical model, `baryonification'.","First, using WL only, we compare the $S_8$ constraints using baryonification to a simulation-calibrated halo model, a simulation-based emulator model and the approach of discarding WL measurements on small angular scales.","We find that model flexibility can shift the value of $S_8$ and degrade the uncertainty.","The kSZ provides additional constraints on the astrophysical parameters and shifts $S_8$ to $S_8=0.823^{+0.019}_{-0.020}$, a higher value than attained using the WL-only analysis.","We measure the suppression of the non-linear matter power spectrum using WL + kSZ and constrain a mean feedback scenario that is more extreme than the predictions from most hydrodynamical simulations.","We constrain the baryon fractions and the gas mass fractions and find them to be generally lower than inferred from X-ray observations and simulation predictions.","We conclude that the WL + kSZ measurements provide a new and complementary benchmark for building a coherent picture of the impact of gas around galaxies across observations."],"url":"http://arxiv.org/abs/2404.06098v1","category":"astro-ph.CO"}
{"created":"2024-04-09 08:00:53","title":"Higgs Alignment from Multicritical-Point Principle in Two Higgs Doublet Models","abstract":"In models with non-minimal Higgs sectors, enforcing (near) Higgs alignment, necessary to prevent significant deviations in the Higgs boson coupling from the standard model prediction, causes a serious fine-tuning problem. We demonstrate that the Higgs alignment is naturally deduced from the multicritical point principle (MPP) in the general two Higgs doublet model. Furthermore, we discuss the possibility of realizing the Yukawa alignment from the MPP, which is necessary to prevent flavor-changing neutral currents mediated by Higgs bosons at tree level.","sentences":["In models with non-minimal Higgs sectors, enforcing (near) Higgs alignment, necessary to prevent significant deviations in the Higgs boson coupling from the standard model prediction, causes a serious fine-tuning problem.","We demonstrate that the Higgs alignment is naturally deduced from the multicritical point principle (MPP) in the general two Higgs doublet model.","Furthermore, we discuss the possibility of realizing the Yukawa alignment from the MPP, which is necessary to prevent flavor-changing neutral currents mediated by Higgs bosons at tree level."],"url":"http://arxiv.org/abs/2404.06096v1","category":"hep-ph"}
{"created":"2024-04-09 07:57:08","title":"Masked Modeling Duo: Towards a Universal Audio Pre-training Framework","abstract":"Self-supervised learning (SSL) using masked prediction has made great strides in general-purpose audio representation. This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting representations of masked input signals that serve as training signals. Unlike conventional methods, M2D obtains a training signal by encoding only the masked part, encouraging the two networks in M2D to model the input. While M2D improves general-purpose audio representations, a specialized representation is essential for real-world applications, such as in industrial and medical domains. The often confidential and proprietary data in such domains is typically limited in size and has a different distribution from that in pre-training datasets. Therefore, we propose M2D for X (M2D-X), which extends M2D to enable the pre-training of specialized representations for an application X. M2D-X learns from M2D and an additional task and inputs background noise. We make the additional task configurable to serve diverse applications, while the background noise helps learn on small data and forms a denoising task that makes representation robust. With these design choices, M2D-X should learn a representation specialized to serve various application needs. Our experiments confirmed that the representations for general-purpose audio, specialized for the highly competitive AudioSet and speech domain, and a small-data medical task achieve top-level performance, demonstrating the potential of using our models as a universal audio pre-training framework. Our code is available online for future studies at https://github.com/nttcslab/m2d","sentences":["Self-supervised learning (SSL) using masked prediction has made great strides in general-purpose audio representation.","This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting representations of masked input signals that serve as training signals.","Unlike conventional methods, M2D obtains a training signal by encoding only the masked part, encouraging the two networks in M2D to model the input.","While M2D improves general-purpose audio representations, a specialized representation is essential for real-world applications, such as in industrial and medical domains.","The often confidential and proprietary data in such domains is typically limited in size and has a different distribution from that in pre-training datasets.","Therefore, we propose M2D for X (M2D-X), which extends M2D to enable the pre-training of specialized representations for an application X. M2D-X learns from M2D and an additional task and inputs background noise.","We make the additional task configurable to serve diverse applications, while the background noise helps learn on small data and forms a denoising task that makes representation robust.","With these design choices, M2D-X should learn a representation specialized to serve various application needs.","Our experiments confirmed that the representations for general-purpose audio, specialized for the highly competitive AudioSet and speech domain, and a small-data medical task achieve top-level performance, demonstrating the potential of using our models as a universal audio pre-training framework.","Our code is available online for future studies at https://github.com/nttcslab/m2d"],"url":"http://arxiv.org/abs/2404.06095v1","category":"eess.AS"}
{"created":"2024-04-09 07:56:52","title":"S-box Security Analysis of NIST Lightweight Cryptography Candidates: A Critical Empirical Study","abstract":"In the resource-constrained world of the digital landscape, lightweight cryptography plays a critical role in safeguarding information and ensuring the security of various systems, devices, and communication channels. Its efficient and resource-friendly nature makes it the ideal solution for applications where computational power is limited. In response to the growing need for platform-specific implementations, NIST issued a call for standardization of Lightweight cryptography algorithms in 2018. Ascon emerged as the winner of this competition. NIST initially established general evaluation criteria for a standard lightweight scheme including security strength, mitigation against side-channel and fault-injection attacks, and implementation efficiency. To verify the security claims, evaluating the individual components used in any cryptographic algorithm is a crucial step. The quality of a substitution box (S-box) significantly impacts the overall security of a cryptographic primitive. This paper analyzes the S-boxes of six finalists in the NIST Lightweight Cryptography (LWC) standardization process. We evaluate them based on well-established cryptographic properties. Our analysis explores how these properties influence the S-boxes' resistance against known cryptanalytic attacks and potential implementation-specific vulnerabilities, thus reflecting on their compliance with NIST's security requirements.","sentences":["In the resource-constrained world of the digital landscape, lightweight cryptography plays a critical role in safeguarding information and ensuring the security of various systems, devices, and communication channels.","Its efficient and resource-friendly nature makes it the ideal solution for applications where computational power is limited.","In response to the growing need for platform-specific implementations, NIST issued a call for standardization of Lightweight cryptography algorithms in 2018.","Ascon emerged as the winner of this competition.","NIST initially established general evaluation criteria for a standard lightweight scheme including security strength, mitigation against side-channel and fault-injection attacks, and implementation efficiency.","To verify the security claims, evaluating the individual components used in any cryptographic algorithm is a crucial step.","The quality of a substitution box (S-box) significantly impacts the overall security of a cryptographic primitive.","This paper analyzes the S-boxes of six finalists in the NIST Lightweight Cryptography (LWC) standardization process.","We evaluate them based on well-established cryptographic properties.","Our analysis explores how these properties influence the S-boxes' resistance against known cryptanalytic attacks and potential implementation-specific vulnerabilities, thus reflecting on their compliance with NIST's security requirements."],"url":"http://arxiv.org/abs/2404.06094v1","category":"cs.CR"}
{"created":"2024-04-09 07:49:30","title":"Hash3D: Training-free Acceleration for 3D Generation","abstract":"The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.","sentences":["The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models.","Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency.","In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training.","Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity.","By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks.","We achieve this through an adaptive grid-based hashing.","Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects.","Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times.","Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds.","The project page is at https://adamdad.github.io/hash3D/."],"url":"http://arxiv.org/abs/2404.06091v1","category":"cs.CV"}
{"created":"2024-04-09 07:49:05","title":"Fair Graph Neural Network with Supervised Contrastive Regularization","abstract":"In recent years, Graph Neural Networks (GNNs) have made significant advancements, particularly in tasks such as node classification, link prediction, and graph representation. However, challenges arise from biases that can be hidden not only in the node attributes but also in the connections between entities. Therefore, ensuring fairness in graph neural network learning has become a critical problem. To address this issue, we propose a novel model for training fairness-aware GNN, which enhances the Counterfactual Augmented Fair Graph Neural Network Framework (CAF). Our approach integrates Supervised Contrastive Loss and Environmental Loss to enhance both accuracy and fairness. Experimental validation on three real datasets demonstrates the superiority of our proposed model over CAF and several other existing graph-based learning methods.","sentences":["In recent years, Graph Neural Networks (GNNs) have made significant advancements, particularly in tasks such as node classification, link prediction, and graph representation.","However, challenges arise from biases that can be hidden not only in the node attributes but also in the connections between entities.","Therefore, ensuring fairness in graph neural network learning has become a critical problem.","To address this issue, we propose a novel model for training fairness-aware GNN, which enhances the Counterfactual Augmented Fair Graph Neural Network Framework (CAF).","Our approach integrates Supervised Contrastive Loss and Environmental Loss to enhance both accuracy and fairness.","Experimental validation on three real datasets demonstrates the superiority of our proposed model over CAF and several other existing graph-based learning methods."],"url":"http://arxiv.org/abs/2404.06090v1","category":"cs.LG"}
{"created":"2024-04-09 07:45:45","title":"Binary Cyclic Transversal Polytopes","abstract":"With every family of finitely many subsets of a finite-dimensional vector space over the Galois-field with two elements we associate a cyclic transversal polytope. It turns out that those polytopes generalize several well-known polytopes that are relevant in combinatorial optimization, among them cut polytopes as well as stable set and matching polytopes. We introduce the class of lifted odd-set inequalities and prove results demonstrating their strength. In particular, we show that they suffice to describe cyclic transversal polytopes if the union of the sets in the family has rank at most two. We also describe extended formulations for cyclic transversal polytopes and introduce a special relaxation hierarchy for them.","sentences":["With every family of finitely many subsets of a finite-dimensional vector space over the Galois-field with two elements we associate a cyclic transversal polytope.","It turns out that those polytopes generalize several well-known polytopes that are relevant in combinatorial optimization, among them cut polytopes as well as stable set and matching polytopes.","We introduce the class of lifted odd-set inequalities and prove results demonstrating their strength.","In particular, we show that they suffice to describe cyclic transversal polytopes if the union of the sets in the family has rank at most two.","We also describe extended formulations for cyclic transversal polytopes and introduce a special relaxation hierarchy for them."],"url":"http://arxiv.org/abs/2404.06088v1","category":"math.CO"}
{"created":"2024-04-09 07:43:00","title":"Telecom wavelength single-photon source based on InGaSb/AlGaSb quantum dot technology","abstract":"Deterministic light sources capable of generating quantum states on-demand at wavelengths compatible with fiber optics and atmospheric transmission are essential for practical applications in quantum communication, photonic quantum computing, and quantum metrology. To this end, single-photon emission at 1500 nm is demonstrated from an InGaSb quantum dot (QD) grown by filling droplet-etched nanoholes for the first time. The QD was embedded in a device structure comprising an antimony-based high refractive index contrast back-reflector designed for cryogenic operation and a solid immersion lens for improved photon extraction. The longitudinal optical (LO) phonon assisted excitation of the QD ground state and quasi-resonant excitation of the QD excited state is realized with a novel compact wavelength-tunable power-stabilized semiconductor laser. These direct approaches to exciting a single QD unlock access to its excitonic fine structure. The neutral exciton-biexciton structure exhibits a negative binding energy of 1.4 meV (2.6 nm) and a fine structure splitting of 24.1+/-0.4 ueV. Furthermore, spectrally pure/isolated emission from a charged single exciton state with a single-photon purity of 95 % is achieved with LO phonon assisted two-color excitation. These results represent a major step forward for the use of the novel antimonide-based QD emitters as deterministic quantum light sources in complex quantum secure networks exploiting the wavelength compatibility with standard telecom fibers.","sentences":["Deterministic light sources capable of generating quantum states on-demand at wavelengths compatible with fiber optics and atmospheric transmission are essential for practical applications in quantum communication, photonic quantum computing, and quantum metrology.","To this end, single-photon emission at 1500 nm is demonstrated from an InGaSb quantum dot (QD) grown by filling droplet-etched nanoholes for the first time.","The QD was embedded in a device structure comprising an antimony-based high refractive index contrast back-reflector designed for cryogenic operation and a solid immersion lens for improved photon extraction.","The longitudinal optical (LO) phonon assisted excitation of the QD ground state and quasi-resonant excitation of the QD excited state is realized with a novel compact wavelength-tunable power-stabilized semiconductor laser.","These direct approaches to exciting a single QD unlock access to its excitonic fine structure.","The neutral exciton-biexciton structure exhibits a negative binding energy of 1.4 meV (2.6 nm) and a fine structure splitting of 24.1+/-0.4 ueV.","Furthermore, spectrally pure/isolated emission from a charged single exciton state with a single-photon purity of 95 % is achieved with LO phonon assisted two-color excitation.","These results represent a major step forward for the use of the novel antimonide-based QD emitters as deterministic quantum light sources in complex quantum secure networks exploiting the wavelength compatibility with standard telecom fibers."],"url":"http://arxiv.org/abs/2404.06083v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 17:59:04","title":"Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?","abstract":"Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback","sentences":["Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes.","In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures.","We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal.","We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs.","Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box.","However, we find that this issue can be mitigated via a binary verification mechanism.","Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated.","Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism.","The project website is hosted at https://andrewliao11.github.io/vlms_feedback"],"url":"http://arxiv.org/abs/2404.06510v1","category":"cs.CV"}
{"created":"2024-04-09 17:28:07","title":"Mechanised Hypersafety Proofs about Structured Data: Extended Version","abstract":"Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.","sentences":["Arrays are a fundamental abstraction to represent collections of data.","It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency.","Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   ","In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs.","To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data.","The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables.","We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness.","Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse.","We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors."],"url":"http://arxiv.org/abs/2404.06477v1","category":"cs.PL"}
{"created":"2024-04-09 16:17:48","title":"Quantum stochastic thermodynamics in the mesoscopic-leads formulation","abstract":"We introduce a numerical method to sample the distributions of charge, heat, and entropy production in open quantum systems coupled strongly to macroscopic reservoirs, with both temporal and energy resolution and beyond the linear-response regime. Our method exploits the mesoscopic-leads formulation, where macroscopic reservoirs are modeled by a finite collection of modes that are continuously damped toward thermal equilibrium by an appropriate Gorini-Kossakowski-Sudarshan-Lindblad master equation. Focussing on non-interacting fermionic systems, we access the time-resolved full counting statistics through a trajectory unraveling of the master equation. We show that the integral fluctuation theorems for the total entropy production, as well as the martingale and uncertainty entropy production, hold. Furthermore, we investigate the fluctuations of the dissipated heat in finite-time information erasure. Conceptually, our approach extends the continuous-time trajectory description of quantum stochastic thermodynamics beyond the regime of weak system-environment coupling.","sentences":["We introduce a numerical method to sample the distributions of charge, heat, and entropy production in open quantum systems coupled strongly to macroscopic reservoirs, with both temporal and energy resolution and beyond the linear-response regime.","Our method exploits the mesoscopic-leads formulation, where macroscopic reservoirs are modeled by a finite collection of modes that are continuously damped toward thermal equilibrium by an appropriate Gorini-Kossakowski-Sudarshan-Lindblad master equation.","Focussing on non-interacting fermionic systems, we access the time-resolved full counting statistics through a trajectory unraveling of the master equation.","We show that the integral fluctuation theorems for the total entropy production, as well as the martingale and uncertainty entropy production, hold.","Furthermore, we investigate the fluctuations of the dissipated heat in finite-time information erasure.","Conceptually, our approach extends the continuous-time trajectory description of quantum stochastic thermodynamics beyond the regime of weak system-environment coupling."],"url":"http://arxiv.org/abs/2404.06426v1","category":"quant-ph"}
{"created":"2024-04-09 13:52:54","title":"Towards the wall-crossing of locally $\\mathbb{Q}_p$-analytic representations of $\\mathrm{GL}_n(K)$ for a $p$-adic field $K$","abstract":"Let $K$ be a finite extension of $\\mathbb{Q}_p$. We study the locally $\\mathbb{Q}_p$-analytic representations $\\pi$ of $\\mathrm{GL}_n(K)$ of integral weights that appear in spaces of $p$-adic automorphic representations. We conjecture that the translation of $\\pi$ to the singular block has an internal structure which is compatible with certain algebraic representations of $\\mathrm{GL}_n$, analogously to the mod $p$ local-global compatibility conjecture of Breuil-Herzig-Hu-Morra-Schraen. We next make some conjectures and speculations on the wall-crossings of $\\pi$. In particular, when $\\pi$ is associated to a two dimensional de Rham Galois representation, we make conjectures and speculations on the relation between the Hodge filtrations of $\\rho$ and the wall-crossings of $\\pi$, which have a flavour of the Breuil-Strauch conjecture. We collect some results towards the conjectures and speculations.","sentences":["Let $K$ be a finite extension of $\\mathbb{Q}_p$. We study the locally $\\mathbb{Q}_p$-analytic representations $\\pi$ of $\\mathrm{GL}_n(K)$ of integral weights that appear in spaces of $p$-adic automorphic representations.","We conjecture that the translation of $\\pi$ to the singular block has an internal structure which is compatible with certain algebraic representations of $\\mathrm{GL}_n$, analogously to the mod $p$ local-global compatibility conjecture of Breuil-Herzig-Hu-Morra-Schraen.","We next make some conjectures and speculations on the wall-crossings of $\\pi$. In particular, when $\\pi$ is associated to a two dimensional de Rham Galois representation, we make conjectures and speculations on the relation between the Hodge filtrations of $\\rho$ and the wall-crossings of $\\pi$, which have a flavour of the Breuil-Strauch conjecture.","We collect some results towards the conjectures and speculations."],"url":"http://arxiv.org/abs/2404.06315v1","category":"math.NT"}
{"created":"2024-04-09 13:15:23","title":"Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles","abstract":"Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation. The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard. In this context, the acquisition of real driving data is considered essential for the verification and validation. For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future. A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable. The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters. This paper provides a summary of the research project and outlines its central idea. To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed.","sentences":["Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation.","The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard.","In this context, the acquisition of real driving data is considered essential for the verification and validation.","For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future.","A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable.","The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters.","This paper provides a summary of the research project and outlines its central idea.","To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed."],"url":"http://arxiv.org/abs/2404.06288v1","category":"cs.RO"}
{"created":"2024-04-09 12:47:30","title":"3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis","abstract":"In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at https://npucvr.github.io/GaGS/","sentences":["In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis.","Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry.","Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction.","Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation.","Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation.","To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation.","In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction.","Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   ","The project is available at https://npucvr.github.io/GaGS/"],"url":"http://arxiv.org/abs/2404.06270v1","category":"cs.CV"}
{"created":"2024-04-09 12:45:32","title":"Kostka polynomials of $G(\\ell,1,m)$","abstract":"For each integer $n, m, \\ell \\ge 1$ such that $n > m$, we exhibit an equivalence between the category of polynomial modules over a paraholic subalgebra $\\mathfrak p$ of an affine Lie algebra of $\\mathfrak{gl}(n\\ell)$ and the module category of the smash product algebra $A$ of the complex reflection group $G(\\ell,1,m)$ with $\\mathbb C [X_1,\\ldots,X_m]$. Then, we transfer the collection of $\\mathfrak p$-modules considered in [Feigin-Makedonskyi-Khoroshkhin, arXiv:2311.12673] to $A$. Applying the Lusztig-Shoji algorithm [Shoji, Invent. Math. 74 (1983)] (or rather its homological variant [K. Ann. Sci. ENS 48(5) (2015)]), we conclude that the multiplicity counts of these modules yield the Kostka polynomials attached to limit symbols in the sense of [Shoji, ASPM 40 (2004)]. This particularly settles a conjecture of Shoji [loc. cit. \\S 3.13] and answers a question in [Shoji, Sci. China Math. 61 (2018)].","sentences":["For each integer $n, m, \\ell \\ge 1$ such that $n > m$, we exhibit an equivalence between the category of polynomial modules over a paraholic subalgebra $\\mathfrak p$ of an affine Lie algebra of $\\mathfrak{gl}(n\\ell)$ and the module category of the smash product algebra $A$ of the complex reflection group $G(\\ell,1,m)$ with $\\mathbb C","[X_1,\\ldots,X_m]$.","Then, we transfer the collection of $\\mathfrak p$-modules considered in [Feigin-Makedonskyi-Khoroshkhin, arXiv:2311.12673] to $A$.","Applying the Lusztig-Shoji algorithm","[Shoji, Invent.","Math. 74 (1983)] (or rather its homological variant [K. Ann.","Sci.","ENS 48(5) (2015)]), we conclude that the multiplicity counts of these modules yield the Kostka polynomials attached to limit symbols in the sense of [Shoji, ASPM 40 (2004)].","This particularly settles a conjecture of Shoji [loc. cit.","\\S 3.13] and answers a question in [Shoji, Sci.","China Math. 61 (2018)]."],"url":"http://arxiv.org/abs/2404.06268v1","category":"math.RT"}
{"created":"2024-04-09 11:00:11","title":"Unified Physical-Digital Attack Detection Challenge","abstract":"Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR) Systems. In real-world scenarios, FRs are confronted with both physical and digital attacks. However, existing algorithms often address only one type of attack at a time, which poses significant limitations in real-world scenarios where FR systems face hybrid physical-digital threats. To facilitate the research of Unified Attack Detection (UAD) algorithms, a large-scale UniAttackData dataset has been collected. UniAttackData is the largest public dataset for Unified Attack Detection, with a total of 28,706 videos, where each unique identity encompasses all advanced attack types. Based on this dataset, we organized a Unified Physical-Digital Face Attack Detection Challenge to boost the research in Unified Attack Detections. It attracted 136 teams for the development phase, with 13 qualifying for the final round. The results re-verified by the organizing team were used for the final ranking. This paper comprehensively reviews the challenge, detailing the dataset introduction, protocol definition, evaluation criteria, and a summary of published results. Finally, we focus on the detailed analysis of the highest-performing algorithms and offer potential directions for unified physical-digital attack detection inspired by this competition. Challenge Website: https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.","sentences":["Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR) Systems.","In real-world scenarios, FRs are confronted with both physical and digital attacks.","However, existing algorithms often address only one type of attack at a time, which poses significant limitations in real-world scenarios where FR systems face hybrid physical-digital threats.","To facilitate the research of Unified Attack Detection (UAD) algorithms, a large-scale UniAttackData dataset has been collected.","UniAttackData is the largest public dataset for Unified Attack Detection, with a total of 28,706 videos, where each unique identity encompasses all advanced attack types.","Based on this dataset, we organized a Unified Physical-Digital Face Attack Detection Challenge to boost the research in Unified Attack Detections.","It attracted 136 teams for the development phase, with 13 qualifying for the final round.","The results re-verified by the organizing team were used for the final ranking.","This paper comprehensively reviews the challenge, detailing the dataset introduction, protocol definition, evaluation criteria, and a summary of published results.","Finally, we focus on the detailed analysis of the highest-performing algorithms and offer potential directions for unified physical-digital attack detection inspired by this competition.","Challenge Website: https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024."],"url":"http://arxiv.org/abs/2404.06211v1","category":"cs.CV"}
{"created":"2024-04-09 09:55:27","title":"Fundamental interactions in self-organized critical dynamics on higher-order networks","abstract":"In functionally complex systems, higher-order connectivity is often revealed in the underlying geometry of networked units. Furthermore, such systems often show signatures of self-organized criticality, a specific type of non-equilibrium collective behaviour associated with an attractor of internal dynamics with long-range correlations and scale invariance, which ensures the robust functioning of complex systems, such as the brain. Here, we highlight the intertwining of features of higher-order geometry and self-organized critical dynamics as a plausible mechanism for the emergence of new properties on a larger scale, representing the central paradigm of the physical notion of complexity. Considering the time scale of the structural evolution with the known separation of the time scale in self-organized criticality, i.e., internal dynamics and external driving, we distinguish three classes of geometries that can shape the self-organized dynamics on them differently. We provide an overview of current trends in the study of collective dynamics phenomena, such as the synchronization of phase oscillators and discrete spin dynamics with higher-order couplings embedded in the faces of simplicial complexes. For a representative example of self-organized critical behaviour induced by higher-order structures, we present a more detailed analysis of the dynamics of field-driven spin reversal on the hysteresis loops in simplicial complexes composed of triangles. These numerical results suggest that two fundamental interactions representing the edge-embedded and triangle-embedded couplings must be taken into account in theoretical models to describe the influence of higher-order geometry on critical dynamics.","sentences":["In functionally complex systems, higher-order connectivity is often revealed in the underlying geometry of networked units.","Furthermore, such systems often show signatures of self-organized criticality, a specific type of non-equilibrium collective behaviour associated with an attractor of internal dynamics with long-range correlations and scale invariance, which ensures the robust functioning of complex systems, such as the brain.","Here, we highlight the intertwining of features of higher-order geometry and self-organized critical dynamics as a plausible mechanism for the emergence of new properties on a larger scale, representing the central paradigm of the physical notion of complexity.","Considering the time scale of the structural evolution with the known separation of the time scale in self-organized criticality, i.e., internal dynamics and external driving, we distinguish three classes of geometries that can shape the self-organized dynamics on them differently.","We provide an overview of current trends in the study of collective dynamics phenomena, such as the synchronization of phase oscillators and discrete spin dynamics with higher-order couplings embedded in the faces of simplicial complexes.","For a representative example of self-organized critical behaviour induced by higher-order structures, we present a more detailed analysis of the dynamics of field-driven spin reversal on the hysteresis loops in simplicial complexes composed of triangles.","These numerical results suggest that two fundamental interactions representing the edge-embedded and triangle-embedded couplings must be taken into account in theoretical models to describe the influence of higher-order geometry on critical dynamics."],"url":"http://arxiv.org/abs/2404.06175v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 08:27:50","title":"Thermal axion production at hard and soft momenta","abstract":"Hot axions, thermally produced in the Early Universe, would contribute to dark radiation and are thus subject to present and future constraints from $N_{\\rm eff}$. In this paper we quantify the contribution to $N_{\\rm eff}$ and its uncertainty in models with axion-gluon couplings from thermal dynamics above the QCD transition. In more detail, we determine the leading-order thermal axion production rate for axion momenta of the order of the temperature adopting three different schemes for the incorporation of the collective dynamics of soft gluons. We show how these three schemes extrapolate differently into the regime of softer axion production, thus giving us a quantitative handle on the theory uncertainty of the rate. Upon solving the Boltzmann equation, we find that this theory uncertainty translates to an uncertainty of at most 0.002 for $N_{\\rm eff}$. The uncertainty from common momentum-averaged approximations to the Boltzmann equation is smaller. We also comment on existing rate determinations in the literature and discuss how QCD transition dynamics would need to be integrated into our results.","sentences":["Hot axions, thermally produced in the Early Universe, would contribute to dark radiation and are thus subject to present and future constraints from $N_{\\rm eff}$. In this paper we quantify the contribution to $N_{\\rm eff}$ and its uncertainty in models with axion-gluon couplings from thermal dynamics above the QCD transition.","In more detail, we determine the leading-order thermal axion production rate for axion momenta of the order of the temperature adopting three different schemes for the incorporation of the collective dynamics of soft gluons.","We show how these three schemes extrapolate differently into the regime of softer axion production, thus giving us a quantitative handle on the theory uncertainty of the rate.","Upon solving the Boltzmann equation, we find that this theory uncertainty translates to an uncertainty of at most 0.002 for $N_{\\rm eff}$. The uncertainty from common momentum-averaged approximations to the Boltzmann equation is smaller.","We also comment on existing rate determinations in the literature and discuss how QCD transition dynamics would need to be integrated into our results."],"url":"http://arxiv.org/abs/2404.06113v1","category":"hep-ph"}
{"created":"2024-04-09 08:08:03","title":"Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines","abstract":"Kurdish libraries have many historical publications that were printed back in the early days when printing devices were brought to Kurdistan. Having a good Optical Character Recognition (OCR) to help process these publications and contribute to the Kurdish languages resources which is crucial as Kurdish is considered a low-resource language. Current OCR systems are unable to extract text from historical documents as they have many issues, including being damaged, very fragile, having many marks left on them, and often written in non-standard fonts and more. This is a massive obstacle in processing these documents as currently processing them requires manual typing which is very time-consuming. In this study, we adopt an open-source OCR framework by Google, Tesseract version 5.0, that has been used to extract text for various languages. Currently, there is no public dataset, and we developed our own by collecting historical documents from Zheen Center for Documentation and Research, which were printed before 1950 and resulted in a dataset of 1233 images of lines with transcription of each. Then we used the Arabic model as our base model and trained the model using the dataset. We used different methods to evaluate our model, Tesseracts built-in evaluator lstmeval indicated a Character Error Rate (CER) of 0.755%. Additionally, Ocreval demonstrated an average character accuracy of 84.02%. Finally, we developed a web application to provide an easy- to-use interface for end-users, allowing them to interact with the model by inputting an image of a page and extracting the text. Having an extensive dataset is crucial to develop OCR systems with reasonable accuracy, as currently, no public datasets are available for historical Kurdish documents; this posed a significant challenge in our work. Additionally, the unaligned spaces between characters and words proved another challenge with our work.","sentences":["Kurdish libraries have many historical publications that were printed back in the early days when printing devices were brought to Kurdistan.","Having a good Optical Character Recognition (OCR) to help process these publications and contribute to the Kurdish languages resources which is crucial as Kurdish is considered a low-resource language.","Current OCR systems are unable to extract text from historical documents as they have many issues, including being damaged, very fragile, having many marks left on them, and often written in non-standard fonts and more.","This is a massive obstacle in processing these documents as currently processing them requires manual typing which is very time-consuming.","In this study, we adopt an open-source OCR framework by Google, Tesseract version 5.0, that has been used to extract text for various languages.","Currently, there is no public dataset, and we developed our own by collecting historical documents from Zheen Center for Documentation and Research, which were printed before 1950 and resulted in a dataset of 1233 images of lines with transcription of each.","Then we used the Arabic model as our base model and trained the model using the dataset.","We used different methods to evaluate our model, Tesseracts built-in evaluator lstmeval indicated a Character Error Rate (CER) of 0.755%.","Additionally, Ocreval demonstrated an average character accuracy of 84.02%.","Finally, we developed a web application to provide an easy- to-use interface for end-users, allowing them to interact with the model by inputting an image of a page and extracting the text.","Having an extensive dataset is crucial to develop OCR systems with reasonable accuracy, as currently, no public datasets are available for historical Kurdish documents; this posed a significant challenge in our work.","Additionally, the unaligned spaces between characters and words proved another challenge with our work."],"url":"http://arxiv.org/abs/2404.06101v1","category":"cs.CL"}
{"created":"2024-04-09 07:48:49","title":"EVE: Enabling Anyone to Train Robot using Augmented Reality","abstract":"The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.","sentences":["The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities.","However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators.","Consequently, only those with access to physical robots produce demonstrations to train robots.","To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot.","With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories.","In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$).","We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics."],"url":"http://arxiv.org/abs/2404.06089v1","category":"cs.HC"}
{"created":"2024-04-09 07:37:41","title":"The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge","abstract":"Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, along with the lowest bitrate among all submissions.","sentences":["Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS).","In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge.","Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, along with the lowest bitrate among all submissions."],"url":"http://arxiv.org/abs/2404.06079v1","category":"eess.AS"}
{"created":"2024-04-09 07:32:30","title":"Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage","abstract":"As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount. AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners. However, existing studies primarily center on safeguarding static copyrights, which simply treats metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory.   In this paper, we present \\textsc{IBis}, a blockchain-based framework tailored for AI model training workflows. \\textsc{IBis} integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants. Our framework addresses concerns regarding data and model provenance and copyright compliance. \\textsc{IBis} enables iterative model retraining and fine-tuning, and offers flexible license checks and renewals. Further, \\textsc{IBis} provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes. We implement \\textsc{IBis} using Daml on the Canton blockchain. Evaluation results showcase the feasibility and scalability of \\textsc{IBis} across varying numbers of users, datasets, models, and licenses.","sentences":["As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount.","AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners.","However, existing studies primarily center on safeguarding static copyrights, which simply treats metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory.   ","In this paper, we present \\textsc{IBis}, a blockchain-based framework tailored for AI model training workflows.","\\textsc{IBis} integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants.","Our framework addresses concerns regarding data and model provenance and copyright compliance.","\\textsc{IBis} enables iterative model retraining and fine-tuning, and offers flexible license checks and renewals.","Further, \\textsc{IBis} provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes.","We implement \\textsc{IBis} using Daml on the Canton blockchain.","Evaluation results showcase the feasibility and scalability of \\textsc{IBis} across varying numbers of users, datasets, models, and licenses."],"url":"http://arxiv.org/abs/2404.06077v1","category":"cs.CR"}
{"created":"2024-04-09 07:02:14","title":"All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis","abstract":"Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing. Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain. With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis. In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application. We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks. In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates. In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions. The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data.","sentences":["Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing.","Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain.","With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis.","In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application.","We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks.","In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates.","In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions.","The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data."],"url":"http://arxiv.org/abs/2404.06063v1","category":"cs.CL"}
{"created":"2024-04-09 06:53:12","title":"Efficient Quantum Circuits for Machine Learning Activation Functions including Constant T-depth ReLU","abstract":"In recent years, Quantum Machine Learning (QML) has increasingly captured the interest of researchers. Among the components in this domain, activation functions hold a fundamental and indispensable role. Our research focuses on the development of activation functions quantum circuits for integration into fault-tolerant quantum computing architectures, with an emphasis on minimizing $T$-depth. Specifically, we present novel implementations of ReLU and leaky ReLU activation functions, achieving constant $T$-depths of 4 and 8, respectively. Leveraging quantum lookup tables, we extend our exploration to other activation functions such as the sigmoid. This approach enables us to customize precision and $T$-depth by adjusting the number of qubits, making our results more adaptable to various application scenarios. This study represents a significant advancement towards enhancing the practicality and application of quantum machine learning.","sentences":["In recent years, Quantum Machine Learning (QML) has increasingly captured the interest of researchers.","Among the components in this domain, activation functions hold a fundamental and indispensable role.","Our research focuses on the development of activation functions quantum circuits for integration into fault-tolerant quantum computing architectures, with an emphasis on minimizing $T$-depth.","Specifically, we present novel implementations of ReLU and leaky ReLU activation functions, achieving constant $T$-depths of 4 and 8, respectively.","Leveraging quantum lookup tables, we extend our exploration to other activation functions such as the sigmoid.","This approach enables us to customize precision and $T$-depth by adjusting the number of qubits, making our results more adaptable to various application scenarios.","This study represents a significant advancement towards enhancing the practicality and application of quantum machine learning."],"url":"http://arxiv.org/abs/2404.06059v1","category":"quant-ph"}
{"created":"2024-04-09 05:21:32","title":"Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs","abstract":"Morphing attacks are an emerging threat to state-of-the-art Face Recognition (FR) systems, which aim to create a single image that contains the biometric information of multiple identities. Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks. However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a black box, treating it no differently than one would a Generative Adversarial Network (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function. We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset. We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared.","sentences":["Morphing attacks are an emerging threat to state-of-the-art Face Recognition (FR) systems, which aim to create a single image that contains the biometric information of multiple identities.","Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks.","However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a black box, treating it no differently than one would a Generative Adversarial Network (GAN) or Varational AutoEncoder (VAE).","We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function.","We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset.","We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared."],"url":"http://arxiv.org/abs/2404.06025v1","category":"cs.CV"}
{"created":"2024-04-09 05:11:28","title":"Band-Attention Modulated RetNet for Face Forgery Detection","abstract":"The transformer networks are extensively utilized in face forgery detection due to their scalability across large datasets.Despite their success, transformers face challenges in balancing the capture of global context, which is crucial for unveiling forgery clues, with computational complexity.To mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a lightweight network designed to efficiently process extensive visual contexts while avoiding catastrophic forgetting.Our approach empowers the target token to perceive global information by assigning differential attention levels to tokens at varying distances. We implement self-attention along both spatial axes, thereby maintaining spatial priors and easing the computational burden.Moreover, we present the adaptive frequency Band-Attention Modulation mechanism, which treats the entire Discrete Cosine Transform spectrogram as a series of frequency bands with learnable weights.Together, BAR-Net achieves favorable performance on several face forgery datasets, outperforming current state-of-the-art methods.","sentences":["The transformer networks are extensively utilized in face forgery detection due to their scalability across large datasets.","Despite their success, transformers face challenges in balancing the capture of global context, which is crucial for unveiling forgery clues, with computational complexity.","To mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a lightweight network designed to efficiently process extensive visual contexts while avoiding catastrophic forgetting.","Our approach empowers the target token to perceive global information by assigning differential attention levels to tokens at varying distances.","We implement self-attention along both spatial axes, thereby maintaining spatial priors and easing the computational burden.","Moreover, we present the adaptive frequency Band-Attention Modulation mechanism, which treats the entire Discrete Cosine Transform spectrogram as a series of frequency bands with learnable weights.","Together, BAR-Net achieves favorable performance on several face forgery datasets, outperforming current state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.06022v1","category":"cs.CV"}
{"created":"2024-04-09 04:47:01","title":"Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem","abstract":"Real-world optimization problems often involve stochastic and dynamic components. Evolutionary algorithms are particularly effective in these scenarios, as they can easily adapt to uncertain and changing environments but often uncertainty and dynamic changes are studied in isolation. In this paper, we explore the use of 3-objective evolutionary algorithms for the chance constrained knapsack problem with dynamic constraints. In our setting, the weights of the items are stochastic and the knapsack's capacity changes over time. We introduce a 3-objective formulation that is able to deal with the stochastic and dynamic components at the same time and is independent of the confidence level required for the constraint. This new approach is then compared to the 2-objective formulation which is limited to a single confidence level. We evaluate the approach using two different multi-objective evolutionary algorithms (MOEAs), namely the global simple evolutionary multi-objective optimizer (GSEMO) and the multi-objective evolutionary algorithm based on decomposition (MOEA/D), across various benchmark scenarios. Our analysis highlights the advantages of the 3-objective formulation over the 2-objective formulation in addressing the dynamic chance constrained knapsack problem.","sentences":["Real-world optimization problems often involve stochastic and dynamic components.","Evolutionary algorithms are particularly effective in these scenarios, as they can easily adapt to uncertain and changing environments but often uncertainty and dynamic changes are studied in isolation.","In this paper, we explore the use of 3-objective evolutionary algorithms for the chance constrained knapsack problem with dynamic constraints.","In our setting, the weights of the items are stochastic and the knapsack's capacity changes over time.","We introduce a 3-objective formulation that is able to deal with the stochastic and dynamic components at the same time and is independent of the confidence level required for the constraint.","This new approach is then compared to the 2-objective formulation which is limited to a single confidence level.","We evaluate the approach using two different multi-objective evolutionary algorithms (MOEAs), namely the global simple evolutionary multi-objective optimizer (GSEMO) and the multi-objective evolutionary algorithm based on decomposition (MOEA/D), across various benchmark scenarios.","Our analysis highlights the advantages of the 3-objective formulation over the 2-objective formulation in addressing the dynamic chance constrained knapsack problem."],"url":"http://arxiv.org/abs/2404.06014v1","category":"cs.NE"}
{"created":"2024-04-09 04:26:16","title":"Collaborative Edge AI Inference over Cloud-RAN","abstract":"In this paper, a cloud radio access network (Cloud-RAN) based collaborative edge AI inference architecture is proposed. Specifically, geographically distributed devices capture real-time noise-corrupted sensory data samples and extract the noisy local feature vectors, which are then aggregated at each remote radio head (RRH) to suppress sensing noise. To realize efficient uplink feature aggregation, we allow each RRH receives local feature vectors from all devices over the same resource blocks simultaneously by leveraging an over-the-air computation (AirComp) technique. Thereafter, these aggregated feature vectors are quantized and transmitted to a central processor (CP) for further aggregation and downstream inference tasks. Our aim in this work is to maximize the inference accuracy via a surrogate accuracy metric called discriminant gain, which measures the discernibility of different classes in the feature space. The key challenges lie on simultaneously suppressing the coupled sensing noise, AirComp distortion caused by hostile wireless channels, and the quantization error resulting from the limited capacity of fronthaul links. To address these challenges, this work proposes a joint transmit precoding, receive beamforming, and quantization error control scheme to enhance the inference accuracy. Extensive numerical experiments demonstrate the effectiveness and superiority of our proposed optimization algorithm compared to various baselines.","sentences":["In this paper, a cloud radio access network (Cloud-RAN) based collaborative edge AI inference architecture is proposed.","Specifically, geographically distributed devices capture real-time noise-corrupted sensory data samples and extract the noisy local feature vectors, which are then aggregated at each remote radio head (RRH) to suppress sensing noise.","To realize efficient uplink feature aggregation, we allow each RRH receives local feature vectors from all devices over the same resource blocks simultaneously by leveraging an over-the-air computation (AirComp) technique.","Thereafter, these aggregated feature vectors are quantized and transmitted to a central processor (CP) for further aggregation and downstream inference tasks.","Our aim in this work is to maximize the inference accuracy via a surrogate accuracy metric called discriminant gain, which measures the discernibility of different classes in the feature space.","The key challenges lie on simultaneously suppressing the coupled sensing noise, AirComp distortion caused by hostile wireless channels, and the quantization error resulting from the limited capacity of fronthaul links.","To address these challenges, this work proposes a joint transmit precoding, receive beamforming, and quantization error control scheme to enhance the inference accuracy.","Extensive numerical experiments demonstrate the effectiveness and superiority of our proposed optimization algorithm compared to various baselines."],"url":"http://arxiv.org/abs/2404.06007v1","category":"cs.IT"}
{"created":"2024-04-09 04:17:51","title":"FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models","abstract":"The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency. Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches. Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference. In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs. Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions. Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes. Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs.","sentences":["The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency.","Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches.","Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference.","In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs.","Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions.","Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes.","Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs."],"url":"http://arxiv.org/abs/2404.06003v1","category":"cs.CL"}
{"created":"2024-04-09 04:04:50","title":"Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis","abstract":"The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. However, existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. A medical image usually contains multiple concepts and the fine-grained concept annotations are difficult to acquire. In this paper, we propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train the CNN with a CAW layer inserted to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, we calculate the orthogonal matrix under the guidance of the concept attention mask. We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.","sentences":["The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications.","To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process.","In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results.","However, existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes.","A medical image usually contains multiple concepts and the fine-grained concept annotations are difficult to acquire.","In this paper, we propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis.","CAW is comprised of a disease diagnosis branch and a concept alignment branch.","In the former branch, we train the CNN with a CAW layer inserted to perform skin lesion diagnosis.","The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix.","In the latter branch, we calculate the orthogonal matrix under the guidance of the concept attention mask.","We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix.","Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance."],"url":"http://arxiv.org/abs/2404.05997v1","category":"cs.CV"}
{"created":"2024-04-09 03:48:42","title":"Automatic Authorities: Power and AI","abstract":"As rapid advances in Artificial Intelligence and the rise of some of history's most potent corporations meet the diminished neoliberal state, people are increasingly subject to power exercised by means of automated systems. Machine learning and related computational technologies now underpin vital government services. They connect consumers and producers in new algorithmic markets. They determine how we find out about everything from how to vote to where to get vaccinated, and whose speech is amplified, reduced, or restricted. And a new wave of products based on Large Language Models (LLMs) will further transform our economic and political lives. Automatic Authorities are automated computational systems used to exercise power over us by determining what we may know, what we may have, and what our options will be. In response to their rise, scholars working on the societal impacts of AI and related technologies have advocated shifting attention from how to make AI systems beneficial or fair towards a critical analysis of these new power relations. But power is everywhere, and is not necessarily bad. On what basis should we object to new or intensified power relations, and what can be done to justify them? This paper introduces the philosophical materials with which to formulate these questions, and offers preliminary answers. It starts by pinning down the concept of power, focusing on the ability that some agents have to shape others' lives. It then explores how AI enables and intensifies the exercise of power so understood, and sketches three problems with power and three ways to solve those problems. It emphasises, in particular, that justifying power requires more than satisfying substantive justificatory criteria; standards of proper authority and procedural legitimacy must also be met. We need to know not only what power may be used for, but how it may be used, and by whom.","sentences":["As rapid advances in Artificial Intelligence and the rise of some of history's most potent corporations meet the diminished neoliberal state, people are increasingly subject to power exercised by means of automated systems.","Machine learning and related computational technologies now underpin vital government services.","They connect consumers and producers in new algorithmic markets.","They determine how we find out about everything from how to vote to where to get vaccinated, and whose speech is amplified, reduced, or restricted.","And a new wave of products based on Large Language Models (LLMs) will further transform our economic and political lives.","Automatic Authorities are automated computational systems used to exercise power over us by determining what we may know, what we may have, and what our options will be.","In response to their rise, scholars working on the societal impacts of AI and related technologies have advocated shifting attention from how to make AI systems beneficial or fair towards a critical analysis of these new power relations.","But power is everywhere, and is not necessarily bad.","On what basis should we object to new or intensified power relations, and what can be done to justify them?","This paper introduces the philosophical materials with which to formulate these questions, and offers preliminary answers.","It starts by pinning down the concept of power, focusing on the ability that some agents have to shape others' lives.","It then explores how AI enables and intensifies the exercise of power so understood, and sketches three problems with power and three ways to solve those problems.","It emphasises, in particular, that justifying power requires more than satisfying substantive justificatory criteria; standards of proper authority and procedural legitimacy must also be met.","We need to know not only what power may be used for, but how it may be used, and by whom."],"url":"http://arxiv.org/abs/2404.05990v1","category":"cs.CY"}
{"created":"2024-04-09 03:45:15","title":"Comparison of Three Programming Error Measures for Explaining Variability in CS1 Grades","abstract":"Programming courses can be challenging for first year university students, especially for those without prior coding experience. Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies). This study examined the relationships between students' rate of programming errors and their grades on two exams. Using an online integrated development environment, data were collected from 280 students in a Java programming course. The course had two parts. The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2. To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests. Compiler and runtime errors were extracted from the snapshots, and three measures -- Error Count, Error Quotient and Repeated Error Density -- were explored to identify the best measure explaining variability in exam grades. Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion. Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades. The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability.","sentences":["Programming courses can be challenging for first year university students, especially for those without prior coding experience.","Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies).","This study examined the relationships between students' rate of programming errors and their grades on two exams.","Using an online integrated development environment, data were collected from 280 students in a Java programming course.","The course had two parts.","The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2.","To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests.","Compiler and runtime errors were extracted from the snapshots, and three measures -- Error Count, Error Quotient and Repeated Error Density -- were explored to identify the best measure explaining variability in exam grades.","Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion.","Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades.","The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability."],"url":"http://arxiv.org/abs/2404.05988v1","category":"cs.PL"}
{"created":"2024-04-09 03:36:39","title":"Boosting Digital Safeguards: Blending Cryptography and Steganography","abstract":"In today's digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data security measures to prevent unauthorized access and exploitation. Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission. Steganography, on the other hand, originates from the Greek term for \"covered writing\" and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible. This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of Generative Adversarial Networks (GANs), to improve upon traditional steganographic methods. By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes. The application of GANs enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection. By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive security system designed to maintain both the privacy and integrity of information. This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden. This fusion of technologies tackles the core challenges of data security in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information security.","sentences":["In today's digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data security measures to prevent unauthorized access and exploitation.","Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission.","Steganography, on the other hand, originates from the Greek term for \"covered writing\" and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible.","This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of Generative Adversarial Networks (GANs), to improve upon traditional steganographic methods.","By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes.","The application of GANs enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection.","By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive security system designed to maintain both the privacy and integrity of information.","This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden.","This fusion of technologies tackles the core challenges of data security in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information security."],"url":"http://arxiv.org/abs/2404.05985v1","category":"cs.CR"}
{"created":"2024-04-09 03:24:10","title":"Tackling Structural Hallucination in Image Translation with Local Diffusion","abstract":"Recent developments in diffusion models have advanced conditioned image generation, yet they struggle with reconstructing out-of-distribution (OOD) images, such as unseen tumors in medical images, causing ``image hallucination'' and risking misdiagnosis. We hypothesize such hallucinations result from local OOD regions in the conditional images. We verify that partitioning the OOD region and conducting separate image generations alleviates hallucinations in several applications. From this, we propose a training-free diffusion framework that reduces hallucination with multiple Local Diffusion processes. Our approach involves OOD estimation followed by two modules: a ``branching'' module generates locally both within and outside OOD regions, and a ``fusion'' module integrates these predictions into one. Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural image datasets, respectively. It also demonstrates compatibility with various pre-trained diffusion models.","sentences":["Recent developments in diffusion models have advanced conditioned image generation, yet they struggle with reconstructing out-of-distribution (OOD) images, such as unseen tumors in medical images, causing ``image hallucination'' and risking misdiagnosis.","We hypothesize such hallucinations result from local OOD regions in the conditional images.","We verify that partitioning the OOD region and conducting separate image generations alleviates hallucinations in several applications.","From this, we propose a training-free diffusion framework that reduces hallucination with multiple Local Diffusion processes.","Our approach involves OOD estimation followed by two modules: a ``branching'' module generates locally both within and outside OOD regions, and a ``fusion'' module integrates these predictions into one.","Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural image datasets, respectively.","It also demonstrates compatibility with various pre-trained diffusion models."],"url":"http://arxiv.org/abs/2404.05980v1","category":"cs.CV"}
{"created":"2024-04-09 03:18:06","title":"Passive Non-line-of-sight imaging of moving targets using Physical embedding and Event-based vision","abstract":"Passive Non-line-of-sight (NLOS) imaging has shown promising applications in imaging occluded objects around corners. However, this inverse problem is highly ill-posed and results in poor reconstruction with traditional physical retrieval methods, particularly in moving target imaging. With the development of neural networks, data-driven methods have greatly improved accuracy, however, heavy reliance on data volume has put great pressure on data collection and dataset fabrication. We propose a physical embedded passive NLOS imaging prototype with event-based vision (PNPE), which induces an event camera for feature extraction of dynamic diffusion spot and leverages simulation dataset to pre-train the physical embedded model before fine-tuning with limited real-shot data. The proposed PNPE is verified by simulation and real-world experiments, and the comparisons of data paradigms also validate the superiority of event-based vision in passive NLOS imaging for moving targets.","sentences":["Passive Non-line-of-sight (NLOS) imaging has shown promising applications in imaging occluded objects around corners.","However, this inverse problem is highly ill-posed and results in poor reconstruction with traditional physical retrieval methods, particularly in moving target imaging.","With the development of neural networks, data-driven methods have greatly improved accuracy, however, heavy reliance on data volume has put great pressure on data collection and dataset fabrication.","We propose a physical embedded passive NLOS imaging prototype with event-based vision (PNPE), which induces an event camera for feature extraction of dynamic diffusion spot and leverages simulation dataset to pre-train the physical embedded model before fine-tuning with limited real-shot data.","The proposed PNPE is verified by simulation and real-world experiments, and the comparisons of data paradigms also validate the superiority of event-based vision in passive NLOS imaging for moving targets."],"url":"http://arxiv.org/abs/2404.05977v1","category":"physics.optics"}
{"created":"2024-04-09 03:10:45","title":"A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling","abstract":"Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications. To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence. Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts. The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service. The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously. AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers. A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance.","sentences":["Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications.","To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence.","Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts.","The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   ","This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service.","The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously.","AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers.","A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance."],"url":"http://arxiv.org/abs/2404.05976v1","category":"cs.LG"}
{"created":"2024-04-09 03:01:33","title":"Search for the Rare Decays $D_s^+\\to h^+(h^{0})e^+e^-$","abstract":"Using 7.33~fb$^{-1}$ of $e^{+}e^{-}$ collision data collected by the BESIII detector at center-of-mass energies in the range of $\\sqrt{s}=4.128 - 4.226$~GeV, we search for the rare decays $D_{s}^+\\to h^+(h^{0})e^{+}e^{-}$, where $h$ represents a kaon or pion. By requiring the $e^{+}e^{-}$ invariant mass to be consistent with a $\\phi(1020)$, $0.98<M(e^{+}e^{-})<1.04$ ~GeV/$c^2$, the decay $D_s^+\\to\\pi^+\\phi,\\phi\\to e^{+}e^{-}$ is observed with a statistical significance of 7.8$\\sigma$, and evidence for the decay $D_s^+\\to\\rho^+\\phi,\\phi\\to e^{+}e^{-}$ is found for the first time with a statistical significance of 4.4$\\sigma$. The decay branching fractions are measured to be $\\mathcal{B}(D_s^+\\to\\pi^+\\phi, \\phi\\to e^{+}e^{-} )=(1.17^{+0.23}_{-0.21}\\pm0.03)\\times 10^{-5}$, and $\\mathcal{B}(D_s^+\\to\\rho^+\\phi, \\phi\\to e^{+}e^{-} )=(2.44^{+0.67}_{-0.62}\\pm 0.16)\\times 10^{-5}$, where the first uncertainties are statistical and the second systematic. No significant signal for the three four-body decays of $D_{s}^{+}\\to \\pi^{+}\\pi^{0}e^{+}e^{-},\\ D_{s}^{+}\\to K^{+}\\pi^{0}e^{+}e^{-}$, and $D_{s}^{+}\\to K_{S}^{0}\\pi^{+}e^{+}e^{-}$ is observed. For $D_{s}^{+}\\to \\pi^{+}\\pi^{0}e^{+}e^{-}$, the $\\phi$ mass region is vetoed to minimize the long-distance effects. The 90$\\%$ confidence level upper limits set on the branching fractions of these decays are in the range of $(7.0-8.1)\\times 10^{-5}$.","sentences":["Using 7.33~fb$^{-1}$ of $e^{+}e^{-}$ collision data collected by the BESIII detector at center-of-mass energies in the range of $\\sqrt{s}=4.128 - 4.226$~GeV, we search for the rare decays $D_{s}^+\\to h^+(h^{0})e^{+}e^{-}$, where $h$ represents a kaon or pion.","By requiring the $e^{+}e^{-}$ invariant mass to be consistent with a $\\phi(1020)$, $0.98<M(e^{+}e^{-})<1.04$ ~GeV/$c^2$, the decay $D_s^+\\to\\pi^+\\phi,\\phi\\to e^{+}e^{-}$ is observed with a statistical significance of 7.8$\\sigma$, and evidence for the decay $D_s^+\\to\\rho^+\\phi,\\phi\\to e^{+}e^{-}$ is found for the first time with a statistical significance of 4.4$\\sigma$. The decay branching fractions are measured to be $\\mathcal{B}(D_s^+\\to\\pi^+\\phi, \\phi\\to e^{+}e^{-} )=(","1.17^{+0.23}_{-0.21}\\pm0.03)\\times 10^{-5}$, and $\\mathcal{B}(D_s^+\\to\\rho^+\\phi, \\phi\\to e^{+}e^{-} )=(","2.44^{+0.67}_{-0.62}\\pm 0.16)\\times 10^{-5}$, where the first uncertainties are statistical and the second systematic.","No significant signal for the three four-body decays of $D_{s}^{+}\\to \\pi^{+}\\pi^{0}e^{+}e^{-},\\ D_{s}^{+}\\to K^{+}\\pi^{0}e^{+}e^{-}$, and $D_{s}^{+}\\to K_{S}^{0}\\pi^{+}e^{+}e^{-}$ is observed.","For $D_{s}^{+}\\to \\pi^{+}\\pi^{0}e^{+}e^{-}$, the $\\phi$ mass region is vetoed to minimize the long-distance effects.","The 90$\\%$ confidence level upper limits set on the branching fractions of these decays are in the range of $(7.0-8.1)\\times 10^{-5}$."],"url":"http://arxiv.org/abs/2404.05973v1","category":"hep-ex"}
{"created":"2024-04-09 02:59:17","title":"Does Transformer Interpretability Transfer to RNNs?","abstract":"Recent advances in recurrent neural network architectures, such as Mamba and RWKV, have enabled RNNs to match or exceed the performance of equal-size transformers in terms of language modeling perplexity and downstream evaluations, suggesting that future systems may be built on completely new architectures. In this paper, we examine if selected interpretability methods originally designed for transformer language models will transfer to these up-and-coming recurrent architectures. Specifically, we focus on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models fine-tuned to produce false outputs under certain conditions. Our results show that most of these techniques are effective when applied to RNNs, and we show that it is possible to improve some of them by taking advantage of RNNs' compressed state.","sentences":["Recent advances in recurrent neural network architectures, such as Mamba and RWKV, have enabled RNNs to match or exceed the performance of equal-size transformers in terms of language modeling perplexity and downstream evaluations, suggesting that future systems may be built on completely new architectures.","In this paper, we examine if selected interpretability methods originally designed for transformer language models will transfer to these up-and-coming recurrent architectures.","Specifically, we focus on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models fine-tuned to produce false outputs under certain conditions.","Our results show that most of these techniques are effective when applied to RNNs, and we show that it is possible to improve some of them by taking advantage of RNNs' compressed state."],"url":"http://arxiv.org/abs/2404.05971v1","category":"cs.LG"}
{"created":"2024-04-09 02:55:12","title":"JSTR: Judgment Improves Scene Text Recognition","abstract":"In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other. While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline. This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text. The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition.","sentences":["In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other.","While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline.","This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text.","The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition."],"url":"http://arxiv.org/abs/2404.05967v1","category":"cs.CV"}
{"created":"2024-04-09 02:53:14","title":"THOUGHTSCULPT: Reasoning with Intermediate Revision and Search","abstract":"We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).","sentences":["We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components.","THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator.","Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output.","Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage)."],"url":"http://arxiv.org/abs/2404.05966v1","category":"cs.CL"}
{"created":"2024-04-09 02:51:32","title":"On the number of minimal forts of a graph","abstract":"In 2018, a fort of a graph was introduced as a non-empty subset of vertices in which no vertex outside of the set has exactly one neighbor in the set. Since then, forts have been used to characterize zero forcing sets, model the zero forcing number as an integer program, and generate lower bounds on the zero forcing number of a Cartesian product. In this article, we investigate the number of minimal forts of a graph, where a fort is minimal if every proper subset is not a fort. In particular, we show that the number of minimal forts of a graph of order at least six is strictly less than Sperner's bound, a famous bound due to Emanuel Sperner (1928) on the size of a collection of subsets where no subset contains another. Then, we derive an explicit formula for the number of minimal forts for several families of graphs, including the path, cycle, and spider graphs. Moreover, we show that the asymptotic growth rate of the number of minimal forts of the spider graph is bounded above by that of the path graph. We conjecture that the asymptotic growth rate of the path graph is extremal over all trees. Finally, we develop methods for constructing minimal forts of graph products using the minimal forts of the original graphs. In the process, we derive explicit formulas and lower bounds on the number of minimal forts for additional families of graphs, such as the wheel, sunlet, and windmill graphs. Most notably, we show that the family of windmill graphs has an exponential number of minimal forts with a maximum asymptotic growth rate of cube root of three, which is the largest asymptotic growth rate we have observed. We conjecture that there exist families of graphs with a larger asymptotic growth rate.","sentences":["In 2018, a fort of a graph was introduced as a non-empty subset of vertices in which no vertex outside of the set has exactly one neighbor in the set.","Since then, forts have been used to characterize zero forcing sets, model the zero forcing number as an integer program, and generate lower bounds on the zero forcing number of a Cartesian product.","In this article, we investigate the number of minimal forts of a graph, where a fort is minimal if every proper subset is not a fort.","In particular, we show that the number of minimal forts of a graph of order at least six is strictly less than Sperner's bound, a famous bound due to Emanuel Sperner (1928) on the size of a collection of subsets where no subset contains another.","Then, we derive an explicit formula for the number of minimal forts for several families of graphs, including the path, cycle, and spider graphs.","Moreover, we show that the asymptotic growth rate of the number of minimal forts of the spider graph is bounded above by that of the path graph.","We conjecture that the asymptotic growth rate of the path graph is extremal over all trees.","Finally, we develop methods for constructing minimal forts of graph products using the minimal forts of the original graphs.","In the process, we derive explicit formulas and lower bounds on the number of minimal forts for additional families of graphs, such as the wheel, sunlet, and windmill graphs.","Most notably, we show that the family of windmill graphs has an exponential number of minimal forts with a maximum asymptotic growth rate of cube root of three, which is the largest asymptotic growth rate we have observed.","We conjecture that there exist families of graphs with a larger asymptotic growth rate."],"url":"http://arxiv.org/abs/2404.05963v1","category":"math.CO"}
{"created":"2024-04-09 02:51:05","title":"LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders","abstract":"Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.","sentences":["Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks.","Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations.","In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder.","LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning.","We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks.","We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).","Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data.","Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data."],"url":"http://arxiv.org/abs/2404.05961v1","category":"cs.CL"}
{"created":"2024-04-09 02:45:39","title":"Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model","abstract":"Subwavelength photonic structures and metamaterials provide revolutionary approaches for controlling light. The inverse design methods proposed for these subwavelength structures are vital to the development of new photonic devices. However, most of the existing inverse design methods cannot realize direct mapping from optical properties to photonic structures but instead rely on forward simulation methods to perform iterative optimization. In this work, we exploit the powerful generative abilities of artificial intelligence (AI) and propose a practical inverse design method based on latent diffusion models. Our method maps directly the optical properties to structures without the requirement of forward simulation and iterative optimization. Here, the given optical properties can work as \"prompts\" and guide the constructed model to correctly \"draw\" the required photonic structures. Experiments show that our direct mapping-based inverse design method can generate subwavelength photonic structures at high fidelity while following the given optical properties. This may change the method used for optical design and greatly accelerate the research on new photonic devices.","sentences":["Subwavelength photonic structures and metamaterials provide revolutionary approaches for controlling light.","The inverse design methods proposed for these subwavelength structures are vital to the development of new photonic devices.","However, most of the existing inverse design methods cannot realize direct mapping from optical properties to photonic structures but instead rely on forward simulation methods to perform iterative optimization.","In this work, we exploit the powerful generative abilities of artificial intelligence (AI) and propose a practical inverse design method based on latent diffusion models.","Our method maps directly the optical properties to structures without the requirement of forward simulation and iterative optimization.","Here, the given optical properties can work as \"prompts\" and guide the constructed model to correctly \"draw\" the required photonic structures.","Experiments show that our direct mapping-based inverse design method can generate subwavelength photonic structures at high fidelity while following the given optical properties.","This may change the method used for optical design and greatly accelerate the research on new photonic devices."],"url":"http://arxiv.org/abs/2404.05959v1","category":"physics.optics"}
{"created":"2024-04-09 02:29:39","title":"VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?","abstract":"Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce \\bench{}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. \\bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on \\bench{}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe \\bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.","sentences":["Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks.","Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding.","In this paper, we introduce \\bench{}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks.","\\bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains.","We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on \\bench{}, revealing significant challenges and performance gaps.","Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs.","We believe \\bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications."],"url":"http://arxiv.org/abs/2404.05955v1","category":"cs.CL"}
{"created":"2024-04-09 02:11:35","title":"Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction","abstract":"Multi-task reinforcement learning (MTRL) demonstrate potential for enhancing the generalization of a robot, enabling it to perform multiple tasks concurrently. However, the performance of MTRL may still be susceptible to conflicts between tasks and negative interference. To facilitate efficient MTRL, we propose Task-Specific Action Correction (TSAC), a general and complementary approach designed for simultaneous learning of multiple tasks. TSAC decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). To alleviate conflicts resulting from excessive focus on specific tasks' details in SP, ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective and achieve generalization across tasks. Additional rewards transform the original problem into a multi-objective MTRL problem. Furthermore, to convert the multi-objective MTRL into a single-objective formulation, TSAC assigns a virtual expected budget to the sparse rewards and employs Lagrangian method to transform a constrained single-objective optimization into an unconstrained one. Experimental evaluations conducted on Meta-World's MT10 and MT50 benchmarks demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in both sample efficiency and effective action execution.","sentences":["Multi-task reinforcement learning (MTRL) demonstrate potential for enhancing the generalization of a robot, enabling it to perform multiple tasks concurrently.","However, the performance of MTRL may still be susceptible to conflicts between tasks and negative interference.","To facilitate efficient MTRL, we propose Task-Specific Action Correction (TSAC), a general and complementary approach designed for simultaneous learning of multiple tasks.","TSAC decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP).","To alleviate conflicts resulting from excessive focus on specific tasks' details in SP, ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective and achieve generalization across tasks.","Additional rewards transform the original problem into a multi-objective MTRL problem.","Furthermore, to convert the multi-objective MTRL into a single-objective formulation, TSAC assigns a virtual expected budget to the sparse rewards and employs Lagrangian method to transform a constrained single-objective optimization into an unconstrained one.","Experimental evaluations conducted on Meta-World's MT10 and MT50 benchmarks demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in both sample efficiency and effective action execution."],"url":"http://arxiv.org/abs/2404.05950v1","category":"cs.LG"}
{"created":"2024-04-09 01:55:11","title":"Shape transition and coexistence in Te isotopes studied with the quadrupole collective Hamiltonian based on a relativistic energy density functional","abstract":"Evolution and coexistence of shape and the related collective states of even-even Te isotopes are investigated within the quadrupole collective model that is based on the nuclear density functional theory. By means of the constrained self-consistent mean-field calculations performed within the relativistic Hartree-Bogoliubov method with a choice of an energy density functional and a pairing force, parameters of the triaxial quadrupole collective Hamiltonian are completely determines. The collective model produces for the near mid-shell nuclei, e.g., $^{116}$Te and $^{118}$Te, low-energy and low-spin excited states, including the $0^+_2$ state, that can be interpreted as the intruder states associated with the strongly deformed prolate minimum in the potential energy surface, along with the $0^+_1$ ground state that is attributed to the normal state based on a weakly oblate deformed global minimum. The collective model calculation suggests the lowering of the $0^+_2$ energy level toward neutron mid-shell, as observed experimentally. The sensitivity of the mean-field and spectroscopic results to the choice of the pairing strength is also analyzed, and it is shown that by increasing the pairing strength description of the low-lying states of the near mid-shell nuclei is improved.","sentences":["Evolution and coexistence of shape and the related collective states of even-even Te isotopes are investigated within the quadrupole collective model that is based on the nuclear density functional theory.","By means of the constrained self-consistent mean-field calculations performed within the relativistic Hartree-Bogoliubov method with a choice of an energy density functional and a pairing force, parameters of the triaxial quadrupole collective Hamiltonian are completely determines.","The collective model produces for the near mid-shell nuclei, e.g., $^{116}$Te and $^{118}$Te, low-energy and low-spin excited states, including the $0^+_2$ state, that can be interpreted as the intruder states associated with the strongly deformed prolate minimum in the potential energy surface, along with the $0^+_1$ ground state that is attributed to the normal state based on a weakly oblate deformed global minimum.","The collective model calculation suggests the lowering of the $0^+_2$ energy level toward neutron mid-shell, as observed experimentally.","The sensitivity of the mean-field and spectroscopic results to the choice of the pairing strength is also analyzed, and it is shown that by increasing the pairing strength description of the low-lying states of the near mid-shell nuclei is improved."],"url":"http://arxiv.org/abs/2404.05944v1","category":"nucl-th"}
{"created":"2024-04-09 01:55:05","title":"Interplay of Machine Translation, Diacritics, and Diacritization","abstract":"We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance. We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages). For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario. We find that MT harms diacritization in LR but benefits significantly in HR for some languages. For (2), MT performance is similar regardless of diacritics being kept or removed. In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models. Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate.","sentences":["We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance.","We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages).","For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario.","We find that MT harms diacritization in LR but benefits significantly in HR for some languages.","For (2), MT performance is similar regardless of diacritics being kept or removed.","In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models.","Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate."],"url":"http://arxiv.org/abs/2404.05943v1","category":"cs.CL"}
{"created":"2024-04-09 00:51:24","title":"Inclusive Practices for Child-Centered AI Design and Testing","abstract":"We explore ideas and inclusive practices for designing and testing child-centered artificially intelligent technologies for neurodivergent children. AI is promising for supporting social communication, self-regulation, and sensory processing challenges common for neurodivergent children. The authors, both neurodivergent individuals and related to neurodivergent people, draw from their professional and personal experiences to offer insights on creating AI technologies that are accessible and include input from neurodivergent children. We offer ideas for designing AI technologies for neurodivergent children and considerations for including them in the design process while accounting for their sensory sensitivities. We conclude by emphasizing the importance of adaptable and supportive AI technologies and design processes and call for further conversation to refine child-centered AI design and testing methods.","sentences":["We explore ideas and inclusive practices for designing and testing child-centered artificially intelligent technologies for neurodivergent children.","AI is promising for supporting social communication, self-regulation, and sensory processing challenges common for neurodivergent children.","The authors, both neurodivergent individuals and related to neurodivergent people, draw from their professional and personal experiences to offer insights on creating AI technologies that are accessible and include input from neurodivergent children.","We offer ideas for designing AI technologies for neurodivergent children and considerations for including them in the design process while accounting for their sensory sensitivities.","We conclude by emphasizing the importance of adaptable and supportive AI technologies and design processes and call for further conversation to refine child-centered AI design and testing methods."],"url":"http://arxiv.org/abs/2404.05920v1","category":"cs.HC"}
{"created":"2024-04-09 00:26:32","title":"Evolving Collective Behavior in Self-Organizing Particle Systems","abstract":"Local interactions drive emergent collective behavior, which pervades biological and social complex systems. But uncovering the interactions that produce a desired behavior remains a core challenge. In this paper, we present EvoSOPS, an evolutionary framework that searches landscapes of stochastic distributed algorithms for those that achieve a mathematically specified target behavior. These algorithms govern self-organizing particle systems (SOPS) comprising individuals with no persistent memory and strictly local sensing and movement. For aggregation, phototaxing, and separation behaviors, EvoSOPS discovers algorithms that achieve 4.2-15.3% higher fitness than those from the existing \"stochastic approach to SOPS\" based on mathematical theory from statistical physics. EvoSOPS is also flexibly applied to new behaviors such as object coating where the stochastic approach would require bespoke, extensive analysis. Finally, we distill insights from the diverse, best-fitness genomes produced for aggregation across repeated EvoSOPS runs to demonstrate how EvoSOPS can bootstrap future theoretical investigations into SOPS algorithms for new behaviors.","sentences":["Local interactions drive emergent collective behavior, which pervades biological and social complex systems.","But uncovering the interactions that produce a desired behavior remains a core challenge.","In this paper, we present EvoSOPS, an evolutionary framework that searches landscapes of stochastic distributed algorithms for those that achieve a mathematically specified target behavior.","These algorithms govern self-organizing particle systems (SOPS) comprising individuals with no persistent memory and strictly local sensing and movement.","For aggregation, phototaxing, and separation behaviors, EvoSOPS discovers algorithms that achieve 4.2-15.3% higher fitness than those from the existing \"stochastic approach to SOPS\" based on mathematical theory from statistical physics.","EvoSOPS is also flexibly applied to new behaviors such as object coating where the stochastic approach would require bespoke, extensive analysis.","Finally, we distill insights from the diverse, best-fitness genomes produced for aggregation across repeated EvoSOPS runs to demonstrate how EvoSOPS can bootstrap future theoretical investigations into SOPS algorithms for new behaviors."],"url":"http://arxiv.org/abs/2404.05915v1","category":"cs.NE"}
{"created":"2024-04-09 00:07:16","title":"Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus","abstract":"Background: Clinical diagnosis is typically reached by following a series of steps recommended by guidelines authored by colleges of experts. Accordingly, guidelines play a crucial role in rationalizing clinical decisions but suffer from limitations as they are built to cover the majority of the population and fail at covering patients with uncommon conditions. Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and practices.   Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs). We apply DRL on synthetic, but realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow the schema of a decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score. We particularly evaluate the robustness of our approaches to noisy and missing data since these frequently occur in EHRs.   Results: In both use cases, and in the presence of imperfect data, our best DRL algorithms exhibit competitive performance when compared to the traditional classifiers, with the added advantage that they enable the progressive generation of a pathway to the suggested diagnosis which can both guide and explain the decision-making process.   Conclusion: DRL offers the opportunity to learn personalized decision pathways to diagnosis. We illustrate with our two use cases their advantages: they generate step-by-step pathways that are self-explanatory; and their correctness is competitive when compared to state-of-the-art approaches.","sentences":["Background: Clinical diagnosis is typically reached by following a series of steps recommended by guidelines authored by colleges of experts.","Accordingly, guidelines play a crucial role in rationalizing clinical decisions but suffer from limitations as they are built to cover the majority of the population and fail at covering patients with uncommon conditions.","Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and practices.   ","Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs).","We apply DRL on synthetic, but realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow the schema of a decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score.","We particularly evaluate the robustness of our approaches to noisy and missing data since these frequently occur in EHRs.   ","Results:","In both use cases, and in the presence of imperfect data, our best DRL algorithms exhibit competitive performance when compared to the traditional classifiers, with the added advantage that they enable the progressive generation of a pathway to the suggested diagnosis which can both guide and explain the decision-making process.   ","Conclusion: DRL offers the opportunity to learn personalized decision pathways to diagnosis.","We illustrate with our two use cases their advantages: they generate step-by-step pathways that are self-explanatory; and their correctness is competitive when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.05913v1","category":"cs.LG"}
{"created":"2024-04-08 23:46:59","title":"Interpretability in Symbolic Regression: a benchmark of Explanatory Methods using the Feynman data set","abstract":"In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy. Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness. Many model-agnostic explanatory methods exists to provide explanations for black-box models. In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression. When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers. This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models. Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures. In addition, we further analyzed four benchmarks from the GP community. The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations. Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models. This benchmark is publicly available for further experiments.","sentences":["In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy.","Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness.","Many model-agnostic explanatory methods exists to provide explanations for black-box models.","In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression.","When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers.","This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models.","Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures.","In addition, we further analyzed four benchmarks from the GP community.","The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations.","Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models.","This benchmark is publicly available for further experiments."],"url":"http://arxiv.org/abs/2404.05908v1","category":"cs.LG"}
{"created":"2024-04-08 23:15:41","title":"Natural Learning","abstract":"We introduce Natural Learning (NL), a novel algorithm that elevates the explainability and interpretability of machine learning to an extreme level. NL simplifies decisions into intuitive rules, like \"We rejected your loan because your income, employment status, and age collectively resemble a rejected prototype more than an accepted prototype.\" When applied to real-life datasets, NL produces impressive results. For example, in a colon cancer dataset with 1545 patients and 10935 genes, NL achieves 98.1% accuracy, comparable to DNNs and RF, by analyzing just 3 genes of test samples against 2 discovered prototypes. Similarly, in the UCI's WDBC dataset, NL achieves 98.3% accuracy using only 7 features and 2 prototypes. Even on the MNIST dataset (0 vs. 1), NL achieves 99.5% accuracy with only 3 pixels from 2 prototype images. NL is inspired by prototype theory, an old concept in cognitive psychology suggesting that people learn single sparse prototypes to categorize objects. Leveraging this relaxed assumption, we redesign Support Vector Machines (SVM), replacing its mathematical formulation with a fully nearest-neighbor-based solution, and to address the curse of dimensionality, we utilize locality-sensitive hashing. Following theory's generalizability principle, we propose a recursive method to prune non-core features. As a result, NL efficiently discovers the sparsest prototypes in O(n^2pL) with high parallelization capacity in terms of n. Evaluation of NL with 17 benchmark datasets shows its significant outperformance compared to decision trees and logistic regression, two methods widely favored in healthcare for their interpretability. Moreover, NL achieves performance comparable to finetuned black-box models such as deep neural networks and random forests in 40% of cases, with only a 1-2% lower average accuracy. The code is available via http://natural-learning.cc.","sentences":["We introduce Natural Learning (NL), a novel algorithm that elevates the explainability and interpretability of machine learning to an extreme level.","NL simplifies decisions into intuitive rules, like \"We rejected your loan because your income, employment status, and age collectively resemble a rejected prototype more than an accepted prototype.\"","When applied to real-life datasets, NL produces impressive results.","For example, in a colon cancer dataset with 1545 patients and 10935 genes, NL achieves 98.1% accuracy, comparable to DNNs and RF, by analyzing just 3 genes of test samples against 2 discovered prototypes.","Similarly, in the UCI's WDBC dataset, NL achieves 98.3% accuracy using only 7 features and 2 prototypes.","Even on the MNIST dataset (0 vs. 1), NL achieves 99.5% accuracy with only 3 pixels from 2 prototype images.","NL is inspired by prototype theory, an old concept in cognitive psychology suggesting that people learn single sparse prototypes to categorize objects.","Leveraging this relaxed assumption, we redesign Support Vector Machines (SVM), replacing its mathematical formulation with a fully nearest-neighbor-based solution, and to address the curse of dimensionality, we utilize locality-sensitive hashing.","Following theory's generalizability principle, we propose a recursive method to prune non-core features.","As a result, NL efficiently discovers the sparsest prototypes in O(n^2pL) with high parallelization capacity in terms of n. Evaluation of NL with 17 benchmark datasets shows its significant outperformance compared to decision trees and logistic regression, two methods widely favored in healthcare for their interpretability.","Moreover, NL achieves performance comparable to finetuned black-box models such as deep neural networks and random forests in 40% of cases, with only a 1-2% lower average accuracy.","The code is available via http://natural-learning.cc."],"url":"http://arxiv.org/abs/2404.05903v1","category":"cs.LG"}
{"created":"2024-04-08 23:10:47","title":"WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents","abstract":"In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem. Due to high variance in website structure, existing approaches often fail. Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites. We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs. To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes. Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation. Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites. On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.","sentences":["In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem.","Due to high variance in website structure, existing approaches often fail.","Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites.","We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs.","To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes.","Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation.","Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites.","On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web."],"url":"http://arxiv.org/abs/2404.05902v1","category":"cs.CL"}
{"created":"2024-04-08 22:40:57","title":"Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning","abstract":"Transit agencies world-wide face tightening budgets. To maintain quality of service while cutting costs, efficient transit network design is essential. But planning a network of public transit routes is a challenging optimization problem. The most successful approaches to date use metaheuristic algorithms to search through the space of solutions by applying low-level heuristics that randomly alter routes in a network. The design of these low-level heuristics has a major impact on the quality of the result. In this paper we use deep reinforcement learning with graph neural nets to learn low-level heuristics for an evolutionary algorithm, instead of designing them manually. These learned heuristics improve the algorithm's results on benchmark synthetic cities with 70 nodes or more, and obtain state-of-the-art results when optimizing operating costs. They also improve upon a simulation of the real transit network in the city of Laval, Canada, by as much as 54% and 18% on two key metrics, and offer cost savings of up to 12% over the city's existing transit network.","sentences":["Transit agencies world-wide face tightening budgets.","To maintain quality of service while cutting costs, efficient transit network design is essential.","But planning a network of public transit routes is a challenging optimization problem.","The most successful approaches to date use metaheuristic algorithms to search through the space of solutions by applying low-level heuristics that randomly alter routes in a network.","The design of these low-level heuristics has a major impact on the quality of the result.","In this paper we use deep reinforcement learning with graph neural nets to learn low-level heuristics for an evolutionary algorithm, instead of designing them manually.","These learned heuristics improve the algorithm's results on benchmark synthetic cities with 70 nodes or more, and obtain state-of-the-art results when optimizing operating costs.","They also improve upon a simulation of the real transit network in the city of Laval, Canada, by as much as 54% and 18% on two key metrics, and offer cost savings of up to 12% over the city's existing transit network."],"url":"http://arxiv.org/abs/2404.05894v1","category":"cs.LG"}
{"created":"2024-04-08 22:29:53","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models","abstract":"Metadata play a crucial role in ensuring the findability, accessibility, interoperability, and reusability of datasets. This paper investigates the potential of large language models (LLMs), specifically GPT-4, to improve adherence to metadata standards. We conducted experiments on 200 random data records describing human samples relating to lung cancer from the NCBI BioSample repository, evaluating GPT-4's ability to suggest edits for adherence to metadata standards. We computed the adherence accuracy of field name-field value pairs through a peer review process, and we observed a marginal average improvement in adherence to the standard data dictionary from 79% to 80% (p<0.01). We then prompted GPT-4 with domain information in the form of the textual descriptions of CEDAR templates and recorded a significant improvement to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be able to correct legacy metadata to ensure satisfactory adherence to standards when unaided, they do show promise for use in automated metadata curation when integrated with a structured knowledge base.","sentences":["Metadata play a crucial role in ensuring the findability, accessibility, interoperability, and reusability of datasets.","This paper investigates the potential of large language models (LLMs), specifically GPT-4, to improve adherence to metadata standards.","We conducted experiments on 200 random data records describing human samples relating to lung cancer from the NCBI BioSample repository, evaluating GPT-4's ability to suggest edits for adherence to metadata standards.","We computed the adherence accuracy of field name-field value pairs through a peer review process, and we observed a marginal average improvement in adherence to the standard data dictionary from 79% to 80% (p<0.01).","We then prompted GPT-4 with domain information in the form of the textual descriptions of CEDAR templates and recorded a significant improvement to 97% from 79% (p<0.01).","These results indicate that, while LLMs may not be able to correct legacy metadata to ensure satisfactory adherence to standards when unaided, they do show promise for use in automated metadata curation when integrated with a structured knowledge base."],"url":"http://arxiv.org/abs/2404.05893v1","category":"cs.AI"}
{"created":"2024-04-08 22:20:59","title":"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence","abstract":"We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer","sentences":["We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture.","Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs.","We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.","We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks.","We release all our models on HuggingFace under the Apache 2.0 license.","Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer"],"url":"http://arxiv.org/abs/2404.05892v1","category":"cs.CL"}
{"created":"2024-04-08 22:20:23","title":"Condition Monitoring with Incomplete Data: An Integrated Variational Autoencoder and Distance Metric Framework","abstract":"Condition monitoring of industrial systems is crucial for ensuring safety and maintenance planning, yet notable challenges arise in real-world settings due to the limited or non-existent availability of fault samples. This paper introduces an innovative solution to this problem by proposing a new method for fault detection and condition monitoring for unseen data. Adopting an approach inspired by zero-shot learning, our method can identify faults and assign a relative health index to various operational conditions. Typically, we have plenty of data on normal operations, some data on compromised conditions, and very few (if any) samples of severe faults. We use a variational autoencoder to capture the probabilistic distribution of previously seen and new unseen conditions. The health status is determined by comparing each sample's deviation from a normal operation reference distribution in the latent space. Faults are detected by establishing a threshold for the health indexes, allowing the model to identify severe, unseen faults with high accuracy, even amidst noise. We validate our approach using the run-to-failure IMS-bearing dataset and compare it with other methods. The health indexes generated by our model closely match the established descriptive model of bearing wear, attesting to the robustness and reliability of our method. These findings highlight the potential of our methodology in augmenting fault detection capabilities within industrial domains, thereby contributing to heightened safety protocols and optimized maintenance practices.","sentences":["Condition monitoring of industrial systems is crucial for ensuring safety and maintenance planning, yet notable challenges arise in real-world settings due to the limited or non-existent availability of fault samples.","This paper introduces an innovative solution to this problem by proposing a new method for fault detection and condition monitoring for unseen data.","Adopting an approach inspired by zero-shot learning, our method can identify faults and assign a relative health index to various operational conditions.","Typically, we have plenty of data on normal operations, some data on compromised conditions, and very few (if any) samples of severe faults.","We use a variational autoencoder to capture the probabilistic distribution of previously seen and new unseen conditions.","The health status is determined by comparing each sample's deviation from a normal operation reference distribution in the latent space.","Faults are detected by establishing a threshold for the health indexes, allowing the model to identify severe, unseen faults with high accuracy, even amidst noise.","We validate our approach using the run-to-failure IMS-bearing dataset and compare it with other methods.","The health indexes generated by our model closely match the established descriptive model of bearing wear, attesting to the robustness and reliability of our method.","These findings highlight the potential of our methodology in augmenting fault detection capabilities within industrial domains, thereby contributing to heightened safety protocols and optimized maintenance practices."],"url":"http://arxiv.org/abs/2404.05891v1","category":"eess.SP"}
{"created":"2024-04-08 21:48:36","title":"GBEC: Geometry-Based Hand-Eye Calibration","abstract":"Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it. Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability. However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment. We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations. To demonstrate improvements, we apply the approach to two different robot-assisted procedures: Transcranial Magnetic Stimulation (TMS) and femoroplasty. We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods. In the experiments, we perform GBEC between the robot end-effector and an optical tracker's rigid body marker attached to the TMS coil or femoroplasty drill guide. Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration. When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix. Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor.","sentences":["Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it.","Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability.","However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment.","We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations.","To demonstrate improvements, we apply the approach to two different robot-assisted procedures:","Transcranial Magnetic Stimulation (TMS) and femoroplasty.","We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods.","In the experiments, we perform GBEC between the robot end-effector and an optical tracker's rigid body marker attached to the TMS coil or femoroplasty drill guide.","Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration.","When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix.","Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor."],"url":"http://arxiv.org/abs/2404.05884v1","category":"cs.RO"}
{"created":"2024-04-08 21:15:36","title":"CodecLM: Aligning Language Models with Tailored Synthetic Data","abstract":"Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.","sentences":["Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals.","To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data.","Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases.","It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs.","To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs.","Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process.","We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions.","We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples.","Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts."],"url":"http://arxiv.org/abs/2404.05875v1","category":"cs.CL"}
{"created":"2024-04-08 21:15:26","title":"Youth as Peer Auditors: Engaging Teenagers with Algorithm Auditing of Machine Learning Applications","abstract":"As artificial intelligence/machine learning (AI/ML) applications become more pervasive in youth lives, supporting them to interact, design, and evaluate applications is crucial. This paper positions youth as auditors of their peers' ML-powered applications to better understand algorithmic systems' opaque inner workings and external impacts. In a two-week workshop, 13 youth (ages 14-15) designed and audited ML-powered applications. We analyzed pre/post clinical interviews in which youth were presented with auditing tasks. The analyses show that after the workshop all youth identified algorithmic biases and inferred dataset and model design issues. Youth also discussed algorithmic justice issues and ML model improvements. Furthermore, youth reflected that auditing provided them new perspectives on model functionality and ideas to improve their own models. This work contributes (1) a conceptualization of algorithm auditing for youth; and (2) empirical evidence of the potential benefits of auditing. We discuss potential uses of algorithm auditing in learning and child-computer interaction research.","sentences":["As artificial intelligence/machine learning (AI/ML) applications become more pervasive in youth lives, supporting them to interact, design, and evaluate applications is crucial.","This paper positions youth as auditors of their peers' ML-powered applications to better understand algorithmic systems' opaque inner workings and external impacts.","In a two-week workshop, 13 youth (ages 14-15) designed and audited ML-powered applications.","We analyzed pre/post clinical interviews in which youth were presented with auditing tasks.","The analyses show that after the workshop all youth identified algorithmic biases and inferred dataset and model design issues.","Youth also discussed algorithmic justice issues and ML model improvements.","Furthermore, youth reflected that auditing provided them new perspectives on model functionality and ideas to improve their own models.","This work contributes (1) a conceptualization of algorithm auditing for youth; and (2) empirical evidence of the potential benefits of auditing.","We discuss potential uses of algorithm auditing in learning and child-computer interaction research."],"url":"http://arxiv.org/abs/2404.05874v1","category":"cs.HC"}
{"created":"2024-04-08 21:10:05","title":"Model Predictive Control based Energy Management System for Home Energy Resiliency","abstract":"As the occurrence of extreme weather events is increasing so are the outages caused by them. During such unplanned outages, a house needs to be provided with an energy supply to maintain habitable conditions by maintaining thermal comfort and servicing at least critical loads. An energy system consisting of rooftop photovoltaic (PV) panels along with battery storage is an excellent carbon-free choice to provide energy resiliency to houses against extreme weather-related outages. However, to provide habitable conditions this energy system has to provide not only for the non-air-conditioning (non-AC) load demand but also for the turning on of the AC system which has a considerably higher startup power requirement as compared to its rated power. Hence, an intelligent automated decision-making controller is needed which can manage the trade-off between competing requirements.   In this paper, we propose such an intelligent controller based on Model Predictive Control (MPC). We compare its performance with a Baseline controller which is unintelligent, and a Rule-Based controller which has some intelligence, based on three resiliency metrics that we have developed. We perform extensive simulations for numerous scenarios involving different energy system sizes and AC startup power requirements. Every simulation is one week long and is carried out for a single-family detached house located in Florida in the aftermath of Hurricane Irma in 2017. The simulation results show that the MPC controller performs better than the other controllers in the more energy-constrained scenarios (smaller PV-battery size, larger AC startup power requirement) in providing both thermal comfort and servicing non-AC loads in a balanced manner.","sentences":["As the occurrence of extreme weather events is increasing so are the outages caused by them.","During such unplanned outages, a house needs to be provided with an energy supply to maintain habitable conditions by maintaining thermal comfort and servicing at least critical loads.","An energy system consisting of rooftop photovoltaic (PV) panels along with battery storage is an excellent carbon-free choice to provide energy resiliency to houses against extreme weather-related outages.","However, to provide habitable conditions this energy system has to provide not only for the non-air-conditioning (non-AC) load demand but also for the turning on of the AC system which has a considerably higher startup power requirement as compared to its rated power.","Hence, an intelligent automated decision-making controller is needed which can manage the trade-off between competing requirements.   ","In this paper, we propose such an intelligent controller based on Model Predictive Control (MPC).","We compare its performance with a Baseline controller which is unintelligent, and a Rule-Based controller which has some intelligence, based on three resiliency metrics that we have developed.","We perform extensive simulations for numerous scenarios involving different energy system sizes and AC startup power requirements.","Every simulation is one week long and is carried out for a single-family detached house located in Florida in the aftermath of Hurricane Irma in 2017.","The simulation results show that the MPC controller performs better than the other controllers in the more energy-constrained scenarios (smaller PV-battery size, larger AC startup power requirement) in providing both thermal comfort and servicing non-AC loads in a balanced manner."],"url":"http://arxiv.org/abs/2404.05873v1","category":"eess.SY"}
{"created":"2024-04-08 21:05:42","title":"Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning","abstract":"Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities.   In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data.","sentences":["Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training.","LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks.","Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data.","However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities.   ","In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset.","We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA.","Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities.","We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish.","Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."],"url":"http://arxiv.org/abs/2404.05868v1","category":"cs.LG"}
{"created":"2024-04-08 20:28:22","title":"Approaching Emergent Risks: An Exploratory Study into Artificial Intelligence Risk Management within Financial Organisations","abstract":"Globally, artificial intelligence (AI) implementation is growing, holding the capability to fundamentally alter organisational processes and decision making. Simultaneously, this brings a multitude of emergent risks to organisations, exposing vulnerabilities in their extant risk management frameworks. This necessitates a greater understanding of how organisations can position themselves in response. This issue is particularly pertinent within the financial sector with relatively mature AI applications matched with severe societal repercussions of potential risk events. Despite this, academic risk management literature is trailing behind the speed of AI implementation. Adopting a management perspective, this study aims to contribute to the understanding of AI risk management in organisations through an exploratory empirical investigation into these practices. In-depth insights are gained through interviews with nine practitioners from different organisations within the UK financial sector. Through examining areas of organisational convergence and divergence, the findings of this study unearth levels of risk management framework readiness and prevailing approaches to risk management at both a processual and organisational level. Whilst enhancing the developing literature concerning AI risk management within organisations, the study simultaneously offers a practical contribution, providing key areas of guidance for practitioners in the operational development of AI risk management frameworks.","sentences":["Globally, artificial intelligence (AI) implementation is growing, holding the capability to fundamentally alter organisational processes and decision making.","Simultaneously, this brings a multitude of emergent risks to organisations, exposing vulnerabilities in their extant risk management frameworks.","This necessitates a greater understanding of how organisations can position themselves in response.","This issue is particularly pertinent within the financial sector with relatively mature AI applications matched with severe societal repercussions of potential risk events.","Despite this, academic risk management literature is trailing behind the speed of AI implementation.","Adopting a management perspective, this study aims to contribute to the understanding of AI risk management in organisations through an exploratory empirical investigation into these practices.","In-depth insights are gained through interviews with nine practitioners from different organisations within the UK financial sector.","Through examining areas of organisational convergence and divergence, the findings of this study unearth levels of risk management framework readiness and prevailing approaches to risk management at both a processual and organisational level.","Whilst enhancing the developing literature concerning AI risk management within organisations, the study simultaneously offers a practical contribution, providing key areas of guidance for practitioners in the operational development of AI risk management frameworks."],"url":"http://arxiv.org/abs/2404.05847v1","category":"cs.CY"}
{"created":"2024-04-08 20:06:33","title":"Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks","abstract":"In this paper, we introduce an alternative approach to enhancing Multi-Agent Reinforcement Learning (MARL) through the integration of domain knowledge and attention-based policy mechanisms. Our methodology focuses on the incorporation of domain-specific expertise into the learning process, which simplifies the development of collaborative behaviors. This approach aims to reduce the complexity and learning overhead typically associated with MARL by enabling agents to concentrate on essential aspects of complex tasks, thus optimizing the learning curve. The utilization of attention mechanisms plays a key role in our model. It allows for the effective processing of dynamic context data and nuanced agent interactions, leading to more refined decision-making. Applied in standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory (SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method has been shown to improve both learning efficiency and the effectiveness of collaborative behaviors. The results indicate that our attention-based approach can be a viable approach for improving the efficiency of MARL training process, integrating domain-specific knowledge at the action level.","sentences":["In this paper, we introduce an alternative approach to enhancing Multi-Agent Reinforcement Learning (MARL) through the integration of domain knowledge and attention-based policy mechanisms.","Our methodology focuses on the incorporation of domain-specific expertise into the learning process, which simplifies the development of collaborative behaviors.","This approach aims to reduce the complexity and learning overhead typically associated with MARL by enabling agents to concentrate on essential aspects of complex tasks, thus optimizing the learning curve.","The utilization of attention mechanisms plays a key role in our model.","It allows for the effective processing of dynamic context data and nuanced agent interactions, leading to more refined decision-making.","Applied in standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory (SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method has been shown to improve both learning efficiency and the effectiveness of collaborative behaviors.","The results indicate that our attention-based approach can be a viable approach for improving the efficiency of MARL training process, integrating domain-specific knowledge at the action level."],"url":"http://arxiv.org/abs/2404.05840v1","category":"cs.LG"}
{"created":"2024-04-08 19:48:36","title":"SambaLingo: Teaching Large Language Models New Languages","abstract":"Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experiments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and checkpoints are made public to facilitate future research.","sentences":["Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages.","One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages.","While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered.","In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages.","Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages.","We scale these experiments across 9 languages and 2 parameter scales (7B and 70B).","We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines.","Additionally, all evaluation code and checkpoints are made public to facilitate future research."],"url":"http://arxiv.org/abs/2404.05829v1","category":"cs.CL"}
{"created":"2024-04-08 19:46:20","title":"Privacy-Preserving Deep Learning Using Deformable Operators for Secure Task Learning","abstract":"In the era of cloud computing and data-driven applications, it is crucial to protect sensitive information to maintain data privacy, ensuring truly reliable systems. As a result, preserving privacy in deep learning systems has become a critical concern. Existing methods for privacy preservation rely on image encryption or perceptual transformation approaches. However, they often suffer from reduced task performance and high computational costs. To address these challenges, we propose a novel Privacy-Preserving framework that uses a set of deformable operators for secure task learning. Our method involves shuffling pixels during the analog-to-digital conversion process to generate visually protected data. Those are then fed into a well-known network enhanced with deformable operators. Using our approach, users can achieve equivalent performance to original images without additional training using a secret key. Moreover, our method enables access control against unauthorized users. Experimental results demonstrate the efficacy of our approach, showcasing its potential in cloud-based scenarios and privacy-sensitive applications.","sentences":["In the era of cloud computing and data-driven applications, it is crucial to protect sensitive information to maintain data privacy, ensuring truly reliable systems.","As a result, preserving privacy in deep learning systems has become a critical concern.","Existing methods for privacy preservation rely on image encryption or perceptual transformation approaches.","However, they often suffer from reduced task performance and high computational costs.","To address these challenges, we propose a novel Privacy-Preserving framework that uses a set of deformable operators for secure task learning.","Our method involves shuffling pixels during the analog-to-digital conversion process to generate visually protected data.","Those are then fed into a well-known network enhanced with deformable operators.","Using our approach, users can achieve equivalent performance to original images without additional training using a secret key.","Moreover, our method enables access control against unauthorized users.","Experimental results demonstrate the efficacy of our approach, showcasing its potential in cloud-based scenarios and privacy-sensitive applications."],"url":"http://arxiv.org/abs/2404.05828v1","category":"cs.CV"}
{"created":"2024-04-08 19:29:15","title":"Decade-long Utilization Patterns of ICSE Technical Papers and Associated Artifacts","abstract":"Context: Annually, ICSE acknowledges a range of papers, a subset of which are paired with research artifacts such as source code, datasets, and supplementary materials, adhering to the Open Science Policy. However, no prior systematic inquiry dives into gauging the influence of ICSE papers using artifact attributes. Objective: We explore the mutual impact between artifacts and their associated papers presented at ICSE over ten years.   Method: We collect data on usage attributes from papers and their artifacts, conduct a statistical assessment to identify differences, and analyze the top five papers in each attribute category.   Results: There is a significant difference between paper citations and the usage of associated artifacts. While statistical analyses show no notable difference between paper citations and GitHub stars, variations exist in views and/or downloads of papers and artifacts.   Conclusion: We provide a thorough overview of ICSE's accepted papers from the last decade, emphasizing the intricate relationship between research papers and their artifacts. To enhance the assessment of artifact influence in software research, we recommend considering key attributes that may be present in one platform but not in another.","sentences":["Context: Annually, ICSE acknowledges a range of papers, a subset of which are paired with research artifacts such as source code, datasets, and supplementary materials, adhering to the Open Science Policy.","However, no prior systematic inquiry dives into gauging the influence of ICSE papers using artifact attributes.","Objective: We explore the mutual impact between artifacts and their associated papers presented at ICSE over ten years.   ","Method: We collect data on usage attributes from papers and their artifacts, conduct a statistical assessment to identify differences, and analyze the top five papers in each attribute category.   ","Results: There is a significant difference between paper citations and the usage of associated artifacts.","While statistical analyses show no notable difference between paper citations and GitHub stars, variations exist in views and/or downloads of papers and artifacts.   ","Conclusion: We provide a thorough overview of ICSE's accepted papers from the last decade, emphasizing the intricate relationship between research papers and their artifacts.","To enhance the assessment of artifact influence in software research, we recommend considering key attributes that may be present in one platform but not in another."],"url":"http://arxiv.org/abs/2404.05826v1","category":"cs.SE"}
{"created":"2024-04-08 19:29:07","title":"LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding","abstract":"Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.","sentences":["Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches.","This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation.","In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc.","By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets."],"url":"http://arxiv.org/abs/2404.05825v1","category":"cs.IR"}
{"created":"2024-04-08 19:21:34","title":"Phenomenon of self-oscillation in bubble dynamics: Bouncing acoustic bubbles","abstract":"Self-oscillations underlie many natural phenomena such as heartbeat, ocean waves, and the pulsation of variable stars. From pendulum clocks to the behavior of animal groups, self-oscillation is one of the keys to the understanding of synchronization phenomena and hence the collective behavior of interacting systems. In this study, we consider two closely spaced bubbles pulsating in the kHz range in response to ultrasonic excitation. A translational bouncing motion emerges from their interaction with a much lower frequency than the bubble pulsation frequency. Our analysis reveals that the observed bubble bouncing exhibits the main features of self-oscillation, such as negative damping and the emergence of a limit cycle. These results highlight unexpected nonlinear effects in the field of microbubbles and give insights into the understanding of synchronization in large bubble clouds.","sentences":["Self-oscillations underlie many natural phenomena such as heartbeat, ocean waves, and the pulsation of variable stars.","From pendulum clocks to the behavior of animal groups, self-oscillation is one of the keys to the understanding of synchronization phenomena and hence the collective behavior of interacting systems.","In this study, we consider two closely spaced bubbles pulsating in the kHz range in response to ultrasonic excitation.","A translational bouncing motion emerges from their interaction with a much lower frequency than the bubble pulsation frequency.","Our analysis reveals that the observed bubble bouncing exhibits the main features of self-oscillation, such as negative damping and the emergence of a limit cycle.","These results highlight unexpected nonlinear effects in the field of microbubbles and give insights into the understanding of synchronization in large bubble clouds."],"url":"http://arxiv.org/abs/2404.05822v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 18:16:22","title":"Self-Labeling in Multivariate Causality and Quantification for Adaptive Machine Learning","abstract":"Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment. Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions. Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning. Several unanswered research questions remain, including self-labeling's compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling. The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling. This paper further develops the self-labeling framework and its theoretical foundations to address these research questions. A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed. A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory.","sentences":["Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment.","Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions.","Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning.","Several unanswered research questions remain, including self-labeling's compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling.","The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling.","This paper further develops the self-labeling framework and its theoretical foundations to address these research questions.","A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed.","A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory."],"url":"http://arxiv.org/abs/2404.05809v1","category":"cs.LG"}
{"created":"2024-04-08 18:05:24","title":"BatSort: Enhanced Battery Classification with Transfer Learning for Battery Sorting and Recycling","abstract":"Battery recycling is a critical process for minimizing environmental harm and resource waste for used batteries. However, it is challenging, largely because sorting batteries is costly and hardly automated to group batteries based on battery types. In this paper, we introduce a machine learning-based approach for battery-type classification and address the daunting problem of data scarcity for the application. We propose BatSort which applies transfer learning to utilize the existing knowledge optimized with large-scale datasets and customizes ResNet to be specialized for classifying battery types. We collected our in-house battery-type dataset of small-scale to guide the knowledge transfer as a case study and evaluate the system performance. We conducted an experimental study and the results show that BatSort can achieve outstanding accuracy of 92.1% on average and up to 96.2% and the performance is stable for battery-type classification. Our solution helps realize fast and automated battery sorting with minimized cost and can be transferred to related industry applications with insufficient data.","sentences":["Battery recycling is a critical process for minimizing environmental harm and resource waste for used batteries.","However, it is challenging, largely because sorting batteries is costly and hardly automated to group batteries based on battery types.","In this paper, we introduce a machine learning-based approach for battery-type classification and address the daunting problem of data scarcity for the application.","We propose BatSort which applies transfer learning to utilize the existing knowledge optimized with large-scale datasets and customizes ResNet to be specialized for classifying battery types.","We collected our in-house battery-type dataset of small-scale to guide the knowledge transfer as a case study and evaluate the system performance.","We conducted an experimental study and the results show that BatSort can achieve outstanding accuracy of 92.1% on average and up to 96.2% and the performance is stable for battery-type classification.","Our solution helps realize fast and automated battery sorting with minimized cost and can be transferred to related industry applications with insufficient data."],"url":"http://arxiv.org/abs/2404.05802v1","category":"cs.CE"}
{"created":"2024-04-08 17:56:43","title":"Language-Independent Representations Improve Zero-Shot Summarization","abstract":"Finetuning pretrained models on downstream generation tasks often leads to catastrophic forgetting in zero-shot conditions. In this work, we focus on summarization and tackle the problem through the lens of language-independent representations. After training on monolingual summarization, we perform zero-shot transfer to new languages or language pairs. We first show naively finetuned models are highly language-specific in both output behavior and internal representations, resulting in poor zero-shot performance. Next, we propose query-key (QK) finetuning to decouple task-specific knowledge from the pretrained language generation abilities. Then, after showing downsides of the standard adversarial language classifier, we propose a balanced variant that more directly enforces language-agnostic representations. Moreover, our qualitative analyses show removing source language identity correlates to zero-shot summarization performance. Our code is openly available.","sentences":["Finetuning pretrained models on downstream generation tasks often leads to catastrophic forgetting in zero-shot conditions.","In this work, we focus on summarization and tackle the problem through the lens of language-independent representations.","After training on monolingual summarization, we perform zero-shot transfer to new languages or language pairs.","We first show naively finetuned models are highly language-specific in both output behavior and internal representations, resulting in poor zero-shot performance.","Next, we propose query-key (QK) finetuning to decouple task-specific knowledge from the pretrained language generation abilities.","Then, after showing downsides of the standard adversarial language classifier, we propose a balanced variant that more directly enforces language-agnostic representations.","Moreover, our qualitative analyses show removing source language identity correlates to zero-shot summarization performance.","Our code is openly available."],"url":"http://arxiv.org/abs/2404.05720v1","category":"cs.CL"}
{"created":"2024-04-08 17:53:21","title":"Responsible Generative AI: What to Generate and What Not","abstract":"In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains. However, ensuring the responsible generation of content by these models is crucial for their real-world applicability. This raises an interesting question: \\textit{What should responsible GenAI generate, and what should it not?} To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable. Specifically, we review recent advancements and challenges in addressing these requirements. Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains. Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI.","sentences":["In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains.","However, ensuring the responsible generation of content by these models is crucial for their real-world applicability.","This raises an interesting question: \\textit{What should responsible GenAI generate, and what should it not?} To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable.","Specifically, we review recent advancements and challenges in addressing these requirements.","Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains.","Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI."],"url":"http://arxiv.org/abs/2404.05783v1","category":"cs.CY"}
{"created":"2024-04-08 17:52:29","title":"SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing","abstract":"Effective editing of personal content holds a pivotal role in enabling individuals to express their creativity, weaving captivating narratives within their visual stories, and elevate the overall quality and impact of their visual content. Therefore, in this work, we introduce SwapAnything, a novel framework that can swap any objects in an image with personalized concepts given by the reference, while keeping the context unchanged. Compared with existing methods for personalized subject swapping, SwapAnything has three unique advantages: (1) precise control of arbitrary objects and parts rather than the main subject, (2) more faithful preservation of context pixels, (3) better adaptation of the personalized concept to the image. First, we propose targeted variable swapping to apply region control over latent feature maps and swap masked variables for faithful context preservation and initial semantic concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt the semantic concept into the original image in terms of target location, shape, style, and content during the image generation process. Extensive results on both human and automatic evaluation demonstrate significant improvements of our approach over baseline methods on personalized swapping. Furthermore, SwapAnything shows its precise and faithful swapping abilities across single object, multiple objects, partial object, and cross-domain swapping tasks. SwapAnything also achieves great performance on text-based swapping and tasks beyond swapping such as object insertion.","sentences":["Effective editing of personal content holds a pivotal role in enabling individuals to express their creativity, weaving captivating narratives within their visual stories, and elevate the overall quality and impact of their visual content.","Therefore, in this work, we introduce SwapAnything, a novel framework that can swap any objects in an image with personalized concepts given by the reference, while keeping the context unchanged.","Compared with existing methods for personalized subject swapping, SwapAnything has three unique advantages: (1) precise control of arbitrary objects and parts rather than the main subject, (2) more faithful preservation of context pixels, (3) better adaptation of the personalized concept to the image.","First, we propose targeted variable swapping to apply region control over latent feature maps and swap masked variables for faithful context preservation and initial semantic concept swapping.","Then, we introduce appearance adaptation, to seamlessly adapt the semantic concept into the original image in terms of target location, shape, style, and content during the image generation process.","Extensive results on both human and automatic evaluation demonstrate significant improvements of our approach over baseline methods on personalized swapping.","Furthermore, SwapAnything shows its precise and faithful swapping abilities across single object, multiple objects, partial object, and cross-domain swapping tasks.","SwapAnything also achieves great performance on text-based swapping and tasks beyond swapping such as object insertion."],"url":"http://arxiv.org/abs/2404.05717v1","category":"cs.CV"}
{"created":"2024-04-08 17:42:08","title":"Learning 3D-Aware GANs from Unposed Images with Template Feature Field","abstract":"Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice. This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF). Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field. Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data. Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives.","sentences":["Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice.","This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF).","Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field.","Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data.","Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives."],"url":"http://arxiv.org/abs/2404.05705v1","category":"cs.CV"}
{"created":"2024-04-08 17:26:28","title":"Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer","abstract":"Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/.","sentences":["Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment.","Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies.","This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer.","The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/."],"url":"http://arxiv.org/abs/2404.05695v1","category":"cs.RO"}
{"created":"2024-04-08 17:24:04","title":"Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding","abstract":"Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common. This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data. We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data. The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering. Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts. We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch. Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks.","sentences":["Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa.","While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common.","This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data.","We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data.","The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering.","Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts.","We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch.","Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks."],"url":"http://arxiv.org/abs/2404.05694v1","category":"cs.CL"}
{"created":"2024-04-08 17:15:37","title":"Automated discovery of symbolic laws governing skill acquisition from naturally occurring data","abstract":"Skill acquisition is a key area of research in cognitive psychology as it encompasses multiple psychological processes. The laws discovered under experimental paradigms are controversial and lack generalizability. This paper aims to unearth the laws of skill learning from large-scale training log data. A two-stage algorithm was developed to tackle the issues of unobservable cognitive states and algorithmic explosion in searching. Initially a deep learning model is employed to determine the learner's cognitive state and assess the feature importance. Subsequently, symbolic regression algorithms are utilized to parse the neural network model into algebraic equations. The experimental results of simulated data demonstrate that the proposed algorithm can accurately restore various preset laws within a certain range of noise, in continues feedback setting. Application of proposed method to Lumosity training data demonstrates superior performance compared to traditional and latest models in terms of fitness. The results indicate the discovery of two new forms of skill acquisition laws, while some previous findings have been reaffirmed.","sentences":["Skill acquisition is a key area of research in cognitive psychology as it encompasses multiple psychological processes.","The laws discovered under experimental paradigms are controversial and lack generalizability.","This paper aims to unearth the laws of skill learning from large-scale training log data.","A two-stage algorithm was developed to tackle the issues of unobservable cognitive states and algorithmic explosion in searching.","Initially a deep learning model is employed to determine the learner's cognitive state and assess the feature importance.","Subsequently, symbolic regression algorithms are utilized to parse the neural network model into algebraic equations.","The experimental results of simulated data demonstrate that the proposed algorithm can accurately restore various preset laws within a certain range of noise, in continues feedback setting.","Application of proposed method to Lumosity training data demonstrates superior performance compared to traditional and latest models in terms of fitness.","The results indicate the discovery of two new forms of skill acquisition laws, while some previous findings have been reaffirmed."],"url":"http://arxiv.org/abs/2404.05689v1","category":"cs.LG"}
{"created":"2024-04-08 17:14:32","title":"David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge","abstract":"ML is shifting from the cloud to the edge. Edge computing reduces the surface exposing private data and enables reliable throughput guarantees in real-time applications. Of the panoply of devices deployed at the edge, resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of magnitude cheaper, and less power-hungry than application processors or GPUs. Thus, enabling intelligence at the deep edge is the zeitgeist, with researchers focusing on unveiling novel approaches to deploy ANNs on these constrained devices. Quantization is a well-established technique that has proved effective in enabling the deployment of neural networks on MCUs; however, it is still an open question to understand the robustness of QNNs in the face of adversarial examples.   To fill this gap, we empirically evaluate the effectiveness of attacks and defenses from (full-precision) ANNs on (constrained) QNNs. Our evaluation includes three QNNs targeting TinyML applications, ten attacks, and six defenses. With this study, we draw a set of interesting findings. First, quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish. Second, quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment. Regarding adversarial defenses, we conclude that input pre-processing defenses show impressive results on small perturbations; however, they fall short as the perturbation increases. At the same time, train-based defenses increase the average point distance to the decision boundary, which holds after quantization. However, we argue that train-based defenses still need to smooth the quantization-shift and gradient misalignment phenomenons to counteract adversarial example transferability to QNNs. All artifacts are open-sourced to enable independent validation of results.","sentences":["ML is shifting from the cloud to the edge.","Edge computing reduces the surface exposing private data and enables reliable throughput guarantees in real-time applications.","Of the panoply of devices deployed at the edge, resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of magnitude cheaper, and less power-hungry than application processors or GPUs.","Thus, enabling intelligence at the deep edge is the zeitgeist, with researchers focusing on unveiling novel approaches to deploy ANNs on these constrained devices.","Quantization is a well-established technique that has proved effective in enabling the deployment of neural networks on MCUs; however, it is still an open question to understand the robustness of QNNs in the face of adversarial examples.   ","To fill this gap, we empirically evaluate the effectiveness of attacks and defenses from (full-precision) ANNs on (constrained) QNNs.","Our evaluation includes three QNNs targeting TinyML applications, ten attacks, and six defenses.","With this study, we draw a set of interesting findings.","First, quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish.","Second, quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment.","Regarding adversarial defenses, we conclude that input pre-processing defenses show impressive results on small perturbations; however, they fall short as the perturbation increases.","At the same time, train-based defenses increase the average point distance to the decision boundary, which holds after quantization.","However, we argue that train-based defenses still need to smooth the quantization-shift and gradient misalignment phenomenons to counteract adversarial example transferability to QNNs.","All artifacts are open-sourced to enable independent validation of results."],"url":"http://arxiv.org/abs/2404.05688v1","category":"cs.LG"}
{"created":"2024-04-08 16:52:21","title":"NAF-DPM: A Nonlinear Activation-Free Diffusion Probabilistic Model for Document Enhancement","abstract":"Real-world documents may suffer various forms of degradation, often resulting in lower accuracy in optical character recognition (OCR) systems. Therefore, a crucial preprocessing step is essential to eliminate noise while preserving text and key features of documents. In this paper, we propose NAF-DPM, a novel generative framework based on a diffusion probabilistic model (DPM) designed to restore the original quality of degraded documents. While DPMs are recognized for their high-quality generated images, they are also known for their large inference time. To mitigate this problem we provide the DPM with an efficient nonlinear activation-free (NAF) network and we employ as a sampler a fast solver of ordinary differential equations, which can converge in a few iterations. To better preserve text characters, we introduce an additional differentiable module based on convolutional recurrent neural networks, simulating the behavior of an OCR system during training. Experiments conducted on various datasets showcase the superiority of our approach, achieving state-of-the-art performance in terms of pixel-level and perceptual similarity metrics. Furthermore, the results demonstrate a notable character error reduction made by OCR systems when transcribing real-world document images enhanced by our framework. Code and pre-trained models are available at https://github.com/ispamm/NAF-DPM.","sentences":["Real-world documents may suffer various forms of degradation, often resulting in lower accuracy in optical character recognition (OCR) systems.","Therefore, a crucial preprocessing step is essential to eliminate noise while preserving text and key features of documents.","In this paper, we propose NAF-DPM, a novel generative framework based on a diffusion probabilistic model (DPM) designed to restore the original quality of degraded documents.","While DPMs are recognized for their high-quality generated images, they are also known for their large inference time.","To mitigate this problem we provide the DPM with an efficient nonlinear activation-free (NAF) network and we employ as a sampler a fast solver of ordinary differential equations, which can converge in a few iterations.","To better preserve text characters, we introduce an additional differentiable module based on convolutional recurrent neural networks, simulating the behavior of an OCR system during training.","Experiments conducted on various datasets showcase the superiority of our approach, achieving state-of-the-art performance in terms of pixel-level and perceptual similarity metrics.","Furthermore, the results demonstrate a notable character error reduction made by OCR systems when transcribing real-world document images enhanced by our framework.","Code and pre-trained models are available at https://github.com/ispamm/NAF-DPM."],"url":"http://arxiv.org/abs/2404.05669v1","category":"cs.CV"}
{"created":"2024-04-08 16:43:52","title":"VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain","abstract":"Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available here: https://github.com/leduckhai/MultiMed.","sentences":["Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain.","In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech.","To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents.","VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration.","Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country.","Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR.","Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%).","All code, data and models are made publicly available here: https://github.com/leduckhai/MultiMed."],"url":"http://arxiv.org/abs/2404.05659v1","category":"cs.CL"}
{"created":"2024-04-08 16:39:34","title":"Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid Framework","abstract":"Industry-wide nuclear power plant operating experience is a critical source of raw data for performing parameter estimations in reliability and risk models. Much operating experience information pertains to failure events and is stored as reports containing unstructured data, such as narratives. Event reports are essential for understanding how failures are initiated and propagated, including the numerous causal relations involved. Causal relation extraction using deep learning represents a significant frontier in the field of natural language processing (NLP), and is crucial since it enables the interpretation of intricate narratives and connections contained within vast amounts of written information. This paper proposed a hybrid framework for causality detection and extraction from nuclear licensee event reports. The main contributions include: (1) we compiled an LER corpus with 20,129 text samples for causality analysis, (2) developed an interactive tool for labeling cause effect pairs, (3) built a deep-learning-based approach for causal relation detection, and (4) developed a knowledge based cause-effect extraction approach.","sentences":["Industry-wide nuclear power plant operating experience is a critical source of raw data for performing parameter estimations in reliability and risk models.","Much operating experience information pertains to failure events and is stored as reports containing unstructured data, such as narratives.","Event reports are essential for understanding how failures are initiated and propagated, including the numerous causal relations involved.","Causal relation extraction using deep learning represents a significant frontier in the field of natural language processing (NLP), and is crucial since it enables the interpretation of intricate narratives and connections contained within vast amounts of written information.","This paper proposed a hybrid framework for causality detection and extraction from nuclear licensee event reports.","The main contributions include: (1) we compiled an LER corpus with 20,129 text samples for causality analysis, (2) developed an interactive tool for labeling cause effect pairs, (3) built a deep-learning-based approach for causal relation detection, and (4) developed a knowledge based cause-effect extraction approach."],"url":"http://arxiv.org/abs/2404.05656v1","category":"cs.CL"}
{"created":"2024-04-08 16:34:35","title":"Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model","abstract":"Human brains image complicated scenes when reading a novel. Replicating this imagination is one of the ultimate goals of AI-Generated Content (AIGC). However, current AIGC methods, such as score-based diffusion, are still deficient in terms of rapidity and efficiency. This deficiency is rooted in the difference between the brain and digital computers. Digital computers have physically separated storage and processing units, resulting in frequent data transfers during iterative calculations, incurring large time and energy overheads. This issue is further intensified by the conversion of inherently continuous and analog generation dynamics, which can be formulated by neural differential equations, into discrete and digital operations. Inspired by the brain, we propose a time-continuous and analog in-memory neural differential equation solver for score-based diffusion, employing emerging resistive memory. The integration of storage and computation within resistive memory synapses surmount the von Neumann bottleneck, benefiting the generative speed and energy efficiency. The closed-loop feedback integrator is time-continuous, analog, and compact, physically implementing an infinite-depth neural network. Moreover, the software-hardware co-design is intrinsically robust to analog noise. We experimentally validate our solution with 180 nm resistive memory in-memory computing macros. Demonstrating equivalent generative quality to the software baseline, our system achieved remarkable enhancements in generative speed for both unconditional and conditional generation tasks, by factors of 64.8 and 156.5, respectively. Moreover, it accomplished reductions in energy consumption by factors of 5.2 and 4.1. Our approach heralds a new horizon for hardware solutions in edge computing for generative AI applications.","sentences":["Human brains image complicated scenes when reading a novel.","Replicating this imagination is one of the ultimate goals of AI-Generated Content (AIGC).","However, current AIGC methods, such as score-based diffusion, are still deficient in terms of rapidity and efficiency.","This deficiency is rooted in the difference between the brain and digital computers.","Digital computers have physically separated storage and processing units, resulting in frequent data transfers during iterative calculations, incurring large time and energy overheads.","This issue is further intensified by the conversion of inherently continuous and analog generation dynamics, which can be formulated by neural differential equations, into discrete and digital operations.","Inspired by the brain, we propose a time-continuous and analog in-memory neural differential equation solver for score-based diffusion, employing emerging resistive memory.","The integration of storage and computation within resistive memory synapses surmount the von Neumann bottleneck, benefiting the generative speed and energy efficiency.","The closed-loop feedback integrator is time-continuous, analog, and compact, physically implementing an infinite-depth neural network.","Moreover, the software-hardware co-design is intrinsically robust to analog noise.","We experimentally validate our solution with 180 nm resistive memory in-memory computing macros.","Demonstrating equivalent generative quality to the software baseline, our system achieved remarkable enhancements in generative speed for both unconditional and conditional generation tasks, by factors of 64.8 and 156.5, respectively.","Moreover, it accomplished reductions in energy consumption by factors of 5.2 and 4.1.","Our approach heralds a new horizon for hardware solutions in edge computing for generative AI applications."],"url":"http://arxiv.org/abs/2404.05648v1","category":"cs.AR"}
{"created":"2024-04-08 16:21:22","title":"3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules","abstract":"We introduce 3D-COCO, an extension of the original MS-COCO dataset providing 3D models and 2D-3D alignment annotations. 3D-COCO was designed to achieve computer vision tasks such as 3D reconstruction or image detection configurable with textual, 2D image, and 3D CAD model queries. We complete the existing MS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. By using an IoU-based method, we match each MS-COCO annotation with the best 3D models to provide a 2D-3D alignment. The open-source nature of 3D-COCO is a premiere that should pave the way for new research on 3D-related topics. The dataset and its source codes is available at https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/","sentences":["We introduce 3D-COCO, an extension of the original MS-COCO dataset providing 3D models and 2D-3D alignment annotations.","3D-COCO was designed to achieve computer vision tasks such as 3D reconstruction or image detection configurable with textual, 2D image, and 3D CAD model queries.","We complete the existing MS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse.","By using an IoU-based method, we match each MS-COCO annotation with the best 3D models to provide a 2D-3D alignment.","The open-source nature of 3D-COCO is a premiere that should pave the way for new research on 3D-related topics.","The dataset and its source codes is available at https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/"],"url":"http://arxiv.org/abs/2404.05641v1","category":"cs.CV"}
{"created":"2024-04-08 16:20:15","title":"Investigating the Impact of Quantization on Adversarial Robustness","abstract":"Quantization is a promising technique for reducing the bit-width of deep models to improve their runtime performance and storage efficiency, and thus becomes a fundamental step for deployment. In real-world scenarios, quantized models are often faced with adversarial attacks which cause the model to make incorrect inferences by introducing slight perturbations. However, recent studies have paid less attention to the impact of quantization on the model robustness. More surprisingly, existing studies on this topic even present inconsistent conclusions, which prompted our in-depth investigation. In this paper, we conduct a first-time analysis of the impact of the quantization pipeline components that can incorporate robust optimization under the settings of Post-Training Quantization and Quantization-Aware Training. Through our detailed analysis, we discovered that this inconsistency arises from the use of different pipelines in different studies, specifically regarding whether robust optimization is performed and at which quantization stage it occurs. Our research findings contribute insights into deploying more secure and robust quantized networks, assisting practitioners in reference for scenarios with high-security requirements and limited resources.","sentences":["Quantization is a promising technique for reducing the bit-width of deep models to improve their runtime performance and storage efficiency, and thus becomes a fundamental step for deployment.","In real-world scenarios, quantized models are often faced with adversarial attacks which cause the model to make incorrect inferences by introducing slight perturbations.","However, recent studies have paid less attention to the impact of quantization on the model robustness.","More surprisingly, existing studies on this topic even present inconsistent conclusions, which prompted our in-depth investigation.","In this paper, we conduct a first-time analysis of the impact of the quantization pipeline components that can incorporate robust optimization under the settings of Post-Training Quantization and Quantization-Aware Training.","Through our detailed analysis, we discovered that this inconsistency arises from the use of different pipelines in different studies, specifically regarding whether robust optimization is performed and at which quantization stage it occurs.","Our research findings contribute insights into deploying more secure and robust quantized networks, assisting practitioners in reference for scenarios with high-security requirements and limited resources."],"url":"http://arxiv.org/abs/2404.05639v1","category":"cs.LG"}
{"created":"2024-04-08 15:54:02","title":"LTNER: Large Language Model Tagging for Named Entity Recognition with Contextualized Entity Marking","abstract":"The use of LLMs for natural language processing has become a popular trend in the past two years, driven by their formidable capacity for context comprehension and learning, which has inspired a wave of research from academics and industry professionals. However, for certain NLP tasks, such as NER, the performance of LLMs still falls short when compared to supervised learning methods. In our research, we developed a NER processing framework called LTNER that incorporates a revolutionary Contextualized Entity Marking Gen Method. By leveraging the cost-effective GPT-3.5 coupled with context learning that does not require additional training, we significantly improved the accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset increased from the initial 85.9% to 91.9%, approaching the performance of supervised fine-tuning. This outcome has led to a deeper understanding of the potential of LLMs.","sentences":["The use of LLMs for natural language processing has become a popular trend in the past two years, driven by their formidable capacity for context comprehension and learning, which has inspired a wave of research from academics and industry professionals.","However, for certain NLP tasks, such as NER, the performance of LLMs still falls short when compared to supervised learning methods.","In our research, we developed a NER processing framework called LTNER that incorporates a revolutionary Contextualized Entity Marking Gen Method.","By leveraging the cost-effective GPT-3.5 coupled with context learning that does not require additional training, we significantly improved the accuracy of LLMs in handling NER tasks.","The F1 score on the CoNLL03 dataset increased from the initial 85.9% to 91.9%, approaching the performance of supervised fine-tuning.","This outcome has led to a deeper understanding of the potential of LLMs."],"url":"http://arxiv.org/abs/2404.05624v1","category":"cs.CL"}
{"created":"2024-04-08 15:40:22","title":"Deep Representation Learning for Multi-functional Degradation Modeling of Community-dwelling Aging Population","abstract":"As the aging population grows, particularly for the baby boomer generation, the United States is witnessing a significant increase in the elderly population experiencing multifunctional disabilities. These disabilities, stemming from a variety of chronic diseases, injuries, and impairments, present a complex challenge due to their multidimensional nature, encompassing both physical and cognitive aspects. Traditional methods often use univariate regression-based methods to model and predict single degradation conditions and assume population homogeneity, which is inadequate to address the complexity and diversity of aging-related degradation. This study introduces a novel framework for multi-functional degradation modeling that captures the multidimensional (e.g., physical and cognitive) and heterogeneous nature of elderly disabilities. Utilizing deep learning, our approach predicts health degradation scores and uncovers latent heterogeneity from elderly health histories, offering both efficient estimation and explainable insights into the diverse effects and causes of aging-related degradation. A real-case study demonstrates the effectiveness and marks a pivotal contribution to accurately modeling the intricate dynamics of elderly degradation, and addresses the healthcare challenges in the aging population.","sentences":["As the aging population grows, particularly for the baby boomer generation, the United States is witnessing a significant increase in the elderly population experiencing multifunctional disabilities.","These disabilities, stemming from a variety of chronic diseases, injuries, and impairments, present a complex challenge due to their multidimensional nature, encompassing both physical and cognitive aspects.","Traditional methods often use univariate regression-based methods to model and predict single degradation conditions and assume population homogeneity, which is inadequate to address the complexity and diversity of aging-related degradation.","This study introduces a novel framework for multi-functional degradation modeling that captures the multidimensional (e.g., physical and cognitive) and heterogeneous nature of elderly disabilities.","Utilizing deep learning, our approach predicts health degradation scores and uncovers latent heterogeneity from elderly health histories, offering both efficient estimation and explainable insights into the diverse effects and causes of aging-related degradation.","A real-case study demonstrates the effectiveness and marks a pivotal contribution to accurately modeling the intricate dynamics of elderly degradation, and addresses the healthcare challenges in the aging population."],"url":"http://arxiv.org/abs/2404.05613v1","category":"cs.LG"}
{"created":"2024-04-08 15:25:25","title":"Graph Neural Networks Automated Design and Deployment on Device-Edge Co-Inference Systems","abstract":"The key to device-edge co-inference paradigm is to partition models into computation-friendly and computation-intensive parts across the device and the edge, respectively. However, for Graph Neural Networks (GNNs), we find that simply partitioning without altering their structures can hardly achieve the full potential of the co-inference paradigm due to various computational-communication overheads of GNN operations over heterogeneous devices. We present GCoDE, the first automatic framework for GNN that innovatively Co-designs the architecture search and the mapping of each operation on Device-Edge hierarchies. GCoDE abstracts the device communication process into an explicit operation and fuses the search of architecture and the operations mapping in a unified space for joint-optimization. Also, the performance-awareness approach, utilized in the constraint-based search process of GCoDE, enables effective evaluation of architecture efficiency in diverse heterogeneous systems. We implement the co-inference engine and runtime dispatcher in GCoDE to enhance the deployment efficiency. Experimental results show that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy reduction compared to existing approaches across various applications and system configurations.","sentences":["The key to device-edge co-inference paradigm is to partition models into computation-friendly and computation-intensive parts across the device and the edge, respectively.","However, for Graph Neural Networks (GNNs), we find that simply partitioning without altering their structures can hardly achieve the full potential of the co-inference paradigm due to various computational-communication overheads of GNN operations over heterogeneous devices.","We present GCoDE, the first automatic framework for GNN that innovatively Co-designs the architecture search and the mapping of each operation on Device-Edge hierarchies.","GCoDE abstracts the device communication process into an explicit operation and fuses the search of architecture and the operations mapping in a unified space for joint-optimization.","Also, the performance-awareness approach, utilized in the constraint-based search process of GCoDE, enables effective evaluation of architecture efficiency in diverse heterogeneous systems.","We implement the co-inference engine and runtime dispatcher in GCoDE to enhance the deployment efficiency.","Experimental results show that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy reduction compared to existing approaches across various applications and system configurations."],"url":"http://arxiv.org/abs/2404.05605v1","category":"cs.LG"}
{"created":"2024-04-08 15:22:38","title":"Self-Explainable Affordance Learning with Embodied Caption","abstract":"In the field of visual affordance learning, previous methods mainly used abundant images or videos that delineate human behavior patterns to identify action possibility regions for object manipulation, with a variety of applications in robotic tasks. However, they encounter a main challenge of action ambiguity, illustrated by the vagueness like whether to beat or carry a drum, and the complexities involved in processing intricate scenes. Moreover, it is important for human intervention to rectify robot errors in time. To address these issues, we introduce Self-Explainable Affordance learning (SEA) with embodied caption. This innovation enables robots to articulate their intentions and bridge the gap between explainable vision-language caption and visual affordance learning. Due to a lack of appropriate dataset, we unveil a pioneering dataset and metrics tailored for this task, which integrates images, heatmaps, and embodied captions. Furthermore, we propose a novel model to effectively combine affordance grounding with self-explanation in a simple but efficient manner. Extensive quantitative and qualitative experiments demonstrate our method's effectiveness.","sentences":["In the field of visual affordance learning, previous methods mainly used abundant images or videos that delineate human behavior patterns to identify action possibility regions for object manipulation, with a variety of applications in robotic tasks.","However, they encounter a main challenge of action ambiguity, illustrated by the vagueness like whether to beat or carry a drum, and the complexities involved in processing intricate scenes.","Moreover, it is important for human intervention to rectify robot errors in time.","To address these issues, we introduce Self-Explainable Affordance learning (SEA) with embodied caption.","This innovation enables robots to articulate their intentions and bridge the gap between explainable vision-language caption and visual affordance learning.","Due to a lack of appropriate dataset, we unveil a pioneering dataset and metrics tailored for this task, which integrates images, heatmaps, and embodied captions.","Furthermore, we propose a novel model to effectively combine affordance grounding with self-explanation in a simple but efficient manner.","Extensive quantitative and qualitative experiments demonstrate our method's effectiveness."],"url":"http://arxiv.org/abs/2404.05603v1","category":"cs.CV"}
{"created":"2024-04-08 15:21:34","title":"Towards an understanding of particle-scale flaws and microstructure evolution in cold-spray via accumulation of single particle impacts","abstract":"Cold spray coatings are the sum of countless individual bonding events between single particles impacting on top of one another at high velocities. Thus, the collective behavior of microparticles must be considered to elucidate the origins of coating flaws at the scale of the particles and larger, or the dynamic evolution of the overall coating microstructure. Laser-induced particle impact testing (LIPIT) has been extensively used to study single-particle impacts, and in this work is adapted to study the accumulation of numerous particles with knowledge of each individual particle's impact parameters (particle size, velocity). The method reproducibly deposits stacks of gold particles (>20 particles) with different characteristic spectra of impact velocity. The quantitative single-particle data are analyzed in a correlative manner to the structure and flaws in the resulting stacks, providing some first semi-quantitative connections between, e.g., strain and recrystallization, or aberrant particle characteristics and defects. The results highlight opportunities for the study of many-particle phenomena in microparticle impact--from interaction of particles in cold spray to multi-step erosion processes--with a quantitative view of the behavior of single particles.","sentences":["Cold spray coatings are the sum of countless individual bonding events between single particles impacting on top of one another at high velocities.","Thus, the collective behavior of microparticles must be considered to elucidate the origins of coating flaws at the scale of the particles and larger, or the dynamic evolution of the overall coating microstructure.","Laser-induced particle impact testing (LIPIT) has been extensively used to study single-particle impacts, and in this work is adapted to study the accumulation of numerous particles with knowledge of each individual particle's impact parameters (particle size, velocity).","The method reproducibly deposits stacks of gold particles (>20 particles) with different characteristic spectra of impact velocity.","The quantitative single-particle data are analyzed in a correlative manner to the structure and flaws in the resulting stacks, providing some first semi-quantitative connections between, e.g., strain and recrystallization, or aberrant particle characteristics and defects.","The results highlight opportunities for the study of many-particle phenomena in microparticle impact--from interaction of particles in cold spray to multi-step erosion processes--with a quantitative view of the behavior of single particles."],"url":"http://arxiv.org/abs/2404.05601v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 15:19:57","title":"Data Readiness for AI: A 360-Degree Survey","abstract":"Data are the critical fuel for Artificial Intelligence (AI) models. Poor quality data produces inaccurate and ineffective AI models that may lead to incorrect or unsafe use. Checking for data readiness is a crucial step in improving data quality. Numerous R&D efforts have been spent on improving data quality. However, standardized metrics for evaluating data readiness for use in AI training are still evolving. In this study, we perform a comprehensive survey of metrics used for verifying AI's data readiness. This survey examines more than 120 papers that are published by ACM Digital Library, IEEE Xplore, other reputable journals, and articles published on the web by prominent AI experts. This survey aims to propose a taxonomy of data readiness for AI (DRAI) metrics for structured and unstructured datasets. We anticipate that this taxonomy can lead to new standards for DRAI metrics that would be used for enhancing the quality and accuracy of AI training and inference.","sentences":["Data are the critical fuel for Artificial Intelligence (AI) models.","Poor quality data produces inaccurate and ineffective AI models that may lead to incorrect or unsafe use.","Checking for data readiness is a crucial step in improving data quality.","Numerous R&D efforts have been spent on improving data quality.","However, standardized metrics for evaluating data readiness for use in AI training are still evolving.","In this study, we perform a comprehensive survey of metrics used for verifying AI's data readiness.","This survey examines more than 120 papers that are published by ACM Digital Library, IEEE Xplore, other reputable journals, and articles published on the web by prominent AI experts.","This survey aims to propose a taxonomy of data readiness for AI (DRAI) metrics for structured and unstructured datasets.","We anticipate that this taxonomy can lead to new standards for DRAI metrics that would be used for enhancing the quality and accuracy of AI training and inference."],"url":"http://arxiv.org/abs/2404.05779v1","category":"cs.LG"}
{"created":"2024-04-08 15:03:57","title":"MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering","abstract":"Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support, which has been demonstrated by their competitive performances in Medical QA. However, while impressive, the required quality bar for medical applications remains far from being achieved. Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content. Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions. Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic. In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering. To the best of our knowledge, MedExpQA includes for the first time reference gold explanations written by medical doctors which can be leveraged to establish various gold-based upper-bounds for comparison with LLMs performance. Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs still has large room for improvement, especially for languages other than English. Furthermore, and despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering. So far the benchmark is available in four languages, but we hope that this work may encourage further development to other languages.","sentences":["Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support, which has been demonstrated by their competitive performances in Medical QA.","However, while impressive, the required quality bar for medical applications remains far from being achieved.","Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content.","Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions.","Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic.","In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering.","To the best of our knowledge, MedExpQA includes for the first time reference gold explanations written by medical doctors which can be leveraged to establish various gold-based upper-bounds for comparison with LLMs performance.","Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs still has large room for improvement, especially for languages other than English.","Furthermore, and despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering.","So far the benchmark is available in four languages, but we hope that this work may encourage further development to other languages."],"url":"http://arxiv.org/abs/2404.05590v1","category":"cs.CL"}
{"created":"2024-04-08 14:43:13","title":"360\u00b0REA: Towards A Reusable Experience Accumulation with 360\u00b0 Assessment for Multi-Agent System","abstract":"Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA.","sentences":["Large language model agents have demonstrated remarkable advancements across various complex tasks.","Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks.","Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents.","We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance.","In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices.","The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment.","To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment.","Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA."],"url":"http://arxiv.org/abs/2404.05569v1","category":"cs.AI"}
{"created":"2024-04-08 14:39:49","title":"Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models","abstract":"Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4$\\times$ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally require 2-4$\\times$ times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation. In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference. Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters). Performance tests using vLLM show that our DS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like Mistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.","sentences":["Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4$\\times$ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios.","However, MoE models generally require 2-4$\\times$ times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation.","In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference.","Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters).","Performance tests using vLLM show that our DS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like Mistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B."],"url":"http://arxiv.org/abs/2404.05567v1","category":"cs.LG"}
{"created":"2024-04-08 14:38:12","title":"Database-Driven Mathematical Inquiry","abstract":"Recent advances in computing have changed not only the nature of mathematical computation, but mathematical proof and inquiry itself. While artificial intelligence and formalized mathematics have been the major topics of this conversation, this paper explores another class of tools for advancing mathematics research: databases of mathematical objects that enable semantic search. In addition to defining and exploring examples of these tools, we illustrate a particular line of research that was inspired and enabled by one such database.","sentences":["Recent advances in computing have changed not only the nature of mathematical computation, but mathematical proof and inquiry itself.","While artificial intelligence and formalized mathematics have been the major topics of this conversation, this paper explores another class of tools for advancing mathematics research: databases of mathematical objects that enable semantic search.","In addition to defining and exploring examples of these tools, we illustrate a particular line of research that was inspired and enabled by one such database."],"url":"http://arxiv.org/abs/2404.05778v1","category":"cs.DB"}
{"created":"2024-04-08 14:37:26","title":"Optimal Flow Admission Control in Edge Computing via Safe Reinforcement Learning","abstract":"With the uptake of intelligent data-driven applications, edge computing infrastructures necessitate a new generation of admission control algorithms to maximize system performance under limited and highly heterogeneous resources. In this paper, we study how to optimally select information flows which belong to different classes and dispatch them to multiple edge servers where deployed applications perform flow analytic tasks. The optimal policy is obtained via constrained Markov decision process (CMDP) theory accounting for the demand of each edge application for specific classes of flows, the constraints on computing capacity of edge servers and of the access network.   We develop DR-CPO, a specialized primal-dual Safe Reinforcement Learning (SRL) method which solves the resulting optimal admission control problem by reward decomposition. DR-CPO operates optimal decentralized control and mitigates effectively state-space explosion while preserving optimality. Compared to existing Deep Reinforcement Learning (DRL) solutions, extensive results show that DR-CPO achieves 15\\% higher reward on a wide variety of environments, while requiring on average only 50\\% of the amount of learning episodes to converge. Finally, we show how to match DR-CPO and load-balancing to dispatch optimally information streams to available edge servers and further improve system performance.","sentences":["With the uptake of intelligent data-driven applications, edge computing infrastructures necessitate a new generation of admission control algorithms to maximize system performance under limited and highly heterogeneous resources.","In this paper, we study how to optimally select information flows which belong to different classes and dispatch them to multiple edge servers where deployed applications perform flow analytic tasks.","The optimal policy is obtained via constrained Markov decision process (CMDP) theory accounting for the demand of each edge application for specific classes of flows, the constraints on computing capacity of edge servers and of the access network.   ","We develop DR-CPO, a specialized primal-dual Safe Reinforcement Learning (SRL) method which solves the resulting optimal admission control problem by reward decomposition.","DR-CPO operates optimal decentralized control and mitigates effectively state-space explosion while preserving optimality.","Compared to existing Deep Reinforcement Learning (DRL) solutions, extensive results show that DR-CPO achieves 15\\% higher reward on a wide variety of environments, while requiring on average only 50\\% of the amount of learning episodes to converge.","Finally, we show how to match DR-CPO and load-balancing to dispatch optimally information streams to available edge servers and further improve system performance."],"url":"http://arxiv.org/abs/2404.05564v1","category":"cs.NI"}
{"created":"2024-04-08 14:28:27","title":"On the Convergence of Continual Learning with Adaptive Methods","abstract":"One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks.","sentences":["One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma.","However, the convergence of continual learning for each sequential task is less studied so far.","In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.","We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.","The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration.","Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks."],"url":"http://arxiv.org/abs/2404.05555v1","category":"cs.LG"}
{"created":"2024-04-08 14:21:34","title":"Alljoined -- A dataset for EEG-to-Image decoding","abstract":"We present Alljoined, a dataset built specifically for EEG-to-Image decoding. Recognizing that an extensive and unbiased sampling of neural responses to visual stimuli is crucial for image reconstruction efforts, we collected data from 8 participants looking at 10,000 natural images each. We have currently gathered 46,080 epochs of brain responses recorded with a 64-channel EEG headset. The dataset combines response-based stimulus timing, repetition between blocks and sessions, and diverse image classes with the goal of improving signal quality. For transparency, we also provide data quality scores. We publicly release the dataset and all code at https://linktr.ee/alljoined1.","sentences":["We present Alljoined, a dataset built specifically for EEG-to-Image decoding.","Recognizing that an extensive and unbiased sampling of neural responses to visual stimuli is crucial for image reconstruction efforts, we collected data from 8 participants looking at 10,000 natural images each.","We have currently gathered 46,080 epochs of brain responses recorded with a 64-channel EEG headset.","The dataset combines response-based stimulus timing, repetition between blocks and sessions, and diverse image classes with the goal of improving signal quality.","For transparency, we also provide data quality scores.","We publicly release the dataset and all code at https://linktr.ee/alljoined1."],"url":"http://arxiv.org/abs/2404.05553v1","category":"q-bio.NC"}
{"created":"2024-04-08 14:15:56","title":"Evaluating Interventional Reasoning Capabilities of Large Language Models","abstract":"Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts.","sentences":["Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system.","As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial.","A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions.","Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention.","We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning.","These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts.","Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts."],"url":"http://arxiv.org/abs/2404.05545v1","category":"cs.LG"}
{"created":"2024-04-08 14:08:56","title":"OPSD: an Offensive Persian Social media Dataset and its baseline evaluations","abstract":"The proliferation of hate speech and offensive comments on social media has become increasingly prevalent due to user activities. Such comments can have detrimental effects on individuals' psychological well-being and social behavior. While numerous datasets in the English language exist in this domain, few equivalent resources are available for Persian language. To address this gap, this paper introduces two offensive datasets. The first dataset comprises annotations provided by domain experts, while the second consists of a large collection of unlabeled data obtained through web crawling for unsupervised learning purposes. To ensure the quality of the former dataset, a meticulous three-stage labeling process was conducted, and kappa measures were computed to assess inter-annotator agreement. Furthermore, experiments were performed on the dataset using state-of-the-art language models, both with and without employing masked language modeling techniques, as well as machine learning algorithms, in order to establish the baselines for the dataset using contemporary cutting-edge approaches. The obtained F1-scores for the three-class and two-class versions of the dataset were 76.9% and 89.9% for XLM-RoBERTa, respectively.","sentences":["The proliferation of hate speech and offensive comments on social media has become increasingly prevalent due to user activities.","Such comments can have detrimental effects on individuals' psychological well-being and social behavior.","While numerous datasets in the English language exist in this domain, few equivalent resources are available for Persian language.","To address this gap, this paper introduces two offensive datasets.","The first dataset comprises annotations provided by domain experts, while the second consists of a large collection of unlabeled data obtained through web crawling for unsupervised learning purposes.","To ensure the quality of the former dataset, a meticulous three-stage labeling process was conducted, and kappa measures were computed to assess inter-annotator agreement.","Furthermore, experiments were performed on the dataset using state-of-the-art language models, both with and without employing masked language modeling techniques, as well as machine learning algorithms, in order to establish the baselines for the dataset using contemporary cutting-edge approaches.","The obtained F1-scores for the three-class and two-class versions of the dataset were 76.9% and 89.9% for XLM-RoBERTa, respectively."],"url":"http://arxiv.org/abs/2404.05540v1","category":"cs.CL"}
{"created":"2024-04-08 14:00:50","title":"Ordre public exceptions for algorithmic surveillance patents","abstract":"This chapter explores the role of patent protection in algorithmic surveillance and whether ordre public exceptions from patentability should apply to such patents, due to their potential to enable human rights violations. It concludes that in most cases, it is undesirable to exclude algorithmic surveillance patents from patentability, as the patent system is ill-equipped to evaluate the impacts of the exploitation of such technologies. Furthermore, the disclosure of such patents has positive externalities from the societal perspective by opening the black box of surveillance for public scrutiny.","sentences":["This chapter explores the role of patent protection in algorithmic surveillance and whether ordre public exceptions from patentability should apply to such patents, due to their potential to enable human rights violations.","It concludes that in most cases, it is undesirable to exclude algorithmic surveillance patents from patentability, as the patent system is ill-equipped to evaluate the impacts of the exploitation of such technologies.","Furthermore, the disclosure of such patents has positive externalities from the societal perspective by opening the black box of surveillance for public scrutiny."],"url":"http://arxiv.org/abs/2404.05534v1","category":"cs.CY"}
{"created":"2024-04-08 13:59:02","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences.","RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used.","In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process.","We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets.","Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative).","The findings from our experiments also shed light on strategies to defend against the preference poisoning attack."],"url":"http://arxiv.org/abs/2404.05530v1","category":"cs.CL"}
{"created":"2024-04-08 13:40:26","title":"IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning for Diverse Workloads","abstract":"This study introduces the Instance-A}ware Index A}dvisor (IA2), a novel deep reinforcement learning (DRL)-based approach for optimizing index selection in databases facing large action spaces of potential candidates. IA2 introduces the Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference State-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index selection by understanding workload-index dependencies and employing adaptive action masking. This method includes a comprehensive workload model, enhancing its ability to adapt to unseen workloads and ensuring robust performance across diverse database environments. Evaluation on benchmarks such as TPC-H reveals IA2's suggested indexes' performance in enhancing runtime, securing a 40% reduction in runtime for complex TPC-H workloads compared to scenarios without indexes, and delivering a 20% improvement over existing state-of-the-art DRL-based index advisors.","sentences":["This study introduces the Instance-A}ware Index A}dvisor (IA2), a novel deep reinforcement learning (DRL)-based approach for optimizing index selection in databases facing large action spaces of potential candidates.","IA2 introduces the Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference State-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index selection by understanding workload-index dependencies and employing adaptive action masking.","This method includes a comprehensive workload model, enhancing its ability to adapt to unseen workloads and ensuring robust performance across diverse database environments.","Evaluation on benchmarks such as TPC-H reveals IA2's suggested indexes' performance in enhancing runtime, securing a 40% reduction in runtime for complex TPC-H workloads compared to scenarios without indexes, and delivering a 20% improvement over existing state-of-the-art DRL-based index advisors."],"url":"http://arxiv.org/abs/2404.05777v1","category":"cs.DB"}
{"created":"2024-04-08 13:35:48","title":"Efficient Distributed Data Structures for Future Many-core Architectures","abstract":"We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory. With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques. To achieve scalability, we study a generic scheme which makes all our implementations hierarchical. We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting. We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability. Our experiments also reveal the performance and scalability power of the hierarchical approach. We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures.","sentences":["We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory.","With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques.","To achieve scalability, we study a generic scheme which makes all our implementations hierarchical.","We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting.","We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability.","Our experiments also reveal the performance and scalability power of the hierarchical approach.","We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures."],"url":"http://arxiv.org/abs/2404.05515v1","category":"cs.DC"}
{"created":"2024-04-08 13:28:11","title":"Synergy of Large Language Model and Model Driven Engineering for Automated Development of Centralized Vehicular Systems","abstract":"We present a prototype of a tool leveraging the synergy of model driven engineering (MDE) and Large Language Models (LLM) for the purpose of software development process automation in the automotive industry. In this approach, the user-provided input is free form textual requirements, which are first translated to Ecore model instance representation using an LLM, which is afterwards checked for consistency using Object Constraint Language (OCL) rules. After successful consistency check, the model instance is fed as input to another LLM for the purpose of code generation. The generated code is evaluated in a simulated environment using CARLA simulator connected to an example centralized vehicle architecture, in an emergency brake scenario.","sentences":["We present a prototype of a tool leveraging the synergy of model driven engineering (MDE) and Large Language Models (LLM) for the purpose of software development process automation in the automotive industry.","In this approach, the user-provided input is free form textual requirements, which are first translated to Ecore model instance representation using an LLM, which is afterwards checked for consistency using Object Constraint Language (OCL) rules.","After successful consistency check, the model instance is fed as input to another LLM for the purpose of code generation.","The generated code is evaluated in a simulated environment using CARLA simulator connected to an example centralized vehicle architecture, in an emergency brake scenario."],"url":"http://arxiv.org/abs/2404.05508v1","category":"cs.SE"}
{"created":"2024-04-08 13:25:03","title":"PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations","abstract":"In this paper, we present our submission to the SemEval-2023 Task~3 \"The Competition of Multimodal Emotion Cause Analysis in Conversations\", focusing on extracting emotion-cause pairs from dialogs. Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes. We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264.","sentences":["In this paper, we present our submission to the SemEval-2023 Task~3 \"The Competition of Multimodal Emotion Cause Analysis in Conversations\", focusing on extracting emotion-cause pairs from dialogs.","Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes.","We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264."],"url":"http://arxiv.org/abs/2404.05502v1","category":"cs.CL"}
{"created":"2024-04-08 13:25:02","title":"Data Science In Olfaction","abstract":"Advances in neural sensing technology are making it possible to observe the olfactory process in great detail. In this paper, we conceptualize smell from a Data Science and AI perspective, that relates the properties of odorants to how they are sensed and analyzed in the olfactory system from the nose to the brain. Drawing distinctions to color vision, we argue that smell presents unique measurement challenges, including the complexity of stimuli, the high dimensionality of the sensory apparatus, as well as what constitutes ground truth. In the face of these challenges, we argue for the centrality of odorant-receptor interactions in developing a theory of olfaction. Such a theory is likely to find widespread industrial applications, and enhance our understanding of smell, and in the longer-term, how it relates to other senses and language. As an initial use case of the data, we present results using machine learning-based classification of neural responses to odors as they are recorded in the mouse olfactory bulb with calcium imaging.","sentences":["Advances in neural sensing technology are making it possible to observe the olfactory process in great detail.","In this paper, we conceptualize smell from a Data Science and AI perspective, that relates the properties of odorants to how they are sensed and analyzed in the olfactory system from the nose to the brain.","Drawing distinctions to color vision, we argue that smell presents unique measurement challenges, including the complexity of stimuli, the high dimensionality of the sensory apparatus, as well as what constitutes ground truth.","In the face of these challenges, we argue for the centrality of odorant-receptor interactions in developing a theory of olfaction.","Such a theory is likely to find widespread industrial applications, and enhance our understanding of smell, and in the longer-term, how it relates to other senses and language.","As an initial use case of the data, we present results using machine learning-based classification of neural responses to odors as they are recorded in the mouse olfactory bulb with calcium imaging."],"url":"http://arxiv.org/abs/2404.05501v1","category":"q-bio.NC"}
{"created":"2024-04-08 13:22:24","title":"Guiding Large Language Models to Generate Computer-Parsable Content","abstract":"We propose a method to guide Large Language Models (LLMs) in generating structured content adhering to specific conventions without fine-tuning. By utilizing coroutine-based content generation constraints through a pre-agreed context-free grammar (CFG), LLMs are directed during decoding to produce formal language compliant outputs. This enhances stability and consistency in generating target data structures, types, or instructions, reducing application development complexities. Experimentally, error rates of GPT-2 and Gemma exceed 95% for DSLs longer than 36 and 282 tokens, respectively. We introduce YieldLang, a coroutine-based DSL generation framework, and evaluate it with LLMs on various tasks including JSON and Mermaid flowchart generation. Compared to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs requiring only about 16.5% of the samples to generate JSON effectively. This enhances usability of LLM-generated content for computer programs.","sentences":["We propose a method to guide Large Language Models (LLMs) in generating structured content adhering to specific conventions without fine-tuning.","By utilizing coroutine-based content generation constraints through a pre-agreed context-free grammar (CFG), LLMs are directed during decoding to produce formal language compliant outputs.","This enhances stability and consistency in generating target data structures, types, or instructions, reducing application development complexities.","Experimentally, error rates of GPT-2 and Gemma exceed 95% for DSLs longer than 36 and 282 tokens, respectively.","We introduce YieldLang, a coroutine-based DSL generation framework, and evaluate it with LLMs on various tasks including JSON and Mermaid flowchart generation.","Compared to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs requiring only about 16.5% of the samples to generate JSON effectively.","This enhances usability of LLM-generated content for computer programs."],"url":"http://arxiv.org/abs/2404.05499v2","category":"cs.SE"}
{"created":"2024-04-08 13:21:25","title":"Characterising the Higgs boson with ATLAS data from Run 2 of the LHC","abstract":"The Higgs boson was discovered by the ATLAS and CMS Collaborations in 2012 using data from Run 1 of the Large Hadron Collider (2010$-$2012). In Run 2 (2015$-$2018), about 140 fb$^{-1}$ of proton$-$proton collisions at a centre-of-mass energy of 13 TeV were collected by the ATLAS experiment. This review presents the most important Run 2 results obtained by the ATLAS Collaboration regarding the properties of the Higgs boson and its interactions with other particles. The performed studies significantly enhance the understanding of the Higgs boson, while hunting for deviations from the predictions of the Standard Model of particle physics.","sentences":["The Higgs boson was discovered by the ATLAS and CMS Collaborations in 2012 using data from Run 1 of the Large Hadron Collider (2010$-$2012).","In Run 2 (2015$-$2018), about 140 fb$^{-1}$ of proton$-$proton collisions at a centre-of-mass energy of 13 TeV were collected by the ATLAS experiment.","This review presents the most important Run 2 results obtained by the ATLAS Collaboration regarding the properties of the Higgs boson and its interactions with other particles.","The performed studies significantly enhance the understanding of the Higgs boson, while hunting for deviations from the predictions of the Standard Model of particle physics."],"url":"http://arxiv.org/abs/2404.05498v1","category":"hep-ex"}
{"created":"2024-04-08 13:17:23","title":"Decisioning Workshop 2023","abstract":"In a knowledge society, the term knowledge must be considered a core resource for organizations. So, beyond being a medium to progress and to innovate, knowledge is one of our most important resources: something necessary to decide.Organizations that are embracing knowledge retention activities are gaining a competitive advantage. Organizational rearrangements from companies, notably outsourcing, increase a possible loss of knowledge, making knowledge retention an essential need for them. When Knowledge is less shared, collaborative decision-making seems harder to obtain insofar as a ``communication breakdown'' characterizes participants' discourse. At best, stakeholders have to finda consensus according to their knowledge. Sharing knowledge ensures its retention and catalyzes the construction of this consensus.   Our vision of collaborative decision-making aims not only at increasing the quality of the first parts of the decision-making process: intelligence and design, but also at increasing the acceptance of the choice. Intelligence and design will be done by more than one individual and constructed together; the decision is more easily accepted. The decided choice will then be shared. Thereby where decision-making could be seen as a constructed model, collaborative decision-making, for us,is seen as the use of socio-technical media to improve decision-making performance and acceptability. The shared decision making is a core activity in a lot of human activities. For example, the sustainable decision-making is the job of not only governments and institutions but also broader society. Recognizing the urgent need for sustainability, we can argue that to realize sustainable development, it must be considered as a decision-making strategy. The location of knowledge in the realization of collaborative decision-making has to be regarded insofar as knowledge sharing leads to improve collaborative decision-making: a ``static view'' has to be structured and constitutes the ``collaborative knowledge.'' Knowledge has an important role in individual decision-making, and we consider that for collaborative decision-making, knowledge has to be shared. What is required is a better understanding of the nature of group work''. Knowledge has to be shared, but how do we share knowledge?","sentences":["In a knowledge society, the term knowledge must be considered a core resource for organizations.","So, beyond being a medium to progress and to innovate, knowledge is one of our most important resources: something necessary to decide.","Organizations that are embracing knowledge retention activities are gaining a competitive advantage.","Organizational rearrangements from companies, notably outsourcing, increase a possible loss of knowledge, making knowledge retention an essential need for them.","When Knowledge is less shared, collaborative decision-making seems harder to obtain insofar as a ``communication breakdown'' characterizes participants' discourse.","At best, stakeholders have to finda consensus according to their knowledge.","Sharing knowledge ensures its retention and catalyzes the construction of this consensus.   ","Our vision of collaborative decision-making aims not only at increasing the quality of the first parts of the decision-making process: intelligence and design, but also at increasing the acceptance of the choice.","Intelligence and design will be done by more than one individual and constructed together; the decision is more easily accepted.","The decided choice will then be shared.","Thereby where decision-making could be seen as a constructed model, collaborative decision-making, for us,is seen as the use of socio-technical media to improve decision-making performance and acceptability.","The shared decision making is a core activity in a lot of human activities.","For example, the sustainable decision-making is the job of not only governments and institutions but also broader society.","Recognizing the urgent need for sustainability, we can argue that to realize sustainable development, it must be considered as a decision-making strategy.","The location of knowledge in the realization of collaborative decision-making has to be regarded insofar as knowledge sharing leads to improve collaborative decision-making: a ``static view'' has to be structured and constitutes the ``collaborative knowledge.''","Knowledge has an important role in individual decision-making, and we consider that for collaborative decision-making, knowledge has to be shared.","What is required is a better understanding of the nature of group work''.","Knowledge has to be shared, but how do we share knowledge?"],"url":"http://arxiv.org/abs/2404.05495v1","category":"cs.CY"}
{"created":"2024-04-08 13:11:11","title":"The Impact of Sanctions on GitHub Developers and Activities","abstract":"The GitHub platform has fueled the creation of truly global software, enabling contributions from developers across various geographical regions of the world. As software becomes more entwined with global politics and social regulations, it becomes similarly subject to government sanctions. In 2019, GitHub restricted access to certain services for users in specific locations but rolled back these restrictions for some communities (e.g., the Iranian community) in 2021. We conducted a large-scale empirical study, collecting approximately 156 thousand user profiles and their 41 million activity points from 2008 to 2022, to understand the response of developers. Our results indicate that many of these targeted developers were able to navigate through the sanctions. Furthermore, once these sanctions were lifted, these developers opted to return to GitHub instead of withdrawing their contributions to the platform. The study indicates that platforms like GitHub play key roles in sustaining global contributions to Open Source Software.","sentences":["The GitHub platform has fueled the creation of truly global software, enabling contributions from developers across various geographical regions of the world.","As software becomes more entwined with global politics and social regulations, it becomes similarly subject to government sanctions.","In 2019, GitHub restricted access to certain services for users in specific locations but rolled back these restrictions for some communities (e.g., the Iranian community) in 2021.","We conducted a large-scale empirical study, collecting approximately 156 thousand user profiles and their 41 million activity points from 2008 to 2022, to understand the response of developers.","Our results indicate that many of these targeted developers were able to navigate through the sanctions.","Furthermore, once these sanctions were lifted, these developers opted to return to GitHub instead of withdrawing their contributions to the platform.","The study indicates that platforms like GitHub play key roles in sustaining global contributions to Open Source Software."],"url":"http://arxiv.org/abs/2404.05489v1","category":"cs.SE"}
{"created":"2024-04-08 13:10:21","title":"Monogenic Octic Polynomials and Their Galois Groups","abstract":"A monic polynomial $f(x)\\in {\\mathbb Z}[x]$ of degree $N$ is called monogenic if $f(x)$ is irreducible over ${\\mathbb Q}$ and $\\{1,\\theta,\\theta^2,\\ldots ,\\theta^{N-1}\\}$ is a basis for the ring of integers of ${\\mathbb Q}(\\theta)$, where $f(\\theta)=0$. In this article, we use the classification of the Galois groups of quartic polynomials, due to Kappe and Warren, to investigate the existence of infinite collections of monogenic quartic polynomials having a prescribed Galois group, such that each member of the collection generates a distinct quartic field. With the exception of the cyclic case, we provide such an infinite single-parameter collection for each possible Galois group. We believe these examples are new, and we provide evidence to support this belief by showing that they are distinct from other infinite collections in the current literature. Finally, we devote a separate section to a discussion concerning, what we believe to be, the still-unresolved cyclic case.","sentences":["A monic polynomial $f(x)\\in {\\mathbb Z}[x]$ of degree $N$ is called monogenic if $f(x)$ is irreducible over ${\\mathbb Q}$ and $\\{1,\\theta,\\theta^2,\\ldots ,\\theta^{N-1}\\}$ is a basis for the ring of integers of ${\\mathbb Q}(\\theta)$, where $f(\\theta)=0$. In this article, we use the classification of the Galois groups of quartic polynomials, due to Kappe and Warren, to investigate the existence of infinite collections of monogenic quartic polynomials having a prescribed Galois group, such that each member of the collection generates a distinct quartic field.","With the exception of the cyclic case, we provide such an infinite single-parameter collection for each possible Galois group.","We believe these examples are new, and we provide evidence to support this belief by showing that they are distinct from other infinite collections in the current literature.","Finally, we devote a separate section to a discussion concerning, what we believe to be, the still-unresolved cyclic case."],"url":"http://arxiv.org/abs/2404.05487v1","category":"math.NT"}
{"created":"2024-04-08 13:05:02","title":"PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of LLM-generated Text?","abstract":"In this paper, we present our submission to the SemEval-2024 Task 8 \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection\", focusing on the detection of machine-generated texts (MGTs) in English. Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set. We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91.","sentences":["In this paper, we present our submission to the SemEval-2024 Task 8 \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection\", focusing on the detection of machine-generated texts (MGTs) in English.","Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set.","We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91."],"url":"http://arxiv.org/abs/2404.05483v1","category":"cs.CL"}
{"created":"2024-04-08 12:46:39","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from fMRI","abstract":"The reconstruction of images observed by subjects from fMRI data collected during visual stimuli has made significant strides in the past decade, thanks to the availability of extensive fMRI datasets and advancements in generative models for image generation. However, the application of visual reconstruction has remained limited. Reconstructing visual imagination presents a greater challenge, with potentially revolutionary applications ranging from aiding individuals with disabilities to verifying witness accounts in court. The primary hurdles in this field are the absence of data collection protocols for visual imagery and the lack of datasets on the subject. Traditionally, fMRI-to-image relies on data collected from subjects exposed to visual stimuli, which poses issues for generating visual imagery based on the difference of brain activity between visual stimulation and visual imagery. For the first time, we have compiled a substantial dataset (around 6h of scans) on visual imagery along with a proposed data collection protocol. We then train a modified version of an fMRI-to-image model and demonstrate the feasibility of reconstructing images from two modes of imagination: from memory and from pure imagination. This marks an important step towards creating a technology that allow direct reconstruction of visual imagery.","sentences":["The reconstruction of images observed by subjects from fMRI data collected during visual stimuli has made significant strides in the past decade, thanks to the availability of extensive fMRI datasets and advancements in generative models for image generation.","However, the application of visual reconstruction has remained limited.","Reconstructing visual imagination presents a greater challenge, with potentially revolutionary applications ranging from aiding individuals with disabilities to verifying witness accounts in court.","The primary hurdles in this field are the absence of data collection protocols for visual imagery and the lack of datasets on the subject.","Traditionally, fMRI-to-image relies on data collected from subjects exposed to visual stimuli, which poses issues for generating visual imagery based on the difference of brain activity between visual stimulation and visual imagery.","For the first time, we have compiled a substantial dataset (around 6h of scans) on visual imagery along with a proposed data collection protocol.","We then train a modified version of an fMRI-to-image model and demonstrate the feasibility of reconstructing images from two modes of imagination: from memory and from pure imagination.","This marks an important step towards creating a technology that allow direct reconstruction of visual imagery."],"url":"http://arxiv.org/abs/2404.05468v1","category":"q-bio.NC"}
{"created":"2024-04-08 12:40:27","title":"Teaching Higher-Order Logic Using Isabelle","abstract":"We present a formalization of higher-order logic in the Isabelle proof assistant, building directly on the foundational framework Isabelle/Pure and developed to be as small and readable as possible. It should therefore serve as a good introduction for someone looking into learning about higher-order logic and proof assistants, without having to study the much more complex Isabelle/HOL with heavier automation. To showcase our development and approach we explain a sample proof, describe the axioms and rules of our higher-order logic, and discuss our experience with teaching the subject in a classroom setting.","sentences":["We present a formalization of higher-order logic in the Isabelle proof assistant, building directly on the foundational framework Isabelle/Pure and developed to be as small and readable as possible.","It should therefore serve as a good introduction for someone looking into learning about higher-order logic and proof assistants, without having to study the much more complex Isabelle/HOL with heavier automation.","To showcase our development and approach we explain a sample proof, describe the axioms and rules of our higher-order logic, and discuss our experience with teaching the subject in a classroom setting."],"url":"http://arxiv.org/abs/2404.05458v1","category":"cs.LO"}
{"created":"2024-04-08 12:31:23","title":"RoT: Enhancing Large Language Models with Reflection on Search Trees","abstract":"Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.","sentences":["Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods.","However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process.","To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods.","It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM.","The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process.","In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines.","In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS).","Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience."],"url":"http://arxiv.org/abs/2404.05449v1","category":"cs.CL"}
{"created":"2024-04-08 12:22:39","title":"Unlocking Adaptive User Experience with Generative AI","abstract":"Developing user-centred applications that address diverse user needs requires rigorous user research. This is time, effort and cost-consuming. With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces. This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT. We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes. To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation. The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process.","sentences":["Developing user-centred applications that address diverse user needs requires rigorous user research.","This is time, effort and cost-consuming.","With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces.","This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT.","We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes.","To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation.","The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process."],"url":"http://arxiv.org/abs/2404.05442v1","category":"cs.HC"}
{"created":"2024-04-08 12:19:04","title":"Tree Search-Based Policy Optimization under Stochastic Execution Delay","abstract":"The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately. However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through a series of experiments on the Atari suite, we demonstrate that although the previous baseline outperforms the naive method in scenarios with constant delay, it underperforms in the face of stochastic delays. In contrast, our approach significantly outperforms the baselines, for both constant and stochastic delays. The code is available at http://github.com/davidva1/Delayed-EZ .","sentences":["The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately.","However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic.","In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation.","We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case.","Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies.","DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue.","Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero.","Through a series of experiments on the Atari suite, we demonstrate that although the previous baseline outperforms the naive method in scenarios with constant delay, it underperforms in the face of stochastic delays.","In contrast, our approach significantly outperforms the baselines, for both constant and stochastic delays.","The code is available at http://github.com/davidva1/Delayed-EZ ."],"url":"http://arxiv.org/abs/2404.05440v1","category":"cs.AI"}
{"created":"2024-04-08 11:55:44","title":"Language Models on a Diet: Cost-Efficient Development of Encoders for Closely-Related Languages via Additional Pretraining","abstract":"The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed. However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters are still very much needed, their primary usage being in enriching large collections of data with metadata necessary for downstream research. We investigate the best way to ensure the existence of such encoder models on the set of very closely related languages - Croatian, Serbian, Bosnian and Montenegrin, by setting up a diverse benchmark for these languages, and comparing the trained-from-scratch models with the new models constructed via additional pretraining of existing multilingual models. We show that comparable performance to dedicated from-scratch models can be obtained by additionally pretraining available multilingual models even with a limited amount of computation. We also show that neighboring languages, in our case Slovenian, can be included in the additional pretraining with little to no loss in the performance of the final model.","sentences":["The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed.","However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters are still very much needed, their primary usage being in enriching large collections of data with metadata necessary for downstream research.","We investigate the best way to ensure the existence of such encoder models on the set of very closely related languages - Croatian, Serbian, Bosnian and Montenegrin, by setting up a diverse benchmark for these languages, and comparing the trained-from-scratch models with the new models constructed via additional pretraining of existing multilingual models.","We show that comparable performance to dedicated from-scratch models can be obtained by additionally pretraining available multilingual models even with a limited amount of computation.","We also show that neighboring languages, in our case Slovenian, can be included in the additional pretraining with little to no loss in the performance of the final model."],"url":"http://arxiv.org/abs/2404.05428v1","category":"cs.CL"}
{"created":"2024-04-08 11:55:09","title":"AutoCodeRover: Autonomous Program Improvement","abstract":"Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum based fault localization using tests, further sharpens the context. Experiments on the recently proposed SWE-bench-lite which consists of 300 real-life Github issues involving bug fixing and feature additions show increased efficacy (resolving more than 20% on SWE-bench-lite), as compared to recent efforts from the AI community. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.","sentences":["Researchers have made significant progress in automating the software development process in the past decades.","Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers.","Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding.","Nevertheless software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions).","In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement.","In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch.","In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented.","We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files.","Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search.","The use of spectrum based fault localization using tests, further sharpens the context.","Experiments on the recently proposed SWE-bench-lite which consists of 300 real-life Github issues involving bug fixing and feature additions show increased efficacy (resolving more than 20% on SWE-bench-lite), as compared to recent efforts from the AI community.","We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved."],"url":"http://arxiv.org/abs/2404.05427v1","category":"cs.SE"}
{"created":"2024-04-08 11:47:46","title":"What Are the Odds? Improving the foundations of Statistical Model Checking","abstract":"Markov decision processes (MDPs) are a fundamental model for decision making under uncertainty. They exhibit non-deterministic choice as well as probabilistic uncertainty. Traditionally, verification algorithms assume exact knowledge of the probabilities that govern the behaviour of an MDP. As this assumption is often unrealistic in practice, statistical model checking (SMC) was developed in the past two decades. It allows to analyse MDPs with unknown transition probabilities and provide probably approximately correct (PAC) guarantees on the result. Model-based SMC algorithms sample the MDP and build a model of it by estimating all transition probabilities, essentially for every transition answering the question: ``What are the odds?'' However, so far the statistical methods employed by the state of the art SMC algorithms are quite naive. Our contribution are several fundamental improvements to those methods: On the one hand, we survey statistics literature for better concentration inequalities; on the other hand, we propose specialised approaches that exploit our knowledge of the MDP. Our improvements are generally applicable to many kinds of problem statements because they are largely independent of the setting. Moreover, our experimental evaluation shows that they lead to significant gains, reducing the number of samples that the SMC algorithm has to collect by up to two orders of magnitude.","sentences":["Markov decision processes (MDPs) are a fundamental model for decision making under uncertainty.","They exhibit non-deterministic choice as well as probabilistic uncertainty.","Traditionally, verification algorithms assume exact knowledge of the probabilities that govern the behaviour of an MDP.","As this assumption is often unrealistic in practice, statistical model checking (SMC) was developed in the past two decades.","It allows to analyse MDPs with unknown transition probabilities and provide probably approximately correct (PAC) guarantees on the result.","Model-based SMC algorithms sample the MDP and build a model of it by estimating all transition probabilities, essentially for every transition answering the question: ``What are the odds?''","However, so far the statistical methods employed by the state of the art SMC algorithms are quite naive.","Our contribution are several fundamental improvements to those methods: On the one hand, we survey statistics literature for better concentration inequalities; on the other hand, we propose specialised approaches that exploit our knowledge of the MDP.","Our improvements are generally applicable to many kinds of problem statements because they are largely independent of the setting.","Moreover, our experimental evaluation shows that they lead to significant gains, reducing the number of samples that the SMC algorithm has to collect by up to two orders of magnitude."],"url":"http://arxiv.org/abs/2404.05424v1","category":"cs.AI"}
{"created":"2024-04-08 11:43:40","title":"Residual Chain Prediction for Autonomous Driving Path Planning","abstract":"In the rapidly evolving field of autonomous driving systems, the refinement of path planning algorithms is paramount for navigating vehicles through dynamic environments, particularly in complex urban scenarios. Traditional path planning algorithms, which are heavily reliant on static rules and manually defined parameters, often fall short in such contexts, highlighting the need for more adaptive, learning-based approaches. Among these, behavior cloning emerges as a noteworthy strategy for its simplicity and efficiency, especially within the realm of end-to-end path planning. However, behavior cloning faces challenges, such as covariate shift when employing traditional Manhattan distance as the metric. Addressing this, our study introduces the novel concept of Residual Chain Loss. Residual Chain Loss dynamically adjusts the loss calculation process to enhance the temporal dependency and accuracy of predicted path points, significantly improving the model's performance without additional computational overhead. Through testing on the nuScenes dataset, we underscore the method's substantial advancements in addressing covariate shift, facilitating dynamic loss adjustments, and ensuring seamless integration with end-to-end path planning frameworks. Our findings highlight the potential of Residual Chain Loss to revolutionize planning component of autonomous driving systems, marking a significant step forward in the quest for level 5 autonomous driving system.","sentences":["In the rapidly evolving field of autonomous driving systems, the refinement of path planning algorithms is paramount for navigating vehicles through dynamic environments, particularly in complex urban scenarios.","Traditional path planning algorithms, which are heavily reliant on static rules and manually defined parameters, often fall short in such contexts, highlighting the need for more adaptive, learning-based approaches.","Among these, behavior cloning emerges as a noteworthy strategy for its simplicity and efficiency, especially within the realm of end-to-end path planning.","However, behavior cloning faces challenges, such as covariate shift when employing traditional Manhattan distance as the metric.","Addressing this, our study introduces the novel concept of Residual Chain Loss.","Residual Chain Loss dynamically adjusts the loss calculation process to enhance the temporal dependency and accuracy of predicted path points, significantly improving the model's performance without additional computational overhead.","Through testing on the nuScenes dataset, we underscore the method's substantial advancements in addressing covariate shift, facilitating dynamic loss adjustments, and ensuring seamless integration with end-to-end path planning frameworks.","Our findings highlight the potential of Residual Chain Loss to revolutionize planning component of autonomous driving systems, marking a significant step forward in the quest for level 5 autonomous driving system."],"url":"http://arxiv.org/abs/2404.05423v1","category":"cs.RO"}
{"created":"2024-04-08 11:36:28","title":"Joint Active and Passive Beamforming for IRS-Aided Wireless Energy Transfer Network Exploiting One-Bit Feedback","abstract":"To reap the active and passive beamforming gain in an intelligent reflecting surface (IRS)-aided wireless network, a typical way is to first acquire the channel state information (CSI) relying on the pilot signal, and then perform the joint beamforming design. However, it is a great challenge when the receiver can neither send pilot signals nor have complex signal processing capabilities due to its hardware limitation. To tackle this problem, we study in this paper an IRS-aided wireless energy transfer (WET) network and propose two joint beamforming design methods, namely, the channel-estimationbased method and the distributed-beamforming-based method, that require only one-bit feedback from the energy receiver (ER) to the energy transmitter (ET). Specifically, for the channelestimation-based method, according to the feedback information, the ET is able to infer the cascaded ET-IRS-ER channel by continually adjusting its transmit beamformer while applying the analytic center cutting plane method (ACCPM). Then, based on the estimated cascaded CSI, the joint beamforming design can be performed by using the existing optimization techniques. While for the distributed-beamforming-based method, we first apply the distributed beamforming algorithm to optimize the IRS reflection coefficients, which is theoretically proven to converge to a local optimum almost surely. Then, the optimal ET's transmit covariance matrix is obtained based on the effective ET-ER channel learned by applying the ACCPM only once. Numerical results demonstrate the effectiveness of our proposed one-bitfeedback-based joint beamforming design schemes while greatly reducing the requirement on the hardware complexity of the ER. In particular, the high accuracy of our IRS-involved cascaded channel estimation method exploiting one-bit feedback is also validated.","sentences":["To reap the active and passive beamforming gain in an intelligent reflecting surface (IRS)-aided wireless network, a typical way is to first acquire the channel state information (CSI) relying on the pilot signal, and then perform the joint beamforming design.","However, it is a great challenge when the receiver can neither send pilot signals nor have complex signal processing capabilities due to its hardware limitation.","To tackle this problem, we study in this paper an IRS-aided wireless energy transfer (WET) network and propose two joint beamforming design methods, namely, the channel-estimationbased method and the distributed-beamforming-based method, that require only one-bit feedback from the energy receiver (ER) to the energy transmitter (ET).","Specifically, for the channelestimation-based method, according to the feedback information, the ET is able to infer the cascaded ET-IRS-ER channel by continually adjusting its transmit beamformer while applying the analytic center cutting plane method (ACCPM).","Then, based on the estimated cascaded CSI, the joint beamforming design can be performed by using the existing optimization techniques.","While for the distributed-beamforming-based method, we first apply the distributed beamforming algorithm to optimize the IRS reflection coefficients, which is theoretically proven to converge to a local optimum almost surely.","Then, the optimal ET's transmit covariance matrix is obtained based on the effective ET-ER channel learned by applying the ACCPM only once.","Numerical results demonstrate the effectiveness of our proposed one-bitfeedback-based joint beamforming design schemes while greatly reducing the requirement on the hardware complexity of the ER.","In particular, the high accuracy of our IRS-involved cascaded channel estimation method exploiting one-bit feedback is also validated."],"url":"http://arxiv.org/abs/2404.05418v1","category":"eess.SY"}
{"created":"2024-04-08 11:33:58","title":"Indexing Analytics to Instances: How Integrating a Dashboard can Support Design Education","abstract":"We investigate how to use AI-based analytics to support design education. The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work. With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them. We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people. We studied the research artifact in 5 situated course contexts, in 3 departments. A total of 236 students used the multiscale design environment. The 9 instructors who taught those students experienced the analytics via the new research artifact.   We derive findings from a qualitative analysis of interviews with instructors regarding their experiences. Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education. We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education. By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances.","sentences":["We investigate how to use AI-based analytics to support design education.","The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work.","With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them.","We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people.","We studied the research artifact in 5 situated course contexts, in 3 departments.","A total of 236 students used the multiscale design environment.","The 9 instructors who taught those students experienced the analytics via the new research artifact.   ","We derive findings from a qualitative analysis of interviews with instructors regarding their experiences.","Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education.","We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education.","By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances."],"url":"http://arxiv.org/abs/2404.05417v1","category":"cs.HC"}
{"created":"2024-04-08 11:33:00","title":"Relation Extraction Using Large Language Models: A Case Study on Acupuncture Point Locations","abstract":"In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness. The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant opportunity for extracting relations related to acupoint locations from textual knowledge sources. This study aims to compare the performance of GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT)) in extracting acupoint-related location relations and assess the impact of pretraining and fine-tuning on GPT's performance. We utilized the World Health Organization Standard Acupuncture Point Locations in the Western Pacific Region (WHO Standard) as our corpus, which consists of descriptions of 361 acupoints. Five types of relations ('direction_of,' 'distance_of,' 'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints were annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5, and fine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included micro-average exact match precision, recall, and F1 scores. Our results demonstrate that fine-tuned GPT-3.5 consistently outperformed other models in F1 scores across all relation types. Overall, it achieved the highest micro-average F1 score of 0.92. This study underscores the effectiveness of LLMs like GPT in extracting relations related to acupoint locations, with implications for accurately modeling acupuncture knowledge and promoting standard implementation in acupuncture training and practice. The findings also contribute to advancing informatics applications in traditional and complementary medicine, showcasing the potential of LLMs in natural language processing.","sentences":["In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness.","The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant opportunity for extracting relations related to acupoint locations from textual knowledge sources.","This study aims to compare the performance of GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT)) in extracting acupoint-related location relations and assess the impact of pretraining and fine-tuning on GPT's performance.","We utilized the World Health Organization Standard Acupuncture Point Locations in the Western Pacific Region (WHO Standard) as our corpus, which consists of descriptions of 361 acupoints.","Five types of relations ('direction_of,' 'distance_of,' 'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints were annotated.","Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5, and fine-tuned GPT-3.5, as well as pre-trained GPT-4.","Performance metrics included micro-average exact match precision, recall, and F1 scores.","Our results demonstrate that fine-tuned GPT-3.5 consistently outperformed other models in F1 scores across all relation types.","Overall, it achieved the highest micro-average F1 score of 0.92.","This study underscores the effectiveness of LLMs like GPT in extracting relations related to acupoint locations, with implications for accurately modeling acupuncture knowledge and promoting standard implementation in acupuncture training and practice.","The findings also contribute to advancing informatics applications in traditional and complementary medicine, showcasing the potential of LLMs in natural language processing."],"url":"http://arxiv.org/abs/2404.05415v1","category":"cs.CL"}
{"created":"2024-04-08 11:14:58","title":"PerkwE_COQA: enhance Persian Conversational Question Answering by combining contextual keyword extraction with Large Language Models","abstract":"Smart cities need the involvement of their residents to enhance quality of life. Conversational query-answering is an emerging approach for user engagement. There is an increasing demand of an advanced conversational question-answering that goes beyond classic systems. Existing approaches have shown that LLMs offer promising capabilities for CQA, but may struggle to capture the nuances of conversational contexts. The new approach involves understanding the content and engaging in a multi-step conversation with the user to fulfill their needs. This paper presents a novel method to elevate the performance of Persian Conversational question-answering (CQA) systems. It combines the strengths of Large Language Models (LLMs) with contextual keyword extraction. Our method extracts keywords specific to the conversational flow, providing the LLM with additional context to understand the user's intent and generate more relevant and coherent responses. We evaluated the effectiveness of this combined approach through various metrics, demonstrating significant improvements in CQA performance compared to an LLM-only baseline. The proposed method effectively handles implicit questions, delivers contextually relevant answers, and tackles complex questions that rely heavily on conversational context. The findings indicate that our method outperformed the evaluation benchmarks up to 8% higher than existing methods and the LLM-only baseline.","sentences":["Smart cities need the involvement of their residents to enhance quality of life.","Conversational query-answering is an emerging approach for user engagement.","There is an increasing demand of an advanced conversational question-answering that goes beyond classic systems.","Existing approaches have shown that LLMs offer promising capabilities for CQA, but may struggle to capture the nuances of conversational contexts.","The new approach involves understanding the content and engaging in a multi-step conversation with the user to fulfill their needs.","This paper presents a novel method to elevate the performance of Persian Conversational question-answering (CQA) systems.","It combines the strengths of Large Language Models (LLMs) with contextual keyword extraction.","Our method extracts keywords specific to the conversational flow, providing the LLM with additional context to understand the user's intent and generate more relevant and coherent responses.","We evaluated the effectiveness of this combined approach through various metrics, demonstrating significant improvements in CQA performance compared to an LLM-only baseline.","The proposed method effectively handles implicit questions, delivers contextually relevant answers, and tackles complex questions that rely heavily on conversational context.","The findings indicate that our method outperformed the evaluation benchmarks up to 8% higher than existing methods and the LLM-only baseline."],"url":"http://arxiv.org/abs/2404.05406v1","category":"cs.CL"}
{"created":"2024-04-08 11:11:31","title":"Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws","abstract":"Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.   More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include:   * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.   * Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.","sentences":["Scaling laws describe the relationship between the size of language models and their capabilities.","Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores.","We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page.","Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications.","Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.   ","More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.","Notable insights include:   *","The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations.","This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.   ","* Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity.","Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity."],"url":"http://arxiv.org/abs/2404.05405v1","category":"cs.CL"}
{"created":"2024-04-08 11:05:45","title":"SoK: Gradient Leakage in Federated Learning","abstract":"Federated learning (FL) enables collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from the gradients they share in FL, known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a milestone to trace their evolution and develops a systematization to uncover their inherent threats. Specifically, we categorize the auxiliary assumptions used by existing GIAs based on their practical accessibility to potential adversaries. To facilitate deeper analysis, we highlight the challenges that GIAs face in practical FL systems from three perspectives: \\textit{local training}, \\textit{model}, and \\textit{post-processing}. We then perform extensive theoretical and empirical evaluations of state-of-the-art GIAs across diverse settings, utilizing eight datasets and thirteen models. Our findings indicate that GIAs have inherent limitations when reconstructing data under practical local training settings. Furthermore, their efficacy is sensitive to the trained model, and even simple post-processing measures applied to gradients can be effective defenses. Overall, our work provides crucial insights into the limited effectiveness of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic.","sentences":["Federated learning (FL) enables collaborative model training among multiple clients without raw data exposure.","However, recent studies have shown that clients' private training data can be reconstructed from the gradients they share in FL, known as gradient inversion attacks (GIAs).","While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored.","To address this gap, we conduct a comprehensive study on GIAs in this work.","We start with a survey of GIAs that establishes a milestone to trace their evolution and develops a systematization to uncover their inherent threats.","Specifically, we categorize the auxiliary assumptions used by existing GIAs based on their practical accessibility to potential adversaries.","To facilitate deeper analysis, we highlight the challenges that GIAs face in practical FL systems from three perspectives: \\textit{local training}, \\textit{model}, and \\textit{post-processing}.","We then perform extensive theoretical and empirical evaluations of state-of-the-art GIAs across diverse settings, utilizing eight datasets and thirteen models.","Our findings indicate that GIAs have inherent limitations when reconstructing data under practical local training settings.","Furthermore, their efficacy is sensitive to the trained model, and even simple post-processing measures applied to gradients can be effective defenses.","Overall, our work provides crucial insights into the limited effectiveness of GIAs in practical FL systems.","By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic."],"url":"http://arxiv.org/abs/2404.05403v1","category":"cs.CR"}
{"created":"2024-04-08 10:57:25","title":"SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety","abstract":"The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets. We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops.","sentences":["The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs).","Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety.","However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential.","This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill.","To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety.","We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months.","We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets.","We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets.","Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops."],"url":"http://arxiv.org/abs/2404.05399v1","category":"cs.CL"}
{"created":"2024-04-08 10:54:34","title":"Emergent polar order in non-polar mixtures with non-reciprocal interactions","abstract":"Phenomenological rules that govern the collective behaviour of complex physical systems are powerful tools because they can make concrete predictions about their universality class based on generic considerations, such as symmetries, conservation laws, and dimensionality. While in most cases such considerations are manifestly ingrained in the constituents, novel phenomenology can emerge when composite units associated with emergent symmetries dominate the behaviour of the system. We study a generic class of active matter systems with non-reciprocal interactions and demonstrate the existence of true long-range polar order in two dimensions and above, both at the linear level and by including all relevant nonlinearities in the Renormalization Group sense. We achieve this by uncovering a mapping of our scalar active mixture theory to the Toner-Tu theory of dry polar active matter by employing a suitably defined polar order parameter. We then demonstrate that the complete effective field theory -- which includes all the soft modes and the relevant nonlinear terms -- belongs to the (Burgers-) Kardar-Parisi-Zhang universality class. This classification allows us to prove the stability of the emergent polar long-range order in scalar non-reciprocal mixtures in two dimensions, and hence a conclusive violation of the Mermin-Wagner theorem.","sentences":["Phenomenological rules that govern the collective behaviour of complex physical systems are powerful tools because they can make concrete predictions about their universality class based on generic considerations, such as symmetries, conservation laws, and dimensionality.","While in most cases such considerations are manifestly ingrained in the constituents, novel phenomenology can emerge when composite units associated with emergent symmetries dominate the behaviour of the system.","We study a generic class of active matter systems with non-reciprocal interactions and demonstrate the existence of true long-range polar order in two dimensions and above, both at the linear level and by including all relevant nonlinearities in the Renormalization Group sense.","We achieve this by uncovering a mapping of our scalar active mixture theory to the Toner-Tu theory of dry polar active matter by employing a suitably defined polar order parameter.","We then demonstrate that the complete effective field theory -- which includes all the soft modes and the relevant nonlinear terms -- belongs to the (Burgers-) Kardar-Parisi-Zhang universality class.","This classification allows us to prove the stability of the emergent polar long-range order in scalar non-reciprocal mixtures in two dimensions, and hence a conclusive violation of the Mermin-Wagner theorem."],"url":"http://arxiv.org/abs/2404.05396v1","category":"cond-mat.soft"}
{"created":"2024-04-08 10:52:29","title":"PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation","abstract":"Beyond class frequency, we recognize the impact of class-wise relationships among various class-specific predictions and the imbalance in label masks on long-tailed segmentation learning. To address these challenges, we propose an innovative Pixel-wise Adaptive Training (PAT) technique tailored for long-tailed segmentation. PAT has two key features: 1) class-wise gradient magnitude homogenization, and 2) pixel-wise class-specific loss adaptation (PCLA). First, the class-wise gradient magnitude homogenization helps alleviate the imbalance among label masks by ensuring equal consideration of the class-wise impact on model updates. Second, PCLA tackles the detrimental impact of both rare classes within the long-tailed distribution and inaccurate predictions from previous training stages by encouraging learning classes with low prediction confidence and guarding against forgetting classes with high confidence. This combined approach fosters robust learning while preventing the model from forgetting previously learned knowledge. PAT exhibits significant performance improvements, surpassing the current state-of-the-art by 2.2% in the NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and intersection over union value by 2.07%, with a particularly notable declination of 0.39% in detecting rare classes compared to Balance Logits Variation, as demonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and NYU.","sentences":["Beyond class frequency, we recognize the impact of class-wise relationships among various class-specific predictions and the imbalance in label masks on long-tailed segmentation learning.","To address these challenges, we propose an innovative Pixel-wise Adaptive Training (PAT) technique tailored for long-tailed segmentation.","PAT has two key features: 1) class-wise gradient magnitude homogenization, and 2) pixel-wise class-specific loss adaptation (PCLA).","First, the class-wise gradient magnitude homogenization helps alleviate the imbalance among label masks by ensuring equal consideration of the class-wise impact on model updates.","Second, PCLA tackles the detrimental impact of both rare classes within the long-tailed distribution and inaccurate predictions from previous training stages by encouraging learning classes with low prediction confidence and guarding against forgetting classes with high confidence.","This combined approach fosters robust learning while preventing the model from forgetting previously learned knowledge.","PAT exhibits significant performance improvements, surpassing the current state-of-the-art by 2.2% in the NyU dataset.","Moreover, it enhances overall pixel-wise accuracy by 2.85% and intersection over union value by 2.07%, with a particularly notable declination of 0.39% in detecting rare classes compared to Balance Logits Variation, as demonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and NYU."],"url":"http://arxiv.org/abs/2404.05393v2","category":"cs.CV"}
{"created":"2024-04-08 10:50:29","title":"Design and implementation of a synchronous Hardware Performance Monitor for a RISC-V space-oriented processor","abstract":"The ability to collect statistics about the execution of a program within a CPU is of the utmost importance across all fields of computing since it allows characterizing the timing performance of a program. This capability is even more relevant in safety-critical software systems, where it is mandatory to analyze software timing requirements to ensure the correct operation of the programs. Moreover, in order to properly evaluate and verify the extra-functional properties of these systems, besides timing performance, there are many other statistics available on a CPU, such as those associated with resource utilization. In this paper, we showcase a Performance Measurement Unit, also known as Hardware Performance Monitor, integrated into a RISC-V On-Board Computer designed for space applications by our research group. The monitoring technique features a novel approach whereby the events triggered are not counted immediately but instead are propagated through the pipeline so that their annotation is synchronized with the executed instruction. Additionally, we demonstrate the use of this PMU in a process to characterize the execution model of the processor. Finally, as an example of the statistics provided by the PMU, the results obtained running the CoreMark and Dhrystone benchmarks on the RISC-V OBC are shown.","sentences":["The ability to collect statistics about the execution of a program within a CPU is of the utmost importance across all fields of computing since it allows characterizing the timing performance of a program.","This capability is even more relevant in safety-critical software systems, where it is mandatory to analyze software timing requirements to ensure the correct operation of the programs.","Moreover, in order to properly evaluate and verify the extra-functional properties of these systems, besides timing performance, there are many other statistics available on a CPU, such as those associated with resource utilization.","In this paper, we showcase a Performance Measurement Unit, also known as Hardware Performance Monitor, integrated into a RISC-V On-Board Computer designed for space applications by our research group.","The monitoring technique features a novel approach whereby the events triggered are not counted immediately but instead are propagated through the pipeline so that their annotation is synchronized with the executed instruction.","Additionally, we demonstrate the use of this PMU in a process to characterize the execution model of the processor.","Finally, as an example of the statistics provided by the PMU, the results obtained running the CoreMark and Dhrystone benchmarks on the RISC-V OBC are shown."],"url":"http://arxiv.org/abs/2404.05389v1","category":"cs.AR"}
{"created":"2024-04-09 17:54:23","title":"Evidence for Primordial Alignment: Insights from Stellar Obliquity Measurements for Compact Sub-Saturn Systems","abstract":"Despite decades of effort, the mechanisms by which the spin axis of a star and the orbital axes of its planets become misaligned remain elusive. Particularly, it is of great interest whether the large spin-orbit misalignments observed are driven primarily by high-eccentricity migration -- expected to have occurred for short-period, isolated planets -- or reflect a more universal process that operates across systems with a variety of present-day architectures. Compact multi-planet systems offer a unique opportunity to differentiate between these competing hypotheses, as their tightly-packed configurations preclude violent dynamical histories, including high-eccentricity migration, allowing them to trace the primordial disk plane. In this context, we report measurements of the sky-projected stellar obliquity ($\\lambda$) via the Rossiter-McLaughlin effect for two sub-Saturns in multiple-transiting systems: TOI-5126 b ($\\lambda=1\\pm 48\\,^{\\circ}$) and TOI-5398 b ($\\lambda=-24^{+14}_{-13} \\,^{\\circ}$). Both are spin-orbit aligned, joining a fast-growing group of just three other compact sub-Saturn systems, all of which exhibit spin-orbit alignment. Our results strongly suggest that sub-Saturn systems are primordially aligned and become misaligned largely in the post-disk phase through violent dynamical interactions inherent to eccentric migration, as appears to be the case increasingly for other exoplanet populations.","sentences":["Despite decades of effort, the mechanisms by which the spin axis of a star and the orbital axes of its planets become misaligned remain elusive.","Particularly, it is of great interest whether the large spin-orbit misalignments observed are driven primarily by high-eccentricity migration -- expected to have occurred for short-period, isolated planets -- or reflect a more universal process that operates across systems with a variety of present-day architectures.","Compact multi-planet systems offer a unique opportunity to differentiate between these competing hypotheses, as their tightly-packed configurations preclude violent dynamical histories, including high-eccentricity migration, allowing them to trace the primordial disk plane.","In this context, we report measurements of the sky-projected stellar obliquity ($\\lambda$) via the Rossiter-McLaughlin effect for two sub-Saturns in multiple-transiting systems:","TOI-5126 b ($\\lambda=1\\pm 48\\,^{\\circ}$) and TOI-5398 b ($\\lambda=-24^{+14}_{-13} \\,^{\\circ}$).","Both are spin-orbit aligned, joining a fast-growing group of just three other compact sub-Saturn systems, all of which exhibit spin-orbit alignment.","Our results strongly suggest that sub-Saturn systems are primordially aligned and become misaligned largely in the post-disk phase through violent dynamical interactions inherent to eccentric migration, as appears to be the case increasingly for other exoplanet populations."],"url":"http://arxiv.org/abs/2404.06504v1","category":"astro-ph.EP"}
{"created":"2024-04-09 17:48:52","title":"Flying With Photons: Rendering Novel Views of Propagating Light","abstract":"We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints. Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution. Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field. This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, discrete-time signal that represents time-varying radiance at ultrafast timescales. Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera. We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction. Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport.","sentences":["We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints.","Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution.","Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field.","This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, discrete-time signal that represents time-varying radiance at ultrafast timescales.","Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera.","We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction.","Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport."],"url":"http://arxiv.org/abs/2404.06493v1","category":"cs.CV"}
{"created":"2024-04-09 17:36:47","title":"Server saturation in skewed networks","abstract":"We consider a model inspired by compatibility constraints that arise between tasks and servers in data centers, cloud computing systems and content delivery networks. The constraints are represented by a bipartite graph or network that interconnects dispatchers with compatible servers. Each dispatcher receives tasks over time and sends every task to a compatible server with the least number of tasks, or to a server with the least number of tasks among $d$ compatible servers selected uniformly at random. We focus on networks where the neighborhood of at least one server is skewed in a limiting regime. This means that a diverging number of dispatchers are in the neighborhood which are each compatible with a uniformly bounded number of servers; thus, the degree of the central server approaches infinity while the degrees of many neighboring dispatchers remain bounded. We prove that each server with a skewed neighborhood saturates, in the sense that the mean number of tasks queueing in front of it in steady state approaches infinity. Paradoxically, this pathological behavior can even arise in random networks where nearly all the servers have at most one task in the limit.","sentences":["We consider a model inspired by compatibility constraints that arise between tasks and servers in data centers, cloud computing systems and content delivery networks.","The constraints are represented by a bipartite graph or network that interconnects dispatchers with compatible servers.","Each dispatcher receives tasks over time and sends every task to a compatible server with the least number of tasks, or to a server with the least number of tasks among $d$ compatible servers selected uniformly at random.","We focus on networks where the neighborhood of at least one server is skewed in a limiting regime.","This means that a diverging number of dispatchers are in the neighborhood which are each compatible with a uniformly bounded number of servers; thus, the degree of the central server approaches infinity while the degrees of many neighboring dispatchers remain bounded.","We prove that each server with a skewed neighborhood saturates, in the sense that the mean number of tasks queueing in front of it in steady state approaches infinity.","Paradoxically, this pathological behavior can even arise in random networks where nearly all the servers have at most one task in the limit."],"url":"http://arxiv.org/abs/2404.06485v1","category":"math.PR"}
{"created":"2024-04-09 17:34:19","title":"RhythmMamba: Fast Remote Physiological Measurement with Arbitrary Length Videos","abstract":"Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing. Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts. This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices. Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG. Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity. The proposed RhythmMamba can be applied to video segments of any length without performance degradation. The codes are available at https://github.com/zizheng-guo/RhythmMamba.","sentences":["Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing.","Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts.","This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices.","Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG.","Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity.","The proposed RhythmMamba can be applied to video segments of any length without performance degradation.","The codes are available at https://github.com/zizheng-guo/RhythmMamba."],"url":"http://arxiv.org/abs/2404.06483v1","category":"cs.CV"}
{"created":"2024-04-09 17:27:10","title":"Multiple mixing in ergodic theory","abstract":"In 1949 V.A. Rokhlin introduced into ergodic theory the k-fold mixing and puzzled the mathematical community with the problem of the mismatch of these invariants. Here's what Rokhlin wrote:   \"The proposed work arose from the author's attempts to solve the well-known spectral problem of the theory of dynamical systems: are there any metrically different dynamical systems with the same continuous (in particular, Lebesgue) spectrum? Using new metric invariants introduced for this purpose, the author tried to find among the ergodic automorphisms of compact commutative groups automorphisms of various metric types. It turned out, however, that for all the indicated automorphisms the new invariants are exactly the same.\"   We discuss this long time open problem.","sentences":["In 1949 V.A. Rokhlin introduced into ergodic theory the k-fold mixing and puzzled the mathematical community with the problem of the mismatch of these invariants.","Here's what Rokhlin wrote:   \"The proposed work arose from the author's attempts to solve the well-known spectral problem of the theory of dynamical systems: are there any metrically different dynamical systems with the same continuous (in particular, Lebesgue) spectrum?","Using new metric invariants introduced for this purpose, the author tried to find among the ergodic automorphisms of compact commutative groups automorphisms of various metric types.","It turned out, however, that for all the indicated automorphisms the new invariants are exactly the same.\"   ","We discuss this long time open problem."],"url":"http://arxiv.org/abs/2404.06476v1","category":"math.DS"}
{"created":"2024-04-09 17:26:13","title":"Two-dimensional turbulence above topography: condensation transition and selection of minimum enstrophy solutions","abstract":"We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel. We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography. The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS). We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay. As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS. This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay.","sentences":["We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel.","We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography.","The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS).","We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay.","As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS.","This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay."],"url":"http://arxiv.org/abs/2404.06475v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 17:15:37","title":"Scaling to 32 GPUs on a Novel Composable System Architecture","abstract":"The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers. This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented. This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands. The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility.","sentences":["The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers.","This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented.","This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands.","The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility."],"url":"http://arxiv.org/abs/2404.06467v1","category":"cs.ET"}
{"created":"2024-04-09 17:12:38","title":"Phase space contraction of degenerately damped random splittings","abstract":"When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping. In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions. The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper. We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model.","sentences":["When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping.","In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions.","The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper.","We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model."],"url":"http://arxiv.org/abs/2404.06465v1","category":"math.PR"}
{"created":"2024-04-09 17:01:43","title":"Thermal fluctuations of matter composition and quark nucleation in compact stars","abstract":"Context. At the extreme densities reached in the core of neutron stars, it is possible that quark deconfined matter is produced. The formation of this new phase of strongly interacting matter is likely to occur via a first-order phase transition for the typical temperatures reached in astrophysical processes. The first seeds of quark matter would then form through a process of nucleation within the metastable hadronic phase. Aims. Here we address the role of the thermal fluctuations in the hadronic composition on the nucleation of two-flavour quark matter. Methods. At finite temperature, thermodynamic quantities in a system fluctuate around average values. Being nucleation a local process, it is possible that it occurs in a subsystem whose composition makes the nucleation easier. We will consider the total probability of the nucleation as the product between the probability that a subsystem has a certain hadronic composition different from the average in the bulk, and the nucleation probability in that subsystem. Results. We will show how those fluctuations of the hadronic composition can increase the efficiency of nucleation already for temperatures $\\sim (0.1-1)$ keV. However, for temperatures $\\lesssim (1-10)$ MeV, the needed overpressure exceeds the maximum pressure reached in compact stars. Finally, for even larger temperatures the process of nucleation can take place, even taking into account finite size effects.","sentences":["Context.","At the extreme densities reached in the core of neutron stars, it is possible that quark deconfined matter is produced.","The formation of this new phase of strongly interacting matter is likely to occur via a first-order phase transition for the typical temperatures reached in astrophysical processes.","The first seeds of quark matter would then form through a process of nucleation within the metastable hadronic phase.","Aims.","Here we address the role of the thermal fluctuations in the hadronic composition on the nucleation of two-flavour quark matter.","Methods.","At finite temperature, thermodynamic quantities in a system fluctuate around average values.","Being nucleation a local process, it is possible that it occurs in a subsystem whose composition makes the nucleation easier.","We will consider the total probability of the nucleation as the product between the probability that a subsystem has a certain hadronic composition different from the average in the bulk, and the nucleation probability in that subsystem.","Results.","We will show how those fluctuations of the hadronic composition can increase the efficiency of nucleation already for temperatures $\\sim (0.1-1)$ keV.","However, for temperatures $\\lesssim (1-10)$ MeV, the needed overpressure exceeds the maximum pressure reached in compact stars.","Finally, for even larger temperatures the process of nucleation can take place, even taking into account finite size effects."],"url":"http://arxiv.org/abs/2404.06463v1","category":"nucl-th"}
{"created":"2024-04-09 17:00:53","title":"Analysis of Distributed Algorithms for Big-data","abstract":"The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis. The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed. Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented. The systems and applications chosen here are of open-source nature, due to their wider applicability.","sentences":["The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis.","The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed.","Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented.","The systems and applications chosen here are of open-source nature, due to their wider applicability."],"url":"http://arxiv.org/abs/2404.06461v1","category":"cs.DC"}
{"created":"2024-04-09 17:00:43","title":"Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction","abstract":"Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements. Their temporal evolution is often driven by transitions between a finite number of discrete states. Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner. AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction.","sentences":["Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements.","Their temporal evolution is often driven by transitions between a finite number of discrete states.","Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling.","We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner.","AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction."],"url":"http://arxiv.org/abs/2404.06460v1","category":"eess.SY"}
{"created":"2024-04-09 17:00:33","title":"A hybrid discrete-continuum modelling approach for the interactions of the immune system with oncolytic viral infections","abstract":"Oncolytic virotherapy, utilizing genetically modified viruses to combat cancer and trigger anti-cancer immune responses, has garnered significant attention in recent years. In our previous work arXiv:2305.12386, we developed a stochastic agent-based model elucidating the spatial dynamics of infected and uninfected cells within solid tumours. Building upon this foundation, we present a novel stochastic agent-based model to describe the intricate interplay between the virus and the immune system; the agents' dynamics are coupled with a balance equation for the concentration of the chemoattractant that guides the movement of immune cells. We formally derive the continuum limit of the model and carry out a systematic quantitative comparison between this system of PDEs and the individual-based model in two spatial dimensions. Furthermore, we describe the traveling waves of the three populations, with the uninfected proliferative cells trying to escape from the infected cells while immune cells infiltrate the tumour.   Simulations show a good agreement between agent-based approaches and numerical results for the continuum model. Some parameter ranges give rise to oscillations of cell number in both models, in line with the behaviour of the corresponding nonspatial model, which presents Hopf bifurcations. Nevertheless, in some situations the behaviours of the two models may differ significantly, suggesting that stochasticity plays a key role in the dynamics. Our results highlight that a too rapid immune response, before the infection is well-established, appears to decrease the efficacy of the therapy and thus some care is needed when oncolytic virotherapy is combined with immunotherapy. This further suggests the importance of clinically improving the modulation of the immune response according to the tumour's characteristics and to the immune capabilities of the patients.","sentences":["Oncolytic virotherapy, utilizing genetically modified viruses to combat cancer and trigger anti-cancer immune responses, has garnered significant attention in recent years.","In our previous work arXiv:2305.12386, we developed a stochastic agent-based model elucidating the spatial dynamics of infected and uninfected cells within solid tumours.","Building upon this foundation, we present a novel stochastic agent-based model to describe the intricate interplay between the virus and the immune system; the agents' dynamics are coupled with a balance equation for the concentration of the chemoattractant that guides the movement of immune cells.","We formally derive the continuum limit of the model and carry out a systematic quantitative comparison between this system of PDEs and the individual-based model in two spatial dimensions.","Furthermore, we describe the traveling waves of the three populations, with the uninfected proliferative cells trying to escape from the infected cells while immune cells infiltrate the tumour.   ","Simulations show a good agreement between agent-based approaches and numerical results for the continuum model.","Some parameter ranges give rise to oscillations of cell number in both models, in line with the behaviour of the corresponding nonspatial model, which presents Hopf bifurcations.","Nevertheless, in some situations the behaviours of the two models may differ significantly, suggesting that stochasticity plays a key role in the dynamics.","Our results highlight that a too rapid immune response, before the infection is well-established, appears to decrease the efficacy of the therapy and thus some care is needed when oncolytic virotherapy is combined with immunotherapy.","This further suggests the importance of clinically improving the modulation of the immune response according to the tumour's characteristics and to the immune capabilities of the patients."],"url":"http://arxiv.org/abs/2404.06459v1","category":"q-bio.PE"}
{"created":"2024-04-09 16:55:56","title":"Sharp Propagation of Chaos for the Ensemble Langevin Sampler","abstract":"The aim of this note is to revisit propagation of chaos for a Langevin-type interacting particle system used for sampling probability measures. The interacting particle system we consider coincides, in the setting of a log-quadratic target distribution, with the ensemble Kalman sampler, for which propagation of chaos was first proved by Ding and Li. Like these authors, we prove propagation of chaos using a synchronous coupling as a starting point, as in Sznitman's classical argument. Instead of relying on a boostrapping argument, however, we use a technique based on stopping times in order to handle the presence of the empirical covariance in the coefficients of the dynamics. This approach originates from numerical analysis and was recently employed to prove mean field limits for consensus-based optimization and related interacting particle systems. In the context of ensemble Langevin sampling, it enables proving pathwise propagation of chaos with optimal rate, whereas previous results were optimal only up to a positive epsilon.","sentences":["The aim of this note is to revisit propagation of chaos for a Langevin-type interacting particle system used for sampling probability measures.","The interacting particle system we consider coincides, in the setting of a log-quadratic target distribution, with the ensemble Kalman sampler, for which propagation of chaos was first proved by Ding and Li.","Like these authors, we prove propagation of chaos using a synchronous coupling as a starting point, as in Sznitman's classical argument.","Instead of relying on a boostrapping argument, however, we use a technique based on stopping times in order to handle the presence of the empirical covariance in the coefficients of the dynamics.","This approach originates from numerical analysis and was recently employed to prove mean field limits for consensus-based optimization and related interacting particle systems.","In the context of ensemble Langevin sampling, it enables proving pathwise propagation of chaos with optimal rate, whereas previous results were optimal only up to a positive epsilon."],"url":"http://arxiv.org/abs/2404.06456v1","category":"math.PR"}
{"created":"2024-04-09 16:53:52","title":"PAAM: A Framework for Coordinated and Priority-Driven Accelerator Management in ROS 2","abstract":"This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform. The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs. PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer. This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems. The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers. The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access. This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains.","sentences":["This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform.","The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs.","PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer.","This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems.","The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers.","The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access.","This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains."],"url":"http://arxiv.org/abs/2404.06452v1","category":"cs.RO"}
{"created":"2024-04-09 16:25:06","title":"A New Hotplug Coded Caching Scheme Using PDAs","abstract":"In the original coded caching model introduced by Maddah-Ali and Niesen in 2014, the server starts broadcasting only after it receives demands from all the users. So, all the users must be active during the delivery phase. In this work, we consider a coded caching model called hotplug coded caching in which some of the users are offline during the delivery phase. This model was first introduced by Ma and Tuninetti (``On Coded Caching Systems with Offline Users,\" 2022 IEEE International Symposium on Information Theory). The concept of Hotplug Placement Delivery Arrays (HpPDAs) for the hotplug coded caching systems was introduced in (``Improved Hotplug Caching Schemes Using PDAs and $t$-Designs,\" \\emph{arXiv:2311.02856}, 2024), in which the authors have constructed HpPDAs from $t$-designs. This work provides a new hotplug coded caching scheme from the existing HpPDAs. The performance comparison of the proposed scheme with the existing schemes is presented. When applied for HpPDAs from $t$-designs, our scheme outperforms the baseline scheme by Ma and Tuninetti, and the Improved $t$-scheme by Rajput and Rajan in some memory regimes.","sentences":["In the original coded caching model introduced by Maddah-Ali and Niesen in 2014, the server starts broadcasting only after it receives demands from all the users.","So, all the users must be active during the delivery phase.","In this work, we consider a coded caching model called hotplug coded caching in which some of the users are offline during the delivery phase.","This model was first introduced by Ma and Tuninetti (``On Coded Caching Systems with Offline Users,\" 2022 IEEE International Symposium on Information Theory).","The concept of Hotplug Placement Delivery Arrays (HpPDAs) for the hotplug coded caching systems was introduced in (``Improved Hotplug Caching Schemes Using PDAs and $t$-Designs,\" \\emph{arXiv:2311.02856}, 2024), in which the authors have constructed HpPDAs from $t$-designs.","This work provides a new hotplug coded caching scheme from the existing HpPDAs.","The performance comparison of the proposed scheme with the existing schemes is presented.","When applied for HpPDAs from $t$-designs, our scheme outperforms the baseline scheme by Ma and Tuninetti, and the Improved $t$-scheme by Rajput and Rajan in some memory regimes."],"url":"http://arxiv.org/abs/2404.06433v1","category":"cs.IT"}
{"created":"2024-04-09 16:12:22","title":"Echo Chambers in the Age of Algorithms: An Audit of Twitter's Friend Recommender System","abstract":"The presence of political misinformation and ideological echo chambers on social media platforms is concerning given the important role that these sites play in the public's exposure to news and current events. Algorithmic systems employed on these platforms are presumed to play a role in these phenomena, but little is known about their mechanisms and effects. In this work, we conduct an algorithmic audit of Twitter's Who-To-Follow friend recommendation system, the first empirical audit that investigates the impact of this algorithm in-situ. We create automated Twitter accounts that initially follow left and right affiliated U.S. politicians during the 2022 U.S. midterm elections and then grow their information networks using the platform's recommender system. We pair the experiment with an observational study of Twitter users who already follow the same politicians. Broadly, we find that while following the recommendation algorithm leads accounts into dense and reciprocal neighborhoods that structurally resemble echo chambers, the recommender also results in less political homogeneity of a user's network compared to accounts growing their networks through social endorsement. Furthermore, accounts that exclusively followed users recommended by the algorithm had fewer opportunities to encounter content centered on false or misleading election narratives compared to choosing friends based on social endorsement.","sentences":["The presence of political misinformation and ideological echo chambers on social media platforms is concerning given the important role that these sites play in the public's exposure to news and current events.","Algorithmic systems employed on these platforms are presumed to play a role in these phenomena, but little is known about their mechanisms and effects.","In this work, we conduct an algorithmic audit of Twitter's Who-To-Follow friend recommendation system, the first empirical audit that investigates the impact of this algorithm in-situ.","We create automated Twitter accounts that initially follow left and right affiliated U.S. politicians during the 2022 U.S. midterm elections and then grow their information networks using the platform's recommender system.","We pair the experiment with an observational study of Twitter users who already follow the same politicians.","Broadly, we find that while following the recommendation algorithm leads accounts into dense and reciprocal neighborhoods that structurally resemble echo chambers, the recommender also results in less political homogeneity of a user's network compared to accounts growing their networks through social endorsement.","Furthermore, accounts that exclusively followed users recommended by the algorithm had fewer opportunities to encounter content centered on false or misleading election narratives compared to choosing friends based on social endorsement."],"url":"http://arxiv.org/abs/2404.06422v1","category":"cs.SI"}
{"created":"2024-04-09 16:03:26","title":"Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems","abstract":"Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.","sentences":["Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy.","Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks.","Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution.","We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along.","A graph neural network (GNN) based low-level distributed control policy executes the assigned plan.","We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks.","In particular, as part of prompt engineering, we provide in-context examples for LLMs.","We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles.","Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS."],"url":"http://arxiv.org/abs/2404.06413v1","category":"cs.RO"}
{"created":"2024-04-09 15:56:12","title":"Helium Reionization from Empirical Quasar Luminosity Functions before and after JWST","abstract":"Recently, models of the quasar luminosity function (QLF) rooted on large observational compilations have been produced that, unlike their predecessors, feature a smooth evolution with time. This bypasses the need to assume an ionizing emissivity evolution when simulating helium reionization with observations-based QLF, thus yielding more robust constraints. We combine one such QLF with a cosmological hydrodynamical simulation and 3D multi-frequency radiative transfer. The simulated reionization history is consistently delayed in comparison to most other models in the literature. The predicted intergalactic medium temperature is larger than the observed one at $z \\lesssim 3$. Through forward modeling of the He II Lyman-$\\alpha$ forest, we show that our model produces an extended helium reionization and successfully matches the bulk of the observed effective optical depth distribution, although it over-ionizes the Universe at $z\\lesssim2.8$ as the effect of small-scale Lyman Limit Systems not being resolved. We thoroughly characterize transmission regions and dark gaps in He II Lyman-$\\alpha$ forest sightlines. We quantify their sensitivity to the helium reionization, opening a new avenue for further observational studies of this epoch. Finally, we explore the implications for helium reionization of the large number of active galactic nuclei revealed at $z\\gtrsim5$ by JWST. We find that such modifications do not affect any observable at $z\\leq4$, except in our most extreme model, indicating that the observed abundance of high-$z$ AGNs does not bear consequences for helium reionization.","sentences":["Recently, models of the quasar luminosity function (QLF) rooted on large observational compilations have been produced that, unlike their predecessors, feature a smooth evolution with time.","This bypasses the need to assume an ionizing emissivity evolution when simulating helium reionization with observations-based QLF, thus yielding more robust constraints.","We combine one such QLF with a cosmological hydrodynamical simulation and 3D multi-frequency radiative transfer.","The simulated reionization history is consistently delayed in comparison to most other models in the literature.","The predicted intergalactic medium temperature is larger than the observed one at $z \\lesssim 3$. Through forward modeling of the He II Lyman-$\\alpha$ forest, we show that our model produces an extended helium reionization and successfully matches the bulk of the observed effective optical depth distribution, although it over-ionizes the Universe at $z\\lesssim2.8$ as the effect of small-scale Lyman Limit Systems not being resolved.","We thoroughly characterize transmission regions and dark gaps in He II Lyman-$\\alpha$ forest sightlines.","We quantify their sensitivity to the helium reionization, opening a new avenue for further observational studies of this epoch.","Finally, we explore the implications for helium reionization of the large number of active galactic nuclei revealed at $z\\gtrsim5$ by JWST.","We find that such modifications do not affect any observable at $z\\leq4$, except in our most extreme model, indicating that the observed abundance of high-$z$ AGNs does not bear consequences for helium reionization."],"url":"http://arxiv.org/abs/2404.06409v1","category":"astro-ph.GA"}
{"created":"2024-04-09 15:45:14","title":"Three ways to decipher the nature of exotic hadrons: multiplets, three-body hadronic molecules, and correlation functions","abstract":"In the past two decades, a plethora of hadronic states beyond the conventional quark model of $q\\bar{q}$ mesons and $qqq$ baryons have been observed experimentally, which motivated extensive studies to understand their nature and the non-perturbative strong interaction. Since most of these exotic states are near the mass thresholds of a pair of conventional hadrons, the prevailing picture is that they are primarily hadronic molecules. In principle, one can verify the molecular nature of these states by thoroughly comparing their masses, decay widths, and production rates in a particular picture with experimental data. However, this is difficult or impossible. First, quantum mechanics allows for the mixing of configurations allowed by symmetries and quantum numbers. Second, data are relatively scarce because of their small production rates and the many difficulties in the experimental measurements. As a result, other alternatives need to be explored. This review summarizes three such approaches that can help disentangle the nature of the many exotic hadrons discovered. In the first approach, based on the molecular interpretations for some exotic states, we study the likely existence of multiplets of hadronic molecules related by various symmetries, such as isospin symmetry, SU(3)-flavor symmetry, heavy quark spin/flavor symmetry, and heavy antiquark diquark symmetry. In the second approach, starting from some hadronic molecular candidates, one can derive the underlying hadron-hadron interactions. With these interactions, one can study related three-body systems and check whether three-body bound states/resonances exist. In the third approach, one can turn to the femtoscopy technique to derive the hadron-hadron interactions, hence inaccessible. This technique provided an unprecedented opportunity to understand the interactions between unstable hadrons.","sentences":["In the past two decades, a plethora of hadronic states beyond the conventional quark model of $q\\bar{q}$ mesons and $qqq$ baryons have been observed experimentally, which motivated extensive studies to understand their nature and the non-perturbative strong interaction.","Since most of these exotic states are near the mass thresholds of a pair of conventional hadrons, the prevailing picture is that they are primarily hadronic molecules.","In principle, one can verify the molecular nature of these states by thoroughly comparing their masses, decay widths, and production rates in a particular picture with experimental data.","However, this is difficult or impossible.","First, quantum mechanics allows for the mixing of configurations allowed by symmetries and quantum numbers.","Second, data are relatively scarce because of their small production rates and the many difficulties in the experimental measurements.","As a result, other alternatives need to be explored.","This review summarizes three such approaches that can help disentangle the nature of the many exotic hadrons discovered.","In the first approach, based on the molecular interpretations for some exotic states, we study the likely existence of multiplets of hadronic molecules related by various symmetries, such as isospin symmetry, SU(3)-flavor symmetry, heavy quark spin/flavor symmetry, and heavy antiquark diquark symmetry.","In the second approach, starting from some hadronic molecular candidates, one can derive the underlying hadron-hadron interactions.","With these interactions, one can study related three-body systems and check whether three-body bound states/resonances exist.","In the third approach, one can turn to the femtoscopy technique to derive the hadron-hadron interactions, hence inaccessible.","This technique provided an unprecedented opportunity to understand the interactions between unstable hadrons."],"url":"http://arxiv.org/abs/2404.06399v1","category":"hep-ph"}
{"created":"2024-04-09 15:43:38","title":"Integrated electro-optics on thin-film lithium niobate","abstract":"Electro-optics serves as the crucial bridge between electronics and photonics, unlocking a wide array of applications ranging from communications and computing to sensing and quantum information. Integrated electro-optics approaches in particular enable essential electronic high-speed control for photonics while offering substantial photonic parallelism for electronics. Recent strides in thin-film lithium niobate photonics have ushered revolutionary advancements in electro-optics. This technology not only offers the requisite strong electro-optic coupling but also boasts ultra-low optical loss and high microwave bandwidth. Further, its tight confinement and compatibility with nanofabrication allow for unprecedented reconfigurability and scalability, facilitating the creation of novel and intricate devices and systems that were once deemed nearly impossible in bulk systems. Building upon this platform, the field has witnessed the emergence of various groundbreaking electro-optic devices surpassing the current state of the art, and introducing functionalities that were previously non-existent. This technological leap forward provides a unique framework to explore various realms of physics as well, including photonic non-Hermitian synthetic dimensions, active topological physics, and quantum electro-optics. In this review, we present the fundamental principles of electro-optics, drawing connections between fundamental science and the forefront of technology. We discuss the accomplishments and future prospects of integrated electro-optics, enabled by thin-film lithium niobate platform.","sentences":["Electro-optics serves as the crucial bridge between electronics and photonics, unlocking a wide array of applications ranging from communications and computing to sensing and quantum information.","Integrated electro-optics approaches in particular enable essential electronic high-speed control for photonics while offering substantial photonic parallelism for electronics.","Recent strides in thin-film lithium niobate photonics have ushered revolutionary advancements in electro-optics.","This technology not only offers the requisite strong electro-optic coupling but also boasts ultra-low optical loss and high microwave bandwidth.","Further, its tight confinement and compatibility with nanofabrication allow for unprecedented reconfigurability and scalability, facilitating the creation of novel and intricate devices and systems that were once deemed nearly impossible in bulk systems.","Building upon this platform, the field has witnessed the emergence of various groundbreaking electro-optic devices surpassing the current state of the art, and introducing functionalities that were previously non-existent.","This technological leap forward provides a unique framework to explore various realms of physics as well, including photonic non-Hermitian synthetic dimensions, active topological physics, and quantum electro-optics.","In this review, we present the fundamental principles of electro-optics, drawing connections between fundamental science and the forefront of technology.","We discuss the accomplishments and future prospects of integrated electro-optics, enabled by thin-film lithium niobate platform."],"url":"http://arxiv.org/abs/2404.06398v1","category":"physics.optics"}
{"created":"2024-04-09 15:41:30","title":"New Contributions to $b \\to s \u03b3$ in Minimal G2HDM","abstract":"We study the flavor-changing bottom quark radiative decay $b \\to s \\gamma$ induced at one-loop level within the minimal gauged two-Higgs-doublet model (G2HDM). Among the three new contributions to this rare process in G2HDM, we find that only the charged Higgs $\\mathcal{H^\\pm}$ contribution can be constrained by the current global fit data in $B$-physics. Other two contributions from the complex vectorial dark matter $\\mathcal{W}$ and dark Higgs $\\mathcal{D}$ are not sensitive to the current data. Combining with theoretical constraints imposed on the scalar potential and electroweak precision data for the oblique parameters, we exclude mass regions $m_{\\mathcal{H}^\\pm} \\lesssim 250$ GeV and $m_{\\mathcal{D}} \\lesssim 100$ GeV at the 95\\% confidence level.","sentences":["We study the flavor-changing bottom quark radiative decay $b \\to s \\gamma$ induced at one-loop level within the minimal gauged two-Higgs-doublet model (G2HDM).","Among the three new contributions to this rare process in G2HDM, we find that only the charged Higgs $\\mathcal{H^\\pm}$ contribution can be constrained by the current global fit data in $B$-physics.","Other two contributions from the complex vectorial dark matter $\\mathcal{W}$ and dark Higgs $\\mathcal{D}$ are not sensitive to the current data.","Combining with theoretical constraints imposed on the scalar potential and electroweak precision data for the oblique parameters, we exclude mass regions $m_{\\mathcal{H}^\\pm} \\lesssim 250$ GeV and $m_{\\mathcal{D}} \\lesssim 100$ GeV at the 95\\% confidence level."],"url":"http://arxiv.org/abs/2404.06397v1","category":"hep-ph"}
{"created":"2024-04-09 15:29:16","title":"The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning","abstract":"Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems. However, misaligned agents can exploit other agents' trust and delegated power to the communication medium. In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents. Power is a measure of the influence one agent's actions have over another agent's policy. By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication. We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication.","sentences":["Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems.","However, misaligned agents can exploit other agents' trust and delegated power to the communication medium.","In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents.","Power is a measure of the influence one agent's actions have over another agent's policy.","By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication.","We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication."],"url":"http://arxiv.org/abs/2404.06387v1","category":"cs.MA"}
{"created":"2024-04-09 15:28:31","title":"Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements","abstract":"Context and Motivation Attack-Defense Trees (ADTs) are a graphical notation used to model and assess security requirements. ADTs are widely popular, as they can facilitate communication between different stakeholders involved in system security evaluation, and they are formal enough to be verified, e.g., with model checkers. Question/Problem While the quality of this notation has been primarily assessed quantitatively, its understandability has never been evaluated despite being mentioned as a key factor for its success. Principal idea/Results In this paper, we conduct an experiment with 25 human subjects to assess the understandability and user acceptance of the ADT notation. The study focuses on performance-based variables and perception-based variables, with the aim of evaluating the relationship between these measures and how they might impact the practical use of the notation. The results confirm a good level of understandability of ADTs. Participants consider them useful, and they show intention to use them. Contribution This is the first study empirically supporting the understandability of ADTs, thereby contributing to the theory of security requirements engineering.","sentences":["Context and Motivation Attack-Defense Trees (ADTs) are a graphical notation used to model and assess security requirements.","ADTs are widely popular, as they can facilitate communication between different stakeholders involved in system security evaluation, and they are formal enough to be verified, e.g., with model checkers.","Question/Problem While the quality of this notation has been primarily assessed quantitatively, its understandability has never been evaluated despite being mentioned as a key factor for its success.","Principal idea/Results In this paper, we conduct an experiment with 25 human subjects to assess the understandability and user acceptance of the ADT notation.","The study focuses on performance-based variables and perception-based variables, with the aim of evaluating the relationship between these measures and how they might impact the practical use of the notation.","The results confirm a good level of understandability of ADTs.","Participants consider them useful, and they show intention to use them.","Contribution","This is the first study empirically supporting the understandability of ADTs, thereby contributing to the theory of security requirements engineering."],"url":"http://arxiv.org/abs/2404.06386v1","category":"cs.SE"}
{"created":"2024-04-09 15:28:18","title":"Toward Reliable Dipole Moments without Single Excitations: The Role of Orbital Rotations and Dynamical Correlation","abstract":"The dipole moment is a crucial molecular property linked to a molecular system's bond polarity and overall electronic structure. To that end, the electronic dipole moment, which results from the electron density of a system, is often used to assess the accuracy and reliability of new electronic structure methods. This work analyses electronic dipole moments computed with the pair coupled cluster doubles (pCCD) ansatz and its linearized coupled cluster (pCCD-LCC) corrections using the canonical Hartree--Fock and pCCD-optimized (localized) orbital bases. The accuracy of pCCD-based dipole moments is assessed against experimental and CCSD(T) reference values using relaxed and unrelaxed density matrices and different basis set sizes. Our test set comprises molecules of various bonding patterns and electronic structures, exposing pCCD-based methods to a wide range of electron correlation effects. Additionally, we investigate the performance of pCCD-in-DFT dipole moments of some model complexes. Finally, our work indicates the importance of orbital relaxation in the pCCD model and shows the limitations of the linearized couple cluster corrections in predicting electronic dipole moments of multiple-bonded systems. Most importantly, pCCD with a linearized CCD correction can reproduce the dipole moment surfaces in singly-bonded molecules, which are comparable to the multi-reference ones.","sentences":["The dipole moment is a crucial molecular property linked to a molecular system's bond polarity and overall electronic structure.","To that end, the electronic dipole moment, which results from the electron density of a system, is often used to assess the accuracy and reliability of new electronic structure methods.","This work analyses electronic dipole moments computed with the pair coupled cluster doubles (pCCD) ansatz and its linearized coupled cluster (pCCD-LCC) corrections using the canonical Hartree--Fock and pCCD-optimized (localized) orbital bases.","The accuracy of pCCD-based dipole moments is assessed against experimental and CCSD(T) reference values using relaxed and unrelaxed density matrices and different basis set sizes.","Our test set comprises molecules of various bonding patterns and electronic structures, exposing pCCD-based methods to a wide range of electron correlation effects.","Additionally, we investigate the performance of pCCD-in-DFT dipole moments of some model complexes.","Finally, our work indicates the importance of orbital relaxation in the pCCD model and shows the limitations of the linearized couple cluster corrections in predicting electronic dipole moments of multiple-bonded systems.","Most importantly, pCCD with a linearized CCD correction can reproduce the dipole moment surfaces in singly-bonded molecules, which are comparable to the multi-reference ones."],"url":"http://arxiv.org/abs/2404.06385v1","category":"physics.chem-ph"}
{"created":"2024-04-09 15:27:58","title":"Master symmetries of non-linear systems","abstract":"We explore new symmetries in two-component third-order Burgers' type systems in (1+1)-dimension using Wang's O-scheme. We also find a master symmetry for a (2+1)-dimensional Davey-Stewartson type system. These results shed light on the behavior of these equations and help us understand their integrability properties. Our approach offers a practical method for identifying symmetries, contributing to the study of integrable systems in mathematics and physics.","sentences":["We explore new symmetries in two-component third-order Burgers' type systems in (1+1)-dimension using Wang's O-scheme.","We also find a master symmetry for a (2+1)-dimensional Davey-Stewartson type system.","These results shed light on the behavior of these equations and help us understand their integrability properties.","Our approach offers a practical method for identifying symmetries, contributing to the study of integrable systems in mathematics and physics."],"url":"http://arxiv.org/abs/2404.06384v1","category":"nlin.SI"}
{"created":"2024-04-09 15:26:26","title":"Maximum Degree in Random Hyperbolic Graphs","abstract":"The random hyperbolic graph, introduced in 2010 by Krioukov, Papadopoulos, Kitsak, Vahdat and Bogu\\~{n}\\'a, is a graph model suitable for modelling a large class of real-world networks, known as complex networks. Gugelmann, Panagiotou and Peter proved that for curvature parameter $\\alpha > 1/2$, the degree sequence of the random hyperbolic graph follows a power-law distribution with controllable exponent up to the maximum degree. To achieve this, they showed, among other results, that with high probability, the maximum degree is $n^{\\frac{1}{2\\alpha} + o(1)}$, where $n$ is the number of nodes. In this paper, we refine this estimate of the maximum degree, and we extend it to the case $\\alpha \\leq 1/2$: we first show that, with high probability, the node with the maximum degree is eventually the one that is the closest to the origin of the underlying hyperbolic space. From this, we get the convergence in distribution of the renormalised maximum degree. Except for the critical case $\\alpha = 1/2$, the limit distribution belongs to the extreme value distribution family (Weibull distribution in the case $\\alpha < 1/2$ and Fr\\'echet distribution in the case $\\alpha > 1/2$).","sentences":["The random hyperbolic graph, introduced in 2010 by Krioukov, Papadopoulos, Kitsak, Vahdat and Bogu\\~{n}\\'a, is a graph model suitable for modelling a large class of real-world networks, known as complex networks.","Gugelmann, Panagiotou and Peter proved that for curvature parameter $\\alpha > 1/2$, the degree sequence of the random hyperbolic graph follows a power-law distribution with controllable exponent up to the maximum degree.","To achieve this, they showed, among other results, that with high probability, the maximum degree is $n^{\\frac{1}{2\\alpha} + o(1)}$, where $n$ is the number of nodes.","In this paper, we refine this estimate of the maximum degree, and we extend it to the case $\\alpha \\leq 1/2$: we first show that, with high probability, the node with the maximum degree is eventually the one that is the closest to the origin of the underlying hyperbolic space.","From this, we get the convergence in distribution of the renormalised maximum degree.","Except for the critical case $\\alpha = 1/2$, the limit distribution belongs to the extreme value distribution family (Weibull distribution in the case $\\alpha < 1/2$ and Fr\\'echet distribution in the case $\\alpha > 1/2$)."],"url":"http://arxiv.org/abs/2404.06383v1","category":"math.PR"}
{"created":"2024-04-09 15:24:53","title":"Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks","abstract":"Arterial traffic interacts with freeway traffic, yet the two are controlled independently. Arterial traffic signals do not take into account freeway traffic and how ramps control ingress traffic and have no control over egress traffic from the freeway. This often results in long queues in either direction that block ramps and spill over to arterial streets or freeway lanes. In this paper, we propose an adaptive arterial traffic control strategy that combines traffic signal control (TSC) and dynamic speed offset (DSO) coordination using a Q-learning algorithm for a traffic network that involves a freeway segment and adjacent arterial streets. The TSC agent computes the signal cycle length and split based on observed intersection demands and adjacent freeway off-ramp queues. The DSO agent computes the relative offset and the recommended speeds of both ways between consecutive intersections based on their physical distance, intersection queues, and signal cycles. We evaluate the performance of the proposed arterial traffic control strategy using microscopic traffic simulations of an arterial corridor with seven intersections near the I-710 freeway. The proposed QL-based control significantly outperforms a fixed-time control and MAXBAND in terms of the travel time and the number of stops under low or moderate demands. In high-demand scenarios, the travel-time benefit provided by the QL-based control is reduced as it mitigates off-ramp and intersection queues, which is a necessary trade-off in our perspective. In addition, mutual benefit is obtained by implementing freeway and arterial traffic control simultaneously.","sentences":["Arterial traffic interacts with freeway traffic, yet the two are controlled independently.","Arterial traffic signals do not take into account freeway traffic and how ramps control ingress traffic and have no control over egress traffic from the freeway.","This often results in long queues in either direction that block ramps and spill over to arterial streets or freeway lanes.","In this paper, we propose an adaptive arterial traffic control strategy that combines traffic signal control (TSC) and dynamic speed offset (DSO) coordination using a Q-learning algorithm for a traffic network that involves a freeway segment and adjacent arterial streets.","The TSC agent computes the signal cycle length and split based on observed intersection demands and adjacent freeway off-ramp queues.","The DSO agent computes the relative offset and the recommended speeds of both ways between consecutive intersections based on their physical distance, intersection queues, and signal cycles.","We evaluate the performance of the proposed arterial traffic control strategy using microscopic traffic simulations of an arterial corridor with seven intersections near the I-710 freeway.","The proposed QL-based control significantly outperforms a fixed-time control and MAXBAND in terms of the travel time and the number of stops under low or moderate demands.","In high-demand scenarios, the travel-time benefit provided by the QL-based control is reduced as it mitigates off-ramp and intersection queues, which is a necessary trade-off in our perspective.","In addition, mutual benefit is obtained by implementing freeway and arterial traffic control simultaneously."],"url":"http://arxiv.org/abs/2404.06382v1","category":"eess.SY"}
{"created":"2024-04-09 15:23:32","title":"Asymptotic-preserving finite difference method for partially dissipative hyperbolic systems","abstract":"In this paper, we analyze the preservation of asymptotic properties of partially dissipative hyperbolic systems when switching to a discrete setting. We prove that one of the simplest consistent and unconditionally stable numerical methods - the central finite-differences scheme - preserves both the asymptotic behaviour and the parabolic relaxation limit of one-dimensional partially dissipative hyperbolic systems which satisfy the Kalman rank condition.   The large time asymptotic-preserving property is achieved by conceiving time-weighted perturbed energy functionals in the spirit of the hypocoercivity theory. For the relaxation-preserving property, drawing inspiration from the observation that solutions in the continuous case exhibit distinct behaviours in low and high frequencies, we introduce a novel discrete Littlewood-Paley theory tailored to the central finite-difference scheme. This allows us to prove Bernstein-type estimates for discrete differential operators and leads to a new relaxation result: the strong convergence of the discrete linearized compressible Euler equations with damping towards the discrete heat equation, uniformly with respect to the mesh parameter.","sentences":["In this paper, we analyze the preservation of asymptotic properties of partially dissipative hyperbolic systems when switching to a discrete setting.","We prove that one of the simplest consistent and unconditionally stable numerical methods - the central finite-differences scheme - preserves both the asymptotic behaviour and the parabolic relaxation limit of one-dimensional partially dissipative hyperbolic systems which satisfy the Kalman rank condition.   ","The large time asymptotic-preserving property is achieved by conceiving time-weighted perturbed energy functionals in the spirit of the hypocoercivity theory.","For the relaxation-preserving property, drawing inspiration from the observation that solutions in the continuous case exhibit distinct behaviours in low and high frequencies, we introduce a novel discrete Littlewood-Paley theory tailored to the central finite-difference scheme.","This allows us to prove Bernstein-type estimates for discrete differential operators and leads to a new relaxation result: the strong convergence of the discrete linearized compressible Euler equations with damping towards the discrete heat equation, uniformly with respect to the mesh parameter."],"url":"http://arxiv.org/abs/2404.06380v1","category":"math.AP"}
{"created":"2024-04-09 15:21:14","title":"Analyzing the Atmospheric Dispersion Correction of the Gemini Planet Imager: residual dispersion above design requirements","abstract":"The Atmospheric Dispersion Corrector (ADC) of the Gemini Planet Imager (GPI) corrects the chromatic dispersion caused by differential atmospheric refraction (DAR), making it an important optic for exoplanet observation. Despite requiring less than 5 mas of residual DAR to avoid potentially affecting the coronagraph, the GPI ADC averages $\\sim7$ and $\\sim11$ mas of residual DAR in $H$ and $J$ band respectively. We analyzed GPI data in those bands to find explanations for the underperformance. We found the model GPI uses to predict DAR underestimates humidity's impact on incident DAR, causing on average a 0.54 mas increase in $H$ band residual DAR. Additionally, the GPI ADC consistently undercorrects in $H$ band by about 7 mas, causing almost all the $H$ band residual DAR. $J$ band does not have such an offset. Perpendicular dispersion induced by the GPI ADC, potentially from a misalignment in the prisms' relative orientation, causes 86% of the residual DAR in $J$ band. Correcting these issues could reduce residual DAR, thereby improving exoplanet detection. We also made a new approximation for the index of refraction of air from 0.7 microns to 1.36 microns that more accurately accounts for the effects of humidity.","sentences":["The Atmospheric Dispersion Corrector (ADC) of the Gemini Planet Imager (GPI) corrects the chromatic dispersion caused by differential atmospheric refraction (DAR), making it an important optic for exoplanet observation.","Despite requiring less than 5 mas of residual DAR to avoid potentially affecting the coronagraph, the GPI ADC averages $\\sim7$ and $\\sim11$ mas of residual DAR in $H$ and $J$ band respectively.","We analyzed GPI data in those bands to find explanations for the underperformance.","We found the model GPI uses to predict DAR underestimates humidity's impact on incident DAR, causing on average a 0.54 mas increase in $H$ band residual DAR.","Additionally, the GPI ADC consistently undercorrects in $H$ band by about 7 mas, causing almost all the $H$ band residual DAR.","$J$ band does not have such an offset.","Perpendicular dispersion induced by the GPI ADC, potentially from a misalignment in the prisms' relative orientation, causes 86% of the residual DAR in $J$ band.","Correcting these issues could reduce residual DAR, thereby improving exoplanet detection.","We also made a new approximation for the index of refraction of air from 0.7 microns to 1.36 microns that more accurately accounts for the effects of humidity."],"url":"http://arxiv.org/abs/2404.06378v1","category":"astro-ph.IM"}
{"created":"2024-04-09 15:21:12","title":"Experimental study of second sound quench detection for superconducting cavities","abstract":"Superconducting RF cavities are used in particle accelerators to provide energy to the particle beam. Such cavities are mostly fabricated in niobium and often operated in superfluid helium. One of their limits of operation is the appearance of a local quench, initiated by a local field enhancement due to a defect, which leads to a normal conducting transition of the cavity. Localizing the quench area can be achieved with temperature mapping systems. Another method is the use of second sound wave propagation in superfluid helium. Measuring the time of propagation of these waves from quench location to special sensors, called Oscillating Superleak Transducers (OSTs), and using their well-known velocity should allow trilateration. However, most of experimental measurements on cavities show \"premature signals\", i.e. the second sound signals arrive earlier on the OSTs than expected. This paper presents several quench experiments on cavities equipped with OSTs and temperature mapping quench detection systems. Two hypotheses can explain the observed premature signals. The first one assesses faster propagation in helium. An experimental setup has been developed for testing this hypothesis, where second sound is created by a localized heater in a controlled environment up to 4.3 kW/cm2 and 2.8 J. Premature signals could not be verified in this setup. A second hypothesis based on a simple model including several processes in niobium and second sound propagation in helium is discussed. The model improves significantly the prediction of the times of arrival of the second sound waves. The overall study shows that the processes in niobium play a prominent role in the second sound detection for superconducting cavities.","sentences":["Superconducting RF cavities are used in particle accelerators to provide energy to the particle beam.","Such cavities are mostly fabricated in niobium and often operated in superfluid helium.","One of their limits of operation is the appearance of a local quench, initiated by a local field enhancement due to a defect, which leads to a normal conducting transition of the cavity.","Localizing the quench area can be achieved with temperature mapping systems.","Another method is the use of second sound wave propagation in superfluid helium.","Measuring the time of propagation of these waves from quench location to special sensors, called Oscillating Superleak Transducers (OSTs), and using their well-known velocity should allow trilateration.","However, most of experimental measurements on cavities show \"premature signals\", i.e. the second sound signals arrive earlier on the OSTs than expected.","This paper presents several quench experiments on cavities equipped with OSTs and temperature mapping quench detection systems.","Two hypotheses can explain the observed premature signals.","The first one assesses faster propagation in helium.","An experimental setup has been developed for testing this hypothesis, where second sound is created by a localized heater in a controlled environment up to 4.3 kW/cm2 and 2.8 J. Premature signals could not be verified in this setup.","A second hypothesis based on a simple model including several processes in niobium and second sound propagation in helium is discussed.","The model improves significantly the prediction of the times of arrival of the second sound waves.","The overall study shows that the processes in niobium play a prominent role in the second sound detection for superconducting cavities."],"url":"http://arxiv.org/abs/2404.06377v1","category":"physics.acc-ph"}
{"created":"2024-04-09 15:01:51","title":"SurveyAgent: A Conversational System for Personalized and Efficient Research Survey","abstract":"In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent's effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.","sentences":["In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers.","Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking.","This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers.","SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level.","This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization.","Our evaluation demonstrates SurveyAgent's effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature."],"url":"http://arxiv.org/abs/2404.06364v1","category":"cs.CL"}
{"created":"2024-04-09 14:44:58","title":"On harmonic maps from the complex plane to hyperbolic 3-space","abstract":"For any twisted ideal polygon in $\\mathbb{H}^3$, we construct a harmonic map from $\\mathbb{C}$ to $\\mathbb{H}^3$ with a polynomial Hopf differential, that is asymptotic to the given polygon, and is a bounded distance from a pleated plane. Our proof uses the harmonic map heat flow. We also show that such a harmonic map is unique once we prescribe the principal part of its Hopf differential.","sentences":["For any twisted ideal polygon in $\\mathbb{H}^3$, we construct a harmonic map from $\\mathbb{C}$ to $\\mathbb{H}^3$ with a polynomial Hopf differential, that is asymptotic to the given polygon, and is a bounded distance from a pleated plane.","Our proof uses the harmonic map heat flow.","We also show that such a harmonic map is unique once we prescribe the principal part of its Hopf differential."],"url":"http://arxiv.org/abs/2404.06354v1","category":"math.DG"}
{"created":"2024-04-09 14:40:54","title":"Rolling Shutter Correction with Intermediate Distortion Flow Estimation","abstract":"This paper proposes to correct the rolling shutter (RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly. Existing methods usually perform correction using the undistortion flow from the RS to GS. They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors. Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart. Nevertheless, this strategy is prone to two shortcomings. First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature. Second, RS-aware forward warping often results in unavoidable artifacts. To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation. More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers. Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further. Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various benchmarks while maintaining high efficiency. The project is available at \\url{https://github.com/ljzycmd/DFRSC}.","sentences":["This paper proposes to correct the rolling shutter","(RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly.","Existing methods usually perform correction using the undistortion flow from the RS to GS.","They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors.","Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart.","Nevertheless, this strategy is prone to two shortcomings.","First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature.","Second, RS-aware forward warping often results in unavoidable artifacts.","To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation.","More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers.","Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further.","Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various benchmarks while maintaining high efficiency.","The project is available at \\url{https://github.com/ljzycmd/DFRSC}."],"url":"http://arxiv.org/abs/2404.06350v1","category":"cs.CV"}
{"created":"2024-04-09 14:30:59","title":"Does the Earth's rotation speed really tend to accelerate for a long time?","abstract":"Recently published studies suggested that the difference between Universal and Coordinated Time UT1-UTC could reach a large positive value in a few years, making it necessary to introduce a negative leap second into the UTC scale for the first time in its history. Based on the latest UT1 series provided by the International Earth Rotation and Reference Systems Service (IERS) and its prediction, it was shown that the tendency to acceleration of the Earth's rotation observed over past four years most likely will return to the deceleration, which is the usual behavior of the Earth rotational dynamics.","sentences":["Recently published studies suggested that the difference between Universal and Coordinated Time UT1-UTC could reach a large positive value in a few years, making it necessary to introduce a negative leap second into the UTC scale for the first time in its history.","Based on the latest UT1 series provided by the International Earth Rotation and Reference Systems Service (IERS) and its prediction, it was shown that the tendency to acceleration of the Earth's rotation observed over past four years most likely will return to the deceleration, which is the usual behavior of the Earth rotational dynamics."],"url":"http://arxiv.org/abs/2404.06343v1","category":"astro-ph.EP"}
{"created":"2024-04-09 14:25:52","title":"Sustainable and Precision Agriculture with the Internet of Everything (IoE)","abstract":"The accelerated pace of global population growth underscores the crucial role of the agricultural sector in mitigating food scarcity, as well as in supporting livelihoods through employment opportunities and bolstering national economies. This sector faces several critical challenges, including resource depletion, socioeconomic issues, gaps in technology and innovation, and the impact of climate change. The introduction of mechanization has significantly transformed agriculture by enhancing sustainability and increasing the productivity of crops. Recently, traditional farming methods have been supplemented with advanced technologies steering the industry towards precision agriculture. The convergence of these advanced technologies has facilitated the automation of various tasks such as water management, crop monitoring, disease management, and harvesting. The concept of Internet of Everything (IoE) has gained traction due to its holistic approach towards integrating various IoT specializations, called IoXs where X referring to a specific domain. This includes areas like the Internet of Sensors (IoS), Internet of Vehicles (IoV), Internet of Energy (IoEn), Internet of Space Things (IoST), Industrial Internet of Things (IIoT), and Internet of Drones (IoD). This paper explores the potential of the Internet of Everything (IoE) in revolutionizing agricultural systems. The focus is on assessing the impact of cutting-edge and novel technologies, such as 6G, molecular communication (MC), Internet of Nano Things (IoNT), Internet of Bio-Nano Things (IoBNT), Internet of Fungus, and designer phages, in significantly improving agricultural yield, efficiency, and productivity. Additionally, the potential of these technologies is evaluated in terms of their applicability, associated challenges, and future research directions within the realm of precision agriculture.","sentences":["The accelerated pace of global population growth underscores the crucial role of the agricultural sector in mitigating food scarcity, as well as in supporting livelihoods through employment opportunities and bolstering national economies.","This sector faces several critical challenges, including resource depletion, socioeconomic issues, gaps in technology and innovation, and the impact of climate change.","The introduction of mechanization has significantly transformed agriculture by enhancing sustainability and increasing the productivity of crops.","Recently, traditional farming methods have been supplemented with advanced technologies steering the industry towards precision agriculture.","The convergence of these advanced technologies has facilitated the automation of various tasks such as water management, crop monitoring, disease management, and harvesting.","The concept of Internet of Everything (IoE) has gained traction due to its holistic approach towards integrating various IoT specializations, called IoXs where X referring to a specific domain.","This includes areas like the Internet of Sensors (IoS), Internet of Vehicles (IoV), Internet of Energy (IoEn), Internet of Space Things (IoST), Industrial Internet of Things (IIoT), and Internet of Drones (IoD).","This paper explores the potential of the Internet of Everything (IoE) in revolutionizing agricultural systems.","The focus is on assessing the impact of cutting-edge and novel technologies, such as 6G, molecular communication (MC), Internet of Nano Things (IoNT), Internet of Bio-Nano Things (IoBNT), Internet of Fungus, and designer phages, in significantly improving agricultural yield, efficiency, and productivity.","Additionally, the potential of these technologies is evaluated in terms of their applicability, associated challenges, and future research directions within the realm of precision agriculture."],"url":"http://arxiv.org/abs/2404.06341v1","category":"eess.SP"}
{"created":"2024-04-09 14:25:31","title":"Experimental System Design of an Active Fault-Tolerant Quadrotor","abstract":"Quadrotors have gained popularity over the last decade, aiding humans in complex tasks such as search and rescue, mapping and exploration. Despite their mechanical simplicity and versatility compared to other types of aerial vehicles, they remain vulnerable to rotor failures. In this paper, we propose an algorithmic and mechanical approach to addressing the quadrotor fault-tolerant problem in case of rotor failures. First, we present a fault-tolerant detection and control scheme that includes various attitude error metrics. The scheme transitions to a fault-tolerant control mode by surrendering the yaw control. Subsequently, to ensure compatibility with platform sensing constraints, we investigate the relationship between variations in robot rotational drag, achieved through a modular mechanical design appendage, resulting in yaw rates within sensor limits. This analysis offers a platform-agnostic framework for designing more reliable and robust quadrotors in the event of rotor failures. Extensive experimental results validate the proposed approach providing insights into successfully designing a cost-effective quadrotor capable of fault-tolerant control. The overall design enhances safety in scenarios of faulty rotors, without the need for additional sensors or computational resources.","sentences":["Quadrotors have gained popularity over the last decade, aiding humans in complex tasks such as search and rescue, mapping and exploration.","Despite their mechanical simplicity and versatility compared to other types of aerial vehicles, they remain vulnerable to rotor failures.","In this paper, we propose an algorithmic and mechanical approach to addressing the quadrotor fault-tolerant problem in case of rotor failures.","First, we present a fault-tolerant detection and control scheme that includes various attitude error metrics.","The scheme transitions to a fault-tolerant control mode by surrendering the yaw control.","Subsequently, to ensure compatibility with platform sensing constraints, we investigate the relationship between variations in robot rotational drag, achieved through a modular mechanical design appendage, resulting in yaw rates within sensor limits.","This analysis offers a platform-agnostic framework for designing more reliable and robust quadrotors in the event of rotor failures.","Extensive experimental results validate the proposed approach providing insights into successfully designing a cost-effective quadrotor capable of fault-tolerant control.","The overall design enhances safety in scenarios of faulty rotors, without the need for additional sensors or computational resources."],"url":"http://arxiv.org/abs/2404.06340v1","category":"cs.RO"}
{"created":"2024-04-09 14:19:45","title":"Software and computing for Run 3 of the ATLAS experiment at the LHC","abstract":"The ATLAS experiment has developed extensive software and distributed computing systems for Run 3 of the LHC. These systems are described in detail, including software infrastructure and workflows, distributed data and workload management, database infrastructure, and validation. The use of these systems to prepare the data for physics analysis and assess its quality are described, along with the software tools used for data analysis itself. An outlook for the development of these projects towards Run 4 is also provided.","sentences":["The ATLAS experiment has developed extensive software and distributed computing systems for Run 3 of the LHC.","These systems are described in detail, including software infrastructure and workflows, distributed data and workload management, database infrastructure, and validation.","The use of these systems to prepare the data for physics analysis and assess its quality are described, along with the software tools used for data analysis itself.","An outlook for the development of these projects towards Run 4 is also provided."],"url":"http://arxiv.org/abs/2404.06335v1","category":"hep-ex"}
{"created":"2024-04-09 14:09:14","title":"Sparse space-time resolvent analysis for statistically-stationary and time-varying flows","abstract":"Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile. Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows. This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles. Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator. Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD. This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method. We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis. This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient. We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis.","sentences":["Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile.","Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows.","This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles.","Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator.","Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD.","This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method.","We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis.","This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient.","We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis."],"url":"http://arxiv.org/abs/2404.06331v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 14:06:21","title":"Flow Fusion, Exploiting Measurement Redundancy for Smarter Allocation","abstract":"In petroleum production systems, continuous multiphase flow rates are essential for efficient operation. They provide situational awareness, enable production optimization, improve reservoir management and planning, and form the basis for allocation. Furthermore, they can be crucial to ensure a fair revenue split between stakeholders for complex production systems where operators share the facilities. Yet, due to complex multiphase flow dynamics and uncertain subsurface fluid properties, the flow rates are challenging to obtain with high accuracy. Consequently, flow rate measurement and estimation solutions, such as multiphase flow meters and virtual flow meters, have different degrees of accuracy and suitability, and impact production decisions and production allocation accordingly.   We propose a field-proven, data-driven framework for reconciliation and allocation. With data validation and reconciliation as the theoretical backbone, the solution exploits measurement redundancy to fuse together relevant flow rate information to infer the most likely flow rates in the production system based on quantifiable uncertainties. The framework consists of four modules: data-processing, uncertainty estimation, reconciliation, and gross error detection. The latter, being the focus of this paper, is a means to identify and mitigate the effect of measurements subject to systematic error, which can invalidate the reconciliation.   In this paper, we highlight that a combination of statistical tests and supporting logic for gross error detection and elimination can be beneficial in obtaining a more justifiable production allocation. Using the maximum power measurement test, the module can be limited in its ability to pinpoint the erroneous measurement. Yet, it is demonstrated that the detections can be convenient indications of gross errors and where these might reside in the production system.","sentences":["In petroleum production systems, continuous multiphase flow rates are essential for efficient operation.","They provide situational awareness, enable production optimization, improve reservoir management and planning, and form the basis for allocation.","Furthermore, they can be crucial to ensure a fair revenue split between stakeholders for complex production systems where operators share the facilities.","Yet, due to complex multiphase flow dynamics and uncertain subsurface fluid properties, the flow rates are challenging to obtain with high accuracy.","Consequently, flow rate measurement and estimation solutions, such as multiphase flow meters and virtual flow meters, have different degrees of accuracy and suitability, and impact production decisions and production allocation accordingly.   ","We propose a field-proven, data-driven framework for reconciliation and allocation.","With data validation and reconciliation as the theoretical backbone, the solution exploits measurement redundancy to fuse together relevant flow rate information to infer the most likely flow rates in the production system based on quantifiable uncertainties.","The framework consists of four modules: data-processing, uncertainty estimation, reconciliation, and gross error detection.","The latter, being the focus of this paper, is a means to identify and mitigate the effect of measurements subject to systematic error, which can invalidate the reconciliation.   ","In this paper, we highlight that a combination of statistical tests and supporting logic for gross error detection and elimination can be beneficial in obtaining a more justifiable production allocation.","Using the maximum power measurement test, the module can be limited in its ability to pinpoint the erroneous measurement.","Yet, it is demonstrated that the detections can be convenient indications of gross errors and where these might reside in the production system."],"url":"http://arxiv.org/abs/2404.06328v1","category":"eess.SP"}
{"created":"2024-04-09 14:01:52","title":"Elastic ribbons in bubble columns: when elasticity, capillarity and gravity govern equilibrium configurations","abstract":"Taking advantage of the competition between elasticity and capillarity has proven to be an efficient way to design structures by folding, bending, or assembling elastic objects in contact with liquid interfaces. Elastocapillary effects often occur at scales where gravity does not play an important role, such as in microfabrication processes. However, the influence of gravity can become significant at the desktop scale, which is relevant for numerous situations including model experiments used to provide a fundamental physics understanding, working at easily accessible scales. We focus here on the case of elastic ribbons placed in two-dimensional bubble columns: by introducing an elastic ribbon inside the central soap films of a staircase bubble structure in a square cross-section column, the deviation from Plateau's laws (capillarity-dominated case dictating the shape of usual foams) can be quantified as a function of the rigidity of the ribbon. For long ribbons, gravity cannot be neglected. We provide a detailed theoretical analysis of the ribbon profile, taking into account capillarity, elasticity and gravity. We compute the total energy of the system and perform energy minimization under constraints, using Lagrangian mechanics. The model is then validated via a comparison with experiments with three different ribbon thicknesses.","sentences":["Taking advantage of the competition between elasticity and capillarity has proven to be an efficient way to design structures by folding, bending, or assembling elastic objects in contact with liquid interfaces.","Elastocapillary effects often occur at scales where gravity does not play an important role, such as in microfabrication processes.","However, the influence of gravity can become significant at the desktop scale, which is relevant for numerous situations including model experiments used to provide a fundamental physics understanding, working at easily accessible scales.","We focus here on the case of elastic ribbons placed in two-dimensional bubble columns: by introducing an elastic ribbon inside the central soap films of a staircase bubble structure in a square cross-section column, the deviation from Plateau's laws (capillarity-dominated case dictating the shape of usual foams) can be quantified as a function of the rigidity of the ribbon.","For long ribbons, gravity cannot be neglected.","We provide a detailed theoretical analysis of the ribbon profile, taking into account capillarity, elasticity and gravity.","We compute the total energy of the system and perform energy minimization under constraints, using Lagrangian mechanics.","The model is then validated via a comparison with experiments with three different ribbon thicknesses."],"url":"http://arxiv.org/abs/2404.06322v1","category":"cond-mat.soft"}
{"created":"2024-04-09 13:58:32","title":"An Overview of Absolute Value Equations: From Theory to Solution Methods and Challenges","abstract":"This paper provides a thorough exploration of the absolute value equations $Ax-|x|=b$, a seemingly straightforward concept that has gained heightened attention in recent years. It is an NP-hard and nondifferentiable problem and equivalent with the standard linear complementarity problem. Offering a comprehensive review of existing literature, the study delves into theorems concerning the existence and nonexistence of solutions to the absolute value equations, along with numerical methods for effectively addressing this complex equation. Going beyond conventional approaches, the paper investigates strategies for obtaining solutions with minimal norms, techniques for correcting infeasible systems, and other pertinent topics. By pinpointing challenging issues and emphasizing open problems, this paper serves as a valuable guide for shaping the future research trajectory in this dynamic and multifaceted field.","sentences":["This paper provides a thorough exploration of the absolute value equations $Ax-|x|=b$, a seemingly straightforward concept that has gained heightened attention in recent years.","It is an NP-hard and nondifferentiable problem and equivalent with the standard linear complementarity problem.","Offering a comprehensive review of existing literature, the study delves into theorems concerning the existence and nonexistence of solutions to the absolute value equations, along with numerical methods for effectively addressing this complex equation.","Going beyond conventional approaches, the paper investigates strategies for obtaining solutions with minimal norms, techniques for correcting infeasible systems, and other pertinent topics.","By pinpointing challenging issues and emphasizing open problems, this paper serves as a valuable guide for shaping the future research trajectory in this dynamic and multifaceted field."],"url":"http://arxiv.org/abs/2404.06319v1","category":"math.OC"}
{"created":"2024-04-09 13:48:53","title":"Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks","abstract":"Quantum computer simulation software is an integral tool for the research efforts in the quantum computing community. An important aspect is the efficiency of respective frameworks, especially for training variational quantum algorithms. Focusing on the widely used Qiskit software environment, we develop the qiskit-torch-module. It improves runtime performance by two orders of magnitude over comparable libraries, while facilitating low-overhead integration with existing codebases. Moreover, the framework provides advanced tools for integrating quantum neural networks with PyTorch. The pipeline is tailored for single-machine compute systems, which constitute a widely employed setup in day-to-day research efforts.","sentences":["Quantum computer simulation software is an integral tool for the research efforts in the quantum computing community.","An important aspect is the efficiency of respective frameworks, especially for training variational quantum algorithms.","Focusing on the widely used Qiskit software environment, we develop the qiskit-torch-module.","It improves runtime performance by two orders of magnitude over comparable libraries, while facilitating low-overhead integration with existing codebases.","Moreover, the framework provides advanced tools for integrating quantum neural networks with PyTorch.","The pipeline is tailored for single-machine compute systems, which constitute a widely employed setup in day-to-day research efforts."],"url":"http://arxiv.org/abs/2404.06314v1","category":"quant-ph"}
{"created":"2024-04-09 13:35:12","title":"Dynamical structures in phase-separating non-reciprocal polar active mixtures","abstract":"Non-reciprocal systems exhibit diverse dynamical phases whose character depends on the type and degree of non-reciprocity. In this study, we theoretically investigate dynamical structures in a mixture of non-reciprocally aligning polar active particles with repulsion, focusing on the performance on (and connection between) different levels of description. Linear stability analyses of the associated continuum model predict a profound influence of non-reciprocity, leading to phase separation, (anti-)flocking and asymmetric clustering behavior. On the microscopic level, particle simulations confirm the emergence of these dynamical phases and allow for a more in-depth investigation of (microscopic) properties, including orientational correlations and susceptibilities. In particular, our findings demonstrate that certain dynamical properties, like a chase-and-run behavior in the asymmetrical clustering phase, are overlooked in mean-field continuum theory, making microscopic simulations an indispensable tool for studying the effects of non-reciprocal alignment couplings.","sentences":["Non-reciprocal systems exhibit diverse dynamical phases whose character depends on the type and degree of non-reciprocity.","In this study, we theoretically investigate dynamical structures in a mixture of non-reciprocally aligning polar active particles with repulsion, focusing on the performance on (and connection between) different levels of description.","Linear stability analyses of the associated continuum model predict a profound influence of non-reciprocity, leading to phase separation, (anti-)flocking and asymmetric clustering behavior.","On the microscopic level, particle simulations confirm the emergence of these dynamical phases and allow for a more in-depth investigation of (microscopic) properties, including orientational correlations and susceptibilities.","In particular, our findings demonstrate that certain dynamical properties, like a chase-and-run behavior in the asymmetrical clustering phase, are overlooked in mean-field continuum theory, making microscopic simulations an indispensable tool for studying the effects of non-reciprocal alignment couplings."],"url":"http://arxiv.org/abs/2404.06305v1","category":"cond-mat.soft"}
{"created":"2024-04-09 13:18:52","title":"nEMO: Dataset of Emotional Speech in Polish","abstract":"Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).","sentences":["Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems.","However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families.","As datasets covering Slavic languages are rare, there is a need to address this research gap.","This paper presents the development of nEMO, a novel corpus of emotional speech in Polish.","The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state.","The text material used was carefully selected to represent the phonetics of the Polish language adequately.","The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0)."],"url":"http://arxiv.org/abs/2404.06292v1","category":"cs.CL"}
{"created":"2024-04-09 13:09:59","title":"Facilities and practices for linear response Hubbard parameters U and J in Abinit","abstract":"Members of the DFT+U family of functionals are increasingly prevalent methods of addressing errors intrinsic to (semi-) local exchange-correlation functionals at minimum computational cost, but require their parameters U and J to be calculated in situ for a given system of interest, simulation scheme, and runtime parameters. The SCF linear response approach offers ab initio acquisition of the U and has recently been extended to compute the J analogously, which measures localized errors related to exchange-like effects. We introduce a renovated post-processor, the lrUJ utility, together with this detailed best-practices guide, to enable users of the popular, open-source Abinit first-principles simulation suite to engage easily with in situ Hubbard parameters and streamline their incorporation into material simulations of interest. Features of this utility, which may also interest users and developers of other DFT codes, include $n$-degree polynomial regression, error analysis, Python plotting facilities, didactic documentation, and avenues for further developments. In this technical introduction and guide, we place particular emphasis on the intricacies and potential pitfalls introduced by the projector augmented wave (PAW) method, SCF mixing schemes, and non-linear response, several of which are translatable to DFT+U(+J) implementations in other packages.","sentences":["Members of the DFT+U family of functionals are increasingly prevalent methods of addressing errors intrinsic to (semi-) local exchange-correlation functionals at minimum computational cost, but require their parameters U and J to be calculated in situ for a given system of interest, simulation scheme, and runtime parameters.","The SCF linear response approach offers ab initio acquisition of the U and has recently been extended to compute the J analogously, which measures localized errors related to exchange-like effects.","We introduce a renovated post-processor, the lrUJ utility, together with this detailed best-practices guide, to enable users of the popular, open-source Abinit first-principles simulation suite to engage easily with in situ Hubbard parameters and streamline their incorporation into material simulations of interest.","Features of this utility, which may also interest users and developers of other DFT codes, include $n$-degree polynomial regression, error analysis, Python plotting facilities, didactic documentation, and avenues for further developments.","In this technical introduction and guide, we place particular emphasis on the intricacies and potential pitfalls introduced by the projector augmented wave (PAW) method, SCF mixing schemes, and non-linear response, several of which are translatable to DFT+U(+J) implementations in other packages."],"url":"http://arxiv.org/abs/2404.06284v1","category":"physics.comp-ph"}
{"created":"2024-04-09 13:08:28","title":"Simple algorithms to test and learn local Hamiltonians","abstract":"We consider the problems of testing and learning an $n$-qubit $k$-local Hamiltonian from queries to its evolution operator with respect the 2-norm of the Pauli spectrum, or equivalently, the normalized Frobenius norm. For testing whether a Hamiltonian is $\\epsilon_1$-close to $k$-local or $\\epsilon_2$-far from $k$-local, we show that $O(1/(\\epsilon_2-\\epsilon_1)^{8})$ queries suffice. This solves two questions posed in a recent work by Bluhm, Caro and Oufkir. For learning up to error $\\epsilon$, we show that $\\exp(O(k^2+k\\log(1/\\epsilon)))$ queries suffice. Our proofs are simple, concise and based on Pauli-analytic techniques.","sentences":["We consider the problems of testing and learning an $n$-qubit $k$-local Hamiltonian from queries to its evolution operator with respect the 2-norm of the Pauli spectrum, or equivalently, the normalized Frobenius norm.","For testing whether a Hamiltonian is $\\epsilon_1$-close to $k$-local or $\\epsilon_2$-far from $k$-local, we show that $O(1/(\\epsilon_2-\\epsilon_1)^{8})$ queries suffice.","This solves two questions posed in a recent work by Bluhm, Caro and Oufkir.","For learning up to error $\\epsilon$, we show that $\\exp(O(k^2+k\\log(1/\\epsilon)))$ queries suffice.","Our proofs are simple, concise and based on Pauli-analytic techniques."],"url":"http://arxiv.org/abs/2404.06282v1","category":"quant-ph"}
{"created":"2024-04-09 12:54:50","title":"XFLEX HYDRO demonstrators grid services assessment and Ancillary Services Matrix elaboration","abstract":"This paper presents the methodology and key results which enabled to establish the so-called Ancillary Service Matrix (ASM) presenting the ability to deliver the different ancillary services of each of the 6 demonstrators of the XFLEX HYDRO research project combined with the applicable technologies studied in this analysis. These technologies include i) the variable speed technology with Doubly Fed Induction Machine (DFIM) or Full Size Frequency Converters (FSFC), ii) the Smart Power Plant Supervisor (SPPS) enabling to extend the operating range of the hydraulic units in turbine mode based on a better knowledge of the hydro unit wear and tear and associated costs over the full unit operating range, iii) the hydraulic short circuit (HSC) operation leading to simultaneous operation of pump and turbines of Pumped Storage Power Plants (PSPP) and iv) the Hydro-Battery-Hybrid (HBH) applied at Run-of-River demonstrator.   The demonstrators considered for this study includes 4 pumped storage power plants, 1 conventional hydro storage plant and 1 run-of-the-river plant. For each demonstrator a 1D simulation model was developed and validated and was further enhanced to include the model of control system enabling to address the various ancillary services. The systematic 1D numerical simulation of ancillary service contribution of each demonstrator and related technologies enabled to quantify the magnitude of active power response to contribute to the different grid services. The results have been scored between 0 and 5 for each ancillary service, allowing to populate the Ancillary Service Matrix which is summarizing the results in a graphical and synthetic way. The analysis of the score of the Ancillary Services Matrix enables the reader to draw several key conclusions about the benefits unlocked by the implementation of these technologies which are summarized in the paper.","sentences":["This paper presents the methodology and key results which enabled to establish the so-called Ancillary Service Matrix (ASM) presenting the ability to deliver the different ancillary services of each of the 6 demonstrators of the XFLEX HYDRO research project combined with the applicable technologies studied in this analysis.","These technologies include i) the variable speed technology with Doubly Fed Induction Machine (DFIM) or Full Size Frequency Converters (FSFC), ii) the Smart Power Plant Supervisor (SPPS) enabling to extend the operating range of the hydraulic units in turbine mode based on a better knowledge of the hydro unit wear and tear and associated costs over the full unit operating range, iii) the hydraulic short circuit (HSC) operation leading to simultaneous operation of pump and turbines of Pumped Storage Power Plants (PSPP) and iv) the Hydro-Battery-Hybrid (HBH) applied at Run-of-River demonstrator.   ","The demonstrators considered for this study includes 4 pumped storage power plants, 1 conventional hydro storage plant and 1 run-of-the-river plant.","For each demonstrator a 1D simulation model was developed and validated and was further enhanced to include the model of control system enabling to address the various ancillary services.","The systematic 1D numerical simulation of ancillary service contribution of each demonstrator and related technologies enabled to quantify the magnitude of active power response to contribute to the different grid services.","The results have been scored between 0 and 5 for each ancillary service, allowing to populate the Ancillary Service Matrix which is summarizing the results in a graphical and synthetic way.","The analysis of the score of the Ancillary Services Matrix enables the reader to draw several key conclusions about the benefits unlocked by the implementation of these technologies which are summarized in the paper."],"url":"http://arxiv.org/abs/2404.06275v1","category":"eess.SY"}
{"created":"2024-04-09 12:47:31","title":"On the Goncharov's conjecture in degree $m-1$ and weight $m$","abstract":"Let $\\mathbb K$ be a field of characteristic zero. We prove that its motivic cohomology in degree $m-1$ and weight $m$ is rationally isomorphic to the cohomology of the polylogarithmic complex. This gives a partial extension of A. Suslin theorem describing the indecomposable $K_3$ of a field.","sentences":["Let $\\mathbb K$ be a field of characteristic zero.","We prove that its motivic cohomology in degree $m-1$ and weight $m$ is rationally isomorphic to the cohomology of the polylogarithmic complex.","This gives a partial extension of A. Suslin theorem describing the indecomposable $K_3$ of a field."],"url":"http://arxiv.org/abs/2404.06271v1","category":"math.AG"}
{"created":"2024-04-09 12:45:50","title":"A recursive smoothing method for input and state estimation of vibrating structures","abstract":"Recursive Bayesian filters have been widely deployed in structural system identification where output-only filters are of higher practicality. Unfortunately, the estimation obtained by instantaneous system inversion via filters can be compromised by an ill-conditionedness of the system, which is a consequence of the architecture of the sensor network. To significantly reduce the ill-conditioning and increase the robustness to available networks, a new recursive smoothing algorithm is proposed for simultaneous input and state estimation of linear systems. Unlike the existing minimum-variance unbiased (MVU) smoothing methods that are restricted to either systems with no direct feedthrough or systems with a full-rank feedforward matrix, the proposed smoothing algorithm is universally applicable to linear systems with and without direct feedthrough as well as those with a rank-deficient feedforward matrix. The proposed smoothing method does not assume any prior knowledge of the statistical characteristics or evolutionary model pertaining to the input. A different indexing of the discrete-time input leads to a distinct linear algebra from the existing MVU smoothing methods. An eight-storey shear frame and the Taipei 101 tower in Taiwan are used as case studies, and a thorough comparison is established with the Augmented Kalman Filter, MVU filters and MVU smoothing methods. It is shown that the incorporation of singular value truncation for system inversion can result in a noticeable improvement in the estimation. Moreover, across various sensor networks and in the presence of a rank-deficient feedforward matrix, the proposed method could achieve at least 67% noise reduction compared to other filters and at least 30% improvement compared to other smoothing methods.","sentences":["Recursive Bayesian filters have been widely deployed in structural system identification where output-only filters are of higher practicality.","Unfortunately, the estimation obtained by instantaneous system inversion via filters can be compromised by an ill-conditionedness of the system, which is a consequence of the architecture of the sensor network.","To significantly reduce the ill-conditioning and increase the robustness to available networks, a new recursive smoothing algorithm is proposed for simultaneous input and state estimation of linear systems.","Unlike the existing minimum-variance unbiased (MVU) smoothing methods that are restricted to either systems with no direct feedthrough or systems with a full-rank feedforward matrix, the proposed smoothing algorithm is universally applicable to linear systems with and without direct feedthrough as well as those with a rank-deficient feedforward matrix.","The proposed smoothing method does not assume any prior knowledge of the statistical characteristics or evolutionary model pertaining to the input.","A different indexing of the discrete-time input leads to a distinct linear algebra from the existing MVU smoothing methods.","An eight-storey shear frame and the Taipei 101 tower in Taiwan are used as case studies, and a thorough comparison is established with the Augmented Kalman Filter, MVU filters and MVU smoothing methods.","It is shown that the incorporation of singular value truncation for system inversion can result in a noticeable improvement in the estimation.","Moreover, across various sensor networks and in the presence of a rank-deficient feedforward matrix, the proposed method could achieve at least 67% noise reduction compared to other filters and at least 30% improvement compared to other smoothing methods."],"url":"http://arxiv.org/abs/2404.06269v1","category":"stat.AP"}
{"created":"2024-04-09 12:38:14","title":"From Stochastic Hamiltonian to Quantum Simulation: Exploring Memory Effects in Exciton Dynamics","abstract":"The unraveling of open quantum system dynamics in terms of stochastic quantum trajectories offers a picture of open system dynamics that consistently considers memory effects stemming from the finite correlation time of environment fluctuations. These fluctuations significantly influence the coherence and energy transport properties of excitonic systems. When their correlation time is comparable to the timescale of the Hamiltonian evolution, it leads to the departure of open system dynamics from the Markovian limit. In this work, we leverage the unraveling of exciton dynamics through stochastic Hamiltonian propagators to design quantum circuits that simulate exciton transport, capturing finite memory effects. In addition to enabling the synthesis of parametrizable quantum circuits, stochastic unitary propagators provide a transparent framework for investigating non-Markovian effects on exciton transport. Our analysis reveals a nuanced relationship between environment correlation time and transport efficiency, identifying a regime of \"memory-assisted\" quantum transport where time-correlated fluctuations allow the system to reach higher efficiency. However, this property is not universal and can only be realized in conjunction with specific features of the system Hamiltonian.","sentences":["The unraveling of open quantum system dynamics in terms of stochastic quantum trajectories offers a picture of open system dynamics that consistently considers memory effects stemming from the finite correlation time of environment fluctuations.","These fluctuations significantly influence the coherence and energy transport properties of excitonic systems.","When their correlation time is comparable to the timescale of the Hamiltonian evolution, it leads to the departure of open system dynamics from the Markovian limit.","In this work, we leverage the unraveling of exciton dynamics through stochastic Hamiltonian propagators to design quantum circuits that simulate exciton transport, capturing finite memory effects.","In addition to enabling the synthesis of parametrizable quantum circuits, stochastic unitary propagators provide a transparent framework for investigating non-Markovian effects on exciton transport.","Our analysis reveals a nuanced relationship between environment correlation time and transport efficiency, identifying a regime of \"memory-assisted\" quantum transport where time-correlated fluctuations allow the system to reach higher efficiency.","However, this property is not universal and can only be realized in conjunction with specific features of the system Hamiltonian."],"url":"http://arxiv.org/abs/2404.06264v1","category":"quant-ph"}
{"created":"2024-04-09 12:35:12","title":"A Constant self-consistent scattering lifetime in superconducting Strontium Ruthenate","abstract":"In this numerical work, we find a self-consistent constant scattering superconducting lifetime for two different values of the disorder parameters, the inverse atomic strength, and the stoichiometric impurity in the triplet paired unconventional super-conductor strontium ruthenate. This finding is relevant for experimentalists given that the expressions for the ultrasound attenuation and the electronic thermal conductivity depend on the superconducting scattering lifetime, and a constant lifetime fits well nonequilibrium experimental data. Henceforth, this work helps experimentalists in their interpretation of the acquired data. Additionally, we encountered tiny imaginary parts of the self-energy that resembles the Miyake-Narikiyo tiny gap out-side the unitary elastic scattering limit, and below the threshold zero gap value of 1.0 meV.","sentences":["In this numerical work, we find a self-consistent constant scattering superconducting lifetime for two different values of the disorder parameters, the inverse atomic strength, and the stoichiometric impurity in the triplet paired unconventional super-conductor strontium ruthenate.","This finding is relevant for experimentalists given that the expressions for the ultrasound attenuation and the electronic thermal conductivity depend on the superconducting scattering lifetime, and a constant lifetime fits well nonequilibrium experimental data.","Henceforth, this work helps experimentalists in their interpretation of the acquired data.","Additionally, we encountered tiny imaginary parts of the self-energy that resembles the Miyake-Narikiyo tiny gap out-side the unitary elastic scattering limit, and below the threshold zero gap value of 1.0 meV."],"url":"http://arxiv.org/abs/2404.06262v1","category":"cond-mat.supr-con"}
{"created":"2024-04-09 12:32:23","title":"Stress out of charmonia","abstract":"We investigate the gravitational form factors of charmonium. Our method is based on a Hamiltonian formalism on the light front known as basis light-front quantization. The charmonium mass spectrum and light-front wave functions were obtained from diagonalizing an effective Hamiltonian that incorporates confinement from holographic QCD and one-gluon exchange interaction from light-front QCD. We proposed a quantum many-body approach to construct the hadronic matrix elements of the energy momentum tensor $T^{++}$ and $T^{+-}$, which are used to extract the gravitational form factors $A(Q^2)$ and $D(Q^2)$. The obtained form factors satisfy the known constraints, e.g. von Laue condition. From these quantities, we also extract the energy, pressure and invariant mass squared distributions of the system. We find that hadrons are multi-layer systems. We also find that there is a tachyonic core within charmonium.","sentences":["We investigate the gravitational form factors of charmonium.","Our method is based on a Hamiltonian formalism on the light front known as basis light-front quantization.","The charmonium mass spectrum and light-front wave functions were obtained from diagonalizing an effective Hamiltonian that incorporates confinement from holographic QCD and one-gluon exchange interaction from light-front QCD.","We proposed a quantum many-body approach to construct the hadronic matrix elements of the energy momentum tensor $T^{++}$ and $T^{+-}$, which are used to extract the gravitational form factors $A(Q^2)$ and $D(Q^2)$. The obtained form factors satisfy the known constraints, e.g. von Laue condition.","From these quantities, we also extract the energy, pressure and invariant mass squared distributions of the system.","We find that hadrons are multi-layer systems.","We also find that there is a tachyonic core within charmonium."],"url":"http://arxiv.org/abs/2404.06259v1","category":"hep-ph"}
{"created":"2024-04-09 12:29:56","title":"DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems","abstract":"The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks. To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver. However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios. Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths. In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system. In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP. In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths. Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions.","sentences":["The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks.","To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver.","However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios.","Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths.","In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system.","In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP.","In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths.","Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.06257v1","category":"cs.NI"}
{"created":"2024-04-09 12:29:11","title":"A Large-Scale Simulation Method for Neuromorphic Circuits","abstract":"Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems. Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the simulation and design of neuromorphic systems. For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component. This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain. To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated.","sentences":["Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems.","Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the simulation and design of neuromorphic systems.","For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component.","This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain.","To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated."],"url":"http://arxiv.org/abs/2404.06255v1","category":"eess.SY"}
{"created":"2024-04-09 12:23:30","title":"ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization","abstract":"How to effectively explore spatial-temporal features is important for video colorization. Instead of stacking multiple frames along the temporal dimension or recurrently propagating estimated features that will accumulate errors or cannot explore information from far-apart frames, we develop a memory-based feature propagation module that can establish reliable connections with features from far-apart frames and alleviate the influence of inaccurately estimated features. To extract better features from each frame for the above-mentioned feature propagation, we explore the features from large-pretrained visual models to guide the feature estimation of each frame so that the estimated features can model complex scenarios. In addition, we note that adjacent frames usually contain similar contents. To explore this property for better spatial and temporal feature utilization, we develop a local attention module to aggregate the features from adjacent frames in a spatial-temporal neighborhood. We formulate our memory-based feature propagation module, large-pretrained visual model guided feature estimation module, and local attention module into an end-to-end trainable network (named ColorMNet) and show that it performs favorably against state-of-the-art methods on both the benchmark datasets and real-world scenarios. The source code and pre-trained models will be available at \\url{https://github.com/yyang181/colormnet}.","sentences":["How to effectively explore spatial-temporal features is important for video colorization.","Instead of stacking multiple frames along the temporal dimension or recurrently propagating estimated features that will accumulate errors or cannot explore information from far-apart frames, we develop a memory-based feature propagation module that can establish reliable connections with features from far-apart frames and alleviate the influence of inaccurately estimated features.","To extract better features from each frame for the above-mentioned feature propagation, we explore the features from large-pretrained visual models to guide the feature estimation of each frame so that the estimated features can model complex scenarios.","In addition, we note that adjacent frames usually contain similar contents.","To explore this property for better spatial and temporal feature utilization, we develop a local attention module to aggregate the features from adjacent frames in a spatial-temporal neighborhood.","We formulate our memory-based feature propagation module, large-pretrained visual model guided feature estimation module, and local attention module into an end-to-end trainable network (named ColorMNet) and show that it performs favorably against state-of-the-art methods on both the benchmark datasets and real-world scenarios.","The source code and pre-trained models will be available at \\url{https://github.com/yyang181/colormnet}."],"url":"http://arxiv.org/abs/2404.06251v1","category":"cs.CV"}
{"created":"2024-04-09 12:13:40","title":"LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks","abstract":"Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.","sentences":["Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video.","Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames.","This can lead to significant robustness and security issues when these trackers are deployed in the real world.","To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest.","This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts.","As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data.","In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data."],"url":"http://arxiv.org/abs/2404.06247v1","category":"cs.CV"}
{"created":"2024-04-09 12:08:24","title":"Confirmable Workflows in OSCAR","abstract":"We discuss what is special about the reproducibility of workflows in computer algebra. It is emphasized how the programming language Julia and the new computer algebra system OSCAR support such a reproducibility, and how users can benefit for their own work.","sentences":["We discuss what is special about the reproducibility of workflows in computer algebra.","It is emphasized how the programming language Julia and the new computer algebra system OSCAR support such a reproducibility, and how users can benefit for their own work."],"url":"http://arxiv.org/abs/2404.06241v1","category":"cs.MS"}
{"created":"2024-04-09 11:53:27","title":"A Semantic Proof of Generalised Cut Elimination for Deep Inference","abstract":"Multiplicative-Additive System Virtual (MAV) is a logic that extends Multiplicative-Additive Linear Logic with a self-dual non-commutative operator expressing the concept of \"before\" or \"sequencing\". MAV is also an extenson of the the logic Basic System Virtual (BV) with additives. Formulas in BV have an appealing reading as processes with parallel and sequential composition. MAV adds internal and external choice operators. BV and MAV are also closely related to Concurrent sKleene Algebras.   Proof systems for MAV and BV are Deep Inference systems, which allow inference rules to be applied anywhere inside a structure. As with any proof system, a key question is whether proofs in MAV can be reduced to a normal form, removing detours and the introduction of structures not present in the original goal. In Sequent Calcluli systems, this property is referred to as Cut Elimination. Deep Inference systems have an analogous Cut rule and other rules that are not present in normalised proofs. Cut Elimination for Deep Inference systems has the same metatheoretic benefits as for Sequent Calculi systems, including consistency and decidability.   Proofs of Cut Elimination for BV, MAV, and other Deep Inference systems present in the literature have relied on intrincate syntactic reasoning and complex termination measures.   We present a concise semantic proof that all MAV proofs can be reduced to a normal form avoiding the Cut rule and other \"non analytic\" rules. We also develop soundness and completeness proofs of MAV (and BV) with respect to a class of models. We have mechanised all our proofs in the Agda proof assistant, which provides both assurance of their correctness as well as yielding an executable normalisation procedure.","sentences":["Multiplicative-Additive System Virtual (MAV) is a logic that extends Multiplicative-Additive Linear Logic with a self-dual non-commutative operator expressing the concept of \"before\" or \"sequencing\".","MAV is also an extenson of the the logic Basic System Virtual (BV) with additives.","Formulas in BV have an appealing reading as processes with parallel and sequential composition.","MAV adds internal and external choice operators.","BV and MAV are also closely related to Concurrent sKleene Algebras.   ","Proof systems for MAV and BV are Deep Inference systems, which allow inference rules to be applied anywhere inside a structure.","As with any proof system, a key question is whether proofs in MAV can be reduced to a normal form, removing detours and the introduction of structures not present in the original goal.","In Sequent Calcluli systems, this property is referred to as Cut Elimination.","Deep Inference systems have an analogous Cut rule and other rules that are not present in normalised proofs.","Cut Elimination for Deep Inference systems has the same metatheoretic benefits as for Sequent Calculi systems, including consistency and decidability.   ","Proofs of Cut Elimination for BV, MAV, and other Deep Inference systems present in the literature have relied on intrincate syntactic reasoning and complex termination measures.   ","We present a concise semantic proof that all MAV proofs can be reduced to a normal form avoiding the Cut rule and other \"non analytic\" rules.","We also develop soundness and completeness proofs of MAV (and BV) with respect to a class of models.","We have mechanised all our proofs in the Agda proof assistant, which provides both assurance of their correctness as well as yielding an executable normalisation procedure."],"url":"http://arxiv.org/abs/2404.06233v1","category":"cs.LO"}
{"created":"2024-04-09 11:27:07","title":"Message Passing Variational Autoregressive Network for Solving Intractable Ising Models","abstract":"Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks. Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems. Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking. Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables. The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures. The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks. Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems.","sentences":["Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks.","Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems.","Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking.","Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables.","The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures.","The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks.","Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems."],"url":"http://arxiv.org/abs/2404.06225v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 10:58:27","title":"Coherence and imaginarity of quantum states","abstract":"Baumgratz, Cramer and Plenio established a rigorous framework (BCP framework) for quantifying the coherence of quantum states [\\href{http://dx.doi.org/10.1103/PhysRevLett.113.140401}{Phys. Rev. Lett. 113, 140401 (2014)}]. In BCP framework, a quantum state is called incoherent if it is diagonal in the fixed orthonormal basis, and a coherence measure should satisfy some conditions. For a fixed orthonormal basis, if a quantum state $\\rho $ has nonzero imaginary part, then $\\rho $ must be coherent. How to quantitatively characterize this fact? In this work, we show that any coherence measure $C$ in BCP framework has the property $C(\\rho )-C($Re$\\rho )\\geq 0$ if $C$ is invariant under state complex conjugation, i.e., $C(\\rho )=C(\\rho ^{\\ast })$, here $\\rho ^{\\ast }$ is the conjugate of $\\rho ,$ Re$\\rho $ is the real part of $\\rho .$ If $C$ does not satisfy $C(\\rho )=C(\\rho ^{\\ast }),$ we can define a new coherence measure $C^{\\prime }(\\rho )=\\frac{1}{2}[C(\\rho )+C(\\rho ^{\\ast })]$ such that $C^{\\prime }(\\rho )=C^{\\prime }(\\rho ^{\\ast }).$ We also establish some similar results for bosonic Gaussian states.","sentences":["Baumgratz, Cramer and Plenio established a rigorous framework (BCP framework) for quantifying the coherence of quantum states [\\href{http://dx.doi.org/10.1103/PhysRevLett.113.140401}{Phys.","Rev. Lett. 113, 140401 (2014)}].","In BCP framework, a quantum state is called incoherent if it is diagonal in the fixed orthonormal basis, and a coherence measure should satisfy some conditions.","For a fixed orthonormal basis, if a quantum state $\\rho $ has nonzero imaginary part, then $\\rho $ must be coherent.","How to quantitatively characterize this fact?","In this work, we show that any coherence measure $C$ in BCP framework has the property $C(\\rho )-C($Re$\\rho )\\geq 0$ if $C$ is invariant under state complex conjugation, i.e., $C(\\rho )=C(\\rho ^{\\ast })$, here $\\rho ^{\\ast }$ is the conjugate of $\\rho ,$ Re$\\rho $ is the real part of $\\rho .$","If $C$ does not satisfy $C(\\rho )=C(\\rho ^{\\ast }),$ we can define a new coherence measure $C^{\\prime }(\\rho )=\\frac{1}{2}[C(\\rho )","+C(\\rho ^{\\ast })]$ such that $C^{\\prime }(\\rho )=C^{\\prime }(\\rho ^{\\ast }).$","We also establish some similar results for bosonic Gaussian states."],"url":"http://arxiv.org/abs/2404.06210v1","category":"quant-ph"}
{"created":"2024-04-09 10:25:48","title":"Dynamics of large oscillator populations with random interactions","abstract":"We explore large populations of phase oscillators interacting via random coupling functions. Two types of coupling terms, the Kuramoto-Daido coupling and the Winfree coupling, are considered. Under the assumption of statistical independence of the phases and the couplings, we derive reduced averaged equations with effective non-random coupling terms. As a particular example, we study interactions that have the same shape but possess random coupling strengths and random phase shifts. While randomness in coupling strengths just renormalizes the interaction, a distribution of the phase shifts in coupling reshapes the coupling function.","sentences":["We explore large populations of phase oscillators interacting via random coupling functions.","Two types of coupling terms, the Kuramoto-Daido coupling and the Winfree coupling, are considered.","Under the assumption of statistical independence of the phases and the couplings, we derive reduced averaged equations with effective non-random coupling terms.","As a particular example, we study interactions that have the same shape but possess random coupling strengths and random phase shifts.","While randomness in coupling strengths just renormalizes the interaction, a distribution of the phase shifts in coupling reshapes the coupling function."],"url":"http://arxiv.org/abs/2404.06193v1","category":"nlin.AO"}
{"created":"2024-04-09 10:13:41","title":"High-Fidelity CZ Gates in Double Quantum Dot -- Circuit QED Systems Beyond the Rotating-Wave Approximation","abstract":"Semiconductor double quantum dot (DQD) qubits coupled via superconducting microwave resonators provide a powerful means of long-range manipulation of the qubits' spin and charge degrees of freedom. Quantum gates can be implemented by parametrically driving the qubits while their transition frequencies are detuned from the resonator frequency. Long-range two-qubit CZ gates have been proposed for the DQD spin qubit within the rotating-wave approximation (RWA). Rapid gates demand strong coupling, but RWA breaks down when coupling strengths become significant relative to system frequencies. Therefore, understanding the detrimental impact of time-dependent terms ignored by RWA is critical for high-fidelity operation. Here, we go beyond RWA to study CZ gate fidelity for both DQD spin and charge qubits. We propose a novel parametric drive on the charge qubit that produces fewer time-dependent terms and show that it outperforms its spin counterpart. We find that drive amplitude - a parameter dropped in RWA - is critical for optimizing fidelity and map out high-fidelity regimes. Our results demonstrate the necessity of going beyond RWA in understanding how long-range gates can be realized in DQD qubits, with charge qubits offering considerable advantages in high-fidelity operation.","sentences":["Semiconductor double quantum dot (DQD) qubits coupled via superconducting microwave resonators provide a powerful means of long-range manipulation of the qubits' spin and charge degrees of freedom.","Quantum gates can be implemented by parametrically driving the qubits while their transition frequencies are detuned from the resonator frequency.","Long-range two-qubit CZ gates have been proposed for the DQD spin qubit within the rotating-wave approximation (RWA).","Rapid gates demand strong coupling, but RWA breaks down when coupling strengths become significant relative to system frequencies.","Therefore, understanding the detrimental impact of time-dependent terms ignored by RWA is critical for high-fidelity operation.","Here, we go beyond RWA to study CZ gate fidelity for both DQD spin and charge qubits.","We propose a novel parametric drive on the charge qubit that produces fewer time-dependent terms and show that it outperforms its spin counterpart.","We find that drive amplitude - a parameter dropped in RWA - is critical for optimizing fidelity and map out high-fidelity regimes.","Our results demonstrate the necessity of going beyond RWA in understanding how long-range gates can be realized in DQD qubits, with charge qubits offering considerable advantages in high-fidelity operation."],"url":"http://arxiv.org/abs/2404.06187v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 10:03:35","title":"AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation","abstract":"This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks. The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved. Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning. Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning. Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available.","sentences":["This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks.","The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved.","Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning.","Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   ","While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning.","Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available."],"url":"http://arxiv.org/abs/2404.06179v1","category":"cs.RO"}
{"created":"2024-04-09 09:55:44","title":"Higher order topological defects in a moir\u00e9 lattice","abstract":"Topological defects are ubiquitous, they manifest in a wide variety of systems such as liquid crystals, magnets or superconductors. The recent quest for nonabelian anyons in condensed matter physics stimulates the interest for topological defects since they can be hosted in vortices in quantum magnets or topological superconductors. In addition to these vortex defects, in this study we propose to investigate edge dislocations in 2D magnets as new building blocks for topological physics since they can be described as vortices in the structural phase field. Here we demonstrate the existence of higher order topological dislocations within the higher order moir\\'e pattern of the van der Waals 2D magnet CrCl3 deposited on Au(111). Surprizingly, these higher order dislocations arise from ordinary simple edge dislocations in the atomic lattice of CrCl3. We provide a theoretical framework explaining the higher order dislocations as vortex with a winding Chern number of 2. We expect that these original defects could stabilize some anyons either in a 2D quantum magnet or within a 2D superconductor coupled to it.","sentences":["Topological defects are ubiquitous, they manifest in a wide variety of systems such as liquid crystals, magnets or superconductors.","The recent quest for nonabelian anyons in condensed matter physics stimulates the interest for topological defects since they can be hosted in vortices in quantum magnets or topological superconductors.","In addition to these vortex defects, in this study we propose to investigate edge dislocations in 2D magnets as new building blocks for topological physics since they can be described as vortices in the structural phase field.","Here we demonstrate the existence of higher order topological dislocations within the higher order moir\\'e pattern of the van der Waals 2D magnet CrCl3 deposited on Au(111).","Surprizingly, these higher order dislocations arise from ordinary simple edge dislocations in the atomic lattice of CrCl3.","We provide a theoretical framework explaining the higher order dislocations as vortex with a winding Chern number of 2.","We expect that these original defects could stabilize some anyons either in a 2D quantum magnet or within a 2D superconductor coupled to it."],"url":"http://arxiv.org/abs/2404.06176v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 09:27:54","title":"Concise Plane Arrangements for Low-Poly Surface and Volume Modelling","abstract":"Plane arrangements are a useful tool for surface and volume modelling. However, their main drawback is poor scalability. We introduce two key novelties that enable the construction of plane arrangements for complex objects and entire scenes: an ordering scheme for the plane insertion and the direct use of input points during arrangement construction. Both ingredients reduce the number of unwanted splits, resulting in improved scalability of the construction mechanism by up to two orders of magnitude compared to existing algorithms. We further introduce a remeshing and simplification technique that allows us to extract low-polygon surface meshes and lightweight convex decompositions of volumes from the arrangement. We show that our approach leads to state-of-the-art results for the aforementioned tasks by comparing it to learning-based and traditional approaches on various different datasets. Our implementation is available at https://github.com/raphaelsulzer/compod .","sentences":["Plane arrangements are a useful tool for surface and volume modelling.","However, their main drawback is poor scalability.","We introduce two key novelties that enable the construction of plane arrangements for complex objects and entire scenes: an ordering scheme for the plane insertion and the direct use of input points during arrangement construction.","Both ingredients reduce the number of unwanted splits, resulting in improved scalability of the construction mechanism by up to two orders of magnitude compared to existing algorithms.","We further introduce a remeshing and simplification technique that allows us to extract low-polygon surface meshes and lightweight convex decompositions of volumes from the arrangement.","We show that our approach leads to state-of-the-art results for the aforementioned tasks by comparing it to learning-based and traditional approaches on various different datasets.","Our implementation is available at https://github.com/raphaelsulzer/compod ."],"url":"http://arxiv.org/abs/2404.06154v1","category":"cs.CG"}
{"created":"2024-04-09 09:02:21","title":"Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond","abstract":"Transformer has made an enormous success in natural language processing and high-level vision over the past few years. However, the complexity of self-attention is quadratic to the image size, which makes it infeasible for high-resolution vision tasks. In this paper, we propose the Mansformer, a Transformer of mixed attention that combines multiple self-attentions, gate, and multi-layer perceptions (MLPs), to explore and employ more possibilities of self-attention. Taking efficiency into account, we design four kinds of self-attention, whose complexities are all linear. By elaborate adjustment of the tensor shapes and dimensions for the dot product, we split the typical self-attention of quadratic complexity into four operations of linear complexity. To adaptively merge these different kinds of self-attention, we take advantage of an architecture similar to Squeeze-and-Excitation Networks. Furthermore, we make it to merge the two-staged Transformer design into one stage by the proposed gated-dconv MLP. Image deblurring is our main target, while extensive quantitative and qualitative evaluations show that this method performs favorably against the state-of-the-art methods far more than simply deblurring. The source codes and trained models will be made available to the public.","sentences":["Transformer has made an enormous success in natural language processing and high-level vision over the past few years.","However, the complexity of self-attention is quadratic to the image size, which makes it infeasible for high-resolution vision tasks.","In this paper, we propose the Mansformer, a Transformer of mixed attention that combines multiple self-attentions, gate, and multi-layer perceptions (MLPs), to explore and employ more possibilities of self-attention.","Taking efficiency into account, we design four kinds of self-attention, whose complexities are all linear.","By elaborate adjustment of the tensor shapes and dimensions for the dot product, we split the typical self-attention of quadratic complexity into four operations of linear complexity.","To adaptively merge these different kinds of self-attention, we take advantage of an architecture similar to Squeeze-and-Excitation Networks.","Furthermore, we make it to merge the two-staged Transformer design into one stage by the proposed gated-dconv MLP.","Image deblurring is our main target, while extensive quantitative and qualitative evaluations show that this method performs favorably against the state-of-the-art methods far more than simply deblurring.","The source codes and trained models will be made available to the public."],"url":"http://arxiv.org/abs/2404.06135v1","category":"cs.CV"}
{"created":"2024-04-09 08:58:36","title":"The turnpike property for high-dimensional interacting agent systems in discrete time","abstract":"We investigate the interior turnpike phenomenon for discrete-time multi-agent optimal control problems. While for continuous systems the turnpike property has been established, we focus here on first-order discretizations of such systems. It is shown that the resulting time-discrete system inherits the turnpike property with estimates of the same type as in the continuous case. In particular, we prove that the discrete time optimal control problem is strictly dissipative and the cheap control assumption holds.","sentences":["We investigate the interior turnpike phenomenon for discrete-time multi-agent optimal control problems.","While for continuous systems the turnpike property has been established, we focus here on first-order discretizations of such systems.","It is shown that the resulting time-discrete system inherits the turnpike property with estimates of the same type as in the continuous case.","In particular, we prove that the discrete time optimal control problem is strictly dissipative and the cheap control assumption holds."],"url":"http://arxiv.org/abs/2404.06134v1","category":"math.OC"}
{"created":"2024-04-09 08:51:44","title":"Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction","abstract":"Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.","sentences":["Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions.","Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps.","Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system.","By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures.","Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM.","It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications.","Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer."],"url":"http://arxiv.org/abs/2404.06128v1","category":"cs.CV"}
{"created":"2024-04-09 08:49:41","title":"Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging","abstract":"Tuning parameters in model predictive control (MPC) presents significant challenges, particularly when there is a notable discrepancy between the controller's predictions and the actual behavior of the closed-loop plant. This mismatch may stem from factors like substantial model-plant differences, limited prediction horizons that do not cover the entire time of interest, or unforeseen system disturbances. Such mismatches can jeopardize both performance and safety, including constraint satisfaction. Traditional methods address this issue by modifying the finite horizon cost function to better reflect the overall operational cost, learning parts of the prediction model from data, or implementing robust MPC strategies, which might be either computationally intensive or overly cautious. As an alternative, directly optimizing or learning the controller parameters to enhance closed-loop performance has been proposed. We apply Bayesian optimization for efficient learning of unknown model parameters and parameterized constraint backoff terms, aiming to improve closed-loop performance of battery fast charging. This approach establishes a hierarchical control framework where Bayesian optimization directly fine-tunes closed-loop behavior towards a global and long-term objective, while MPC handles lower-level, short-term control tasks. For lithium-ion battery fast charging, we show that the learning approach not only ensures safe operation but also maximizes closed-loop performance. This includes maintaining the battery's operation below its maximum terminal voltage and reducing charging times, all achieved using a standard nominal MPC model with a short horizon and notable initial model-plant mismatch.","sentences":["Tuning parameters in model predictive control (MPC) presents significant challenges, particularly when there is a notable discrepancy between the controller's predictions and the actual behavior of the closed-loop plant.","This mismatch may stem from factors like substantial model-plant differences, limited prediction horizons that do not cover the entire time of interest, or unforeseen system disturbances.","Such mismatches can jeopardize both performance and safety, including constraint satisfaction.","Traditional methods address this issue by modifying the finite horizon cost function to better reflect the overall operational cost, learning parts of the prediction model from data, or implementing robust MPC strategies, which might be either computationally intensive or overly cautious.","As an alternative, directly optimizing or learning the controller parameters to enhance closed-loop performance has been proposed.","We apply Bayesian optimization for efficient learning of unknown model parameters and parameterized constraint backoff terms, aiming to improve closed-loop performance of battery fast charging.","This approach establishes a hierarchical control framework where Bayesian optimization directly fine-tunes closed-loop behavior towards a global and long-term objective, while MPC handles lower-level, short-term control tasks.","For lithium-ion battery fast charging, we show that the learning approach not only ensures safe operation but also maximizes closed-loop performance.","This includes maintaining the battery's operation below its maximum terminal voltage and reducing charging times, all achieved using a standard nominal MPC model with a short horizon and notable initial model-plant mismatch."],"url":"http://arxiv.org/abs/2404.06125v1","category":"eess.SY"}
{"created":"2024-04-09 08:10:04","title":"Exploring Diverse Sounds: Identifying Outliers in a Music Corpus","abstract":"Existing research on music recommendation systems primarily focuses on recommending similar music, thereby often neglecting diverse and distinctive musical recordings. Musical outliers can provide valuable insights due to the inherent diversity of music itself. In this paper, we explore music outliers, investigating their potential usefulness for music discovery and recommendation systems. We argue that not all outliers should be treated as noise, as they can offer interesting perspectives and contribute to a richer understanding of an artist's work. We introduce the concept of 'Genuine' music outliers and provide a definition for them. These genuine outliers can reveal unique aspects of an artist's repertoire and hold the potential to enhance music discovery by exposing listeners to novel and diverse musical experiences.","sentences":["Existing research on music recommendation systems primarily focuses on recommending similar music, thereby often neglecting diverse and distinctive musical recordings.","Musical outliers can provide valuable insights due to the inherent diversity of music itself.","In this paper, we explore music outliers, investigating their potential usefulness for music discovery and recommendation systems.","We argue that not all outliers should be treated as noise, as they can offer interesting perspectives and contribute to a richer understanding of an artist's work.","We introduce the concept of 'Genuine' music outliers and provide a definition for them.","These genuine outliers can reveal unique aspects of an artist's repertoire and hold the potential to enhance music discovery by exposing listeners to novel and diverse musical experiences."],"url":"http://arxiv.org/abs/2404.06103v1","category":"cs.SD"}
{"created":"2024-04-09 08:02:43","title":"The majorant method for the fermionic effective action","abstract":"We revisit the problem of controlling Polchinski's equation by the solution of an associate Hamilton-Jacobi equation which determines a norm majorant for the fermionic effective action. This method, referred to as the Majorant Method, was first introduced by D. Brydges and J. Wright in 1988, but its original formulation contains a gap which has never been addressed. We overcome this gap and show that the majorant equation and its existence condition are analogous to the ones originally obtained by Brydges and Wright. As an application of the method, we investigate systems with quartic perturbations.","sentences":["We revisit the problem of controlling Polchinski's equation by the solution of an associate Hamilton-Jacobi equation which determines a norm majorant for the fermionic effective action.","This method, referred to as the Majorant Method, was first introduced by D. Brydges and J. Wright in 1988, but its original formulation contains a gap which has never been addressed.","We overcome this gap and show that the majorant equation and its existence condition are analogous to the ones originally obtained by Brydges and Wright.","As an application of the method, we investigate systems with quartic perturbations."],"url":"http://arxiv.org/abs/2404.06099v1","category":"math-ph"}
{"created":"2024-04-09 07:50:05","title":"Analytic thermodynamic properties of the Lieb-Liniger gas","abstract":"We present a comprehensive review on the state-of-the-art of the approximate analytic approaches describing the finite-temperature thermodynamic quantities of the Lieb-Liniger model of the one-dimensional (1D) Bose gas with contact repulsive interactions. This paradigmatic model of quantum many-body-theory plays an important role in many areas of physics -- thanks to its integrability and possible experimental realization using, e.g., ensembles of ultracold bosonic atoms confined to quasi-1D geometries. The thermodynamics of the uniform Lieb-Liniger gas can be obtained numerically using the exact thermal Bethe ansatz (TBA) method, first derived in 1969 by Yang and Yang. However, the TBA numerical calculations do not allow for the in-depth understanding of the underlying physical mechanisms that govern the thermodynamic behavior of the Lieb-Liniger gas at finite temperature. Our work is then motivated by the insights that emerge naturally from the transparency of closed-form analytic results, which are derived here in six different regimes of the gas and which exhibit an excellent agreement with the TBA numerics. Our findings can be further adopted for characterising the equilibrium properties of inhomogeneous (e.g., harmonically trapped) 1D Bose gases within the local density approximation and for the development of improved hydrodynamic theories, allowing for the calculation of breathing mode frequencies which depend on the underlying thermodynamic equation of state. Our analytic approaches can be applied to other systems including impurities in a quantum bath, liquid helium-4, and ultracold Bose gas mixtures.","sentences":["We present a comprehensive review on the state-of-the-art of the approximate analytic approaches describing the finite-temperature thermodynamic quantities of the Lieb-Liniger model of the one-dimensional (1D) Bose gas with contact repulsive interactions.","This paradigmatic model of quantum many-body-theory plays an important role in many areas of physics -- thanks to its integrability and possible experimental realization using, e.g., ensembles of ultracold bosonic atoms confined to quasi-1D geometries.","The thermodynamics of the uniform Lieb-Liniger gas can be obtained numerically using the exact thermal Bethe ansatz (TBA) method, first derived in 1969 by Yang and Yang.","However, the TBA numerical calculations do not allow for the in-depth understanding of the underlying physical mechanisms that govern the thermodynamic behavior of the Lieb-Liniger gas at finite temperature.","Our work is then motivated by the insights that emerge naturally from the transparency of closed-form analytic results, which are derived here in six different regimes of the gas and which exhibit an excellent agreement with the TBA numerics.","Our findings can be further adopted for characterising the equilibrium properties of inhomogeneous (e.g., harmonically trapped)","1D Bose gases within the local density approximation and for the development of improved hydrodynamic theories, allowing for the calculation of breathing mode frequencies which depend on the underlying thermodynamic equation of state.","Our analytic approaches can be applied to other systems including impurities in a quantum bath, liquid helium-4, and ultracold Bose gas mixtures."],"url":"http://arxiv.org/abs/2404.06092v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-09 07:45:06","title":"The Overlap Gap Property limits limit swapping in QAOA","abstract":"The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for combinatorial optimization problem. We show that under the assumption that the Overlap Gap Property (OGP) in the solution space for the Max-$q$-XORSAT is a monotonic increasing function, the swapping of limits in QAOA leads to suboptimal results limited by the OGP. Furthermore, since the performance of QAOA for the pure $q$-spin model matches asymptotically for Max-$q$-XORSAT on large-girth regular hypergraph, we show that the average-case value obtained by QAOA for the pure $q$-spin model for even $q\\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely. This suggests that a necessary condition for the validity of limit swapping in QAOA is the absence of OGP in a given combinatorial optimization problem. A corollary of this is that the spectral gap of a Hamiltonian exhibiting the OGP will close in the thermodynamic limit resulting in a limitation of the quantum adiabatic theorem and efficient optimization of QAOA parameters. Furthermore, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to Montanari's classical algorithm in solving the mean field spin glass problem, the best known classical algorithm.","sentences":["The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for combinatorial optimization problem.","We show that under the assumption that the Overlap Gap Property (OGP) in the solution space for the Max-$q$-XORSAT is a monotonic increasing function, the swapping of limits in QAOA leads to suboptimal results limited by the OGP.","Furthermore, since the performance of QAOA for the pure $q$-spin model matches asymptotically for Max-$q$-XORSAT on large-girth regular hypergraph, we show that the average-case value obtained by QAOA for the pure $q$-spin model for even $q\\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely.","This suggests that a necessary condition for the validity of limit swapping in QAOA is the absence of OGP in a given combinatorial optimization problem.","A corollary of this is that the spectral gap of a Hamiltonian exhibiting the OGP will close in the thermodynamic limit resulting in a limitation of the quantum adiabatic theorem and efficient optimization of QAOA parameters.","Furthermore, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to Montanari's classical algorithm in solving the mean field spin glass problem, the best known classical algorithm."],"url":"http://arxiv.org/abs/2404.06087v1","category":"quant-ph"}
{"created":"2024-04-09 07:39:33","title":"Energetic bounds on gyrokinetic instabilities. Part 4. Bounce-averaged electrons","abstract":"Upper bounds on the growth of instabilities in gyrokinetic systems have recently been derived by considering the optimal perturbations that maximise the growth of a chosen energy norm. This technique has previously been applied to two-species gyrokinetic systems with fully kinetic ions and electrons. However, in tokamaks and stellarators, the expectation from linear instability analyses is that the most important kinetic-electron contribution to ion-scale modes comes from the trapped electrons, which bounce faster than the timescale upon which instabilities evolve. As a result, a fully-kinetic electron response is not required to describe unstable modes in most cases. Here, we apply the optimal mode analysis to a reduced two-species system that consists of fully gyrokinetic ions and bounce-averaged electrons with the aim of finding a tighter bound on ion-scale instabilities in toroidal geometry. This analysis yields bounds that are greatly reduced in comparison to the earlier two-species result. Moreover, if the energy norm is properly chosen, wave-particle resonance effects can be captured, reproducing the stabilisation of density-gradient-driven instabilities in maximum-$J$ devices. The optimal mode analysis also reveals that the maximum-$J$ property has an additional stabilising effect on ion-temperature-gradient-driven instabilities, even in the absence of an electron-free energy source. This effect is explained in terms of the concept of mode inertia, making it distinct from other mechanisms.","sentences":["Upper bounds on the growth of instabilities in gyrokinetic systems have recently been derived by considering the optimal perturbations that maximise the growth of a chosen energy norm.","This technique has previously been applied to two-species gyrokinetic systems with fully kinetic ions and electrons.","However, in tokamaks and stellarators, the expectation from linear instability analyses is that the most important kinetic-electron contribution to ion-scale modes comes from the trapped electrons, which bounce faster than the timescale upon which instabilities evolve.","As a result, a fully-kinetic electron response is not required to describe unstable modes in most cases.","Here, we apply the optimal mode analysis to a reduced two-species system that consists of fully gyrokinetic ions and bounce-averaged electrons with the aim of finding a tighter bound on ion-scale instabilities in toroidal geometry.","This analysis yields bounds that are greatly reduced in comparison to the earlier two-species result.","Moreover, if the energy norm is properly chosen, wave-particle resonance effects can be captured, reproducing the stabilisation of density-gradient-driven instabilities in maximum-$J$ devices.","The optimal mode analysis also reveals that the maximum-$J$ property has an additional stabilising effect on ion-temperature-gradient-driven instabilities, even in the absence of an electron-free energy source.","This effect is explained in terms of the concept of mode inertia, making it distinct from other mechanisms."],"url":"http://arxiv.org/abs/2404.06081v1","category":"physics.plasm-ph"}
{"created":"2024-04-09 07:39:21","title":"Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures","abstract":"This study aims to establish a computer-aided diagnosis system for endobronchial ultrasound (EBUS) surgery to assist physicians in the preliminary diagnosis of metastatic cancer. This involves arranging immediate examinations for other sites of metastatic cancer after EBUS surgery, eliminating the need to wait for reports, thereby shortening the waiting time by more than half and enabling patients to detect other cancers earlier, allowing for early planning and implementation of treatment plans. Unlike previous studies on cell image classification, which have abundant datasets for training, this study must also be able to make effective classifications despite the limited amount of case data for lung metastatic cancer. In the realm of small data set classification methods, Few-shot learning (FSL) has become mainstream in recent years. Through its ability to train on small datasets and its strong generalization capabilities, FSL shows potential in this task of lung metastatic cell image classification. This study will adopt the approach of Few-shot learning, referencing existing proposed models, and designing a model architecture for classifying lung metastases cell images. Batch Spectral Regularization (BSR) will be incorporated as a loss update parameter, and the Finetune method of PMF will be modified. In terms of test results, the addition of BSR and the modified Finetune method further increases the accuracy by 8.89% to 65.60%, outperforming other FSL methods. This study confirms that FSL is superior to supervised and transfer learning in classifying metastatic cancer and demonstrates that using BSR as a loss function and modifying Finetune can enhance the model's capabilities.","sentences":["This study aims to establish a computer-aided diagnosis system for endobronchial ultrasound (EBUS) surgery to assist physicians in the preliminary diagnosis of metastatic cancer.","This involves arranging immediate examinations for other sites of metastatic cancer after EBUS surgery, eliminating the need to wait for reports, thereby shortening the waiting time by more than half and enabling patients to detect other cancers earlier, allowing for early planning and implementation of treatment plans.","Unlike previous studies on cell image classification, which have abundant datasets for training, this study must also be able to make effective classifications despite the limited amount of case data for lung metastatic cancer.","In the realm of small data set classification methods, Few-shot learning (FSL) has become mainstream in recent years.","Through its ability to train on small datasets and its strong generalization capabilities, FSL shows potential in this task of lung metastatic cell image classification.","This study will adopt the approach of Few-shot learning, referencing existing proposed models, and designing a model architecture for classifying lung metastases cell images.","Batch Spectral Regularization (BSR) will be incorporated as a loss update parameter, and the Finetune method of PMF will be modified.","In terms of test results, the addition of BSR and the modified Finetune method further increases the accuracy by 8.89% to 65.60%, outperforming other FSL methods.","This study confirms that FSL is superior to supervised and transfer learning in classifying metastatic cancer and demonstrates that using BSR as a loss function and modifying Finetune can enhance the model's capabilities."],"url":"http://arxiv.org/abs/2404.06080v1","category":"eess.IV"}
{"created":"2024-04-09 07:32:55","title":"End-to-end training of Multimodal Model and ranking Model","abstract":"Traditional recommender systems heavily rely on ID features, which often encounter challenges related to cold-start and generalization. Modeling pre-extracted content features can mitigate these issues, but is still a suboptimal solution due to the discrepancies between training tasks and model parameters. End-to-end training presents a promising solution for these problems, yet most of the existing works mainly focus on retrieval models, leaving the multimodal techniques under-utilized. In this paper, we propose an industrial multimodal recommendation framework named EM3: End-to-end training of Multimodal Model and ranking Model, which sufficiently utilizes multimodal information and allows personalized ranking tasks to directly train the core modules in the multimodal model to obtain more task-oriented content features, without overburdening resource consumption. First, we propose Fusion-Q-Former, which consists of transformers and a set of trainable queries, to fuse different modalities and generate fixed-length and robust multimodal embeddings. Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length. Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings. In experiments, we implement EM3 on different ranking models in two scenario, achieving significant improvements in both offline evaluation and online A/B test, verifying the generalizability of our method. Ablation studies and visualization are also performed. Furthermore, we also conduct experiments on two public datasets to show that our proposed method outperforms the state-of-the-art methods.","sentences":["Traditional recommender systems heavily rely on ID features, which often encounter challenges related to cold-start and generalization.","Modeling pre-extracted content features can mitigate these issues, but is still a suboptimal solution due to the discrepancies between training tasks and model parameters.","End-to-end training presents a promising solution for these problems, yet most of the existing works mainly focus on retrieval models, leaving the multimodal techniques under-utilized.","In this paper, we propose an industrial multimodal recommendation framework named EM3: End-to-end training of Multimodal Model and ranking Model, which sufficiently utilizes multimodal information and allows personalized ranking tasks to directly train the core modules in the multimodal model to obtain more task-oriented content features, without overburdening resource consumption.","First, we propose Fusion-Q-Former, which consists of transformers and a set of trainable queries, to fuse different modalities and generate fixed-length and robust multimodal embeddings.","Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length.","Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings.","In experiments, we implement EM3 on different ranking models in two scenario, achieving significant improvements in both offline evaluation and online A/B test, verifying the generalizability of our method.","Ablation studies and visualization are also performed.","Furthermore, we also conduct experiments on two public datasets to show that our proposed method outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.06078v1","category":"cs.IR"}
{"created":"2024-04-09 07:17:55","title":"Transmit and Receive Antenna Port Selection for Channel Capacity Maximization in Fluid-MIMO Systems","abstract":"In this letter, we study a discrete optimization problem, namely, the maximization of channel capacity in fluid multiple-input multiple-output (fluid-MIMO) systems through the selection of antenna ports/positions at the transmitter and the receiver. First, we present a new joint convex relaxation (JCR) problem by using an upper bound on the channel capacity and exploiting the binary nature of optimization variables. Then, we develop and analyze two optimization algorithms with different performance-complexity tradeoffs: the first is based on JCR and reduced exhaustive search (JCR&RES), while the second on JCR and alternating optimization (JCR&AO). Finally, numerical results show that the proposed algorithms significantly outperform two baseline schemes, the random port selection and the conventional MIMO setup.","sentences":["In this letter, we study a discrete optimization problem, namely, the maximization of channel capacity in fluid multiple-input multiple-output (fluid-MIMO) systems through the selection of antenna ports/positions at the transmitter and the receiver.","First, we present a new joint convex relaxation (JCR) problem by using an upper bound on the channel capacity and exploiting the binary nature of optimization variables.","Then, we develop and analyze two optimization algorithms with different performance-complexity tradeoffs: the first is based on JCR and reduced exhaustive search (JCR&RES), while the second on JCR and alternating optimization (JCR&AO).","Finally, numerical results show that the proposed algorithms significantly outperform two baseline schemes, the random port selection and the conventional MIMO setup."],"url":"http://arxiv.org/abs/2404.06072v1","category":"eess.SP"}
{"created":"2024-04-09 07:09:48","title":"Weak colourings of Kirkman triple systems","abstract":"A $\\delta$-colouring of the point set of a block design is said to be {\\em weak} if no block is monochromatic. The {\\em weak chromatic number} $\\chi(S)$ of a block design $S$ is the smallest integer $\\delta$ such that $S$ has a weak $\\delta$-colouring. It has previously been shown that any Steiner triple system has weak chromatic number at least $3$ and that for each $v\\equiv 1$ or $3\\pmod{6}$ there exists a Steiner triple system on $v$ points that has weak chromatic number $3$. Moreover, for each integer $\\delta \\geq 3$ there exist infinitely many Steiner triple systems with weak chromatic number $\\delta$.   We consider colourings of the subclass of Steiner triple systems which are resolvable, namely Kirkman triple systems. We show that for each $v\\equiv 3\\pmod{6}$ there exists a Kirkman triple system on $v$ points with weak chromatic number $3$. We also show that for each integer $\\delta \\geq 3$, there exist infinitely many Kirkman triple systems with weak chromatic number $\\delta$. We close with several open problems.","sentences":["A $\\delta$-colouring of the point set of a block design is said to be {\\em weak} if no block is monochromatic.","The {\\em weak chromatic number} $\\chi(S)$ of a block design $S$ is the smallest integer $\\delta$ such that $S$ has a weak $\\delta$-colouring.","It has previously been shown that any Steiner triple system has weak chromatic number at least $3$ and that for each $v\\equiv 1$ or $3\\pmod{6}$ there exists a Steiner triple system on $v$ points that has weak chromatic number $3$. Moreover, for each integer $\\delta \\geq 3$ there exist infinitely many Steiner triple systems with weak chromatic number $\\delta$.   We consider colourings of the subclass of Steiner triple systems which are resolvable, namely Kirkman triple systems.","We show that for each $v\\equiv 3\\pmod{6}$ there exists a Kirkman triple system on $v$ points with weak chromatic number $3$. We also show that for each integer $\\delta \\geq 3$, there exist infinitely many Kirkman triple systems with weak chromatic number $\\delta$.","We close with several open problems."],"url":"http://arxiv.org/abs/2404.06066v1","category":"math.CO"}
{"created":"2024-04-09 06:58:59","title":"Asymptotic values, deficiencies and Bank-Laine functions","abstract":"Some results are proved concerning asymptotic and deficient values in connection with the second order linear differential equation $y'' + Ay = 0$, in which the coefficient $A$ is entire.","sentences":["Some results are proved concerning asymptotic and deficient values in connection with the second order linear differential equation $y'' +","Ay = 0$, in which the coefficient $A$ is entire."],"url":"http://arxiv.org/abs/2404.06062v1","category":"math.CV"}
{"created":"2024-04-09 06:57:31","title":"A preconditioned iteration method for solving saddle point problems","abstract":"This paper introduces a preconditioned method designed to comprehensively address the saddle point system with the aim of improving convergence efficiency. In the preprocessor construction phase, a technical approach for solving the approximate inverse matrix of sparse matrices is presented. The effectiveness of the proposed method is demonstrated through numerical examples, emphasizing its efficacy in approximating the inverse matrix. Furthermore, the preprocessing technology includes a low-rank processing step, effectively reducing algorithmic complexity. Numerical experiments validate the effectiveness and feasibility of PSLR-GMRES in solving the saddle point system.","sentences":["This paper introduces a preconditioned method designed to comprehensively address the saddle point system with the aim of improving convergence efficiency.","In the preprocessor construction phase, a technical approach for solving the approximate inverse matrix of sparse matrices is presented.","The effectiveness of the proposed method is demonstrated through numerical examples, emphasizing its efficacy in approximating the inverse matrix.","Furthermore, the preprocessing technology includes a low-rank processing step, effectively reducing algorithmic complexity.","Numerical experiments validate the effectiveness and feasibility of PSLR-GMRES in solving the saddle point system."],"url":"http://arxiv.org/abs/2404.06061v1","category":"math.NA"}
{"created":"2024-04-09 06:45:46","title":"Demonstration of Lossy Linear Transformations and Two-Photon Interference on a Photonic Chip","abstract":"Studying quantum correlations in the presence of loss is of critical importance for the physical modeling of real quantum systems. Here, we demonstrate the control of spatial correlations between entangled photons in a photonic chip, designed and modeled using the singular value decomposition approach. We show that engineered loss, using an auxiliary waveguide, allows one to invert the spatial statistics from bunching to antibunching. Furthermore, we study the photon statistics within the loss-emulating channel and observe photon coincidences, which may provide insights into the design of quantum photonic integrated chips.","sentences":["Studying quantum correlations in the presence of loss is of critical importance for the physical modeling of real quantum systems.","Here, we demonstrate the control of spatial correlations between entangled photons in a photonic chip, designed and modeled using the singular value decomposition approach.","We show that engineered loss, using an auxiliary waveguide, allows one to invert the spatial statistics from bunching to antibunching.","Furthermore, we study the photon statistics within the loss-emulating channel and observe photon coincidences, which may provide insights into the design of quantum photonic integrated chips."],"url":"http://arxiv.org/abs/2404.06056v1","category":"quant-ph"}
{"created":"2024-04-09 06:40:28","title":"Online/Offline Learning to Enable Robust Beamforming: Limited Feedback Meets Deep Generative Models","abstract":"Robust beamforming is a pivotal technique in massive multiple-input multiple-output (MIMO) systems as it mitigates interference among user equipment (UE). One current risk-neutral approach to robust beamforming is the stochastic weighted minimum mean square error method (WMMSE). However, this method necessitates statistical channel information, which is typically inaccessible, particularly in fifth-generation new radio frequency division duplex cellular systems with limited feedback. To tackle this challenge, we propose a novel approach that leverages a channel variational auto-encoder (CVAE) to simulate channel behaviors using limited feedback, eliminating the need for specific distribution assumptions present in existing methods. To seamlessly integrate model learning into practical wireless communication systems, this paper introduces two learning strategies to prepare the CVAE model for practical deployment. Firstly, motivated by the digital twin technology, we advocate employing a high-performance channel simulator to generate training data, enabling pretraining of the proposed CVAE while ensuring non-disruption to the practical wireless communication system. Moreover, we present an alternative online method for CVAE learning, where online training data is sourced based on channel estimations using Type II codebook. Numerical results demonstrate the effectiveness of these strategies, highlighting their exceptional performance in channel generation and robust beamforming applications.","sentences":["Robust beamforming is a pivotal technique in massive multiple-input multiple-output (MIMO) systems as it mitigates interference among user equipment (UE).","One current risk-neutral approach to robust beamforming is the stochastic weighted minimum mean square error method (WMMSE).","However, this method necessitates statistical channel information, which is typically inaccessible, particularly in fifth-generation new radio frequency division duplex cellular systems with limited feedback.","To tackle this challenge, we propose a novel approach that leverages a channel variational auto-encoder (CVAE) to simulate channel behaviors using limited feedback, eliminating the need for specific distribution assumptions present in existing methods.","To seamlessly integrate model learning into practical wireless communication systems, this paper introduces two learning strategies to prepare the CVAE model for practical deployment.","Firstly, motivated by the digital twin technology, we advocate employing a high-performance channel simulator to generate training data, enabling pretraining of the proposed CVAE while ensuring non-disruption to the practical wireless communication system.","Moreover, we present an alternative online method for CVAE learning, where online training data is sourced based on channel estimations using Type II codebook.","Numerical results demonstrate the effectiveness of these strategies, highlighting their exceptional performance in channel generation and robust beamforming applications."],"url":"http://arxiv.org/abs/2404.06055v1","category":"stat.AP"}
{"created":"2024-04-09 06:40:17","title":"Pseudo MIMO (pMIMO): An Energy and Spectral Efficient MIMO-OFDM System","abstract":"This article introduces an energy and spectral efficient multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) transmission scheme designed for the future sixth generation (6G) wireless communication networks. The approach involves connecting each receiving radio frequency (RF) chain with multiple antenna elements and conducting sample-level adjustments for receiving beamforming patterns. The proposed system architecture and the dedicated signal processing methods enable the scheme to transmit a bigger number of parallel data streams than the number of receiving RF chains, achieving a spectral efficiency performance close to that of a fully digital (FD) MIMO system with the same number of antenna elements, each equipped with an RF chain. We refer to this system as a ''pseudo MIMO'' system due to its ability to mimic the functionality of additional invisible RF chains. The article begins with introducing the underlying principles of pseudo MIMO and discussing potential hardware architectures for its implementation. We then highlight several advantages of integrating pseudo MIMO into next-generation wireless networks. To demonstrate the superiority of our proposed pseudo MIMO transmission scheme to conventional MIMO systems, simulation results are presented. Additionally, we validate the feasibility of this new scheme by building the first pseudo MIMO prototype. Furthermore, we present some key challenges and outline potential directions for future research.","sentences":["This article introduces an energy and spectral efficient multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) transmission scheme designed for the future sixth generation (6G) wireless communication networks.","The approach involves connecting each receiving radio frequency (RF) chain with multiple antenna elements and conducting sample-level adjustments for receiving beamforming patterns.","The proposed system architecture and the dedicated signal processing methods enable the scheme to transmit a bigger number of parallel data streams than the number of receiving RF chains, achieving a spectral efficiency performance close to that of a fully digital (FD) MIMO system with the same number of antenna elements, each equipped with an RF chain.","We refer to this system as a ''pseudo MIMO'' system due to its ability to mimic the functionality of additional invisible RF chains.","The article begins with introducing the underlying principles of pseudo MIMO and discussing potential hardware architectures for its implementation.","We then highlight several advantages of integrating pseudo MIMO into next-generation wireless networks.","To demonstrate the superiority of our proposed pseudo MIMO transmission scheme to conventional MIMO systems, simulation results are presented.","Additionally, we validate the feasibility of this new scheme by building the first pseudo MIMO prototype.","Furthermore, we present some key challenges and outline potential directions for future research."],"url":"http://arxiv.org/abs/2404.06054v1","category":"eess.SP"}
{"created":"2024-04-09 06:29:17","title":"Regularized relativistic corrections for polyelectronic and polyatomic systems with explicitly correlated Gaussians","abstract":"Drachmann's regularization approach is implemented for floating explicitly correlated Gaussians (fECGs) and molecular systems. Earlier applications of Drachmannized relativistic corrections for molecular systems were hindered due to the unknown analytic matrix elements of $1/r_{ix}1/r_{jy}$-type operators with fECGs. In the present work, one of the $1/r$ factors is approximated by a linear combination of Gaussians, which results in calculable integrals. The numerical approach is found to be precise and robust over a range of molecular systems and nuclear configurations, and thus, it opens the route towards an automated evaluation of high-precision relativistic corrections over potential energy surfaces of polyatomic systems. Furthermore, the newly developed integration approach makes it possible to construct the matrix representation of the square of the electronic Hamiltonian relevant for energy lower-bound as well as time-dependent computations of molecular systems with a flexible and high-precision fECG basis representation.","sentences":["Drachmann's regularization approach is implemented for floating explicitly correlated Gaussians (fECGs) and molecular systems.","Earlier applications of Drachmannized relativistic corrections for molecular systems were hindered due to the unknown analytic matrix elements of $1/r_{ix}1/r_{jy}$-type operators with fECGs.","In the present work, one of the $1/r$ factors is approximated by a linear combination of Gaussians, which results in calculable integrals.","The numerical approach is found to be precise and robust over a range of molecular systems and nuclear configurations, and thus, it opens the route towards an automated evaluation of high-precision relativistic corrections over potential energy surfaces of polyatomic systems.","Furthermore, the newly developed integration approach makes it possible to construct the matrix representation of the square of the electronic Hamiltonian relevant for energy lower-bound as well as time-dependent computations of molecular systems with a flexible and high-precision fECG basis representation."],"url":"http://arxiv.org/abs/2404.06051v1","category":"physics.chem-ph"}
{"created":"2024-04-09 06:22:50","title":"Quantum computing topological invariants of two-dimensional quantum matter","abstract":"Quantum algorithms provide a potential strategy for solving computational problems that are intractable by classical means. Computing the topological invariants of topological matter is one central problem in research on quantum materials, and a variety of numerical approaches for this purpose have been developed. However, the complexity of quantum many-body Hamiltonians makes calculations of topological invariants challenging for interacting systems. Here, we present two quantum circuits for calculating Chern numbers of two-dimensional quantum matter on quantum computers. Both circuits combine a gate-based adiabatic time-evolution over the discretized Brillouin zone with particular phase estimation techniques. The first algorithm uses many qubits, and we analyze it using a tensor-network simulator of quantum circuits. The second circuit uses fewer circuits, and we implement it experimentally on a quantum computer based on superconducting qubits. Our results establish a method for computing topological invariants with quantum circuits, taking a step towards characterizing interacting topological quantum matter using quantum computers.","sentences":["Quantum algorithms provide a potential strategy for solving computational problems that are intractable by classical means.","Computing the topological invariants of topological matter is one central problem in research on quantum materials, and a variety of numerical approaches for this purpose have been developed.","However, the complexity of quantum many-body Hamiltonians makes calculations of topological invariants challenging for interacting systems.","Here, we present two quantum circuits for calculating Chern numbers of two-dimensional quantum matter on quantum computers.","Both circuits combine a gate-based adiabatic time-evolution over the discretized Brillouin zone with particular phase estimation techniques.","The first algorithm uses many qubits, and we analyze it using a tensor-network simulator of quantum circuits.","The second circuit uses fewer circuits, and we implement it experimentally on a quantum computer based on superconducting qubits.","Our results establish a method for computing topological invariants with quantum circuits, taking a step towards characterizing interacting topological quantum matter using quantum computers."],"url":"http://arxiv.org/abs/2404.06048v1","category":"quant-ph"}
{"created":"2024-04-09 06:07:03","title":"Automatic Configuration Tuning on Cloud Database: A Survey","abstract":"Faced with the challenges of big data, modern cloud database management systems are designed to efficiently store, organize, and retrieve data, supporting optimal performance, scalability, and reliability for complex data processing and analysis. However, achieving good performance in modern databases is non-trivial as they are notorious for having dozens of configurable knobs, such as hardware setup, software setup, database physical and logical design, etc., that control runtime behaviors and impact database performance. To find the optimal configuration for achieving optimal performance, extensive research has been conducted on automatic parameter tuning in DBMS. This paper provides a comprehensive survey of predominant configuration tuning techniques, including Bayesian optimization-based solutions, Neural network-based solutions, Reinforcement learning-based solutions, and Search-based solutions. Moreover, it investigates the fundamental aspects of parameter tuning pipeline, including tuning objective, workload characterization, feature pruning, knowledge from experience, configuration recommendation, and experimental settings. We highlight technique comparisons in each component, corresponding solutions, and introduce the experimental setting for performance evaluation. Finally, we conclude this paper and present future research opportunities. This paper aims to assist future researchers and practitioners in gaining a better understanding of automatic parameter tuning in cloud databases by providing state-of-the-art existing solutions, research directions, and evaluation benchmarks.","sentences":["Faced with the challenges of big data, modern cloud database management systems are designed to efficiently store, organize, and retrieve data, supporting optimal performance, scalability, and reliability for complex data processing and analysis.","However, achieving good performance in modern databases is non-trivial as they are notorious for having dozens of configurable knobs, such as hardware setup, software setup, database physical and logical design, etc., that control runtime behaviors and impact database performance.","To find the optimal configuration for achieving optimal performance, extensive research has been conducted on automatic parameter tuning in DBMS.","This paper provides a comprehensive survey of predominant configuration tuning techniques, including Bayesian optimization-based solutions, Neural network-based solutions, Reinforcement learning-based solutions, and Search-based solutions.","Moreover, it investigates the fundamental aspects of parameter tuning pipeline, including tuning objective, workload characterization, feature pruning, knowledge from experience, configuration recommendation, and experimental settings.","We highlight technique comparisons in each component, corresponding solutions, and introduce the experimental setting for performance evaluation.","Finally, we conclude this paper and present future research opportunities.","This paper aims to assist future researchers and practitioners in gaining a better understanding of automatic parameter tuning in cloud databases by providing state-of-the-art existing solutions, research directions, and evaluation benchmarks."],"url":"http://arxiv.org/abs/2404.06043v1","category":"cs.DB"}
{"created":"2024-04-09 06:01:52","title":"Tailoring the energy landscape of a Bloch point singularity with curvature","abstract":"Topological defects, or singularities, play a key role in the statics and dynamics of complex systems. In magnetism, Bloch point singularities represent point defects that mediate the nucleation of textures such as skyrmions and hopfions. However, while the textures are typically stabilised in chiral magnets, the influence of chirality on the Bloch point singularities remains relatively unexplored. Here we harness advanced three-dimensional nanofabrication to explore the influence of chirality on Bloch point singularities by introducing curvature-induced symmetry breaking in a ferromagnetic nanowire. Combining X-ray magnetic microscopy with the application of in situ magnetic fields, we demonstrate that Bloch point singularity-containing domain walls are stabilised in straight regions of the sample, and determine that curvature can be used to tune the energy landscape of the Bloch points. Not only are we able to pattern pinning points but, by controlling the gradient of curvature, we define asymmetric potential wells to realise a robust Bloch point shift-register with non-reciprocal behaviour. These insights into the influence of symmetry and chirality on singularities offers a route to the controlled nucleation and propagation of topological textures, providing opportunities for logic and computing devices.","sentences":["Topological defects, or singularities, play a key role in the statics and dynamics of complex systems.","In magnetism, Bloch point singularities represent point defects that mediate the nucleation of textures such as skyrmions and hopfions.","However, while the textures are typically stabilised in chiral magnets, the influence of chirality on the Bloch point singularities remains relatively unexplored.","Here we harness advanced three-dimensional nanofabrication to explore the influence of chirality on Bloch point singularities by introducing curvature-induced symmetry breaking in a ferromagnetic nanowire.","Combining X-ray magnetic microscopy with the application of in situ magnetic fields, we demonstrate that Bloch point singularity-containing domain walls are stabilised in straight regions of the sample, and determine that curvature can be used to tune the energy landscape of the Bloch points.","Not only are we able to pattern pinning points but, by controlling the gradient of curvature, we define asymmetric potential wells to realise a robust Bloch point shift-register with non-reciprocal behaviour.","These insights into the influence of symmetry and chirality on singularities offers a route to the controlled nucleation and propagation of topological textures, providing opportunities for logic and computing devices."],"url":"http://arxiv.org/abs/2404.06042v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 05:58:15","title":"Breathing New Life into Existing Visualizations: A Natural Language-Driven Manipulation Framework","abstract":"We propose an approach to manipulate existing interactive visualizations to answer users' natural language queries. We analyze the natural language tasks and propose a design space of a hierarchical task structure, which allows for a systematic decomposition of complex queries. We introduce a four-level visualization manipulation space to facilitate in-situ manipulations for visualizations, enabling a fine-grained control over the visualization elements. Our methods comprise two essential components: the natural language-to-task translator and the visualization manipulation parser. The natural language-to-task translator employs advanced NLP techniques to extract structured, hierarchical tasks from natural language queries, even those with varying degrees of ambiguity. The visualization manipulation parser leverages the hierarchical task structure to streamline these tasks into a sequence of atomic visualization manipulations. To illustrate the effectiveness of our approach, we provide real-world examples and experimental results. The evaluation highlights the precision of our natural language parsing capabilities and underscores the smooth transformation of visualization manipulations.","sentences":["We propose an approach to manipulate existing interactive visualizations to answer users' natural language queries.","We analyze the natural language tasks and propose a design space of a hierarchical task structure, which allows for a systematic decomposition of complex queries.","We introduce a four-level visualization manipulation space to facilitate in-situ manipulations for visualizations, enabling a fine-grained control over the visualization elements.","Our methods comprise two essential components: the natural language-to-task translator and the visualization manipulation parser.","The natural language-to-task translator employs advanced NLP techniques to extract structured, hierarchical tasks from natural language queries, even those with varying degrees of ambiguity.","The visualization manipulation parser leverages the hierarchical task structure to streamline these tasks into a sequence of atomic visualization manipulations.","To illustrate the effectiveness of our approach, we provide real-world examples and experimental results.","The evaluation highlights the precision of our natural language parsing capabilities and underscores the smooth transformation of visualization manipulations."],"url":"http://arxiv.org/abs/2404.06039v1","category":"cs.HC"}
{"created":"2024-04-09 05:56:06","title":"Effects of prestress in the coating of an elastic disk","abstract":"An elastic disk is coated with an elastic rod, uniformly prestressed with a tensile or compressive axial force. The prestress state is assumed to be induced by three different models of external radial load or by 'shrink-fit' forcing the coating onto the disk. The prestressed coating/disk system, when loaded with an additional and arbitrary incremental external load, experiences incremental displacement, strain, and stress, which are solved via complex potentials. The analysis incorporates models for both perfect and imperfect bonding at the coating/disk interface. The derived solution highlights the significant influence not only of the prestress but also of the method employed to generate it. These two factors lead, in different ways, to a loss or an increase in incremental stiffness for compressive or tensile prestress. The first bifurcation load of the structure (which differs for different prestress generations) is determined in a perturbative way. The results emphasize the importance of modelling the load and may find applications in flexible electronics and robot arms subject to pressure or uniformly-distributed radial forces.","sentences":["An elastic disk is coated with an elastic rod, uniformly prestressed with a tensile or compressive axial force.","The prestress state is assumed to be induced by three different models of external radial load or by 'shrink-fit' forcing the coating onto the disk.","The prestressed coating/disk system, when loaded with an additional and arbitrary incremental external load, experiences incremental displacement, strain, and stress, which are solved via complex potentials.","The analysis incorporates models for both perfect and imperfect bonding at the coating/disk interface.","The derived solution highlights the significant influence not only of the prestress but also of the method employed to generate it.","These two factors lead, in different ways, to a loss or an increase in incremental stiffness for compressive or tensile prestress.","The first bifurcation load of the structure (which differs for different prestress generations) is determined in a perturbative way.","The results emphasize the importance of modelling the load and may find applications in flexible electronics and robot arms subject to pressure or uniformly-distributed radial forces."],"url":"http://arxiv.org/abs/2404.06038v1","category":"physics.class-ph"}
{"created":"2024-04-09 05:49:04","title":"Space-Time Video Super-resolution with Neural Operator","abstract":"This paper addresses the task of space-time video super-resolution (ST-VSR). Existing methods generally suffer from inaccurate motion estimation and motion compensation (MEMC) problems for large motions. Inspired by recent progress in physics-informed neural networks, we model the challenges of MEMC in ST-VSR as a mapping between two continuous function spaces. Specifically, our approach transforms independent low-resolution representations in the coarse-grained continuous function space into refined representations with enriched spatiotemporal details in the fine-grained continuous function space. To achieve efficient and accurate MEMC, we design a Galerkin-type attention function to perform frame alignment and temporal interpolation. Due to the linear complexity of the Galerkin-type attention mechanism, our model avoids patch partitioning and offers global receptive fields, enabling precise estimation of large motions. The experimental results show that the proposed method surpasses state-of-the-art techniques in both fixed-size and continuous space-time video super-resolution tasks.","sentences":["This paper addresses the task of space-time video super-resolution (ST-VSR).","Existing methods generally suffer from inaccurate motion estimation and motion compensation (MEMC) problems for large motions.","Inspired by recent progress in physics-informed neural networks, we model the challenges of MEMC in ST-VSR as a mapping between two continuous function spaces.","Specifically, our approach transforms independent low-resolution representations in the coarse-grained continuous function space into refined representations with enriched spatiotemporal details in the fine-grained continuous function space.","To achieve efficient and accurate MEMC, we design a Galerkin-type attention function to perform frame alignment and temporal interpolation.","Due to the linear complexity of the Galerkin-type attention mechanism, our model avoids patch partitioning and offers global receptive fields, enabling precise estimation of large motions.","The experimental results show that the proposed method surpasses state-of-the-art techniques in both fixed-size and continuous space-time video super-resolution tasks."],"url":"http://arxiv.org/abs/2404.06036v1","category":"cs.CV"}
{"created":"2024-04-09 05:38:59","title":"Inverse melting and intertwined orders in PrCuSb$_2$","abstract":"Much of the rich physics of correlated systems is manifested in the diverse range of intertwined ordered phases and other quantum states that are associated with different electronic and structural degrees of freedom. Here we find that PrCuSb$_2$ exhibits such phenomena, which at ambient pressure exhibits a fragile antiferromagnetic order, where cooling in a small $c$ axis magnetic field leads to an additional transition to a field-induced ferromagnetic state. This corresponds to an 'inverse melting' effect, whereby further cooling the system restores symmetries of the paramagnetic state broken at the antiferromagnetic transition. Moreover, hydrostatic pressure induces an additional first-order transition at low temperatures, which despite being not likely associated with solely magnetic degrees of freedom, is closely entwined with the magnetic order, disappearing once antiferromagnetism is destroyed by pressure or magnetic fields. Consequently, PrCuSb$_2$ presents a distinct scenario for interplay between different orders, underscoring the breadth of such behaviors within one family of correlated materials.","sentences":["Much of the rich physics of correlated systems is manifested in the diverse range of intertwined ordered phases and other quantum states that are associated with different electronic and structural degrees of freedom.","Here we find that PrCuSb$_2$ exhibits such phenomena, which at ambient pressure exhibits a fragile antiferromagnetic order, where cooling in a small $c$ axis magnetic field leads to an additional transition to a field-induced ferromagnetic state.","This corresponds to an 'inverse melting' effect, whereby further cooling the system restores symmetries of the paramagnetic state broken at the antiferromagnetic transition.","Moreover, hydrostatic pressure induces an additional first-order transition at low temperatures, which despite being not likely associated with solely magnetic degrees of freedom, is closely entwined with the magnetic order, disappearing once antiferromagnetism is destroyed by pressure or magnetic fields.","Consequently, PrCuSb$_2$ presents a distinct scenario for interplay between different orders, underscoring the breadth of such behaviors within one family of correlated materials."],"url":"http://arxiv.org/abs/2404.06032v1","category":"cond-mat.str-el"}
{"created":"2024-04-09 05:30:58","title":"Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation","abstract":"The domain of computer vision has experienced significant advancements in facial-landmark detection, becoming increasingly essential across various applications such as augmented reality, facial recognition, and emotion analysis. Unlike object detection or semantic segmentation, which focus on identifying objects and outlining boundaries, faciallandmark detection aims to precisely locate and track critical facial features. However, deploying deep learning-based facial-landmark detection models on embedded systems with limited computational resources poses challenges due to the complexity of facial features, especially in dynamic settings. Additionally, ensuring robustness across diverse ethnicities and expressions presents further obstacles. Existing datasets often lack comprehensive representation of facial nuances, particularly within populations like those in Taiwan. This paper introduces a novel approach to address these challenges through the development of a knowledge distillation method. By transferring knowledge from larger models to smaller ones, we aim to create lightweight yet powerful deep learning models tailored specifically for facial-landmark detection tasks. Our goal is to design models capable of accurately locating facial landmarks under varying conditions, including diverse expressions, orientations, and lighting environments. The ultimate objective is to achieve high accuracy and real-time performance suitable for deployment on embedded systems. This method was successfully implemented and achieved a top 6th place finish out of 165 participants in the IEEE ICME 2024 PAIR competition.","sentences":["The domain of computer vision has experienced significant advancements in facial-landmark detection, becoming increasingly essential across various applications such as augmented reality, facial recognition, and emotion analysis.","Unlike object detection or semantic segmentation, which focus on identifying objects and outlining boundaries, faciallandmark detection aims to precisely locate and track critical facial features.","However, deploying deep learning-based facial-landmark detection models on embedded systems with limited computational resources poses challenges due to the complexity of facial features, especially in dynamic settings.","Additionally, ensuring robustness across diverse ethnicities and expressions presents further obstacles.","Existing datasets often lack comprehensive representation of facial nuances, particularly within populations like those in Taiwan.","This paper introduces a novel approach to address these challenges through the development of a knowledge distillation method.","By transferring knowledge from larger models to smaller ones, we aim to create lightweight yet powerful deep learning models tailored specifically for facial-landmark detection tasks.","Our goal is to design models capable of accurately locating facial landmarks under varying conditions, including diverse expressions, orientations, and lighting environments.","The ultimate objective is to achieve high accuracy and real-time performance suitable for deployment on embedded systems.","This method was successfully implemented and achieved a top 6th place finish out of 165 participants in the IEEE ICME 2024 PAIR competition."],"url":"http://arxiv.org/abs/2404.06029v1","category":"cs.CV"}
{"created":"2024-04-09 05:16:59","title":"Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks","abstract":"Distributed massive multiple-input multiple output (mMIMO) system for low earth orbit (LEO) satellite networks is introduced as a promising technique to provide broadband connectivity. Nevertheless, several challenges persist in implementing distributed mMIMO systems for LEO satellite networks. These challenges include providing scalable massive access implementation as the system complexity increases with network size. Another challenging issue is the asynchronous arrival of signals at the user terminals due to the different propagation delays among distributed antennas in space, which destroys the coherent transmission, and consequently degrades the system performance. In this paper, we propose a scalable distributed mMIMO system for LEO satellite networks based on dynamic user-centric clustering. Aiming to obtain scalable implementation, new algorithms for initial cooperative access, cluster selection, and cluster handover are provided. In addition, phase shift-aware precoding is implemented to compensate for the propagation delay phase shifts. The performance of the proposed user-centric distributed mMIMO is compared with two baseline configurations: the non-cooperative transmission systems, where each user connects to only a single satellite, and the full-cooperative distributed mMIMO systems, where all satellites contribute serving each user. The numerical results show the potential of the proposed distributed mMIMO system to enhance system spectral efficiency when compared to noncooperative transmission systems. Additionally, it demonstrates the ability to minimize the serving cluster size for each user, thereby reducing the overall system complexity in comparison to the full-cooperative distributed mMIMO systems.","sentences":["Distributed massive multiple-input multiple output (mMIMO) system for low earth orbit (LEO) satellite networks is introduced as a promising technique to provide broadband connectivity.","Nevertheless, several challenges persist in implementing distributed mMIMO systems for LEO satellite networks.","These challenges include providing scalable massive access implementation as the system complexity increases with network size.","Another challenging issue is the asynchronous arrival of signals at the user terminals due to the different propagation delays among distributed antennas in space, which destroys the coherent transmission, and consequently degrades the system performance.","In this paper, we propose a scalable distributed mMIMO system for LEO satellite networks based on dynamic user-centric clustering.","Aiming to obtain scalable implementation, new algorithms for initial cooperative access, cluster selection, and cluster handover are provided.","In addition, phase shift-aware precoding is implemented to compensate for the propagation delay phase shifts.","The performance of the proposed user-centric distributed mMIMO is compared with two baseline configurations: the non-cooperative transmission systems, where each user connects to only a single satellite, and the full-cooperative distributed mMIMO systems, where all satellites contribute serving each user.","The numerical results show the potential of the proposed distributed mMIMO system to enhance system spectral efficiency when compared to noncooperative transmission systems.","Additionally, it demonstrates the ability to minimize the serving cluster size for each user, thereby reducing the overall system complexity in comparison to the full-cooperative distributed mMIMO systems."],"url":"http://arxiv.org/abs/2404.06024v1","category":"cs.IT"}
{"created":"2024-04-09 05:07:26","title":"Combinational Nonuniform Timeslicing of Dynamic Networks","abstract":"Dynamic networks represent the complex and evolving interrelationships between real-world entities. Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis. Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem. In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem. Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis. We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data. The findings suggest that combining the two approaches offers the potential for more effective network analysis.","sentences":["Dynamic networks represent the complex and evolving interrelationships between real-world entities.","Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis.","Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem.","In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem.","Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis.","We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data.","The findings suggest that combining the two approaches offers the potential for more effective network analysis."],"url":"http://arxiv.org/abs/2404.06021v1","category":"cs.HC"}
{"created":"2024-04-09 04:59:22","title":"Preprocessed GMRES for fast solution of linear equations","abstract":"The article mainly introduces preprocessing algorithms for solving linear equation systems. This algorithm uses three algorithms as inner iterations, namely RPCG algorithm, ADI algorithm, and Kaczmarz algorithm. Then, it uses BA-GMRES as an outer iteration to solve the linear equation system. These three algorithms can indirectly generate preprocessing matrices, which are used for solving equation systems. In addition, we provide corresponding convergence analysis and numerical examples. Through numerical examples, we demonstrate the effectiveness and feasibility of these preprocessing methods. Furthermore, in the Kaczmarz algorithm, we introduce both constant step size and adaptive step size, and extend the parameter range of the Kaczmarz algorithm to $\\alpha\\in(0,\\infty)$. We also study the solution rate of linear equation systems using different step sizes. Numerical examples show that both constant step size and adaptive step size have higher solution efficiency than the solving algorithm without preprocessing.","sentences":["The article mainly introduces preprocessing algorithms for solving linear equation systems.","This algorithm uses three algorithms as inner iterations, namely RPCG algorithm, ADI algorithm, and Kaczmarz algorithm.","Then, it uses BA-GMRES as an outer iteration to solve the linear equation system.","These three algorithms can indirectly generate preprocessing matrices, which are used for solving equation systems.","In addition, we provide corresponding convergence analysis and numerical examples.","Through numerical examples, we demonstrate the effectiveness and feasibility of these preprocessing methods.","Furthermore, in the Kaczmarz algorithm, we introduce both constant step size and adaptive step size, and extend the parameter range of the Kaczmarz algorithm to $\\alpha\\in(0,\\infty)$. We also study the solution rate of linear equation systems using different step sizes.","Numerical examples show that both constant step size and adaptive step size have higher solution efficiency than the solving algorithm without preprocessing."],"url":"http://arxiv.org/abs/2404.06018v1","category":"math.NA"}
{"created":"2024-04-09 04:31:10","title":"Magnetic field control of continuous N\u00e9el vector rotation and N\u00e9el temperature in a van der Waals antiferromagnet","abstract":"In a collinear antiferromagnet, spins tend to cant towards the direction of an applied magnetic field, thereby decreasing the energy of the system. The canting angle becomes negligible when the magnetic field is small so that the induced anisotropic energy is substantially lower than the exchange energy. However, this tiny anisotropy can play a significant role when the intrinsic anisotropy of the antiferromagnet is small. In our work, we conduct direct imaging of the N\\'eel vector in a two-dimensional easy-plane antiferromagnet, MnPSe$_3$, with negligible spin canting under an external in-plane magnetic field. The small inherent in-plane anisotropy allows for the continuous rotation of the N\\'eel vector by ramping up the magnetic field in samples from the bulk to the monolayer. In monolayer samples, the applied magnetic field elevates the N\\'eel temperature 10$\\%$ at 5 tesla, as the combination of intrinsic and field-induced anisotropies set a critical temperature scale for fluctuations of the otherwise disordered N\\'eel vector field. Our study illuminates the contribution of field-induced anisotropy in two dimensional magnets with in-plane anisotropy. We also demonstrate that the strain can tune the spin flop transition field strength by one order of magnitude.","sentences":["In a collinear antiferromagnet, spins tend to cant towards the direction of an applied magnetic field, thereby decreasing the energy of the system.","The canting angle becomes negligible when the magnetic field is small so that the induced anisotropic energy is substantially lower than the exchange energy.","However, this tiny anisotropy can play a significant role when the intrinsic anisotropy of the antiferromagnet is small.","In our work, we conduct direct imaging of the N\\'eel vector in a two-dimensional easy-plane antiferromagnet, MnPSe$_3$, with negligible spin canting under an external in-plane magnetic field.","The small inherent in-plane anisotropy allows for the continuous rotation of the N\\'eel vector by ramping up the magnetic field in samples from the bulk to the monolayer.","In monolayer samples, the applied magnetic field elevates the N\\'eel temperature 10$\\%$ at 5 tesla, as the combination of intrinsic and field-induced anisotropies set a critical temperature scale for fluctuations of the otherwise disordered N\\'eel vector field.","Our study illuminates the contribution of field-induced anisotropy in two dimensional magnets with in-plane anisotropy.","We also demonstrate that the strain can tune the spin flop transition field strength by one order of magnitude."],"url":"http://arxiv.org/abs/2404.06010v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 04:30:23","title":"Compact Subvarieties of the Moduli Space of Complex Abelian Varieties","abstract":"We determine the maximal dimension of compact subvarieties of $\\mathcal{A}_g$, the moduli space of complex principally polarized abelian varieties of dimension $g$, and the maximal dimension of a compact subvariety through a very general point of $\\mathcal{A}_g$. This also allows us to draw some conclusions for compact subvarieties of the moduli space of complex curves of compact type.","sentences":["We determine the maximal dimension of compact subvarieties of $\\mathcal{A}_g$, the moduli space of complex principally polarized abelian varieties of dimension $g$, and the maximal dimension of a compact subvariety through a very general point of $\\mathcal{A}_g$. This also allows us to draw some conclusions for compact subvarieties of the moduli space of complex curves of compact type."],"url":"http://arxiv.org/abs/2404.06009v1","category":"math.AG"}
{"created":"2024-04-09 04:06:03","title":"Efficient quantum Gibbs samplers with Kubo--Martin--Schwinger detailed balance condition","abstract":"Lindblad dynamics and other open-system dynamics provide a promising path towards efficient Gibbs sampling on quantum computers. In these proposals, the Lindbladian is obtained via an algorithmic construction akin to designing an artificial thermostat in classical Monte Carlo or molecular dynamics methods, rather than treated as an approximation to weakly coupled system-bath unitary dynamics. Recently, Chen, Kastoryano, and Gily\\'en (arXiv:2311.09207) introduced the first efficiently implementable Lindbladian satisfying the Kubo--Martin--Schwinger (KMS) detailed balance condition, which ensures that the Gibbs state is a fixed point of the dynamics and is applicable to non-commuting Hamiltonians. This Gibbs sampler uses a continuously parameterized set of jump operators, and the energy resolution required for implementing each jump operator depends only logarithmically on the precision and the mixing time. In this work, we build upon the structural characterization of KMS detailed balanced Lindbladians by Fagnola and Umanit\\`a, and develop a family of efficient quantum Gibbs samplers that only use a discrete set of jump operators (the number can be as few as one). Our methodology simplifies the implementation and the analysis of Lindbladian-based quantum Gibbs samplers, and encompasses the construction of Chen, Kastoryano, and Gily\\'en as a special instance.","sentences":["Lindblad dynamics and other open-system dynamics provide a promising path towards efficient Gibbs sampling on quantum computers.","In these proposals, the Lindbladian is obtained via an algorithmic construction akin to designing an artificial thermostat in classical Monte Carlo or molecular dynamics methods, rather than treated as an approximation to weakly coupled system-bath unitary dynamics.","Recently, Chen, Kastoryano, and Gily\\'en (arXiv:2311.09207) introduced the first efficiently implementable Lindbladian satisfying the Kubo--Martin--Schwinger (KMS) detailed balance condition, which ensures that the Gibbs state is a fixed point of the dynamics and is applicable to non-commuting Hamiltonians.","This Gibbs sampler uses a continuously parameterized set of jump operators, and the energy resolution required for implementing each jump operator depends only logarithmically on the precision and the mixing time.","In this work, we build upon the structural characterization of KMS detailed balanced Lindbladians by Fagnola and Umanit\\`a, and develop a family of efficient quantum Gibbs samplers that only use a discrete set of jump operators (the number can be as few as one).","Our methodology simplifies the implementation and the analysis of Lindbladian-based quantum Gibbs samplers, and encompasses the construction of Chen, Kastoryano, and Gily\\'en as a special instance."],"url":"http://arxiv.org/abs/2404.05998v1","category":"quant-ph"}
{"created":"2024-04-09 04:01:55","title":"First-Order Vortex Lattice Melting in Bilayer Ice: A Monte Carlo Method Study","abstract":"Inspired by the stable bilayer water ice grown in the laboratory (Nature 577, 60, 2020), we propose a model representing water ice as a two-layer six-vertex model. Using the loop update Monte Carlo method, we unveil meaningful findings. While the square lattice six-vertex model exhibits an antiferromagnetic to disordered phase transition known as the Berezinskii-Kosterlitz-Thouless transition, we observe a different scenario for the bilayer six-vertex model, where the transition type transforms into an Ising transition. We discover the emergence of vortices in the disordered phase, and to stabilize them, vortex excitation is induced. This leads to the presence of distinct 1/2 filling and 2/3 filling vortex lattice phases. Importantly, we identify the phase transitions between the vortex lattice phase and the disorder phase, as well as between the 1/2 and 2/3 vortex lattices, as being of first order. Our findings provide valuable insights into the nature of phase transitions occurring in layered water ice and artificial spin ice systems in experimental setups.","sentences":["Inspired by the stable bilayer water ice grown in the laboratory (Nature 577, 60, 2020), we propose a model representing water ice as a two-layer six-vertex model.","Using the loop update Monte Carlo method, we unveil meaningful findings.","While the square lattice six-vertex model exhibits an antiferromagnetic to disordered phase transition known as the Berezinskii-Kosterlitz-Thouless transition, we observe a different scenario for the bilayer six-vertex model, where the transition type transforms into an Ising transition.","We discover the emergence of vortices in the disordered phase, and to stabilize them, vortex excitation is induced.","This leads to the presence of distinct 1/2 filling and 2/3 filling vortex lattice phases.","Importantly, we identify the phase transitions between the vortex lattice phase and the disorder phase, as well as between the 1/2 and 2/3 vortex lattices, as being of first order.","Our findings provide valuable insights into the nature of phase transitions occurring in layered water ice and artificial spin ice systems in experimental setups."],"url":"http://arxiv.org/abs/2404.05996v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 03:47:48","title":"Event-enhanced Retrieval in Real-time Search","abstract":"The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions. However, existing EBR models often face the \"semantic drift\" problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information. To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate contrastive learning to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of information retrieval. The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval .","sentences":["The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions.","However, existing EBR models often face the \"semantic drift\" problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps.","This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information.","To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR.","We incorporate contrastive learning to accompany pairwise learning for encoder optimization.","Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning.","This decoder module can be removed during inference.","Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance.","We believe that this approach will provide new perspectives in the field of information retrieval.","The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval ."],"url":"http://arxiv.org/abs/2404.05989v1","category":"cs.CL"}
{"created":"2024-04-09 03:28:59","title":"On Achievable Covert Communication Performance under CSI Estimation Error and Feedback Delay","abstract":"Covert communication's effectiveness critically depends on precise channel state information (CSI). This paper investigates the impact of imperfect CSI on achievable covert communication performance in a two-hop relay system. Firstly, we introduce a two-hop covert transmission scheme utilizing channel inversion power control (CIPC) to manage opportunistic interference, eliminating the receiver's self-interference. Given that CSI estimation error (CEE) and feedback delay (FD) are the two primary factors leading to imperfect CSI, we construct a comprehensive theoretical model to accurately characterize their effects on CSI quality. With the aid of this model, we then derive closed-form solutions for detection error probability (DEP) and covert rate (CR), establishing an analytical framework to delineate the inherent relationship between CEE, FD, and covert performance. Furthermore, to mitigate the adverse effects of imperfect CSI on achievable covert performance, we investigate the joint optimization of channel inversion power and data symbol length to maximize CR under DEP constraints and propose an iterative alternating algorithm to solve the bi-dimensional non-convex optimization problem. Finally, extensive experimental results validate our theoretical framework and illustrate the impact of imperfect CSI on achievable covert performance.","sentences":["Covert communication's effectiveness critically depends on precise channel state information (CSI).","This paper investigates the impact of imperfect CSI on achievable covert communication performance in a two-hop relay system.","Firstly, we introduce a two-hop covert transmission scheme utilizing channel inversion power control (CIPC) to manage opportunistic interference, eliminating the receiver's self-interference.","Given that CSI estimation error (CEE) and feedback delay (FD) are the two primary factors leading to imperfect CSI, we construct a comprehensive theoretical model to accurately characterize their effects on CSI quality.","With the aid of this model, we then derive closed-form solutions for detection error probability (DEP) and covert rate (CR), establishing an analytical framework to delineate the inherent relationship between CEE, FD, and covert performance.","Furthermore, to mitigate the adverse effects of imperfect CSI on achievable covert performance, we investigate the joint optimization of channel inversion power and data symbol length to maximize CR under DEP constraints and propose an iterative alternating algorithm to solve the bi-dimensional non-convex optimization problem.","Finally, extensive experimental results validate our theoretical framework and illustrate the impact of imperfect CSI on achievable covert performance."],"url":"http://arxiv.org/abs/2404.05983v1","category":"cs.IT"}
{"created":"2024-04-09 03:20:04","title":"Properties of a Majorana fermion ensemble with exciton-like mass","abstract":"Considering the relativistic scenario, we dedicate our study to the relativistic quantum description of one-dimensional Majorana fermions. Thus, we focus on aspects related to exciton-like particles. Seeking to reach our purpose, one analyzes the relativistic quantum mechanical system characterized by an effective mass distribution. In this context, we adopt an exciton-like position-dependent mass without impurity, i.e., without electromagnetic interactions. From this perspective, one notes results of noteworthy interest as consequences of the theory adopted. For instance, we highlight that, even without interaction, exciton-like Majorana fermions manifest theoretically bound states. Also, we construct a Majorana fermion ensemble with effective mass immersed in a thermal reservoir. That allows for a thorough investigation of the thermodynamic properties of the system. Among the thermodynamic characteristics studied in the canonical ensemble, we focus on the Helmholtz free energy, mean energy, entropy, and heat capacity. The numerical results obtained for these thermodynamic properties corroborate the validity of the Dulong-Petit law for our system.","sentences":["Considering the relativistic scenario, we dedicate our study to the relativistic quantum description of one-dimensional Majorana fermions.","Thus, we focus on aspects related to exciton-like particles.","Seeking to reach our purpose, one analyzes the relativistic quantum mechanical system characterized by an effective mass distribution.","In this context, we adopt an exciton-like position-dependent mass without impurity, i.e., without electromagnetic interactions.","From this perspective, one notes results of noteworthy interest as consequences of the theory adopted.","For instance, we highlight that, even without interaction, exciton-like Majorana fermions manifest theoretically bound states.","Also, we construct a Majorana fermion ensemble with effective mass immersed in a thermal reservoir.","That allows for a thorough investigation of the thermodynamic properties of the system.","Among the thermodynamic characteristics studied in the canonical ensemble, we focus on the Helmholtz free energy, mean energy, entropy, and heat capacity.","The numerical results obtained for these thermodynamic properties corroborate the validity of the Dulong-Petit law for our system."],"url":"http://arxiv.org/abs/2404.05978v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 03:05:01","title":"On Variation of Light Curves and Broad Emission Lines for Periodic QSOs from co-rotating Supermassive binary black holes in elliptical orbits","abstract":"Context. Periodic QSOs are considered as candidates of supermassive binary black hole (BBH) systems in galactic centers. Further confirmation of these candidates may require different lines of observational evidences. Aims. Assuming the Doopler boosting scenario, in this paper we investigate the (coherent) variations of both broad emission lines (BELs) and continuum light curves for active BBH systems surrounding by a circumbinary broad line region (cBLR) and focus on their dependence on the eccentric orbital configuration. Methods. We calculate the variation of continuum light and the Doppler enhanced/weakened photoionization of each BLR cloud according to the motion of BBHs in elliptical orbits, and finally obtain the coherent variation of the continuum and BELs. Results. We find that both the amplitude and variation pattern of the continuum light curves and the evolution of the BEL profiles sensitively depend on the eccentric orbital configuration of BBH systems. If only the secondary BH is active, both the variation amplitudes of continuum light curves and BELs increase with increasing BBH inclination angles and orbital eccentricities, but decrease with increasing BBH mass ratio. If both BHs are active, the asymmetry in the ionization of BLR clouds at different areas caused by the Doppler boosting effect of the secondary BH is weakened due to that of the primary BH at the opposite direction, which leads to systematically smaller variation amplitudes of both continuum light curves and BELs than the cases with only secondary BH activated. Conclusions. The coherent variations of the BEL profiles with the continuum light for those periodic QSOs provide an important way to confirm the existence of BBHs in their center. Future joint analysis of the light curves and multi-epoch observed BEL profiles for periodic QSOs may lead to the identification of a number of BBH systems.","sentences":["Context.","Periodic QSOs are considered as candidates of supermassive binary black hole (BBH) systems in galactic centers.","Further confirmation of these candidates may require different lines of observational evidences.","Aims.","Assuming the Doopler boosting scenario, in this paper we investigate the (coherent) variations of both broad emission lines (BELs) and continuum light curves for active BBH systems surrounding by a circumbinary broad line region (cBLR) and focus on their dependence on the eccentric orbital configuration.","Methods.","We calculate the variation of continuum light and the Doppler enhanced/weakened photoionization of each BLR cloud according to the motion of BBHs in elliptical orbits, and finally obtain the coherent variation of the continuum and BELs.","Results.","We find that both the amplitude and variation pattern of the continuum light curves and the evolution of the BEL profiles sensitively depend on the eccentric orbital configuration of BBH systems.","If only the secondary BH is active, both the variation amplitudes of continuum light curves and BELs increase with increasing BBH inclination angles and orbital eccentricities, but decrease with increasing BBH mass ratio.","If both BHs are active, the asymmetry in the ionization of BLR clouds at different areas caused by the Doppler boosting effect of the secondary BH is weakened due to that of the primary BH at the opposite direction, which leads to systematically smaller variation amplitudes of both continuum light curves and BELs than the cases with only secondary BH activated.","Conclusions.","The coherent variations of the BEL profiles with the continuum light for those periodic QSOs provide an important way to confirm the existence of BBHs in their center.","Future joint analysis of the light curves and multi-epoch observed BEL profiles for periodic QSOs may lead to the identification of a number of BBH systems."],"url":"http://arxiv.org/abs/2404.05975v1","category":"astro-ph.GA"}
{"created":"2024-04-09 02:57:44","title":"On the random generation of Butcher trees","abstract":"We provide a probabilistic representation of the solutions of ordinary differential equations (ODEs) by random generation of Butcher trees. This approach complements and simplifies a recent probabilistic representation of ODE solutions, by removing the need to generate random branching times. The random sampling of trees allows one to improve numerical accuracy by Monte Carlo iterations whereas the finite order truncation of Butcher series can have higher time complexity, as shown in examples.","sentences":["We provide a probabilistic representation of the solutions of ordinary differential equations (ODEs) by random generation of Butcher trees.","This approach complements and simplifies a recent probabilistic representation of ODE solutions, by removing the need to generate random branching times.","The random sampling of trees allows one to improve numerical accuracy by Monte Carlo iterations whereas the finite order truncation of Butcher series can have higher time complexity, as shown in examples."],"url":"http://arxiv.org/abs/2404.05969v1","category":"math.CA"}
{"created":"2024-04-09 02:52:55","title":"Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone?","abstract":"Software vulnerabilities (SVs) have become a common, serious, and crucial concern to safety-critical security systems. That leads to significant progress in the use of AI-based methods for software vulnerability detection (SVD). In practice, although AI-based methods have been achieving promising performances in SVD and other domain applications (e.g., computer vision), they are well-known to fail in detecting the ground-truth label of input data (referred to as out-of-distribution, OOD, data) lying far away from the training data distribution (i.e., in-distribution, ID). This drawback leads to serious issues where the models fail to indicate when they are likely mistaken. To address this problem, OOD detectors (i.e., determining whether an input is ID or OOD) have been applied before feeding the input data to the downstream AI-based modules. While OOD detection has been widely designed for computer vision and medical diagnosis applications, automated AI-based techniques for OOD source code data detection have not yet been well-studied and explored. To this end, in this paper, we propose an innovative deep learning-based approach addressing the OOD source code data identification problem. Our method is derived from an information-theoretic perspective with the use of innovative cluster-contrastive learning to effectively learn and leverage source code characteristics, enhancing data representation learning for solving the problem. The rigorous and comprehensive experiments on real-world source code datasets show the effectiveness and advancement of our approach compared to state-of-the-art baselines by a wide margin. In short, on average, our method achieves a significantly higher performance from around 15.27%, 7.39%, and 4.93% on the FPR, AUROC, and AUPR measures, respectively, in comparison with the baselines.","sentences":["Software vulnerabilities (SVs) have become a common, serious, and crucial concern to safety-critical security systems.","That leads to significant progress in the use of AI-based methods for software vulnerability detection (SVD).","In practice, although AI-based methods have been achieving promising performances in SVD and other domain applications (e.g., computer vision), they are well-known to fail in detecting the ground-truth label of input data (referred to as out-of-distribution, OOD, data) lying far away from the training data distribution (i.e., in-distribution, ID).","This drawback leads to serious issues where the models fail to indicate when they are likely mistaken.","To address this problem, OOD detectors (i.e., determining whether an input is ID or OOD) have been applied before feeding the input data to the downstream AI-based modules.","While OOD detection has been widely designed for computer vision and medical diagnosis applications, automated AI-based techniques for OOD source code data detection have not yet been well-studied and explored.","To this end, in this paper, we propose an innovative deep learning-based approach addressing the OOD source code data identification problem.","Our method is derived from an information-theoretic perspective with the use of innovative cluster-contrastive learning to effectively learn and leverage source code characteristics, enhancing data representation learning for solving the problem.","The rigorous and comprehensive experiments on real-world source code datasets show the effectiveness and advancement of our approach compared to state-of-the-art baselines by a wide margin.","In short, on average, our method achieves a significantly higher performance from around 15.27%, 7.39%, and 4.93% on the FPR, AUROC, and AUPR measures, respectively, in comparison with the baselines."],"url":"http://arxiv.org/abs/2404.05964v1","category":"cs.CR"}
{"created":"2024-04-09 02:51:09","title":"Wasserstein Dependent Graph Attention Network for Collaborative Filtering with Uncertainty","abstract":"Collaborative filtering (CF) is an essential technique in recommender systems that provides personalized recommendations by only leveraging user-item interactions. However, most CF methods represent users and items as fixed points in the latent space, lacking the ability to capture uncertainty. In this paper, we propose a novel approach, called the Wasserstein dependent Graph ATtention network (W-GAT), for collaborative filtering with uncertainty. We utilize graph attention network and Wasserstein distance to address the limitations of LightGCN and Kullback-Leibler divergence (KL) divergence to learn Gaussian embedding for each user and item. Additionally, our method incorporates Wasserstein-dependent mutual information further to increase the similarity between positive pairs and to tackle the challenges induced by KL divergence. Experimental results on three benchmark datasets show the superiority of W-GAT compared to several representative baselines. Extensive experimental analysis validates the effectiveness of W-GAT in capturing uncertainty by modeling the range of user preferences and categories associated with items.","sentences":["Collaborative filtering (CF) is an essential technique in recommender systems that provides personalized recommendations by only leveraging user-item interactions.","However, most CF methods represent users and items as fixed points in the latent space, lacking the ability to capture uncertainty.","In this paper, we propose a novel approach, called the Wasserstein dependent Graph ATtention network (W-GAT), for collaborative filtering with uncertainty.","We utilize graph attention network and Wasserstein distance to address the limitations of LightGCN and Kullback-Leibler divergence (KL) divergence to learn Gaussian embedding for each user and item.","Additionally, our method incorporates Wasserstein-dependent mutual information further to increase the similarity between positive pairs and to tackle the challenges induced by KL divergence.","Experimental results on three benchmark datasets show the superiority of W-GAT compared to several representative baselines.","Extensive experimental analysis validates the effectiveness of W-GAT in capturing uncertainty by modeling the range of user preferences and categories associated with items."],"url":"http://arxiv.org/abs/2404.05962v1","category":"cs.IR"}
{"created":"2024-04-09 02:17:19","title":"Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function","abstract":"Implementing obstacle avoidance in dynamic environments is a challenging problem for robots. Model predictive control (MPC) is a popular strategy for dealing with this type of problem, and recent work mainly uses control barrier function (CBF) as hard constraints to ensure that the system state remains in the safe set. However, in crowded scenarios, effective solutions may not be obtained due to infeasibility problems, resulting in degraded controller performance. We propose a new MPC framework that integrates CBF to tackle the issue of obstacle avoidance in dynamic environments, in which the infeasibility problem induced by hard constraints operating over the whole prediction horizon is solved by softening the constraints and introducing exact penalty, prompting the robot to actively seek out new paths. At the same time, generalized CBF is extended as a single-step safety constraint of the controller to enhance the safety of the robot during navigation. The efficacy of the proposed method is first shown through simulation experiments, in which a double-integrator system and a unicycle system are employed, and the proposed method outperforms other controllers in terms of safety, feasibility, and navigation efficiency. Furthermore, real-world experiment on an MR1000 robot is implemented to demonstrate the effectiveness of the proposed method.","sentences":["Implementing obstacle avoidance in dynamic environments is a challenging problem for robots.","Model predictive control (MPC) is a popular strategy for dealing with this type of problem, and recent work mainly uses control barrier function (CBF) as hard constraints to ensure that the system state remains in the safe set.","However, in crowded scenarios, effective solutions may not be obtained due to infeasibility problems, resulting in degraded controller performance.","We propose a new MPC framework that integrates CBF to tackle the issue of obstacle avoidance in dynamic environments, in which the infeasibility problem induced by hard constraints operating over the whole prediction horizon is solved by softening the constraints and introducing exact penalty, prompting the robot to actively seek out new paths.","At the same time, generalized CBF is extended as a single-step safety constraint of the controller to enhance the safety of the robot during navigation.","The efficacy of the proposed method is first shown through simulation experiments, in which a double-integrator system and a unicycle system are employed, and the proposed method outperforms other controllers in terms of safety, feasibility, and navigation efficiency.","Furthermore, real-world experiment on an MR1000 robot is implemented to demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.05952v1","category":"cs.RO"}
{"created":"2024-04-09 02:06:08","title":"Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms","abstract":"Graph partitioning is a key fundamental problem in the area of big graph computation. Previous works do not consider the practical requirements when optimizing the big data analysis in real applications. In this paper, motivated by optimizing the big data computing applications, two typical problems of graph partitioning are studied. The first problem is to optimize the performance of specific workloads by graph partitioning, which lacks of algorithms with performance guarantees. The second problem is to optimize the computation of motifs by graph partitioning, which has not been focused by previous works. First, the formal definitions of the above two problems are introduced, and the semidefinite programming representations are also designed based on the analysis of the properties of the two problems. For the motif based partitioning problem, it is proved to be NP-complete even for the special case of $k=2$ and the motif is a triangle, and its inapproximability is also shown by proving that there are no efficient algorithms with finite approximation ratio. Finally, using the semidefinite programming and sophisticated rounding techniques, the bi-criteria $O(\\sqrt{\\log n\\log k})$-approximation algorithms with polynomial time cost are designed and analyzed for them.","sentences":["Graph partitioning is a key fundamental problem in the area of big graph computation.","Previous works do not consider the practical requirements when optimizing the big data analysis in real applications.","In this paper, motivated by optimizing the big data computing applications, two typical problems of graph partitioning are studied.","The first problem is to optimize the performance of specific workloads by graph partitioning, which lacks of algorithms with performance guarantees.","The second problem is to optimize the computation of motifs by graph partitioning, which has not been focused by previous works.","First, the formal definitions of the above two problems are introduced, and the semidefinite programming representations are also designed based on the analysis of the properties of the two problems.","For the motif based partitioning problem, it is proved to be NP-complete even for the special case of $k=2$ and the motif is a triangle, and its inapproximability is also shown by proving that there are no efficient algorithms with finite approximation ratio.","Finally, using the semidefinite programming and sophisticated rounding techniques, the bi-criteria $O(\\sqrt{\\log n\\log k})$-approximation algorithms with polynomial time cost are designed and analyzed for them."],"url":"http://arxiv.org/abs/2404.05949v1","category":"cs.DB"}
{"created":"2024-04-09 01:55:49","title":"Differential fuzz testing to detect tampering in sensor systems and its application to arms control authentication","abstract":"In future nuclear arms control treaties, it will be necessary to authenticate the hardware and software components of verification measurement systems, i.e., to ensure these systems are functioning as intended and have not been tampered with by malicious actors. While methods such as source code hashing and static analysis can help verify the integrity of software components, they may not be capable of detecting tampering with environment variables, external libraries, or the firmware and hardware of radiation measurement systems. In this article, we introduce the concept of physical differential fuzz testing as a challenge-response-style tamper indicator that can holistically and simultaneously test all the above components in a cyber-physical system. In essence, we randomly sample (or \"fuzz\") the untampered system's parameter space, including both normal and off-normal parameter values, and consider the time series of outputs as the baseline signature of the system. Re-running the same input sequence on a untampered system will produce an output sequence consistent with this baseline, while running the same input sequence on a tampered system will produce a modified output sequence and raise an alarm. We then apply this concept to authenticating the radiation measurement equipment in nuclear weapon verification systems and conduct demonstration fuzz testing measurements with a sodium iodide (NaI) gamma ray spectrometer. Because there is Poisson noise in the measured output spectra, we also use a mechanism for comparing inherently noisy or stochastic fuzzing sequences. We show that physical differential fuzz testing can detect two types of tamper attempts, and conclude that it is a promising framework for authenticating future cyber-physical systems in nuclear arms control, safeguards, and beyond.","sentences":["In future nuclear arms control treaties, it will be necessary to authenticate the hardware and software components of verification measurement systems, i.e., to ensure these systems are functioning as intended and have not been tampered with by malicious actors.","While methods such as source code hashing and static analysis can help verify the integrity of software components, they may not be capable of detecting tampering with environment variables, external libraries, or the firmware and hardware of radiation measurement systems.","In this article, we introduce the concept of physical differential fuzz testing as a challenge-response-style tamper indicator that can holistically and simultaneously test all the above components in a cyber-physical system.","In essence, we randomly sample (or \"fuzz\") the untampered system's parameter space, including both normal and off-normal parameter values, and consider the time series of outputs as the baseline signature of the system.","Re-running the same input sequence on a untampered system will produce an output sequence consistent with this baseline, while running the same input sequence on a tampered system will produce a modified output sequence and raise an alarm.","We then apply this concept to authenticating the radiation measurement equipment in nuclear weapon verification systems and conduct demonstration fuzz testing measurements with a sodium iodide (NaI) gamma ray spectrometer.","Because there is Poisson noise in the measured output spectra, we also use a mechanism for comparing inherently noisy or stochastic fuzzing sequences.","We show that physical differential fuzz testing can detect two types of tamper attempts, and conclude that it is a promising framework for authenticating future cyber-physical systems in nuclear arms control, safeguards, and beyond."],"url":"http://arxiv.org/abs/2404.05946v1","category":"physics.ins-det"}
{"created":"2024-04-09 01:50:28","title":"Formal principle with convergence for rational curves of Goursat type","abstract":"We propose a conjecture that a general member of a bracket-generating family of rational curves in a complex manifold satisfies the formal principle with convergence, namely, any formal equivalence between such curves is convergent. If the normal bundles of the rational curves are positive, the conjecture follows from the results of Commichau-Grauert and Hirschowitz. We prove the conjecture for the opposite case when the normal bundles are furthest from positive vector bundles among bracket-generating families, namely, when the families of rational curves are of Goursat type. The proof uses natural ODEs associated to rational curves of Goursat type and corresponding Cartan connections constructed by Doubrov-Komrakov-Morimoto. As an example, we see that a general line on a smooth cubic fourfold satisfies the formal principle with convergence.","sentences":["We propose a conjecture that a general member of a bracket-generating family of rational curves in a complex manifold satisfies the formal principle with convergence, namely, any formal equivalence between such curves is convergent.","If the normal bundles of the rational curves are positive, the conjecture follows from the results of Commichau-Grauert and Hirschowitz.","We prove the conjecture for the opposite case when the normal bundles are furthest from positive vector bundles among bracket-generating families, namely, when the families of rational curves are of Goursat type.","The proof uses natural ODEs associated to rational curves of Goursat type and corresponding Cartan connections constructed by Doubrov-Komrakov-Morimoto.","As an example, we see that a general line on a smooth cubic fourfold satisfies the formal principle with convergence."],"url":"http://arxiv.org/abs/2404.05941v1","category":"math.CV"}
{"created":"2024-04-09 01:47:55","title":"Machine-learning-inspired quantum control in many-body dynamics","abstract":"Achieving precise preparation of quantum many-body states is crucial for the practical implementation of quantum computation and quantum simulation. However, the inherent challenges posed by unavoidable excitations at critical points during quench processes necessitate careful design of control fields. In this work, we introduce a promising and versatile dynamic control neural network tailored to optimize control fields. We address the problem of suppressing defect density and enhancing cat-state fidelity during the passage across the critical point in the quantum Ising model. Our method facilitates seamless transitions between different objective functions by adjusting the {optimization strategy}. In comparison to gradient-based power-law quench methods, our approach demonstrates significant advantages for both small system sizes and long-term evolutions. We provide a detailed analysis of the specific forms of control fields and summarize common features for experimental implementation. Furthermore, numerical simulations demonstrate the robustness of our proposal against random noise and spin number fluctuations. The optimized defect density and cat-state fidelity exhibit a transition at a critical ratio of the quench duration to the system size, coinciding with the quantum speed limit for quantum evolution.","sentences":["Achieving precise preparation of quantum many-body states is crucial for the practical implementation of quantum computation and quantum simulation.","However, the inherent challenges posed by unavoidable excitations at critical points during quench processes necessitate careful design of control fields.","In this work, we introduce a promising and versatile dynamic control neural network tailored to optimize control fields.","We address the problem of suppressing defect density and enhancing cat-state fidelity during the passage across the critical point in the quantum Ising model.","Our method facilitates seamless transitions between different objective functions by adjusting the {optimization strategy}.","In comparison to gradient-based power-law quench methods, our approach demonstrates significant advantages for both small system sizes and long-term evolutions.","We provide a detailed analysis of the specific forms of control fields and summarize common features for experimental implementation.","Furthermore, numerical simulations demonstrate the robustness of our proposal against random noise and spin number fluctuations.","The optimized defect density and cat-state fidelity exhibit a transition at a critical ratio of the quench duration to the system size, coinciding with the quantum speed limit for quantum evolution."],"url":"http://arxiv.org/abs/2404.05940v1","category":"quant-ph"}
{"created":"2024-04-09 01:45:10","title":"Real-Valued 2-D Direction of Arrival Estimation via Sparse Representation","abstract":"Despite many advantages of direction-of-arrivals (DOAs) in sparse representation domain, they have high computational complexity. This paper presents a new method for real-valued 2-D DOAs estimation of sources in a uniform circular array configuration. This method uses a transformation based on phase mode excitation in uniform circular arrays which called real beamspace L1-SVD (RB-L1SVD). This unitary transformation converts complex manifold matrix to real one, so that the computational complexity is decreased with respect to complex valued computations,its computation, at least, is one,fourth of the complex-valued case; moreover, some benefits from using this transformation are robustness to array imperfections, a better noise suppression because of exploiting an additional real structure, and etc. Numerical results demonstrate the better performance of the proposed approach over previous techniques such as C-L1SVD, RB-ESPRIT, and RB-MUSIC, especially in low signal-to-noise ratios.","sentences":["Despite many advantages of direction-of-arrivals (DOAs) in sparse representation domain, they have high computational complexity.","This paper presents a new method for real-valued 2-D DOAs estimation of sources in a uniform circular array configuration.","This method uses a transformation based on phase mode excitation in uniform circular arrays which called real beamspace L1-SVD (RB-L1SVD).","This unitary transformation converts complex manifold matrix to real one, so that the computational complexity is decreased with respect to complex valued computations,its computation, at least, is one,fourth of the complex-valued case; moreover, some benefits from using this transformation are robustness to array imperfections, a better noise suppression because of exploiting an additional real structure, and etc.","Numerical results demonstrate the better performance of the proposed approach over previous techniques such as C-L1SVD, RB-ESPRIT, and RB-MUSIC, especially in low signal-to-noise ratios."],"url":"http://arxiv.org/abs/2404.05939v1","category":"eess.SP"}
{"created":"2024-04-09 01:42:54","title":"De-aberration for transcranial photoacoustic computed tomography through an adult human skull","abstract":"Noninvasive transcranial photoacoustic computed tomography (PACT) of the human brain, despite its clinical potential, remains impeded by the acoustic distortion induced by the human skull. The distortion, which is attributed to the markedly different material properties of the skull relative to soft tissue, results in heavily aberrated PACT images -- a problem that has remained unsolved in the past two decades. Herein, we report the first successful experimental demonstration of the de-aberration of PACT images through an ex-vivo adult human skull using a homogeneous elastic model for the skull. Using only the geometry, position, and orientation of the skull, we accurately de-aberrate the PACT images of light-absorbing phantoms acquired through an ex-vivo human skull, in terms of the recovered phantom features, for different levels of phantom complexity and positions. Our work addresses the longstanding challenge of skull-induced aberrations in transcranial PACT and advances the field towards unlocking the full potential of transcranial human brain PACT.","sentences":["Noninvasive transcranial photoacoustic computed tomography (PACT) of the human brain, despite its clinical potential, remains impeded by the acoustic distortion induced by the human skull.","The distortion, which is attributed to the markedly different material properties of the skull relative to soft tissue, results in heavily aberrated PACT images -- a problem that has remained unsolved in the past two decades.","Herein, we report the first successful experimental demonstration of the de-aberration of PACT images through an ex-vivo adult human skull using a homogeneous elastic model for the skull.","Using only the geometry, position, and orientation of the skull, we accurately de-aberrate the PACT images of light-absorbing phantoms acquired through an ex-vivo human skull, in terms of the recovered phantom features, for different levels of phantom complexity and positions.","Our work addresses the longstanding challenge of skull-induced aberrations in transcranial PACT and advances the field towards unlocking the full potential of transcranial human brain PACT."],"url":"http://arxiv.org/abs/2404.05937v1","category":"physics.med-ph"}
{"created":"2024-04-09 01:38:32","title":"Learning Symmetric Hamiltonian","abstract":"Hamiltonian Learning is a process of recovering system Hamiltonian from measurements, which is a fundamental problem in quantum information processing. In this study, we investigate the problem of learning the symmetric Hamiltonian from its eigenstate. Inspired by the application of group theory in block diagonal secular determination, we have derived a method to determine the number of linearly independent equations about the Hamiltonian unknowns obtained from an eigenstate. This number corresponds to the degeneracy of the associated irreducible representation of the Hamiltonian symmetry group. To illustrate our approach, we examine the XXX Hamiltonian and the XXZ Hamiltonian. We first determine the Hamiltonian symmetry group, then work out the decomposition of irreducible representation, which serves as foundation for analyzing the uniqueness of recovered Hamiltonian. Our numerical findings consistently align with our theoretical analysis.","sentences":["Hamiltonian Learning is a process of recovering system Hamiltonian from measurements, which is a fundamental problem in quantum information processing.","In this study, we investigate the problem of learning the symmetric Hamiltonian from its eigenstate.","Inspired by the application of group theory in block diagonal secular determination, we have derived a method to determine the number of linearly independent equations about the Hamiltonian unknowns obtained from an eigenstate.","This number corresponds to the degeneracy of the associated irreducible representation of the Hamiltonian symmetry group.","To illustrate our approach, we examine the XXX Hamiltonian and the XXZ Hamiltonian.","We first determine the Hamiltonian symmetry group, then work out the decomposition of irreducible representation, which serves as foundation for analyzing the uniqueness of recovered Hamiltonian.","Our numerical findings consistently align with our theoretical analysis."],"url":"http://arxiv.org/abs/2404.05936v1","category":"quant-ph"}
{"created":"2024-04-09 01:38:05","title":"Dressed Majorana fermion in a hybrid nanowire","abstract":"The low-energy theory of hybrid nanowire systems fails to define Majorana fermion (MF) in the strong tunneling and magnetic field strength. To address this limitation, we propose a holistic approach to define MF in which the quasi-excitation in nanowire and superconductor constitutes together its own ``antiparticles''. This definition is general, beyond the constraint presented in the low-energy theory. It reveals that the Majorana phase depends not only on the chemical potential and Zeeman energy in nanowire but also on those of superconductor, and that the mismatch of chemical potential leads not to observe MF. Such a broader perspective provides more specific experimental guidance under various conditions","sentences":["The low-energy theory of hybrid nanowire systems fails to define Majorana fermion (MF) in the strong tunneling and magnetic field strength.","To address this limitation, we propose a holistic approach to define MF in which the quasi-excitation in nanowire and superconductor constitutes together its own ``antiparticles''.","This definition is general, beyond the constraint presented in the low-energy theory.","It reveals that the Majorana phase depends not only on the chemical potential and Zeeman energy in nanowire but also on those of superconductor, and that the mismatch of chemical potential leads not to observe MF.","Such a broader perspective provides more specific experimental guidance under various conditions"],"url":"http://arxiv.org/abs/2404.05935v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 01:35:13","title":"Verification of Recursively Defined Quantum Circuits","abstract":"Recursive techniques have recently been introduced into quantum programming so that a variety of large quantum circuits and algorithms can be elegantly and economically programmed. In this paper, we present a proof system for formal verification of the correctness of recursively defined quantum circuits. The soundness and (relative) completeness of the proof system are established. To demonstrating its effectiveness, a series of application examples of the proof system are given, including (multi-qubit) controlled gates, a quantum circuit generating (multi-qubit) GHZ (Greenberger-Horne-Zeilinger) states, recursive definition of quantum Fourier transform, quantum state preparation, and quantum random-access memories (QRAM).","sentences":["Recursive techniques have recently been introduced into quantum programming so that a variety of large quantum circuits and algorithms can be elegantly and economically programmed.","In this paper, we present a proof system for formal verification of the correctness of recursively defined quantum circuits.","The soundness and (relative) completeness of the proof system are established.","To demonstrating its effectiveness, a series of application examples of the proof system are given, including (multi-qubit) controlled gates, a quantum circuit generating (multi-qubit) GHZ (Greenberger-Horne-Zeilinger) states, recursive definition of quantum Fourier transform, quantum state preparation, and quantum random-access memories (QRAM)."],"url":"http://arxiv.org/abs/2404.05934v1","category":"quant-ph"}
{"created":"2024-04-09 01:17:04","title":"A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series","abstract":"Quantifying relationships between components of a complex system is critical to understanding the rich network of interactions that characterize the behavior of the system. Traditional methods for detecting pairwise dependence of time series, such as Pearson correlation, Granger causality, and mutual information, are computed directly in the space of measured time-series values. But for systems in which interactions are mediated by statistical properties of the time series (`time-series features') over longer timescales, this approach can fail to capture the underlying dependence from limited and noisy time-series data, and can be challenging to interpret. Addressing these issues, here we introduce an information-theoretic method for detecting dependence between time series mediated by time-series features that provides interpretable insights into the nature of the interactions. Our method extracts a candidate set of time-series features from sliding windows of the source time series and assesses their role in mediating a relationship to values of the target process. Across simulations of three different generative processes, we demonstrate that our feature-based approach can outperform a traditional inference approach based on raw time-series values, especially in challenging scenarios characterized by short time-series lengths, high noise levels, and long interaction timescales. Our work introduces a new tool for inferring and interpreting feature-mediated interactions from time-series data, contributing to the broader landscape of quantitative analysis in complex systems research, with potential applications in various domains including but not limited to neuroscience, finance, climate science, and engineering.","sentences":["Quantifying relationships between components of a complex system is critical to understanding the rich network of interactions that characterize the behavior of the system.","Traditional methods for detecting pairwise dependence of time series, such as Pearson correlation, Granger causality, and mutual information, are computed directly in the space of measured time-series values.","But for systems in which interactions are mediated by statistical properties of the time series (`time-series features') over longer timescales, this approach can fail to capture the underlying dependence from limited and noisy time-series data, and can be challenging to interpret.","Addressing these issues, here we introduce an information-theoretic method for detecting dependence between time series mediated by time-series features that provides interpretable insights into the nature of the interactions.","Our method extracts a candidate set of time-series features from sliding windows of the source time series and assesses their role in mediating a relationship to values of the target process.","Across simulations of three different generative processes, we demonstrate that our feature-based approach can outperform a traditional inference approach based on raw time-series values, especially in challenging scenarios characterized by short time-series lengths, high noise levels, and long interaction timescales.","Our work introduces a new tool for inferring and interpreting feature-mediated interactions from time-series data, contributing to the broader landscape of quantitative analysis in complex systems research, with potential applications in various domains including but not limited to neuroscience, finance, climate science, and engineering."],"url":"http://arxiv.org/abs/2404.05929v1","category":"physics.data-an"}
{"created":"2024-04-09 00:58:09","title":"Experimental Demonstration of Controllable PT and anti-PT Coupling in a non-Hermitian Metamaterial","abstract":"Non-Hermiticity has recently emerged as a rapidly developing field due to its exotic characteristics related to open systems, where the dissipation plays a critical role. In the presence of balanced energy gain and loss with environment, the system exhibits parity-time (PT) symmetry, meanwhile as the conjugate counterpart, anti-PT symmetry can be achieved with dissipative coupling within the system. Here, we demonstrate the coherence of complex dissipative coupling can control the transition between PT and anti-PT symmetry in an electromagnetic metamaterial. Notably, the achievement of the anti-PT symmetric phase is independent of variations in dissipation. Furthermore, we observe phase transitions as the system crosses exceptional points in both anti-PT and PT symmetric metamaterial configurations, achieved by manipulating the frequency and dissipation of resonators. This work provides a promising metamaterial design for broader exploration of non-Hermitian physics and practical application with controllable Hamiltonian.","sentences":["Non-Hermiticity has recently emerged as a rapidly developing field due to its exotic characteristics related to open systems, where the dissipation plays a critical role.","In the presence of balanced energy gain and loss with environment, the system exhibits parity-time (PT) symmetry, meanwhile as the conjugate counterpart, anti-PT symmetry can be achieved with dissipative coupling within the system.","Here, we demonstrate the coherence of complex dissipative coupling can control the transition between PT and anti-PT symmetry in an electromagnetic metamaterial.","Notably, the achievement of the anti-PT symmetric phase is independent of variations in dissipation.","Furthermore, we observe phase transitions as the system crosses exceptional points in both anti-PT and PT symmetric metamaterial configurations, achieved by manipulating the frequency and dissipation of resonators.","This work provides a promising metamaterial design for broader exploration of non-Hermitian physics and practical application with controllable Hamiltonian."],"url":"http://arxiv.org/abs/2404.05922v1","category":"physics.optics"}
{"created":"2024-04-09 00:54:11","title":"Quantum Generative Adversarial Networks in a Silicon Photonic Chip with Maximum Expressibility","abstract":"Generative adversarial networks (GANs) have achieved remarkable success with realistic tasks such as creating realistic images, texts, and audio. Combining GANs and quantum computing, quantum GANs are thought to have an exponential advantage over their classical counterparts due to the stronger expressibility of quantum circuits. In this research, a two-qubit silicon quantum photonic chip is created, capable of executing arbitrary controlled-unitary (CU) operations and generating any 2-qubit pure state, thus making it an excellent platform for quantum GANs. To capture complex data patterns, a hybrid generator is proposed to inject nonlinearity into quantum GANs. As a demonstration, three generative tasks, covering both pure quantum versions of GANs (PQ-GAN) and hybrid quantum-classical GANs (HQC-GANs), are successfully carried out on the chip, including high-fidelity single-qubit state learning, classical distributions loading, and compressed image production. The experiment results prove that silicon quantum photonic chips have great potential in generative learning applications.","sentences":["Generative adversarial networks (GANs) have achieved remarkable success with realistic tasks such as creating realistic images, texts, and audio.","Combining GANs and quantum computing, quantum GANs are thought to have an exponential advantage over their classical counterparts due to the stronger expressibility of quantum circuits.","In this research, a two-qubit silicon quantum photonic chip is created, capable of executing arbitrary controlled-unitary (CU) operations and generating any 2-qubit pure state, thus making it an excellent platform for quantum GANs.","To capture complex data patterns, a hybrid generator is proposed to inject nonlinearity into quantum GANs.","As a demonstration, three generative tasks, covering both pure quantum versions of GANs (PQ-GAN) and hybrid quantum-classical GANs (HQC-GANs), are successfully carried out on the chip, including high-fidelity single-qubit state learning, classical distributions loading, and compressed image production.","The experiment results prove that silicon quantum photonic chips have great potential in generative learning applications."],"url":"http://arxiv.org/abs/2404.05921v1","category":"quant-ph"}
{"created":"2024-04-09 00:33:52","title":"Excited-State Dynamics and Optically Detected Magnetic Resonance of Solid-State Spin Defects from First Principles","abstract":"Optically detected magnetic resonance (ODMR) is an efficient and reliable method that enables initialization and readout of spin states through spin-photon interface. In general, high quantum efficiency and large spin-dependent photoluminescence (PL) contrast are desirable for reliable quantum information readout. However, reliable prediction of the ODMR contrast from first-principles requires accurate description of complex spin polarization mechanisms of spin defects. These mechanisms often include multiple radiative and nonradiative processes in particular intersystem crossing (ISC) among multiple excited electronic states. In this work we present our implementation of the first-principles ODMR contrast, by solving kinetic master equation with calculated rates from ab initio electronic structure methods then benchmark the implementation on the case of the negatively-charged nitrogen vacancy (NV) center in diamond. We show the importance of correct description of multi-reference electronic states for accurate prediction of excitation energy, spin-orbit coupling, and the rate of ISC. Moreover, we underscore the importance of pseudo Jahn-Teller effect for the spin-orbit coupling, and the dynamical Jahn-Teller effect for electron-phonon coupling, key factors determining ISC rates and ODMR contrast. We show good agreement between our first-principles calculations and the experimental ODMR contrast under magnetic field. We then demonstrate reliable predictions of magnetic field direction, pump power, and microwave frequency dependency, as important parameters for ODMR experiments. Our work clarifies the important excited-state relaxation mechanisms determining ODMR contrast and provides a predictive computational platform for spin polarization and optical readout of solid-state quantum defects from first principles.","sentences":["Optically detected magnetic resonance (ODMR) is an efficient and reliable method that enables initialization and readout of spin states through spin-photon interface.","In general, high quantum efficiency and large spin-dependent photoluminescence (PL) contrast are desirable for reliable quantum information readout.","However, reliable prediction of the ODMR contrast from first-principles requires accurate description of complex spin polarization mechanisms of spin defects.","These mechanisms often include multiple radiative and nonradiative processes in particular intersystem crossing (ISC) among multiple excited electronic states.","In this work we present our implementation of the first-principles ODMR contrast, by solving kinetic master equation with calculated rates from ab initio electronic structure methods then benchmark the implementation on the case of the negatively-charged nitrogen vacancy (NV) center in diamond.","We show the importance of correct description of multi-reference electronic states for accurate prediction of excitation energy, spin-orbit coupling, and the rate of ISC.","Moreover, we underscore the importance of pseudo Jahn-Teller effect for the spin-orbit coupling, and the dynamical Jahn-Teller effect for electron-phonon coupling, key factors determining ISC rates and ODMR contrast.","We show good agreement between our first-principles calculations and the experimental ODMR contrast under magnetic field.","We then demonstrate reliable predictions of magnetic field direction, pump power, and microwave frequency dependency, as important parameters for ODMR experiments.","Our work clarifies the important excited-state relaxation mechanisms determining ODMR contrast and provides a predictive computational platform for spin polarization and optical readout of solid-state quantum defects from first principles."],"url":"http://arxiv.org/abs/2404.05917v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 00:05:45","title":"LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation","abstract":"Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI) scans is crucial for prompt and effective treatment. However, this process faces the challenge of precise delineation due to the tumors' complex heterogeneity. Moreover, energy sustainability targets and resource limitations, especially in developing countries, require efficient and accessible medical imaging solutions. The proposed architecture, a Lightweight 3D ATtention U-Net with Parallel convolutions, LATUP-Net, addresses these issues. It is specifically designed to reduce computational requirements significantly while maintaining high segmentation performance. By incorporating parallel convolutions, it enhances feature representation by capturing multi-scale information. It further integrates an attention mechanism to refine segmentation through selective feature recalibration. LATUP-Net achieves promising segmentation performance: the average Dice scores for the whole tumor, tumor core, and enhancing tumor on the BraTS2020 dataset are 88.41%, 83.82%, and 73.67%, and on the BraTS2021 dataset, they are 90.29%, 89.54%, and 83.92%, respectively. Hausdorff distance metrics further indicate its improved ability to delineate tumor boundaries. With its significantly reduced computational demand using only 3.07 M parameters, about 59 times fewer than other state-of-the-art models, and running on a single V100 GPU, LATUP-Net stands out as a promising solution for real-world clinical applications, particularly in settings with limited resources. Investigations into the model's interpretability, utilizing gradient-weighted class activation mapping and confusion matrices, reveal that while attention mechanisms enhance the segmentation of small regions, their impact is nuanced. Achieving the most accurate tumor delineation requires carefully balancing local and global features.","sentences":["Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI) scans is crucial for prompt and effective treatment.","However, this process faces the challenge of precise delineation due to the tumors' complex heterogeneity.","Moreover, energy sustainability targets and resource limitations, especially in developing countries, require efficient and accessible medical imaging solutions.","The proposed architecture, a Lightweight 3D ATtention U-Net with Parallel convolutions, LATUP-Net, addresses these issues.","It is specifically designed to reduce computational requirements significantly while maintaining high segmentation performance.","By incorporating parallel convolutions, it enhances feature representation by capturing multi-scale information.","It further integrates an attention mechanism to refine segmentation through selective feature recalibration.","LATUP-Net achieves promising segmentation performance: the average Dice scores for the whole tumor, tumor core, and enhancing tumor on the BraTS2020 dataset are 88.41%, 83.82%, and 73.67%, and on the BraTS2021 dataset, they are 90.29%, 89.54%, and 83.92%, respectively.","Hausdorff distance metrics further indicate its improved ability to delineate tumor boundaries.","With its significantly reduced computational demand using only 3.07 M parameters, about 59 times fewer than other state-of-the-art models, and running on a single V100 GPU, LATUP-Net stands out as a promising solution for real-world clinical applications, particularly in settings with limited resources.","Investigations into the model's interpretability, utilizing gradient-weighted class activation mapping and confusion matrices, reveal that while attention mechanisms enhance the segmentation of small regions, their impact is nuanced.","Achieving the most accurate tumor delineation requires carefully balancing local and global features."],"url":"http://arxiv.org/abs/2404.05911v1","category":"eess.IV"}
{"created":"2024-04-08 23:44:10","title":"A robust test of general relativity at the galactic scales by combining strong lensing systems and gravitational wave standard sirens","abstract":"The measurement of the parametrized post-Newtonian parameter $\\gamma_{\\rm{PPN}}$ is a robust test of general relativity (GR). In some modified theories of gravity, $\\gamma_{\\rm{PPN}}$ may evolve with the redshift and deviate from one at high redshifts. This means that precise constraints on $\\gamma_{\\rm{PPN}}$ acquired in the solar system experiments could not be sufficient to test such theories and it is necessary to constrain $\\gamma_{\\rm{PPN}}$with high precision at high redshifts. However, in many approaches aimed at extragalactic tests of GR, the results might be biased due to entanglement of various factors, such as cosmic curvature, cosmic opacity, and the Hubble constant. Strong lensing systems naturally provide a laboratory to test $\\gamma_{\\rm{PPN}}$ at galactic scales and high redshifts, but there is degeneracy between measured strength of gravity and cosmic distances in the lensing system. Gravitational waves (GWs) from binary neutron star mergers (standard sirens) provide a direct way to break this degeneracy by providing self-calibrated measurements of the luminosity distance. {We investigate the possibility of estimating $\\gamma_{\\rm{PPN}}$ by combining well measured strongly lensed systems with GW signals from coalescing neutron stars. Such combination provides a cosmological-model independent, relatively pure and unbiased method for the inference of $\\gamma_{\\rm{PPN}}$ parameter, avoiding the influence of the above factors and the mass-sheet degeneracy in the lens.} Based on the simulated future 55 lensed quasar systems we demonstrated that the precision of $\\gamma_{\\rm{PPN}}$ parameter obtained by our method could be of order of $\\sim 10^{-2}$. One may reasonably expect that our approach will play an increasingly important role in precise testing the validity of general relativity at galactic scales and high redshifts.","sentences":["The measurement of the parametrized post-Newtonian parameter $\\gamma_{\\rm{PPN}}$ is a robust test of general relativity (GR).","In some modified theories of gravity, $\\gamma_{\\rm{PPN}}$ may evolve with the redshift and deviate from one at high redshifts.","This means that precise constraints on $\\gamma_{\\rm{PPN}}$ acquired in the solar system experiments could not be sufficient to test such theories and it is necessary to constrain $","\\gamma_{\\rm{PPN}}$with high precision at high redshifts.","However, in many approaches aimed at extragalactic tests of GR, the results might be biased due to entanglement of various factors, such as cosmic curvature, cosmic opacity, and the Hubble constant.","Strong lensing systems naturally provide a laboratory to test $\\gamma_{\\rm{PPN}}$ at galactic scales and high redshifts, but there is degeneracy between measured strength of gravity and cosmic distances in the lensing system.","Gravitational waves (GWs) from binary neutron star mergers (standard sirens) provide a direct way to break this degeneracy by providing self-calibrated measurements of the luminosity distance.","{We investigate the possibility of estimating $\\gamma_{\\rm{PPN}}$ by combining well measured strongly lensed systems with GW signals from coalescing neutron stars.","Such combination provides a cosmological-model independent, relatively pure and unbiased method for the inference of $\\gamma_{\\rm{PPN}}$ parameter, avoiding the influence of the above factors and the mass-sheet degeneracy in the lens.}","Based on the simulated future 55 lensed quasar systems we demonstrated that the precision of $\\gamma_{\\rm{PPN}}$ parameter obtained by our method could be of order of $\\sim 10^{-2}$. One may reasonably expect that our approach will play an increasingly important role in precise testing the validity of general relativity at galactic scales and high redshifts."],"url":"http://arxiv.org/abs/2404.05907v1","category":"gr-qc"}
{"created":"2024-04-08 23:30:15","title":"Computing Transition Pathways for the Study of Rare Events Using Deep Reinforcement Learning","abstract":"Understanding the transition events between metastable states in complex systems is an important subject in the fields of computational physics, chemistry and biology. The transition pathway plays an important role in characterizing the mechanism underlying the transition, for example, in the study of conformational changes of bio-molecules. In fact, computing the transition pathway is a challenging task for complex and high-dimensional systems. In this work, we formulate the path-finding task as a cost minimization problem over a particular path space. The cost function is adapted from the Freidlin-Wentzell action functional so that it is able to deal with rough potential landscapes. The path-finding problem is then solved using a actor-critic method based on the deep deterministic policy gradient algorithm (DDPG). The method incorporates the potential force of the system in the policy for generating episodes and combines physical properties of the system with the learning process for molecular systems. The exploitation and exploration nature of reinforcement learning enables the method to efficiently sample the transition events and compute the globally optimal transition pathway. We illustrate the effectiveness of the proposed method using three benchmark systems including an extended Mueller system and the Lennard-Jones system of seven particles.","sentences":["Understanding the transition events between metastable states in complex systems is an important subject in the fields of computational physics, chemistry and biology.","The transition pathway plays an important role in characterizing the mechanism underlying the transition, for example, in the study of conformational changes of bio-molecules.","In fact, computing the transition pathway is a challenging task for complex and high-dimensional systems.","In this work, we formulate the path-finding task as a cost minimization problem over a particular path space.","The cost function is adapted from the Freidlin-Wentzell action functional so that it is able to deal with rough potential landscapes.","The path-finding problem is then solved using a actor-critic method based on the deep deterministic policy gradient algorithm (DDPG).","The method incorporates the potential force of the system in the policy for generating episodes and combines physical properties of the system with the learning process for molecular systems.","The exploitation and exploration nature of reinforcement learning enables the method to efficiently sample the transition events and compute the globally optimal transition pathway.","We illustrate the effectiveness of the proposed method using three benchmark systems including an extended Mueller system and the Lennard-Jones system of seven particles."],"url":"http://arxiv.org/abs/2404.05905v1","category":"physics.comp-ph"}
{"created":"2024-04-08 23:00:01","title":"Theories of Frege structure equivalent to Feferman's system $\\mathsf{T}_0$","abstract":"Feferman (1975) defines an impredicative system $\\mathsf{T}_0$ of explicit mathematics, which is proof-theoretically equivalent to the subsystem $\\Delta^1_2$-$\\mathsf{CA} + \\mathsf{BI}$ of second-order arithmetic. In this paper, we propose several systems of Frege structure with the same proof-theoretic strength as $\\mathsf{T}_0$. To be precise, we first consider the Kripke--Feferman theory, which is one of the most famous truth theories, and we extend it by two kinds of induction principles inspired by (J\\\"ager et al. 2001). In addition, we give similar results for the system based on Aczel's original Frege structure (Aczel 1980). Finally, we equip Cantini's supervaluation-style theory with the notion of universes, the strength of which was an open problem in (Kahle 2001).","sentences":["Feferman (1975) defines an impredicative system $\\mathsf{T}_0$ of explicit mathematics, which is proof-theoretically equivalent to the subsystem $\\Delta^1_2$-$\\mathsf{CA} + \\mathsf{BI}$ of second-order arithmetic.","In this paper, we propose several systems of Frege structure with the same proof-theoretic strength as $\\mathsf{T}_0$. To be precise, we first consider the Kripke--Feferman theory, which is one of the most famous truth theories, and we extend it by two kinds of induction principles inspired by (J\\\"ager et al. 2001).","In addition, we give similar results for the system based on Aczel's original Frege structure (Aczel 1980).","Finally, we equip Cantini's supervaluation-style theory with the notion of universes, the strength of which was an open problem in (Kahle 2001)."],"url":"http://arxiv.org/abs/2404.05899v1","category":"math.LO"}
{"created":"2024-04-09 17:59:55","title":"Disentangling transitions in topological order induced by boundary decoherence","abstract":"We study the entanglement structure of topological orders subject to decoherence on the bipartition boundary. Focusing on the toric codes in $d$ space dimensions for $d=2,3,4$, we explore whether the boundary decoherence may be able to induce a disentangling transition, characterized by the destruction of mixed-state long-range entanglement across the bipartition, measured by topological entanglement negativity. A key insight of our approach is the connection between the negativity spectrum of the decohered mixed states and emergent symmetry-protected topological orders under certain symmetry-preserving perturbation localized on the bipartition boundary. This insight allows us to analytically derive the exact results of entanglement negativity without using a replica trick.","sentences":["We study the entanglement structure of topological orders subject to decoherence on the bipartition boundary.","Focusing on the toric codes in $d$ space dimensions for $d=2,3,4$, we explore whether the boundary decoherence may be able to induce a disentangling transition, characterized by the destruction of mixed-state long-range entanglement across the bipartition, measured by topological entanglement negativity.","A key insight of our approach is the connection between the negativity spectrum of the decohered mixed states and emergent symmetry-protected topological orders under certain symmetry-preserving perturbation localized on the bipartition boundary.","This insight allows us to analytically derive the exact results of entanglement negativity without using a replica trick."],"url":"http://arxiv.org/abs/2404.06514v1","category":"quant-ph"}
{"created":"2024-04-09 17:57:29","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling","abstract":"Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.","sentences":["Tokenisation is a core part of language models (LMs).","It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM.","While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now.","We refer to such subwords as near duplicates.","In this paper, we study the impact of near duplicate subwords on LM training efficiency.","First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates.","We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords.","Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting.","Second, we investigate the impact of naturally occurring near duplicates on LMs.","Here, we see that merging them considerably hurts LM performance.","Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements."],"url":"http://arxiv.org/abs/2404.06508v1","category":"cs.CL"}
{"created":"2024-04-09 17:52:51","title":"Detection of Rydberg lines from the atmosphere of Betelgeuse","abstract":"Emission lines from Rydberg transitions are detected for the first time from a region close to the surface of Betelgeuse. The H30${\\alpha}$ line is observed at 231.905 GHz, with a FWHM ~42 km/s and extended wings. A second line at 232.025 GHz (FWHM ~21 km/s), is modeled as a combination of Rydberg transitions of abundant low First Ionization Potential metals. Both H30${\\alpha}$ and the Rydberg combined line X30${\\alpha}$ are fitted by Voigt profiles, and collisional broadening with electrons may be partly responsible for the Lorentzian contribution, indicating electron densities of a few 10$^8$cm$^{-3}$. X30${\\alpha}$ is located in a relatively smooth ring at a projected radius of 0.9x the optical photospheric radius R$_*$, whereas H30${\\alpha}$ is more clumpy, reaching a peak at ~1.4R$_*$. We use a semi-empirical thermodynamic atmospheric model of Betelgeuse to compute the 232 GHz (1.29mm) continuum and line profiles making simple assumptions. Photoionized abundant metals dominate the electron density and the predicted surface of continuum optical depth unity at 232 GHz occurs at ~1.3R$_*$, in good agreement with observations. Assuming a Saha-Boltzmann distribution for the level populations of Mg, Si, and Fe, the model predicts that the X30${\\alpha}$ emission arises in a region of radially-increasing temperature and turbulence. Inclusion of ionized C and non-LTE effects could modify the integrated fluxes and location of emission. These simulations confirm the identity of the Rydberg transition lines observed towards Betelgeuse, and reveal that such diagnostics can improve future atmospheric models.","sentences":["Emission lines from Rydberg transitions are detected for the first time from a region close to the surface of Betelgeuse.","The H30${\\alpha}$ line is observed at 231.905 GHz, with a FWHM ~42 km/s and extended wings.","A second line at 232.025 GHz (FWHM ~21 km/s), is modeled as a combination of Rydberg transitions of abundant low First Ionization Potential metals.","Both H30${\\alpha}$ and the Rydberg combined line X30${\\alpha}$ are fitted by Voigt profiles, and collisional broadening with electrons may be partly responsible for the Lorentzian contribution, indicating electron densities of a few 10$^8$cm$^{-3}$. X30${\\alpha}$ is located in a relatively smooth ring at a projected radius of 0.9x the optical photospheric radius R$_*$, whereas H30${\\alpha}$ is more clumpy, reaching a peak at ~1.4R$_*$.","We use a semi-empirical thermodynamic atmospheric model of Betelgeuse to compute the 232 GHz (1.29mm) continuum and line profiles making simple assumptions.","Photoionized abundant metals dominate the electron density and the predicted surface of continuum optical depth unity at 232 GHz occurs at ~1.3R$_*$, in good agreement with observations.","Assuming a Saha-Boltzmann distribution for the level populations of Mg, Si, and Fe, the model predicts that the X30${\\alpha}$ emission arises in a region of radially-increasing temperature and turbulence.","Inclusion of ionized C and non-LTE effects could modify the integrated fluxes and location of emission.","These simulations confirm the identity of the Rydberg transition lines observed towards Betelgeuse, and reveal that such diagnostics can improve future atmospheric models."],"url":"http://arxiv.org/abs/2404.06501v1","category":"astro-ph.SR"}
{"created":"2024-04-09 17:50:38","title":"Simultaneous linear connectivity of neural networks modulo permutation","abstract":"Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.","sentences":["Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier.","Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately.","In this work, we refine these arguments into three distinct claims of increasing strength.","We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks.","In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable.","This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss.","In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences.","Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively.","Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks."],"url":"http://arxiv.org/abs/2404.06498v1","category":"cs.LG"}
{"created":"2024-04-09 17:21:41","title":"Regression Discontinuity Design with Spillovers","abstract":"Researchers who estimate treatment effects using a regression discontinuity design (RDD) typically assume that there are no spillovers between the treated and control units. This may be unrealistic. We characterize the estimand of RDD in a setting where spillovers occur between units that are close in their values of the running variable. Under the assumption that spillovers are linear-in-means, we show that the estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression. Specifically, RDD estimates direct treatment effect when radius is of larger order than the bandwidth, and total treatment effect when radius is of smaller order than the bandwidth. In the more realistic regime where radius is of similar order as the bandwidth, the RDD estimand is a mix of the above effects. To recover direct and spillover effects, we propose incorporating estimated spillover terms into local linear regression -- the local analog of peer effects regression. We also clarify the settings under which the donut-hole RD is able to eliminate the effects of spillovers.","sentences":["Researchers who estimate treatment effects using a regression discontinuity design (RDD) typically assume that there are no spillovers between the treated and control units.","This may be unrealistic.","We characterize the estimand of RDD in a setting where spillovers occur between units that are close in their values of the running variable.","Under the assumption that spillovers are linear-in-means, we show that the estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression.","Specifically, RDD estimates direct treatment effect when radius is of larger order than the bandwidth, and total treatment effect when radius is of smaller order than the bandwidth.","In the more realistic regime where radius is of similar order as the bandwidth, the RDD estimand is a mix of the above effects.","To recover direct and spillover effects, we propose incorporating estimated spillover terms into local linear regression -- the local analog of peer effects regression.","We also clarify the settings under which the donut-hole RD is able to eliminate the effects of spillovers."],"url":"http://arxiv.org/abs/2404.06471v1","category":"econ.EM"}
{"created":"2024-04-09 16:49:42","title":"The Central Spanning Tree Problem","abstract":"Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing. Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a. the minimum routing cost tree. When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees. Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees. In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases. On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton. We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants.","sentences":["Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing.","Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a.","the minimum routing cost tree.","When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees.","Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees.","In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases.","On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton.","We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants."],"url":"http://arxiv.org/abs/2404.06447v1","category":"cs.DM"}
{"created":"2024-04-09 16:15:03","title":"ZeST: Zero-Shot Material Transfer from a Single Image","abstract":"We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image. ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest","sentences":["We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image.","ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image.","This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues.","The method works on real images without any training resulting a zero-shot approach.","Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials.","We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations.","Project Page: https://ttchengab.github.io/zest"],"url":"http://arxiv.org/abs/2404.06425v1","category":"cs.CV"}
{"created":"2024-04-09 16:10:39","title":"Bayesian Survival Analysis by Approximate Inference of Neural Networks","abstract":"Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions. In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated. In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance. For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty. This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times. For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error. Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives.","sentences":["Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions.","In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated.","In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance.","For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty.","This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times.","For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error.","Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives."],"url":"http://arxiv.org/abs/2404.06421v1","category":"cs.LG"}
{"created":"2024-04-09 16:04:12","title":"Quasi-Particle Self-Consistent $GW$ for Molecules","abstract":"We present the formalism and implementation of quasi-particle self-consistent GW (qsGW) and eigenvalue only quasi-particle self-consistent GW (evGW) adapted to standard quantum chemistry packages. Our implementation is benchmarked against high-level quantum chemistry computations (coupled-cluster theory) and experimental results using a representative set of molecules. Furthermore, we compare the qsGW approach for five molecules relevant for organic photovoltaics to self-consistent GW results (scGW) and analyze the effects of the self-consistency on the ground state density by comparing calculated dipole moments to their experimental values. We show that qsGW makes a significant improvement over conventional G0W0 and that partially self-consistent flavors (in particular evGW) can be excellent alternatives.","sentences":["We present the formalism and implementation of quasi-particle self-consistent GW (qsGW) and eigenvalue only quasi-particle self-consistent GW (evGW) adapted to standard quantum chemistry packages.","Our implementation is benchmarked against high-level quantum chemistry computations (coupled-cluster theory) and experimental results using a representative set of molecules.","Furthermore, we compare the qsGW approach for five molecules relevant for organic photovoltaics to self-consistent GW results (scGW) and analyze the effects of the self-consistency on the ground state density by comparing calculated dipole moments to their experimental values.","We show that qsGW makes a significant improvement over conventional G0W0 and that partially self-consistent flavors (in particular evGW) can be excellent alternatives."],"url":"http://arxiv.org/abs/2404.06415v1","category":"physics.chem-ph"}
{"created":"2024-04-09 15:50:29","title":"Radiation Tolerance of the LHCb Outer Tracker: in the Lab and in the Forward Region at the LHC","abstract":"During the detector construction phase between 2004 and 2006, it was discovered that the LHCb Outer Tracker (OT) detector suffered from gain loss after irradiation in the laboratory at moderate intensities. Under irradiation an insulating layer was formed on the anode wire. The aging was caused by contamination of the counting gas due to outgassing of the glue used in construction namely araldite AY103-1. The gain loss was concentrated upstream the gas flow, and at moderate irradiation intensity only. The aging rate was reduced by longterm flushing and by the addition of a few percent of O2 to the gas mixture. Furthermore, applying a large positive high voltage (beyond the amplification regime) has shown to remove the insulating deposits without damaging the wire surface. This paper presents the history of the developments together with the characteristics and the culprit of the aging phenomenon and the resulting detector performance in situ.","sentences":["During the detector construction phase between 2004 and 2006, it was discovered that the LHCb Outer Tracker (OT) detector suffered from gain loss after irradiation in the laboratory at moderate intensities.","Under irradiation an insulating layer was formed on the anode wire.","The aging was caused by contamination of the counting gas due to outgassing of the glue used in construction namely araldite AY103-1.","The gain loss was concentrated upstream the gas flow, and at moderate irradiation intensity only.","The aging rate was reduced by longterm flushing and by the addition of a few percent of O2 to the gas mixture.","Furthermore, applying a large positive high voltage (beyond the amplification regime) has shown to remove the insulating deposits without damaging the wire surface.","This paper presents the history of the developments together with the characteristics and the culprit of the aging phenomenon and the resulting detector performance in situ."],"url":"http://arxiv.org/abs/2404.06402v1","category":"physics.ins-det"}
{"created":"2024-04-09 15:48:31","title":"Bounded Edit Distance: Optimal Static and Dynamic Algorithms for Small Integer Weights","abstract":"The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis. In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin [JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit. A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors. Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved. The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance. Only recently, Das et al. [STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz [FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound. In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update. Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time.","sentences":["The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other.","The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis.","In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin","[JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit.","A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors.","Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   ","Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved.","The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance.","Only recently, Das et al.","[STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz","[FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound.","In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update.","Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time."],"url":"http://arxiv.org/abs/2404.06401v1","category":"cs.DS"}
{"created":"2024-04-09 15:35:02","title":"Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity","abstract":"One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier. This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   In this paper, we conduct a fine-grained analysis of this connectivity phenomenon. First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance. This finding suggests that the landscape should be nearly convex in a certain sense. Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths. These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10.","sentences":["One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier.","This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   ","In this paper, we conduct a fine-grained analysis of this connectivity phenomenon.","First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance.","This finding suggests that the landscape should be nearly convex in a certain sense.","Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths.","These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10."],"url":"http://arxiv.org/abs/2404.06391v1","category":"cs.LG"}
{"created":"2024-04-09 14:26:05","title":"Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems","abstract":"Sparse recovery principles play an important role in solving many nonlinear ill-posed inverse problems. We investigate a variational framework with support Oracle for compressed sensing sparse reconstructions, where the available measurements are nonlinear and possibly corrupted by noise. A graph neural network, named Oracle-Net, is proposed to predict the support from the nonlinear measurements and is integrated into a regularized recovery model to enforce sparsity. The derived nonsmooth optimization problem is then efficiently solved through a constrained proximal gradient method. Error bounds on the approximate solution of the proposed Oracle-based optimization are provided in the context of the ill-posed Electrical Impedance Tomography problem. Numerical solutions of the EIT nonlinear inverse reconstruction problem confirm the potential of the proposed method which improves the reconstruction quality from undersampled measurements, under sparsity assumptions.","sentences":["Sparse recovery principles play an important role in solving many nonlinear ill-posed inverse problems.","We investigate a variational framework with support Oracle for compressed sensing sparse reconstructions, where the available measurements are nonlinear and possibly corrupted by noise.","A graph neural network, named Oracle-Net, is proposed to predict the support from the nonlinear measurements and is integrated into a regularized recovery model to enforce sparsity.","The derived nonsmooth optimization problem is then efficiently solved through a constrained proximal gradient method.","Error bounds on the approximate solution of the proposed Oracle-based optimization are provided in the context of the ill-posed Electrical Impedance Tomography problem.","Numerical solutions of the EIT nonlinear inverse reconstruction problem confirm the potential of the proposed method which improves the reconstruction quality from undersampled measurements, under sparsity assumptions."],"url":"http://arxiv.org/abs/2404.06342v1","category":"math.NA"}
{"created":"2024-04-09 14:25:27","title":"Finding fake reviews in e-commerce platforms by using hybrid algorithms","abstract":"Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data. In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers. Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction. By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets. The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches. Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs.","sentences":["Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data.","In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers.","Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction.","By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets.","The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches.","Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs."],"url":"http://arxiv.org/abs/2404.06339v1","category":"cs.CL"}
{"created":"2024-04-09 14:02:34","title":"From chiral EFT to perturbative QCD: a Bayesian model mixing approach to symmetric nuclear matter","abstract":"Constraining the equation of state (EOS) of strongly interacting, dense matter is the focus of intense experimental, observational, and theoretical effort. Chiral effective field theory ($\\chi$EFT) can describe the EOS between the typical densities of nuclei and those in the outer cores of neutron stars while perturbative QCD (pQCD) can be applied to properties of deconfined quark matter, both with quantified theoretical uncertainties. However, describing the complete range of densities between nuclear saturation and an almost-free quark gas with a single EOS that has well-quantified uncertainties is a challenging problem. In this work, we argue that Bayesian multi-model inference from $\\chi$EFT and pQCD can help bridge the gap between the two theories: we combine the Gaussian random variables that constitute the theories' predictions for the pressure as a function of the density in symmetric nuclear matter. We do this using two Bayesian model mixing procedures: a pointwise approach, and a correlated approach implemented via a Gaussian process (GP), and present results for the pressure and speed of sound in each. The second method produces a smooth $\\chi$EFT-to-pQCD EOS. Without input data in the intermediate region, the choice of prior on the EOS, encoded through the GP kernel, as the prior on the EOS function space significantly affects the result in that region. We also discuss future extensions and applications to neutron star matter guided by recent EOS constraints from nuclear theory, nuclear experiment, and multi-messenger astronomy.","sentences":["Constraining the equation of state (EOS) of strongly interacting, dense matter is the focus of intense experimental, observational, and theoretical effort.","Chiral effective field theory ($\\chi$EFT) can describe the EOS between the typical densities of nuclei and those in the outer cores of neutron stars while perturbative QCD (pQCD) can be applied to properties of deconfined quark matter, both with quantified theoretical uncertainties.","However, describing the complete range of densities between nuclear saturation and an almost-free quark gas with a single EOS that has well-quantified uncertainties is a challenging problem.","In this work, we argue that Bayesian multi-model inference from $\\chi$EFT and pQCD can help bridge the gap between the two theories: we combine the Gaussian random variables that constitute the theories' predictions for the pressure as a function of the density in symmetric nuclear matter.","We do this using two Bayesian model mixing procedures: a pointwise approach, and a correlated approach implemented via a Gaussian process (GP), and present results for the pressure and speed of sound in each.","The second method produces a smooth $\\chi$EFT-to-pQCD EOS.","Without input data in the intermediate region, the choice of prior on the EOS, encoded through the GP kernel, as the prior on the EOS function space significantly affects the result in that region.","We also discuss future extensions and applications to neutron star matter guided by recent EOS constraints from nuclear theory, nuclear experiment, and multi-messenger astronomy."],"url":"http://arxiv.org/abs/2404.06323v1","category":"nucl-th"}
{"created":"2024-04-09 13:47:37","title":"On adversarial training and the 1 Nearest Neighbor classifier","abstract":"The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples. While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations. In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training. We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity. In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy. In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training. Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier. our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier","sentences":["The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples.","While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations.","In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training.","We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity.","In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy.","In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training.","Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier.","our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier"],"url":"http://arxiv.org/abs/2404.06313v1","category":"cs.LG"}
{"created":"2024-04-09 13:34:19","title":"Constraining the Coronal Properties of AB Dor in the Radio Regime","abstract":"We present a multiwavelength study of AB Doradus, combining modelling that incorporates a spectropolarimetric magnetic field map with 8.4 GHz radio interferometry to measure the coronal extent and density of this young star. We use the surface magnetic field map to produce a 3D extrapolation of AB Dor's coronal magnetic field. From this model we create synthetic radio images throughout the stellar rotation period which we can compare with the interferometric radio observations. Our models reproduce the two-lobe structure seen in the radio observations. We successfully fit the observed flux magnitude and lobe separation with our model. We conclude that that the features seen in the radio images are a result of centrifugal containment of hot gas at the peak of closed magnetic loops, and that the corona of AB Dor extends to about 8-10 stellar radii, making it much more extended than the present-day solar corona.","sentences":["We present a multiwavelength study of AB Doradus, combining modelling that incorporates a spectropolarimetric magnetic field map with 8.4 GHz radio interferometry to measure the coronal extent and density of this young star.","We use the surface magnetic field map to produce a 3D extrapolation of AB Dor's coronal magnetic field.","From this model we create synthetic radio images throughout the stellar rotation period which we can compare with the interferometric radio observations.","Our models reproduce the two-lobe structure seen in the radio observations.","We successfully fit the observed flux magnitude and lobe separation with our model.","We conclude that that the features seen in the radio images are a result of centrifugal containment of hot gas at the peak of closed magnetic loops, and that the corona of AB Dor extends to about 8-10 stellar radii, making it much more extended than the present-day solar corona."],"url":"http://arxiv.org/abs/2404.06304v1","category":"astro-ph.SR"}
{"created":"2024-04-09 13:19:25","title":"Optimal Stopping with Interdependent Values","abstract":"We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values. In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents. Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model. For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare. On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values. We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents. All our results are constructive and use simple stopping rules.","sentences":["We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values.","In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents.","Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model.","For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare.","On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   ","We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values.","We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents.","All our results are constructive and use simple stopping rules."],"url":"http://arxiv.org/abs/2404.06293v1","category":"cs.GT"}
{"created":"2024-04-09 13:13:24","title":"Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training","abstract":"The key to multi-label image classification (MLC) is to improve model performance by leveraging label correlations. Unfortunately, it has been shown that overemphasizing co-occurrence relationships can cause the overfitting issue of the model, ultimately leading to performance degradation. In this paper, we provide a causal inference framework to show that the correlative features caused by the target object and its co-occurring objects can be regarded as a mediator, which has both positive and negative impacts on model predictions. On the positive side, the mediator enhances the recognition performance of the model by capturing co-occurrence relationships; on the negative side, it has the harmful causal effect that causes the model to make an incorrect prediction for the target object, even when only co-occurring objects are present in an image. To address this problem, we propose a counterfactual reasoning method to measure the total direct effect, achieved by enhancing the direct effect caused only by the target object. Due to the unknown location of the target object, we propose patching-based training and inference to accomplish this goal, which divides an image into multiple patches and identifies the pivot patch that contains the target object. Experimental results on multiple benchmark datasets with diverse configurations validate that the proposed method can achieve state-of-the-art performance.","sentences":["The key to multi-label image classification (MLC) is to improve model performance by leveraging label correlations.","Unfortunately, it has been shown that overemphasizing co-occurrence relationships can cause the overfitting issue of the model, ultimately leading to performance degradation.","In this paper, we provide a causal inference framework to show that the correlative features caused by the target object and its co-occurring objects can be regarded as a mediator, which has both positive and negative impacts on model predictions.","On the positive side, the mediator enhances the recognition performance of the model by capturing co-occurrence relationships; on the negative side, it has the harmful causal effect that causes the model to make an incorrect prediction for the target object, even when only co-occurring objects are present in an image.","To address this problem, we propose a counterfactual reasoning method to measure the total direct effect, achieved by enhancing the direct effect caused only by the target object.","Due to the unknown location of the target object, we propose patching-based training and inference to accomplish this goal, which divides an image into multiple patches and identifies the pivot patch that contains the target object.","Experimental results on multiple benchmark datasets with diverse configurations validate that the proposed method can achieve state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.06287v1","category":"cs.CV"}
{"created":"2024-04-09 13:05:07","title":"IsoDAR@Yemilab: Preliminary Design Report -- Volume I: Cyclotron Driver","abstract":"This Preliminary Design Report (PDR) describes the IsoDAR electron-antineutrino source. Volumes I and II are site-independent and describe the cyclotron driver providing a 10~mA proton beam, and the medium energy beam transport line and target, respectively. Volume III describes the installation at the Yemilab underground laboratory in South Korea. The IsoDAR driver and target will produce a mole of electron-antineutrinos over the course of five years. Paired with a kton-scale liquid scintillator detector, it will enable an impressive particle physics program including searches for new symmetries, new interactions and new particles. Here in Volume I, we describe the driver, which includes the ion source, low energy beam transport, and cyclotron. The latter features radiofrequency quadrupole (RFQ) direct axial injection and represents the first accelerator purpose-built to make use of vortex motion.","sentences":["This Preliminary Design Report (PDR) describes the IsoDAR electron-antineutrino source.","Volumes I and II are site-independent and describe the cyclotron driver providing a 10~mA proton beam, and the medium energy beam transport line and target, respectively.","Volume III describes the installation at the Yemilab underground laboratory in South Korea.","The IsoDAR driver and target will produce a mole of electron-antineutrinos over the course of five years.","Paired with a kton-scale liquid scintillator detector, it will enable an impressive particle physics program including searches for new symmetries, new interactions and new particles.","Here in Volume I, we describe the driver, which includes the ion source, low energy beam transport, and cyclotron.","The latter features radiofrequency quadrupole (RFQ) direct axial injection and represents the first accelerator purpose-built to make use of vortex motion."],"url":"http://arxiv.org/abs/2404.06281v1","category":"physics.acc-ph"}
{"created":"2024-04-09 12:48:24","title":"Robust Confidence Intervals in Stereo Matching using Possibility Theory","abstract":"We propose a method for estimating disparity confidence intervals in stereo matching problems. Confidence intervals provide complementary information to usual confidence measures. To the best of our knowledge, this is the first method creating disparity confidence intervals based on the cost volume. This method relies on possibility distributions to interpret the epistemic uncertainty of the cost volume. Our method has the benefit of having a white-box nature, differing in this respect from current state-of-the-art deep neural networks approaches. The accuracy and size of confidence intervals are validated using the Middlebury stereo datasets as well as a dataset of satellite images. This contribution is freely available on GitHub.","sentences":["We propose a method for estimating disparity confidence intervals in stereo matching problems.","Confidence intervals provide complementary information to usual confidence measures.","To the best of our knowledge, this is the first method creating disparity confidence intervals based on the cost volume.","This method relies on possibility distributions to interpret the epistemic uncertainty of the cost volume.","Our method has the benefit of having a white-box nature, differing in this respect from current state-of-the-art deep neural networks approaches.","The accuracy and size of confidence intervals are validated using the Middlebury stereo datasets as well as a dataset of satellite images.","This contribution is freely available on GitHub."],"url":"http://arxiv.org/abs/2404.06273v1","category":"cs.CV"}
{"created":"2024-04-09 12:47:47","title":"The Gravitational Chiral Anomaly at Finite Temperature and Density","abstract":"We investigate the gravitational anomaly vertex $\\langle TTJ_5\\rangle$ (graviton - graviton - axial current) under conditions of finite density and temperature. Through a direct analysis of perturbative contributions, we demonstrate that neither finite temperature nor finite fermion density affects the gravitational chiral anomaly. These results find application in several contexts, from topological materials to the early universe plasma. They affect the decay of any axion or axion-like particle into gravitational waves, in very dense and hot environments.","sentences":["We investigate the gravitational anomaly vertex $\\langle TTJ_5\\rangle$ (graviton - graviton - axial current) under conditions of finite density and temperature.","Through a direct analysis of perturbative contributions, we demonstrate that neither finite temperature nor finite fermion density affects the gravitational chiral anomaly.","These results find application in several contexts, from topological materials to the early universe plasma.","They affect the decay of any axion or axion-like particle into gravitational waves, in very dense and hot environments."],"url":"http://arxiv.org/abs/2404.06272v1","category":"hep-th"}
{"created":"2024-04-09 12:25:06","title":"From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data","abstract":"Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.","sentences":["Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis.","Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia.","This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs).","Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging.","To address these issues, we propose Triplet Training for differential diagnosis with limited target data.","It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset.","Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%.","We further provide insights into the training process by visualizing changes in the latent space after each step.","Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study.","Our code is available at https://github.com/ai-med/TripletTraining."],"url":"http://arxiv.org/abs/2404.06253v1","category":"cs.CV"}
{"created":"2024-04-09 12:06:21","title":"Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation","abstract":"Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data security. To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data. To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3. For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases). The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic. Our code is available at https://github.com/AwesomeLemon/HyFree-S3","sentences":["Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data security.","To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data.","To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3.","For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases).","The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic.","Our code is available at https://github.com/AwesomeLemon/HyFree-S3"],"url":"http://arxiv.org/abs/2404.06240v1","category":"cs.CV"}
{"created":"2024-04-09 11:50:50","title":"One-Dimensional Model for Coupled Flexural, Torsional and Extensional Motions of Elastic Beams with Embedded Point Magnets under Magnetic Fields","abstract":"This paper establishes a one-dimensional theoretical model for an elastic beam with embedded point magnets in extensional, bending and torsional motions. The beam has a circular cross-section. The point magnets may be in the interior or at the ends of the beam. They are assumed to be small and rigid, and may be ferromagnetic with spontaneous magnetic moments that can change their directions with respect to the magnets. The model shows that the extensional, flexural and torsional motions of the beam may be coupled by the point magnets when an external magnetic field is applied. A few examples are given. Piezoelectric beams with point magnets are also discussed.","sentences":["This paper establishes a one-dimensional theoretical model for an elastic beam with embedded point magnets in extensional, bending and torsional motions.","The beam has a circular cross-section.","The point magnets may be in the interior or at the ends of the beam.","They are assumed to be small and rigid, and may be ferromagnetic with spontaneous magnetic moments that can change their directions with respect to the magnets.","The model shows that the extensional, flexural and torsional motions of the beam may be coupled by the point magnets when an external magnetic field is applied.","A few examples are given.","Piezoelectric beams with point magnets are also discussed."],"url":"http://arxiv.org/abs/2404.06232v1","category":"physics.app-ph"}
{"created":"2024-04-09 11:28:28","title":"Precision measurements on Si-","abstract":"High-precision measurements of the electron affinities (EA) of the three stable isotopes of silicon, $^{28}$Si, $^{29}$Si and $^{30}$Si, have been performed at the cryogenic electrostatic ion-beam storage ring DESIREE. The quantum states of the ions were manipulated using laser depletion, and the ions were photodetached by laser photodetachment threshold spectroscopy. These EA values are the first reported for $^{29}$Si$^-$ and $^{30}$Si$^-$ and provide a reduced uncertainty for $^{28}$Si$^-$. The resulting EAs are $EA(^{28}$Si$) = 1.38952201(17)$ eV, $EA(^{29}$Si$) = 1.38952172(12)$ eV and $EA(^{29}$Si$) = 1.38952078(12)$ eV, with the corresponding isotope shifts $IS(^{29-28}$Si$) = 0.29(16)$ micro eV and $IS(^{30-28}$Si$) = 1.23(16) $ micro eV. In addition to these measurements, the resolution and signal-to-background level was sufficient to reveal the hyperfine structure splitting in the $^{29}$Si$^-$ isotope, which we report to be $1.8(4) micro eV.","sentences":["High-precision measurements of the electron affinities (EA) of the three stable isotopes of silicon, $^{28}$Si, $^{29}$Si and $^{30}$Si, have been performed at the cryogenic electrostatic ion-beam storage ring DESIREE.","The quantum states of the ions were manipulated using laser depletion, and the ions were photodetached by laser photodetachment threshold spectroscopy.","These EA values are the first reported for $^{29}$Si$^-$ and $^{30}$Si$^-$ and provide a reduced uncertainty for $^{28}$Si$^-$. The resulting EAs are $EA(^{28}$Si$)","= 1.38952201(17)$ eV, $EA(^{29}$Si$)","= 1.38952172(12)$ eV and $EA(^{29}$Si$)","= 1.38952078(12)$ eV, with the corresponding isotope shifts $IS(^{29-28}$Si$) = 0.29(16)$ micro eV and $IS(^{30-28}$Si$) = 1.23(16) $ micro eV. In addition to these measurements, the resolution and signal-to-background level was sufficient to reveal the hyperfine structure splitting in the $^{29}$Si$^-$ isotope, which we report to be $1.8(4) micro eV."],"url":"http://arxiv.org/abs/2404.06226v1","category":"physics.atom-ph"}
{"created":"2024-04-09 10:47:01","title":"Further Understanding of a Local Gaussian Process Approximation: Characterising Convergence in the Finite Regime","abstract":"We show that common choices of kernel functions for a highly accurate and massively scalable nearest-neighbour based GP regression model (GPnn: \\cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as dataset-size $n$ increases. For isotropic kernels such as Mat\\'{e}rn and squared-exponential, an upper bound on the predictive MSE can be obtained as $O(n^{-\\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and $d>p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on the input distribution. Similar bounds can be found under model misspecification and combined to give overall rates of convergence of both MSE and an important calibration metric. We show that lower bounds on $n$ can be given in terms of $m$, $l$, $p$, $d$, a tolerance $\\varepsilon$ and a probability $\\delta$. When $m$ is chosen to be $O(n^{\\frac{p}{p+d}})$ minimax optimal rates of convergence are attained. Finally, we demonstrate empirical performance and show that in many cases convergence occurs faster than the upper bounds given here.","sentences":["We show that common choices of kernel functions for a highly accurate and massively scalable nearest-neighbour based GP regression model (GPnn: \\cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as dataset-size $n$ increases.","For isotropic kernels such as Mat\\'{e}rn and squared-exponential, an upper bound on the predictive MSE can be obtained as $O(n^{-\\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and $d>p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on the input distribution.","Similar bounds can be found under model misspecification and combined to give overall rates of convergence of both MSE and an important calibration metric.","We show that lower bounds on $n$ can be given in terms of $m$, $l$, $p$, $d$, a tolerance $\\varepsilon$ and a probability $\\delta$.","When $m$ is chosen to be $O(n^{\\frac{p}{p+d}})$ minimax optimal rates of convergence are attained.","Finally, we demonstrate empirical performance and show that in many cases convergence occurs faster than the upper bounds given here."],"url":"http://arxiv.org/abs/2404.06200v1","category":"math.ST"}
{"created":"2024-04-09 10:40:26","title":"SUPPLY: Sustainable multi-UAV Performance-aware Placement Algorithm for Flying Networks","abstract":"Unmanned Aerial Vehicles (UAVs) are used for a wide range of applications. Due to characteristics such as the ability to hover and carry cargo on-board, rotary-wing UAVs have been considered suitable platforms for carrying communications nodes, including Wi-Fi Access Points and cellular Base Stations. This gave rise to the concept of Flying Networks (FNs), now making part of the so-called Non-Terrestrial Networks (NTNs) defined in 3GPP. In scenarios where the deployment of terrestrial networks is not feasible, the use of FNs has emerged as a solution to provide wireless connectivity. However, the management of the communications resources in FNs imposes significant challenges, especially regarding the positioning of the UAVs so that the Quality of Service (QoS) offered to the Ground Users (GUs) and devices is maximized. Moreover, unlike terrestrial networks that are directly connected to the power grid, UAVs typically rely on on-board batteries that need to be recharged. In order to maximize the UAVs' flying time, the energy consumed by the UAVs needs to be minimized. When it comes to multi-UAV placement, most state-of-the-art solutions focus on maximizing the coverage area and assume that the UAVs keep hovering in a fixed position while serving GUs. Also, they do not address the energy-aware multi-UAV placement problem in networking scenarios where the GUs may have different QoS requirements and may not be uniformly distributed across the area of interest. In this work, we propose the Sustainable multi-UAV Performance-aware Placement (SUPPLY) algorithm. SUPPLY defines the energy and performance-aware positioning of multiple UAVs in an FN. To accomplish this, SUPPLY defines trajectories that minimize UAVs' energy consumption, while ensuring the targeted QoS levels. The obtained results show up to 25% energy consumption reduction with minimal impact on throughput and delay.","sentences":["Unmanned Aerial Vehicles (UAVs) are used for a wide range of applications.","Due to characteristics such as the ability to hover and carry cargo on-board, rotary-wing UAVs have been considered suitable platforms for carrying communications nodes, including Wi-Fi Access Points and cellular Base Stations.","This gave rise to the concept of Flying Networks (FNs), now making part of the so-called Non-Terrestrial Networks (NTNs) defined in 3GPP.","In scenarios where the deployment of terrestrial networks is not feasible, the use of FNs has emerged as a solution to provide wireless connectivity.","However, the management of the communications resources in FNs imposes significant challenges, especially regarding the positioning of the UAVs so that the Quality of Service (QoS) offered to the Ground Users (GUs) and devices is maximized.","Moreover, unlike terrestrial networks that are directly connected to the power grid, UAVs typically rely on on-board batteries that need to be recharged.","In order to maximize the UAVs' flying time, the energy consumed by the UAVs needs to be minimized.","When it comes to multi-UAV placement, most state-of-the-art solutions focus on maximizing the coverage area and assume that the UAVs keep hovering in a fixed position while serving GUs.","Also, they do not address the energy-aware multi-UAV placement problem in networking scenarios where the GUs may have different QoS requirements and may not be uniformly distributed across the area of interest.","In this work, we propose the Sustainable multi-UAV Performance-aware Placement (SUPPLY) algorithm.","SUPPLY defines the energy and performance-aware positioning of multiple UAVs in an FN.","To accomplish this, SUPPLY defines trajectories that minimize UAVs' energy consumption, while ensuring the targeted QoS levels.","The obtained results show up to 25% energy consumption reduction with minimal impact on throughput and delay."],"url":"http://arxiv.org/abs/2404.06197v1","category":"cs.NI"}
{"created":"2024-04-09 08:57:27","title":"Radial and vertical constraints on the icy origin of H$_{2}$CO in the HD 163296 Protoplanetary Disk","abstract":"H$_2$CO is a small organic molecule widely detected in protoplanetary disks. As a precursor to grain-surface formation of CH$_3$OH, H$_2$CO is considered an important precursor of O-bearing organic molecules that are locked in ices. Still, since gas-phase reactions can also form H$_2$CO, there remains an open question on the channels by which organics form in disks, and how much the grain versus the gas pathways impact the overall organic reservoir. We present spectrally and spatially resolved Atacama Large Millimeter/submillimeter Array (ALMA) observations of several ortho- and para-H$_2$CO transitions toward the bright protoplanetary disk around the Herbig Ae star HD 163296. We derive column density, excitation temperature, and ortho-to-para ratio (OPR) radial profiles for H$_2$CO, as well as disk-averaged values of $N_{\\mathrm{T}}\\sim4\\times 10^{12}$ cm$^{-2}$, $T_{\\mathrm{ex}}\\sim20$ K, and $\\mathrm{OPR}\\sim2.7$, respectively. We empirically determine the vertical structure of the emission, finding vertical heights of $z/r\\sim0.1$. From the profiles, we find a relatively constant $\\mathrm{OPR}\\sim2.7$ with radius, but still consistent with $3.0$ among the uncertainties, a secondary increase of $N_{\\mathrm{T}}$ in the outer disk, and low $T_{\\mathrm{ex}}$ values that decrease with disk radius. Our resulting radial, vertical, and OPR constraints suggest an increased UV penetration beyond the dust millimeter edge, consistent with an icy origin but also with cold gas-phase chemistry. This Herbig disk contrasts previous results for the T Tauri disk, TW Hya, which had a larger contribution from cold gas-phase chemistry. More observations of other sources are needed to disentangle the dominant formation pathway of H$_2$CO in protoplanetary disks.","sentences":["H$_2$CO is a small organic molecule widely detected in protoplanetary disks.","As a precursor to grain-surface formation of CH$_3$OH, H$_2$CO is considered an important precursor of O-bearing organic molecules that are locked in ices.","Still, since gas-phase reactions can also form H$_2$CO, there remains an open question on the channels by which organics form in disks, and how much the grain versus the gas pathways impact the overall organic reservoir.","We present spectrally and spatially resolved Atacama Large Millimeter/submillimeter Array (ALMA) observations of several ortho-","and para-H$_2$CO transitions toward the bright protoplanetary disk around the Herbig Ae star HD 163296.","We derive column density, excitation temperature, and ortho-to-para ratio (OPR) radial profiles for H$_2$CO, as well as disk-averaged values of $N_{\\mathrm{T}}\\sim4\\times 10^{12}$ cm$^{-2}$, $T_{\\mathrm{ex}}\\sim20$ K, and $\\mathrm{OPR}\\sim2.7$, respectively.","We empirically determine the vertical structure of the emission, finding vertical heights of $z/r\\sim0.1$. From the profiles, we find a relatively constant $\\mathrm{OPR}\\sim2.7$ with radius, but still consistent with $3.0$ among the uncertainties, a secondary increase of $N_{\\mathrm{T}}$ in the outer disk, and low $T_{\\mathrm{ex}}$ values that decrease with disk radius.","Our resulting radial, vertical, and OPR constraints suggest an increased UV penetration beyond the dust millimeter edge, consistent with an icy origin but also with cold gas-phase chemistry.","This Herbig disk contrasts previous results for the T Tauri disk, TW Hya, which had a larger contribution from cold gas-phase chemistry.","More observations of other sources are needed to disentangle the dominant formation pathway of H$_2$CO in protoplanetary disks."],"url":"http://arxiv.org/abs/2404.06133v1","category":"astro-ph.EP"}
{"created":"2024-04-09 08:40:08","title":"Low-Loss Silicon Directional Coupler with Arbitrary Coupling Ratios for Broadband Wavelength Operation Based on Bent Waveguides","abstract":"We demonstrate a design for a high-performance $2 \\times 2$ splitter meeting the essential requirements of broadband coupling, support for arbitrary coupling ratio, ultra low-loss, high fabrication tolerance, and a compact footprint.   This is achieved based on a rigorous coupled mode theory analysis of the broadband response of the bent directional coupler (DC) and by demonstrating a full coupling model, with measured broadband values of 0.4, 0.5, 0.6, and 0.7.   As a benchmark, we demonstrate a 0.5:0.5 splitter that significantly reduces coupling variation from 0.391 in the traditional DC to just 0.051 over an 80 nm wavelength span.   This represents a remarkable 7.67 times reduction in coupling variation.   Further, newly-invented low-loss bends were used in the proposed design leading to an ultra low-loss design with negligible excess loss ($\\mathrm{0.003 \\pm 0.013 \\ dB}$).   The proposed 0.5:0.5 silicon strip waveguide-based design is tolerant and shows consistently low coupling variation over a full 300 mm wafer showcasing a maximum cross coupling variation of 0.112 over 80 nm wavelength range, at the extreme edge of the wafer.   Futhermore, we augmented the wafer mapping with a waveguide width fabrication tolerance study, confirming the tolerance of the device with a mere 0.061 maximum coupling variation with a waveguide width deviation of $\\pm 20$ nm over 80 nm wavelength range.   These specs make the proposed splitter an attractive component for practical applications with mass production.","sentences":["We demonstrate a design for a high-performance $2 \\times 2$ splitter meeting the essential requirements of broadband coupling, support for arbitrary coupling ratio, ultra low-loss, high fabrication tolerance, and a compact footprint.   ","This is achieved based on a rigorous coupled mode theory analysis of the broadband response of the bent directional coupler (DC) and by demonstrating a full coupling model, with measured broadband values of 0.4, 0.5, 0.6, and 0.7.   ","As a benchmark, we demonstrate a 0.5:0.5 splitter that significantly reduces coupling variation from 0.391 in the traditional DC to just 0.051 over an 80 nm wavelength span.   ","This represents a remarkable 7.67 times reduction in coupling variation.   ","Further, newly-invented low-loss bends were used in the proposed design leading to an ultra low-loss design with negligible excess loss ($\\mathrm{0.003 \\pm 0.013 \\ dB}$).   ","The proposed 0.5:0.5 silicon strip waveguide-based design is tolerant and shows consistently low coupling variation over a full 300 mm wafer showcasing a maximum cross coupling variation of 0.112 over 80 nm wavelength range, at the extreme edge of the wafer.   ","Futhermore, we augmented the wafer mapping with a waveguide width fabrication tolerance study, confirming the tolerance of the device with a mere 0.061 maximum coupling variation with a waveguide width deviation of $\\pm 20$ nm over 80 nm wavelength range.   ","These specs make the proposed splitter an attractive component for practical applications with mass production."],"url":"http://arxiv.org/abs/2404.06117v1","category":"physics.optics"}
{"created":"2024-04-09 07:25:30","title":"LIPT: Latency-aware Image Processing Transformer","abstract":"Transformer is leading a trend in the field of image processing. Despite the great success that existing lightweight image processing transformers have achieved, they are tailored to FLOPs or parameters reduction, rather than practical inference acceleration. In this paper, we present a latency-aware image processing transformer, termed LIPT. We devise the low-latency proportion LIPT block that substitutes memory-intensive operators with the combination of self-attention and convolutions to achieve practical speedup. Specifically, we propose a novel non-volatile sparse masking self-attention (NVSM-SA) that utilizes a pre-computing sparse mask to capture contextual information from a larger window with no extra computation overload. Besides, a high-frequency reparameterization module (HRM) is proposed to make LIPT block reparameterization friendly, which improves the model's detail reconstruction capability. Extensive experiments on multiple image processing tasks (e.g., image super-resolution (SR), JPEG artifact reduction, and image denoising) demonstrate the superiority of LIPT on both latency and PSNR. LIPT achieves real-time GPU inference with state-of-the-art performance on multiple image SR benchmarks.","sentences":["Transformer is leading a trend in the field of image processing.","Despite the great success that existing lightweight image processing transformers have achieved, they are tailored to FLOPs or parameters reduction, rather than practical inference acceleration.","In this paper, we present a latency-aware image processing transformer, termed LIPT.","We devise the low-latency proportion LIPT block that substitutes memory-intensive operators with the combination of self-attention and convolutions to achieve practical speedup.","Specifically, we propose a novel non-volatile sparse masking self-attention (NVSM-SA) that utilizes a pre-computing sparse mask to capture contextual information from a larger window with no extra computation overload.","Besides, a high-frequency reparameterization module (HRM) is proposed to make LIPT block reparameterization friendly, which improves the model's detail reconstruction capability.","Extensive experiments on multiple image processing tasks (e.g., image super-resolution (SR), JPEG artifact reduction, and image denoising) demonstrate the superiority of LIPT on both latency and PSNR.","LIPT achieves real-time GPU inference with state-of-the-art performance on multiple image SR benchmarks."],"url":"http://arxiv.org/abs/2404.06075v1","category":"cs.CV"}
{"created":"2024-04-09 07:10:38","title":"Kernels of Perturbed Hankel Operators","abstract":"In the classical Hardy space $H^2(\\mathbb{D})$, it is well-known that the kernel of the Hankel operator is invariant under the action of shift operator S and sometimes nearly invariant under the action of backward shift operator $S^{*}$. It appears in this paper that kernels of finite rank perturbations of Hankel operators are almost shift invariant as well as nearly $S^*$- invariant with finite defect. This allows us to obtain a structure of the kernel in several important cases by applying a recent theorem due to Chalendar, Gallardo, and Partington.","sentences":["In the classical Hardy space $H^2(\\mathbb{D})$, it is well-known that the kernel of the Hankel operator is invariant under the action of shift operator S and sometimes nearly invariant under the action of backward shift operator $S^{*}$. It appears in this paper that kernels of finite rank perturbations of Hankel operators are almost shift invariant as well as nearly $S^*$- invariant with finite defect.","This allows us to obtain a structure of the kernel in several important cases by applying a recent theorem due to Chalendar, Gallardo, and Partington."],"url":"http://arxiv.org/abs/2404.06067v1","category":"math.FA"}
{"created":"2024-04-09 07:08:00","title":"Unified Entropy Optimization for Open-Set Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain. Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts. In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes. Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence. To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data. Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data. Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence. Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework. The code is available at https://github.com/gaozhengqing/UniEnt","sentences":["Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain.","Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts.","In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes.","Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence.","To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data.","Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data.","Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence.","Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework.","The code is available at https://github.com/gaozhengqing/UniEnt"],"url":"http://arxiv.org/abs/2404.06065v1","category":"cs.CV"}
{"created":"2024-04-09 06:57:00","title":"Bose-Einstein condensate as a quantum gravity probe; \"Erste Abhandlung\"","abstract":"We consider a Bose-Einstein condensate interacting with a gravitational wave for the case when the gravitational fluctuations are quantized in order to incorporate quantum gravity effects into the theory. We observe that the solution of the time-dependent part of the pseudo-Goldstone boson has infusions from the noise induced by gravitons and the corresponding differential equation of motion is Langevin-like. Using this result, we obtain the quantum gravity modified Fisher information which has been termed as the quantum gravitational Fisher information (QGFI). The inverse square root of the stochastic average of the QGFI gives the minimum uncertainty in the measurement of the gravitational wave amplitude. The minimum uncertainty does not go to infinity as the measurement time approaches zero in a quantum gravity setup rather it has a measurable finite value for gravitons with high squeezing. Finally, we observe the effect of decoherence due to interacting phonon modes in the QGFI and observe a less obvious decoherence effect for higher squeezing of the initial graviton.","sentences":["We consider a Bose-Einstein condensate interacting with a gravitational wave for the case when the gravitational fluctuations are quantized in order to incorporate quantum gravity effects into the theory.","We observe that the solution of the time-dependent part of the pseudo-Goldstone boson has infusions from the noise induced by gravitons and the corresponding differential equation of motion is Langevin-like.","Using this result, we obtain the quantum gravity modified Fisher information which has been termed as the quantum gravitational Fisher information (QGFI).","The inverse square root of the stochastic average of the QGFI gives the minimum uncertainty in the measurement of the gravitational wave amplitude.","The minimum uncertainty does not go to infinity as the measurement time approaches zero in a quantum gravity setup rather it has a measurable finite value for gravitons with high squeezing.","Finally, we observe the effect of decoherence due to interacting phonon modes in the QGFI and observe a less obvious decoherence effect for higher squeezing of the initial graviton."],"url":"http://arxiv.org/abs/2404.06060v1","category":"hep-th"}
{"created":"2024-04-09 06:33:13","title":"How coherence measurements of a qubit steer its quantum environment","abstract":"Repetitive Ramsey interferometry measurements (RIMs) are often used to measure qubit coherence, assuming that the environment remains unaffected after each measurement and the outcomes of all measurements are independent and identically distributed (i.i.d.). While this assumption is often valid for a classical environment, it may not hold for a quantum environment due to non-negligible backaction. Here we present a general theoretical framework to account for the measurement backaction in sequential RIMs. We show that an RIM induces a quantum channel on the quantum environment, and sequential RIMs gradually steer the quantum environment to the fixed points of the channel. For the first time, we reveal three distinct environment steering effects -- polarization, depolarization and metastable polarization, depending on the commutativity of the noise operator $B$ and the environment Hamiltonian $H_e$: (1) When $[B,H_e]=0$, the quantum environment is gradually polarized to different eigenstates of $B$ as the number $m$ of repetitive RIMs increases; (2) When $[B,H_e]\\neq 0$, the quantum environment is gradually depolarized to a maximally mixed state of its whole Hilbert space or a Hilbert subspace; (3) When $[B,H_e]\\neq 0$ but one of $H_e$ and $B$ is a small perturbation on the other, metastable polarization can happen, such that the quantum environment is first polarized for a finite range of $m$ but becomes gradually depolarized as $m$ increases further. The environment steering also makes the measurement statistics of sequential RIMs develop non-i.i.d. features. Realistic examples of central spin models are also presented. Our work not only elucidates the measurement backaction and statistics of repetitive qubit coherence measurements, but is also useful for designing protocols to engineer the state or dynamics of a quantum environment with a qubit ancilla.","sentences":["Repetitive Ramsey interferometry measurements (RIMs) are often used to measure qubit coherence, assuming that the environment remains unaffected after each measurement and the outcomes of all measurements are independent and identically distributed (i.i.d.).","While this assumption is often valid for a classical environment, it may not hold for a quantum environment due to non-negligible backaction.","Here we present a general theoretical framework to account for the measurement backaction in sequential RIMs.","We show that an RIM induces a quantum channel on the quantum environment, and sequential RIMs gradually steer the quantum environment to the fixed points of the channel.","For the first time, we reveal three distinct environment steering effects -- polarization, depolarization and metastable polarization, depending on the commutativity of the noise operator $B$ and the environment Hamiltonian $H_e$: (1) When $[B,H_e]=0$, the quantum environment is gradually polarized to different eigenstates of $B$ as the number $m$ of repetitive RIMs increases; (2) When $[B,H_e]\\neq 0$, the quantum environment is gradually depolarized to a maximally mixed state of its whole Hilbert space or a Hilbert subspace; (3) When $[B,H_e]\\neq 0$ but one of $H_e$ and $B$ is a small perturbation on the other, metastable polarization can happen, such that the quantum environment is first polarized for a finite range of $m$ but becomes gradually depolarized as $m$ increases further.","The environment steering also makes the measurement statistics of sequential RIMs develop non-i.i.d. features.","Realistic examples of central spin models are also presented.","Our work not only elucidates the measurement backaction and statistics of repetitive qubit coherence measurements, but is also useful for designing protocols to engineer the state or dynamics of a quantum environment with a qubit ancilla."],"url":"http://arxiv.org/abs/2404.06053v1","category":"quant-ph"}
{"created":"2024-04-09 06:24:45","title":"Relativistic imprints on dispersion measure space distortions","abstract":"We investigate the three-dimensional clustering of sources emitting electromagnetic pulses traveling through cold electron plasma, whose radial distance is inferred from their dispersion measure. As a distance indicator, dispersion measure is systematically affected by inhomogeneities in the electron density along the line of sight and special and general relativistic effects, similar to the case of redshift surveys. We present analytic expressions for the correlation function of fast radio bursts (FRBs), and for the galaxy-FRB cross-correlation function, in the presence of these dispersion measure-space distortions. We find that the even multipoles of these correlations are primarily dominated by non-local contributions (e.g. the electron density fluctuations integrated along the line of sight), while the dipole also receives a significant contribution from the Doppler effect, one of the major relativistic effects. A large number of FRBs, $\\mathcal{O}(10^{5}\\sim10^{6})$, expected to be observed in the Square Kilometre Array, would be enough to measure the even multipoles at very high significance, ${\\rm S}/{\\rm N} \\approx 100$, and perhaps to make a first detection of the dipole (${\\rm S}/{\\rm N} \\approx 10$) in the FRB correlation function and FRB-galaxy cross correlation function. This measurement could open a new window to study and test cosmological models.","sentences":["We investigate the three-dimensional clustering of sources emitting electromagnetic pulses traveling through cold electron plasma, whose radial distance is inferred from their dispersion measure.","As a distance indicator, dispersion measure is systematically affected by inhomogeneities in the electron density along the line of sight and special and general relativistic effects, similar to the case of redshift surveys.","We present analytic expressions for the correlation function of fast radio bursts (FRBs), and for the galaxy-FRB cross-correlation function, in the presence of these dispersion measure-space distortions.","We find that the even multipoles of these correlations are primarily dominated by non-local contributions (e.g. the electron density fluctuations integrated along the line of sight), while the dipole also receives a significant contribution from the Doppler effect, one of the major relativistic effects.","A large number of FRBs, $\\mathcal{O}(10^{5}\\sim10^{6})$, expected to be observed in the Square Kilometre Array, would be enough to measure the even multipoles at very high significance, ${\\rm S}/{\\rm N} \\approx 100$, and perhaps to make a first detection of the dipole (${\\rm S}/{\\rm N} \\approx 10$) in the FRB correlation function and FRB-galaxy cross correlation function.","This measurement could open a new window to study and test cosmological models."],"url":"http://arxiv.org/abs/2404.06049v1","category":"astro-ph.CO"}
{"created":"2024-04-09 06:15:33","title":"Nuclear charge radii of germanium isotopes around $N$ = 40","abstract":"Collinear laser spectroscopy measurements were performed on $^{68-74}$Ge isotopes ($Z = 32$) at ISOLDE-CERN, by probing the $4s^2 4p^2 \\, ^3\\!P_1 \\rightarrow 4s^2 4p 5s \\, ^3\\!P_1^o$ atomic transition (269~nm) of germanium. Nuclear charge radii are determined via the measured isotope shifts, revealing a larger local variation than the neighboring isotopic chains. Nuclear density functional theory with the Fayans functionals Fy($\\Delta r$,HFB) and Fy(IVP), and the SV-min Skyrme describes the experimental data for the differential charge radii $\\delta\\langle r^{2} \\rangle$ and charge radii $R_{\\rm c}$ within the theoretical uncertainties. The observed large variation in the charge radii of germanium isotopes is better accounted for by theoretical models incorporating ground state quadrupole correlations. This suggests that the polarization effects due to pairing and deformation contribute to the observed large odd-even staggering in the charge radii of the Ge isotopic chain.","sentences":["Collinear laser spectroscopy measurements were performed on $^{68-74}$Ge isotopes ($Z = 32$) at ISOLDE-CERN, by probing the $4s^2 4p^2 \\, ^3\\!P_1 \\rightarrow 4s^2 4p 5s \\, ^3\\!P_1^o$ atomic transition (269~nm) of germanium.","Nuclear charge radii are determined via the measured isotope shifts, revealing a larger local variation than the neighboring isotopic chains.","Nuclear density functional theory with the Fayans functionals Fy($\\Delta r$,HFB) and Fy(IVP), and the SV-min Skyrme describes the experimental data for the differential charge radii $\\delta\\langle r^{2} \\rangle$ and charge radii $R_{\\rm c}$ within the theoretical uncertainties.","The observed large variation in the charge radii of germanium isotopes is better accounted for by theoretical models incorporating ground state quadrupole correlations.","This suggests that the polarization effects due to pairing and deformation contribute to the observed large odd-even staggering in the charge radii of the Ge isotopic chain."],"url":"http://arxiv.org/abs/2404.06046v1","category":"nucl-ex"}
{"created":"2024-04-09 05:44:01","title":"Low-rank generalized alternating direction implicit iteration method for solving matrix equations","abstract":"This paper presents an effective low-rank generalized alternating direction implicit iteration (R-GADI) method for solving large-scale sparse and stable Lyapunov matrix equations and continuous-time algebraic Riccati matrix equations. The method is based on generalized alternating direction implicit iteration (GADI), which exploits the low-rank property of matrices and utilizes the Cholesky factorization approach for solving. The advantage of the new algorithm lies in its direct and efficient low-rank formulation, which is a variant of the Cholesky decomposition in the Lyapunov GADI method, saving storage space and making it computationally effective. When solving the continuous-time algebraic Riccati matrix equation, the Riccati equation is first simplified to a Lyapunov equation using the Newton method, and then the R-GADI method is employed for computation. Additionally, we analyze the convergence of the R-GADI method and prove its consistency with the convergence of the GADI method. Finally, the effectiveness of the new algorithm is demonstrated through corresponding numerical experiments.","sentences":["This paper presents an effective low-rank generalized alternating direction implicit iteration (R-GADI) method for solving large-scale sparse and stable Lyapunov matrix equations and continuous-time algebraic Riccati matrix equations.","The method is based on generalized alternating direction implicit iteration (GADI), which exploits the low-rank property of matrices and utilizes the Cholesky factorization approach for solving.","The advantage of the new algorithm lies in its direct and efficient low-rank formulation, which is a variant of the Cholesky decomposition in the Lyapunov GADI method, saving storage space and making it computationally effective.","When solving the continuous-time algebraic Riccati matrix equation, the Riccati equation is first simplified to a Lyapunov equation using the Newton method, and then the R-GADI method is employed for computation.","Additionally, we analyze the convergence of the R-GADI method and prove its consistency with the convergence of the GADI method.","Finally, the effectiveness of the new algorithm is demonstrated through corresponding numerical experiments."],"url":"http://arxiv.org/abs/2404.06034v1","category":"math.NA"}
{"created":"2024-04-09 05:44:00","title":"Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for Multi-exposure Image Fusion","abstract":"In recent years, deep learning networks have made remarkable strides in the domain of multi-exposure image fusion. Nonetheless, prevailing approaches often involve directly feeding over-exposed and under-exposed images into the network, which leads to the under-utilization of inherent information present in the source images. Additionally, unsupervised techniques predominantly employ rudimentary weighted summation for color channel processing, culminating in an overall desaturated final image tone. To partially mitigate these issues, this study proposes a gamma correction module specifically designed to fully leverage latent information embedded within source images. Furthermore, a modified transformer block, embracing with self-attention mechanisms, is introduced to optimize the fusion process. Ultimately, a novel color enhancement algorithm is presented to augment image saturation while preserving intricate details. The source code is available at this <a href=\"https://github.com/ZhiyingDu/BHFMEF\" rel=\"external noopener nofollow\" class=\"link-external link-https\">https://github.com/ZhiyingDu/BHFMEF</a> url.","sentences":["In recent years, deep learning networks have made remarkable strides in the domain of multi-exposure image fusion.","Nonetheless, prevailing approaches often involve directly feeding over-exposed and under-exposed images into the network, which leads to the under-utilization of inherent information present in the source images.","Additionally, unsupervised techniques predominantly employ rudimentary weighted summation for color channel processing, culminating in an overall desaturated final image tone.","To partially mitigate these issues, this study proposes a gamma correction module specifically designed to fully leverage latent information embedded within source images.","Furthermore, a modified transformer block, embracing with self-attention mechanisms, is introduced to optimize the fusion process.","Ultimately, a novel color enhancement algorithm is presented to augment image saturation while preserving intricate details.","The source code is available at this <a href=\"https://github.com/ZhiyingDu/BHFMEF\" rel=\"external noopener nofollow\" class=\"link-external link-https\">https://github.com/ZhiyingDu/BHFMEF</a> url."],"url":"http://arxiv.org/abs/2404.06033v1","category":"cs.CV"}
{"created":"2024-04-09 05:04:44","title":"Tests of the Kerr Hypothesis with MAXI J1803-298 Using Different RELXILL_NK Flavors","abstract":"Iron line spectroscopy has been one of the leading methods not only for measuring the spins of accreting black holes but also for testing fundamental physics. Basing on such a method, we present an analysis of a dataset observed simultaneously by NuSTAR and NICER for the black hole binary candidate MAXI J1803-298, which shows prominent relativistic reflection features. Various relxill_nk flavors are utilized to test the Kerr black hole hypothesis. The results obtained from our analysis provide stringent constraints on Johannsen deformation parameter $\\alpha_{13}$ with the highest precise to date, namely $\\alpha_{13}=0.023^{+0.071}_{-0.038}$ from relxillD_nk and $\\alpha_{13}=0.006^{+0.045}_{-0.022}$ from relxillion_nk respectively in 3-$\\sigma$ credible lever, where Johannsen metric reduces to Kerr metric when $\\alpha_{13}$ vanishes. Furthermore, we investigate the best model-fit results using Akaike Information Criterion and assess its systematic uncertainties.","sentences":["Iron line spectroscopy has been one of the leading methods not only for measuring the spins of accreting black holes but also for testing fundamental physics.","Basing on such a method, we present an analysis of a dataset observed simultaneously by NuSTAR and NICER for the black hole binary candidate MAXI J1803-298, which shows prominent relativistic reflection features.","Various relxill_nk flavors are utilized to test the Kerr black hole hypothesis.","The results obtained from our analysis provide stringent constraints on Johannsen deformation parameter $\\alpha_{13}$ with the highest precise to date, namely $\\alpha_{13}=0.023^{+0.071}_{-0.038}$ from relxillD_nk and $\\alpha_{13}=0.006^{+0.045}_{-0.022}$ from relxillion_nk respectively in 3-$\\sigma$ credible lever, where Johannsen metric reduces to Kerr metric when $\\alpha_{13}$ vanishes.","Furthermore, we investigate the best model-fit results using Akaike Information Criterion and assess its systematic uncertainties."],"url":"http://arxiv.org/abs/2404.06020v1","category":"astro-ph.HE"}
{"created":"2024-04-09 05:01:07","title":"Robust Advertisement Pricing","abstract":"We consider the robust pricing problem of an advertising platform that charges a producer for disclosing hard evidence of product quality to a consumer before trading. Multiple equilibria arise since consumer beliefs and producer's contingent advertisement purchases are interdependent. To tackle strategic uncertainty, the platform offers each producer's quality type a menu of disclosure-probability-and-price plans to maximize its revenue guaranteed across all equilibria. The optimal menus offer a continuum of plans with strictly increasing marginal prices for higher disclosure probabilities. Full disclosure is implemented in the unique equilibrium. All partial-disclosure plans, though off-path, preclude bad equilibrium play. This solution admits a tractable price function that suggests volume-based pricing can outperform click-based pricing when strategic uncertainty is accounted for. Moreover, the platform prioritizes attracting higher types into service and offers them higher rents despite symmetric information between the platform and the producer.","sentences":["We consider the robust pricing problem of an advertising platform that charges a producer for disclosing hard evidence of product quality to a consumer before trading.","Multiple equilibria arise since consumer beliefs and producer's contingent advertisement purchases are interdependent.","To tackle strategic uncertainty, the platform offers each producer's quality type a menu of disclosure-probability-and-price plans to maximize its revenue guaranteed across all equilibria.","The optimal menus offer a continuum of plans with strictly increasing marginal prices for higher disclosure probabilities.","Full disclosure is implemented in the unique equilibrium.","All partial-disclosure plans, though off-path, preclude bad equilibrium play.","This solution admits a tractable price function that suggests volume-based pricing can outperform click-based pricing when strategic uncertainty is accounted for.","Moreover, the platform prioritizes attracting higher types into service and offers them higher rents despite symmetric information between the platform and the producer."],"url":"http://arxiv.org/abs/2404.06019v1","category":"econ.TH"}
{"created":"2024-04-09 04:45:18","title":"Feel-Good Thompson Sampling for Contextual Dueling Bandits","abstract":"Contextual dueling bandits, where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling bandits by incorporating contextual information for decision-making and preference learning. Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling bandits. However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual bandits. In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling bandits. At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling bandits. This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis. We show that our algorithm achieves nearly minimax-optimal regret, i.e., $\\tilde{\\mathcal{O}}(d\\sqrt T)$, where $d$ is the model dimension and $T$ is the time horizon. Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin.","sentences":["Contextual dueling bandits, where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling bandits by incorporating contextual information for decision-making and preference learning.","Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling bandits.","However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual bandits.","In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling bandits.","At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling bandits.","This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis.","We show that our algorithm achieves nearly minimax-optimal regret, i.e., $\\tilde{\\mathcal{O}}(d\\sqrt T)$, where $d$ is the model dimension and $T$ is the time horizon.","Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin."],"url":"http://arxiv.org/abs/2404.06013v1","category":"cs.LG"}
{"created":"2024-04-09 04:41:05","title":"Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data","abstract":"The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.","sentences":["The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics.","However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology.","In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion.","Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE).","Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds.","We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks.","Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks."],"url":"http://arxiv.org/abs/2404.06012v1","category":"cs.CV"}
{"created":"2024-04-09 04:11:25","title":"Privacy Preserving Prompt Engineering: A Survey","abstract":"Pre-trained language models (PLMs) have demonstrated significant proficiency in solving a wide range of general natural language processing (NLP) tasks. Researchers have observed a direct correlation between the performance of these models and their sizes. As a result, the sizes of these models have notably expanded in recent years, persuading researchers to adopt the term large language models (LLMs) to characterize the larger-sized PLMs. The increased size is accompanied by a distinct capability known as in-context learning (ICL), which represents a specialized form of prompting. This enables the utilization of LLMs for specific downstream tasks by presenting them with demonstration examples while keeping the model parameters frozen. Although interesting, privacy concerns have become a major obstacle in its widespread usage. Multiple studies have examined the privacy risks linked to ICL and prompting in general, and have devised techniques to alleviate these risks. Thus, there is a necessity to organize these mitigation techniques for the benefit of the community. This survey provides a systematic overview of the privacy protection methods employed during ICL and prompting in general. We review, analyze, and compare different methods under this paradigm. Furthermore, we provide a summary of the resources accessible for the development of these frameworks. Finally, we discuss the limitations of these frameworks and offer a detailed examination of the promising areas that necessitate further exploration.","sentences":["Pre-trained language models (PLMs) have demonstrated significant proficiency in solving a wide range of general natural language processing (NLP) tasks.","Researchers have observed a direct correlation between the performance of these models and their sizes.","As a result, the sizes of these models have notably expanded in recent years, persuading researchers to adopt the term large language models (LLMs) to characterize the larger-sized PLMs.","The increased size is accompanied by a distinct capability known as in-context learning (ICL), which represents a specialized form of prompting.","This enables the utilization of LLMs for specific downstream tasks by presenting them with demonstration examples while keeping the model parameters frozen.","Although interesting, privacy concerns have become a major obstacle in its widespread usage.","Multiple studies have examined the privacy risks linked to ICL and prompting in general, and have devised techniques to alleviate these risks.","Thus, there is a necessity to organize these mitigation techniques for the benefit of the community.","This survey provides a systematic overview of the privacy protection methods employed during ICL and prompting in general.","We review, analyze, and compare different methods under this paradigm.","Furthermore, we provide a summary of the resources accessible for the development of these frameworks.","Finally, we discuss the limitations of these frameworks and offer a detailed examination of the promising areas that necessitate further exploration."],"url":"http://arxiv.org/abs/2404.06001v1","category":"cs.CL"}
{"created":"2024-04-09 03:52:58","title":"Polynomial-time derivation of optimal k-tree topology from Markov networks","abstract":"Characterization of joint probability distribution for large networks of random variables remains a challenging task in data science. Probabilistic graph approximation with simple topologies has practically been resorted to; typically the tree topology makes joint probability computation much simpler and can be effective for statistical inference on insufficient data. However, to characterize network components where multiple variables cooperate closely to influence others, model topologies beyond a tree are needed, which unfortunately are infeasible to acquire. In particular, our previous work has related optimal approximation of Markov networks of tree-width k >=2 closely to the graph-theoretic problem of finding maximum spanning k-tree (MSkT), which is a provably intractable task.   This paper investigates optimal approximation of Markov networks with k-tree topology that retains some designated underlying subgraph. Such a subgraph may encode certain background information that arises in scientific applications, for example, about a known significant pathway in gene networks or the indispensable backbone connectivity in the residue interaction graphs for a biomolecule 3D structure. In particular, it is proved that the \\beta-retaining MSkT problem, for a number of classes \\beta of graphs, admit O(n^{k+1})-time algorithms for every fixed k>= 1. These \\beta-retaining MSkT algorithms offer efficient solutions for approximation of Markov networks with k-tree topology in the situation where certain persistent information needs to be retained.","sentences":["Characterization of joint probability distribution for large networks of random variables remains a challenging task in data science.","Probabilistic graph approximation with simple topologies has practically been resorted to; typically the tree topology makes joint probability computation much simpler and can be effective for statistical inference on insufficient data.","However, to characterize network components where multiple variables cooperate closely to influence others, model topologies beyond a tree are needed, which unfortunately are infeasible to acquire.","In particular, our previous work has related optimal approximation of Markov networks of tree-width k >=2 closely to the graph-theoretic problem of finding maximum spanning k-tree (MSkT), which is a provably intractable task.   ","This paper investigates optimal approximation of Markov networks with k-tree topology that retains some designated underlying subgraph.","Such a subgraph may encode certain background information that arises in scientific applications, for example, about a known significant pathway in gene networks or the indispensable backbone connectivity in the residue interaction graphs for a biomolecule 3D structure.","In particular, it is proved that the \\beta-retaining MSkT problem, for a number of classes \\beta of graphs, admit O(n^{k+1})-time algorithms for every fixed k>= 1.","These \\beta-retaining MSkT algorithms offer efficient solutions for approximation of Markov networks with k-tree topology in the situation where certain persistent information needs to be retained."],"url":"http://arxiv.org/abs/2404.05991v1","category":"cs.DS"}
{"created":"2024-04-09 02:32:14","title":"Constraining a disformal Schwarzschild black hole in DHOST theories with the orbit of the S2 star","abstract":"With the observed data of the S2 orbit around the black hole Sgr A$^*$ and the Markov Chain Monte Carlo method, we make a constraint on parameters of a disformal Schwarzschild black hole in quadratic degenerate higher-order scalartensor (DHOST) theories. This black hole belongs to a class of non-stealth solutions and owns an extra disformal parameter described the deviation from general relativity. Our results show that the best fit value of the disformal parameter is positive. However, in the range of $1\\sigma$, we also find that general relativity remains to be consistent with the observation of the S2 orbit.","sentences":["With the observed data of the S2 orbit around the black hole Sgr A$^*$ and the Markov Chain Monte Carlo method, we make a constraint on parameters of a disformal Schwarzschild black hole in quadratic degenerate higher-order scalartensor (DHOST) theories.","This black hole belongs to a class of non-stealth solutions and owns an extra disformal parameter described the deviation from general relativity.","Our results show that the best fit value of the disformal parameter is positive.","However, in the range of $1\\sigma$, we also find that general relativity remains to be consistent with the observation of the S2 orbit."],"url":"http://arxiv.org/abs/2404.05957v1","category":"gr-qc"}
{"created":"2024-04-09 02:29:55","title":"Distributed Tikhonov regularization for ill-posed inverse problems from a Bayesian perspective","abstract":"We exploit the similarities between Tikhonov regularization and Bayesian hierarchical models to propose a regularization scheme that acts like a distributed Tikhonov regularization where the amount of regularization varies from component to component. In the standard formulation, Tikhonov regularization compensates for the inherent ill-conditioning of linear inverse problems by augmenting the data fidelity term measuring the mismatch between the data and the model output with a scaled penalty functional. The selection of the scaling is the core problem in Tikhonov regularization. If an estimate of the amount of noise in the data is available, a popular way is to use the Morozov discrepancy principle, stating that the scaling parameter should be chosen so as to guarantee that the norm of the data fitting error is approximately equal to the norm of the noise in the data. A too small value of the regularization parameter would yield a solution that fits to the noise while a too large value would lead to an excessive penalization of the solution. In many applications, it would be preferable to apply distributed regularization, replacing the regularization scalar by a vector valued parameter, allowing different regularization for different components of the unknown, or for groups of them. A distributed Tikhonov-inspired regularization is particularly well suited when the data have significantly different sensitivity to different components, or to promote sparsity of the solution. The numerical scheme that we propose, while exploiting the Bayesian interpretation of the inverse problem and identifying the Tikhonov regularization with the Maximum A Posteriori (MAP) estimation, requires no statistical tools. A combination of numerical linear algebra and optimization tools makes the scheme computationally efficient and suitable for problems where the matrix is not explicitly available.","sentences":["We exploit the similarities between Tikhonov regularization and Bayesian hierarchical models to propose a regularization scheme that acts like a distributed Tikhonov regularization where the amount of regularization varies from component to component.","In the standard formulation, Tikhonov regularization compensates for the inherent ill-conditioning of linear inverse problems by augmenting the data fidelity term measuring the mismatch between the data and the model output with a scaled penalty functional.","The selection of the scaling is the core problem in Tikhonov regularization.","If an estimate of the amount of noise in the data is available, a popular way is to use the Morozov discrepancy principle, stating that the scaling parameter should be chosen so as to guarantee that the norm of the data fitting error is approximately equal to the norm of the noise in the data.","A too small value of the regularization parameter would yield a solution that fits to the noise while a too large value would lead to an excessive penalization of the solution.","In many applications, it would be preferable to apply distributed regularization, replacing the regularization scalar by a vector valued parameter, allowing different regularization for different components of the unknown, or for groups of them.","A distributed Tikhonov-inspired regularization is particularly well suited when the data have significantly different sensitivity to different components, or to promote sparsity of the solution.","The numerical scheme that we propose, while exploiting the Bayesian interpretation of the inverse problem and identifying the Tikhonov regularization with the Maximum A Posteriori (MAP) estimation, requires no statistical tools.","A combination of numerical linear algebra and optimization tools makes the scheme computationally efficient and suitable for problems where the matrix is not explicitly available."],"url":"http://arxiv.org/abs/2404.05956v1","category":"math.NA"}
{"created":"2024-04-09 01:57:08","title":"On the robustness of double-word addition algorithms","abstract":"We demonstrate that, even when there are moderate overlaps in the inputs of sloppy or accurate double-word addition algorithms in the QD library, these algorithms still guarantee error bounds of $O(u^2(|a|+|b|))$ in faithful rounding. Furthermore, under certain additional conditions, the accurate algorithm can achieve a relative error bound of $O(u^2)$ in the presence of moderate overlaps in the inputs in faithful rounding. Consequently, in double-word multiplication and addition operations, we can safely omit the normalization step of double-word multiplication and replace the accurate addition algorithm with the sloppy one. Numerical experiments confirm that this approach nearly doubles the performance of double-word multiplication and addition operations, with negligible precision costs. Moreover, in directed rounding mode, the signs of the errors of the two algorithms are consistent with the rounding direction, even in the presence of input overlap. This allows us to avoid changing the rounding mode in interval arithmetic. We also prove that the relative error bound of the sloppy addition algorithm exceeds $3u^2$ if and only if the input meets the condition of Sterbenz's Lemma when rounding to nearest. These findings suggest that the two addition algorithms are more robust than previously believed.","sentences":["We demonstrate that, even when there are moderate overlaps in the inputs of sloppy or accurate double-word addition algorithms in the QD library, these algorithms still guarantee error bounds of $O(u^2(|a|+|b|))$ in faithful rounding.","Furthermore, under certain additional conditions, the accurate algorithm can achieve a relative error bound of $O(u^2)$ in the presence of moderate overlaps in the inputs in faithful rounding.","Consequently, in double-word multiplication and addition operations, we can safely omit the normalization step of double-word multiplication and replace the accurate addition algorithm with the sloppy one.","Numerical experiments confirm that this approach nearly doubles the performance of double-word multiplication and addition operations, with negligible precision costs.","Moreover, in directed rounding mode, the signs of the errors of the two algorithms are consistent with the rounding direction, even in the presence of input overlap.","This allows us to avoid changing the rounding mode in interval arithmetic.","We also prove that the relative error bound of the sloppy addition algorithm exceeds $3u^2$ if and only if the input meets the condition of Sterbenz's Lemma when rounding to nearest.","These findings suggest that the two addition algorithms are more robust than previously believed."],"url":"http://arxiv.org/abs/2404.05948v1","category":"math.NA"}
{"created":"2024-04-09 01:43:02","title":"Neural networks can be FLOP-efficient integrators of 1D oscillatory integrands","abstract":"We demonstrate that neural networks can be FLOP-efficient integrators of one-dimensional oscillatory integrands. We train a feed-forward neural network to compute integrals of highly oscillatory 1D functions. The training set is a parametric combination of functions with varying characters and oscillatory behavior degrees. Numerical examples show that these networks are FLOP-efficient for sufficiently oscillatory integrands with an average FLOP gain of 1000 FLOPs. The network calculates oscillatory integrals better than traditional quadrature methods under the same computational budget or number of floating point operations. We find that feed-forward networks of 5 hidden layers are satisfactory for a relative accuracy of 0.001. The computational burden of inference of the neural network is relatively small, even compared to inner-product pattern quadrature rules. We postulate that our result follows from learning latent patterns in the oscillatory integrands that are otherwise opaque to traditional numerical integrators.","sentences":["We demonstrate that neural networks can be FLOP-efficient integrators of one-dimensional oscillatory integrands.","We train a feed-forward neural network to compute integrals of highly oscillatory 1D functions.","The training set is a parametric combination of functions with varying characters and oscillatory behavior degrees.","Numerical examples show that these networks are FLOP-efficient for sufficiently oscillatory integrands with an average FLOP gain of 1000 FLOPs.","The network calculates oscillatory integrals better than traditional quadrature methods under the same computational budget or number of floating point operations.","We find that feed-forward networks of 5 hidden layers are satisfactory for a relative accuracy of 0.001.","The computational burden of inference of the neural network is relatively small, even compared to inner-product pattern quadrature rules.","We postulate that our result follows from learning latent patterns in the oscillatory integrands that are otherwise opaque to traditional numerical integrators."],"url":"http://arxiv.org/abs/2404.05938v1","category":"cs.LG"}
{"created":"2024-04-09 01:14:15","title":"Remnant masses from 1D+ core-collapse supernovae simulations: bimodal neutron star mass distribution and black holes in the low-mass gap","abstract":"The explosion of core-collapse supernovae (CCSNe) is an extremely challenging problem, and there are still large uncertainties regarding which stars lead to successful explosions that leave behind a neutron star, and which ones will form a black hole instead. In this paper, we simulate 341 progenitors at three different metallicities using spherically symmetric simulations that include neutrino-driven convection via a mixing-length theory. We use these simulations to improve previously derived explosion criteria based on the density and entropy profiles of the pre-supernova progenitor. We also provide numerical fits to calculate the final mass of neutron stars based on either compactness, the location of the Si/Si-O interface, or the Chandrasekhar mass. The neutron star birth mass distribution derived from our 1D+ simulations is bimodal, contrary to what the most popular 1D CCSN simulations have shown so far. We compare the theoretically derived neutron star mass distributions with the observed ones and discuss potential implications for population synthesis studies. We also analyze the black hole mass distribution predicted by our simulations. To be consistent with current models of matter ejection in failed SNe, a large fraction of the envelope must be expelled, leading to small black holes in the low-mass gap. One black hole in this mass region has recently been observed in the GW230529 event by the LIGO-Virgo-KAGRA collaboration. Our results naturally agree with this detection, which the most popular prescriptions for explodability and remnant masses are not able to reproduce. In general, we find that the explosion outcome and mass of the remnant strongly depend on the pre-collapse structure of the progenitor. However, their dependence on the initial mass of the star and the mass of the CO core is highly uncertain and non-linear.","sentences":["The explosion of core-collapse supernovae (CCSNe) is an extremely challenging problem, and there are still large uncertainties regarding which stars lead to successful explosions that leave behind a neutron star, and which ones will form a black hole instead.","In this paper, we simulate 341 progenitors at three different metallicities using spherically symmetric simulations that include neutrino-driven convection via a mixing-length theory.","We use these simulations to improve previously derived explosion criteria based on the density and entropy profiles of the pre-supernova progenitor.","We also provide numerical fits to calculate the final mass of neutron stars based on either compactness, the location of the Si/Si-O interface, or the Chandrasekhar mass.","The neutron star birth mass distribution derived from our 1D+ simulations is bimodal, contrary to what the most popular 1D CCSN simulations have shown so far.","We compare the theoretically derived neutron star mass distributions with the observed ones and discuss potential implications for population synthesis studies.","We also analyze the black hole mass distribution predicted by our simulations.","To be consistent with current models of matter ejection in failed SNe, a large fraction of the envelope must be expelled, leading to small black holes in the low-mass gap.","One black hole in this mass region has recently been observed in the GW230529 event by the LIGO-Virgo-KAGRA collaboration.","Our results naturally agree with this detection, which the most popular prescriptions for explodability and remnant masses are not able to reproduce.","In general, we find that the explosion outcome and mass of the remnant strongly depend on the pre-collapse structure of the progenitor.","However, their dependence on the initial mass of the star and the mass of the CO core is highly uncertain and non-linear."],"url":"http://arxiv.org/abs/2404.05927v1","category":"astro-ph.HE"}
{"created":"2024-04-09 01:12:40","title":"Cohen-Macaulay representations of Artin-Schelter Gorenstein algebras of dimension one","abstract":"The existence of a tilting or silting object is an important feature for an algebraic triangulated category since it gives an equivalence with the derived category of a ring. By applying tilting theory, we study Cohen-Macaulay representations of $\\mathbb N$-graded Artin-Schelter Gorenstein algebras $A=\\bigoplus_{i\\in \\mathbb N}A_i$, where do not assume that $A_0$ is a field. This is a large class of noncommutative Gorenstein rings containing Gorenstein orders. In this paper, we concentrate on the case where $A$ has dimension one. Under the assumptions that $A$ is ring-indecomposable and $A_0$ has finite global dimension, we show that the stable category $\\operatorname{\\underline{\\mathsf{CM}}}_0^{\\mathbb Z}A$ always admits an (explicitly constructed) silting object. We also show that $\\operatorname{\\underline{\\mathsf{CM}}}_0^{\\mathbb Z}A$ admits a tilting object if and only if either $A$ is Artin-Schelter regular or the average Gorenstein parameter $p^A_\\mathrm{av} \\in {\\mathbb Q}$ of $A$ is non-positive. These results are far-reaching generalizations of the results of Buchweitz, Iyama, and Yamaura. We give two different proofs of the second result; one is based on Orlov-type semiorthogonal decompositions, and the other is based on a more direct calculation. We apply our results to a Gorenstein tiled order $A$ to prove that $\\operatorname{\\underline{\\mathsf{CM}}}^{\\mathbb Z}A$ is equivalent to the derived category of the incidence algebra of an (explicitly constructed) poset. We also apply our results and Koszul duality to prove that the derived category $\\mathsf{D}^\\mathrm{b}(\\operatorname{\\mathsf{qgr}} A)$ of a smooth noncommutative projective quadric hypersurface $\\operatorname{\\mathsf{qgr}} A$ admits an (explicitly constructed) tilting object, which contains the tilting object of $\\operatorname{\\underline{\\mathsf{CM}}}^{\\mathbb Z}A$ due to Smith and Van den Bergh as a direct summand.","sentences":["The existence of a tilting or silting object is an important feature for an algebraic triangulated category since it gives an equivalence with the derived category of a ring.","By applying tilting theory, we study Cohen-Macaulay representations of $\\mathbb N$-graded Artin-Schelter Gorenstein algebras $A=\\bigoplus_{i\\in \\mathbb N}A_i$, where do not assume that $A_0$ is a field.","This is a large class of noncommutative Gorenstein rings containing Gorenstein orders.","In this paper, we concentrate on the case where $A$ has dimension one.","Under the assumptions that $A$ is ring-indecomposable and $A_0$ has finite global dimension, we show that the stable category $\\operatorname{\\underline{\\mathsf{CM}}}_0^{\\mathbb Z}A$ always admits an (explicitly constructed) silting object.","We also show that $\\operatorname{\\underline{\\mathsf{CM}}}_0^{\\mathbb Z}A$ admits a tilting object if and only if either $A$ is Artin-Schelter regular or the average Gorenstein parameter $p^A_\\mathrm{av} \\in {\\mathbb Q}$ of $A$ is non-positive.","These results are far-reaching generalizations of the results of Buchweitz, Iyama, and Yamaura.","We give two different proofs of the second result; one is based on Orlov-type semiorthogonal decompositions, and the other is based on a more direct calculation.","We apply our results to a Gorenstein tiled order $A$ to prove that $\\operatorname{\\underline{\\mathsf{CM}}}^{\\mathbb Z}A$ is equivalent to the derived category of the incidence algebra of an (explicitly constructed) poset.","We also apply our results and Koszul duality to prove that the derived category $\\mathsf{D}^\\mathrm{b}(\\operatorname{\\mathsf{qgr}} A)$ of a smooth noncommutative projective quadric hypersurface $\\operatorname{\\mathsf{qgr}} A$ admits an (explicitly constructed) tilting object, which contains the tilting object of $\\operatorname{\\underline{\\mathsf{CM}}}^{\\mathbb Z}A$ due to Smith and Van den Bergh as a direct summand."],"url":"http://arxiv.org/abs/2404.05925v1","category":"math.RT"}
{"created":"2024-04-09 00:26:05","title":"Seebeck Effect of Dirac Electrons in Organic Conductors under Hydrostatic Pressure Using a Tight-Binding Model Derived from First Principles","abstract":"The Seebeck coefficient is examined for two-dimensional Dirac electrons in the three-quarter filled organic conductor alpha-(BEDT-TTF)_2I_3 under hydrostatic pressure, where the Seebeck coefficient is proportional to the ratio of the thermoelectric conductivity to the electrical conductivity. We present an improved tight-binding model in two dimensions with transfer energies determined from first-principles density functional theory calculations with an experimentally determined crystal structure. The temperatutre dependence of the Seebeck coefficient is calculated by adding impurity and electron-phonon scatterings. Noting a zero-gap state due to the Dirac cone, which results in a competition from contributions between the conduction and valence bands, we show positive S_x and S_y at finite temperatures and analyze them in terms of spectral conductivity. The relevance of the calculated S_x (perpendicular to the molecular stacking axis) to the experiment is discussed.","sentences":["The Seebeck coefficient is examined for two-dimensional Dirac electrons in the three-quarter filled organic conductor alpha-(BEDT-TTF)_2I_3 under hydrostatic pressure, where the Seebeck coefficient is proportional to the ratio of the thermoelectric conductivity to the electrical conductivity.","We present an improved tight-binding model in two dimensions with transfer energies determined from first-principles density functional theory calculations with an experimentally determined crystal structure.","The temperatutre dependence of the Seebeck coefficient is calculated by adding impurity and electron-phonon scatterings.","Noting a zero-gap state due to the Dirac cone, which results in a competition from contributions between the conduction and valence bands, we show positive S_x and S_y at finite temperatures and analyze them in terms of spectral conductivity.","The relevance of the calculated S_x (perpendicular to the molecular stacking axis) to the experiment is discussed."],"url":"http://arxiv.org/abs/2404.05914v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 23:47:26","title":"Minimum variance threshold for epsilon-lexicase selection","abstract":"Parent selection plays an important role in evolutionary algorithms, and many strategies exist to select the parent pool before breeding the next generation. Methods often rely on average error over the entire dataset as a criterion to select the parents, which can lead to an information loss due to aggregation of all test cases. Under epsilon-lexicase selection, the population goes to a selection pool that is iteratively reduced by using each test individually, discarding individuals with an error higher than the elite error plus the median absolute deviation (MAD) of errors for that particular test case. In an attempt to better capture differences in performance of individuals on cases, we propose a new criteria that splits errors into two partitions that minimize the total variance within partitions. Our method was embedded into the FEAT symbolic regression algorithm, and evaluated with the SRBench framework, containing 122 black-box synthetic and real-world regression problems. The empirical results show a better performance of our approach compared to traditional epsilon-lexicase selection in the real-world datasets while showing equivalent performance on the synthetic dataset.","sentences":["Parent selection plays an important role in evolutionary algorithms, and many strategies exist to select the parent pool before breeding the next generation.","Methods often rely on average error over the entire dataset as a criterion to select the parents, which can lead to an information loss due to aggregation of all test cases.","Under epsilon-lexicase selection, the population goes to a selection pool that is iteratively reduced by using each test individually, discarding individuals with an error higher than the elite error plus the median absolute deviation (MAD) of errors for that particular test case.","In an attempt to better capture differences in performance of individuals on cases, we propose a new criteria that splits errors into two partitions that minimize the total variance within partitions.","Our method was embedded into the FEAT symbolic regression algorithm, and evaluated with the SRBench framework, containing 122 black-box synthetic and real-world regression problems.","The empirical results show a better performance of our approach compared to traditional epsilon-lexicase selection in the real-world datasets while showing equivalent performance on the synthetic dataset."],"url":"http://arxiv.org/abs/2404.05909v1","category":"cs.NE"}
{"created":"2024-04-08 23:00:06","title":"Distributionally Robust Optimization with Decision-Dependent Information Discovery","abstract":"We study two-stage distributionally robust optimization (DRO) problems with decision-dependent information discovery (DDID) wherein (a portion of) the uncertain parameters are revealed only if an (often costly) investment is made in the first stage. This class of problems finds many important applications in selection problems (e.g., in hiring, project portfolio optimization, or optimal sensor location). Despite the problem's wide applicability, it has not been previously studied. We propose a framework for modeling and approximately solving DRO problems with DDID. We formulate the problem as a min-max-min-max problem and adopt the popular K-adaptability approximation scheme, which chooses K candidate recourse actions here-and-now and implements the best of those actions after the uncertain parameters that were chosen to be observed are revealed. We then present a decomposition algorithm that solves the K-adaptable formulation exactly. In particular, we devise a cutting plane algorithm which iteratively solves a relaxed version of the problem, evaluates the true objective value of the corresponding solution, generates valid cuts, and imposes them in the relaxed problem. For the evaluation problem, we develop a branch-and-cut algorithm that provably converges to an optimal solution. We showcase the effectiveness of our framework on the R&D project portfolio optimization problem and the best box problem.","sentences":["We study two-stage distributionally robust optimization (DRO) problems with decision-dependent information discovery (DDID) wherein (a portion of) the uncertain parameters are revealed only if an (often costly) investment is made in the first stage.","This class of problems finds many important applications in selection problems (e.g., in hiring, project portfolio optimization, or optimal sensor location).","Despite the problem's wide applicability, it has not been previously studied.","We propose a framework for modeling and approximately solving DRO problems with DDID.","We formulate the problem as a min-max-min-max problem and adopt the popular K-adaptability approximation scheme, which chooses K candidate recourse actions here-and-now and implements the best of those actions after the uncertain parameters that were chosen to be observed are revealed.","We then present a decomposition algorithm that solves the K-adaptable formulation exactly.","In particular, we devise a cutting plane algorithm which iteratively solves a relaxed version of the problem, evaluates the true objective value of the corresponding solution, generates valid cuts, and imposes them in the relaxed problem.","For the evaluation problem, we develop a branch-and-cut algorithm that provably converges to an optimal solution.","We showcase the effectiveness of our framework on the R&D project portfolio optimization problem and the best box problem."],"url":"http://arxiv.org/abs/2404.05900v1","category":"math.OC"}
{"created":"2024-04-08 22:52:37","title":"ClusterRadar: an Interactive Web-Tool for the Multi-Method Exploration of Spatial Clusters Over Time","abstract":"Spatial cluster analysis, the detection of localized patterns of similarity in geospatial data, has a wide-range of applications for scientific discovery and practical decision making. One way to detect spatial clusters is by using local indicators of spatial association, such as Local Moran's I or Getis-Ord Gi*. However, different indicators tend to produce substantially different results due to their distinct operational characteristics. Choosing a suitable method or comparing results from multiple methods is a complex task. Furthermore, spatial clusters are dynamic and it is often useful to track their evolution over time, which adds an additional layer of complexity. ClusterRadar is a web-tool designed to address these analytical challenges. The tool allows users to easily perform spatial clustering and analyze the results in an interactive environment, uniquely prioritizing temporal analysis and the comparison of multiple methods. The tool's interactive dashboard presents several visualizations, each offering a distinct perspective of the temporal and methodological aspects of the spatial clustering results. ClusterRadar has several features designed to maximize its utility to a broad user-base, including support for various geospatial formats, and a fully in-browser execution environment to preserve the privacy of sensitive data. Feedback from a varied set of researchers suggests ClusterRadar's potential for enhancing the temporal analysis of spatial clusters.","sentences":["Spatial cluster analysis, the detection of localized patterns of similarity in geospatial data, has a wide-range of applications for scientific discovery and practical decision making.","One way to detect spatial clusters is by using local indicators of spatial association, such as Local Moran's I or Getis-Ord Gi*.","However, different indicators tend to produce substantially different results due to their distinct operational characteristics.","Choosing a suitable method or comparing results from multiple methods is a complex task.","Furthermore, spatial clusters are dynamic and it is often useful to track their evolution over time, which adds an additional layer of complexity.","ClusterRadar is a web-tool designed to address these analytical challenges.","The tool allows users to easily perform spatial clustering and analyze the results in an interactive environment, uniquely prioritizing temporal analysis and the comparison of multiple methods.","The tool's interactive dashboard presents several visualizations, each offering a distinct perspective of the temporal and methodological aspects of the spatial clustering results.","ClusterRadar has several features designed to maximize its utility to a broad user-base, including support for various geospatial formats, and a fully in-browser execution environment to preserve the privacy of sensitive data.","Feedback from a varied set of researchers suggests ClusterRadar's potential for enhancing the temporal analysis of spatial clusters."],"url":"http://arxiv.org/abs/2404.05897v1","category":"cs.HC"}
{"created":"2024-04-08 21:51:53","title":"Instability of quadratic band degeneracies and the emergence of Dirac points","abstract":"Consider the Schr\\\"{o}dinger operator $H = -\\Delta + V$, where the potential $V$ is $\\mathbb{Z}^2$-periodic and invariant under spatial inversion, complex conjugation, and $\\pi/2$ rotation. We show that, under typical small linear deformations of $V$, the quadratic band degeneracy points, occurring over the high-symmetry quasimomentum ${\\bf M}$ (see [24, 25]) each split into two separated degeneracies over perturbed quasimomenta ${\\bf D}^+$ and ${\\bf D}^-$, and that these degeneracies are Dirac points. The local character of the degenerate dispersion surfaces about the emergent Dirac points are tilted, elliptical cones. Correspondingly, the dynamics of wavepackets spectrally localized near either ${\\bf D}^+$ or ${\\bf D}^-$ are governed by a system of Dirac equations with an advection term. Generalizations are discussed.","sentences":["Consider the Schr\\\"{o}dinger operator $H = -\\Delta + V$, where the potential $V$ is $\\mathbb{Z}^2$-periodic and invariant under spatial inversion, complex conjugation, and $\\pi/2$ rotation.","We show that, under typical small linear deformations of $V$, the quadratic band degeneracy points, occurring over the high-symmetry quasimomentum ${\\bf M}$ (see [24, 25]) each split into two separated degeneracies over perturbed quasimomenta ${\\bf D}^+$ and ${\\bf D}^-$, and that these degeneracies are Dirac points.","The local character of the degenerate dispersion surfaces about the emergent Dirac points are tilted, elliptical cones.","Correspondingly, the dynamics of wavepackets spectrally localized near either ${\\bf D}^+$ or ${\\bf D}^-$ are governed by a system of Dirac equations with an advection term.","Generalizations are discussed."],"url":"http://arxiv.org/abs/2404.05886v1","category":"math-ph"}
{"created":"2024-04-08 21:26:04","title":"Rapid and Precise Topological Comparison with Merge Tree Neural Networks","abstract":"Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes. To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison. The MTNN enables rapid and high-quality similarity computation. We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison. Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism. We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets. Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency. In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%.","sentences":["Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes.","To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison.","The MTNN enables rapid and high-quality similarity computation.","We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison.","Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism.","We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets.","Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency.","In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%."],"url":"http://arxiv.org/abs/2404.05879v1","category":"cs.LG"}
{"created":"2024-04-08 21:19:10","title":"Privacy and Security of Women's Reproductive Health Apps in a Changing Legal Landscape","abstract":"FemTech, a rising trend in mobile apps, empowers women to digitally manage their health and family planning. However, privacy and security vulnerabilities in period-tracking and fertility-monitoring apps present significant risks, such as unintended pregnancies and legal consequences. Our approach involves manual observations of privacy policies and app permissions, along with dynamic and static analysis using multiple evaluation frameworks. Our research reveals that many of these apps gather personally identifiable information (PII) and sensitive healthcare data. Furthermore, our analysis identifies that 61% of the code vulnerabilities found in the apps are classified under the top-ten Open Web Application Security Project (OWASP) vulnerabilities. Our research emphasizes the significance of tackling the privacy and security vulnerabilities present in period-tracking and fertility-monitoring mobile apps. By highlighting these crucial risks, we aim to initiate a vital discussion and advocate for increased accountability and transparency of digital tools for women's health. We encourage the industry to prioritize user privacy and security, ultimately promoting a safer and more secure environment for women's health management.","sentences":["FemTech, a rising trend in mobile apps, empowers women to digitally manage their health and family planning.","However, privacy and security vulnerabilities in period-tracking and fertility-monitoring apps present significant risks, such as unintended pregnancies and legal consequences.","Our approach involves manual observations of privacy policies and app permissions, along with dynamic and static analysis using multiple evaluation frameworks.","Our research reveals that many of these apps gather personally identifiable information (PII) and sensitive healthcare data.","Furthermore, our analysis identifies that 61% of the code vulnerabilities found in the apps are classified under the top-ten Open Web Application Security Project (OWASP) vulnerabilities.","Our research emphasizes the significance of tackling the privacy and security vulnerabilities present in period-tracking and fertility-monitoring mobile apps.","By highlighting these crucial risks, we aim to initiate a vital discussion and advocate for increased accountability and transparency of digital tools for women's health.","We encourage the industry to prioritize user privacy and security, ultimately promoting a safer and more secure environment for women's health management."],"url":"http://arxiv.org/abs/2404.05876v1","category":"cs.CR"}
{"created":"2024-04-08 21:09:59","title":"TabConv: Low-Computation CNN Inference via Table Lookups","abstract":"Convolutional Neural Networks (CNNs) have demonstrated remarkable ability throughout the field of computer vision. However, CNN inference requires a large number of arithmetic operations, making them expensive to deploy in hardware. Current approaches alleviate this issue by developing hardware-supported, algorithmic processes to simplify spatial convolution functions. However, these methods still heavily rely on matrix multiplication, leading to significant computational overhead. To bridge the gap between hardware, algorithmic acceleration, and approximate matrix multiplication, we propose TabConv, a novel, table-based approximation for convolution to significantly reduce arithmetic operations during inference. Additionally, we introduce a priority masking technique based on cosine similarity to select layers for table-based approximation, thereby maintaining the model performance. We evaluate our approach on popular CNNs: ResNet-18, ResNet-34, and NetworkInNetwork (NIN). TabConv preserves over 93% of the original model's performance while reducing arithmetic operations by 36.5%, 25.8%, and 99.4% for ResNet-18 on CIFAR-10, CIFAR-100, and MNIST, respectively, 35.6% and 99.3% for ResNet-34 on CIFAR-10 and MNIST, and 98.9% for NIN on MNIST, achieving low-computation inference.","sentences":["Convolutional Neural Networks (CNNs) have demonstrated remarkable ability throughout the field of computer vision.","However, CNN inference requires a large number of arithmetic operations, making them expensive to deploy in hardware.","Current approaches alleviate this issue by developing hardware-supported, algorithmic processes to simplify spatial convolution functions.","However, these methods still heavily rely on matrix multiplication, leading to significant computational overhead.","To bridge the gap between hardware, algorithmic acceleration, and approximate matrix multiplication, we propose TabConv, a novel, table-based approximation for convolution to significantly reduce arithmetic operations during inference.","Additionally, we introduce a priority masking technique based on cosine similarity to select layers for table-based approximation, thereby maintaining the model performance.","We evaluate our approach on popular CNNs: ResNet-18, ResNet-34, and NetworkInNetwork (NIN).","TabConv preserves over 93% of the original model's performance while reducing arithmetic operations by 36.5%, 25.8%, and 99.4% for ResNet-18 on CIFAR-10, CIFAR-100, and MNIST, respectively, 35.6% and 99.3% for ResNet-34 on CIFAR-10 and MNIST, and 98.9% for NIN on MNIST, achieving low-computation inference."],"url":"http://arxiv.org/abs/2404.05872v1","category":"cs.CV"}
{"created":"2024-04-08 20:54:58","title":"Effectiveness of Self-Assessment Software to Evaluate Preclinical Operative Procedures","abstract":"Objectives: To assess the effectiveness of digital scanning techniques for self-assessment and of preparations and restorations in preclinical dental education when compared to traditional faculty grading. Methods: Forty-four separate Class I (#30-O), Class II (#30-MO) preparations, and class II amalgam restorations (#31-MO) were generated respectively under preclinical assessment setting. Calibrated faculty evaluated the preparations and restorations using a standard rubric from preclinical operative class. The same teeth were scanned using Planmeca PlanScan intraoral scanner and graded using the Romexis E4D Compare Software. Each tooth was compared against a corresponding gold standard tooth with tolerance intervals ranging from 100{\\mu}m to 500{\\mu}m. These scores were compared to traditional faculty grades using a linear mixed model to estimate the mean differences at 95% confidence interval for each tolerance level. Results: The average Compare Software grade of Class I preparation at 300{\\mu}m tolerance had the smallest mean difference of 1.64 points on a 100 points scale compared to the average faculty grade. Class II preparation at 400{\\mu}m tolerance had the smallest mean difference of 0.41 points. Finally, Class II Restoration at 300{\\mu}m tolerance had the smallest mean difference at 0.20 points. Conclusion: In this study, tolerance levels that best correlated the Compare Software grades with the faculty grades were determined for three operative procedures: class I preparation, class II preparation and class II restoration. This Compare Software can be used as a useful adjunct method for more objective grading. It also can be used by students as a great self-assessment tool.","sentences":["Objectives: To assess the effectiveness of digital scanning techniques for self-assessment and of preparations and restorations in preclinical dental education when compared to traditional faculty grading.","Methods: Forty-four separate Class I (#30-O), Class II (#30-MO) preparations, and class II amalgam restorations (#31-MO) were generated respectively under preclinical assessment setting.","Calibrated faculty evaluated the preparations and restorations using a standard rubric from preclinical operative class.","The same teeth were scanned using Planmeca PlanScan intraoral scanner and graded using the Romexis E4D Compare Software.","Each tooth was compared against a corresponding gold standard tooth with tolerance intervals ranging from 100{\\mu}m to 500{\\mu}m.","These scores were compared to traditional faculty grades using a linear mixed model to estimate the mean differences at 95% confidence interval for each tolerance level.","Results:","The average Compare Software grade of Class I preparation at 300{\\mu}m tolerance had the smallest mean difference of 1.64 points on a 100 points scale compared to the average faculty grade.","Class II preparation at 400{\\mu}m tolerance had the smallest mean difference of 0.41 points.","Finally, Class II Restoration at 300{\\mu}m tolerance had the smallest mean difference at 0.20 points.","Conclusion:","In this study, tolerance levels that best correlated the Compare Software grades with the faculty grades were determined for three operative procedures: class","I preparation, class II preparation and class II restoration.","This Compare Software can be used as a useful adjunct method for more objective grading.","It also can be used by students as a great self-assessment tool."],"url":"http://arxiv.org/abs/2404.05865v1","category":"q-bio.OT"}
{"created":"2024-04-08 20:36:11","title":"Quantum Fault Trees and Minimal Cut Sets Identification","abstract":"Fault Trees represent an essential tool in the reliability and risk assessment of engineering systems. By decomposing the structure of the system into Boolean function, Fault Trees allow the quantitative and qualitative analysis of the system. One of the main important tasks in Fault Tree analysis is the identification of Minimal Cut Sets, defined as groups of components that present the least path of resistance toward a system's failure. Identifying them allows reliability engineers to enhance the reliability and safety of the system, making system failures less likely to occur. However, the minimal cut set identification problem is challenging to solve, due to the exponential growth experienced in the number of feasible configurations as the system's size grows linearly. Over the last few years, quantum computation has been heralded as a promising tool to tackle computational challenges of increased complexity. The reason for this is the promising prospects that the use of quantum effects has for challenging computational tasks. However, its application into Probabilistic Risk Assessment and reliability engineering, and in particular to challenges related to the Fault Tree model, is still uncharted territory. To fill this gap, the objective of the paper is to integrate quantum computation into the Fault Tree Model and present an assessment of their capabilities for the minimal cut set identification problem. To this end, this paper proposes a novel algorithm to encode a fault tree into a quantum computer and to perform the identification of minimal cut sets with increased efficiency via the application of the Quantum Amplitude Amplification protocol. For validation purposes, a series of theoretical and numerical results, the latter obtained using a quantum simulator, are presented in which the proposed algorithm is compared against traditional approaches, such as Monte Carlo sampling.","sentences":["Fault Trees represent an essential tool in the reliability and risk assessment of engineering systems.","By decomposing the structure of the system into Boolean function, Fault Trees allow the quantitative and qualitative analysis of the system.","One of the main important tasks in Fault Tree analysis is the identification of Minimal Cut Sets, defined as groups of components that present the least path of resistance toward a system's failure.","Identifying them allows reliability engineers to enhance the reliability and safety of the system, making system failures less likely to occur.","However, the minimal cut set identification problem is challenging to solve, due to the exponential growth experienced in the number of feasible configurations as the system's size grows linearly.","Over the last few years, quantum computation has been heralded as a promising tool to tackle computational challenges of increased complexity.","The reason for this is the promising prospects that the use of quantum effects has for challenging computational tasks.","However, its application into Probabilistic Risk Assessment and reliability engineering, and in particular to challenges related to the Fault Tree model, is still uncharted territory.","To fill this gap, the objective of the paper is to integrate quantum computation into the Fault Tree Model and present an assessment of their capabilities for the minimal cut set identification problem.","To this end, this paper proposes a novel algorithm to encode a fault tree into a quantum computer and to perform the identification of minimal cut sets with increased efficiency via the application of the Quantum Amplitude Amplification protocol.","For validation purposes, a series of theoretical and numerical results, the latter obtained using a quantum simulator, are presented in which the proposed algorithm is compared against traditional approaches, such as Monte Carlo sampling."],"url":"http://arxiv.org/abs/2404.05853v1","category":"stat.CO"}
{"created":"2024-04-08 20:32:21","title":"Entanglement entropy in lattices with non-abelian gauge groups","abstract":"Entanglement entropy, taken here to be geometric, requires a geometrically separable Hilbert space. In lattice gauge theories, it is not immediately clear if the physical Hilbert space is geometrically separable. In a previous paper we have shown that the physical Hilbert space in pure gauge abelian lattice theories exhibits some form of geometric scaling with the lattice volume, which suggest that the space is locally factorizable and, therefore, geometrically separable. In this paper, we provide strong evidence that indicates that this scaling is not present when the group is non-abelian. We do so by looking at the scaling of the dimension of the physical Hilbert space of theories with certain discrete groups. The lack of an appropriate scaling implies that the physical Hilbert space of such a theory does not admit a local factorization. We then extend the reasoning, as sensibly possible, to SU(2) and SU(N) to reach the same conclusion. Lastly, we show that the addition of matter fields to non-abelian lattice gauge theories makes the resulting physical Hilbert space locally factorizable.","sentences":["Entanglement entropy, taken here to be geometric, requires a geometrically separable Hilbert space.","In lattice gauge theories, it is not immediately clear if the physical Hilbert space is geometrically separable.","In a previous paper we have shown that the physical Hilbert space in pure gauge abelian lattice theories exhibits some form of geometric scaling with the lattice volume, which suggest that the space is locally factorizable and, therefore, geometrically separable.","In this paper, we provide strong evidence that indicates that this scaling is not present when the group is non-abelian.","We do so by looking at the scaling of the dimension of the physical Hilbert space of theories with certain discrete groups.","The lack of an appropriate scaling implies that the physical Hilbert space of such a theory does not admit a local factorization.","We then extend the reasoning, as sensibly possible, to SU(2) and SU(N) to reach the same conclusion.","Lastly, we show that the addition of matter fields to non-abelian lattice gauge theories makes the resulting physical Hilbert space locally factorizable."],"url":"http://arxiv.org/abs/2404.05851v1","category":"hep-th"}
{"created":"2024-04-08 20:31:41","title":"Witnessing Quantum Entanglement Using Resonant Inelastic X-ray Scattering","abstract":"Although entanglement is both a central ingredient in our understanding of quantum many-body systems and an essential resource for quantum technologies, we only have a limited ability to quantify entanglement in real quantum materials. Thus far, entanglement metrology in quantum materials has been limited to measurements involving Hermitian operators, such as the detection of spin entanglement using inelastic neutron scattering. Here, we devise a method to extract the quantum Fisher information (QFI) from non-Hermitian operators and formulate an entanglement witness for resonant inelastic x-ray scattering (RIXS). Our approach is then applied to the model iridate dimer system Ba$_3$CeIr$_2$O$_9$ and used to directly test for entanglement of the electronic orbitals between neighboring Ir sites. We find that entanglement is challenging to detect under standard conditions, but that it could be achieved by analyzing the outgoing x-ray polarization or via specific choices of momentum and energy. Our protocol provides a new handle for entanglement detection, which offers routes to related types of entanglement witness (such as orbitally-resolved measurements) and to the generalization to out-of-equilibrium settings accessed in ultrafast settings.","sentences":["Although entanglement is both a central ingredient in our understanding of quantum many-body systems and an essential resource for quantum technologies, we only have a limited ability to quantify entanglement in real quantum materials.","Thus far, entanglement metrology in quantum materials has been limited to measurements involving Hermitian operators, such as the detection of spin entanglement using inelastic neutron scattering.","Here, we devise a method to extract the quantum Fisher information (QFI) from non-Hermitian operators and formulate an entanglement witness for resonant inelastic x-ray scattering (RIXS).","Our approach is then applied to the model iridate dimer system Ba$_3$CeIr$_2$O$_9$ and used to directly test for entanglement of the electronic orbitals between neighboring Ir sites.","We find that entanglement is challenging to detect under standard conditions, but that it could be achieved by analyzing the outgoing x-ray polarization or via specific choices of momentum and energy.","Our protocol provides a new handle for entanglement detection, which offers routes to related types of entanglement witness (such as orbitally-resolved measurements) and to the generalization to out-of-equilibrium settings accessed in ultrafast settings."],"url":"http://arxiv.org/abs/2404.05850v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 20:15:38","title":"Estimate of the time required to perform a nonadiabatic holonomic quantum computation","abstract":"Nonadiabatic holonomic quantum computation has been proposed as a method for implementing quantum logic gates with the same robustness as adiabatic holonomic quantum computation but with shorter execution times. In this paper, we establish an isoholonomic inequality for quantum gates, which provides a lower bound on the lengths of cyclic transformations of the computational space that generate a specific gate. Then, as a corollary, we derive a nonadiabatic execution time estimate for holonomic gates. In addition, we demonstrate that under certain dimensional conditions, the isoholonomic inequality is tight in the sense that every gate on the computational space can be implemented holonomically and unitarily in a time-optimal way. We illustrate the results by showing that the procedures for implementing a universal set of holonomic gates proposed in a pioneering paper on nonadiabatic holonomic quantum computation saturate the isoholonomic inequality and are thus time-optimal.","sentences":["Nonadiabatic holonomic quantum computation has been proposed as a method for implementing quantum logic gates with the same robustness as adiabatic holonomic quantum computation but with shorter execution times.","In this paper, we establish an isoholonomic inequality for quantum gates, which provides a lower bound on the lengths of cyclic transformations of the computational space that generate a specific gate.","Then, as a corollary, we derive a nonadiabatic execution time estimate for holonomic gates.","In addition, we demonstrate that under certain dimensional conditions, the isoholonomic inequality is tight in the sense that every gate on the computational space can be implemented holonomically and unitarily in a time-optimal way.","We illustrate the results by showing that the procedures for implementing a universal set of holonomic gates proposed in a pioneering paper on nonadiabatic holonomic quantum computation saturate the isoholonomic inequality and are thus time-optimal."],"url":"http://arxiv.org/abs/2404.05844v1","category":"quant-ph"}
{"created":"2024-04-08 19:48:44","title":"Spin high-harmonic generation through terahertz laser-driven phonons","abstract":"In the realm of open quantum systems, steady states and high-harmonic generation (HHG) existing far from equilibrium have become core pillars of ultrafast science. Most solid-state research explores charge HHG with limited investigations into spin degrees of freedom. In this study, we theoretically address spin HHG in the steady state resulting from the terahertz laser-driven spin-phonon coupling in a dissipative dimerized spin-1/2 chain. Instead of directly driving spins using time-dependent magnetic fields, we employ the magnetophononic mechanism, where the laser first drives the lattice, and then the excited lattice subsequently drives the spins. We investigate the role of various model parameters for optimizing HHG. Increasing the laser's amplitude amplifies spin HHG beyond the perturbative regime, enhancing both harmonic amplitudes and orders. We find that configuring the drive frequency far below the spin band yields the highest harmonic order. Additionally, we provide a theory matching the numerical results under weak spin-phonon coupling and propose an experimental procedure to probe the emission spectrum of spin HHG.","sentences":["In the realm of open quantum systems, steady states and high-harmonic generation (HHG) existing far from equilibrium have become core pillars of ultrafast science.","Most solid-state research explores charge HHG with limited investigations into spin degrees of freedom.","In this study, we theoretically address spin HHG in the steady state resulting from the terahertz laser-driven spin-phonon coupling in a dissipative dimerized spin-1/2 chain.","Instead of directly driving spins using time-dependent magnetic fields, we employ the magnetophononic mechanism, where the laser first drives the lattice, and then the excited lattice subsequently drives the spins.","We investigate the role of various model parameters for optimizing HHG.","Increasing the laser's amplitude amplifies spin HHG beyond the perturbative regime, enhancing both harmonic amplitudes and orders.","We find that configuring the drive frequency far below the spin band yields the highest harmonic order.","Additionally, we provide a theory matching the numerical results under weak spin-phonon coupling and propose an experimental procedure to probe the emission spectrum of spin HHG."],"url":"http://arxiv.org/abs/2404.05830v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 19:23:17","title":"Quantum Adversarial Learning for Kernel Methods","abstract":"We show that hybrid quantum classifiers based on quantum kernel methods and support vector machines are vulnerable against adversarial attacks, namely small engineered perturbations of the input data can deceive the classifier into predicting the wrong result. Nonetheless, we also show that simple defence strategies based on data augmentation with a few crafted perturbations can make the classifier robust against new attacks. Our results find applications in security-critical learning problems and in mitigating the effect of some forms of quantum noise, since the attacker can also be understood as part of the surrounding environment.","sentences":["We show that hybrid quantum classifiers based on quantum kernel methods and support vector machines are vulnerable against adversarial attacks, namely small engineered perturbations of the input data can deceive the classifier into predicting the wrong result.","Nonetheless, we also show that simple defence strategies based on data augmentation with a few crafted perturbations can make the classifier robust against new attacks.","Our results find applications in security-critical learning problems and in mitigating the effect of some forms of quantum noise, since the attacker can also be understood as part of the surrounding environment."],"url":"http://arxiv.org/abs/2404.05824v1","category":"quant-ph"}
{"created":"2024-04-08 18:36:18","title":"Towards Explainable Automated Neuroanatomy","abstract":"We present a novel method for quantifying the microscopic structure of brain tissue. It is based on the automated recognition of interpretable features obtained by analyzing the shapes of cells. This contrasts with prevailing methods of brain anatomical analysis in two ways. First, contemporary methods use gray-scale values derived from smoothed version of the anatomical images, which dissipated valuable information from the texture of the images. Second, contemporary analysis uses the output of black-box Convolutional Neural Networks, while our system makes decisions based on interpretable features obtained by analyzing the shapes of individual cells. An important benefit of this open-box approach is that the anatomist can understand and correct the decisions made by the computer. Our proposed system can accurately localize and identify existing brain structures. This can be used to align and coregistar brains and will facilitate connectomic studies for reverse engineering of brain circuitry.","sentences":["We present a novel method for quantifying the microscopic structure of brain tissue.","It is based on the automated recognition of interpretable features obtained by analyzing the shapes of cells.","This contrasts with prevailing methods of brain anatomical analysis in two ways.","First, contemporary methods use gray-scale values derived from smoothed version of the anatomical images, which dissipated valuable information from the texture of the images.","Second, contemporary analysis uses the output of black-box Convolutional Neural Networks, while our system makes decisions based on interpretable features obtained by analyzing the shapes of individual cells.","An important benefit of this open-box approach is that the anatomist can understand and correct the decisions made by the computer.","Our proposed system can accurately localize and identify existing brain structures.","This can be used to align and coregistar brains and will facilitate connectomic studies for reverse engineering of brain circuitry."],"url":"http://arxiv.org/abs/2404.05814v1","category":"cs.CV"}
{"created":"2024-04-08 18:16:25","title":"Ground State Preparation via Dynamical Cooling","abstract":"Quantum algorithms for probing ground-state properties of quantum systems require good initial states. Projection-based methods such as eigenvalue filtering rely on inputs that have a significant overlap with the low-energy subspace, which can be challenging for large, strongly-correlated systems. This issue has motivated the study of physically-inspired dynamical approaches such as thermodynamic cooling. In this work, we introduce a ground-state preparation algorithm based on the simulation of quantum dynamics. Our main insight is to transform the Hamiltonian by a shifted sign function via quantum signal processing, effectively mapping eigenvalues into positive and negative subspaces separated by a large gap. This automatically ensures that all states within each subspace conserve energy with respect to the transformed Hamiltonian. Subsequent time-evolution with a perturbed Hamiltonian induces transitions to lower-energy states while preventing unwanted jumps to higher energy states. The approach does not rely on a priori knowledge of energy gaps and requires no additional qubits to model a bath. Furthermore, it makes $\\tilde{\\mathcal{O}}(d^{\\,3/2}/\\epsilon)$ queries to the time-evolution operator of the system and $\\tilde{\\mathcal{O}}(d^{\\,3/2})$ queries to a block-encoding of the perturbation, for $d$ cooling steps and an $\\epsilon$-accurate energy resolution. Our results provide a framework for combining quantum signal processing and Hamiltonian simulation to design heuristic quantum algorithms for ground-state preparation.","sentences":["Quantum algorithms for probing ground-state properties of quantum systems require good initial states.","Projection-based methods such as eigenvalue filtering rely on inputs that have a significant overlap with the low-energy subspace, which can be challenging for large, strongly-correlated systems.","This issue has motivated the study of physically-inspired dynamical approaches such as thermodynamic cooling.","In this work, we introduce a ground-state preparation algorithm based on the simulation of quantum dynamics.","Our main insight is to transform the Hamiltonian by a shifted sign function via quantum signal processing, effectively mapping eigenvalues into positive and negative subspaces separated by a large gap.","This automatically ensures that all states within each subspace conserve energy with respect to the transformed Hamiltonian.","Subsequent time-evolution with a perturbed Hamiltonian induces transitions to lower-energy states while preventing unwanted jumps to higher energy states.","The approach does not rely on a priori knowledge of energy gaps and requires no additional qubits to model a bath.","Furthermore, it makes $\\tilde{\\mathcal{O}}(d^{\\,3/2}/\\epsilon)$ queries to the time-evolution operator of the system and $\\tilde{\\mathcal{O}}(d^{\\,3/2})$ queries to a block-encoding of the perturbation, for $d$ cooling steps and an $\\epsilon$-accurate energy resolution.","Our results provide a framework for combining quantum signal processing and Hamiltonian simulation to design heuristic quantum algorithms for ground-state preparation."],"url":"http://arxiv.org/abs/2404.05810v1","category":"quant-ph"}
{"created":"2024-04-08 18:01:11","title":"Coherent Heat Transfer Leads to Genuine Quantum Enhancement in Performances of Continuous Engines","abstract":"The conventional continuous quantum heat engines rely on incoherent heat transfer with the baths and, thus, have limited capability to outperform their classical counterparts. In this work, we introduce distinct continuous quantum heat engines that utilize coherent heat transfer with baths, yielding significant quantum enhancement in performance. These continuous engines, termed as coherent engines, consist of one qutrit system and two photonic baths and enable coherent heat transfer via two-photon transitions involving three-body interactions between the system and hot and cold baths. The closest quantum incoherent analogs are those that only allow incoherent heat transfer between the qutrit and the baths via one-photon transitions relying on two-body interactions between the system and hot or cold baths. We demonstrate that coherent engines deliver much higher power output and a much lower signal-to-noise ratio in power, where the latter signifies the reliability of an engine, compared to incoherent engines. Coherent engines manifest more non-classical features than incoherent engines because they violate the classical thermodynamic uncertainty relation by a greater amount and for a wider range of parameters. Importantly, coherent engines can operate close to or at the fundamental lower limit on reliability given by the quantum version of the thermodynamic uncertainty relation, making them highly reliable. These genuine enhancements in performance by hundreds of folds over incoherent engines and the saturation of the quantum limit by coherent engines are directly attributed to its capacity to harness higher energetic coherence which is, again, a consequence of coherent heat transfer. The experimental feasibility of coherent engines and the improved understanding of how quantum properties can enhance performance may find important implications in emerging quantum technologies.","sentences":["The conventional continuous quantum heat engines rely on incoherent heat transfer with the baths and, thus, have limited capability to outperform their classical counterparts.","In this work, we introduce distinct continuous quantum heat engines that utilize coherent heat transfer with baths, yielding significant quantum enhancement in performance.","These continuous engines, termed as coherent engines, consist of one qutrit system and two photonic baths and enable coherent heat transfer via two-photon transitions involving three-body interactions between the system and hot and cold baths.","The closest quantum incoherent analogs are those that only allow incoherent heat transfer between the qutrit and the baths via one-photon transitions relying on two-body interactions between the system and hot or cold baths.","We demonstrate that coherent engines deliver much higher power output and a much lower signal-to-noise ratio in power, where the latter signifies the reliability of an engine, compared to incoherent engines.","Coherent engines manifest more non-classical features than incoherent engines because they violate the classical thermodynamic uncertainty relation by a greater amount and for a wider range of parameters.","Importantly, coherent engines can operate close to or at the fundamental lower limit on reliability given by the quantum version of the thermodynamic uncertainty relation, making them highly reliable.","These genuine enhancements in performance by hundreds of folds over incoherent engines and the saturation of the quantum limit by coherent engines are directly attributed to its capacity to harness higher energetic coherence which is, again, a consequence of coherent heat transfer.","The experimental feasibility of coherent engines and the improved understanding of how quantum properties can enhance performance may find important implications in emerging quantum technologies."],"url":"http://arxiv.org/abs/2404.05799v1","category":"quant-ph"}
{"created":"2024-04-08 18:00:02","title":"Extinction values towards embedded planets in protoplanetary disks estimated from hydrodynamic simulations","abstract":"The upcoming new coronographs with deeper contrast limits, together with planned and current high-contrast imaging campaigns will push the detectability limit of protoplanets. These planet-hunting campaigns present a new opportunity to characterize protoplanets and their surrounding environments. However, there are clear uncertainties as to what are the extinction levels at different regions of protoplanetary disks, which will impede our ability to characterize young planets. A correct understanding of the expected extinction together with multiple photometric observations will lead to constraints on the extinction levels, dust growth, disk evolution and protoplanetary accretion rates. In this work, we used hydrodynamic simulations and protoplanetary disk observational constraints obtained from both dust and gas emission to explore the expected extinction maps for continuum filters associated with strong hydrogen lines as tracers of accretion and key broadband photometric filters. We provide a scaling relationship for the extinction as a function of planetary separation and disk mass for three different gas giant masses. We also report values for a subset of disks of interest targetted by multiple imaging campaigns. The described values will be useful for the optimal design of future planet-hunting surveys and for giving context to non-detections in protoplanetary disks and the observed fluxes of point sources along with the birth conditions of protoplanets.","sentences":["The upcoming new coronographs with deeper contrast limits, together with planned and current high-contrast imaging campaigns will push the detectability limit of protoplanets.","These planet-hunting campaigns present a new opportunity to characterize protoplanets and their surrounding environments.","However, there are clear uncertainties as to what are the extinction levels at different regions of protoplanetary disks, which will impede our ability to characterize young planets.","A correct understanding of the expected extinction together with multiple photometric observations will lead to constraints on the extinction levels, dust growth, disk evolution and protoplanetary accretion rates.","In this work, we used hydrodynamic simulations and protoplanetary disk observational constraints obtained from both dust and gas emission to explore the expected extinction maps for continuum filters associated with strong hydrogen lines as tracers of accretion and key broadband photometric filters.","We provide a scaling relationship for the extinction as a function of planetary separation and disk mass for three different gas giant masses.","We also report values for a subset of disks of interest targetted by multiple imaging campaigns.","The described values will be useful for the optimal design of future planet-hunting surveys and for giving context to non-detections in protoplanetary disks and the observed fluxes of point sources along with the birth conditions of protoplanets."],"url":"http://arxiv.org/abs/2404.05788v1","category":"astro-ph.EP"}
{"created":"2024-04-08 17:58:22","title":"Predicting Overtakes in Trucks Using CAN Data","abstract":"Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions. Accordingly, we investigate the detection of truck overtakes from CAN data. Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task. Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features. We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier. Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging. The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant). We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores. The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR). However, the latter is kept above 91% near the overtake trigger. Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers.","sentences":["Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions.","Accordingly, we investigate the detection of truck overtakes from CAN data.","Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task.","Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features.","We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier.","Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging.","The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant).","We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores.","The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR).","However, the latter is kept above 91% near the overtake trigger.","Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers."],"url":"http://arxiv.org/abs/2404.05723v1","category":"cs.LG"}
{"created":"2024-04-08 17:52:11","title":"Reconstructing the recombination history by combining early and late cosmological probes","abstract":"We develop and apply a new framework for reconstructing the ionization history during the epoch of recombination with combinations of cosmic microwave background (CMB), baryon acoustic oscillation (BAO) and supernova data. We find a wide range of ionization histories that are consistent with current CMB data, and also that cosmological parameter constraints are significantly weakened once freedom in recombination is introduced. BAO data partially break the degeneracy between cosmological parameters and the recombination model, and are therefore important in these reconstructions. The 95% confidence upper limits on H0 are 80.1 (70.7) km/s/Mpc given CMB (CMB+BAO) data, assuming no other changes are made to the standard cosmological model. Including Cepheid-calibrated supernova data in the analysis drives a preference for non-standard recombination histories with visibility functions that peak early and exhibit appreciable skewness. Forthcoming measurements from SPT-3G will reduce the uncertainties in our reconstructions by about a factor of two.","sentences":["We develop and apply a new framework for reconstructing the ionization history during the epoch of recombination with combinations of cosmic microwave background (CMB), baryon acoustic oscillation (BAO) and supernova data.","We find a wide range of ionization histories that are consistent with current CMB data, and also that cosmological parameter constraints are significantly weakened once freedom in recombination is introduced.","BAO data partially break the degeneracy between cosmological parameters and the recombination model, and are therefore important in these reconstructions.","The 95% confidence upper limits on H0 are 80.1 (70.7) km/s/Mpc given CMB (CMB+BAO) data, assuming no other changes are made to the standard cosmological model.","Including Cepheid-calibrated supernova data in the analysis drives a preference for non-standard recombination histories with visibility functions that peak early and exhibit appreciable skewness.","Forthcoming measurements from SPT-3G will reduce the uncertainties in our reconstructions by about a factor of two."],"url":"http://arxiv.org/abs/2404.05715v1","category":"astro-ph.CO"}
{"created":"2024-04-08 17:52:03","title":"What if you have only one copy? Low-depth quantum circuits have no advantage in decision problems!","abstract":"The conventional approach to understanding the characteristics of an unknown quantum state involves having numerous identical independent copies of the system in that state. However, we demonstrate that gleaning insights into specific properties is feasible even with a single-state sample. Perhaps surprisingly, the confidence level of our findings increases proportionally with the number of qubits. Our conclusions apply to quantum states with low circuit complexity, including noise-affected ones. Additionally, this extends to learning from a solitary sample of probability distributions. Our results establish a strong lower bound for discriminating quantum states with low complexity. Furthermore, we reveal no quantum advantage in decision problems involving low-depth quantum circuits. Our results can be used to verify NISQ devices.","sentences":["The conventional approach to understanding the characteristics of an unknown quantum state involves having numerous identical independent copies of the system in that state.","However, we demonstrate that gleaning insights into specific properties is feasible even with a single-state sample.","Perhaps surprisingly, the confidence level of our findings increases proportionally with the number of qubits.","Our conclusions apply to quantum states with low circuit complexity, including noise-affected ones.","Additionally, this extends to learning from a solitary sample of probability distributions.","Our results establish a strong lower bound for discriminating quantum states with low complexity.","Furthermore, we reveal no quantum advantage in decision problems involving low-depth quantum circuits.","Our results can be used to verify NISQ devices."],"url":"http://arxiv.org/abs/2404.05714v1","category":"quant-ph"}
{"created":"2024-04-08 17:43:53","title":"An Ogus Principle for Zip period maps: The Hasse invariant's vanishing order via `Frobenius and the Hodge filtration'","abstract":"This paper generalizes a result of Ogus that, under certain technical conditions, the vanishing order of the Hasse invariant of a family $Y/X$ of $n$-dimensional Calabi-Yau varieties in characteristic $p$ at a point $x$ of $X$ equals the \"conjugate line position\" of $H^n_{\\text{dR}}(Y/X)$ at $x$, i.e. the largest $i$ such that the line of the conjugate filtration is contained in $\\text{Fil}^i$ of the Hodge filtration. For every triple $(G,\\mu,r)$ consisting of a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and an $\\mathbf{F}_p$-representation $r$ of $G$, we state a generalized Ogus Principle. If $\\zeta:X \\to \\text{$G$-$\\mathtt{Zip}$}^{\\mu}$ is a smooth morphism (=`Zip period map'), then the group theoretic Ogus Principle implies an Ogus Principle on $X$. We deduce an Ogus Principle for several Hodge and abelian-type Shimura varieties and the moduli space of K3 surfaces.","sentences":["This paper generalizes a result of Ogus that, under certain technical conditions, the vanishing order of the Hasse invariant of a family $Y/X$ of $n$-dimensional Calabi-Yau varieties in characteristic $p$ at a point $x$ of $X$ equals the \"conjugate line position\" of $H^n_{\\text{dR}}(Y/X)$ at $x$, i.e. the largest $i$ such that the line of the conjugate filtration is contained in $\\text{Fil}^i$ of the Hodge filtration.","For every triple $(G,\\mu,r)$ consisting of a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and an $\\mathbf{F}_p$-representation $r$ of $G$, we state a generalized Ogus Principle.","If $\\zeta:X \\to \\text{$G$-$\\mathtt{Zip}$}^{\\mu}$ is a smooth morphism (=`Zip period map'), then the group theoretic Ogus Principle implies an Ogus Principle on $X$. We deduce an Ogus Principle for several Hodge and abelian-type Shimura varieties and the moduli space of K3 surfaces."],"url":"http://arxiv.org/abs/2404.05707v1","category":"math.AG"}
{"created":"2024-04-08 17:35:41","title":"On the estimation of complex statistics combining different surveys","abstract":"The importance of exploring a potential integration among surveys has been acknowledged in order to enhance effectiveness and minimize expenses. In this work, we employ the alignment method to combine information from two different surveys for the estimation of complex statistics. The derivation of the alignment weights poses challenges in case of complex statistics due to their non-linear form. To overcome this, we propose to use a linearized variable associated with the complex statistic under consideration. Linearized variables have been widely used to derive variance estimates, thus allowing for the estimation of the variance of the combined complex statistics estimates. Simulations conducted show the effectiveness of the proposed approach, resulting to the reduction of the variance of the combined complex statistics estimates. Also, in some cases, the usage of the alignment weights derived using the linearized variable associated with a complex statistic, could result in a further reduction of the variance of the combined estimates.","sentences":["The importance of exploring a potential integration among surveys has been acknowledged in order to enhance effectiveness and minimize expenses.","In this work, we employ the alignment method to combine information from two different surveys for the estimation of complex statistics.","The derivation of the alignment weights poses challenges in case of complex statistics due to their non-linear form.","To overcome this, we propose to use a linearized variable associated with the complex statistic under consideration.","Linearized variables have been widely used to derive variance estimates, thus allowing for the estimation of the variance of the combined complex statistics estimates.","Simulations conducted show the effectiveness of the proposed approach, resulting to the reduction of the variance of the combined complex statistics estimates.","Also, in some cases, the usage of the alignment weights derived using the linearized variable associated with a complex statistic, could result in a further reduction of the variance of the combined estimates."],"url":"http://arxiv.org/abs/2404.05702v1","category":"stat.ME"}
{"created":"2024-04-08 17:29:07","title":"BOLD v4: A Centralized Bioinformatics Platform for DNA-based Biodiversity Data","abstract":"BOLD, the Barcode of Life Data System, supports the acquisition, storage, validation, analysis, and publication of DNA barcodes, activities requiring the integration of molecular, morphological, and distributional data. Its pivotal role in curating the reference library of DNA barcodes, coupled with its data management and analysis capabilities, make it a central resource for biodiversity science. It enables rapid, accurate identification of specimens and also reveals patterns of genetic diversity and evolutionary relationships among taxa. Launched in 2005, BOLD has become an increasingly powerful tool for advancing understanding of planetary biodiversity. It currently hosts 17 million specimen records and 14 million barcodes that provide coverage for more than a million species from every continent and ocean. The platform has the long-term goal of providing a consistent, accurate system for identifying all species of eukaryotes. BOLD's integrated analytical tools, full data lifecycle support, and secure collaboration framework distinguish it from other biodiversity platforms. BOLD v4 brought enhanced data management and analysis capabilities as well as novel functionality for data dissemination and publication. Its next version will include features to strengthen its utility to the research community, governments, industry, and society-at-large.","sentences":["BOLD, the Barcode of Life Data System, supports the acquisition, storage, validation, analysis, and publication of DNA barcodes, activities requiring the integration of molecular, morphological, and distributional data.","Its pivotal role in curating the reference library of DNA barcodes, coupled with its data management and analysis capabilities, make it a central resource for biodiversity science.","It enables rapid, accurate identification of specimens and also reveals patterns of genetic diversity and evolutionary relationships among taxa.","Launched in 2005, BOLD has become an increasingly powerful tool for advancing understanding of planetary biodiversity.","It currently hosts 17 million specimen records and 14 million barcodes that provide coverage for more than a million species from every continent and ocean.","The platform has the long-term goal of providing a consistent, accurate system for identifying all species of eukaryotes.","BOLD's integrated analytical tools, full data lifecycle support, and secure collaboration framework distinguish it from other biodiversity platforms.","BOLD v4 brought enhanced data management and analysis capabilities as well as novel functionality for data dissemination and publication.","Its next version will include features to strengthen its utility to the research community, governments, industry, and society-at-large."],"url":"http://arxiv.org/abs/2404.05696v1","category":"cs.DB"}
{"created":"2024-04-08 17:16:13","title":"The neutrino background from non-jetted active galactic nuclei","abstract":"Aims. We calculate the contribution to the neutrino background from the non-jetted active galactic nuclei (AGN) population following the recent IceCube association of TeV neutrinos with NGC 1068. Methods. We exploit our robust knowledge of the AGN X-ray luminosity function and evolution and convert it to the neutrino band by using NGC 1068 as a benchmark and a theoretically motivated neutrino spectrum. Results. The resulting neutrino background up to redshift 5 does not violate either the IceCube diffuse flux or the upper bounds for non-jetted AGN, although barely so. This is consistent with a scenario where the latter class makes a substantial contribution mostly below 1 PeV, while jetted AGN, i.e. blazars, dominate above this energy, in intriguing agreement with the dip in the neutrino data at ~ 300 TeV. More and better IceCube data on Seyfert galaxies will allow us to constrain the fraction of neutrino emitters among non-jetted AGN.","sentences":["Aims.","We calculate the contribution to the neutrino background from the non-jetted active galactic nuclei (AGN) population following the recent IceCube association of TeV neutrinos with NGC 1068.","Methods.","We exploit our robust knowledge of the AGN X-ray luminosity function and evolution and convert it to the neutrino band by using NGC 1068 as a benchmark and a theoretically motivated neutrino spectrum.","Results.","The resulting neutrino background up to redshift 5 does not violate either the IceCube diffuse flux or the upper bounds for non-jetted AGN, although barely so.","This is consistent with a scenario where the latter class makes a substantial contribution mostly below 1 PeV, while jetted AGN, i.e. blazars, dominate above this energy, in intriguing agreement with the dip in the neutrino data at ~ 300 TeV. More and better IceCube data on Seyfert galaxies will allow us to constrain the fraction of neutrino emitters among non-jetted AGN."],"url":"http://arxiv.org/abs/2404.05690v1","category":"astro-ph.HE"}
{"created":"2024-04-08 17:10:07","title":"Global phase diagram of doped quantum spin liquid on the Kagome lattice","abstract":"It has long been believed that doped quantum spin liquids (QSLs) can give rise to fascinating quantum phases, including the possibility of high-temperature superconductivity (SC) as proposed by P. W. Anderson's resonating valence bond (RVB) scenario. The Kagome lattice $t$-$J$ model is known to exhibit spin liquid behavior at half-filling, making it an ideal system for studying the properties of doped QSL. In this study, we employ the fermionic projected entangled simplex state (PESS) method to investigate the ground state properties of the Kagome lattice $t$-$J$ model with $t/J = 3.0$. Our results reveal a phase transition from charge density wave (CDW) states to uniform states around a critical doping level $\\delta_c \\approx 0.27$. Within the CDW phase, we observe different types of Wigner crystal (WC) formulated by doped holes that are energetically favored. As we enter the uniform phase, a non-Fermi liquid (NFL) state emerges within the doping range $0.27 < \\delta < 0.32$, characterized by an exponential decay of all correlation functions. With further hole doping, we discover the appearance of a pair density wave (PDW) state within a narrow doping region $0.32 < \\delta < 1/3$. We also discuss the potential experimental implications of our findings.","sentences":["It has long been believed that doped quantum spin liquids (QSLs) can give rise to fascinating quantum phases, including the possibility of high-temperature superconductivity (SC) as proposed by P. W. Anderson's resonating valence bond (RVB) scenario.","The Kagome lattice $t$-$J$ model is known to exhibit spin liquid behavior at half-filling, making it an ideal system for studying the properties of doped QSL.","In this study, we employ the fermionic projected entangled simplex state (PESS) method to investigate the ground state properties of the Kagome lattice $t$-$J$ model with $t/J = 3.0$. Our results reveal a phase transition from charge density wave (CDW) states to uniform states around a critical doping level $\\delta_c \\approx 0.27$.","Within the CDW phase, we observe different types of Wigner crystal (WC) formulated by doped holes that are energetically favored.","As we enter the uniform phase, a non-Fermi liquid (NFL) state emerges within the doping range $0.27 < \\delta < 0.32$, characterized by an exponential decay of all correlation functions.","With further hole doping, we discover the appearance of a pair density wave (PDW) state within a narrow doping region $0.32 <","\\delta <","1/3$. We also discuss the potential experimental implications of our findings."],"url":"http://arxiv.org/abs/2404.05685v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 16:58:31","title":"SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation","abstract":"While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists. Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views. Based on our in-depth analysis, we found the reasons are mainly twofold. First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing \"mirroring\" artifacts (e.g., the glasses appear in the back). Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered. This makes it possible to generate \"face\" in non-frontal views, due to its easiness to fool the discriminator. In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head's geometric characteristics and efficiently mitigates many of the generated artifacts. We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images. The combination of these efforts results in visually superior outcomes with significantly fewer artifacts. Our code and dataset are publicly available at https://lhyfst.github.io/spherehead.","sentences":["While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists.","Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views.","Based on our in-depth analysis, we found the reasons are mainly twofold.","First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing \"mirroring\" artifacts (e.g., the glasses appear in the back).","Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered.","This makes it possible to generate \"face\" in non-frontal views, due to its easiness to fool the discriminator.","In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head's geometric characteristics and efficiently mitigates many of the generated artifacts.","We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images.","The combination of these efforts results in visually superior outcomes with significantly fewer artifacts.","Our code and dataset are publicly available at https://lhyfst.github.io/spherehead."],"url":"http://arxiv.org/abs/2404.05680v1","category":"cs.CV"}
{"created":"2024-04-08 16:58:19","title":"Overview of projective quantum measurements","abstract":"We provide an overview of standard \"projective\" quantum measurements with the goal of elucidating connections between theory and experiment. We make use of a unitary \"Stinespring\" representation of measurements on a dilated Hilbert space that includes both the physical degrees of freedom and those of the measurement apparatus. We explain how this unitary representation (i) is guaranteed by the axioms of quantum mechanics, (ii) relates to both the Kraus and von Neumann representations, and (iii) corresponds to the physical time evolution of the system and apparatus during the measurement process. The Stinespring representation also offers significant conceptual insight into measurements, helps connects theory and experiment, is particularly useful in describing protocols involving midcircuit measurements and outcome-dependent operations, and establishes that all quantum operations are compatible with relativistic locality, among other insights.","sentences":["We provide an overview of standard \"projective\" quantum measurements with the goal of elucidating connections between theory and experiment.","We make use of a unitary \"Stinespring\" representation of measurements on a dilated Hilbert space that includes both the physical degrees of freedom and those of the measurement apparatus.","We explain how this unitary representation (i) is guaranteed by the axioms of quantum mechanics, (ii) relates to both the Kraus and von Neumann representations, and (iii) corresponds to the physical time evolution of the system and apparatus during the measurement process.","The Stinespring representation also offers significant conceptual insight into measurements, helps connects theory and experiment, is particularly useful in describing protocols involving midcircuit measurements and outcome-dependent operations, and establishes that all quantum operations are compatible with relativistic locality, among other insights."],"url":"http://arxiv.org/abs/2404.05679v1","category":"quant-ph"}
{"created":"2024-04-08 16:57:44","title":"Flexible Fairness Learning via Inverse Conditional Permutation","abstract":"Equalized odds, as a popular notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm prediction when conditioning on the true outcome. Despite rapid advancements, most of the current research focuses on the violation of equalized odds caused by one sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed. We address this gap by introducing a fairness learning approach that integrates adversarial learning with a novel inverse conditional permutation. This approach effectively and flexibly handles multiple sensitive attributes, potentially of mixed data types. The efficacy and flexibility of our method are demonstrated through both simulation studies and empirical analysis of real-world datasets.","sentences":["Equalized odds, as a popular notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm prediction when conditioning on the true outcome.","Despite rapid advancements, most of the current research focuses on the violation of equalized odds caused by one sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed.","We address this gap by introducing a fairness learning approach that integrates adversarial learning with a novel inverse conditional permutation.","This approach effectively and flexibly handles multiple sensitive attributes, potentially of mixed data types.","The efficacy and flexibility of our method are demonstrated through both simulation studies and empirical analysis of real-world datasets."],"url":"http://arxiv.org/abs/2404.05678v2","category":"stat.ML"}
{"created":"2024-04-08 16:56:07","title":"Dark matter free dwarf galaxy formation at the the tips of the tentacles of jellyfish galaxies","abstract":"When falling into a galaxy cluster, galaxies experience a loss of gas due to ram pressure stripping. In particular, disk galaxies lose gas from their disks and very large tentacles of gas can be formed. Because of the morphology of these stripped galaxies they have been referred to as Jellyfish galaxies. It has been found that star formation is triggered not only in the disk, but also in the tentacles of such Jellyfish galaxies. The observed star forming regions located in the tentacles of those galaxies have been found to be as massive as $3\\times10^7$ M$_{\\odot}$ and with sizes $> 100$ pc. Interestingly, these parameters in mass and size agree with those of dwarf galaxies. In this work we make use of the state of the art magneto-hydrodynamical cosmological simulation Illustris TNG-50, to study massive jellyfish galaxies with long tentacles. We find that, in the tentacles of TNG-50 Jellyfish galaxies, the star formation regions (gas+stars) formed could be as massive as $\\sim2\\times10^8$ M$_{\\odot}$. A particular star forming region was analyzed. This region has a star formation rate of $0.04$ M$_{\\odot}$/yr, it is metal rich, has an average age of $0.46$ Gyr, and has a half mass radius of $\\sim1$ kpc, typical of standard dwarf galaxies. Most importantly, this region is gravitationally self-bound. All and all, we identify a new type of dwarf galaxy being born from the gas tentacles of jellyfish galaxies, that by construction lacks a dark matter (hereafter DM) halo.","sentences":["When falling into a galaxy cluster, galaxies experience a loss of gas due to ram pressure stripping.","In particular, disk galaxies lose gas from their disks and very large tentacles of gas can be formed.","Because of the morphology of these stripped galaxies they have been referred to as Jellyfish galaxies.","It has been found that star formation is triggered not only in the disk, but also in the tentacles of such Jellyfish galaxies.","The observed star forming regions located in the tentacles of those galaxies have been found to be as massive as $3\\times10^7$ M$_{\\odot}$ and with sizes $> 100$ pc.","Interestingly, these parameters in mass and size agree with those of dwarf galaxies.","In this work we make use of the state of the art magneto-hydrodynamical cosmological simulation Illustris TNG-50, to study massive jellyfish galaxies with long tentacles.","We find that, in the tentacles of TNG-50 Jellyfish galaxies, the star formation regions (gas+stars) formed could be as massive as $\\sim2\\times10^8","$ M$_{\\odot}$. A particular star forming region was analyzed.","This region has a star formation rate of $0.04$ M$_{\\odot}$/yr, it is metal rich, has an average age of $0.46$ Gyr, and has a half mass radius of $\\sim1$ kpc, typical of standard dwarf galaxies.","Most importantly, this region is gravitationally self-bound.","All and all, we identify a new type of dwarf galaxy being born from the gas tentacles of jellyfish galaxies, that by construction lacks a dark matter (hereafter DM) halo."],"url":"http://arxiv.org/abs/2404.05676v1","category":"astro-ph.GA"}
{"created":"2024-04-08 16:54:41","title":"Bayesian Inverse Ising Problem with Three-body Interactions","abstract":"In this paper, we solve the inverse Ising problem with three-body interaction. Using the mean-field approximation, we find a tractable expansion of the normalizing constant. This facilitates estimation, which is known to be quite challenging for the Ising model. We then develop a novel hybrid MCMC algorithm that integrates Adaptive Metropolis Hastings (AMH), Hamiltonian Monte Carlo (HMC), and the Manifold-Adjusted Langevin Algorithm (MALA), which converges quickly and mixes well. We demonstrate the robustness of our algorithm using data simulated with a structure under which parameter estimation is known to be challenging, such as in the presence of a phase transition and at the critical point of the system.","sentences":["In this paper, we solve the inverse Ising problem with three-body interaction.","Using the mean-field approximation, we find a tractable expansion of the normalizing constant.","This facilitates estimation, which is known to be quite challenging for the Ising model.","We then develop a novel hybrid MCMC algorithm that integrates Adaptive Metropolis Hastings (AMH), Hamiltonian Monte Carlo (HMC), and the Manifold-Adjusted Langevin Algorithm (MALA), which converges quickly and mixes well.","We demonstrate the robustness of our algorithm using data simulated with a structure under which parameter estimation is known to be challenging, such as in the presence of a phase transition and at the critical point of the system."],"url":"http://arxiv.org/abs/2404.05671v1","category":"stat.ME"}
{"created":"2024-04-08 16:50:51","title":"Substructures of the Weyl group and their physical applications","abstract":"We study substructures of the Weyl group of conformal transformations of the metric of (pseudo)Riemannian manifolds. These substructures are identified by differential constraints on the conformal factors of the transformations which are chosen such that their composition is associative. Mathematically, apart from rare exceptions, they are partial associative groupoids, not groups, so they do not have an algebra of infinitesimal transformations, but this limitation can be partially circumvented using some of their properties cleverly. We classify and discuss the substructures with two-derivatives differential constraints, the most famous of which being known as the harmonic or restricted Weyl group in the physics literature, but we also show the existence of a lightcone constraint which realizes a proper subgroup of the Weyl group. We then show the physical implications that come from invariance under the two most important substructures, concentrating on classical properties of the energy-momentum tensor and a generalization of the quantum trace anomaly. We also elaborate further on the harmonic substructure, which can be interpreted as partial gauge fixing of full Weyl invariance using BRST methods. Finally, we discuss how to construct differential constraints of arbitrary higher-derivative order and present, as examples, generalizations involving scalar constraints with four and six derivatives.","sentences":["We study substructures of the Weyl group of conformal transformations of the metric of (pseudo)Riemannian manifolds.","These substructures are identified by differential constraints on the conformal factors of the transformations which are chosen such that their composition is associative.","Mathematically, apart from rare exceptions, they are partial associative groupoids, not groups, so they do not have an algebra of infinitesimal transformations, but this limitation can be partially circumvented using some of their properties cleverly.","We classify and discuss the substructures with two-derivatives differential constraints, the most famous of which being known as the harmonic or restricted Weyl group in the physics literature, but we also show the existence of a lightcone constraint which realizes a proper subgroup of the Weyl group.","We then show the physical implications that come from invariance under the two most important substructures, concentrating on classical properties of the energy-momentum tensor and a generalization of the quantum trace anomaly.","We also elaborate further on the harmonic substructure, which can be interpreted as partial gauge fixing of full Weyl invariance using BRST methods.","Finally, we discuss how to construct differential constraints of arbitrary higher-derivative order and present, as examples, generalizations involving scalar constraints with four and six derivatives."],"url":"http://arxiv.org/abs/2404.05665v1","category":"hep-th"}
{"created":"2024-04-08 16:46:25","title":"BinaryDM: Towards Accurate Binarization of Diffusion Model","abstract":"With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs. However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths. In this paper, we propose BinaryDM, a novel accurate quantization-aware training approach to push the weights of diffusion models towards the limit of 1-bit. Firstly, we present a Learnable Multi-basis Binarizer (LMB) to recover the representations generated by the binarized DM, which improves the information in details of representations crucial to the DM. Secondly, a Low-rank Representation Mimicking (LRM) is applied to enhance the binarization-aware optimization of the DM, alleviating the optimization direction ambiguity caused by fine-grained alignment. Moreover, a progressive initialization strategy is applied to training DMs to avoid convergence difficulties. Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths. As the first binarization method for diffusion models, BinaryDM achieves impressive 16.0 times FLOPs and 27.1 times storage savings with 1-bit weight and 4-bit activation, showcasing its substantial advantages and potential for deploying DMs on resource-limited scenarios.","sentences":["With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs.","However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths.","In this paper, we propose BinaryDM, a novel accurate quantization-aware training approach to push the weights of diffusion models towards the limit of 1-bit.","Firstly, we present a Learnable Multi-basis Binarizer (LMB) to recover the representations generated by the binarized DM, which improves the information in details of representations crucial to the DM.","Secondly, a Low-rank Representation Mimicking (LRM) is applied to enhance the binarization-aware optimization of the DM, alleviating the optimization direction ambiguity caused by fine-grained alignment.","Moreover, a progressive initialization strategy is applied to training DMs to avoid convergence difficulties.","Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths.","As the first binarization method for diffusion models, BinaryDM achieves impressive 16.0 times FLOPs and 27.1 times storage savings with 1-bit weight and 4-bit activation, showcasing its substantial advantages and potential for deploying DMs on resource-limited scenarios."],"url":"http://arxiv.org/abs/2404.05662v1","category":"cs.CV"}
{"created":"2024-04-08 16:40:15","title":"MLP Can Be A Good Transformer Learner","abstract":"Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.","sentences":["Self-attention mechanism is the key of the Transformer but often criticized for its computation demands.","Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs.","This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations.","We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity.","Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks.","Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks.","Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise.","Code is available at https://github.com/sihaoevery/lambda_vit."],"url":"http://arxiv.org/abs/2404.05657v1","category":"cs.CV"}
{"created":"2024-04-09 17:49:07","title":"Existence of Mexican-hat dispersion and symmetry group of a layer","abstract":"Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines. In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature. This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups. The method applies to non magnetic materials both with or without spin-orbit coupling. Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups. Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties. The results presented here might serve also as a guiding tool for design of new two-dimensional materials.","sentences":["Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines.","In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature.","This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups.","The method applies to non magnetic materials both with or without spin-orbit coupling.","Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups.","Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties.","The results presented here might serve also as a guiding tool for design of new two-dimensional materials."],"url":"http://arxiv.org/abs/2404.06494v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 17:37:08","title":"GO4Align: Group Optimization for Multi-Task Alignment","abstract":"This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs.","sentences":["This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks.","To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations.","Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs."],"url":"http://arxiv.org/abs/2404.06486v1","category":"cs.LG"}
{"created":"2024-04-09 16:45:34","title":"Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition","abstract":"Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions. While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition. Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation. Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling). Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition. Our code is publicly available at https://github.com/CVI-SZU/MDHR.","sentences":["Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions.","While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition.","Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation.","Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling).","Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition.","Our code is publicly available at https://github.com/CVI-SZU/MDHR."],"url":"http://arxiv.org/abs/2404.06443v1","category":"cs.CV"}
{"created":"2024-04-09 15:36:50","title":"MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies","abstract":"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .","sentences":["The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation.","This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative.","In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs.","While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research.","Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling.","For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation.","We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS.","With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal.","Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications.","MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM ."],"url":"http://arxiv.org/abs/2404.06395v1","category":"cs.CL"}
{"created":"2024-04-09 15:02:01","title":"Dynamic Resolution Guidance for Facial Expression Recognition","abstract":"Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging. This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy. Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER). The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution. We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches. The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications.","sentences":["Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging.","This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy.","Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER).","The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution.","We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches.","The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications."],"url":"http://arxiv.org/abs/2404.06365v1","category":"cs.CV"}
{"created":"2024-04-09 13:17:23","title":"Size selection of crack front defects: Multiple fracture-plane interactions and intrinsic lengthscales","abstract":"Material failure is mediated by the propagation of cracks, which in realistic 3D materials typically involve multiple coexisting fracture planes. Multiple fracture-plane interactions create poorly understood out-of-plane crack structures, such as step defects on tensile fracture surfaces. Steps form once a slowly moving, distorted crack front segments into disconnected overlapping fracture planes separated by a stabilizing distance $h_{\\rm max}$. Our experiments on numerous brittle hydrogels reveal that $h_{\\rm max}$ varies linearly with both a nonlinear elastic length $\\Gamma(v)/\\mu$ and a dissipation length $\\xi$. Here, $\\Gamma(v)$ is the measured crack velocity $v$-dependent fracture energy and $\\mu$ is the shear modulus. These intrinsic lengthscales point the way to a fundamental understanding of multiple-crack interactions in 3D that lead to the formation of stable out-of-plane fracture structures.","sentences":["Material failure is mediated by the propagation of cracks, which in realistic 3D materials typically involve multiple coexisting fracture planes.","Multiple fracture-plane interactions create poorly understood out-of-plane crack structures, such as step defects on tensile fracture surfaces.","Steps form once a slowly moving, distorted crack front segments into disconnected overlapping fracture planes separated by a stabilizing distance $h_{\\rm max}$. Our experiments on numerous brittle hydrogels reveal that $h_{\\rm max}$ varies linearly with both a nonlinear elastic length $\\Gamma(v)/\\mu$ and a dissipation length $\\xi$. Here, $\\Gamma(v)$ is the measured crack velocity $v$-dependent fracture energy and $\\mu$ is the shear modulus.","These intrinsic lengthscales point the way to a fundamental understanding of multiple-crack interactions in 3D that lead to the formation of stable out-of-plane fracture structures."],"url":"http://arxiv.org/abs/2404.06289v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 10:52:56","title":"Adaptive Unit Root Inference in Autoregressions using the Lasso Solution Path","abstract":"We show that the activation knot of a potentially non-stationary regressor on the adaptive Lasso solution path in autoregressions can be leveraged for selection-free inference about a unit root. The resulting test has asymptotic power against local alternatives in $1/T$ neighbourhoods, unlike post-selection inference methods based on consistent model selection. Exploiting the information enrichment principle devised by Reinschl\\\"ussel and Arnold arXiv:2402.16580 [stat.ME] to improve the Lasso-based selection of ADF models, we propose a composite statistic and analyse its asymptotic distribution and local power function. Monte Carlo evidence shows that the combined test dominates the comparable post-selection inference methods of Tibshirani et al. [JASA, 2016, 514, 600-620] and may surpass the power of established unit root tests against local alternatives. We apply the new tests to groundwater level time series for Germany and find evidence rejecting stochastic trends to explain observed long-term declines in mean water levels.","sentences":["We show that the activation knot of a potentially non-stationary regressor on the adaptive Lasso solution path in autoregressions can be leveraged for selection-free inference about a unit root.","The resulting test has asymptotic power against local alternatives in $1/T$ neighbourhoods, unlike post-selection inference methods based on consistent model selection.","Exploiting the information enrichment principle devised by Reinschl\\\"ussel and Arnold arXiv:2402.16580","[stat.","ME] to improve the Lasso-based selection of ADF models, we propose a composite statistic and analyse its asymptotic distribution and local power function.","Monte Carlo evidence shows that the combined test dominates the comparable post-selection inference methods of Tibshirani et al.","[JASA, 2016, 514, 600-620] and may surpass the power of established unit root tests against local alternatives.","We apply the new tests to groundwater level time series for Germany and find evidence rejecting stochastic trends to explain observed long-term declines in mean water levels."],"url":"http://arxiv.org/abs/2404.06205v1","category":"stat.ME"}
{"created":"2024-04-09 09:07:16","title":"A look at the operator product expansion in critical dynamics","abstract":"We consider the critical relaxation of the Ising model, the so-called model A, and study its operator product expansion. Within perturbation theory, we focus on the operator product expansions of the two-point function and the response function. At the fixed point, we normalize the coefficients and the scaling variables so that the result displays universality. The role of the fluctuation-dissipation theorem is also discussed, and it is shown that it provides non-perturbative relations among the operator product expansion coefficients. Finally, the large N limit is considered.","sentences":["We consider the critical relaxation of the Ising model, the so-called model A, and study its operator product expansion.","Within perturbation theory, we focus on the operator product expansions of the two-point function and the response function.","At the fixed point, we normalize the coefficients and the scaling variables so that the result displays universality.","The role of the fluctuation-dissipation theorem is also discussed, and it is shown that it provides non-perturbative relations among the operator product expansion coefficients.","Finally, the large N limit is considered."],"url":"http://arxiv.org/abs/2404.06142v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 08:57:02","title":"REPUBLIC: A variability-preserving systematic-correction algorithm for PLATO's multi-camera light curves","abstract":"Space-based photometry missions produce exquisite light curves that contain a wealth of stellar variability on a wide range of timescales. Light curves also typically contain significant instrumental systematics -- spurious, non-astrophysical trends that are common, in varying degrees, to many light curves. Empirical systematics-correction approaches using the information in the light curves themselves have been very successful, but tend to suppress astrophysical signals, particularly on longer timescales. Unlike its predecessors, the PLATO mission will use multiple cameras to monitor the same stars. We present REPUBLIC, a novel systematics-correction algorithm which exploits this multi-camera configuration to correct systematics that differ between cameras, while preserving the component of each star's signal that is common to all cameras, regardless of timescale. Through simulations with astrophysical signals (star spots and planetary transits), Kepler-like errors, and white noise, we demonstrate REPUBLIC's ability to preserve long-term astrophysical signals usually lost in standard correction techniques. We also explore REPUBLIC's performance with different number of cameras and systematic properties. We conclude that REPUBLIC should be considered a potential complement to existing strategies for systematic correction in multi-camera surveys, with its utility contingent upon further validation and adaptation to the specific characteristics of the PLATO mission data","sentences":["Space-based photometry missions produce exquisite light curves that contain a wealth of stellar variability on a wide range of timescales.","Light curves also typically contain significant instrumental systematics -- spurious, non-astrophysical trends that are common, in varying degrees, to many light curves.","Empirical systematics-correction approaches using the information in the light curves themselves have been very successful, but tend to suppress astrophysical signals, particularly on longer timescales.","Unlike its predecessors, the PLATO mission will use multiple cameras to monitor the same stars.","We present REPUBLIC, a novel systematics-correction algorithm which exploits this multi-camera configuration to correct systematics that differ between cameras, while preserving the component of each star's signal that is common to all cameras, regardless of timescale.","Through simulations with astrophysical signals (star spots and planetary transits), Kepler-like errors, and white noise, we demonstrate REPUBLIC's ability to preserve long-term astrophysical signals usually lost in standard correction techniques.","We also explore REPUBLIC's performance with different number of cameras and systematic properties.","We conclude that REPUBLIC should be considered a potential complement to existing strategies for systematic correction in multi-camera surveys, with its utility contingent upon further validation and adaptation to the specific characteristics of the PLATO mission data"],"url":"http://arxiv.org/abs/2404.06132v1","category":"astro-ph.IM"}
{"created":"2024-04-09 03:54:28","title":"AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts","abstract":"As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment","sentences":["As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase.","We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas.","To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories.","Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy.","We plan to release this dataset to the community to further research and to help benchmark LLM models for safety.","To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models.","We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories.","We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores.","Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment"],"url":"http://arxiv.org/abs/2404.05993v1","category":"cs.LG"}
{"created":"2024-04-09 00:43:45","title":"AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression","abstract":"Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server. However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups. To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature. Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training. In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents. We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance ($0-2\\%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression.","sentences":["Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server.","However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups.","To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature.","Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training.","In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents.","We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies.","Our experiments show that the proposed method achieves superior performance ($0-2\\%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression."],"url":"http://arxiv.org/abs/2404.05919v1","category":"cs.LG"}
{"created":"2024-04-08 20:52:16","title":"Optimal robust exact first-order differentiators with Lipschitz continuous output","abstract":"The signal differentiation problem involves the development of algorithms that allow to recover a signal's derivatives from noisy measurements. This paper develops a first-order differentiator with the following combination of properties: robustness to measurement noise, exactness in the absence of noise, optimal worst-case differentiation error, and Lipschitz continuous output where the output's Lipschitz constant is a tunable parameter. This combination of advantageous properties is not shared by any existing differentiator. Both continuous-time and sample-based versions of the differentiator are developed and theoretical guarantees are established for both. The continuous-time version of the differentiator consists in a regularized and sliding-mode-filtered linear adaptive differentiator. The sample-based, implementable version is then obtained through appropriate discretization. An illustrative example is provided to highlight the features of the developed differentiator.","sentences":["The signal differentiation problem involves the development of algorithms that allow to recover a signal's derivatives from noisy measurements.","This paper develops a first-order differentiator with the following combination of properties: robustness to measurement noise, exactness in the absence of noise, optimal worst-case differentiation error, and Lipschitz continuous output where the output's Lipschitz constant is a tunable parameter.","This combination of advantageous properties is not shared by any existing differentiator.","Both continuous-time and sample-based versions of the differentiator are developed and theoretical guarantees are established for both.","The continuous-time version of the differentiator consists in a regularized and sliding-mode-filtered linear adaptive differentiator.","The sample-based, implementable version is then obtained through appropriate discretization.","An illustrative example is provided to highlight the features of the developed differentiator."],"url":"http://arxiv.org/abs/2404.05863v1","category":"eess.SY"}
{"created":"2024-04-08 20:42:10","title":"A Neuromorphic Approach to Obstacle Avoidance in Robot Manipulation","abstract":"Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs). Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges. SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification. Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots.   To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator. Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive. We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline.   Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed. Trajectory adaptations had low impacts on safety and predictability criteria. Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods. Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation. Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods.","sentences":["Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs).","Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges.","SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification.","Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots.   ","To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator.","Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive.","We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline.   ","Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed.","Trajectory adaptations had low impacts on safety and predictability criteria.","Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods.","Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation.","Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods."],"url":"http://arxiv.org/abs/2404.05858v1","category":"cs.RO"}
{"created":"2024-04-08 20:02:19","title":"Parameter-Adaptive Approximate MPC: Tuning Neural-Network Controllers without Re-Training","abstract":"Model Predictive Control (MPC) is a method to control nonlinear systems with guaranteed stability and constraint satisfaction but suffers from high computation times. Approximate MPC (AMPC) with neural networks (NNs) has emerged to address this limitation, enabling deployment on resource-constrained embedded systems. However, when tuning AMPCs for real-world systems, large datasets need to be regenerated and the NN needs to be retrained at every tuning step. This work introduces a novel, parameter-adaptive AMPC architecture capable of online tuning without recomputing large datasets and retraining. By incorporating local sensitivities of nonlinear programs, the proposed method not only mimics optimal MPC inputs but also adjusts to changes in physical parameters of the model using linear predictions while still guaranteeing stability. We showcase the effectiveness of parameter-adaptive AMPC by controlling the swing-ups of two different real cartpole systems with a severely resource-constrained microcontroller (MCU). We use the same NN across both system instances that have different parameters. This work not only represents the first experimental demonstration of AMPC for fast-moving systems on low-cost MCUs to the best of our knowledge, but also showcases generalization across system instances and variations through our parameter-adaptation method. Taken together, these contributions represent a marked step toward the practical application of AMPC in real-world systems.","sentences":["Model Predictive Control (MPC) is a method to control nonlinear systems with guaranteed stability and constraint satisfaction but suffers from high computation times.","Approximate MPC (AMPC) with neural networks (NNs) has emerged to address this limitation, enabling deployment on resource-constrained embedded systems.","However, when tuning AMPCs for real-world systems, large datasets need to be regenerated and the NN needs to be retrained at every tuning step.","This work introduces a novel, parameter-adaptive AMPC architecture capable of online tuning without recomputing large datasets and retraining.","By incorporating local sensitivities of nonlinear programs, the proposed method not only mimics optimal MPC inputs but also adjusts to changes in physical parameters of the model using linear predictions while still guaranteeing stability.","We showcase the effectiveness of parameter-adaptive AMPC by controlling the swing-ups of two different real cartpole systems with a severely resource-constrained microcontroller (MCU).","We use the same NN across both system instances that have different parameters.","This work not only represents the first experimental demonstration of AMPC for fast-moving systems on low-cost MCUs to the best of our knowledge, but also showcases generalization across system instances and variations through our parameter-adaptation method.","Taken together, these contributions represent a marked step toward the practical application of AMPC in real-world systems."],"url":"http://arxiv.org/abs/2404.05835v1","category":"eess.SY"}
{"created":"2024-04-08 19:59:47","title":"Reconstructing rotation curves with artificial neural networks","abstract":"Galactic rotation curves have been served as indispensable tools for determining the distribution of mass within galaxies. Despite several advances in precision observations, some discrepancies still persist between the inferred matter distribution from luminosity and observed rotation velocities, often attributed to the presence of dark matter. Traditional parametric models, while insightful, struggle with the complexity of galaxies with prominent bulges or non-circular motions, but in contrast, non-parametric methods offer a promising alternative, adapting to the intricate nature of rotation curves without any prior assumptions. In this work, we employ artificial neural networks to reconstruct rotation curves of 17 spiral galaxies from high-quality data, demonstrating the efficacy of the non-parametric approaches in characterizing galactic dynamics. Our findings underscore the importance of employing diverse methodological approaches to comprehensively understand galactic dynamics in modern astrophysics research. Moreover, the non-parametric reconstruction approach with neural networks presents a promising avenue for further investigations, capable of generating interpolations based on the intrinsic patterns of the data.","sentences":["Galactic rotation curves have been served as indispensable tools for determining the distribution of mass within galaxies.","Despite several advances in precision observations, some discrepancies still persist between the inferred matter distribution from luminosity and observed rotation velocities, often attributed to the presence of dark matter.","Traditional parametric models, while insightful, struggle with the complexity of galaxies with prominent bulges or non-circular motions, but in contrast, non-parametric methods offer a promising alternative, adapting to the intricate nature of rotation curves without any prior assumptions.","In this work, we employ artificial neural networks to reconstruct rotation curves of 17 spiral galaxies from high-quality data, demonstrating the efficacy of the non-parametric approaches in characterizing galactic dynamics.","Our findings underscore the importance of employing diverse methodological approaches to comprehensively understand galactic dynamics in modern astrophysics research.","Moreover, the non-parametric reconstruction approach with neural networks presents a promising avenue for further investigations, capable of generating interpolations based on the intrinsic patterns of the data."],"url":"http://arxiv.org/abs/2404.05833v1","category":"astro-ph.GA"}
{"created":"2024-04-08 17:33:11","title":"Dynamical stability and chaos in artificial neural network trajectories along training","abstract":"The process of training an artificial neural network involves iteratively adapting its parameters so as to minimize the error of the network's prediction, when confronted with a learning task. This iterative change can be naturally interpreted as a trajectory in network space -- a time series of networks -- and thus the training algorithm (e.g. gradient descent optimization of a suitable loss function) can be interpreted as a dynamical system in graph space. In order to illustrate this interpretation, here we study the dynamical properties of this process by analyzing through this lens the network trajectories of a shallow neural network, and its evolution through learning a simple classification task. We systematically consider different ranges of the learning rate and explore both the dynamical and orbital stability of the resulting network trajectories, finding hints of regular and chaotic behavior depending on the learning rate regime. Our findings are put in contrast to common wisdom on convergence properties of neural networks and dynamical systems theory. This work also contributes to the cross-fertilization of ideas between dynamical systems theory, network theory and machine learning","sentences":["The process of training an artificial neural network involves iteratively adapting its parameters so as to minimize the error of the network's prediction, when confronted with a learning task.","This iterative change can be naturally interpreted as a trajectory in network space -- a time series of networks -- and thus the training algorithm (e.g. gradient descent optimization of a suitable loss function) can be interpreted as a dynamical system in graph space.","In order to illustrate this interpretation, here we study the dynamical properties of this process by analyzing through this lens the network trajectories of a shallow neural network, and its evolution through learning a simple classification task.","We systematically consider different ranges of the learning rate and explore both the dynamical and orbital stability of the resulting network trajectories, finding hints of regular and chaotic behavior depending on the learning rate regime.","Our findings are put in contrast to common wisdom on convergence properties of neural networks and dynamical systems theory.","This work also contributes to the cross-fertilization of ideas between dynamical systems theory, network theory and machine learning"],"url":"http://arxiv.org/abs/2404.05782v1","category":"cs.LG"}
{"created":"2024-04-08 17:18:30","title":"Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic Segmentation for Satellite Imagery","abstract":"Satellite imagery is crucial for tasks like environmental monitoring and urban planning. Typically, it relies on semantic segmentation or Land Use Land Cover (LULC) classification to categorize each pixel. Despite the advancements brought about by Deep Neural Networks (DNNs), their performance in segmentation tasks is hindered by challenges such as limited availability of labeled data, class imbalance and the inherent variability and complexity of satellite images. In order to mitigate those issues, our study explores the effectiveness of a Cut-and-Paste augmentation technique for semantic segmentation in satellite images. We adapt this augmentation, which usually requires labeled instances, to the case of semantic segmentation. By leveraging the connected components in the semantic segmentation labels, we extract instances that are then randomly pasted during training. Using the DynamicEarthNet dataset and a U-Net model for evaluation, we found that this augmentation significantly enhances the mIoU score on the test set from 37.9 to 44.1. This finding highlights the potential of the Cut-and-Paste augmentation to improve the generalization capabilities of semantic segmentation models in satellite imagery.","sentences":["Satellite imagery is crucial for tasks like environmental monitoring and urban planning.","Typically, it relies on semantic segmentation or Land Use Land Cover (LULC) classification to categorize each pixel.","Despite the advancements brought about by Deep Neural Networks (DNNs), their performance in segmentation tasks is hindered by challenges such as limited availability of labeled data, class imbalance and the inherent variability and complexity of satellite images.","In order to mitigate those issues, our study explores the effectiveness of a Cut-and-Paste augmentation technique for semantic segmentation in satellite images.","We adapt this augmentation, which usually requires labeled instances, to the case of semantic segmentation.","By leveraging the connected components in the semantic segmentation labels, we extract instances that are then randomly pasted during training.","Using the DynamicEarthNet dataset and a U-Net model for evaluation, we found that this augmentation significantly enhances the mIoU score on the test set from 37.9 to 44.1.","This finding highlights the potential of the Cut-and-Paste augmentation to improve the generalization capabilities of semantic segmentation models in satellite imagery."],"url":"http://arxiv.org/abs/2404.05693v1","category":"cs.CV"}
{"created":"2024-04-08 16:55:49","title":"MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation","abstract":"In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.","sentences":["In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities.","As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows.","Addressing this need, MoMA specializes in subject-driven personalized image generation.","Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator.","This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model.","To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images.","Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness.","Our work is open-source, thereby providing universal access to these advancements."],"url":"http://arxiv.org/abs/2404.05674v1","category":"cs.CV"}
{"created":"2024-04-08 16:38:16","title":"Optimized Bandpasses for the Habitable Worlds Observatory's ExoEarth Survey","abstract":"A primary scientific goal of the future Habitable Worlds Observatory will be the direct detection and characterization of Earth-like planets. Estimates of the exoplanet yields for this concept will help guide mission design through detailed trade studies. It is therefore critical that yield estimation codes optimally adapt observations to the mission's performance parameters to ensure accurate trade studies. To aid in this, we implement wavelength optimization in yield calculations for the first time, allowing the yield code to determine the ideal detection and characterization bandpasses. We use this new capability to confirm the observational wavelength assumptions made for the LUVOIR-B study, namely that the optimum detection wavelength is 500 nm for the majority of targets and the optimum wavelength to detect water is near 1000 nm, given LUVOIR-B's assumed instrument performance parameters. We show that including the wavelength dependent albedo of an Earth twin as a prior provides no significant benefit to the yields of exoEarth candidates and caution against tuning observations to modern Earth twins. We also show that coronagraphs whose inner working angles are similar to step functions may benefit from wavelength optimization and demonstrate how wavelength-dependent instrument performance can impact the optimum wavelengths for detection and characterization. The optimization methods we implement automate wavelength selection and remove uncertainties regarding these choices, helping to adapt the observations to the instrument's performance parameters.","sentences":["A primary scientific goal of the future Habitable Worlds Observatory will be the direct detection and characterization of Earth-like planets.","Estimates of the exoplanet yields for this concept will help guide mission design through detailed trade studies.","It is therefore critical that yield estimation codes optimally adapt observations to the mission's performance parameters to ensure accurate trade studies.","To aid in this, we implement wavelength optimization in yield calculations for the first time, allowing the yield code to determine the ideal detection and characterization bandpasses.","We use this new capability to confirm the observational wavelength assumptions made for the LUVOIR-B study, namely that the optimum detection wavelength is 500 nm for the majority of targets and the optimum wavelength to detect water is near 1000 nm, given LUVOIR-B's assumed instrument performance parameters.","We show that including the wavelength dependent albedo of an Earth twin as a prior provides no significant benefit to the yields of exoEarth candidates and caution against tuning observations to modern Earth twins.","We also show that coronagraphs whose inner working angles are similar to step functions may benefit from wavelength optimization and demonstrate how wavelength-dependent instrument performance can impact the optimum wavelengths for detection and characterization.","The optimization methods we implement automate wavelength selection and remove uncertainties regarding these choices, helping to adapt the observations to the instrument's performance parameters."],"url":"http://arxiv.org/abs/2404.05654v2","category":"astro-ph.EP"}
{"created":"2024-04-08 16:36:07","title":"Group-specific discriminant analysis reveals statistically validated sex differences in lateralization of brain functional network","abstract":"Lateralization is a fundamental feature of the human brain, where sex differences have been observed. Conventional studies in neuroscience on sex-specific lateralization are typically conducted on univariate statistical comparisons between male and female groups. However, these analyses often lack effective validation of group specificity. Here, we formulate modeling sex differences in lateralization of functional networks as a dual-classification problem, consisting of first-order classification for left vs. right functional networks and second-order classification for male vs. female models. To capture sex-specific patterns, we develop the Group-Specific Discriminant Analysis (GSDA) for first-order classification. The evaluation on two public neuroimaging datasets demonstrates the efficacy of GSDA in learning sex-specific models from functional networks, achieving a significant improvement in group specificity over baseline methods. The major sex differences are in the strength of lateralization and the interactions within and between lobes. The GSDA-based method is generic in nature and can be adapted to other group-specific analyses such as handedness-specific or disease-specific analyses.","sentences":["Lateralization is a fundamental feature of the human brain, where sex differences have been observed.","Conventional studies in neuroscience on sex-specific lateralization are typically conducted on univariate statistical comparisons between male and female groups.","However, these analyses often lack effective validation of group specificity.","Here, we formulate modeling sex differences in lateralization of functional networks as a dual-classification problem, consisting of first-order classification for left vs. right functional networks and second-order classification for male vs. female models.","To capture sex-specific patterns, we develop the Group-Specific Discriminant Analysis (GSDA) for first-order classification.","The evaluation on two public neuroimaging datasets demonstrates the efficacy of GSDA in learning sex-specific models from functional networks, achieving a significant improvement in group specificity over baseline methods.","The major sex differences are in the strength of lateralization and the interactions within and between lobes.","The GSDA-based method is generic in nature and can be adapted to other group-specific analyses such as handedness-specific or disease-specific analyses."],"url":"http://arxiv.org/abs/2404.05781v1","category":"q-bio.NC"}
{"created":"2024-04-08 15:37:56","title":"Local behaviour of non-local hypoelliptic equations: divergence form","abstract":"We derive the Strong Harnack inequality for a class of hypoelliptic integro-differential equations in divergence form. The proof is based on a priori estimates, and as such extends the first non-stochastic approach of the non-local parabolic Strong Harnack inequality by Kassmann-Weidner [arXiv:2303.05975] to hypoelliptic equations. In a first step, we derive a local bound on the non-local tail on upper level sets by exploiting the coercivity of the cross terms. In a second step, we perform a De Giorgi argument in $L^1$, since we control the tail term only in $L^1$. This yields a linear $L^1$ to $L^\\infty$ bound. Consequentially, we prove polynomial upper and exponential lower bounds on the fundamental solution by adapting Aronson's method to non-local hypoelliptic equations.","sentences":["We derive the Strong Harnack inequality for a class of hypoelliptic integro-differential equations in divergence form.","The proof is based on a priori estimates, and as such extends the first non-stochastic approach of the non-local parabolic Strong Harnack inequality by Kassmann-Weidner [arXiv:2303.05975] to hypoelliptic equations.","In a first step, we derive a local bound on the non-local tail on upper level sets by exploiting the coercivity of the cross terms.","In a second step, we perform a De Giorgi argument in $L^1$, since we control the tail term only in $L^1$. This yields a linear $L^1$ to $L^\\infty$ bound.","Consequentially, we prove polynomial upper and exponential lower bounds on the fundamental solution by adapting Aronson's method to non-local hypoelliptic equations."],"url":"http://arxiv.org/abs/2404.05612v1","category":"math.AP"}
{"created":"2024-04-08 15:29:46","title":"A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion","abstract":"Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution. However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly. To address this, we propose a training-free plug-and-play watermark framework for SDs. Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility. Furthermore, it performs robustly under various attacks. We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model.","sentences":["Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability.","This has also raised security concerns on social media, as malicious users can create and disseminate harmful content.","Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution.","However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly.","To address this, we propose a training-free plug-and-play watermark framework for SDs.","Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process.","Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility.","Furthermore, it performs robustly under various attacks.","We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model."],"url":"http://arxiv.org/abs/2404.05607v1","category":"cs.CV"}
{"created":"2024-04-08 14:58:52","title":"Towards More General Video-based Deepfake Detection through Facial Feature Guided Adaptation for Foundation Model","abstract":"With the rise of deep learning, generative models have enabled the creation of highly realistic synthetic images, presenting challenges due to their potential misuse. While research in Deepfake detection has grown rapidly in response, many detection methods struggle with unseen Deepfakes generated by new synthesis techniques. To address this generalisation challenge, we propose a novel Deepfake detection approach by adapting rich information encoded inside the Foundation Models with rich information encoded inside, specifically using the image encoder from CLIP which has demonstrated strong zero-shot capability for downstream tasks. Inspired by the recent advances of parameter efficient fine-tuning, we propose a novel side-network-based decoder to extract spatial and temporal cues from the given video clip, with the promotion of the Facial Component Guidance (FCG) to guidencourage the spatial feature to include features of key facial parts for more robust and general Deepfake detection. Through extensive cross-dataset evaluations, our approach exhibits superior effectiveness in identifying unseen Deepfake samples, achieving notable performance improvementsuccess even with limited training samples and manipulation types. Our model secures an average performance enhancement of 0.9% AUROC in cross-dataset assessments comparing with state-of-the-art methods, especiallytablishing a significant lead of achieving 4.4% improvement on the challenging DFDC dataset.","sentences":["With the rise of deep learning, generative models have enabled the creation of highly realistic synthetic images, presenting challenges due to their potential misuse.","While research in Deepfake detection has grown rapidly in response, many detection methods struggle with unseen Deepfakes generated by new synthesis techniques.","To address this generalisation challenge, we propose a novel Deepfake detection approach by adapting rich information encoded inside the Foundation Models with rich information encoded inside, specifically using the image encoder from CLIP which has demonstrated strong zero-shot capability for downstream tasks.","Inspired by the recent advances of parameter efficient fine-tuning, we propose a novel side-network-based decoder to extract spatial and temporal cues from the given video clip, with the promotion of the Facial Component Guidance (FCG) to guidencourage the spatial feature to include features of key facial parts for more robust and general Deepfake detection.","Through extensive cross-dataset evaluations, our approach exhibits superior effectiveness in identifying unseen Deepfake samples, achieving notable performance improvementsuccess even with limited training samples and manipulation types.","Our model secures an average performance enhancement of 0.9% AUROC in cross-dataset assessments comparing with state-of-the-art methods, especiallytablishing a significant lead of achieving 4.4% improvement on the challenging DFDC dataset."],"url":"http://arxiv.org/abs/2404.05583v1","category":"cs.CV"}
{"created":"2024-04-08 14:57:16","title":"Learning Prehensile Dexterity by Imitating and Emulating State-only Observations","abstract":"When humans learn physical skills (e.g., learn to play tennis), we tend to first observe and learn what an expert is doing. But this is often insufficient. Therefore, we subsequently engage in practice, where we try to emulate the expert. Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations. CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system. This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels. The second stage involves emulation: learn a motion refinement policy to make adjustments to the motion prior of the robot hand such that the desired object motion is reenacted. CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no need for additional teleoperated or labeled demonstrations). Detailed experiments reveal that i) Imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases.","sentences":["When humans learn physical skills (e.g., learn to play tennis), we tend to first observe and learn what an expert is doing.","But this is often insufficient.","Therefore, we subsequently engage in practice, where we try to emulate the expert.","Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations.","CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system.","This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels.","The second stage involves emulation: learn a motion refinement policy to make adjustments to the motion prior of the robot hand such that the desired object motion is reenacted.","CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no need for additional teleoperated or labeled demonstrations).","Detailed experiments reveal that i)","Imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases."],"url":"http://arxiv.org/abs/2404.05582v1","category":"cs.RO"}
{"created":"2024-04-08 14:52:48","title":"Dynamic Backtracking in GFlowNets: Enhancing Decision Steps with Reward-Dependent Adjustment Mechanisms","abstract":"Generative Flow Networks (GFlowNets) are probabilistic models predicated on Markov flows, employing specific amortization algorithms to learn stochastic policies that generate compositional substances including biomolecules, chemical materials, and more. Demonstrating formidable prowess in generating high-performance biochemical molecules, GFlowNets accelerate the discovery of scientific substances, effectively circumventing the time-consuming, labor-intensive, and costly shortcomings intrinsic to conventional material discovery. However, previous work often struggles to accumulate exploratory experience and is prone to becoming disoriented within expansive sampling spaces. Attempts to address this issue, such as LS-GFN, are limited to local greedy searches and lack broader global adjustments. This paper introduces a novel GFlowNets variant, the Dynamic Backtracking GFN (DB-GFN), which enhances the adaptability of decision-making steps through a reward-based dynamic backtracking mechanism. DB-GFN permits backtracking during the network construction process according to the current state's reward value, thus correcting disadvantageous decisions and exploring alternative pathways during the exploration process. Applied to generative tasks of biochemical molecules and genetic material sequences, DB-GFN surpasses existing GFlowNets models and traditional reinforcement learning methods in terms of sample quality, exploration sample quantity, and training convergence speed. Furthermore, the orthogonal nature of DB-GFN suggests its potential as a powerful tool for future improvements in GFlowNets, with the promise of integrating with other strategies to achieve more efficient search performance.","sentences":["Generative Flow Networks (GFlowNets) are probabilistic models predicated on Markov flows, employing specific amortization algorithms to learn stochastic policies that generate compositional substances including biomolecules, chemical materials, and more.","Demonstrating formidable prowess in generating high-performance biochemical molecules, GFlowNets accelerate the discovery of scientific substances, effectively circumventing the time-consuming, labor-intensive, and costly shortcomings intrinsic to conventional material discovery.","However, previous work often struggles to accumulate exploratory experience and is prone to becoming disoriented within expansive sampling spaces.","Attempts to address this issue, such as LS-GFN, are limited to local greedy searches and lack broader global adjustments.","This paper introduces a novel GFlowNets variant, the Dynamic Backtracking GFN (DB-GFN), which enhances the adaptability of decision-making steps through a reward-based dynamic backtracking mechanism.","DB-GFN permits backtracking during the network construction process according to the current state's reward value, thus correcting disadvantageous decisions and exploring alternative pathways during the exploration process.","Applied to generative tasks of biochemical molecules and genetic material sequences, DB-GFN surpasses existing GFlowNets models and traditional reinforcement learning methods in terms of sample quality, exploration sample quantity, and training convergence speed.","Furthermore, the orthogonal nature of DB-GFN suggests its potential as a powerful tool for future improvements in GFlowNets, with the promise of integrating with other strategies to achieve more efficient search performance."],"url":"http://arxiv.org/abs/2404.05576v2","category":"cs.LG"}
{"created":"2024-04-08 14:46:32","title":"Wetting on Silicone Surfaces","abstract":"Silicone is frequently used as a model system to investigate and tune wetting on soft materials. Silicone is biocompatible and shows excellent thermal, chemical, and UV stability. Moreover, the mechanical properties of the surface can be easily varied by several orders of magnitude in a controlled manner. Polydimethylsiloxane (PDMS) is a popular choice for coating applications such as lubrication, self-cleaning, and drag reduction, facilitated by low surface energy. Aiming to understand the underlying interactions and forces, motivated numerous and detailed investigations of the static and dynamic wetting behavior of drops on PDMS-based surfaces. Here, we recognize the three most prevalent PDMS surface variants, namely liquid-infused (SLIPS/LIS), elastomeric, and liquid-like (SOCAL) surfaces. To understand, optimize, and tune the wetting properties of these PDMS surfaces, we review and compare their similarities and differences by discussing (i) the chemical and molecular structure, and (ii) the static and dynamic wetting behavior. We also provide (iii) an overview of methods and techniques to characterize PDMS-based surfaces and their wetting behavior. The static and dynamic wetting ridge is given particular attention, as it dominates energy dissipation, adhesion, and friction of sliding drops and influences the durability of the surfaces. We also discuss special features such as cloaking and wetting-induced phase separation. Key challenges and opportunities of these three surface variants are outlined.","sentences":["Silicone is frequently used as a model system to investigate and tune wetting on soft materials.","Silicone is biocompatible and shows excellent thermal, chemical, and UV stability.","Moreover, the mechanical properties of the surface can be easily varied by several orders of magnitude in a controlled manner.","Polydimethylsiloxane (PDMS) is a popular choice for coating applications such as lubrication, self-cleaning, and drag reduction, facilitated by low surface energy.","Aiming to understand the underlying interactions and forces, motivated numerous and detailed investigations of the static and dynamic wetting behavior of drops on PDMS-based surfaces.","Here, we recognize the three most prevalent PDMS surface variants, namely liquid-infused (SLIPS/LIS), elastomeric, and liquid-like (SOCAL) surfaces.","To understand, optimize, and tune the wetting properties of these PDMS surfaces, we review and compare their similarities and differences by discussing (i) the chemical and molecular structure, and (ii) the static and dynamic wetting behavior.","We also provide (iii) an overview of methods and techniques to characterize PDMS-based surfaces and their wetting behavior.","The static and dynamic wetting ridge is given particular attention, as it dominates energy dissipation, adhesion, and friction of sliding drops and influences the durability of the surfaces.","We also discuss special features such as cloaking and wetting-induced phase separation.","Key challenges and opportunities of these three surface variants are outlined."],"url":"http://arxiv.org/abs/2404.05571v1","category":"cond-mat.soft"}
{"created":"2024-04-08 14:30:42","title":"TIM: A Time Interval Machine for Audio-Visual Action Recognition","abstract":"Diverse actions give rise to rich audio-visual signals in long videos. Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels. We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events. We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input. The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action.   We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy. Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test. Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance. Code and models at: https://github.com/JacobChalk/TIM","sentences":["Diverse actions give rise to rich audio-visual signals in long videos.","Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels.","We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events.","We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input.","The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action.   ","We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition.","On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy.","Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test.","Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance.","Code and models at: https://github.com/JacobChalk/TIM"],"url":"http://arxiv.org/abs/2404.05559v2","category":"cs.CV"}
{"created":"2024-04-08 14:19:25","title":"Improving Quantum and Classical Decomposition Methods for Vehicle Routing","abstract":"Quantum computing is a promising technology to address combinatorial optimization problems, for example via the quantum approximate optimization algorithm (QAOA). Its potential, however, hinges on scaling toy problems to sizes relevant for industry. In this study, we address this challenge by an elaborate combination of two decomposition methods, namely graph shrinking and circuit cutting. Graph shrinking reduces the problem size before encoding into QAOA circuits, while circuit cutting decomposes quantum circuits into fragments for execution on medium-scale quantum computers. Our shrinking method adaptively reduces the problem such that the resulting QAOA circuits are particularly well-suited for circuit cutting. Moreover, we integrate two cutting techniques which allows us to run the resulting circuit fragments sequentially on the same device. We demonstrate the utility of our method by successfully applying it to the archetypical traveling salesperson problem (TSP) which often occurs as a sub-problem in practically relevant vehicle routing applications. For a TSP with seven cities, we are able to retrieve an optimum solution by consecutively running two 7-qubit QAOA circuits. Without decomposition methods, we would require five times as many qubits. Our results offer insights into the performance of algorithms for combinatorial optimization problems within the constraints of current quantum technology.","sentences":["Quantum computing is a promising technology to address combinatorial optimization problems, for example via the quantum approximate optimization algorithm (QAOA).","Its potential, however, hinges on scaling toy problems to sizes relevant for industry.","In this study, we address this challenge by an elaborate combination of two decomposition methods, namely graph shrinking and circuit cutting.","Graph shrinking reduces the problem size before encoding into QAOA circuits, while circuit cutting decomposes quantum circuits into fragments for execution on medium-scale quantum computers.","Our shrinking method adaptively reduces the problem such that the resulting QAOA circuits are particularly well-suited for circuit cutting.","Moreover, we integrate two cutting techniques which allows us to run the resulting circuit fragments sequentially on the same device.","We demonstrate the utility of our method by successfully applying it to the archetypical traveling salesperson problem (TSP) which often occurs as a sub-problem in practically relevant vehicle routing applications.","For a TSP with seven cities, we are able to retrieve an optimum solution by consecutively running two 7-qubit QAOA circuits.","Without decomposition methods, we would require five times as many qubits.","Our results offer insights into the performance of algorithms for combinatorial optimization problems within the constraints of current quantum technology."],"url":"http://arxiv.org/abs/2404.05551v1","category":"quant-ph"}
{"created":"2024-04-08 14:06:52","title":"Cell-Free Multi-User MIMO Equalization via In-Context Learning","abstract":"Large pre-trained sequence models, such as transformers, excel as few-shot learners capable of in-context learning (ICL). In ICL, a model is trained to adapt its operation to a new task based on limited contextual information, typically in the form of a few training examples for the given task. Previous work has explored the use of ICL for channel equalization in single-user multi-input and multiple-output (MIMO) systems. In this work, we demonstrate that ICL can be also used to tackle the problem of multi-user equalization in cell-free MIMO systems with limited fronthaul capacity. In this scenario, a task is defined by channel statistics, signal-to-noise ratio, and modulation schemes. The context encompasses the users' pilot sequences, the corresponding quantized received signals, and the current received data signal. Different prompt design strategies are proposed and evaluated that encompass also large-scale fading and modulation information. Experiments demonstrate that ICL-based equalization provides estimates with lower mean squared error as compared to the linear minimum mean squared error equalizer, especially in the presence of limited fronthaul capacity and pilot contamination.","sentences":["Large pre-trained sequence models, such as transformers, excel as few-shot learners capable of in-context learning (ICL).","In ICL, a model is trained to adapt its operation to a new task based on limited contextual information, typically in the form of a few training examples for the given task.","Previous work has explored the use of ICL for channel equalization in single-user multi-input and multiple-output (MIMO) systems.","In this work, we demonstrate that ICL can be also used to tackle the problem of multi-user equalization in cell-free MIMO systems with limited fronthaul capacity.","In this scenario, a task is defined by channel statistics, signal-to-noise ratio, and modulation schemes.","The context encompasses the users' pilot sequences, the corresponding quantized received signals, and the current received data signal.","Different prompt design strategies are proposed and evaluated that encompass also large-scale fading and modulation information.","Experiments demonstrate that ICL-based equalization provides estimates with lower mean squared error as compared to the linear minimum mean squared error equalizer, especially in the presence of limited fronthaul capacity and pilot contamination."],"url":"http://arxiv.org/abs/2404.05538v1","category":"cs.IT"}
{"created":"2024-04-08 13:20:48","title":"Stability Mechanisms for Predictive Safety Filters","abstract":"Predictive safety filters enable the integration of potentially unsafe learning-based control approaches and humans into safety-critical systems. In addition to simple constraint satisfaction, many control problems involve additional stability requirements that may vary depending on the specific use case or environmental context. In this work, we address this problem by augmenting predictive safety filters with stability guarantees, ranging from bounded convergence to uniform asymptotic stability. The proposed framework extends well-known stability results from model predictive control (MPC) theory while supporting commonly used design techniques. As a result, straightforward extensions to dynamic trajectory tracking problems can be easily adapted, as outlined in this article. The practicality of the framework is demonstrated using an automotive advanced driver assistance scenario, involving a reference trajectory stabilization problem.","sentences":["Predictive safety filters enable the integration of potentially unsafe learning-based control approaches and humans into safety-critical systems.","In addition to simple constraint satisfaction, many control problems involve additional stability requirements that may vary depending on the specific use case or environmental context.","In this work, we address this problem by augmenting predictive safety filters with stability guarantees, ranging from bounded convergence to uniform asymptotic stability.","The proposed framework extends well-known stability results from model predictive control (MPC) theory while supporting commonly used design techniques.","As a result, straightforward extensions to dynamic trajectory tracking problems can be easily adapted, as outlined in this article.","The practicality of the framework is demonstrated using an automotive advanced driver assistance scenario, involving a reference trajectory stabilization problem."],"url":"http://arxiv.org/abs/2404.05496v1","category":"cs.SY"}
{"created":"2024-04-08 13:13:12","title":"Design, fabrication and test of a 5 GHz klystron based on the kladistron principle","abstract":"A new bunching method, named \"kladistron\" has been developed at CEA in order to provide high efficiency klystrons. A first \"kladistron\" prototype was designed and realized. It was adapted from the 4.9 GHz TH2166 from Thales, where the interaction line was transformed from 6 to 16 cavities. The design and fabrication phases of this prototype are developed in this paper. The kladistron prototype was tested in Thales facility. Its efficiency is finally lower (41 %) than expected (55 %), moreover it presents a spurious oscillation at 4.96 GHz. After analysis of the experimental results, it is concluded that the discrepancy between design and real frequencies is the cause for the low efficiency while the spurious oscillation results from a high gain peak at 4.96 GHz.","sentences":["A new bunching method, named \"kladistron\" has been developed at CEA in order to provide high efficiency klystrons.","A first \"kladistron\" prototype was designed and realized.","It was adapted from the 4.9 GHz TH2166 from Thales, where the interaction line was transformed from 6 to 16 cavities.","The design and fabrication phases of this prototype are developed in this paper.","The kladistron prototype was tested in Thales facility.","Its efficiency is finally lower (41 %) than expected (55 %), moreover it presents a spurious oscillation at 4.96 GHz.","After analysis of the experimental results, it is concluded that the discrepancy between design and real frequencies is the cause for the low efficiency while the spurious oscillation results from a high gain peak at 4.96 GHz."],"url":"http://arxiv.org/abs/2404.05493v1","category":"physics.acc-ph"}
{"created":"2024-04-08 12:50:36","title":"Towards Reconfigurable Linearizable Reads","abstract":"Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single machine that performs client operations one at a time. To reduce the performance cost of providing this illusion, many specialized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance. The main difference between these specialized algorithms is their performance under different workloads. Unfortunately, since a datastore's workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore's performance is often suboptimal as it cannot adapt to workload changes. In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime. The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems. Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations. This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes.","sentences":["Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single machine that performs client operations one at a time.","To reduce the performance cost of providing this illusion, many specialized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance.","The main difference between these specialized algorithms is their performance under different workloads.","Unfortunately, since a datastore's workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore's performance is often suboptimal as it cannot adapt to workload changes.","In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime.","The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems.","Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations.","This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes."],"url":"http://arxiv.org/abs/2404.05470v1","category":"cs.DC"}
{"created":"2024-04-08 11:54:49","title":"Test-Time Zero-Shot Temporal Action Localization","abstract":"Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.","sentences":["Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training.","Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data.","While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications.","Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos.","These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data.","To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL).","In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM).","T3AL operates in three steps.","First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video.","Then, action localization is performed adopting a novel procedure inspired by self-supervised learning.","Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals.","We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets.","Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach."],"url":"http://arxiv.org/abs/2404.05426v1","category":"cs.CV"}
{"created":"2024-04-08 11:20:28","title":"Anatomical Conditioning for Contrastive Unpaired Image-to-Image Translation of Optical Coherence Tomography Images","abstract":"For a unified analysis of medical images from different modalities, data harmonization using image-to-image (I2I) translation is desired. We study this problem employing an optical coherence tomography (OCT) data set of Spectralis-OCT and Home-OCT images. I2I translation is challenging because the images are unpaired, and a bijective mapping does not exist due to the information discrepancy between both domains. This problem has been addressed by the Contrastive Learning for Unpaired I2I Translation (CUT) approach, but it reduces semantic consistency. To restore the semantic consistency, we support the style decoder using an additional segmentation decoder. Our approach increases the similarity between the style-translated images and the target distribution. Importantly, we improve the segmentation of biomarkers in Home-OCT images in an unsupervised domain adaptation scenario. Our data harmonization approach provides potential for the monitoring of diseases, e.g., age related macular disease, using different OCT devices.","sentences":["For a unified analysis of medical images from different modalities, data harmonization using image-to-image (I2I) translation is desired.","We study this problem employing an optical coherence tomography (OCT) data set of Spectralis-OCT and Home-OCT images.","I2I translation is challenging because the images are unpaired, and a bijective mapping does not exist due to the information discrepancy between both domains.","This problem has been addressed by the Contrastive Learning for Unpaired I2I Translation (CUT) approach, but it reduces semantic consistency.","To restore the semantic consistency, we support the style decoder using an additional segmentation decoder.","Our approach increases the similarity between the style-translated images and the target distribution.","Importantly, we improve the segmentation of biomarkers in Home-OCT images in an unsupervised domain adaptation scenario.","Our data harmonization approach provides potential for the monitoring of diseases, e.g., age related macular disease, using different OCT devices."],"url":"http://arxiv.org/abs/2404.05409v1","category":"eess.IV"}
{"created":"2024-04-08 10:54:23","title":"On an optimal AFEM for elastoplasticity","abstract":"In this paper, optimal convergence for an adaptive finite element algorithm for elastoplasticity is considered. To this end, the proposed adaptive algorithm is established within the abstract framework of the axioms of adaptivity [Comput. Math. Appl., 67(6) (2014), 1195-1253], which provides a specific proceeding to prove the optimal convergence of the scheme. The proceeding is based on verifying four axioms, which ensure the optimal convergence. The verification is done by using results from [Numer. Math., 132(1) (2016), 131-154], which presents an alternative approach to optimality without explicitly relying on the axioms.","sentences":["In this paper, optimal convergence for an adaptive finite element algorithm for elastoplasticity is considered.","To this end, the proposed adaptive algorithm is established within the abstract framework of the axioms of adaptivity [Comput.","Math.","Appl., 67(6) (2014), 1195-1253], which provides a specific proceeding to prove the optimal convergence of the scheme.","The proceeding is based on verifying four axioms, which ensure the optimal convergence.","The verification is done by using results from [Numer.","Math., 132(1) (2016), 131-154], which presents an alternative approach to optimality without explicitly relying on the axioms."],"url":"http://arxiv.org/abs/2404.05395v1","category":"math.NA"}
{"created":"2024-04-08 10:45:29","title":"Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance","abstract":"Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG.","sentences":["Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space.","However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality.","To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models.","Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step.","In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions.","Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level.","Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost.","our codes are available at https://github.com/SmilesDZgk/S-CFG."],"url":"http://arxiv.org/abs/2404.05384v1","category":"cs.CV"}
{"created":"2024-04-08 09:57:02","title":"A parameter-free clustering algorithm for missing datasets","abstract":"Missing datasets, in which some objects have missing values in certain dimensions, are prevalent in the Real-world. Existing clustering algorithms for missing datasets first impute the missing values and then perform clustering. However, both the imputation and clustering processes require input parameters. Too many input parameters inevitably increase the difficulty of obtaining accurate clustering results. Although some studies have shown that decision graphs can replace the input parameters of clustering algorithms, current decision graphs require equivalent dimensions among objects and are therefore not suitable for missing datasets. To this end, we propose a Single-Dimensional Clustering algorithm, i.e., SDC. SDC, which removes the imputation process and adapts the decision graph to the missing datasets by splitting dimension and partition intersection fusion, can obtain valid clustering results on the missing datasets without input parameters. Experiments demonstrate that, across three evaluation metrics, SDC outperforms baseline algorithms by at least 13.7%(NMI), 23.8%(ARI), and 8.1%(Purity).","sentences":["Missing datasets, in which some objects have missing values in certain dimensions, are prevalent in the Real-world.","Existing clustering algorithms for missing datasets first impute the missing values and then perform clustering.","However, both the imputation and clustering processes require input parameters.","Too many input parameters inevitably increase the difficulty of obtaining accurate clustering results.","Although some studies have shown that decision graphs can replace the input parameters of clustering algorithms, current decision graphs require equivalent dimensions among objects and are therefore not suitable for missing datasets.","To this end, we propose a Single-Dimensional Clustering algorithm, i.e., SDC.","SDC, which removes the imputation process and adapts the decision graph to the missing datasets by splitting dimension and partition intersection fusion, can obtain valid clustering results on the missing datasets without input parameters.","Experiments demonstrate that, across three evaluation metrics, SDC outperforms baseline algorithms by at least 13.7%(NMI), 23.8%(ARI), and 8.1%(Purity)."],"url":"http://arxiv.org/abs/2404.05363v1","category":"cs.LG"}
{"created":"2024-04-08 09:53:42","title":"Optimal Controller Realizations against False Data Injections in Cooperative Driving","abstract":"To enhance the robustness of cooperative driving to cyberattacks, we study a controller-oriented approach to mitigate the effect of a class of False-Data Injection (FDI) attacks. By reformulating a given dynamic Cooperative Adaptive Cruise Control (CACC) scheme (the base controller), we recognize that the base controller can be represented by a class of new but equivalent controllers (base controller realizations) that exhibits the same platooning behavior with varying robustness in the presence of attacks. We propose a prescriptive synthesis framework where the base controller and the system dynamics are written in new coordinates via an invertible coordinate transformation on the controller state. Because the input-output behavior is invariant under coordinate transformations, the input-output behavior is unaffected (so controller realizations do not change the system's closed-loop performance). However, each base controller realization may require a different combination of sensors. To this end, we obtain the optimal combination of sensors that minimizes the effect of FDI attacks by solving a Linear Matrix Inequality (LMI), while quantifying the FDI's attack impact through reachability analysis. Through simulation studies, we demonstrate that this approach enhances the robustness of cooperative driving, without relying on a detection scheme and maintaining all system properties.","sentences":["To enhance the robustness of cooperative driving to cyberattacks, we study a controller-oriented approach to mitigate the effect of a class of False-Data Injection (FDI) attacks.","By reformulating a given dynamic Cooperative Adaptive Cruise Control (CACC) scheme (the base controller), we recognize that the base controller can be represented by a class of new but equivalent controllers (base controller realizations) that exhibits the same platooning behavior with varying robustness in the presence of attacks.","We propose a prescriptive synthesis framework where the base controller and the system dynamics are written in new coordinates via an invertible coordinate transformation on the controller state.","Because the input-output behavior is invariant under coordinate transformations, the input-output behavior is unaffected (so controller realizations do not change the system's closed-loop performance).","However, each base controller realization may require a different combination of sensors.","To this end, we obtain the optimal combination of sensors that minimizes the effect of FDI attacks by solving a Linear Matrix Inequality (LMI), while quantifying the FDI's attack impact through reachability analysis.","Through simulation studies, we demonstrate that this approach enhances the robustness of cooperative driving, without relying on a detection scheme and maintaining all system properties."],"url":"http://arxiv.org/abs/2404.05361v1","category":"cs.SY"}
{"created":"2024-04-08 09:52:28","title":"Global solutions to quadratic systems of stochastic reaction-diffusion equations in space-dimension two","abstract":"We prove the existence of global-in-time regular solutions to a system of stochastic quadratic reaction-diffusion equations. Global-in-time existence is based on a $L^\\infty$-estimate obtained by an approach {\\`a} la De Giorgi, as in [GoudonVasseur10]. The adaptation of this technique to the stochastic case requires in its final step an $L^2\\ln(L^2)$-bound, furnished by an estimate by duality on the entropy inequality, as in [DesvillettesFellnerPierreVovelle07]. In our stochastic context, and similarly to [DebusscheRoselloVovelle2021], we need to solve a backward SPDE to exploit the duality technique","sentences":["We prove the existence of global-in-time regular solutions to a system of stochastic quadratic reaction-diffusion equations.","Global-in-time existence is based on a $L^\\infty$-estimate obtained by an approach {\\`a} la De Giorgi, as in [GoudonVasseur10].","The adaptation of this technique to the stochastic case requires in its final step an $L^2\\ln(L^2)$-bound, furnished by an estimate by duality on the entropy inequality, as in [DesvillettesFellnerPierreVovelle07].","In our stochastic context, and similarly to [DebusscheRoselloVovelle2021], we need to solve a backward SPDE to exploit the duality technique"],"url":"http://arxiv.org/abs/2404.05360v1","category":"math.AP"}
{"created":"2024-04-08 09:48:09","title":"On a port-Hamiltonian formulation and structure-preserving numerical approximations for thermodynamic compressible fluid flow","abstract":"The high volatility of renewable energies calls for more energy efficiency. Thus, different physical systems need to be coupled efficiently although they run on various time scales. Here, the port-Hamiltonian (pH) modeling framework comes into play as it has several advantages, e.g., physical properties are encoded in the system structure and systems running on different time scales can be coupled easily. Additionally, pH systems coupled by energy-preserving conditions are still pH. Furthermore, in the energy transition hydrogen becomes an important player and unlike in natural gas, its temperature-dependence is of importance. Thus, we introduce an infinite dimensional pH formulation of the compressible non-isothermal Euler equations to model flow with temperature-dependence. We set up the underlying Stokes-Dirac structure and deduce the boundary port variables. We introduce coupling conditions into our pH formulation, such that the whole network system is pH itself. This is achieved by using energy-preserving coupling conditions, i.e., mass conservation and equality of total enthalpy, at the coupling nodes. Furthermore, to close the system a third coupling condition is needed. Here, equality of the outgoing entropy at coupling nodes is used and included into our systems in a structure-preserving way. Following that, we adapt the structure-preserving aproximation methods from the isothermal to the non-isothermal case. Academic numerical examples will support our analytical findings.","sentences":["The high volatility of renewable energies calls for more energy efficiency.","Thus, different physical systems need to be coupled efficiently although they run on various time scales.","Here, the port-Hamiltonian (pH) modeling framework comes into play as it has several advantages, e.g., physical properties are encoded in the system structure and systems running on different time scales can be coupled easily.","Additionally, pH systems coupled by energy-preserving conditions are still pH. Furthermore, in the energy transition hydrogen becomes an important player and unlike in natural gas, its temperature-dependence is of importance.","Thus, we introduce an infinite dimensional pH formulation of the compressible non-isothermal Euler equations to model flow with temperature-dependence.","We set up the underlying Stokes-Dirac structure and deduce the boundary port variables.","We introduce coupling conditions into our pH formulation, such that the whole network system is pH itself.","This is achieved by using energy-preserving coupling conditions, i.e., mass conservation and equality of total enthalpy, at the coupling nodes.","Furthermore, to close the system a third coupling condition is needed.","Here, equality of the outgoing entropy at coupling nodes is used and included into our systems in a structure-preserving way.","Following that, we adapt the structure-preserving aproximation methods from the isothermal to the non-isothermal case.","Academic numerical examples will support our analytical findings."],"url":"http://arxiv.org/abs/2404.05358v1","category":"math.NA"}
{"created":"2024-04-08 09:38:22","title":"Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized Smoothing","abstract":"Randomized smoothing is the primary certified robustness method for accessing the robustness of deep learning models to adversarial perturbations in the l2-norm, by adding isotropic Gaussian noise to the input image and returning the majority votes over the base classifier. Theoretically, it provides a certified norm bound, ensuring predictions of adversarial examples are stable within this bound. A notable constraint limiting widespread adoption is the necessity to retrain base models entirely from scratch to attain a robust version. This is because the base model fails to learn the noise-augmented data distribution to give an accurate vote. One intuitive way to overcome this challenge is to involve a custom-trained denoiser to eliminate the noise. However, this approach is inefficient and sub-optimal. Inspired by recent large model training procedures, we explore an alternative way named PEFTSmoothing to adapt the base model to learn the Gaussian noise-augmented data with Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box settings. Extensive results demonstrate the effectiveness and efficiency of PEFTSmoothing, which allow us to certify over 98% accuracy for ViT on CIFAR-10, 20% higher than SoTA denoised smoothing, and over 61% accuracy on ImageNet which is 30% higher than CNN-based denoiser and comparable to the Diffusion-based denoiser.","sentences":["Randomized smoothing is the primary certified robustness method for accessing the robustness of deep learning models to adversarial perturbations in the l2-norm, by adding isotropic Gaussian noise to the input image and returning the majority votes over the base classifier.","Theoretically, it provides a certified norm bound, ensuring predictions of adversarial examples are stable within this bound.","A notable constraint limiting widespread adoption is the necessity to retrain base models entirely from scratch to attain a robust version.","This is because the base model fails to learn the noise-augmented data distribution to give an accurate vote.","One intuitive way to overcome this challenge is to involve a custom-trained denoiser to eliminate the noise.","However, this approach is inefficient and sub-optimal.","Inspired by recent large model training procedures, we explore an alternative way named PEFTSmoothing to adapt the base model to learn the Gaussian noise-augmented data with Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box settings.","Extensive results demonstrate the effectiveness and efficiency of PEFTSmoothing, which allow us to certify over 98% accuracy for ViT on CIFAR-10, 20% higher than SoTA denoised smoothing, and over 61% accuracy on ImageNet which is 30% higher than CNN-based denoiser and comparable to the Diffusion-based denoiser."],"url":"http://arxiv.org/abs/2404.05350v1","category":"cs.LG"}
{"created":"2024-04-08 09:29:34","title":"Non-linear Model Predictive Control for Multi-task GPS-free Autonomous Navigation in Vineyards","abstract":"Autonomous navigation is the foundation of agricultural robots. This paper focuses on developing an advanced autonomous navigation system for a rover operating within row-based crops. A position-agnostic system is proposed to address the challenging situation when standard localization methods, like GPS, fail due to unfavorable weather or obstructed signals. This breakthrough is especially vital in densely vegetated regions, including areas covered by thick tree canopies or pergola vineyards. This work proposed a novel system that leverages a single RGB-D camera and a Non-linear Model Predictive Control strategy to navigate through entire rows, adapting to various crop spacing. The presented solution demonstrates versatility in handling diverse crop densities, environmental factors, and multiple navigation tasks to support agricultural activities at an extremely cost-effective implementation. Experimental validation in simulated and real vineyards underscores the system's robustness and competitiveness in both standard row traversal and target objects approach.","sentences":["Autonomous navigation is the foundation of agricultural robots.","This paper focuses on developing an advanced autonomous navigation system for a rover operating within row-based crops.","A position-agnostic system is proposed to address the challenging situation when standard localization methods, like GPS, fail due to unfavorable weather or obstructed signals.","This breakthrough is especially vital in densely vegetated regions, including areas covered by thick tree canopies or pergola vineyards.","This work proposed a novel system that leverages a single RGB-D camera and a Non-linear Model Predictive Control strategy to navigate through entire rows, adapting to various crop spacing.","The presented solution demonstrates versatility in handling diverse crop densities, environmental factors, and multiple navigation tasks to support agricultural activities at an extremely cost-effective implementation.","Experimental validation in simulated and real vineyards underscores the system's robustness and competitiveness in both standard row traversal and target objects approach."],"url":"http://arxiv.org/abs/2404.05343v1","category":"cs.RO"}
{"created":"2024-04-08 09:27:42","title":"Comparative Analysis of Image Enhancement Techniques for Brain Tumor Segmentation: Contrast, Histogram, and Hybrid Approaches","abstract":"This study systematically investigates the impact of image enhancement techniques on Convolutional Neural Network (CNN)-based Brain Tumor Segmentation, focusing on Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE), and their hybrid variations. Employing the U-Net architecture on a dataset of 3064 Brain MRI images, the research delves into preprocessing steps, including resizing and enhancement, to optimize segmentation accuracy. A detailed analysis of the CNN-based U-Net architecture, training, and validation processes is provided. The comparative analysis, utilizing metrics such as Accuracy, Loss, MSE, IoU, and DSC, reveals that the hybrid approach CLAHE-HE consistently outperforms others. Results highlight its superior accuracy (0.9982, 0.9939, 0.9936 for training, testing, and validation, respectively) and robust segmentation overlap, with Jaccard values of 0.9862, 0.9847, and 0.9864, and Dice values of 0.993, 0.9923, and 0.9932 for the same phases, emphasizing its potential in neuro-oncological applications. The study concludes with a call for refinement in segmentation methodologies to further enhance diagnostic precision and treatment planning in neuro-oncology.","sentences":["This study systematically investigates the impact of image enhancement techniques on Convolutional Neural Network (CNN)-based Brain Tumor Segmentation, focusing on Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE), and their hybrid variations.","Employing the U-Net architecture on a dataset of 3064 Brain MRI images, the research delves into preprocessing steps, including resizing and enhancement, to optimize segmentation accuracy.","A detailed analysis of the CNN-based U-Net architecture, training, and validation processes is provided.","The comparative analysis, utilizing metrics such as Accuracy, Loss, MSE, IoU, and DSC, reveals that the hybrid approach CLAHE-HE consistently outperforms others.","Results highlight its superior accuracy (0.9982, 0.9939, 0.9936 for training, testing, and validation, respectively) and robust segmentation overlap, with Jaccard values of 0.9862, 0.9847, and 0.9864, and Dice values of 0.993, 0.9923, and 0.9932 for the same phases, emphasizing its potential in neuro-oncological applications.","The study concludes with a call for refinement in segmentation methodologies to further enhance diagnostic precision and treatment planning in neuro-oncology."],"url":"http://arxiv.org/abs/2404.05341v1","category":"eess.IV"}
{"created":"2024-04-08 09:26:31","title":"GPS-free Autonomous Navigation in Cluttered Tree Rows with Deep Semantic Segmentation","abstract":"Segmentation-based autonomous navigation has recently been presented as an appealing approach to guiding robotic platforms through crop rows without requiring perfect GPS localization. Nevertheless, current techniques are restricted to situations where the distinct separation between the plants and the sky allows for the identification of the row's center. However, tall, dense vegetation, such as high tree rows and orchards, is the primary cause of GPS signal blockage. In this study, we increase the overall robustness and adaptability of the control algorithm by extending the segmentation-based robotic guiding to those cases where canopies and branches occlude the sky and prevent the utilization of GPS and earlier approaches. An efficient Deep Neural Network architecture has been used to address semantic segmentation, performing the training with synthetic data only. Numerous vineyards and tree fields have undergone extensive testing in both simulation and real-world to show the solution's competitive benefits.","sentences":["Segmentation-based autonomous navigation has recently been presented as an appealing approach to guiding robotic platforms through crop rows without requiring perfect GPS localization.","Nevertheless, current techniques are restricted to situations where the distinct separation between the plants and the sky allows for the identification of the row's center.","However, tall, dense vegetation, such as high tree rows and orchards, is the primary cause of GPS signal blockage.","In this study, we increase the overall robustness and adaptability of the control algorithm by extending the segmentation-based robotic guiding to those cases where canopies and branches occlude the sky and prevent the utilization of GPS and earlier approaches.","An efficient Deep Neural Network architecture has been used to address semantic segmentation, performing the training with synthetic data only.","Numerous vineyards and tree fields have undergone extensive testing in both simulation and real-world to show the solution's competitive benefits."],"url":"http://arxiv.org/abs/2404.05338v1","category":"cs.RO"}
{"created":"2024-04-08 09:22:41","title":"PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for the Neural Processing of Portuguese","abstract":"Leveraging research on the neural modelling of Portuguese, we contribute a collection of datasets for an array of language processing tasks and a corresponding collection of fine-tuned neural language models on these downstream tasks. To align with mainstream benchmarks in the literature, originally developed in English, and to kick start their Portuguese counterparts, the datasets were machine-translated from English with a state-of-the-art translation engine. The resulting PORTULAN ExtraGLUE benchmark is a basis for research on Portuguese whose improvement can be pursued in future work. Similarly, the respective fine-tuned neural language models, developed with a low-rank adaptation approach, are made available as baselines that can stimulate future work on the neural processing of Portuguese. All datasets and models have been developed and are made available for two variants of Portuguese: European and Brazilian.","sentences":["Leveraging research on the neural modelling of Portuguese, we contribute a collection of datasets for an array of language processing tasks and a corresponding collection of fine-tuned neural language models on these downstream tasks.","To align with mainstream benchmarks in the literature, originally developed in English, and to kick start their Portuguese counterparts, the datasets were machine-translated from English with a state-of-the-art translation engine.","The resulting PORTULAN ExtraGLUE benchmark is a basis for research on Portuguese whose improvement can be pursued in future work.","Similarly, the respective fine-tuned neural language models, developed with a low-rank adaptation approach, are made available as baselines that can stimulate future work on the neural processing of Portuguese.","All datasets and models have been developed and are made available for two variants of Portuguese: European and Brazilian."],"url":"http://arxiv.org/abs/2404.05333v2","category":"cs.CL"}
{"created":"2024-04-08 09:11:04","title":"A Power Management and Control System for Portable Ecosystem Monitoring Devices","abstract":"Recent advances in Internet of Things (IoT) and Artificial Intelligence (AI) technologies help ecosystem monitoring to shift towards automated monitoring with low power sensors and embedded vision on powerful processing units. Vision-based monitoring devices need an effective power management and control system (PMCS) with system-adapted power input and output capabilities to achieve power-efficient and self-sustainable operation. Here, we present a universal power management solution for automated ecosystem monitoring devices, compatible with commonly used off-the-shelf edge processing units (EPUs). The proposed design is specifically adapted for battery-powered EPU systems by incorporating power-matched energy harvesting (EH), a power switch with low-power sleep mode, and simple system integration in an MCU-less architecture with automated operation. We use a 4-month environmental case study to monitor plant growth under 4mg microplastic (MP) exposure, demonstrating that the setup achieved continuous and sustainable operation. In this plant phenology case study, our power management module is deployed in an embedded vision camera equipped with a 5W solar panel and five various environmental sensors. This work shows the usability of the power management board in environmentally relevant use cases and for tasks in agricultural applications.","sentences":["Recent advances in Internet of Things (IoT) and Artificial Intelligence (AI) technologies help ecosystem monitoring to shift towards automated monitoring with low power sensors and embedded vision on powerful processing units.","Vision-based monitoring devices need an effective power management and control system (PMCS) with system-adapted power input and output capabilities to achieve power-efficient and self-sustainable operation.","Here, we present a universal power management solution for automated ecosystem monitoring devices, compatible with commonly used off-the-shelf edge processing units (EPUs).","The proposed design is specifically adapted for battery-powered EPU systems by incorporating power-matched energy harvesting (EH), a power switch with low-power sleep mode, and simple system integration in an MCU-less architecture with automated operation.","We use a 4-month environmental case study to monitor plant growth under 4mg microplastic (MP) exposure, demonstrating that the setup achieved continuous and sustainable operation.","In this plant phenology case study, our power management module is deployed in an embedded vision camera equipped with a 5W solar panel and five various environmental sensors.","This work shows the usability of the power management board in environmentally relevant use cases and for tasks in agricultural applications."],"url":"http://arxiv.org/abs/2404.05322v1","category":"eess.SY"}
{"created":"2024-04-08 08:57:41","title":"Energy exchange statistics and fluctuation theorem for non-thermal asymptotic states","abstract":"Exchange energy statistics between two bodies at different thermal equilibrium obey the Jarzynski-W\\'ojcik fluctuation theorem. The corresponding energy scale factor is the difference of the inverse temperatures associated to the bodies at equilibrium. In this work, we consider a dissipative quantum dynamics leading the quantum system towards a, possibly non-thermal, asymptotic state. To generalize the Jarzynski-W\\'ojcik theorem to non-thermal states, we identify a sufficient condition ${\\cal I}$ for the existence of an energy scale factor $\\eta^{*}$ that is unique, finite and time-independent, such that the characteristic function of the exchange energy distribution becomes identically equal to $1$ for any time. This $\\eta^*$ plays the role of the difference of inverse temperatures. We discuss the physical interpretation of the condition ${\\cal I}$, showing that it amounts to an almost complete memory loss of the initial state. The robustness of our results against quantifiable deviations from the validity of ${\\cal I}$ is evaluated by experimental studies on a single nitrogen-vacancy center subjected to a sequence of laser pulses and dissipation.","sentences":["Exchange energy statistics between two bodies at different thermal equilibrium obey the Jarzynski-W\\'ojcik fluctuation theorem.","The corresponding energy scale factor is the difference of the inverse temperatures associated to the bodies at equilibrium.","In this work, we consider a dissipative quantum dynamics leading the quantum system towards a, possibly non-thermal, asymptotic state.","To generalize the Jarzynski-W\\'ojcik theorem to non-thermal states, we identify a sufficient condition ${\\cal I}$ for the existence of an energy scale factor $\\eta^{*}$ that is unique, finite and time-independent, such that the characteristic function of the exchange energy distribution becomes identically equal to $1$ for any time.","This $\\eta^*$ plays the role of the difference of inverse temperatures.","We discuss the physical interpretation of the condition ${\\cal I}$, showing that it amounts to an almost complete memory loss of the initial state.","The robustness of our results against quantifiable deviations from the validity of ${\\cal I}$ is evaluated by experimental studies on a single nitrogen-vacancy center subjected to a sequence of laser pulses and dissipation."],"url":"http://arxiv.org/abs/2404.05310v1","category":"quant-ph"}
{"created":"2024-04-08 08:47:46","title":"Liquid Neural Network-based Adaptive Learning vs. Incremental Learning for Link Load Prediction amid Concept Drift due to Network Failures","abstract":"Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a learning model leveraging newly available data. A primary limitation of these techniques is their reliance on substantial amounts of data for retraining. The necessity of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two consecutive retrainings. In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may induce a drastic shift in distribution and pattern of traffic data, thus requiring a timely model adaptation. In this work, we address this challenge for the problem of traffic forecasting and propose an approach that exploits adaptive learning algorithms, namely, liquid neural networks, which are capable of self-adaptation to abrupt changes in data patterns without requiring any retraining. Through extensive simulations of failure scenarios, we compare the predictive performance of our proposed approach to that of a reference method based on incremental learning. Experimental results show that our proposed approach outperforms incremental learning-based methods in situations where the shifts in traffic patterns are drastic.","sentences":["Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a learning model leveraging newly available data.","A primary limitation of these techniques is their reliance on substantial amounts of data for retraining.","The necessity of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two consecutive retrainings.","In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may induce a drastic shift in distribution and pattern of traffic data, thus requiring a timely model adaptation.","In this work, we address this challenge for the problem of traffic forecasting and propose an approach that exploits adaptive learning algorithms, namely, liquid neural networks, which are capable of self-adaptation to abrupt changes in data patterns without requiring any retraining.","Through extensive simulations of failure scenarios, we compare the predictive performance of our proposed approach to that of a reference method based on incremental learning.","Experimental results show that our proposed approach outperforms incremental learning-based methods in situations where the shifts in traffic patterns are drastic."],"url":"http://arxiv.org/abs/2404.05304v1","category":"cs.NI"}
{"created":"2024-04-08 08:42:47","title":"Texture Classification Network Integrating Adaptive Wavelet Transform","abstract":"Graves' disease is a common condition that is diagnosed clinically by determining the smoothness of the thyroid texture and its morphology in ultrasound images. Currently, the most widely used approach for the automated diagnosis of Graves' disease utilizes Convolutional Neural Networks (CNNs) for both feature extraction and classification. However, these methods demonstrate limited efficacy in capturing texture features. Given the high capacity of wavelets in describing texture features, this research integrates learnable wavelet modules utilizing the Lifting Scheme into CNNs and incorporates a parallel wavelet branch into the ResNet18 model to enhance texture feature extraction. Our model can analyze texture features in spatial and frequency domains simultaneously, leading to optimized classification accuracy. We conducted experiments on collected ultrasound datasets and publicly available natural image texture datasets, our proposed network achieved 97.27% accuracy and 95.60% recall on ultrasound datasets, 60.765% accuracy on natural image texture datasets, surpassing the accuracy of ResNet and conrming the effectiveness of our approach.","sentences":["Graves' disease is a common condition that is diagnosed clinically by determining the smoothness of the thyroid texture and its morphology in ultrasound images.","Currently, the most widely used approach for the automated diagnosis of Graves' disease utilizes Convolutional Neural Networks (CNNs) for both feature extraction and classification.","However, these methods demonstrate limited efficacy in capturing texture features.","Given the high capacity of wavelets in describing texture features, this research integrates learnable wavelet modules utilizing the Lifting Scheme into CNNs and incorporates a parallel wavelet branch into the ResNet18 model to enhance texture feature extraction.","Our model can analyze texture features in spatial and frequency domains simultaneously, leading to optimized classification accuracy.","We conducted experiments on collected ultrasound datasets and publicly available natural image texture datasets, our proposed network achieved 97.27% accuracy and 95.60% recall on ultrasound datasets, 60.765% accuracy on natural image texture datasets, surpassing the accuracy of ResNet and conrming the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.05300v1","category":"cs.CV"}
{"created":"2024-04-08 08:32:39","title":"Online Learning of Joint-Muscle Mapping Using Vision in Tendon-driven Musculoskeletal Humanoids","abstract":"The body structures of tendon-driven musculoskeletal humanoids are complex, and accurate modeling is difficult, because they are made by imitating the body structures of human beings. For this reason, we have not been able to move them accurately like ordinary humanoids driven by actuators in each axis, and large internal muscle tension and slack of tendon wires have emerged by the model error between its geometric model and the actual robot. Therefore, we construct a joint-muscle mapping (JMM) using a neural network (NN), which expresses a nonlinear relationship between joint angles and muscle lengths, and aim to move tendon-driven musculoskeletal humanoids accurately by updating the JMM online from data of the actual robot. In this study, the JMM is updated online by using the vision of the robot so that it moves to the correct position (Vision Updater). Also, we execute another update to modify muscle antagonisms correctly (Antagonism Updater). By using these two updaters, the error between the target and actual joint angles decrease to about 40% in 5 minutes, and we show through a manipulation experiment that the tendon-driven musculoskeletal humanoid Kengoro becomes able to move as intended. This novel system can adapt to the state change and growth of robots, because it updates the JMM online successively.","sentences":["The body structures of tendon-driven musculoskeletal humanoids are complex, and accurate modeling is difficult, because they are made by imitating the body structures of human beings.","For this reason, we have not been able to move them accurately like ordinary humanoids driven by actuators in each axis, and large internal muscle tension and slack of tendon wires have emerged by the model error between its geometric model and the actual robot.","Therefore, we construct a joint-muscle mapping (JMM) using a neural network (NN), which expresses a nonlinear relationship between joint angles and muscle lengths, and aim to move tendon-driven musculoskeletal humanoids accurately by updating the JMM online from data of the actual robot.","In this study, the JMM is updated online by using the vision of the robot so that it moves to the correct position (Vision Updater).","Also, we execute another update to modify muscle antagonisms correctly (Antagonism Updater).","By using these two updaters, the error between the target and actual joint angles decrease to about 40% in 5 minutes, and we show through a manipulation experiment that the tendon-driven musculoskeletal humanoid Kengoro becomes able to move as intended.","This novel system can adapt to the state change and growth of robots, because it updates the JMM online successively."],"url":"http://arxiv.org/abs/2404.05295v1","category":"cs.RO"}
{"created":"2024-04-08 07:59:04","title":"MC$^2$: Multi-concept Guidance for Customized Multi-concept Generation","abstract":"Customized text-to-image generation aims to synthesize instantiations of user-specified concepts and has achieved unprecedented progress in handling individual concept. However, when extending to multiple customized concepts, existing methods exhibit limitations in terms of flexibility and fidelity, only accommodating the combination of limited types of models and potentially resulting in a mix of characteristics from different concepts. In this paper, we introduce the Multi-concept guidance for Multi-concept customization, termed MC$^2$, for improved flexibility and fidelity. MC$^2$ decouples the requirements for model architecture via inference time optimization, allowing the integration of various heterogeneous single-concept customized models. It adaptively refines the attention weights between visual and textual tokens, directing image regions to focus on their associated words while diminishing the impact of irrelevant ones. Extensive experiments demonstrate that MC$^2$ even surpasses previous methods that require additional training in terms of consistency with input prompt and reference images. Moreover, MC$^2$ can be extended to elevate the compositional capabilities of text-to-image generation, yielding appealing results. Code will be publicly available at https://github.com/JIANGJiaXiu/MC-2.","sentences":["Customized text-to-image generation aims to synthesize instantiations of user-specified concepts and has achieved unprecedented progress in handling individual concept.","However, when extending to multiple customized concepts, existing methods exhibit limitations in terms of flexibility and fidelity, only accommodating the combination of limited types of models and potentially resulting in a mix of characteristics from different concepts.","In this paper, we introduce the Multi-concept guidance for Multi-concept customization, termed MC$^2$, for improved flexibility and fidelity.","MC$^2$ decouples the requirements for model architecture via inference time optimization, allowing the integration of various heterogeneous single-concept customized models.","It adaptively refines the attention weights between visual and textual tokens, directing image regions to focus on their associated words while diminishing the impact of irrelevant ones.","Extensive experiments demonstrate that MC$^2$ even surpasses previous methods that require additional training in terms of consistency with input prompt and reference images.","Moreover, MC$^2$ can be extended to elevate the compositional capabilities of text-to-image generation, yielding appealing results.","Code will be publicly available at https://github.com/JIANGJiaXiu/MC-2."],"url":"http://arxiv.org/abs/2404.05268v1","category":"cs.CV"}
{"created":"2024-04-08 07:51:20","title":"Robust Anthropomorphic Robotic Manipulation through Biomimetic Distributed Compliance","abstract":"The impressive capabilities of humans to robustly perform manipulation relies on compliant interactions, enabled through the structure and materials spatially distributed in our hands. We propose by mimicking this distributed compliance in an anthropomorphic robotic hand, the open-loop manipulation robustness increases and observe the emergence of human-like behaviours. To achieve this, we introduce the ADAPT Hand equipped with tunable compliance throughout the skin, fingers, and the wrist. Through extensive automated pick-and-place tests, we show the grasping robustness closely mirrors an estimated geometric theoretical limit, while `stress-testing' the robot hand to perform 800+ grasps. Finally, 24 items with largely varying geometries are grasped in a constrained environment with a success rate of 93\\%. We demonstrate the hand-object self-organization behavior underlines this extreme robustness, where the hand automatically exhibits different grasp types depending on object geometries. Furthermore, the robot grasp type mimics a natural human grasp with a direct similarity of 68\\%.","sentences":["The impressive capabilities of humans to robustly perform manipulation relies on compliant interactions, enabled through the structure and materials spatially distributed in our hands.","We propose by mimicking this distributed compliance in an anthropomorphic robotic hand, the open-loop manipulation robustness increases and observe the emergence of human-like behaviours.","To achieve this, we introduce the ADAPT Hand equipped with tunable compliance throughout the skin, fingers, and the wrist.","Through extensive automated pick-and-place tests, we show the grasping robustness closely mirrors an estimated geometric theoretical limit, while `stress-testing' the robot hand to perform 800+ grasps.","Finally, 24 items with largely varying geometries are grasped in a constrained environment with a success rate of 93\\%.","We demonstrate the hand-object self-organization behavior underlines this extreme robustness, where the hand automatically exhibits different grasp types depending on object geometries.","Furthermore, the robot grasp type mimics a natural human grasp with a direct similarity of 68\\%."],"url":"http://arxiv.org/abs/2404.05262v1","category":"cs.RO"}
{"created":"2024-04-08 07:51:08","title":"T3DRIS: Advancing Conformal RIS Design through In-depth Analysis of Mutual Coupling Effects","abstract":"This paper presents a theoretical and mathematical framework for the design of a conformal reconfigurable intelligent surface (RIS) that adapts to non-planar geometries, which is a critical advancement for the deployment of RIS on non-planar and irregular surfaces as envisioned in smart radio environments. Previous research focused mainly on the optimization of RISs assuming a predetermined shape, while neglecting the intricate interplay between shape optimization, phase optimization, and mutual coupling effects. Our contribution, the T3DRIS framework, addresses this fundamental problem by integrating the configuration and shape optimization of RISs into a unified model and design framework, thus facilitating the application of RIS technology to a wider spectrum of environmental objects. The mathematical core of T3DRIS is rooted in optimizing the 3D deployment of the unit cells and tuning circuits, aiming at maximizing the communication performance. Through rigorous full-wave simulations and a comprehensive set of numerical analyses, we validate the proposed approach and demonstrate its superior performance and applicability over contemporary designs. This study-the first of its kind-paves the way for a new direction in RIS research, emphasizing the importance of a theoretical and mathematical perspective in tackling the challenges of conformal RISs.","sentences":["This paper presents a theoretical and mathematical framework for the design of a conformal reconfigurable intelligent surface (RIS) that adapts to non-planar geometries, which is a critical advancement for the deployment of RIS on non-planar and irregular surfaces as envisioned in smart radio environments.","Previous research focused mainly on the optimization of RISs assuming a predetermined shape, while neglecting the intricate interplay between shape optimization, phase optimization, and mutual coupling effects.","Our contribution, the T3DRIS framework, addresses this fundamental problem by integrating the configuration and shape optimization of RISs into a unified model and design framework, thus facilitating the application of RIS technology to a wider spectrum of environmental objects.","The mathematical core of T3DRIS is rooted in optimizing the 3D deployment of the unit cells and tuning circuits, aiming at maximizing the communication performance.","Through rigorous full-wave simulations and a comprehensive set of numerical analyses, we validate the proposed approach and demonstrate its superior performance and applicability over contemporary designs.","This study-the first of its kind-paves the way for a new direction in RIS research, emphasizing the importance of a theoretical and mathematical perspective in tackling the challenges of conformal RISs."],"url":"http://arxiv.org/abs/2404.05261v1","category":"cs.IT"}
{"created":"2024-04-08 07:34:39","title":"CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement","abstract":"Low-light image enhancement (LLIE) aims to improve low-illumination images. However, existing methods face two challenges: (1) uncertainty in restoration from diverse brightness degradations; (2) loss of texture and color information caused by noise suppression and light enhancement. In this paper, we propose a novel enhancement approach, CodeEnhance, by leveraging quantized priors and image refinement to address these challenges. In particular, we reframe LLIE as learning an image-to-code mapping from low-light images to discrete codebook, which has been learned from high-quality images. To enhance this process, a Semantic Embedding Module (SEM) is introduced to integrate semantic information with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt the pre-learned codebook to better suit the distinct characteristics of our low-light dataset. Additionally, we present an Interactive Feature Transformation (IFT) module to refine texture and color information during image reconstruction, allowing for interactive enhancement based on user preferences. Extensive experiments on both real-world and synthetic benchmarks demonstrate that the incorporation of prior knowledge and controllable information transfer significantly enhances LLIE performance in terms of quality and fidelity. The proposed CodeEnhance exhibits superior robustness to various degradations, including uneven illumination, noise, and color distortion.","sentences":["Low-light image enhancement (LLIE) aims to improve low-illumination images.","However, existing methods face two challenges: (1) uncertainty in restoration from diverse brightness degradations; (2) loss of texture and color information caused by noise suppression and light enhancement.","In this paper, we propose a novel enhancement approach, CodeEnhance, by leveraging quantized priors and image refinement to address these challenges.","In particular, we reframe LLIE as learning an image-to-code mapping from low-light images to discrete codebook, which has been learned from high-quality images.","To enhance this process, a Semantic Embedding Module (SEM) is introduced to integrate semantic information with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt the pre-learned codebook to better suit the distinct characteristics of our low-light dataset.","Additionally, we present an Interactive Feature Transformation (IFT) module to refine texture and color information during image reconstruction, allowing for interactive enhancement based on user preferences.","Extensive experiments on both real-world and synthetic benchmarks demonstrate that the incorporation of prior knowledge and controllable information transfer significantly enhances LLIE performance in terms of quality and fidelity.","The proposed CodeEnhance exhibits superior robustness to various degradations, including uneven illumination, noise, and color distortion."],"url":"http://arxiv.org/abs/2404.05253v1","category":"cs.CV"}
{"created":"2024-04-08 06:36:42","title":"ITA-ECBS: A Bounded-Suboptimal Algorithm for Combined Target-Assignment and Path-Finding Problem","abstract":"Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for multiple robots, plays a critical role in many applications. Sometimes, assigning a specific target to each agent also presents a challenge. The Combined Target-Assignment and Path-Finding (TAPF) problem, a variant of MAPF, requires simultaneously assigning targets to agents and planning collision-free paths. Several algorithms, including CBM, CBS-TA, and ITA-CBS, can optimally solve the TAPF problem, with ITA-CBS being the leading method of flowtime. However, the only existing suboptimal method ECBS-TA, is derived from CBS-TA rather than ITA-CBS, and adapting the optimal ITA-CBS method to its bounded-suboptimal variant is a challenge due to the variability of target assignment solutions in different search nodes. We introduce ITA-ECBS as the first bounded-suboptimal variant of ITA-CBS. ITA-ECBS employs focal search to enhance efficiency and determines target assignments based on a new lower bound matrix. We show that ITA-ECBS outperforms the baseline method ECBS-TA in 87.42% of 54,033 test cases.","sentences":["Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for multiple robots, plays a critical role in many applications.","Sometimes, assigning a specific target to each agent also presents a challenge.","The Combined Target-Assignment and Path-Finding (TAPF) problem, a variant of MAPF, requires simultaneously assigning targets to agents and planning collision-free paths.","Several algorithms, including CBM, CBS-TA, and ITA-CBS, can optimally solve the TAPF problem, with ITA-CBS being the leading method of flowtime.","However, the only existing suboptimal method ECBS-TA, is derived from CBS-TA rather than ITA-CBS, and adapting the optimal ITA-CBS method to its bounded-suboptimal variant is a challenge due to the variability of target assignment solutions in different search nodes.","We introduce ITA-ECBS as the first bounded-suboptimal variant of ITA-CBS.","ITA-ECBS employs focal search to enhance efficiency and determines target assignments based on a new lower bound matrix.","We show that ITA-ECBS outperforms the baseline method ECBS-TA in 87.42% of 54,033 test cases."],"url":"http://arxiv.org/abs/2404.05223v1","category":"cs.AI"}
{"created":"2024-04-08 06:35:09","title":"LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models","abstract":"Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.","sentences":["Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability.","Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge.","The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison.","This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation.","Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks.","In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria.","(2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components.","With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP).","The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc."],"url":"http://arxiv.org/abs/2404.05221v1","category":"cs.CL"}
{"created":"2024-04-08 06:32:11","title":"StylizedGS: Controllable Stylization for 3D Gaussian Splatting","abstract":"With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing. It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way. However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles. Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration. In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The 3DGS brings the benefits of high efficiency. We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization. Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities. Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS.","sentences":["With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing.","It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way.","However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles.","Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration.","In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation.","The 3DGS brings the benefits of high efficiency.","We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization.","Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content.","Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities.","Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls.","Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS."],"url":"http://arxiv.org/abs/2404.05220v1","category":"cs.CV"}
{"created":"2024-04-08 06:11:15","title":"Network-Constrained Unit Commitment with Flexible Temporal Resolution","abstract":"Modern network-constrained unit commitment (NCUC) bears a heavy computational burden due to the ever-growing model scale. This situation becomes more challenging when detailed operational characteristics, complicated constraints, and multiple objectives are considered. We propose a novel simplification method to determine the flexible temporal resolution for acceleration and near-optimal solutions. The flexible temporal resolution is determined by analyzing the impact on generators in each adaptive time period with awareness of congestion effects. Additionally, multiple improvements are employed on the existing NCUC model compatible with flexible temporal resolution to reduce the number of integer variables while preserving the original features. A case study using the IEEE 118-bus and the Polish 2736-bus systems verifies that the proposed method achieves substantial acceleration with low cost variation and high accuracy.","sentences":["Modern network-constrained unit commitment (NCUC) bears a heavy computational burden due to the ever-growing model scale.","This situation becomes more challenging when detailed operational characteristics, complicated constraints, and multiple objectives are considered.","We propose a novel simplification method to determine the flexible temporal resolution for acceleration and near-optimal solutions.","The flexible temporal resolution is determined by analyzing the impact on generators in each adaptive time period with awareness of congestion effects.","Additionally, multiple improvements are employed on the existing NCUC model compatible with flexible temporal resolution to reduce the number of integer variables while preserving the original features.","A case study using the IEEE 118-bus and the Polish 2736-bus systems verifies that the proposed method achieves substantial acceleration with low cost variation and high accuracy."],"url":"http://arxiv.org/abs/2404.05217v1","category":"eess.SY"}
{"created":"2024-04-08 06:07:32","title":"Spatio-Temporal Attention and Gaussian Processes for Personalized Video Gaze Estimation","abstract":"Gaze is an essential prompt for analyzing human behavior and attention. Recently, there has been an increasing interest in determining gaze direction from facial videos. However, video gaze estimation faces significant challenges, such as understanding the dynamic evolution of gaze in video sequences, dealing with static backgrounds, and adapting to variations in illumination. To address these challenges, we propose a simple and novel deep learning model designed to estimate gaze from videos, incorporating a specialized attention module. Our method employs a spatial attention mechanism that tracks spatial dynamics within videos. This technique enables accurate gaze direction prediction through a temporal sequence model, adeptly transforming spatial observations into temporal insights, thereby significantly improving gaze estimation accuracy. Additionally, our approach integrates Gaussian processes to include individual-specific traits, facilitating the personalization of our model with just a few labeled samples. Experimental results confirm the efficacy of the proposed approach, demonstrating its success in both within-dataset and cross-dataset settings. Specifically, our proposed approach achieves state-of-the-art performance on the Gaze360 dataset, improving by $2.5^\\circ$ without personalization. Further, by personalizing the model with just three samples, we achieved an additional improvement of $0.8^\\circ$. The code and pre-trained models are available at \\url{https://github.com/jswati31/stage}.","sentences":["Gaze is an essential prompt for analyzing human behavior and attention.","Recently, there has been an increasing interest in determining gaze direction from facial videos.","However, video gaze estimation faces significant challenges, such as understanding the dynamic evolution of gaze in video sequences, dealing with static backgrounds, and adapting to variations in illumination.","To address these challenges, we propose a simple and novel deep learning model designed to estimate gaze from videos, incorporating a specialized attention module.","Our method employs a spatial attention mechanism that tracks spatial dynamics within videos.","This technique enables accurate gaze direction prediction through a temporal sequence model, adeptly transforming spatial observations into temporal insights, thereby significantly improving gaze estimation accuracy.","Additionally, our approach integrates Gaussian processes to include individual-specific traits, facilitating the personalization of our model with just a few labeled samples.","Experimental results confirm the efficacy of the proposed approach, demonstrating its success in both within-dataset and cross-dataset settings.","Specifically, our proposed approach achieves state-of-the-art performance on the Gaze360 dataset, improving by $2.5^\\circ$ without personalization.","Further, by personalizing the model with just three samples, we achieved an additional improvement of $0.8^\\circ$. The code and pre-trained models are available at \\url{https://github.com/jswati31/stage}."],"url":"http://arxiv.org/abs/2404.05215v1","category":"cs.CV"}
{"created":"2024-04-08 05:23:12","title":"iVPT: Improving Task-relevant Information Sharing in Visual Prompt Tuning by Cross-layer Dynamic Connection","abstract":"Recent progress has shown great potential of visual prompt tuning (VPT) when adapting pre-trained vision transformers to various downstream tasks. However, most existing solutions independently optimize prompts at each layer, thereby neglecting the usage of task-relevant information encoded in prompt tokens across layers. Additionally, existing prompt structures are prone to interference from task-irrelevant noise in input images, which can do harm to the sharing of task-relevant information. In this paper, we propose a novel VPT approach, \\textbf{iVPT}. It innovatively incorporates a cross-layer dynamic connection (CDC) for input prompt tokens from adjacent layers, enabling effective sharing of task-relevant information. Furthermore, we design a dynamic aggregation (DA) module that facilitates selective sharing of information between layers. The combination of CDC and DA enhances the flexibility of the attention process within the VPT framework. Building upon these foundations, iVPT introduces an attentive reinforcement (AR) mechanism, by automatically identifying salient image tokens, which are further enhanced by prompt tokens in an additive manner. Extensive experiments on 24 image classification and semantic segmentation benchmarks clearly demonstrate the advantage of the proposed iVPT, compared to the state-of-the-art counterparts.","sentences":["Recent progress has shown great potential of visual prompt tuning (VPT) when adapting pre-trained vision transformers to various downstream tasks.","However, most existing solutions independently optimize prompts at each layer, thereby neglecting the usage of task-relevant information encoded in prompt tokens across layers.","Additionally, existing prompt structures are prone to interference from task-irrelevant noise in input images, which can do harm to the sharing of task-relevant information.","In this paper, we propose a novel VPT approach, \\textbf{iVPT}.","It innovatively incorporates a cross-layer dynamic connection (CDC) for input prompt tokens from adjacent layers, enabling effective sharing of task-relevant information.","Furthermore, we design a dynamic aggregation (DA) module that facilitates selective sharing of information between layers.","The combination of CDC and DA enhances the flexibility of the attention process within the VPT framework.","Building upon these foundations, iVPT introduces an attentive reinforcement (AR) mechanism, by automatically identifying salient image tokens, which are further enhanced by prompt tokens in an additive manner.","Extensive experiments on 24 image classification and semantic segmentation benchmarks clearly demonstrate the advantage of the proposed iVPT, compared to the state-of-the-art counterparts."],"url":"http://arxiv.org/abs/2404.05207v1","category":"cs.CV"}
{"created":"2024-04-08 05:02:48","title":"Decision Transformer for Wireless Communications: A New Paradigm of Resource Management","abstract":"As the next generation of mobile systems evolves, artificial intelligence (AI) is expected to deeply integrate with wireless communications for resource management in variable environments. In particular, deep reinforcement learning (DRL) is an important tool for addressing stochastic optimization issues of resource allocation. However, DRL has to start each new training process from the beginning once the state and action spaces change, causing low sample efficiency and poor generalization ability. Moreover, each DRL training process may take a large number of epochs to converge, which is unacceptable for time-sensitive scenarios. In this paper, we adopt an alternative AI technology, namely, the Decision Transformer (DT), and propose a DT-based adaptive decision architecture for wireless resource management. This architecture innovates through constructing pre-trained models in the cloud and then fine-tuning personalized models at the edges. By leveraging the power of DT models learned over extensive datasets, the proposed architecture is expected to achieve rapid convergence with many fewer training epochs and higher performance in a new context, e.g., similar tasks with different state and action spaces, compared with DRL. We then design DT frameworks for two typical communication scenarios: Intelligent reflecting surfaces-aided communications and unmanned aerial vehicle-aided edge computing. Simulations demonstrate that the proposed DT frameworks achieve over $3$-$6$ times speedup in convergence and better performance relative to the classic DRL method, namely, proximal policy optimization.","sentences":["As the next generation of mobile systems evolves, artificial intelligence (AI) is expected to deeply integrate with wireless communications for resource management in variable environments.","In particular, deep reinforcement learning (DRL) is an important tool for addressing stochastic optimization issues of resource allocation.","However, DRL has to start each new training process from the beginning once the state and action spaces change, causing low sample efficiency and poor generalization ability.","Moreover, each DRL training process may take a large number of epochs to converge, which is unacceptable for time-sensitive scenarios.","In this paper, we adopt an alternative AI technology, namely, the Decision Transformer (DT), and propose a DT-based adaptive decision architecture for wireless resource management.","This architecture innovates through constructing pre-trained models in the cloud and then fine-tuning personalized models at the edges.","By leveraging the power of DT models learned over extensive datasets, the proposed architecture is expected to achieve rapid convergence with many fewer training epochs and higher performance in a new context, e.g., similar tasks with different state and action spaces, compared with DRL.","We then design DT frameworks for two typical communication scenarios: Intelligent reflecting surfaces-aided communications and unmanned aerial vehicle-aided edge computing.","Simulations demonstrate that the proposed DT frameworks achieve over $3$-$6$ times speedup in convergence and better performance relative to the classic DRL method, namely, proximal policy optimization."],"url":"http://arxiv.org/abs/2404.05199v1","category":"eess.SP"}
{"created":"2024-04-08 04:43:40","title":"Foundations for operator algebraic tricategories","abstract":"An operator algebraic tricategory is a higher categorical analogue of an operator algebra. For algebraic tricategories, Gordon, Power, and Street proved that every algebraic tricategory is equivalent to a Gray-category, a result later refined by Gurski. We adapt this result to the context of functional analysis, showing that every operator algebraic tricategory is equivalent to an operator Gray-category. We then categorify the Gelfand-Naimark theorem for operator algebras, inductively proving that every (small) operator algebraic tricategory is equivalent to a concrete operator Gray-category. We also provide several examples of interest for operator algebraic tricategories.","sentences":["An operator algebraic tricategory is a higher categorical analogue of an operator algebra.","For algebraic tricategories, Gordon, Power, and Street proved that every algebraic tricategory is equivalent to a Gray-category, a result later refined by Gurski.","We adapt this result to the context of functional analysis, showing that every operator algebraic tricategory is equivalent to an operator Gray-category.","We then categorify the Gelfand-Naimark theorem for operator algebras, inductively proving that every (small) operator algebraic tricategory is equivalent to a concrete operator Gray-category.","We also provide several examples of interest for operator algebraic tricategories."],"url":"http://arxiv.org/abs/2404.05193v1","category":"math.OA"}
{"created":"2024-04-08 04:41:39","title":"ATFNet: Adaptive Time-Frequency Ensembled Network for Long-term Time Series Forecasting","abstract":"The intricate nature of time series data analysis benefits greatly from the distinct advantages offered by time and frequency domain representations. While the time domain is superior in representing local dependencies, particularly in non-periodic series, the frequency domain excels in capturing global dependencies, making it ideal for series with evident periodic patterns. To capitalize on both of these strengths, we propose ATFNet, an innovative framework that combines a time domain module and a frequency domain module to concurrently capture local and global dependencies in time series data. Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel mechanism for dynamically adjusting the weights between the two modules based on the periodicity of the input time series. In the frequency domain module, we enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT, designed to address the challenge of discrete frequency misalignment. Additionally, our Complex-valued Spectrum Attention mechanism offers a novel approach to discern the intricate relationships between different frequency combinations. Extensive experiments across multiple real-world datasets demonstrate that our ATFNet framework outperforms current state-of-the-art methods in long-term time series forecasting.","sentences":["The intricate nature of time series data analysis benefits greatly from the distinct advantages offered by time and frequency domain representations.","While the time domain is superior in representing local dependencies, particularly in non-periodic series, the frequency domain excels in capturing global dependencies, making it ideal for series with evident periodic patterns.","To capitalize on both of these strengths, we propose ATFNet, an innovative framework that combines a time domain module and a frequency domain module to concurrently capture local and global dependencies in time series data.","Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel mechanism for dynamically adjusting the weights between the two modules based on the periodicity of the input time series.","In the frequency domain module, we enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT, designed to address the challenge of discrete frequency misalignment.","Additionally, our Complex-valued Spectrum Attention mechanism offers a novel approach to discern the intricate relationships between different frequency combinations.","Extensive experiments across multiple real-world datasets demonstrate that our ATFNet framework outperforms current state-of-the-art methods in long-term time series forecasting."],"url":"http://arxiv.org/abs/2404.05192v1","category":"cs.LG"}
{"created":"2024-04-08 04:13:35","title":"Adaptive Learning for Multi-view Stereo Reconstruction","abstract":"Deep learning has recently demonstrated its excellent performance on the task of multi-view stereo (MVS). However, loss functions applied for deep MVS are rarely studied. In this paper, we first analyze existing loss functions' properties for deep depth based MVS approaches. Regression based loss leads to inaccurate continuous results by computing mathematical expectation, while classification based loss outputs discretized depth values. To this end, we then propose a novel loss function, named adaptive Wasserstein loss, which is able to narrow down the difference between the true and predicted probability distributions of depth. Besides, a simple but effective offset module is introduced to better achieve sub-pixel prediction accuracy. Extensive experiments on different benchmarks, including DTU, Tanks and Temples and BlendedMVS, show that the proposed method with the adaptive Wasserstein loss and the offset module achieves state-of-the-art performance.","sentences":["Deep learning has recently demonstrated its excellent performance on the task of multi-view stereo (MVS).","However, loss functions applied for deep MVS are rarely studied.","In this paper, we first analyze existing loss functions' properties for deep depth based MVS approaches.","Regression based loss leads to inaccurate continuous results by computing mathematical expectation, while classification based loss outputs discretized depth values.","To this end, we then propose a novel loss function, named adaptive Wasserstein loss, which is able to narrow down the difference between the true and predicted probability distributions of depth.","Besides, a simple but effective offset module is introduced to better achieve sub-pixel prediction accuracy.","Extensive experiments on different benchmarks, including DTU, Tanks and Temples and BlendedMVS, show that the proposed method with the adaptive Wasserstein loss and the offset module achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.05181v1","category":"cs.CV"}
{"created":"2024-04-08 03:29:58","title":"Adapting to Covariate Shift in Real-time by Encoding Trees with Motion Equations","abstract":"Input distribution shift presents a significant problem in many real-world systems. Here we present Xenovert, an adaptive algorithm that can dynamically adapt to changes in input distribution. It is a perfect binary tree that adaptively divides a continuous input space into several intervals of uniform density while receiving a continuous stream of input. This process indirectly maps the source distribution to the shifted target distribution, preserving the data's relationship with the downstream decoder/operation, even after the shift occurs. In this paper, we demonstrated how a neural network integrated with Xenovert achieved better results in 4 out of 5 shifted datasets, saving the hurdle of retraining a machine learning model. We anticipate that Xenovert can be applied to many more applications that require adaptation to unforeseen input distribution shifts, even when the distribution shift is drastic.","sentences":["Input distribution shift presents a significant problem in many real-world systems.","Here we present Xenovert, an adaptive algorithm that can dynamically adapt to changes in input distribution.","It is a perfect binary tree that adaptively divides a continuous input space into several intervals of uniform density while receiving a continuous stream of input.","This process indirectly maps the source distribution to the shifted target distribution, preserving the data's relationship with the downstream decoder/operation, even after the shift occurs.","In this paper, we demonstrated how a neural network integrated with Xenovert achieved better results in 4 out of 5 shifted datasets, saving the hurdle of retraining a machine learning model.","We anticipate that Xenovert can be applied to many more applications that require adaptation to unforeseen input distribution shifts, even when the distribution shift is drastic."],"url":"http://arxiv.org/abs/2404.05168v1","category":"cs.LG"}
{"created":"2024-04-08 03:06:19","title":"Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular Videos","abstract":"In this work, we pioneer Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos. In contrast to previous NeRF methods that reconstruct dynamic scenes from the colors and volume densities of individual points, Semantic Flow learns semantics from continuous flows that contain rich 3D motion information. As there is 2D-to-3D ambiguity problem in the viewing direction when extracting 3D flow features from 2D video frames, we consider the volume densities as opacity priors that describe the contributions of flow features to the semantics on the frames. More specifically, we first learn a flow network to predict flows in the dynamic scene, and propose a flow feature aggregation module to extract flow features from video frames. Then, we propose a flow attention module to extract motion information from flow features, which is followed by a semantic network to output semantic logits of flows. We integrate the logits with volume densities in the viewing direction to supervise the flow features with semantic labels on video frames. Experimental results show that our model is able to learn from multiple dynamic scenes and supports a series of new tasks such as instance-level scene editing, semantic completions, dynamic scene tracking and semantic adaption on novel scenes. Codes are available at https://github.com/tianfr/Semantic-Flow/.","sentences":["In this work, we pioneer Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos.","In contrast to previous NeRF methods that reconstruct dynamic scenes from the colors and volume densities of individual points, Semantic Flow learns semantics from continuous flows that contain rich 3D motion information.","As there is 2D-to-3D ambiguity problem in the viewing direction when extracting 3D flow features from 2D video frames, we consider the volume densities as opacity priors that describe the contributions of flow features to the semantics on the frames.","More specifically, we first learn a flow network to predict flows in the dynamic scene, and propose a flow feature aggregation module to extract flow features from video frames.","Then, we propose a flow attention module to extract motion information from flow features, which is followed by a semantic network to output semantic logits of flows.","We integrate the logits with volume densities in the viewing direction to supervise the flow features with semantic labels on video frames.","Experimental results show that our model is able to learn from multiple dynamic scenes and supports a series of new tasks such as instance-level scene editing, semantic completions, dynamic scene tracking and semantic adaption on novel scenes.","Codes are available at https://github.com/tianfr/Semantic-Flow/."],"url":"http://arxiv.org/abs/2404.05163v1","category":"cs.CV"}
{"created":"2024-04-08 02:02:15","title":"UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather","abstract":"LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress. However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather. The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications. To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models. UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes. Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains. Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations. We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather. Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods. The code will be released.","sentences":["LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress.","However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather.","The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications.","To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models.","UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes.","Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains.","Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations.","We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather.","Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods.","The code will be released."],"url":"http://arxiv.org/abs/2404.05145v1","category":"cs.CV"}
{"created":"2024-04-08 01:25:38","title":"LLM-BT: Performing Robotic Adaptive Tasks based on Large Language Models and Behavior Trees","abstract":"Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks. However, handling external disturbances during tasks is still an open challenge. This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs). It utilizes ChatGPT to reason the descriptive steps of tasks. In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm. Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs. Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks. Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances. Our method is validated with simulation in different practical scenarios.","sentences":["Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks.","However, handling external disturbances during tasks is still an open challenge.","This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs).","It utilizes ChatGPT to reason the descriptive steps of tasks.","In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm.","Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs.","Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks.","Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances.","Our method is validated with simulation in different practical scenarios."],"url":"http://arxiv.org/abs/2404.05134v1","category":"cs.RO"}
{"created":"2024-04-08 01:00:37","title":"Well-posedness of the 2D surface quasi-geostrophic equation in variable Lebesgue spaces","abstract":"In this paper, we are mainly concerned with the well-posedness of the dissipative surface quasi-geostrophic equation in the framework of variable Lebesgue spaces. Based on some analytical results developed in the variable Lebesgue spaces and the $L^{p}$-$L^{q}$ decay estimates of the fractional heat kernel, we establish, for the 2D dissipative surface quasi-geostrophic equation, the global well-posedness in the space $\\mathcal{L}^{p(\\cdot)}_{\\frac{2}{\\alpha-1}}(\\mathbb{R}^{2},L^{\\infty}(0,\\infty))$ and the local well-posedness in the space $L^{q(\\cdot)}(0,T; L^{p}(\\mathbb{R}^{2}))$.","sentences":["In this paper, we are mainly concerned with the well-posedness of the dissipative surface quasi-geostrophic equation in the framework of variable Lebesgue spaces.","Based on some analytical results developed in the variable Lebesgue spaces and the $L^{p}$-$L^{q}$ decay estimates of the fractional heat kernel, we establish, for the 2D dissipative surface quasi-geostrophic equation, the global well-posedness in the space $\\mathcal{L}^{p(\\cdot)}_{\\frac{2}{\\alpha-1}}(\\mathbb{R}^{2},L^{\\infty}(0,\\infty))$ and the local well-posedness in the space $L^{q(\\cdot)}(0,T; L^{p}(\\mathbb{R}^{2}))$."],"url":"http://arxiv.org/abs/2404.05127v1","category":"math.AP"}
{"created":"2024-04-08 00:36:19","title":"Nanouniverse: Virtual Instancing of Structural Detail and Adaptive Shell Mapping","abstract":"Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches. In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing. Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements. Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration. We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing. We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment. Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes. Our method also supports advanced effects such as clipping planes and animations. We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms.","sentences":["Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches.","In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing.","Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements.","Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration.","We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing.","We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment.","Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes.","Our method also supports advanced effects such as clipping planes and animations.","We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms."],"url":"http://arxiv.org/abs/2404.05116v1","category":"cs.GR"}
{"created":"2024-04-08 00:13:05","title":"Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation","abstract":"In Generalized Few-shot Segmentation (GFSS), a model is trained with a large corpus of base class samples and then adapted on limited samples of novel classes. This paper focuses on the relevance between base and novel classes, and improves GFSS in two aspects: 1) mining the similarity between base and novel classes to promote the learning of novel classes, and 2) mitigating the class imbalance issue caused by the volume difference between the support set and the training set. Specifically, we first propose a similarity transition matrix to guide the learning of novel classes with base class knowledge. Then, we leverage the Label-Distribution-Aware Margin (LDAM) loss and Transductive Inference to the GFSS task to address the problem of class imbalance as well as overfitting the support set. In addition, by extending the probability transition matrix, the proposed method can mitigate the catastrophic forgetting of base classes when learning novel classes. With a simple training phase, our proposed method can be applied to any segmentation network trained on base classes. We validated our methods on the adapted version of OpenEarthMap. Compared to existing GFSS baselines, our method excels them all from 3% to 7% and ranks second in the OpenEarthMap Land Cover Mapping Few-Shot Challenge at the completion of this paper. Code: https://github.com/earth-insights/ClassTrans","sentences":["In Generalized Few-shot Segmentation (GFSS), a model is trained with a large corpus of base class samples and then adapted on limited samples of novel classes.","This paper focuses on the relevance between base and novel classes, and improves GFSS in two aspects: 1) mining the similarity between base and novel classes to promote the learning of novel classes, and 2) mitigating the class imbalance issue caused by the volume difference between the support set and the training set.","Specifically, we first propose a similarity transition matrix to guide the learning of novel classes with base class knowledge.","Then, we leverage the Label-Distribution-Aware Margin (LDAM) loss and Transductive Inference to the GFSS task to address the problem of class imbalance as well as overfitting the support set.","In addition, by extending the probability transition matrix, the proposed method can mitigate the catastrophic forgetting of base classes when learning novel classes.","With a simple training phase, our proposed method can be applied to any segmentation network trained on base classes.","We validated our methods on the adapted version of OpenEarthMap.","Compared to existing GFSS baselines, our method excels them all from 3% to 7% and ranks second in the OpenEarthMap Land Cover Mapping Few-Shot Challenge at the completion of this paper.","Code: https://github.com/earth-insights/ClassTrans"],"url":"http://arxiv.org/abs/2404.05111v1","category":"cs.CV"}
{"created":"2024-04-07 22:31:34","title":"Active Test-Time Adaptation: Theoretical Analyses and An Algorithm","abstract":"Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies.   To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting.   We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA","sentences":["Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings.","Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies.   ","To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting.   ","We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee.","We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF).","We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques.","Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods.","Our code is available at https://github.com/divelab/ATTA"],"url":"http://arxiv.org/abs/2404.05094v1","category":"cs.LG"}
{"created":"2024-04-07 22:00:50","title":"A Note on LoRA","abstract":"LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy. This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale. Without introducing new experiments, we aim to improve the understanding and application of LoRA.","sentences":["LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy.","This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale.","Without introducing new experiments, we aim to improve the understanding and application of LoRA."],"url":"http://arxiv.org/abs/2404.05086v1","category":"cs.LG"}
{"created":"2024-04-07 20:49:26","title":"STAIC regularization for spatio-temporal image reconstruction","abstract":"We propose a regularization-based image restoration scheme for 2D images recorded over time (2D+t). We design an infimal convolution-based regularization function which we call spatio-temporal Adaptive Infimal Convolution (STAIC) regularization. We formulate the infimal convolution in the form of an additive decomposition of the 2D+t image such that the extent of spatial and temporal smoothing is controlled in a spatially and temporally varying manner. This makes the regularization adaptable to the local characteristics of the motion leading to an improved ability to handle noise. We also develop a minimization method for image reconstruction by using the proposed form of regularization. We demonstrate the effectiveness of the proposed regularization using TIRF images recorded over time and compare with some selected existing regularizations.","sentences":["We propose a regularization-based image restoration scheme for 2D images recorded over time (2D+t).","We design an infimal convolution-based regularization function which we call spatio-temporal Adaptive Infimal Convolution (STAIC) regularization.","We formulate the infimal convolution in the form of an additive decomposition of the 2D+t image such that the extent of spatial and temporal smoothing is controlled in a spatially and temporally varying manner.","This makes the regularization adaptable to the local characteristics of the motion leading to an improved ability to handle noise.","We also develop a minimization method for image reconstruction by using the proposed form of regularization.","We demonstrate the effectiveness of the proposed regularization using TIRF images recorded over time and compare with some selected existing regularizations."],"url":"http://arxiv.org/abs/2404.05070v1","category":"eess.IV"}
{"created":"2024-04-07 20:37:08","title":"Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization","abstract":"The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems. The method divides the area covered by the system into several zones and assigns them anchor pair sets. The pair sets are determined during calibration based on localization root mean square error (RMSE). The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones. The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference. For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work. The proposed method has been tested with simulations and experiments. The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy. In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm.","sentences":["The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems.","The method divides the area covered by the system into several zones and assigns them anchor pair sets.","The pair sets are determined during calibration based on localization root mean square error (RMSE).","The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones.","The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference.","For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work.","The proposed method has been tested with simulations and experiments.","The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy.","In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm."],"url":"http://arxiv.org/abs/2404.05067v1","category":"cs.RO"}
{"created":"2024-04-07 20:15:40","title":"Automated Prediction of Breast Cancer Response to Neoadjuvant Chemotherapy from DWI Data","abstract":"Effective surgical planning for breast cancer hinges on accurately predicting pathological complete response (pCR) to neoadjuvant chemotherapy (NAC). Diffusion-weighted MRI (DWI) and machine learning offer a non-invasive approach for early pCR assessment. However, most machine-learning models require manual tumor segmentation, a cumbersome and error-prone task. We propose a deep learning model employing \"Size-Adaptive Lesion Weighting\" for automatic DWI tumor segmentation to enhance pCR prediction accuracy. Despite histopathological changes during NAC complicating DWI image segmentation, our model demonstrates robust performance. Utilizing the BMMR2 challenge dataset, it matches human experts in pCR prediction pre-NAC with an area under the curve (AUC) of 0.76 vs. 0.796, and surpasses standard automated methods mid-NAC, with an AUC of 0.729 vs. 0.654 and 0.576. Our approach represents a significant advancement in automating breast cancer treatment planning, enabling more reliable pCR predictions without manual segmentation.","sentences":["Effective surgical planning for breast cancer hinges on accurately predicting pathological complete response (pCR) to neoadjuvant chemotherapy (NAC).","Diffusion-weighted MRI (DWI) and machine learning offer a non-invasive approach for early pCR assessment.","However, most machine-learning models require manual tumor segmentation, a cumbersome and error-prone task.","We propose a deep learning model employing \"Size-Adaptive Lesion Weighting\" for automatic DWI tumor segmentation to enhance pCR prediction accuracy.","Despite histopathological changes during NAC complicating DWI image segmentation, our model demonstrates robust performance.","Utilizing the BMMR2 challenge dataset, it matches human experts in pCR prediction pre-NAC with an area under the curve (AUC) of 0.76 vs. 0.796, and surpasses standard automated methods mid-NAC, with an AUC of 0.729 vs. 0.654 and 0.576.","Our approach represents a significant advancement in automating breast cancer treatment planning, enabling more reliable pCR predictions without manual segmentation."],"url":"http://arxiv.org/abs/2404.05061v1","category":"cs.CV"}
{"created":"2024-04-07 20:05:49","title":"A robust assessment for invariant representations","abstract":"The performance of machine learning models can be impacted by changes in data over time. A promising approach to address this challenge is invariant learning, with a particular focus on a method known as invariant risk minimization (IRM). This technique aims to identify a stable data representation that remains effective with out-of-distribution (OOD) data. While numerous studies have developed IRM-based methods adaptive to data augmentation scenarios, there has been limited attention on directly assessing how well these representations preserve their invariant performance under varying conditions. In our paper, we propose a novel method to evaluate invariant performance, specifically tailored for IRM-based methods. We establish a bridge between the conditional expectation of an invariant predictor across different environments through the likelihood ratio. Our proposed criterion offers a robust basis for evaluating invariant performance. We validate our approach with theoretical support and demonstrate its effectiveness through extensive numerical studies.These experiments illustrate how our method can assess the invariant performance of various representation techniques.","sentences":["The performance of machine learning models can be impacted by changes in data over time.","A promising approach to address this challenge is invariant learning, with a particular focus on a method known as invariant risk minimization (IRM).","This technique aims to identify a stable data representation that remains effective with out-of-distribution (OOD) data.","While numerous studies have developed IRM-based methods adaptive to data augmentation scenarios, there has been limited attention on directly assessing how well these representations preserve their invariant performance under varying conditions.","In our paper, we propose a novel method to evaluate invariant performance, specifically tailored for IRM-based methods.","We establish a bridge between the conditional expectation of an invariant predictor across different environments through the likelihood ratio.","Our proposed criterion offers a robust basis for evaluating invariant performance.","We validate our approach with theoretical support and demonstrate its effectiveness through extensive numerical studies.","These experiments illustrate how our method can assess the invariant performance of various representation techniques."],"url":"http://arxiv.org/abs/2404.05058v1","category":"cs.LG"}
{"created":"2024-04-07 19:23:28","title":"Facial Affective Behavior Analysis with Instruction Tuning","abstract":"Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images. However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors. The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks. However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency. To address these challenges, we introduce (i) an instruction-following dataset for two FABA tasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii) a new MLLM \"EmoLA\" as a strong baseline to the community. Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, i.e., fine-grained facial movement, interpretability, and reasoning. Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM. We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets. The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench. On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models.","sentences":["Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images.","However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors.","The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks.","However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency.","To address these challenges, we introduce (i) an instruction-following dataset for two FABA tasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii) a new MLLM \"EmoLA\" as a strong baseline to the community.","Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, i.e., fine-grained facial movement, interpretability, and reasoning.","Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM.","We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets.","The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench.","On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models."],"url":"http://arxiv.org/abs/2404.05052v1","category":"cs.CV"}
{"created":"2024-04-07 18:53:30","title":"How Do OSS Developers Utilize Architectural Solutions from Q&A Sites: An Empirical Study","abstract":"Developers utilize programming-related knowledge (e.g., code snippets) on Q&A sites (e.g., Stack Overflow) that functionally matches the programming problems they encounter in their development. Despite extensive research on Q&A sites, being a high-level and important type of development-related knowledge, architectural solutions (e.g., architecture tactics) and their utilization are rarely explored. To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study. For the mining study, we mined 984 commits and issues (i.e., 821 commits and 163 issues) from 893 Open-Source Software (OSS) projects on GitHub that explicitly referenced architectural solutions from Stack Overflow (SO) and Software Engineering Stack Exchange (SWESE). For the survey study, we identified practitioners involved in the utilization of these architectural solutions and surveyed 227 of them to further understand how practitioners utilize architectural solutions from Q&A sites in their OSS development. Our main findings are that: (1) OSS practitioners use architectural solutions from Q&A sites to solve a large variety (15 categories) of architectural problems, wherein Component design issue, Architectural anti-pattern, and Security issue are dominant; (2) Seven categories of architectural solutions from Q&A sites have been utilized to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most utilized architectural solutions; (3) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects.","sentences":["Developers utilize programming-related knowledge (e.g., code snippets) on Q&A sites (e.g., Stack Overflow) that functionally matches the programming problems they encounter in their development.","Despite extensive research on Q&A sites, being a high-level and important type of development-related knowledge, architectural solutions (e.g., architecture tactics) and their utilization are rarely explored.","To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study.","For the mining study, we mined 984 commits and issues (i.e., 821 commits and 163 issues) from 893 Open-Source Software (OSS) projects on GitHub that explicitly referenced architectural solutions from Stack Overflow (SO) and Software Engineering Stack Exchange (SWESE).","For the survey study, we identified practitioners involved in the utilization of these architectural solutions and surveyed 227 of them to further understand how practitioners utilize architectural solutions from Q&A sites in their OSS development.","Our main findings are that: (1) OSS practitioners use architectural solutions from Q&A sites to solve a large variety (15 categories) of architectural problems, wherein Component design issue, Architectural anti-pattern, and Security issue are dominant; (2) Seven categories of architectural solutions from Q&A sites have been utilized to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most utilized architectural solutions; (3) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects."],"url":"http://arxiv.org/abs/2404.05041v1","category":"cs.SE"}
{"created":"2024-04-07 18:52:10","title":"Lagrangian operator inference enhanced with structure-preserving machine learning for nonintrusive model reduction of mechanical systems","abstract":"Complex mechanical systems often exhibit strongly nonlinear behavior due to the presence of nonlinearities in the energy dissipation mechanisms, material constitutive relationships, or geometric/connectivity mechanics. Numerical modeling of these systems leads to nonlinear full-order models that possess an underlying Lagrangian structure. This work proposes a Lagrangian operator inference method enhanced with structure-preserving machine learning to learn nonlinear reduced-order models (ROMs) of nonlinear mechanical systems. This two-step approach first learns the best-fit linear Lagrangian ROM via Lagrangian operator inference and then presents a structure-preserving machine learning method to learn nonlinearities in the reduced space. The proposed approach can learn a structure-preserving nonlinear ROM purely from data, unlike the existing operator inference approaches that require knowledge about the mathematical form of nonlinear terms. From a machine learning perspective, it accelerates the training of the structure-preserving neural network by providing an informed prior, and it reduces the computational cost of the network training by operating on the reduced space. The method is first demonstrated on two simulated examples: a conservative nonlinear rod model and a two-dimensional nonlinear membrane with nonlinear internal damping. Finally, the method is demonstrated on an experimental dataset consisting of digital image correlation measurements taken from a lap-joint beam structure from which a predictive model is learned that captures amplitude-dependent frequency and damping characteristics accurately. The numerical results demonstrate that the proposed approach yields generalizable nonlinear ROMs that exhibit bounded energy error, capture the nonlinear characteristics reliably, and provide accurate long-time predictions outside the training data regime.","sentences":["Complex mechanical systems often exhibit strongly nonlinear behavior due to the presence of nonlinearities in the energy dissipation mechanisms, material constitutive relationships, or geometric/connectivity mechanics.","Numerical modeling of these systems leads to nonlinear full-order models that possess an underlying Lagrangian structure.","This work proposes a Lagrangian operator inference method enhanced with structure-preserving machine learning to learn nonlinear reduced-order models (ROMs) of nonlinear mechanical systems.","This two-step approach first learns the best-fit linear Lagrangian ROM via Lagrangian operator inference and then presents a structure-preserving machine learning method to learn nonlinearities in the reduced space.","The proposed approach can learn a structure-preserving nonlinear ROM purely from data, unlike the existing operator inference approaches that require knowledge about the mathematical form of nonlinear terms.","From a machine learning perspective, it accelerates the training of the structure-preserving neural network by providing an informed prior, and it reduces the computational cost of the network training by operating on the reduced space.","The method is first demonstrated on two simulated examples: a conservative nonlinear rod model and a two-dimensional nonlinear membrane with nonlinear internal damping.","Finally, the method is demonstrated on an experimental dataset consisting of digital image correlation measurements taken from a lap-joint beam structure from which a predictive model is learned that captures amplitude-dependent frequency and damping characteristics accurately.","The numerical results demonstrate that the proposed approach yields generalizable nonlinear ROMs that exhibit bounded energy error, capture the nonlinear characteristics reliably, and provide accurate long-time predictions outside the training data regime."],"url":"http://arxiv.org/abs/2404.05040v1","category":"cs.CE"}
{"created":"2024-04-07 17:25:52","title":"DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology","abstract":"In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images. To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.","sentences":["In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears.","However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images.","To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline.","Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images.","To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift.","We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin.","A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets.","All models are available at github.com/marrlab/DinoBloom."],"url":"http://arxiv.org/abs/2404.05022v1","category":"cs.CV"}
{"created":"2024-04-07 16:49:07","title":"MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators","abstract":"Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions. A largely overlooked problem in T2V is that existing models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations. In this paper, we propose \\textbf{MagicTime}, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation. First, we design a MagicAdapter scheme to decouple spatial and temporal training, encode more physical knowledge from metamorphic videos, and transform pre-trained T2V models to generate metamorphic videos. Second, we introduce a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have a wider variation range and cover dramatic object metamorphic processes, thus embodying more physical knowledge than general videos. Finally, we introduce a Magic Text-Encoder to improve the understanding of metamorphic video prompts. Furthermore, we create a time-lapse video-text dataset called \\textbf{ChronoMagic}, specifically curated to unlock the metamorphic video generation ability. Extensive experiments demonstrate the superiority and effectiveness of MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse video generation is a promising path toward building metamorphic simulators of the physical world.","sentences":["Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.","A largely overlooked problem in T2V is that existing models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations.","In this paper, we propose \\textbf{MagicTime}, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation.","First, we design a MagicAdapter scheme to decouple spatial and temporal training, encode more physical knowledge from metamorphic videos, and transform pre-trained T2V models to generate metamorphic videos.","Second, we introduce a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have a wider variation range and cover dramatic object metamorphic processes, thus embodying more physical knowledge than general videos.","Finally, we introduce a Magic Text-Encoder to improve the understanding of metamorphic video prompts.","Furthermore, we create a time-lapse video-text dataset called \\textbf{ChronoMagic}, specifically curated to unlock the metamorphic video generation ability.","Extensive experiments demonstrate the superiority and effectiveness of MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse video generation is a promising path toward building metamorphic simulators of the physical world."],"url":"http://arxiv.org/abs/2404.05014v1","category":"cs.CV"}
{"created":"2024-04-07 16:47:18","title":"Inducing a Metal-Insulator Transition through Systematic Alterations of Local Rewriting Rules in a Quantum Graph","abstract":"The Anderson localization transition in quantum graphs has garnered significant recent attention due to its relevance to many-body localization studies. Typically, graphs are constructed using top-down methods. Here, we explore a bottom-up approach, employing a simple local rewriting rule to construct the graph. Through the use of ratio statistics for the energy spectrum and Kullback-Leibler divergence correlations for the eigenstates, numerical analysis demonstrates that slight adjustments to the rewriting rule can induce a transition from a localized to an extended quantum phase. This extended state exhibits non-ergodic behavior, akin to the non-ergodic extended phase observed in the Porter-Rosenzweig model and suggested for many-body localization. Thus, by adapting straightforward local rewriting rules, it becomes feasible to assemble complex graphs from which desired global quantum phases emerge. This approach holds promise for numerical investigations and could be implemented in building optical realizations of complex networks using optical fibers and beam splitters.","sentences":["The Anderson localization transition in quantum graphs has garnered significant recent attention due to its relevance to many-body localization studies.","Typically, graphs are constructed using top-down methods.","Here, we explore a bottom-up approach, employing a simple local rewriting rule to construct the graph.","Through the use of ratio statistics for the energy spectrum and Kullback-Leibler divergence correlations for the eigenstates, numerical analysis demonstrates that slight adjustments to the rewriting rule can induce a transition from a localized to an extended quantum phase.","This extended state exhibits non-ergodic behavior, akin to the non-ergodic extended phase observed in the Porter-Rosenzweig model and suggested for many-body localization.","Thus, by adapting straightforward local rewriting rules, it becomes feasible to assemble complex graphs from which desired global quantum phases emerge.","This approach holds promise for numerical investigations and could be implemented in building optical realizations of complex networks using optical fibers and beam splitters."],"url":"http://arxiv.org/abs/2404.05013v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-07 16:16:44","title":"Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats","abstract":"Dynamic routing is one of the representative control scheme in transportation, production lines, and data transmission. In the modern context of connectivity and autonomy, routing decisions are potentially vulnerable to malicious attacks. In this paper, we consider the dynamic routing problem over parallel traffic links in the face of such threats. An attacker is capable of increasing or destabilizing traffic queues by strategic manipulating the nominally optimal routing decisions. A defender is capable of securing the correct routing decision. Attacking and defensive actions induce technological costs. The defender has no prior information about the attacker's strategy. We develop an least-square policy iteration algorithm for the defender to compute a cost-aware and threat-adaptive defensive strategy. The policy evaluation step computes a weight vector that minimizes the sampled temporal-difference error. We derive a concrete theoretical upper bound on the evaluation error based on the theory of value function approximation. The policy improvement step solves a minimax problem and thus iteratively computes the Markov perfect equilibrium of the security game. We also discuss the training error of the entire policy iteration process.","sentences":["Dynamic routing is one of the representative control scheme in transportation, production lines, and data transmission.","In the modern context of connectivity and autonomy, routing decisions are potentially vulnerable to malicious attacks.","In this paper, we consider the dynamic routing problem over parallel traffic links in the face of such threats.","An attacker is capable of increasing or destabilizing traffic queues by strategic manipulating the nominally optimal routing decisions.","A defender is capable of securing the correct routing decision.","Attacking and defensive actions induce technological costs.","The defender has no prior information about the attacker's strategy.","We develop an least-square policy iteration algorithm for the defender to compute a cost-aware and threat-adaptive defensive strategy.","The policy evaluation step computes a weight vector that minimizes the sampled temporal-difference error.","We derive a concrete theoretical upper bound on the evaluation error based on the theory of value function approximation.","The policy improvement step solves a minimax problem and thus iteratively computes the Markov perfect equilibrium of the security game.","We also discuss the training error of the entire policy iteration process."],"url":"http://arxiv.org/abs/2404.05008v1","category":"eess.SY"}
{"created":"2024-04-07 15:44:20","title":"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression","abstract":"The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.","sentences":["The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny.","Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations.","This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms.","Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts.","This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks.","We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content.","By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability.","Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications.","This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions."],"url":"http://arxiv.org/abs/2404.04997v1","category":"cs.LG"}
{"created":"2024-04-07 15:34:40","title":"Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM","abstract":"As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments. Previous methods don't excel in extracting long-range contextual features and overlook the connectivity between discrete pixels. Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks. Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images. In addition, the single-position prompt of SAM is very insufficient for prior guidance. To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS. To this end, we first introduce a dual structure with SAM's paradigm to enhance feature learning of marine images. Then, we propose a Multi-level Coupled Prompt (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM's encoder with adapters. Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM's encoder. Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels. With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques. Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets. The code is available at https://github.com/Drchip61/Dual_SAM.","sentences":["As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments.","Previous methods don't excel in extracting long-range contextual features and overlook the connectivity between discrete pixels.","Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks.","Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images.","In addition, the single-position prompt of SAM is very insufficient for prior guidance.","To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS.","To this end, we first introduce a dual structure with SAM's paradigm to enhance feature learning of marine images.","Then, we propose a Multi-level Coupled Prompt (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM's encoder with adapters.","Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM's encoder.","Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels.","With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques.","Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets.","The code is available at https://github.com/Drchip61/Dual_SAM."],"url":"http://arxiv.org/abs/2404.04996v1","category":"cs.CV"}
{"created":"2024-04-07 15:23:28","title":"MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models","abstract":"The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning. To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions in both free-form and multiple-choice. We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE. Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages. However, their generalization capabilities are limited in multi-language experiments. Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families. These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for benchmarking and solution development.","sentences":["The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters.","Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning.","To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German.","MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions in both free-form and multiple-choice.","We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE.","Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages.","However, their generalization capabilities are limited in multi-language experiments.","Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families.","These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for benchmarking and solution development."],"url":"http://arxiv.org/abs/2404.04990v1","category":"cs.CL"}
{"created":"2024-04-07 15:06:48","title":"Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection","abstract":"We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection, a novel video anomaly detection methodology that combines pseudo-anomalies, dynamic anomaly weighting, and a distinction loss function to improve detection accuracy. By training on pseudo-anomalies, our approach adapts to the variability of normal and anomalous behaviors without fixed anomaly thresholds. Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech datasets, where individual models are tailored for each scene. These achievements highlight DDL's effectiveness in advancing anomaly detection, offering a scalable and adaptable solution for video surveillance challenges.","sentences":["We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection, a novel video anomaly detection methodology that combines pseudo-anomalies, dynamic anomaly weighting, and a distinction loss function to improve detection accuracy.","By training on pseudo-anomalies, our approach adapts to the variability of normal and anomalous behaviors without fixed anomaly thresholds.","Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech datasets, where individual models are tailored for each scene.","These achievements highlight DDL's effectiveness in advancing anomaly detection, offering a scalable and adaptable solution for video surveillance challenges."],"url":"http://arxiv.org/abs/2404.04986v1","category":"cs.CV"}
{"created":"2024-04-07 14:21:37","title":"FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation","abstract":"Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appealing where only unlabeled images are needed for the adaptation. Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain. In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation. It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set and a pseudo target-domain set. To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the domain-invariant structure features, generating high-quality pseudo labels for target-domain images. We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels. Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases.","sentences":["Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appealing where only unlabeled images are needed for the adaptation.","Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain.","In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation.","It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set and a pseudo target-domain set.","To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the domain-invariant structure features, generating high-quality pseudo labels for target-domain images.","We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels.","Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases."],"url":"http://arxiv.org/abs/2404.04971v1","category":"cs.CV"}
{"created":"2024-04-07 14:19:22","title":"Temporal Generalization Estimation in Evolving Graphs","abstract":"Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve. We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory. In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution. Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction. In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance. Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs. The ablation studies underscore the necessity of graph reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction.","sentences":["Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve.","We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time.","To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory.","In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution.","Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction.","In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance.","Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs.","The ablation studies underscore the necessity of graph reconstruction.","For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction."],"url":"http://arxiv.org/abs/2404.04969v1","category":"cs.LG"}
{"created":"2024-04-07 13:02:21","title":"SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning","abstract":"Large language models (LLMs) are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains. However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer. In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models. Utilizing this framework, we trained a financial multi-task LLM named \"SilverSight\". Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities.","sentences":["Large language models (LLMs) are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains.","However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer.","In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models.","Utilizing this framework, we trained a financial multi-task LLM named \"SilverSight\".","Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities."],"url":"http://arxiv.org/abs/2404.04949v1","category":"cs.CL"}
{"created":"2024-04-07 12:57:41","title":"AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment","abstract":"Recent video editing advancements rely on accurate pose sequences to animate subjects. However, these efforts are not suitable for cross-species animation due to pose misalignment between species (for example, the poses of a cat differs greatly from that of a pig due to differences in body structure). In this paper, we present AnimateZoo, a zero-shot diffusion-based video generator to address this challenging cross-species animation issue, aiming to accurately produce animal animations while preserving the background. The key technique used in our AnimateZoo is subject alignment, which includes two steps. First, we improve appearance feature extraction by integrating a Laplacian detail booster and a prompt-tuning identity extractor. These components are specifically designed to capture essential appearance information, including identity and fine details. Second, we align shape features and address conflicts from differing subjects by introducing a scale-information remover. This ensures accurate cross-species animation. Moreover, we introduce two high-quality animal video datasets featuring a wide variety of species. Trained on these extensive datasets, our model is capable of generating videos characterized by accurate movements, consistent appearance, and high-fidelity frames, without the need for the pre-inference fine-tuning that prior arts required. Extensive experiments showcase the outstanding performance of our method in cross-species action following tasks, demonstrating exceptional shape adaptation capability. The project page is available at https://justinxu0.github.io/AnimateZoo/.","sentences":["Recent video editing advancements rely on accurate pose sequences to animate subjects.","However, these efforts are not suitable for cross-species animation due to pose misalignment between species (for example, the poses of a cat differs greatly from that of a pig due to differences in body structure).","In this paper, we present AnimateZoo, a zero-shot diffusion-based video generator to address this challenging cross-species animation issue, aiming to accurately produce animal animations while preserving the background.","The key technique used in our AnimateZoo is subject alignment, which includes two steps.","First, we improve appearance feature extraction by integrating a Laplacian detail booster and a prompt-tuning identity extractor.","These components are specifically designed to capture essential appearance information, including identity and fine details.","Second, we align shape features and address conflicts from differing subjects by introducing a scale-information remover.","This ensures accurate cross-species animation.","Moreover, we introduce two high-quality animal video datasets featuring a wide variety of species.","Trained on these extensive datasets, our model is capable of generating videos characterized by accurate movements, consistent appearance, and high-fidelity frames, without the need for the pre-inference fine-tuning that prior arts required.","Extensive experiments showcase the outstanding performance of our method in cross-species action following tasks, demonstrating exceptional shape adaptation capability.","The project page is available at https://justinxu0.github.io/AnimateZoo/."],"url":"http://arxiv.org/abs/2404.04946v1","category":"cs.CV"}
{"created":"2024-04-07 10:49:59","title":"CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis","abstract":"Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, several factors have impeded its further proliferation as next-generation 3D media. To establish a ubiquitous presence in everyday media formats, such as images and videos, it is imperative to devise a solution that effectively fulfills three key objectives: fast encoding and decoding time, compact model sizes, and high-quality renderings. Despite significant advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of a novel encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we develop a novel finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 150x and 20x reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets, such as ShapeNet and Objaverse.","sentences":["Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes.","However, several factors have impeded its further proliferation as next-generation 3D media.","To establish a ubiquitous presence in everyday media formats, such as images and videos, it is imperative to devise a solution that effectively fulfills three key objectives: fast encoding and decoding time, compact model sizes, and high-quality renderings.","Despite significant advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized.","In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of a novel encoder and decoder architecture that can generate a NeRF representation in a single forward pass.","Furthermore, inspired by the recent parameter-efficient finetuning approaches, we develop a novel finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes.","The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 150x and 20x reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets, such as ShapeNet and Objaverse."],"url":"http://arxiv.org/abs/2404.04913v1","category":"cs.CV"}
{"created":"2024-04-07 09:59:10","title":"Cracking and complexity of self-gravitating dissipative compact objects","abstract":"The concept of cracking refers to the tendency of a fluid distribution to \"split'', once it abandons the equilibrium. In this manuscript we develop a general formalism to describe the occurrence of cracking within a dissipative fluid distribution, in comoving coordinates. The role of dissipative processes in the occurrence of cracking is brought out. Next, we relate the occurrence of cracking with the concept of complexity for self-gravitating objects defined in [1-3]. More specifically we relate the occurrence of cracking with the condition of the vanishing of the scalar function intended to measure the complexity of the fluid distribution (the complexity factor). We also relate the occurrence of cracking with the specific mode of leaving the equilibrium. Thus, we prove that leaving the equilibrium in either, the homologous (H), or the quasi--homologous regime (QH), prevents the occurrence of cracking. Also, it is shown that imposing the condition of vanishing complexity factor alone, (independently of the mode of leaving the equilibrium) prevents the occurrence of cracking in the non-dissipative geodesic case, and in the non-dissipative isotropic case. These results bring out further the relevance of the complexity factor and its related definition of complexity, in the study of self-gravitating systems.","sentences":["The concept of cracking refers to the tendency of a fluid distribution to \"split'', once it abandons the equilibrium.","In this manuscript we develop a general formalism to describe the occurrence of cracking within a dissipative fluid distribution, in comoving coordinates.","The role of dissipative processes in the occurrence of cracking is brought out.","Next, we relate the occurrence of cracking with the concept of complexity for self-gravitating objects defined in [1-3].","More specifically we relate the occurrence of cracking with the condition of the vanishing of the scalar function intended to measure the complexity of the fluid distribution (the complexity factor).","We also relate the occurrence of cracking with the specific mode of leaving the equilibrium.","Thus, we prove that leaving the equilibrium in either, the homologous (H), or the quasi--homologous regime (QH), prevents the occurrence of cracking.","Also, it is shown that imposing the condition of vanishing complexity factor alone, (independently of the mode of leaving the equilibrium) prevents the occurrence of cracking in the non-dissipative geodesic case, and in the non-dissipative isotropic case.","These results bring out further the relevance of the complexity factor and its related definition of complexity, in the study of self-gravitating systems."],"url":"http://arxiv.org/abs/2404.04901v1","category":"gr-qc"}
{"created":"2024-04-07 09:32:14","title":"Tensorized Ant Colony Optimization for GPU Acceleration","abstract":"Ant Colony Optimization (ACO) is renowned for its effectiveness in solving Traveling Salesman Problems, yet it faces computational challenges in CPU-based environments, particularly with large-scale instances. In response, we introduce a Tensorized Ant Colony Optimization (TensorACO) to utilize the advancements of GPU acceleration. As the core, TensorACO fully transforms ant system and ant path into tensor forms, a process we refer to as tensorization. For the tensorization of ant system, we propose a preprocessing method to reduce the computational overhead by calculating the probability transition matrix. In the tensorization of ant path, we propose an index mapping method to accelerate the update of pheromone matrix by replacing the mechanism of sequential path update with parallel matrix operations. Additionally, we introduce an Adaptive Independent Roulette (AdaIR) method to overcome the challenges of parallelizing ACO's selection mechanism on GPUs. Comprehensive experiments demonstrate the superior performance of TensorACO achieving up to 1921$\\times$ speedup over standard ACO. Moreover, the AdaIR method further improves TensorACO's convergence speed by 80% and solution quality by 2%. Source codes are available at https://github.com/EMI-Group/tensoraco.","sentences":["Ant Colony Optimization (ACO) is renowned for its effectiveness in solving Traveling Salesman Problems, yet it faces computational challenges in CPU-based environments, particularly with large-scale instances.","In response, we introduce a Tensorized Ant Colony Optimization (TensorACO) to utilize the advancements of GPU acceleration.","As the core, TensorACO fully transforms ant system and ant path into tensor forms, a process we refer to as tensorization.","For the tensorization of ant system, we propose a preprocessing method to reduce the computational overhead by calculating the probability transition matrix.","In the tensorization of ant path, we propose an index mapping method to accelerate the update of pheromone matrix by replacing the mechanism of sequential path update with parallel matrix operations.","Additionally, we introduce an Adaptive Independent Roulette (AdaIR) method to overcome the challenges of parallelizing ACO's selection mechanism on GPUs.","Comprehensive experiments demonstrate the superior performance of TensorACO achieving up to 1921$\\times$ speedup over standard ACO.","Moreover, the AdaIR method further improves TensorACO's convergence speed by 80% and solution quality by 2%.","Source codes are available at https://github.com/EMI-Group/tensoraco."],"url":"http://arxiv.org/abs/2404.04895v1","category":"cs.NE"}
{"created":"2024-04-07 09:05:09","title":"TimeGPT in Load Forecasting: A Large Time Series Model Perspective","abstract":"Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce. Inspired by the outstanding performance of large language models (LLMs) in computer vision and natural language processing, this paper aims to discuss the potential of large time series models in load forecasting with scarce historical data. Specifically, the large time series model is constructed as a time series generative pre-trained transformer (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.). Then, the scarce historical load data is used to fine-tune the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting. Simulation results show that TimeGPT outperforms the benchmarks (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times. However, it cannot be guaranteed that TimeGPT is always superior to benchmarks for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data. In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset.","sentences":["Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce.","Inspired by the outstanding performance of large language models (LLMs) in computer vision and natural language processing, this paper aims to discuss the potential of large time series models in load forecasting with scarce historical data.","Specifically, the large time series model is constructed as a time series generative pre-trained transformer (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.).","Then, the scarce historical load data is used to fine-tune the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting.","Simulation results show that TimeGPT outperforms the benchmarks (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times.","However, it cannot be guaranteed that TimeGPT is always superior to benchmarks for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data.","In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset."],"url":"http://arxiv.org/abs/2404.04885v1","category":"cs.LG"}
{"created":"2024-04-07 09:01:50","title":"Mixture of Low-rank Experts for Transferable AI-Generated Image Detection","abstract":"Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information. This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources. Existing methods struggle to generalize across unseen generative models when provided with limited sample sources. Inspired by the zero-shot transferability of pre-trained vision-language models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains. This paper presents a novel parameter-efficient fine-tuning approach, mixture of low-rank experts, to fully exploit CLIP-ViT's potential while preserving knowledge and expanding capacity for transferable detection. We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure. Extensive experiments on public benchmarks show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations. Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.Acc across unseen diffusion and autoregressive models. This even outperforms the baseline with just 0.28% of the training data. Our code and pre-trained models will be available at https://github.com/zhliuworks/CLIPMoLE.","sentences":["Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information.","This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources.","Existing methods struggle to generalize across unseen generative models when provided with limited sample sources.","Inspired by the zero-shot transferability of pre-trained vision-language models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains.","This paper presents a novel parameter-efficient fine-tuning approach, mixture of low-rank experts, to fully exploit CLIP-ViT's potential while preserving knowledge and expanding capacity for transferable detection.","We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure.","Extensive experiments on public benchmarks show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations.","Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.","Acc across unseen diffusion and autoregressive models.","This even outperforms the baseline with just 0.28% of the training data.","Our code and pre-trained models will be available at https://github.com/zhliuworks/CLIPMoLE."],"url":"http://arxiv.org/abs/2404.04883v1","category":"cs.CV"}
{"created":"2024-04-09 17:49:58","title":"Lattice determination of the Batalin-Vilkovisky function and the strong running interaction","abstract":"The Batalin-Vilkovisky function is a central component in the modern formulation of the background field method and the physical applications derived from it. In the present work we report on novel lattice results for this particular quantity, obtained by capitalizing on its equality with the Kugo-Ojima function in the Landau gauge. The results of the lattice simulation are in very good agreement with the predictions derived from a continuum analysis based on the corresponding Schwinger-Dyson equations. In addition, we show that an important relation connecting this function with the ghost propagator is fulfilled rather accurately. With the aid of these results, we carry out the first completely lattice-based determination of the process-independent strong running interaction, employed in a variety of phenomenological studies.","sentences":["The Batalin-Vilkovisky function is a central component in the modern formulation of the background field method and the physical applications derived from it.","In the present work we report on novel lattice results for this particular quantity, obtained by capitalizing on its equality with the Kugo-Ojima function in the Landau gauge.","The results of the lattice simulation are in very good agreement with the predictions derived from a continuum analysis based on the corresponding Schwinger-Dyson equations.","In addition, we show that an important relation connecting this function with the ghost propagator is fulfilled rather accurately.","With the aid of these results, we carry out the first completely lattice-based determination of the process-independent strong running interaction, employed in a variety of phenomenological studies."],"url":"http://arxiv.org/abs/2404.06496v1","category":"hep-lat"}
{"created":"2024-04-09 17:44:01","title":"Convergence analysis of novel discontinuous Galerkin methods for a convection dominated problem","abstract":"In this paper, we propose and analyze a numerically stable and convergent scheme for a convection-diffusion-reaction equation in the convection-dominated regime. Discontinuous Galerkin (DG) methods are considered since standard finite element methods for the convection-dominated equation cause spurious oscillations. We choose to follow a novel DG finite element differential calculus framework introduced in Feng et al. (2016) and approximate the infinite-dimensional operators in the equation with the finite-dimensional DG differential operators. Specifically, we construct the numerical method by using the dual-wind discontinuous Galerkin (DWDG) formulation for the diffusive term and the average discrete gradient operator for the convective term along with standard DG stabilization. We prove that the method converges optimally in the convection-dominated regime. Numerical results are provided to support the theoretical findings.","sentences":["In this paper, we propose and analyze a numerically stable and convergent scheme for a convection-diffusion-reaction equation in the convection-dominated regime.","Discontinuous Galerkin (DG) methods are considered since standard finite element methods for the convection-dominated equation cause spurious oscillations.","We choose to follow a novel DG finite element differential calculus framework introduced in Feng et al. (2016) and approximate the infinite-dimensional operators in the equation with the finite-dimensional DG differential operators.","Specifically, we construct the numerical method by using the dual-wind discontinuous Galerkin (DWDG) formulation for the diffusive term and the average discrete gradient operator for the convective term along with standard DG stabilization.","We prove that the method converges optimally in the convection-dominated regime.","Numerical results are provided to support the theoretical findings."],"url":"http://arxiv.org/abs/2404.06490v1","category":"math.NA"}
{"created":"2024-04-09 17:40:29","title":"Uncovering Tidal Treasures: Automated Classification of Faint Tidal Features in DECaLS Data","abstract":"Tidal features are a key observable prediction of the hierarchical model of galaxy formation and contain a wealth of information about the properties and history of a galaxy. Modern wide-field surveys such as LSST and Euclid will revolutionise the study of tidal features. However, the volume of data will far surpass the capacity to inspect each galaxy to identify the feature visually, thereby motivating an urgent need to develop automated detection methods. This paper presents a visual classification of $\\sim$2,000 galaxies from the DECaLS survey into different tidal feature categories: arms, streams, shells, and diffuse. Using these labels, we trained a Convolutional Neural Network (CNN) to reproduce the assigned visual classifications. Overall our network performed well and retrieved a median $81.1^{+5.8}_{-6.5}$, $65.7^{+5.0}_{-8.4}$, $91.3^{+6.0}_{-5.9}$, and $82.3^{+1.4}_{-7.9}$ per cent of the actual instances of arm, stream, shell, and diffuse features respectively for just 20 per cent contamination. We verified that the network was classifying the images correctly by using a Gradient-weighted Class Activation Mapping analysis to highlight important regions on the images for a given classification. This is the first demonstration of using CNNs to classify tidal features into sub-categories, and it will pave the way for the identification of different categories of tidal features in the vast samples of galaxies that forthcoming wide-field surveys will deliver.","sentences":["Tidal features are a key observable prediction of the hierarchical model of galaxy formation and contain a wealth of information about the properties and history of a galaxy.","Modern wide-field surveys such as LSST and Euclid will revolutionise the study of tidal features.","However, the volume of data will far surpass the capacity to inspect each galaxy to identify the feature visually, thereby motivating an urgent need to develop automated detection methods.","This paper presents a visual classification of $\\sim$2,000 galaxies from the DECaLS survey into different tidal feature categories: arms, streams, shells, and diffuse.","Using these labels, we trained a Convolutional Neural Network (CNN) to reproduce the assigned visual classifications.","Overall our network performed well and retrieved a median $81.1^{+5.8}_{-6.5}$, $65.7^{+5.0}_{-8.4}$, $91.3^{+6.0}_{-5.9}$, and $82.3^{+1.4}_{-7.9}$ per cent of the actual instances of arm, stream, shell, and diffuse features respectively for just 20 per cent contamination.","We verified that the network was classifying the images correctly by using a Gradient-weighted Class Activation Mapping analysis to highlight important regions on the images for a given classification.","This is the first demonstration of using CNNs to classify tidal features into sub-categories, and it will pave the way for the identification of different categories of tidal features in the vast samples of galaxies that forthcoming wide-field surveys will deliver."],"url":"http://arxiv.org/abs/2404.06487v1","category":"astro-ph.GA"}
{"created":"2024-04-09 17:01:10","title":"Periodic solutions to integro-differential equations: variational formulation, symmetry, and regularity","abstract":"We consider nonconstant periodic constrained minimizers of semilinear elliptic equations for integro-differential operators in $\\mathbb{R}$. We prove that, after an appropriate translation, each of them is necessarily an even function which is decreasing in half its period. In particular, it has only two critical points in half its period, the absolute maximum and minimum. If these statements hold for all nonconstant periodic solutions, and not only for constrained minimizers, remains as an open problem.   Our results apply to operators with kernels in two different classes: kernels $K$ which are convex and kernels for which $K(\\tau^{1/2})$ is a completely monotonic function of $\\tau$. This last new class arose in our previous work on nonlocal Delaunay surfaces in $\\mathbb{R}^n$. Due to their symmetry of revolution, it gave rise to a 1d problem involving an operator with a nonconvex kernel. Our proofs are based on a not so well-known Riesz rearrangement inequality on the circle $\\mathbb{S}^1$ established in 1976.   We also put in evidence a new regularity fact which is a truly nonlocal-semilinear effect and also occurs in the nonperiodic setting. Namely, for nonlinearities in $C^\\beta$ and when $2s+\\beta <1$ ($2s$ being the order of the operator), the solution is not always $C^{2s+\\beta-\\epsilon}$ for all $\\epsilon>0$.","sentences":["We consider nonconstant periodic constrained minimizers of semilinear elliptic equations for integro-differential operators in $\\mathbb{R}$. We prove that, after an appropriate translation, each of them is necessarily an even function which is decreasing in half its period.","In particular, it has only two critical points in half its period, the absolute maximum and minimum.","If these statements hold for all nonconstant periodic solutions, and not only for constrained minimizers, remains as an open problem.   ","Our results apply to operators with kernels in two different classes: kernels $K$ which are convex and kernels for which $K(\\tau^{1/2})$ is a completely monotonic function of $\\tau$. This last new class arose in our previous work on nonlocal Delaunay surfaces in $\\mathbb{R}^n$. Due to their symmetry of revolution, it gave rise to a 1d problem involving an operator with a nonconvex kernel.","Our proofs are based on a not so well-known Riesz rearrangement inequality on the circle $\\mathbb{S}^1$ established in 1976.   ","We also put in evidence a new regularity fact which is a truly nonlocal-semilinear effect and also occurs in the nonperiodic setting.","Namely, for nonlinearities in $C^\\beta$ and when $2s+\\beta <1$ ($2s$ being the order of the operator), the solution is not always $C^{2s+\\beta-\\epsilon}$ for all $\\epsilon>0$."],"url":"http://arxiv.org/abs/2404.06462v1","category":"math.AP"}
{"created":"2024-04-09 16:51:08","title":"Deep-Learning Database of Density Functional Theory Hamiltonians for Twisted Materials","abstract":"Moir\\'e-twisted materials have garnered significant research interest due to their distinctive properties and intriguing physics. However, conducting first-principles studies on such materials faces challenges, notably the formidable computational cost associated with simulating ultra-large twisted structures. This obstacle impedes the construction of a twisted materials database crucial for datadriven materials discovery. Here, by using high-throughput calculations and state-of-the-art neural network methods, we construct a Deep-learning Database of density functional theory (DFT) Hamiltonians for Twisted materials named DDHT. The DDHT database comprises trained neural-network models of over a hundred homo-bilayer and hetero-bilayer moir\\'e-twisted materials. These models enable accurate prediction of the DFT Hamiltonian for these materials across arbitrary twist angles, with an averaged mean absolute error of approximately 1.0 meV or lower. The database facilitates the exploration of flat bands and correlated materials platforms within ultra-large twisted structures.","sentences":["Moir\\'e-twisted materials have garnered significant research interest due to their distinctive properties and intriguing physics.","However, conducting first-principles studies on such materials faces challenges, notably the formidable computational cost associated with simulating ultra-large twisted structures.","This obstacle impedes the construction of a twisted materials database crucial for datadriven materials discovery.","Here, by using high-throughput calculations and state-of-the-art neural network methods, we construct a Deep-learning Database of density functional theory (DFT) Hamiltonians for Twisted materials named DDHT.","The DDHT database comprises trained neural-network models of over a hundred homo-bilayer and hetero-bilayer moir\\'e-twisted materials.","These models enable accurate prediction of the DFT Hamiltonian for these materials across arbitrary twist angles, with an averaged mean absolute error of approximately 1.0 meV or lower.","The database facilitates the exploration of flat bands and correlated materials platforms within ultra-large twisted structures."],"url":"http://arxiv.org/abs/2404.06449v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 16:28:54","title":"Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks","abstract":"With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.","sentences":["With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation.","In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning.","For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires.","Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models.","Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance.","Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered."],"url":"http://arxiv.org/abs/2404.06437v1","category":"cs.CV"}
{"created":"2024-04-09 16:25:07","title":"Quantum Graph Optimization Algorithm","abstract":"Quadratic unconstrained binary optimization (QUBO) tasks are very important in chemistry, finance, job scheduling, and so on, which can be represented using graph structures, with the variables as nodes and the interaction between them as edges. Variational quantum algorithms, especially the Quantum Approximate Optimization Algorithm (QAOA) and its variants, present a promising way, potentially exceeding the capabilities of classical algorithms, for addressing QUBO tasks. However, the possibility of using message-passing machines, inspired by classical graph neural networks, to enhance the power and performance of these quantum algorithms for QUBO tasks was not investigated. This study introduces a novel variational quantum graph optimization algorithm that integrates the message-passing mechanism, which demonstrates significant improvements in performance for solving QUBO problems in terms of resource efficiency and solution precision, compared to QAOA, its variants, and other quantum graph neural networks. Furthermore, in terms of scalability on QUBO tasks, our algorithm shows superior performance compared to QAOA, presenting a substantial advancement in the field of quantum approximate optimization.","sentences":["Quadratic unconstrained binary optimization (QUBO) tasks are very important in chemistry, finance, job scheduling, and so on, which can be represented using graph structures, with the variables as nodes and the interaction between them as edges.","Variational quantum algorithms, especially the Quantum Approximate Optimization Algorithm (QAOA) and its variants, present a promising way, potentially exceeding the capabilities of classical algorithms, for addressing QUBO tasks.","However, the possibility of using message-passing machines, inspired by classical graph neural networks, to enhance the power and performance of these quantum algorithms for QUBO tasks was not investigated.","This study introduces a novel variational quantum graph optimization algorithm that integrates the message-passing mechanism, which demonstrates significant improvements in performance for solving QUBO problems in terms of resource efficiency and solution precision, compared to QAOA, its variants, and other quantum graph neural networks.","Furthermore, in terms of scalability on QUBO tasks, our algorithm shows superior performance compared to QAOA, presenting a substantial advancement in the field of quantum approximate optimization."],"url":"http://arxiv.org/abs/2404.06434v1","category":"quant-ph"}
{"created":"2024-04-09 16:24:56","title":"Alfven Wave Mode Conversion in Neutron Star Magnetospheres: A Semi-analytic Approach","abstract":"We write down the force-free electrodynamics (FFE) equations in dipole coordinates, and solve for normal modes corresponding to Alfv\\'enic perturbations in the magnetosphere of a neutron star. We show that a single Alfv\\'en wave propagating on dipole field lines spontaneously sources a fast magnetosonic (fms) wave at the next order in the perturbation expansion, without needing 3-wave interaction. The frequency of the sourced fms wave is twice the original Alfv\\'en wave frequency, and the wave propagates spherically outwards. The properties of the outgoing fms wave can be computed exactly using the usual devices of classical electrodynamics. We extend the calculation to the closed zone of a rotating neutron star magnetosphere, and show that the Alfv\\'en wave also sources a spherical fms wave but at the same frequency as the primary Alfv\\'en wave.","sentences":["We write down the force-free electrodynamics (FFE) equations in dipole coordinates, and solve for normal modes corresponding to Alfv\\'enic perturbations in the magnetosphere of a neutron star.","We show that a single Alfv\\'en wave propagating on dipole field lines spontaneously sources a fast magnetosonic (fms) wave at the next order in the perturbation expansion, without needing 3-wave interaction.","The frequency of the sourced fms wave is twice the original Alfv\\'en wave frequency, and the wave propagates spherically outwards.","The properties of the outgoing fms wave can be computed exactly using the usual devices of classical electrodynamics.","We extend the calculation to the closed zone of a rotating neutron star magnetosphere, and show that the Alfv\\'en wave also sources a spherical fms wave but at the same frequency as the primary Alfv\\'en wave."],"url":"http://arxiv.org/abs/2404.06431v1","category":"astro-ph.HE"}
{"created":"2024-04-09 16:19:23","title":"Maximality and Cauchy developments of Lorentzian length spaces","abstract":"This article suggests the definition of 'Lorentzian space' weakening the notion of Lorentzian length space just as much that it allows for a functor from the category of causally continuous Lorentzian manifolds to the corresponding category of Lorentzian spaces, and considers three problems in the context of maximal Cauchy developments of Lorentzian length spaces (LLSs): The first is to define pointed Gromov-Hausdorff metrics for spatially and temporally noncompact LLSs, the second to present an explicit non-spacetime example of a maximal vacuum Cauchy development in the LLS category, the third to define canonical representatives for developments. A certain regularity property for geodesics plays a key role in each of the problems.","sentences":["This article suggests the definition of 'Lorentzian space' weakening the notion of Lorentzian length space just as much that it allows for a functor from the category of causally continuous Lorentzian manifolds to the corresponding category of Lorentzian spaces, and considers three problems in the context of maximal Cauchy developments of Lorentzian length spaces (LLSs): The first is to define pointed Gromov-Hausdorff metrics for spatially and temporally noncompact LLSs, the second to present an explicit non-spacetime example of a maximal vacuum Cauchy development in the LLS category, the third to define canonical representatives for developments.","A certain regularity property for geodesics plays a key role in each of the problems."],"url":"http://arxiv.org/abs/2404.06428v1","category":"math.DG"}
{"created":"2024-04-09 16:06:34","title":"Existence and uniqueness theorems for one class of Hammerstein-type nonlinear integral equations","abstract":"The class of nonlinear integral equations on the positive half-line with a monotone operator of Hammerstein type is studied. With various partial representations of the corresponding kernel and nonlinearity, this class of equations has applications in the dynamic theory of $p$-adic strings, in the kinetic theory of gases, in the theory of radiation transfer and in the mathematical theory of the geographical spread of epidemic diseases. A constructive theorem for the existence of a nontrivial bounded solution is proved. The asymptotic behavior of the constructed solution at infinity is studied. We also prove a theorem for the uniqueness of a solution in the class of nonnegative nontrivial and bounded functions. At the end of the work, specific particular examples of the kernel and nonlinearity of this class of equations are given, which are of independent interest.","sentences":["The class of nonlinear integral equations on the positive half-line with a monotone operator of Hammerstein type is studied.","With various partial representations of the corresponding kernel and nonlinearity, this class of equations has applications in the dynamic theory of $p$-adic strings, in the kinetic theory of gases, in the theory of radiation transfer and in the mathematical theory of the geographical spread of epidemic diseases.","A constructive theorem for the existence of a nontrivial bounded solution is proved.","The asymptotic behavior of the constructed solution at infinity is studied.","We also prove a theorem for the uniqueness of a solution in the class of nonnegative nontrivial and bounded functions.","At the end of the work, specific particular examples of the kernel and nonlinearity of this class of equations are given, which are of independent interest."],"url":"http://arxiv.org/abs/2404.06416v1","category":"math.AP"}
{"created":"2024-04-09 15:54:03","title":"Emergent Dynamics in Neural Cellular Automata","abstract":"Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA). Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures. However, the conditions required for an NCA to display dynamic patterns remain unexplored. Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models. Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames. Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output. We thus propose a design principle for creating dynamic NCA.","sentences":["Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA).","Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures.","However, the conditions required for an NCA to display dynamic patterns remain unexplored.","Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models.","Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames.","Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output.","We thus propose a design principle for creating dynamic NCA."],"url":"http://arxiv.org/abs/2404.06406v1","category":"cs.CV"}
{"created":"2024-04-09 15:23:35","title":"Causal third-order viscous hydrodynamics within relaxation-time approximation","abstract":"In the present work, we derive a linearly stable and causal theory of relativistic third-order viscous hydrodynamics from the Boltzmann equation with relaxation-time approximation. We employ viscous correction to the distribution function obtained using a Chapman-Enskog like iterative solution of the Boltzmann equation. Our derivation highlights the necessity of incorporating a new dynamical degree of freedom, specifically an irreducible tensors of rank three, within this framework. This differs from the recent formulation of causal third-order theory from the method of moments which requires two dynamical degrees of freedom: an irreducible third-rank and a fourth-rank tensor. We verify the linear stability and causality of the proposed formulation by examining perturbations around a global equilibrium state.","sentences":["In the present work, we derive a linearly stable and causal theory of relativistic third-order viscous hydrodynamics from the Boltzmann equation with relaxation-time approximation.","We employ viscous correction to the distribution function obtained using a Chapman-Enskog like iterative solution of the Boltzmann equation.","Our derivation highlights the necessity of incorporating a new dynamical degree of freedom, specifically an irreducible tensors of rank three, within this framework.","This differs from the recent formulation of causal third-order theory from the method of moments which requires two dynamical degrees of freedom: an irreducible third-rank and a fourth-rank tensor.","We verify the linear stability and causality of the proposed formulation by examining perturbations around a global equilibrium state."],"url":"http://arxiv.org/abs/2404.06381v1","category":"hep-ph"}
{"created":"2024-04-09 15:11:40","title":"ABP estimate and comparison principle for cone degenerate quasilinear elliptic equations","abstract":"In this paper, we study the cone degenerate quasilinear elliptic equations. We provide the existence of the viscosity solutions by proving Alexandrov-Bakelman-Pucci and H\\\"older estimates. Further more, we give the comparison principle by an equivalent transformation.","sentences":["In this paper, we study the cone degenerate quasilinear elliptic equations.","We provide the existence of the viscosity solutions by proving Alexandrov-Bakelman-Pucci and H\\\"older estimates.","Further more, we give the comparison principle by an equivalent transformation."],"url":"http://arxiv.org/abs/2404.06372v1","category":"math.AP"}
{"created":"2024-04-09 15:03:10","title":"Did Louis de Broglie miss the discovery of the Schr\u00f6dinger equation?","abstract":"In this note, we discuss a historical point regarding Schr\\\"odinger's discovery of the famous quantum wave equation in 1926 following de Broglie's fundamental works published in 1923-1925 regarding the introduction of matter waves. Drawing on the work of historians and personal analysis, we show that de Broglie was very close to the discovery of the Schr\\\"odinger equation (at least for the stationary one-electron problem).","sentences":["In this note, we discuss a historical point regarding Schr\\\"odinger's discovery of the famous quantum wave equation in 1926 following de Broglie's fundamental works published in 1923-1925 regarding the introduction of matter waves.","Drawing on the work of historians and personal analysis, we show that de Broglie was very close to the discovery of the Schr\\\"odinger equation (at least for the stationary one-electron problem)."],"url":"http://arxiv.org/abs/2404.06366v1","category":"physics.hist-ph"}
{"created":"2024-04-09 14:18:05","title":"On the 576-fold periodicity of the spectrum SQFT: The proof of the lower bound via the Anderson duality pairing","abstract":"We are aimed at giving a differential geometric, and accordingly physical, explanation of the 576-periodicity of TMF. In this paper, we settle the problem of giving the lower bound 576. We formulate the problem as follows: we assume a spectrum $\\mathrm{SQFT}$ with some conditions, suggest from physical considerations about the classifying spectrum for two-dimensional $\\mathcal{N}=(0,1)$-supersymmetric quantum field theories, and show that the periodicity of $\\mathrm{SQFT}$ is no less than 576. The main tool for the proof is the analogue of the Anderson duality pairing introduced by the second-named author and Tachikawa. We do not rely on the Segal-Stolz-Teichner conjecture, so in particular we do not use any comparison map with TMF.","sentences":["We are aimed at giving a differential geometric, and accordingly physical, explanation of the 576-periodicity of TMF.","In this paper, we settle the problem of giving the lower bound 576.","We formulate the problem as follows: we assume a spectrum $\\mathrm{SQFT}$ with some conditions, suggest from physical considerations about the classifying spectrum for two-dimensional $\\mathcal{N}=(0,1)$-supersymmetric quantum field theories, and show that the periodicity of $\\mathrm{SQFT}$ is no less than 576.","The main tool for the proof is the analogue of the Anderson duality pairing introduced by the second-named author and Tachikawa.","We do not rely on the Segal-Stolz-Teichner conjecture, so in particular we do not use any comparison map with TMF."],"url":"http://arxiv.org/abs/2404.06333v1","category":"math.AT"}
{"created":"2024-04-09 13:56:20","title":"Variational Optimization for Constructing Inverse Potentials of Proton-Proton Scattering: A Phase Function Method Study","abstract":"Background: The phase-shift analysis for proton-proton scattering has been studied by various research groups using the realistic potentials to be comprised of various internal interactions based on an exchange of pions and mesons, involving a large number of parameters. Purpose: The goal of the research is to construct inverse potentials for various l-channels of proton-proton (pp) elastic scattering using the 3-parameter Morse function in combination with atomic Hulthen by utilizing the phase function method and variational optimization technique. Methodology: The implementation of variational optimization begins with randomly assigning initial values to the Morse model parameters. Utilizing the Morse + Hulthen potential as input, the phase equations for various l-channels are numerically solved using the RK-5 method for obtaining the simulated Scattering Phase Shift (SPS). Mean Squared error between simulated and expected SPS has been chosen as the cost function. Variational optimization proceeds iteratively by adjusting potential parameters and re-evaluating the cost function until convergence is achieved. Results: All the obtained scattering phase shifts for various l-channels have been found to converge to a mean squared error <= 0.3. The computed cross-sections matched the experimental ones to less than 1% for energies up to 25 MeV. The scattering parameters are also found to closely match the experimental data. Conclusion: The inverse potentials constructed for various l-channels using Morse + atomic Hulthen are on par with the currently available high-precision realistic potentials.","sentences":["Background: The phase-shift analysis for proton-proton scattering has been studied by various research groups using the realistic potentials to be comprised of various internal interactions based on an exchange of pions and mesons, involving a large number of parameters.","Purpose: The goal of the research is to construct inverse potentials for various l-channels of proton-proton (pp) elastic scattering using the 3-parameter Morse function in combination with atomic Hulthen by utilizing the phase function method and variational optimization technique.","Methodology:","The implementation of variational optimization begins with randomly assigning initial values to the Morse model parameters.","Utilizing the Morse + Hulthen potential as input, the phase equations for various l-channels are numerically solved using the RK-5 method for obtaining the simulated Scattering Phase Shift (SPS).","Mean Squared error between simulated and expected SPS has been chosen as the cost function.","Variational optimization proceeds iteratively by adjusting potential parameters and re-evaluating the cost function until convergence is achieved.","Results: All the obtained scattering phase shifts for various l-channels have been found to converge to a mean squared error <= 0.3.","The computed cross-sections matched the experimental ones to less than 1% for energies up to 25 MeV.","The scattering parameters are also found to closely match the experimental data.","Conclusion: The inverse potentials constructed for various l-channels using Morse + atomic Hulthen are on par with the currently available high-precision realistic potentials."],"url":"http://arxiv.org/abs/2404.06318v1","category":"nucl-th"}
{"created":"2024-04-09 13:42:42","title":"Dynamical dark energy in light of cosmic distance measurements II: a study using current observations","abstract":"We extract key information of dark energy from current observations of BAO, OHD and $H_0$, and find hints of dynamical behaviour of dark energy. In particular, a dynamical dark energy model whose equation of state crosses $-1$ is favoured by observations. We also find that the Universe has started accelerating at a lower redshift than expected.","sentences":["We extract key information of dark energy from current observations of BAO, OHD and $H_0$, and find hints of dynamical behaviour of dark energy.","In particular, a dynamical dark energy model whose equation of state crosses $-1$ is favoured by observations.","We also find that the Universe has started accelerating at a lower redshift than expected."],"url":"http://arxiv.org/abs/2404.06310v1","category":"astro-ph.CO"}
{"created":"2024-04-09 13:34:15","title":"Dynamical dark energy in light of cosmic distance measurements I: a demonstration using simulated datasets","abstract":"We develop methods to extract key dark energy information from cosmic distance measurements including the BAO scales and supernovae luminosity distances. Demonstrated using simulated datasets of the complete DESI, LSST and Roman surveys designed for BAO and SNe distance measurements, we show that using our method, the dynamical behaviour of the energy, pressure, equation of state (with its time derivative) of dark energy and the cosmic deceleration function can all be accurately recovered from high-quality data, which allows for robust diagnostic tests for dark energy models.","sentences":["We develop methods to extract key dark energy information from cosmic distance measurements including the BAO scales and supernovae luminosity distances.","Demonstrated using simulated datasets of the complete DESI, LSST and Roman surveys designed for BAO and SNe distance measurements, we show that using our method, the dynamical behaviour of the energy, pressure, equation of state (with its time derivative) of dark energy and the cosmic deceleration function can all be accurately recovered from high-quality data, which allows for robust diagnostic tests for dark energy models."],"url":"http://arxiv.org/abs/2404.06303v1","category":"astro-ph.CO"}
{"created":"2024-04-09 09:21:56","title":"(Not) Understanding Latin Poetic Style with Deep Learning","abstract":"This article summarizes some mostly unsuccessful attempts to understand authorial style by examining the attention of various neural networks (LSTMs and CNNs) trained on a corpus of classical Latin verse that has been encoded to include sonic and metrical features. Carefully configured neural networks are shown to be extremely strong authorship classifiers, so it is hoped that they might therefore teach `traditional' readers something about how the authors differ in style. Sadly their reasoning is, so far, inscrutable. While the overall goal has not yet been reached, this work reports some useful findings in terms of effective ways to encode and embed verse, the relative strengths and weaknesses of the neural network families, and useful (and not so useful) techniques for designing and inspecting NN models in this domain. This article suggests that, for poetry, CNNs are better choices than LSTMs -- they train more quickly, have equivalent accuracy, and (potentially) offer better interpretability. Based on a great deal of experimentation, it also suggests that simple, trainable embeddings are more effective than domain-specific schemes, and stresses the importance of techniques to reduce overfitting, like dropout and batch normalization.","sentences":["This article summarizes some mostly unsuccessful attempts to understand authorial style by examining the attention of various neural networks (LSTMs and CNNs) trained on a corpus of classical Latin verse that has been encoded to include sonic and metrical features.","Carefully configured neural networks are shown to be extremely strong authorship classifiers, so it is hoped that they might therefore teach `traditional' readers something about how the authors differ in style.","Sadly their reasoning is, so far, inscrutable.","While the overall goal has not yet been reached, this work reports some useful findings in terms of effective ways to encode and embed verse, the relative strengths and weaknesses of the neural network families, and useful (and not so useful) techniques for designing and inspecting NN models in this domain.","This article suggests that, for poetry, CNNs are better choices than LSTMs -- they train more quickly, have equivalent accuracy, and (potentially) offer better interpretability.","Based on a great deal of experimentation, it also suggests that simple, trainable embeddings are more effective than domain-specific schemes, and stresses the importance of techniques to reduce overfitting, like dropout and batch normalization."],"url":"http://arxiv.org/abs/2404.06150v1","category":"cs.CL"}
{"created":"2024-04-09 09:15:39","title":"Probing the Berezinskii-Kosterlitz-Thouless vortex unbinding transition in two-dimensional superconductors using local noise magnetometry","abstract":"The melting of quasi-long-range superconductivity in two spatial dimensions occurs through the proliferation and unbinding of vortex-antivortex pairs -- a phenomenon known as the Berezinskii-Kosterlitz-Thouless (BKT) transition. Although signatures of this transition have been observed in bulk measurements, these experiments are often complicated, ambiguous, and unable to resolve the rich physics of the vortex unbinding transition. Here we show that local noise magnetometry is a sensitive, noninvasive probe that can provide direct information about the scale-dependent vortex dynamics. In particular, by resolving the distance and temperature dependence of the magnetic noise, it may be possible to experimentally study the renormalization group flow equations of the vortex gas and track the onset of vortex unbinding in situ. Specifically, we predict i) a nonmonotonic dependence of the noise on temperature and ii) the local noise is almost independent of the sample-probe distance at the BKT transition. We also show that noise magnetometry can distinguish Gaussian superconducting order-parameter fluctuations from topological vortex fluctuations and can detect the emergence of unbound vortices. The weak distance dependence at the BKT transition can also be used to distinguish it from quasiparticle background noise. Our predictions may be within experimental reach for a number of unconventional superconductors.","sentences":["The melting of quasi-long-range superconductivity in two spatial dimensions occurs through the proliferation and unbinding of vortex-antivortex pairs -- a phenomenon known as the Berezinskii-Kosterlitz-Thouless (BKT) transition.","Although signatures of this transition have been observed in bulk measurements, these experiments are often complicated, ambiguous, and unable to resolve the rich physics of the vortex unbinding transition.","Here we show that local noise magnetometry is a sensitive, noninvasive probe that can provide direct information about the scale-dependent vortex dynamics.","In particular, by resolving the distance and temperature dependence of the magnetic noise, it may be possible to experimentally study the renormalization group flow equations of the vortex gas and track the onset of vortex unbinding in situ.","Specifically, we predict i) a nonmonotonic dependence of the noise on temperature and ii) the local noise is almost independent of the sample-probe distance at the BKT transition.","We also show that noise magnetometry can distinguish Gaussian superconducting order-parameter fluctuations from topological vortex fluctuations and can detect the emergence of unbound vortices.","The weak distance dependence at the BKT transition can also be used to distinguish it from quasiparticle background noise.","Our predictions may be within experimental reach for a number of unconventional superconductors."],"url":"http://arxiv.org/abs/2404.06147v1","category":"cond-mat.supr-con"}
{"created":"2024-04-09 08:46:38","title":"In-vivo imaging of the human thalamus: a comprehensive evaluation of structural magnetic resonance imaging approaches for thalamic nuclei differentiation at 7T","abstract":"The thalamus is a subcortical structure of central importance to brain function, which is organized in smaller nuclei with specialized roles. Despite significant functional and clinical relevance, locating and distinguishing the different thalamic nuclei in vivo, non-invasively, has proved challenging with conventional imaging techniques, such as T$_{1}$ and T$_{2}$-weighted magnetic resonance imaging (MRI). This key limitation has prompted extensive research efforts, and several new candidate MRI sequences for thalamic imaging have been proposed, especially at 7T. However, studies to date have mainly been centered on individual techniques, and often focused on subsets of specific nuclei. It is now critical to evaluate which options are best for which nuclei, and which are globally the most informative. This work addresses these questions through a comprehensive evaluation of thalamic structural imaging techniques in humans at 7T, including several variants of T$_{1}$, T$_{2}$, T$_{2}$* and magnetic susceptibility-based contrasts. All images were obtained from the same participants, to allow direct comparisons without anatomical variability confounds. The different contrasts were qualitatively and quantitatively analyzed with dedicated approaches, referenced to well-established thalamic atlases. Overall, the analyses showed that quantitative susceptibility mapping (QSM) and T$_{1}$-weighted MP2RAGE tuned to maximize gray-to-white matter contrast are currently the most valuable options. The two contrasts display unique, complementary features and, together, enable the distinction of the majority of known nuclei. Likewise, their combined information could provide a powerful input for automatic segmentation approaches. To our knowledge, this study represents the most comprehensive assessment of structural MRI contrasts for thalamic imaging to date.","sentences":["The thalamus is a subcortical structure of central importance to brain function, which is organized in smaller nuclei with specialized roles.","Despite significant functional and clinical relevance, locating and distinguishing the different thalamic nuclei in vivo, non-invasively, has proved challenging with conventional imaging techniques, such as T$_{1}$ and T$_{2}$-weighted magnetic resonance imaging (MRI).","This key limitation has prompted extensive research efforts, and several new candidate MRI sequences for thalamic imaging have been proposed, especially at 7T. However, studies to date have mainly been centered on individual techniques, and often focused on subsets of specific nuclei.","It is now critical to evaluate which options are best for which nuclei, and which are globally the most informative.","This work addresses these questions through a comprehensive evaluation of thalamic structural imaging techniques in humans at 7T, including several variants of T$_{1}$, T$_{2}$, T$_{2}$* and magnetic susceptibility-based contrasts.","All images were obtained from the same participants, to allow direct comparisons without anatomical variability confounds.","The different contrasts were qualitatively and quantitatively analyzed with dedicated approaches, referenced to well-established thalamic atlases.","Overall, the analyses showed that quantitative susceptibility mapping (QSM) and T$_{1}$-weighted MP2RAGE tuned to maximize gray-to-white matter contrast are currently the most valuable options.","The two contrasts display unique, complementary features and, together, enable the distinction of the majority of known nuclei.","Likewise, their combined information could provide a powerful input for automatic segmentation approaches.","To our knowledge, this study represents the most comprehensive assessment of structural MRI contrasts for thalamic imaging to date."],"url":"http://arxiv.org/abs/2404.06122v1","category":"physics.med-ph"}
{"created":"2024-04-09 07:44:16","title":"On the stability of the Abrikosov lattice in the Lowest Landau Level","abstract":"We study the Lowest Landau Level equation set on simply and doubly-periodic domains (in other words, rectangles and strips with appropriate boundary conditions). To begin with, we study well-posedness and establish the existence of stationary solutions. Then we investigate the linear stability of the lattice solution and prove it is stable for the (hexagonal) Abrikosov lattice, but unstable for rectangular lattices.","sentences":["We study the Lowest Landau Level equation set on simply and doubly-periodic domains (in other words, rectangles and strips with appropriate boundary conditions).","To begin with, we study well-posedness and establish the existence of stationary solutions.","Then we investigate the linear stability of the lattice solution and prove it is stable for the (hexagonal) Abrikosov lattice, but unstable for rectangular lattices."],"url":"http://arxiv.org/abs/2404.06085v1","category":"math.AP"}
{"created":"2024-04-09 06:10:15","title":"Object Dynamics Modeling with Hierarchical Point Cloud-based Representations","abstract":"Modeling object dynamics with a neural network is an important problem with numerous applications. Most recent work has been based on graph neural networks. However, physics happens in 3D space, where geometric information potentially plays an important role in modeling physical phenomena. In this work, we propose a novel U-net architecture based on continuous point convolution which naturally embeds information from 3D coordinates and allows for multi-scale feature representations with established downsampling and upsampling procedures. Bottleneck layers in the downsampled point clouds lead to better long-range interaction modeling. Besides, the flexibility of point convolutions allows our approach to generalize to sparsely sampled points from mesh vertices and dynamically generate features on important interaction points on mesh faces. Experimental results demonstrate that our approach significantly improves the state-of-the-art, especially in scenarios that require accurate gravity or collision reasoning.","sentences":["Modeling object dynamics with a neural network is an important problem with numerous applications.","Most recent work has been based on graph neural networks.","However, physics happens in 3D space, where geometric information potentially plays an important role in modeling physical phenomena.","In this work, we propose a novel U-net architecture based on continuous point convolution which naturally embeds information from 3D coordinates and allows for multi-scale feature representations with established downsampling and upsampling procedures.","Bottleneck layers in the downsampled point clouds lead to better long-range interaction modeling.","Besides, the flexibility of point convolutions allows our approach to generalize to sparsely sampled points from mesh vertices and dynamically generate features on important interaction points on mesh faces.","Experimental results demonstrate that our approach significantly improves the state-of-the-art, especially in scenarios that require accurate gravity or collision reasoning."],"url":"http://arxiv.org/abs/2404.06044v1","category":"cs.CV"}
{"created":"2024-04-09 05:34:19","title":"FuSeBMC AI: Acceleration of Hybrid Approach through Machine Learning","abstract":"We present FuSeBMC-AI, a test generation tool grounded in machine learning techniques. FuSeBMC-AI extracts various features from the program and employs support vector machine and neural network models to predict a hybrid approach optimal configuration. FuSeBMC-AI utilizes Bounded Model Checking and Fuzzing as back-end verification engines. FuSeBMC-AI outperforms the default configuration of the underlying verification engine in certain cases while concurrently diminishing resource consumption.","sentences":["We present FuSeBMC-AI, a test generation tool grounded in machine learning techniques.","FuSeBMC-AI extracts various features from the program and employs support vector machine and neural network models to predict a hybrid approach optimal configuration.","FuSeBMC-AI utilizes Bounded Model Checking and Fuzzing as back-end verification engines.","FuSeBMC-AI outperforms the default configuration of the underlying verification engine in certain cases while concurrently diminishing resource consumption."],"url":"http://arxiv.org/abs/2404.06031v1","category":"cs.CR"}
{"created":"2024-04-09 05:33:41","title":"Optimization methods for solving matrix equations","abstract":"In this paper, we focus on using optimization methods to solve matrix equations by transforming the problem of solving the Sylvester matrix equation or continuous algebraic Riccati equation into an optimization problem. Initially, we use a constrained convex optimization method (CCOM) to solve the Sylvester matrix equation with $\\ell_{2,1}$-norm, where we provide a convergence analysis and numerical examples of CCOM; however, the results show that the algorithm is not efficient. To address this issue, we employ classical quasi-Newton methods such as DFP and BFGS algorithms to solve the Sylvester matrix equation and present the convergence and numerical results of the algorithm. Additionally, we compare these algorithms with the CG algorithm and AR algorithm, and our results demonstrate that the presented algorithms are effective. Furthermore, we propose a unified framework of the alternating direction multiplier method (ADMM) for directly solving the continuous algebraic Riccati equation (CARE), and we provide the convergence and numerical results of ADMM. Our experimental results indicate that ADMM is an effective optimization algorithm for solving CARE. Finally, to improve the effectiveness of the optimization method for solving Riccati equation, we propose the Newton-ADMM algorithm framework, where the outer iteration of this method is the classical Newton method, and the inner iteration involves using ADMM to solve Lyapunov matrix equations inexactly. We also provide the convergence and numerical results of this algorithm, which our results demonstrate are more efficient than ADMM for solving CARE.","sentences":["In this paper, we focus on using optimization methods to solve matrix equations by transforming the problem of solving the Sylvester matrix equation or continuous algebraic Riccati equation into an optimization problem.","Initially, we use a constrained convex optimization method (CCOM) to solve the Sylvester matrix equation with $\\ell_{2,1}$-norm, where we provide a convergence analysis and numerical examples of CCOM; however, the results show that the algorithm is not efficient.","To address this issue, we employ classical quasi-Newton methods such as DFP and BFGS algorithms to solve the Sylvester matrix equation and present the convergence and numerical results of the algorithm.","Additionally, we compare these algorithms with the CG algorithm and AR algorithm, and our results demonstrate that the presented algorithms are effective.","Furthermore, we propose a unified framework of the alternating direction multiplier method (ADMM) for directly solving the continuous algebraic Riccati equation (CARE), and we provide the convergence and numerical results of ADMM.","Our experimental results indicate that ADMM is an effective optimization algorithm for solving CARE.","Finally, to improve the effectiveness of the optimization method for solving Riccati equation, we propose the Newton-ADMM algorithm framework, where the outer iteration of this method is the classical Newton method, and the inner iteration involves using ADMM to solve Lyapunov matrix equations inexactly.","We also provide the convergence and numerical results of this algorithm, which our results demonstrate are more efficient than ADMM for solving CARE."],"url":"http://arxiv.org/abs/2404.06030v1","category":"math.NA"}
{"created":"2024-04-09 04:48:24","title":"Passive None-line-of-sight imaging with arbitrary scene condition and detection pattern in small amount of prior data","abstract":"Passive Non-Line-of-Sight (NLOS) imaging requires to reconstruct objects which cannot be seen in line without using external controllable light sources. It can be widely applied in areas like counter-terrorism, urban-Warfare, autonomous-driving and robot-vision. Existing methods for passive NLOS typically required extensive prior information and significant computational resources to establish light transport matrices or train neural networks. These constraints pose significant challenges for transitioning models to different NLOS scenarios. Thus, the pressing issue in passive NLOS imaging currently lies in whether it is possible to estimate the light transport matrices which corresponding to relay surfaces and scenes, as well as the specific distribution of targets, with a small amount of prior knowledge. In this work, we hypothesized a high-dimensional manifold and mathematically proved its existence. Within this high-dimensional manifold, the structural information of obscured targets is minimally disrupted. Therefore, we proposed a universal framework named High-Dimensional Projection Selection (HDPS) which can establish this high-dimensional manifold and output its projection onto corresponding surfaces on low-dimensional. HDPS can be applied to most mature network architectures and estimate the distribution of target and light spot obtained by camera with only minimal prior data. Certainly, with the help of the estimated information, it can establish a high-dimensional manifold consisting of target and input. As demonstrated in experiment, our framework, even when applied to the most basic network structures, can achieve higher accuracy results with significantly smaller amounts of prior data. Thereby, our approach enables passive NLOS scenarios to reconstruct target by limited prior data and computational resources.","sentences":["Passive Non-Line-of-Sight (NLOS) imaging requires to reconstruct objects which cannot be seen in line without using external controllable light sources.","It can be widely applied in areas like counter-terrorism, urban-Warfare, autonomous-driving and robot-vision.","Existing methods for passive NLOS typically required extensive prior information and significant computational resources to establish light transport matrices or train neural networks.","These constraints pose significant challenges for transitioning models to different NLOS scenarios.","Thus, the pressing issue in passive NLOS imaging currently lies in whether it is possible to estimate the light transport matrices which corresponding to relay surfaces and scenes, as well as the specific distribution of targets, with a small amount of prior knowledge.","In this work, we hypothesized a high-dimensional manifold and mathematically proved its existence.","Within this high-dimensional manifold, the structural information of obscured targets is minimally disrupted.","Therefore, we proposed a universal framework named High-Dimensional Projection Selection (HDPS) which can establish this high-dimensional manifold and output its projection onto corresponding surfaces on low-dimensional.","HDPS can be applied to most mature network architectures and estimate the distribution of target and light spot obtained by camera with only minimal prior data.","Certainly, with the help of the estimated information, it can establish a high-dimensional manifold consisting of target and input.","As demonstrated in experiment, our framework, even when applied to the most basic network structures, can achieve higher accuracy results with significantly smaller amounts of prior data.","Thereby, our approach enables passive NLOS scenarios to reconstruct target by limited prior data and computational resources."],"url":"http://arxiv.org/abs/2404.06015v1","category":"physics.optics"}
{"created":"2024-04-09 03:57:04","title":"Noise induced coherent ergotropy of a quantum heat engine","abstract":"We theoretically identify the noise-induced coherent contribution to the ergotropy of a four-level quantum heat engine coupled to a unimodal quantum cavity. We utilize a protocol where the passive state's quasiprobabilities can be analytically identified from the population-coherence coupled reduced density matrix. The reduced density matrix elements are evaluated using a microscopic quantum master equation formalism. Multiple ergotropies within the same coherence interval, each characterized by a positive and pronounced coherent contribution, are observed. These ergotropies are a result of population inversion as well as quasiprobability-population inversion, controllable through the coherence measure parameters. The optimal flux and power of the engine are found to be at moderate values of ergotropy with increasing values of noise-induced coherence. The optimal power at different coherences is found to possess a constant ergotropy.","sentences":["We theoretically identify the noise-induced coherent contribution to the ergotropy of a four-level quantum heat engine coupled to a unimodal quantum cavity.","We utilize a protocol where the passive state's quasiprobabilities can be analytically identified from the population-coherence coupled reduced density matrix.","The reduced density matrix elements are evaluated using a microscopic quantum master equation formalism.","Multiple ergotropies within the same coherence interval, each characterized by a positive and pronounced coherent contribution, are observed.","These ergotropies are a result of population inversion as well as quasiprobability-population inversion, controllable through the coherence measure parameters.","The optimal flux and power of the engine are found to be at moderate values of ergotropy with increasing values of noise-induced coherence.","The optimal power at different coherences is found to possess a constant ergotropy."],"url":"http://arxiv.org/abs/2404.05994v1","category":"quant-ph"}
{"created":"2024-04-09 03:38:59","title":"Commute with Community: Enhancing Shared Travel through Social Networks","abstract":"Shared mobility redefines urban transportation, offering economic and environmental benefits by reducing pollution and urban congestion. However, in the post-pandemic era, the shared mobility sector is grappling with a crisis of trust, particularly concerning passenger hesistancy towards shared transportation options. To address these problems, in this paper we take social network into consideration and propose a novel carpooling matching framework based on graph neural network and reinforcement learning,increasing the carpooling rate to 48% and reducing the average delay time to 6.1 minutes and average detour distance to 2.8km. Furthermore, we introduce an innovative metric, termed 'tolerance' for mobility scheduling models to effectively quantify users' sensitivity to social distancing. We conduct a sensitivity analysis to demonstrate that our model offers a viable approach to amplify the benefits, delivering resilient strategies for the advancement and proliferation of shared mobility incentives.","sentences":["Shared mobility redefines urban transportation, offering economic and environmental benefits by reducing pollution and urban congestion.","However, in the post-pandemic era, the shared mobility sector is grappling with a crisis of trust, particularly concerning passenger hesistancy towards shared transportation options.","To address these problems, in this paper we take social network into consideration and propose a novel carpooling matching framework based on graph neural network and reinforcement learning,increasing the carpooling rate to 48% and reducing the average delay time to 6.1 minutes and average detour distance to 2.8km.","Furthermore, we introduce an innovative metric, termed 'tolerance' for mobility scheduling models to effectively quantify users' sensitivity to social distancing.","We conduct a sensitivity analysis to demonstrate that our model offers a viable approach to amplify the benefits, delivering resilient strategies for the advancement and proliferation of shared mobility incentives."],"url":"http://arxiv.org/abs/2404.05987v1","category":"cs.SI"}
{"created":"2024-04-09 03:27:09","title":"A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics","abstract":"Despite accuracy and computation benchmarks being widely available to help choose among neural network models, these are usually trained on datasets with many classes, and do not give a precise idea of performance for applications of few (< 10) classes. The conventional procedure to predict performance is to train and test repeatedly on the different models and dataset variations of interest. However, this is computationally expensive. We propose an efficient classification difficulty measure that is calculated from the number of classes and intra- and inter-class similarity metrics of the dataset. After a single stage of training and testing per model family, relative performance for different datasets and models of the same family can be predicted by comparing difficulty measures - without further training and testing. We show how this measure can help a practitioner select a computationally efficient model for a small dataset 6 to 29x faster than through repeated training and testing. We give an example of use of the measure for an industrial application in which options are identified to select a model 42% smaller than the baseline YOLOv5-nano model, and if class merging from 3 to 2 classes meets requirements, 85% smaller.","sentences":["Despite accuracy and computation benchmarks being widely available to help choose among neural network models, these are usually trained on datasets with many classes, and do not give a precise idea of performance for applications of few (< 10) classes.","The conventional procedure to predict performance is to train and test repeatedly on the different models and dataset variations of interest.","However, this is computationally expensive.","We propose an efficient classification difficulty measure that is calculated from the number of classes and intra- and inter-class similarity metrics of the dataset.","After a single stage of training and testing per model family, relative performance for different datasets and models of the same family can be predicted by comparing difficulty measures - without further training and testing.","We show how this measure can help a practitioner select a computationally efficient model for a small dataset 6 to 29x faster than through repeated training and testing.","We give an example of use of the measure for an industrial application in which options are identified to select a model 42% smaller than the baseline YOLOv5-nano model, and if class merging from 3 to 2 classes meets requirements, 85% smaller."],"url":"http://arxiv.org/abs/2404.05981v1","category":"cs.LG"}
{"created":"2024-04-09 03:01:20","title":"On the second boundary value problem for mean curvature flow in Minkowski space","abstract":"This is a sequel to [2] and [3], which study the second boundary value problems for mean curvature flow. Consequently, we construct the translating solitons with prescribed Gauss image in Minkowski space.","sentences":["This is a sequel to [2] and [3], which study the second boundary value problems for mean curvature flow.","Consequently, we construct the translating solitons with prescribed Gauss image in Minkowski space."],"url":"http://arxiv.org/abs/2404.05972v1","category":"math.DG"}
{"created":"2024-04-09 02:53:03","title":"A gluing construction of singular solutions for a fully non-linear equation in conformal geometry","abstract":"In this paper we produce families of complete, non-compact Riemannian metrics with positive constant $\\sigma_2$--curvature on the sphere $\\mathbb S^n$, $n>4$, with a prescribed singular set $\\Lambda$ given by a disjoint union of closed submanifolds whose dimension is positive and strictly less than $(n-\\sqrt{n}-2)/2$. The $\\sigma_2$--curvature in conformal geometry is defined as the second elementary symmetric polynomial of the eigenvalues of the Schouten tensor, which yields a fully non-linear PDE for the conformal factor. We show that the classical gluing method of Mazzeo-Pacard (JDG 1996) for the scalar curvature still works in the fully non-linear setting. This is a consequence of the conformal properties of the $\\sigma_2$ equation, which imply that the linearized operator has good mapping properties in weighted spaces. Our method could be potentially generalized to any $\\sigma_k$, $2\\leq k<n/2$, nevertheless, the numerology becomes too involved.","sentences":["In this paper we produce families of complete, non-compact Riemannian metrics with positive constant $\\sigma_2$--curvature on the sphere $\\mathbb S^n$, $n>4$, with a prescribed singular set $\\Lambda$ given by a disjoint union of closed submanifolds whose dimension is positive and strictly less than $(n-\\sqrt{n}-2)/2$. The $\\sigma_2$--curvature in conformal geometry is defined as the second elementary symmetric polynomial of the eigenvalues of the Schouten tensor, which yields a fully non-linear PDE for the conformal factor.","We show that the classical gluing method of Mazzeo-Pacard (JDG 1996) for the scalar curvature still works in the fully non-linear setting.","This is a consequence of the conformal properties of the $\\sigma_2$ equation, which imply that the linearized operator has good mapping properties in weighted spaces.","Our method could be potentially generalized to any $\\sigma_k$, $2\\leq k<n/2$, nevertheless, the numerology becomes too involved."],"url":"http://arxiv.org/abs/2404.05965v1","category":"math.DG"}
{"created":"2024-04-09 02:21:23","title":"3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards","abstract":"Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture. One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches. However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning. In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data. The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training. The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch pruning.","sentences":["Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture.","One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches.","However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning.","In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data.","The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training.","The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction.","Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data.","The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting.","The characterization improvements contributed to the precision and efficacy of robotic branch pruning."],"url":"http://arxiv.org/abs/2404.05953v1","category":"cs.RO"}
{"created":"2024-04-09 01:31:21","title":"Lagrangian loci in moduli of abelian surfaces","abstract":"We show that any smooth surface germ in the moduli of abelian surfaces arises from a Lagrangian fibration of abelian surfaces. By Donagi-Markman's cubic condition, the key issue of the proof is to find a suitable affine structure with a compatible cubic form on the base space of the family. We achieve this by analyzing the properties of cubic forms in two variables and proving the existence of the solution of the resulting partial differential equations by Cauchy-Kowalewski Theorem. Modifying the argument, we show also that a smooth curve germ in the moduli of abelian surfaces arises from a Lagrangian fibration if and only if the curve is a null curve with respect to the natural holomorphic conformal structure on the moduli of abelian surfaces.","sentences":["We show that any smooth surface germ in the moduli of abelian surfaces arises from a Lagrangian fibration of abelian surfaces.","By Donagi-Markman's cubic condition, the key issue of the proof is to find a suitable affine structure with a compatible cubic form on the base space of the family.","We achieve this by analyzing the properties of cubic forms in two variables and proving the existence of the solution of the resulting partial differential equations by Cauchy-Kowalewski Theorem.","Modifying the argument, we show also that a smooth curve germ in the moduli of abelian surfaces arises from a Lagrangian fibration if and only if the curve is a null curve with respect to the natural holomorphic conformal structure on the moduli of abelian surfaces."],"url":"http://arxiv.org/abs/2404.05931v1","category":"math.AG"}
{"created":"2024-04-09 01:12:46","title":"Resolving turbulence and drag over textured surfaces using texture-less simulations: the case of slip/no-slip textures","abstract":"We study the effect of surface texture on an overlying turbulent flow for the case of textures made of an alternating slip/no-slip pattern, a common model for superhydrophobic surfaces but also a particularly simple form of texture. For texture sizes $L^+ \\lesssim 20$, we have previously reported that turbulence remained smooth-wall-like, other than experiencing an apparent origin offset for different flow components. For slip/no-slip textures, this effect reduced to the flow experiencing slip conditions in the streamwise and spanwise directions and zero transpiration at the surface. The overlying turbulence effectively perceived such boundary conditions at least up to texture sizes $L^+ \\approx 50$. However, beyond $L^+ \\approx 20$ the texture interacted with the overlying turbulence in a non-homogeneous fashion, additional Reynolds stresses arose and turbulence was no longer smooth-wall-like. This is the typical effect of surface texture observed for rough surfaces, and results in an increase in drag relative to smooth-wall flows. In this paper, we argue that this occurs because the texture modifies the overlying turbulence through non-linear, cross-advective terms between the background turbulence and the texture-coherent flow directly induced by the surface topology. To verify this, we conduct homogeneous-slip-length simulations where we introduce additional, forcing terms in the Navier-Stokes equations to capture the effect of this non-linear interaction on the background turbulence. The interaction can then be accounted for without the need to resolve the surface texture. We show that the additional terms quantitatively capture the changes in the flow up to texture sizes $L^+ \\approx 70$--$100$, including not just the roughness function but also the flow statistics and structure.","sentences":["We study the effect of surface texture on an overlying turbulent flow for the case of textures made of an alternating slip/no-slip pattern, a common model for superhydrophobic surfaces but also a particularly simple form of texture.","For texture sizes $L^+ \\lesssim 20$, we have previously reported that turbulence remained smooth-wall-like, other than experiencing an apparent origin offset for different flow components.","For slip/no-slip textures, this effect reduced to the flow experiencing slip conditions in the streamwise and spanwise directions and zero transpiration at the surface.","The overlying turbulence effectively perceived such boundary conditions at least up to texture sizes $L^+ \\approx 50$.","However, beyond $L^+ \\approx 20$ the texture interacted with the overlying turbulence in a non-homogeneous fashion, additional Reynolds stresses arose and turbulence was no longer smooth-wall-like.","This is the typical effect of surface texture observed for rough surfaces, and results in an increase in drag relative to smooth-wall flows.","In this paper, we argue that this occurs because the texture modifies the overlying turbulence through non-linear, cross-advective terms between the background turbulence and the texture-coherent flow directly induced by the surface topology.","To verify this, we conduct homogeneous-slip-length simulations where we introduce additional, forcing terms in the Navier-Stokes equations to capture the effect of this non-linear interaction on the background turbulence.","The interaction can then be accounted for without the need to resolve the surface texture.","We show that the additional terms quantitatively capture the changes in the flow up to texture sizes $L^+ \\approx 70$--$100$, including not just the roughness function but also the flow statistics and structure."],"url":"http://arxiv.org/abs/2404.05926v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 23:58:46","title":"Equation of State of Hot Neutron Star Matter using Finite Range Simple Effective Interaction","abstract":"The equation of state of hot neutron star matter of n+p+e+$\\mu$ composition in $\\beta$-equilibrium is studied for both neutrino-free isothermal and neutrino-trapped isentropic conditions, using the formalism where the thermal evolution is built upon its zero-temperature predictions in a self-consistent manner. The accuracy of the parabolic approximation, often used in the finite temperature calculation of hot neutron star matter, is verified by comparing with the results obtained from the exact evaluation in the neutrino-free neutron star matter. The equation of state of neutrino-trapped isentropic matter at low entropic condition, relevant to the core-collapsing supernovae, is formulated. In the isentropic matter, the particle fractions and equation of state have marginal variance as entropy per particle varies between 1 to 3 (in the unit of k$_B$), but the temperature profile shows marked variation. The isentropes are found to be much less sensitive to the nuclear matter incompressibility, but have a large dependence on the slope parameter L. The bulk properties of the neutron stars predicted by the isentropic equation of state for different entropy are calculated. A model calculation for the early stage evolution of the protoneutron star to neutron star configuration is also given.","sentences":["The equation of state of hot neutron star matter of n+p+e+$\\mu$ composition in $\\beta$-equilibrium is studied for both neutrino-free isothermal and neutrino-trapped isentropic conditions, using the formalism where the thermal evolution is built upon its zero-temperature predictions in a self-consistent manner.","The accuracy of the parabolic approximation, often used in the finite temperature calculation of hot neutron star matter, is verified by comparing with the results obtained from the exact evaluation in the neutrino-free neutron star matter.","The equation of state of neutrino-trapped isentropic matter at low entropic condition, relevant to the core-collapsing supernovae, is formulated.","In the isentropic matter, the particle fractions and equation of state have marginal variance as entropy per particle varies between 1 to 3 (in the unit of k$_B$), but the temperature profile shows marked variation.","The isentropes are found to be much less sensitive to the nuclear matter incompressibility, but have a large dependence on the slope parameter L.","The bulk properties of the neutron stars predicted by the isentropic equation of state for different entropy are calculated.","A model calculation for the early stage evolution of the protoneutron star to neutron star configuration is also given."],"url":"http://arxiv.org/abs/2404.05910v1","category":"nucl-th"}
{"created":"2024-04-08 23:08:38","title":"Quantum-inspired activation functions in the convolutional neural network","abstract":"Driven by the significant advantages offered by quantum computing, research in quantum machine learning has increased in recent years. While quantum speed-up has been demonstrated in some applications of quantum machine learning, a comprehensive understanding of its underlying mechanisms for improved performance remains elusive. Our study fills this gap by examining the expressibility of quantum circuits integrated within a convolutional neural network (CNN). Through numerical training on the MNIST dataset, our hybrid quantum-classical CNN model exhibited superior feature selection capabilities and significantly reduced the required training steps compared to the classical CNN. To understand the root of this enhanced performance, we conducted an analytical investigation of the functional expressibility of quantum circuits and derived a quantum activation function. We demonstrated that this quantum activation is more efficient in selecting important features and discarding unimportant information of input images. These findings not only deepen our comprehension of quantum-enhanced machine-learning models but also advance the classical machine-learning technique by introducing the quantum-inspired activation function.","sentences":["Driven by the significant advantages offered by quantum computing, research in quantum machine learning has increased in recent years.","While quantum speed-up has been demonstrated in some applications of quantum machine learning, a comprehensive understanding of its underlying mechanisms for improved performance remains elusive.","Our study fills this gap by examining the expressibility of quantum circuits integrated within a convolutional neural network (CNN).","Through numerical training on the MNIST dataset, our hybrid quantum-classical CNN model exhibited superior feature selection capabilities and significantly reduced the required training steps compared to the classical CNN.","To understand the root of this enhanced performance, we conducted an analytical investigation of the functional expressibility of quantum circuits and derived a quantum activation function.","We demonstrated that this quantum activation is more efficient in selecting important features and discarding unimportant information of input images.","These findings not only deepen our comprehension of quantum-enhanced machine-learning models but also advance the classical machine-learning technique by introducing the quantum-inspired activation function."],"url":"http://arxiv.org/abs/2404.05901v1","category":"quant-ph"}
{"created":"2024-04-08 22:54:14","title":"Inexact Simplification of Symbolic Regression Expressions with Locality-sensitive Hashing","abstract":"Symbolic regression (SR) searches for parametric models that accurately fit a dataset, prioritizing simplicity and interpretability. Despite this secondary objective, studies point out that the models are often overly complex due to redundant operations, introns, and bloat that arise during the iterative process, and can hinder the search with repeated exploration of bloated segments. Applying a fast heuristic algebraic simplification may not fully simplify the expression and exact methods can be infeasible depending on size or complexity of the expressions. We propose a novel agnostic simplification and bloat control for SR employing an efficient memoization with locality-sensitive hashing (LHS). The idea is that expressions and their sub-expressions traversed during the iterative simplification process are stored in a dictionary using LHS, enabling efficient retrieval of similar structures. We iterate through the expression, replacing subtrees with others of same hash if they result in a smaller expression. Empirical results shows that applying this simplification during evolution performs equal or better than without simplification in minimization of error, significantly reducing the number of nonlinear functions. This technique can learn simplification rules that work in general or for a specific problem, and improves convergence while reducing model complexity.","sentences":["Symbolic regression (SR) searches for parametric models that accurately fit a dataset, prioritizing simplicity and interpretability.","Despite this secondary objective, studies point out that the models are often overly complex due to redundant operations, introns, and bloat that arise during the iterative process, and can hinder the search with repeated exploration of bloated segments.","Applying a fast heuristic algebraic simplification may not fully simplify the expression and exact methods can be infeasible depending on size or complexity of the expressions.","We propose a novel agnostic simplification and bloat control for SR employing an efficient memoization with locality-sensitive hashing (LHS).","The idea is that expressions and their sub-expressions traversed during the iterative simplification process are stored in a dictionary using LHS, enabling efficient retrieval of similar structures.","We iterate through the expression, replacing subtrees with others of same hash if they result in a smaller expression.","Empirical results shows that applying this simplification during evolution performs equal or better than without simplification in minimization of error, significantly reducing the number of nonlinear functions.","This technique can learn simplification rules that work in general or for a specific problem, and improves convergence while reducing model complexity."],"url":"http://arxiv.org/abs/2404.05898v1","category":"cs.NE"}
{"created":"2024-04-08 22:16:05","title":"Current dependence of the low bias resistance of small capacitance Josephson junctions","abstract":"The dc current-voltage characteristics of small Josephson junctions reveal features that are not observed in larger junctions, in particular, a switch to the finite voltage state at current values much less than the expected critical current of the junction and a finite resistance in the nominally superconducting regime. Both phenomena are due to the increased sensitivity to noise associated with the small capacitance of the Josephson junction and have been extensively studied a few decades ago. Here I focus on the current bias dependence of the differential resistance of the junction at low current bias in the nominally superconducting regime, using a quantum Langevin equation approach that enables a physically transparent incorporation of the noise environment of the junction. A similar approach might be useful in modeling the sensitivity of superconducting qubits to noise in the microwave regime.","sentences":["The dc current-voltage characteristics of small Josephson junctions reveal features that are not observed in larger junctions, in particular, a switch to the finite voltage state at current values much less than the expected critical current of the junction and a finite resistance in the nominally superconducting regime.","Both phenomena are due to the increased sensitivity to noise associated with the small capacitance of the Josephson junction and have been extensively studied a few decades ago.","Here I focus on the current bias dependence of the differential resistance of the junction at low current bias in the nominally superconducting regime, using a quantum Langevin equation approach that enables a physically transparent incorporation of the noise environment of the junction.","A similar approach might be useful in modeling the sensitivity of superconducting qubits to noise in the microwave regime."],"url":"http://arxiv.org/abs/2404.05890v1","category":"cond-mat.supr-con"}
{"created":"2024-04-08 20:36:58","title":"The nonlinear wave equation with nonlinear Wentzell boundary conditions on time-dependent compact Riemannian manifolds","abstract":"We prove a local well-posedness result for an evolution problem consisting of a semilinear wave equation with subcritical nonlinearities posed on a time-dependent compact Riemannian manifold and supplied with a nonlinear dynamical boundary condition of Wentzell type.","sentences":["We prove a local well-posedness result for an evolution problem consisting of a semilinear wave equation with subcritical nonlinearities posed on a time-dependent compact Riemannian manifold and supplied with a nonlinear dynamical boundary condition of Wentzell type."],"url":"http://arxiv.org/abs/2404.05855v1","category":"math.AP"}
{"created":"2024-04-08 20:30:30","title":"The Schwarzschild-de Sitter Metric of Nonlocal $\\sqrt{dS}$ Gravity","abstract":"It is already known that a simple nonlocal de Sitter gravity model, which we denote as $\\sqrt{dS}$ gravity, contains an exact vacuum cosmological solution which mimics dark energy and dark matter and is in very good agreement with the standard model of cosmology. This success of $\\sqrt{dS}$ gravity motivated us to investigate how it works at lower than cosmic scale -- galactic and the solar system. This paper contains our investigation of the corresponding Schwarzschild-de Sitter metric of the $\\sqrt{dS}$ gravity model. To get exact solution, it is necessary to solve the corresponding nonlinear differential equation, what is a very complicated and difficult problem. What we obtained is a solution of linearized equation, which is related to space metric far from the massive body, where gravitational field is weak. The obtained approximate solution is of particular interest for examining the possible role of non-local de Sitter gravity $\\sqrt{dS}$ in describing the effects in galactic dynamics that are usually attributed to dark matter. The solution has been tested on the Milky Way and the spiral galaxy M33 and is in good agreement with observational measurements.","sentences":["It is already known that a simple nonlocal de Sitter gravity model, which we denote as $\\sqrt{dS}$ gravity, contains an exact vacuum cosmological solution which mimics dark energy and dark matter and is in very good agreement with the standard model of cosmology.","This success of $\\sqrt{dS}$ gravity motivated us to investigate how it works at lower than cosmic scale -- galactic and the solar system.","This paper contains our investigation of the corresponding Schwarzschild-de Sitter metric of the $\\sqrt{dS}$ gravity model.","To get exact solution, it is necessary to solve the corresponding nonlinear differential equation, what is a very complicated and difficult problem.","What we obtained is a solution of linearized equation, which is related to space metric far from the massive body, where gravitational field is weak.","The obtained approximate solution is of particular interest for examining the possible role of non-local de Sitter gravity $\\sqrt{dS}$ in describing the effects in galactic dynamics that are usually attributed to dark matter.","The solution has been tested on the Milky Way and the spiral galaxy M33 and is in good agreement with observational measurements."],"url":"http://arxiv.org/abs/2404.05848v1","category":"physics.gen-ph"}
{"created":"2024-04-08 20:21:41","title":"Quasispins of vacancy defects and their interactions in disordered antiferromagnets","abstract":"Vacancy defects in disordered magnetic materials are known to act as effective spins, ``quasispins'', in response to an external magnetic field. In the dilute limit, the contributions of such ``quasispins'' to the magnetic susceptibility $\\chi_\\text{vac}(T)\\propto N_\\text{vac}/T$ are singular in the limit of low temperatures $T$ and match those of free spins. With increasing the density of vacancies, their interactions may become essential. Motivated by frustrated and quasi-one-dimensional magnetic materials, we study analytically quasispins and their interactions in a generic system that has short-range antiferromagnetic order and lacks long-range order. We predict that if the vacancy defect does not disrupt the short-range antiferromagnetic order around it, the quasispin value matches the value of spins of the magnetic atoms in the material, and the correlators of the quasispins of different vacancies match the spin-spin correlators in the vacancy-free material. We confirm our conclusions by exact calculations for Ising chains with nearest-neighbour and next-to-nearest-neighbour interactions. We also compute the first virial correction to the susceptibility of a magnetic material due to the interactions of vacancy quasispins.","sentences":["Vacancy defects in disordered magnetic materials are known to act as effective spins, ``quasispins'', in response to an external magnetic field.","In the dilute limit, the contributions of such ``quasispins'' to the magnetic susceptibility $\\chi_\\text{vac}(T)\\propto N_\\text{vac}/T$ are singular in the limit of low temperatures $T$ and match those of free spins.","With increasing the density of vacancies, their interactions may become essential.","Motivated by frustrated and quasi-one-dimensional magnetic materials, we study analytically quasispins and their interactions in a generic system that has short-range antiferromagnetic order and lacks long-range order.","We predict that if the vacancy defect does not disrupt the short-range antiferromagnetic order around it, the quasispin value matches the value of spins of the magnetic atoms in the material, and the correlators of the quasispins of different vacancies match the spin-spin correlators in the vacancy-free material.","We confirm our conclusions by exact calculations for Ising chains with nearest-neighbour and next-to-nearest-neighbour interactions.","We also compute the first virial correction to the susceptibility of a magnetic material due to the interactions of vacancy quasispins."],"url":"http://arxiv.org/abs/2404.05845v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-08 20:00:39","title":"Fourier neural operator for large eddy simulation of compressible Rayleigh-Taylor turbulence","abstract":"The Fourier neural operator (FNO) framework is applied to the large eddy simulation (LES) of three-dimensional compressible Rayleigh-Taylor (RT) turbulence with miscible fluids at Atwood number $A_t=0.5$, stratification parameter $Sr=1.0$, and Reynolds numbers $Re=10000$ and 30000. The FNO model is first used for predicting three-dimensional compressible turbulence. The different magnitudes of physical fields are normalized using root mean square values for an easier training of FNO models. In the \\emph{a posteriori} tests, the FNO model outperforms the velocity gradient model (VGM), the dynamic Smagorinsky model (DSM), and implicit large eddy simulation (ILES) in predicting various statistical quantities and instantaneous structures, and is particularly superior to traditional LES methods in predicting temperature fields and velocity divergence. Moreover, the computational efficiency of the FNO model is much higher than that of traditional LES methods. FNO models trained with short-time, low Reynolds number data exhibit a good generalization performance on longer-time predictions and higher Reynolds numbers in the \\emph{a posteriori} tests.","sentences":["The Fourier neural operator (FNO) framework is applied to the large eddy simulation (LES) of three-dimensional compressible Rayleigh-Taylor (RT) turbulence with miscible fluids at Atwood number $A_t=0.5$, stratification parameter $Sr=1.0$, and Reynolds numbers $Re=10000$ and 30000.","The FNO model is first used for predicting three-dimensional compressible turbulence.","The different magnitudes of physical fields are normalized using root mean square values for an easier training of FNO models.","In the \\emph{a posteriori} tests, the FNO model outperforms the velocity gradient model (VGM), the dynamic Smagorinsky model (DSM), and implicit large eddy simulation (ILES) in predicting various statistical quantities and instantaneous structures, and is particularly superior to traditional LES methods in predicting temperature fields and velocity divergence.","Moreover, the computational efficiency of the FNO model is much higher than that of traditional LES methods.","FNO models trained with short-time, low Reynolds number data exhibit a good generalization performance on longer-time predictions and higher Reynolds numbers in the \\emph{a posteriori} tests."],"url":"http://arxiv.org/abs/2404.05834v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 19:00:06","title":"The $SU(3)_C\\times SU(3)_L\\times U(1)_X$ (331) Model:Addressing the Fermion Families Problem within Horizontal Anomalies Cancellation","abstract":"One of the most important and unanswered problems in particle physics is the origin of the three generations of quarks and leptons. The standard Model does not provide any hint regarding its sequential charge assignments, which remain a fundamental mystery of Nature. One possible solution of the puzzle is to look for charge assignments, in a given gauge theory, that are inter-generational, by employing the cancellation of the gravitational and gauge anomalies horizontally. The 331 model, based on an $SU(3)_C\\times SU(3)_L \\times U(1)_X$ does it in an economic way, and defines a possible extension of the Standard Model, where the number of families has to be necessarily three. We review the model in Frampton's formulation, that predicts the existence of bileptons. Another characteristics of the model is to unify the $SU(3)_C\\times SU(2)_L \\times U(1)_X$ into the 331 symmetry at a scale which is in the TeV range, and can be tested at the LHC. Expressions of the scalar mass eigenstates and of the renormalization group equations of the model are also presented.","sentences":["One of the most important and unanswered problems in particle physics is the origin of the three generations of quarks and leptons.","The standard Model does not provide any hint regarding its sequential charge assignments, which remain a fundamental mystery of Nature.","One possible solution of the puzzle is to look for charge assignments, in a given gauge theory, that are inter-generational, by employing the cancellation of the gravitational and gauge anomalies horizontally.","The 331 model, based on an $SU(3)_C\\times SU(3)_L \\times U(1)_X$ does it in an economic way, and defines a possible extension of the Standard Model, where the number of families has to be necessarily three.","We review the model in Frampton's formulation, that predicts the existence of bileptons.","Another characteristics of the model is to unify the $SU(3)_C\\times SU(2)_L \\times U(1)_X$ into the 331 symmetry at a scale which is in the TeV range, and can be tested at the LHC.","Expressions of the scalar mass eigenstates and of the renormalization group equations of the model are also presented."],"url":"http://arxiv.org/abs/2404.05821v1","category":"hep-ph"}
{"created":"2024-04-08 18:55:04","title":"Generalizing the Eight Levels Theorem: A Journey to Mersenne Prime Discoveries and New Polynomial Classes","abstract":"Mersenne primes, renowned for their captivating form as $2^p - 1$, have intrigued mathematicians for centuries. In this paper, we embark on a captivating quest to unveil the intricate nature of Mersenne primes, seamlessly integrating methods with the Eight Levels Theorem. Initially, we extend the Eight Levels Theorem and introduce an innovative approach that harmoniously combines arithmetic and differential techniques to compute the coefficients of the polynomial expansions of $x^n + y^n$ in terms of binary quadratic forms. This endeavor leads us to the genesis of novel polynomial sequences as we scrutinize the coefficients within this expansion. Our research unearths previously uncharted connections between Mersenne numbers and the derivatives of specific polynomial sequences. By forging this linkage, we not only enhance our understanding of Mersenne primes but also bridge the divide between well-established sequences in number theory and differential equations. This broadens the applicability of our findings across diverse scientific domains, revealing fresh avenues for exploration in number theory and beyond. These polynomial bridges serve as conduits between these sequences, unveiling exciting prospects for future research. This interdisciplinary exploration opens up exciting possibilities for the broader implications of Mersenne primes, extending their significance beyond the realm of pure mathematics. In our paper, we also delve into the intriguing influence of the Golden Ratio constant, which unveils segments reminiscent of the beauty found in nature, adding an unexpected dimension to the world of arithmetic. In this harmonious journey, the allure of Mersenne primes resonates through the symphony of mathematical discovery.","sentences":["Mersenne primes, renowned for their captivating form as $2^p - 1$, have intrigued mathematicians for centuries.","In this paper, we embark on a captivating quest to unveil the intricate nature of Mersenne primes, seamlessly integrating methods with the Eight Levels Theorem.","Initially, we extend the Eight Levels Theorem and introduce an innovative approach that harmoniously combines arithmetic and differential techniques to compute the coefficients of the polynomial expansions of $x^n + y^n$ in terms of binary quadratic forms.","This endeavor leads us to the genesis of novel polynomial sequences as we scrutinize the coefficients within this expansion.","Our research unearths previously uncharted connections between Mersenne numbers and the derivatives of specific polynomial sequences.","By forging this linkage, we not only enhance our understanding of Mersenne primes but also bridge the divide between well-established sequences in number theory and differential equations.","This broadens the applicability of our findings across diverse scientific domains, revealing fresh avenues for exploration in number theory and beyond.","These polynomial bridges serve as conduits between these sequences, unveiling exciting prospects for future research.","This interdisciplinary exploration opens up exciting possibilities for the broader implications of Mersenne primes, extending their significance beyond the realm of pure mathematics.","In our paper, we also delve into the intriguing influence of the Golden Ratio constant, which unveils segments reminiscent of the beauty found in nature, adding an unexpected dimension to the world of arithmetic.","In this harmonious journey, the allure of Mersenne primes resonates through the symphony of mathematical discovery."],"url":"http://arxiv.org/abs/2404.05772v1","category":"math.GM"}
{"created":"2024-04-08 18:41:55","title":"Label Propagation Training Schemes for Physics-Informed Neural Networks and Gaussian Processes","abstract":"This paper proposes a semi-supervised methodology for training physics-informed machine learning methods. This includes self-training of physics-informed neural networks and physics-informed Gaussian processes in isolation, and the integration of the two via co-training. We demonstrate via extensive numerical experiments how these methods can ameliorate the issue of propagating information forward in time, which is a common failure mode of physics-informed machine learning.","sentences":["This paper proposes a semi-supervised methodology for training physics-informed machine learning methods.","This includes self-training of physics-informed neural networks and physics-informed Gaussian processes in isolation, and the integration of the two via co-training.","We demonstrate via extensive numerical experiments how these methods can ameliorate the issue of propagating information forward in time, which is a common failure mode of physics-informed machine learning."],"url":"http://arxiv.org/abs/2404.05817v1","category":"cs.LG"}
{"created":"2024-04-08 18:15:13","title":"Slax: A Composable JAX Library for Rapid and Flexible Prototyping of Spiking Neural Networks","abstract":"Recent advances to algorithms for training spiking neural networks (SNNs) often leverage their unique dynamics. While backpropagation through time (BPTT) with surrogate gradients dominate the field, a rich landscape of alternatives can situate algorithms across various points in the performance, bio-plausibility, and complexity landscape. Evaluating and comparing algorithms is currently a cumbersome and error-prone process, requiring them to be repeatedly re-implemented. We introduce Slax, a JAX-based library designed to accelerate SNN algorithm design, compatible with the broader JAX and Flax ecosystem. Slax provides optimized implementations of diverse training algorithms, allowing direct performance comparison. Its toolkit includes methods to visualize and debug algorithms through loss landscapes, gradient similarities, and other metrics of model behavior during training.","sentences":["Recent advances to algorithms for training spiking neural networks (SNNs) often leverage their unique dynamics.","While backpropagation through time (BPTT) with surrogate gradients dominate the field, a rich landscape of alternatives can situate algorithms across various points in the performance, bio-plausibility, and complexity landscape.","Evaluating and comparing algorithms is currently a cumbersome and error-prone process, requiring them to be repeatedly re-implemented.","We introduce Slax, a JAX-based library designed to accelerate SNN algorithm design, compatible with the broader JAX and Flax ecosystem.","Slax provides optimized implementations of diverse training algorithms, allowing direct performance comparison.","Its toolkit includes methods to visualize and debug algorithms through loss landscapes, gradient similarities, and other metrics of model behavior during training."],"url":"http://arxiv.org/abs/2404.05807v1","category":"cs.NE"}
{"created":"2024-04-08 18:04:02","title":"Vector meson production using the Balitsky-Kovchegov equation including the dipole orientation","abstract":"In this proceedings a solution of the target-rapidity Balitsky-Kovchegov (BK) equation is presented considering the complete impact-parameter dependence, including the orientation of the dipole with respect to the impact-parameter vector. The target-rapidity formulation of the BK equation introduces non-locality in rapidity. Three different prescriptions are considered to take into account the rapidities preceding the initial condition. The solutions are used to compute the structure functions of the proton and the diffractive photo- and electro-production of J/{\\psi} off protons. The predictions agree well with HERA data, confirming that the target-rapidity Balitsky-Kovchegov equation with the full impact-parameter dependence is a viable tool to study the small Bjorken-x limit of perturbative QCD at current facilities like RHIC and LHC as well as in future colliders like the EIC.","sentences":["In this proceedings a solution of the target-rapidity Balitsky-Kovchegov (BK) equation is presented considering the complete impact-parameter dependence, including the orientation of the dipole with respect to the impact-parameter vector.","The target-rapidity formulation of the BK equation introduces non-locality in rapidity.","Three different prescriptions are considered to take into account the rapidities preceding the initial condition.","The solutions are used to compute the structure functions of the proton and the diffractive photo- and electro-production of J/{\\psi} off protons.","The predictions agree well with HERA data, confirming that the target-rapidity Balitsky-Kovchegov equation with the full impact-parameter dependence is a viable tool to study the small Bjorken-x limit of perturbative QCD at current facilities like RHIC and LHC as well as in future colliders like the EIC."],"url":"http://arxiv.org/abs/2404.05800v1","category":"hep-ph"}
{"created":"2024-04-08 18:00:30","title":"Electrical control of superconducting spin valves using ferromagnetic helices","abstract":"The geometrical properties of a helical ferromagnet are shown theoretically to control the critical temperature of a proximity-coupled superconductor. Using the Usadel equation for diffusive spin transport, we provide self-consistent analysis of how curvature and torsion modulate the proximity effect. When the helix is attached to a piezoelectric actuator, the pitch of the helix -- and hence the superconducting transition -- can be controlled electrically.","sentences":["The geometrical properties of a helical ferromagnet are shown theoretically to control the critical temperature of a proximity-coupled superconductor.","Using the Usadel equation for diffusive spin transport, we provide self-consistent analysis of how curvature and torsion modulate the proximity effect.","When the helix is attached to a piezoelectric actuator, the pitch of the helix -- and hence the superconducting transition -- can be controlled electrically."],"url":"http://arxiv.org/abs/2404.05798v1","category":"cond-mat.supr-con"}
{"created":"2024-04-08 18:00:06","title":"Deep Learning the Intergalactic Medium using Lyman-alpha Forest at $ 4 \\leq z \\leq 5$","abstract":"Unveiling the thermal history of the intergalactic medium (IGM) at $4 \\leq z \\leq 5$ holds the potential to reveal early onset HeII reionization or lingering thermal fluctuations from HI reionization. We set out to reconstruct the IGM gas properties along simulated Lyman-alpha forest data on pixel-by-pixel basis, employing deep Bayesian neural networks. Our approach leverages the Sherwood-Relics simulation suite, consisting of diverse thermal histories, to generate mock spectra. Our convolutional and residual networks with likelihood metric predicts the Ly$\\alpha$ optical depth-weighted density or temperature for each pixel in the Ly$\\alpha$ forest skewer. We find that our network can successfully reproduce IGM conditions with high fidelity across range of instrumental signal-to-noise. These predictions are subsequently translated into the temperature-density plane, facilitating the derivation of reliable constraints on thermal parameters. This allows us to estimate temperature at mean cosmic density, $T_{\\rm 0}$ with one sigma confidence $\\delta T_{\\rm 0} \\sim 1000{\\rm K}$ using only one $20$Mpc/h sightline ($\\Delta z\\simeq 0.04$) with a typical reionization history. Existing studies utilize redshift pathlength comparable to $\\Delta z\\simeq 4$ for similar constraints. We can also provide more stringent constraints on the slope ($1\\sigma$ confidence interval $\\delta {\\rm \\gamma} \\lesssim 0.1$) of the IGM temperature-density relation as compared to other traditional approaches. We test the reconstruction on a single high signal-to-noise observed spectrum ($20$ Mpc/h segment), and recover thermal parameters consistent with current measurements. This machine learning approach has the potential to provide accurate yet robust measurements of IGM thermal history at the redshifts in question.","sentences":["Unveiling the thermal history of the intergalactic medium (IGM) at $4 \\leq z \\leq 5$ holds the potential to reveal early onset HeII reionization or lingering thermal fluctuations from HI reionization.","We set out to reconstruct the IGM gas properties along simulated Lyman-alpha forest data on pixel-by-pixel basis, employing deep Bayesian neural networks.","Our approach leverages the Sherwood-Relics simulation suite, consisting of diverse thermal histories, to generate mock spectra.","Our convolutional and residual networks with likelihood metric predicts the Ly$\\alpha$ optical depth-weighted density or temperature for each pixel in the Ly$\\alpha$ forest skewer.","We find that our network can successfully reproduce IGM conditions with high fidelity across range of instrumental signal-to-noise.","These predictions are subsequently translated into the temperature-density plane, facilitating the derivation of reliable constraints on thermal parameters.","This allows us to estimate temperature at mean cosmic density, $T_{\\rm 0}$ with one sigma confidence $\\delta T_{\\rm 0} \\sim 1000{\\rm K}$ using only one $20$Mpc/h sightline ($\\Delta z\\simeq 0.04$) with a typical reionization history.","Existing studies utilize redshift pathlength comparable to $\\Delta z\\simeq 4$ for similar constraints.","We can also provide more stringent constraints on the slope ($1\\sigma$ confidence interval $\\delta {\\rm \\gamma} \\lesssim 0.1$) of the IGM temperature-density relation as compared to other traditional approaches.","We test the reconstruction on a single high signal-to-noise observed spectrum ($20$ Mpc/h segment), and recover thermal parameters consistent with current measurements.","This machine learning approach has the potential to provide accurate yet robust measurements of IGM thermal history at the redshifts in question."],"url":"http://arxiv.org/abs/2404.05794v1","category":"astro-ph.CO"}
{"created":"2024-04-08 17:59:44","title":"A Large-Scale Exploration of $\u03bc$-Transfer","abstract":"Large neural network models have become a mainstay of natural language processing and computer vision, yet their initialization and learning rates are set in a largely heuristic fashion, potentially varying from paper to paper and one model size to the next. The $\\mu$-Parameterization ($\\mu$P) offers a potential solution to these challenges, yielding scaling rules for model initialization and learning rates, and reportedly enabling zero-shot hyperparameter transfer from small to large models in a variety of cases.   Despite the evident promise, the $\\mu$P scaling rules are not yet widely adopted, perhaps due to higher implementation complexity, many variations, or complex theoretical background. This work investigates $\\mu$P empirically, focusing on the ubiquitous transformer architecture, and aims to answer a simple question: does $\\mu$-Transfer yield optimal learning rates in practice? From models with 2M to 10B parameters, we show that $\\mu$-Transfer works as intended for the majority of important cases, but also identify some surprising cases where it may not.","sentences":["Large neural network models have become a mainstay of natural language processing and computer vision, yet their initialization and learning rates are set in a largely heuristic fashion, potentially varying from paper to paper and one model size to the next.","The $\\mu$-Parameterization ($\\mu$P) offers a potential solution to these challenges, yielding scaling rules for model initialization and learning rates, and reportedly enabling zero-shot hyperparameter transfer from small to large models in a variety of cases.   ","Despite the evident promise, the $\\mu$P scaling rules are not yet widely adopted, perhaps due to higher implementation complexity, many variations, or complex theoretical background.","This work investigates $\\mu$P empirically, focusing on the ubiquitous transformer architecture, and aims to answer a simple question: does $\\mu$-Transfer yield optimal learning rates in practice?","From models with 2M to 10B parameters, we show that $\\mu$-Transfer works as intended for the majority of important cases, but also identify some surprising cases where it may not."],"url":"http://arxiv.org/abs/2404.05728v1","category":"cs.LG"}
{"created":"2024-04-08 17:50:24","title":"A de Sitter S-matrix from amputated cosmological correlators","abstract":"Extending scattering to states with unphysical mass values (particles ``off their mass shell'') has been instrumental in developing modern amplitude technology for Minkowski spacetime. Here, we study the off-shell correlators which underpin the recently proposed S-matrix for scattering on de Sitter spacetime. By labelling each particle with both a spatial momentum and an independent ``energy'' variable (the de Sitter analogue of a 4-momentum), we find that the practical computation of these correlators is greatly simplified. This allows us to derive compact expressions for all 3- and 4-particle S-matrices at tree-level for scalar fields coupled through any derivative interactions. As on Minkowski, we find that the 3-particle and exchange part of the 4-particle S-matrices are unique (up to crossing). The remaining contact part of the 4-particle S-matrix is an analytic function of just two differential operators, which become the usual Mandelstam variables in the Minkowski limit. Finally, we introduce a spectral decomposition for the tree-level exchange of a heavy field responsible for a cosmological collider signal. Once projected onto physical mass eigenstates, these S-matrix elements encode the statistical properties of the early inflationary perturbations.","sentences":["Extending scattering to states with unphysical mass values (particles ``off their mass shell'') has been instrumental in developing modern amplitude technology for Minkowski spacetime.","Here, we study the off-shell correlators which underpin the recently proposed S-matrix for scattering on de Sitter spacetime.","By labelling each particle with both a spatial momentum and an independent ``energy'' variable (the de Sitter analogue of a 4-momentum), we find that the practical computation of these correlators is greatly simplified.","This allows us to derive compact expressions for all 3- and 4-particle S-matrices at tree-level for scalar fields coupled through any derivative interactions.","As on Minkowski, we find that the 3-particle and exchange part of the 4-particle S-matrices are unique (up to crossing).","The remaining contact part of the 4-particle S-matrix is an analytic function of just two differential operators, which become the usual Mandelstam variables in the Minkowski limit.","Finally, we introduce a spectral decomposition for the tree-level exchange of a heavy field responsible for a cosmological collider signal.","Once projected onto physical mass eigenstates, these S-matrix elements encode the statistical properties of the early inflationary perturbations."],"url":"http://arxiv.org/abs/2404.05712v1","category":"hep-th"}
{"created":"2024-04-08 17:37:22","title":"Case Study: Neural Network Malware Detection Verification for Feature and Image Datasets","abstract":"Malware, or software designed with harmful intent, is an ever-evolving threat that can have drastic effects on both individuals and institutions. Neural network malware classification systems are key tools for combating these threats but are vulnerable to adversarial machine learning attacks. These attacks perturb input data to cause misclassification, bypassing protective systems. Existing defenses often rely on enhancing the training process, thereby increasing the model's robustness to these perturbations, which is quantified using verification. While training improvements are necessary, we propose focusing on the verification process used to evaluate improvements to training. As such, we present a case study that evaluates a novel verification domain that will help to ensure tangible safeguards against adversaries and provide a more reliable means of evaluating the robustness and effectiveness of anti-malware systems. To do so, we describe malware classification and two types of common malware datasets (feature and image datasets), demonstrate the certified robustness accuracy of malware classifiers using the Neural Network Verification (NNV) and Neural Network Enumeration (nnenum) tools, and outline the challenges and future considerations necessary for the improvement and refinement of the verification of malware classification. By evaluating this novel domain as a case study, we hope to increase its visibility, encourage further research and scrutiny, and ultimately enhance the resilience of digital systems against malicious attacks.","sentences":["Malware, or software designed with harmful intent, is an ever-evolving threat that can have drastic effects on both individuals and institutions.","Neural network malware classification systems are key tools for combating these threats but are vulnerable to adversarial machine learning attacks.","These attacks perturb input data to cause misclassification, bypassing protective systems.","Existing defenses often rely on enhancing the training process, thereby increasing the model's robustness to these perturbations, which is quantified using verification.","While training improvements are necessary, we propose focusing on the verification process used to evaluate improvements to training.","As such, we present a case study that evaluates a novel verification domain that will help to ensure tangible safeguards against adversaries and provide a more reliable means of evaluating the robustness and effectiveness of anti-malware systems.","To do so, we describe malware classification and two types of common malware datasets (feature and image datasets), demonstrate the certified robustness accuracy of malware classifiers using the Neural Network Verification (NNV) and Neural Network Enumeration (nnenum) tools, and outline the challenges and future considerations necessary for the improvement and refinement of the verification of malware classification.","By evaluating this novel domain as a case study, we hope to increase its visibility, encourage further research and scrutiny, and ultimately enhance the resilience of digital systems against malicious attacks."],"url":"http://arxiv.org/abs/2404.05703v1","category":"cs.CR"}
{"created":"2024-04-08 17:33:56","title":"Negative Photo Conductivity Triggered with Visible Light in Wide Bandgap Oxide-Based Optoelectronic Crossbar Memristive Array for Photograph Sensing and Neuromorphic Computing Applications","abstract":"Photoresponsivity studies of wide-bandgap oxide-based devices have emerged as a vibrant and popular research area. Researchers have explored various material systems in their quest to develop devices capable of responding to illumination. In this study, we engineered a mature wide bandgap oxide-based bilayer heterostructure synaptic memristor to emulate the human brain for applications in neuromorphic computing and photograph sensing. The device exhibits advanced electric and electro-photonic synaptic functions, such as long-term potentiation (LTP), long-term depression (LTD), and paired pulse facilitation (PPF), by applying successive electric and photonic pulses. Moreover, the device exhibits exceptional electrical SET and photonic RESET endurance, maintaining its stability for a minimum of 1200 cycles without any degradation. Density functional theory calculations of the band structures provide insights into the conduction mechanism of the device. Based on this memristor array, we developed an autoencoder and convolutional neural network for noise reduction and image recognition tasks, which achieves a peak signal-to-noise ratio of 562 and high accuracy of 84.23%, while consuming lower energy by four orders of magnitude compared with the Tesla P40 GPU. This groundbreaking research not only opens doors for the integration of our device into image processing but also represents a significant advancement in the realm of in-memory computing and photograph sensing features in a single cell.","sentences":["Photoresponsivity studies of wide-bandgap oxide-based devices have emerged as a vibrant and popular research area.","Researchers have explored various material systems in their quest to develop devices capable of responding to illumination.","In this study, we engineered a mature wide bandgap oxide-based bilayer heterostructure synaptic memristor to emulate the human brain for applications in neuromorphic computing and photograph sensing.","The device exhibits advanced electric and electro-photonic synaptic functions, such as long-term potentiation (LTP), long-term depression (LTD), and paired pulse facilitation (PPF), by applying successive electric and photonic pulses.","Moreover, the device exhibits exceptional electrical SET and photonic RESET endurance, maintaining its stability for a minimum of 1200 cycles without any degradation.","Density functional theory calculations of the band structures provide insights into the conduction mechanism of the device.","Based on this memristor array, we developed an autoencoder and convolutional neural network for noise reduction and image recognition tasks, which achieves a peak signal-to-noise ratio of 562 and high accuracy of 84.23%, while consuming lower energy by four orders of magnitude compared with the Tesla P40 GPU.","This groundbreaking research not only opens doors for the integration of our device into image processing but also represents a significant advancement in the realm of in-memory computing and photograph sensing features in a single cell."],"url":"http://arxiv.org/abs/2404.05701v1","category":"physics.app-ph"}
{"created":"2024-04-08 16:41:11","title":"Error estimates for the discretization of bilinear control problems governed by semilinear elliptic PDEs","abstract":"This paper studies an optimal control problem governed by a semilinear elliptic equation, in which the control acts in a multiplicative or bilinear way as the reaction coefficient of the equation. We focus on the numerical discretization of the problem. The discretization is carried out by using the finite element method, with piecewise constant functions for the control and continuous piecewise linear functions for the state and the adjoint state. We first prove convergence of the solutions of the discrete problems to solutions of the continuous problem. We also demonstrate that strict local solutions of the continuous problem can be approximated by local solutions of the discrete problems. Next we obtain an error estimate of order $O(h)$ for the difference between continuous and discrete locally optimal controls. To obtain this result we assume no-gap second order sufficient optimality conditions. As it is usual in this kind of discretization, a superconvergence phenomenon of order $O(h^2)$ is observed in numerical experiments for the error estimates of the state and adjoint state. The last part of the paper is dedicated to explain this behaviour. A numerical experiment confirming these results is included.","sentences":["This paper studies an optimal control problem governed by a semilinear elliptic equation, in which the control acts in a multiplicative or bilinear way as the reaction coefficient of the equation.","We focus on the numerical discretization of the problem.","The discretization is carried out by using the finite element method, with piecewise constant functions for the control and continuous piecewise linear functions for the state and the adjoint state.","We first prove convergence of the solutions of the discrete problems to solutions of the continuous problem.","We also demonstrate that strict local solutions of the continuous problem can be approximated by local solutions of the discrete problems.","Next we obtain an error estimate of order $O(h)$ for the difference between continuous and discrete locally optimal controls.","To obtain this result we assume no-gap second order sufficient optimality conditions.","As it is usual in this kind of discretization, a superconvergence phenomenon of order $O(h^2)$ is observed in numerical experiments for the error estimates of the state and adjoint state.","The last part of the paper is dedicated to explain this behaviour.","A numerical experiment confirming these results is included."],"url":"http://arxiv.org/abs/2404.05658v1","category":"math.OC"}
{"created":"2024-04-08 16:38:50","title":"Convergence rates for the finite volume scheme of the stochastic heat equation","abstract":"In this contribution, we provide convergence rates for the finite volume scheme of the stochastic heat equation with multiplicative Lipschitz noise and homogeneous Neumann boundary conditions (SHE). More precisely, we give an error estimate for the $L^2$-norm of the space-time discretization of SHE by a semi-implicit Euler scheme with respect to time and a TPFA scheme with respect to space and the variational solution of SHE. The only regularity assumptions additionally needed is spatial regularity of the initial datum and smoothness of the diffusive term.","sentences":["In this contribution, we provide convergence rates for the finite volume scheme of the stochastic heat equation with multiplicative Lipschitz noise and homogeneous Neumann boundary conditions (SHE).","More precisely, we give an error estimate for the $L^2$-norm of the space-time discretization of SHE by a semi-implicit Euler scheme with respect to time and a TPFA scheme with respect to space and the variational solution of SHE.","The only regularity assumptions additionally needed is spatial regularity of the initial datum and smoothness of the diffusive term."],"url":"http://arxiv.org/abs/2404.05655v1","category":"math.NA"}
{"created":"2024-04-08 16:25:45","title":"Existence and uniqueness of a saddle-node bifurcation point for nonlinear equations in general domains","abstract":"This paper provides a direct method of establishing the existence and uniqueness of saddle-node bifurcations for nonlinear equations in general domains. The method employs the scaled extended quotient whose saddle points correspond to the saddle-node bifurcations. The uniqueness of the saddle-node bifurcation point directly stems from the uniqueness of the saddle point. The method is applied to solving open problems involving the existence and uniqueness of the maximum saddle-node bifurcation for the positive solutions curve to an elliptic boundary value problem with a convex-concave nonlinearity in general domains.","sentences":["This paper provides a direct method of establishing the existence and uniqueness of saddle-node bifurcations for nonlinear equations in general domains.","The method employs the scaled extended quotient whose saddle points correspond to the saddle-node bifurcations.","The uniqueness of the saddle-node bifurcation point directly stems from the uniqueness of the saddle point.","The method is applied to solving open problems involving the existence and uniqueness of the maximum saddle-node bifurcation for the positive solutions curve to an elliptic boundary value problem with a convex-concave nonlinearity in general domains."],"url":"http://arxiv.org/abs/2404.05643v1","category":"math.AP"}
{"created":"2024-04-08 16:16:58","title":"First-order phase transitions in the cores of neutron stars","abstract":"I explore various scenarios for the phase transition within neutron-star matter. I do so by generating large model-agnostic ensemble using Gaussian Processes, both with and without explicit inclusion of first-order phase transitions (PTs). The ensemble is conditioned with state-of-the-art astrophysical and theoretical inputs in a fully Bayesian approach. I study how the current data affect the posterior probability of the location and the strength of the first-order PT. I find that peak structure of the sound speed is stable against inclusion of PTs. Furthermore, while the current data cannot differentiate between a crossover and a first-order PT, it suggests an exceedingly low probability of the absence of either within the stable branch of neutron stars.","sentences":["I explore various scenarios for the phase transition within neutron-star matter.","I do so by generating large model-agnostic ensemble using Gaussian Processes, both with and without explicit inclusion of first-order phase transitions (PTs).","The ensemble is conditioned with state-of-the-art astrophysical and theoretical inputs in a fully Bayesian approach.","I study how the current data affect the posterior probability of the location and the strength of the first-order PT.","I find that peak structure of the sound speed is stable against inclusion of PTs.","Furthermore, while the current data cannot differentiate between a crossover and a first-order PT, it suggests an exceedingly low probability of the absence of either within the stable branch of neutron stars."],"url":"http://arxiv.org/abs/2404.05637v1","category":"nucl-th"}
{"created":"2024-04-08 15:43:56","title":"Quantum tomography of structured light patterns from simple intensity measurements","abstract":"We study the tomography of spatial qudits encoded on structured light photons. While direct position measurements with cameras do not provide an informationally complete Positive Operator Valued Measure (POVM) in the space of fixed order modes, we complement this POVM with an astigmatic transformation. The enlarged POVM is informationally complete, allowing full characterization of the spatial quantum state from simple intensity measurements in both the intense and in the low photocount regimes. For intense light, the standard technique of linear inversion is used. For the low photocount regime, we employ Bayesian mean inference, and study how the quality of the tomographic reconstruction behaves as we increase the photocounts. In both cases, we also perform the tomography using a convolutional neural network, which displays an increased flexibility in exchange for a slightly lower quality reconstruction in some of the cases. These methods will be useful for classical and quantum communication with structured light.","sentences":["We study the tomography of spatial qudits encoded on structured light photons.","While direct position measurements with cameras do not provide an informationally complete Positive Operator Valued Measure (POVM) in the space of fixed order modes, we complement this POVM with an astigmatic transformation.","The enlarged POVM is informationally complete, allowing full characterization of the spatial quantum state from simple intensity measurements in both the intense and in the low photocount regimes.","For intense light, the standard technique of linear inversion is used.","For the low photocount regime, we employ Bayesian mean inference, and study how the quality of the tomographic reconstruction behaves as we increase the photocounts.","In both cases, we also perform the tomography using a convolutional neural network, which displays an increased flexibility in exchange for a slightly lower quality reconstruction in some of the cases.","These methods will be useful for classical and quantum communication with structured light."],"url":"http://arxiv.org/abs/2404.05616v1","category":"quant-ph"}
{"created":"2024-04-08 15:43:29","title":"Tensor neural networks for high-dimensional Fokker-Planck equations","abstract":"We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks. The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions. The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions. We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks. One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation. To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy. We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions.","sentences":["We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks.","The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions.","The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions.","We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks.","One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation.","To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy.","We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions."],"url":"http://arxiv.org/abs/2404.05615v1","category":"math.NA"}
{"created":"2024-04-08 15:35:58","title":"On global solutions of heat equations with time-dependent nonlinearities on unimodular Lie groups","abstract":"In this work, we study the global well-posedeness of the heat equation with variable time-dependent nonlinearity of the form $\\varphi(t)f(u)$ on unimodular Lie groups when the differential operator arises as the sum of squares of H\\\"ormander vector fields. For general unimodular Lie groups, we derive the necessary conditions for the nonexistence of global positive solutions. This gives different conditions in the cases of compact, polynomial, and exponential volume growth groups. In the case of the Heisenberg groups $\\mathbb{H}^{n}$, we also derive sufficient conditions, which coincide with the necessary ones in the case of $\\mathbb{H}^{1}$ (and this is also true for $\\mathbb{R}^{n}$). In particular, in the case of the Heisenberg group $\\mathbb{H}^{1}$ we obtain the necessary and sufficient conditions under which the aforesaid initial value problem with variable nonlinearity has a global positive solution.","sentences":["In this work, we study the global well-posedeness of the heat equation with variable time-dependent nonlinearity of the form $\\varphi(t)f(u)$ on unimodular Lie groups when the differential operator arises as the sum of squares of H\\\"ormander vector fields.","For general unimodular Lie groups, we derive the necessary conditions for the nonexistence of global positive solutions.","This gives different conditions in the cases of compact, polynomial, and exponential volume growth groups.","In the case of the Heisenberg groups $\\mathbb{H}^{n}$, we also derive sufficient conditions, which coincide with the necessary ones in the case of $\\mathbb{H}^{1}$ (and this is also true for $\\mathbb{R}^{n}$).","In particular, in the case of the Heisenberg group $\\mathbb{H}^{1}$ we obtain the necessary and sufficient conditions under which the aforesaid initial value problem with variable nonlinearity has a global positive solution."],"url":"http://arxiv.org/abs/2404.05611v1","category":"math.AP"}
{"created":"2024-04-08 15:25:50","title":"Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction","abstract":"Face meshes in consistent topology serve as the foundation for many face-related applications, such as 3DMM constrained face reconstruction and expression retargeting. Traditional methods commonly acquire topology uniformed face meshes by two separate steps: multi-view stereo (MVS) to reconstruct shapes followed by non-rigid registration to align topology, but struggles with handling noise and non-lambertian surfaces. Recently neural volume rendering techniques have been rapidly evolved and shown great advantages in 3D reconstruction or novel view synthesis. Our goal is to leverage the superiority of neural volume rendering into multi-view reconstruction of face mesh with consistent topology. We propose a mesh volume rendering method that enables directly optimizing mesh geometry while preserving topology, and learning implicit features to model complex facial appearance from multi-view images. The key innovation lies in spreading sparse mesh features into the surrounding space to simulate radiance field required for volume rendering, which facilitates backpropagation of gradients from images to mesh geometry and implicit appearance features. Our proposed feature spreading module exhibits deformation invariance, enabling photorealistic rendering seamlessly after mesh editing. We conduct experiments on multi-view face image dataset to evaluate the reconstruction and implement an application for photorealistic rendering of animated face mesh.","sentences":["Face meshes in consistent topology serve as the foundation for many face-related applications, such as 3DMM constrained face reconstruction and expression retargeting.","Traditional methods commonly acquire topology uniformed face meshes by two separate steps: multi-view stereo (MVS) to reconstruct shapes followed by non-rigid registration to align topology, but struggles with handling noise and non-lambertian surfaces.","Recently neural volume rendering techniques have been rapidly evolved and shown great advantages in 3D reconstruction or novel view synthesis.","Our goal is to leverage the superiority of neural volume rendering into multi-view reconstruction of face mesh with consistent topology.","We propose a mesh volume rendering method that enables directly optimizing mesh geometry while preserving topology, and learning implicit features to model complex facial appearance from multi-view images.","The key innovation lies in spreading sparse mesh features into the surrounding space to simulate radiance field required for volume rendering, which facilitates backpropagation of gradients from images to mesh geometry and implicit appearance features.","Our proposed feature spreading module exhibits deformation invariance, enabling photorealistic rendering seamlessly after mesh editing.","We conduct experiments on multi-view face image dataset to evaluate the reconstruction and implement an application for photorealistic rendering of animated face mesh."],"url":"http://arxiv.org/abs/2404.05606v1","category":"cs.CV"}
{"created":"2024-04-08 15:24:20","title":"Technical Report: The Graph Spectral Token -- Enhancing Graph Transformers with Spectral Information","abstract":"Graph Transformers have emerged as a powerful alternative to Message-Passing Graph Neural Networks (MP-GNNs) to address limitations such as over-squashing of information exchange. However, incorporating graph inductive bias into transformer architectures remains a significant challenge. In this report, we propose the Graph Spectral Token, a novel approach to directly encode graph spectral information, which captures the global structure of the graph, into the transformer architecture. By parameterizing the auxiliary [CLS] token and leaving other tokens representing graph nodes, our method seamlessly integrates spectral information into the learning process. We benchmark the effectiveness of our approach by enhancing two existing graph transformers, GraphTrans and SubFormer. The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10% improvements on large graph benchmark datasets while maintaining efficiency comparable to MP-GNNs. SubFormer-Spec demonstrates strong performance across various datasets.","sentences":["Graph Transformers have emerged as a powerful alternative to Message-Passing Graph Neural Networks (MP-GNNs) to address limitations such as over-squashing of information exchange.","However, incorporating graph inductive bias into transformer architectures remains a significant challenge.","In this report, we propose the Graph Spectral Token, a novel approach to directly encode graph spectral information, which captures the global structure of the graph, into the transformer architecture.","By parameterizing the auxiliary [CLS] token and leaving other tokens representing graph nodes, our method seamlessly integrates spectral information into the learning process.","We benchmark the effectiveness of our approach by enhancing two existing graph transformers, GraphTrans and SubFormer.","The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10% improvements on large graph benchmark datasets while maintaining efficiency comparable to MP-GNNs.","SubFormer-Spec demonstrates strong performance across various datasets."],"url":"http://arxiv.org/abs/2404.05604v1","category":"cs.LG"}
{"created":"2024-04-08 15:21:17","title":"SpeechAlign: Aligning Speech Generation to Human Preferences","abstract":"Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.","sentences":["Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out.","However, the integration of human feedback to align speech outputs to human preferences is often neglected.","This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance.","Then we explore leveraging learning from human feedback to bridge the distribution gap.","We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences.","SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model.","This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones.","Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model.","Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models.","Code and models will be available at https://github.com/0nutation/SpeechGPT."],"url":"http://arxiv.org/abs/2404.05600v1","category":"cs.CL"}
{"created":"2024-04-08 15:17:37","title":"Little Rip and Pseudo Rip cosmological models with coupled dark energy based on a new generalized entropy","abstract":"We study Little Rip (LR) and Pseudo Rip (PR) cosmological models containing two coupled fluids: dark energy and dark matter. We assume a spatially flat Friedmann-Robertson-Walker (FRW) universe. The interaction between the dark energy and the dark matter fluid components is described in terms of the parameters in the generalized equation of state (EoS) in presence of the bulk viscosity. We consider entropic cosmology and use a description based on a new generalized entropy function, which was proposed by Nojiri-Odintsov-Faraoni [1]. Conditions for the appearance of the (LR) and the (PR) in terms of the parameters of the (EoS) are obtained. Introducing an energy density $\\rho_g$ corresponding to a specified entropy function $S_g$, together with an interaction term $Q$ in the gravitational equations of motion, we derive modified forms of the EoS parameters. We discuss the corrections of the thermodynamic parameters associated with the generalized entropy function. Properties of the late universe as well as in the early universe in this formalism are pointed out.","sentences":["We study Little Rip (LR) and Pseudo Rip (PR) cosmological models containing two coupled fluids: dark energy and dark matter.","We assume a spatially flat Friedmann-Robertson-Walker (FRW) universe.","The interaction between the dark energy and the dark matter fluid components is described in terms of the parameters in the generalized equation of state (EoS) in presence of the bulk viscosity.","We consider entropic cosmology and use a description based on a new generalized entropy function, which was proposed by Nojiri-Odintsov-Faraoni [1].","Conditions for the appearance of the (LR) and the (PR) in terms of the parameters of the (EoS) are obtained.","Introducing an energy density $\\rho_g$ corresponding to a specified entropy function $S_g$, together with an interaction term $Q$ in the gravitational equations of motion, we derive modified forms of the EoS parameters.","We discuss the corrections of the thermodynamic parameters associated with the generalized entropy function.","Properties of the late universe as well as in the early universe in this formalism are pointed out."],"url":"http://arxiv.org/abs/2404.05597v1","category":"gr-qc"}
{"created":"2024-04-08 15:09:53","title":"Unruh-DeWitt Particle Detectors in Bouncing Cosmologies","abstract":"We study semi-classical particle production in non-singular bouncing cosmologies by employing the Unruh-DeWitt model of a particle detector propagating in this class of spacetimes. The scale factor for the bouncing cosmology is derived analytically and is inspired by the modified Friedmann equation employed in the loop quantum cosmology literature. We examine how the detector response varies with the free parameters in this model such as the equation of state during the contraction phase and the critical energy density during the bounce phase. We also investigate whether such a signature in the particle detector survives at late times.","sentences":["We study semi-classical particle production in non-singular bouncing cosmologies by employing the Unruh-DeWitt model of a particle detector propagating in this class of spacetimes.","The scale factor for the bouncing cosmology is derived analytically and is inspired by the modified Friedmann equation employed in the loop quantum cosmology literature.","We examine how the detector response varies with the free parameters in this model such as the equation of state during the contraction phase and the critical energy density during the bounce phase.","We also investigate whether such a signature in the particle detector survives at late times."],"url":"http://arxiv.org/abs/2404.05592v1","category":"gr-qc"}
{"created":"2024-04-08 15:04:51","title":"Variable-Pitch-Propeller Mechanism Design, and Development of Heliquad for Mid-flight Flipping and Fault-Tolerant-Control","abstract":"This paper presents the design of Variable-Pitch-Propeller mechanism and its application on a quadcopter called Heliquad to demonstrate its unique capabilities. The input-output relationship is estimated for a generic mechanism. Various singularities and actuator sizing requirements are also analyzed. The mechanism is manufactured, and the validated input-output relationship is implemented in the controller of Heliquad. Heliquad is controlled by a unified non-switching cascaded attitude-rate controller, followed by a unique Neural-Network-based reconfigurable control allocation to approximate nonlinear relationship between the control input and actuator command. The Heliquad prototype's mid-flight flip experiment validates the controller's tracking performance in upright as well as inverted conditions. The prototype is then flown in upright condition with only three of its working actuators. To the best of the authors' knowledge, the cambered airfoil propeller-equipped Heliquad prototype demonstrates full-attitude control, including yaw-rate, on three working actuators for the first time in the literature. Finally, the utility of this novel capability is demonstrated by safe recovery and precise landing post-mid-flight actuator failure crisis. Overall, the controller tracks the references well for all the experiments, and the output of the NN-based control allocation remains bounded throughout.","sentences":["This paper presents the design of Variable-Pitch-Propeller mechanism and its application on a quadcopter called Heliquad to demonstrate its unique capabilities.","The input-output relationship is estimated for a generic mechanism.","Various singularities and actuator sizing requirements are also analyzed.","The mechanism is manufactured, and the validated input-output relationship is implemented in the controller of Heliquad.","Heliquad is controlled by a unified non-switching cascaded attitude-rate controller, followed by a unique Neural-Network-based reconfigurable control allocation to approximate nonlinear relationship between the control input and actuator command.","The Heliquad prototype's mid-flight flip experiment validates the controller's tracking performance in upright as well as inverted conditions.","The prototype is then flown in upright condition with only three of its working actuators.","To the best of the authors' knowledge, the cambered airfoil propeller-equipped Heliquad prototype demonstrates full-attitude control, including yaw-rate, on three working actuators for the first time in the literature.","Finally, the utility of this novel capability is demonstrated by safe recovery and precise landing post-mid-flight actuator failure crisis.","Overall, the controller tracks the references well for all the experiments, and the output of the NN-based control allocation remains bounded throughout."],"url":"http://arxiv.org/abs/2404.05591v1","category":"eess.SY"}
{"created":"2024-04-08 14:59:55","title":"Examples of Atoms Absorbing Photon via Schr\u00f6dinger Equation and Vacuum Fluctuations","abstract":"The absorption of photons by atoms encompasses fundamental quantum mechanical aspects, particularly the emergence of randomness to account for the inherent unpredictability in absorption outcomes. We demonstrate that vacuum fluctuations can be the origin of this randomness. An illustrative example of this is the absorption of a single photon by two symmetrically arranged atoms. In the absence of a mechanism to introduce randomness, the Schr\\\"odinger equation alone governs the time evolution of the process until an entangled state of the two atoms emerges. This entangled state consists of two components: one in which the first atom is excited by the photon while the second remains in the ground state, and another in which the first atom remains in the ground state while the second is excited by the photon. These components form a superposition state characterized by an unbreakable symmetry in the absence of external influences. Consequently, the absorption process remains incomplete. When vacuum fluctuations come into play, they can induce fluctuations in the weights of these components, akin to Brownian motion. Over time, one component diminishes, thereby breaking the entanglement between the two atoms and allowing the photon absorption process to conclude. The remaining component ultimately determines which atom completes the photon absorption. Similar studies involving different numbers of atoms can be conducted. Vacuum fluctuations not only introduce randomness but also have the potential to give rise to the Born rule in this context. Furthermore, the Casimir effect, which is closely tied to vacuum fluctuations, presents a promising experimental avenue for validating this mechanism.","sentences":["The absorption of photons by atoms encompasses fundamental quantum mechanical aspects, particularly the emergence of randomness to account for the inherent unpredictability in absorption outcomes.","We demonstrate that vacuum fluctuations can be the origin of this randomness.","An illustrative example of this is the absorption of a single photon by two symmetrically arranged atoms.","In the absence of a mechanism to introduce randomness, the Schr\\\"odinger equation alone governs the time evolution of the process until an entangled state of the two atoms emerges.","This entangled state consists of two components: one in which the first atom is excited by the photon while the second remains in the ground state, and another in which the first atom remains in the ground state while the second is excited by the photon.","These components form a superposition state characterized by an unbreakable symmetry in the absence of external influences.","Consequently, the absorption process remains incomplete.","When vacuum fluctuations come into play, they can induce fluctuations in the weights of these components, akin to Brownian motion.","Over time, one component diminishes, thereby breaking the entanglement between the two atoms and allowing the photon absorption process to conclude.","The remaining component ultimately determines which atom completes the photon absorption.","Similar studies involving different numbers of atoms can be conducted.","Vacuum fluctuations not only introduce randomness but also have the potential to give rise to the Born rule in this context.","Furthermore, the Casimir effect, which is closely tied to vacuum fluctuations, presents a promising experimental avenue for validating this mechanism."],"url":"http://arxiv.org/abs/2404.05585v1","category":"quant-ph"}
{"created":"2024-04-08 14:59:53","title":"Neural Cellular Automata for Lightweight, Robust and Explainable Classification of White Blood Cell Images","abstract":"Diagnosis of hematological malignancies depends on accurate identification of white blood cells in peripheral blood smears. Deep learning techniques are emerging as a viable solution to scale and optimize this process by automatic identification of cells in laboratories. However, these techniques face several challenges such as limited generalizability, sensitivity to domain shifts and lack of explainability. Here, we are introducing a novel approach based on neural cellular automata (NCA) for white blood cell classification. We test our approach on three datasets of white blood cell images and show that we achieve competitive performance compared to conventional methods. Our NCA-based method is significantly smaller in terms of parameters and exhibits robustness to domain shifts. Furthermore, the architecture is inherently explainable, providing insights into the decision process for each classification, helping experts understand and validate model predictions. Results demonstrate that NCA not only can be used for image classification, but also address key challenges of conventional methods, indicating a high potential for applicability in clinical practice.","sentences":["Diagnosis of hematological malignancies depends on accurate identification of white blood cells in peripheral blood smears.","Deep learning techniques are emerging as a viable solution to scale and optimize this process by automatic identification of cells in laboratories.","However, these techniques face several challenges such as limited generalizability, sensitivity to domain shifts and lack of explainability.","Here, we are introducing a novel approach based on neural cellular automata (NCA) for white blood cell classification.","We test our approach on three datasets of white blood cell images and show that we achieve competitive performance compared to conventional methods.","Our NCA-based method is significantly smaller in terms of parameters and exhibits robustness to domain shifts.","Furthermore, the architecture is inherently explainable, providing insights into the decision process for each classification, helping experts understand and validate model predictions.","Results demonstrate that NCA not only can be used for image classification, but also address key challenges of conventional methods, indicating a high potential for applicability in clinical practice."],"url":"http://arxiv.org/abs/2404.05584v1","category":"cs.CV"}
{"created":"2024-04-08 14:56:56","title":"Design and Simulation of Time-energy Optimal Anti-swing Trajectory Planner for Autonomous Tower Cranes","abstract":"For autonomous crane lifting, optimal trajectories of the crane are required as reference inputs to the crane controller to facilitate feedforward control. Reducing the unactuated payload motion is a crucial issue for under-actuated tower cranes with spherical pendulum dynamics. The planned trajectory should be optimal in terms of both operating time and energy consumption, to facilitate optimum output spending optimum effort. This article proposes an anti-swing tower crane trajectory planner that can provide time-energy optimal solutions for the Computer-Aided Lift Planning (CALP) system developed at Nanyang Technological University, which facilitates collision-free lifting path planning of robotized tower cranes in autonomous construction sites. The current work introduces a trajectory planning module to the system that utilizes the geometric outputs from the path planning module and optimally scales them with time information. Firstly, analyzing the non-linear dynamics of the crane operations, the tower crane is established as differentially flat. Subsequently, the multi-objective trajectory optimization problems for all the crane operations are formulated in the flat output space through consideration of the mechanical and safety constraints. Two multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II) and Generalized Differential Evolution 3 (GDE3), are extensively compared via statistical measures based on the closeness of solutions to the Pareto front, distribution of solutions in the solution space and the runtime, to select the optimization engine of the planner. Finally, the crane operation trajectories are obtained via the corresponding planned flat output trajectories. Studies simulating real-world lifting scenarios are conducted to verify the effectiveness and reliability of the proposed module of the lift planning system.","sentences":["For autonomous crane lifting, optimal trajectories of the crane are required as reference inputs to the crane controller to facilitate feedforward control.","Reducing the unactuated payload motion is a crucial issue for under-actuated tower cranes with spherical pendulum dynamics.","The planned trajectory should be optimal in terms of both operating time and energy consumption, to facilitate optimum output spending optimum effort.","This article proposes an anti-swing tower crane trajectory planner that can provide time-energy optimal solutions for the Computer-Aided Lift Planning (CALP) system developed at Nanyang Technological University, which facilitates collision-free lifting path planning of robotized tower cranes in autonomous construction sites.","The current work introduces a trajectory planning module to the system that utilizes the geometric outputs from the path planning module and optimally scales them with time information.","Firstly, analyzing the non-linear dynamics of the crane operations, the tower crane is established as differentially flat.","Subsequently, the multi-objective trajectory optimization problems for all the crane operations are formulated in the flat output space through consideration of the mechanical and safety constraints.","Two multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II) and Generalized Differential Evolution 3 (GDE3), are extensively compared via statistical measures based on the closeness of solutions to the Pareto front, distribution of solutions in the solution space and the runtime, to select the optimization engine of the planner.","Finally, the crane operation trajectories are obtained via the corresponding planned flat output trajectories.","Studies simulating real-world lifting scenarios are conducted to verify the effectiveness and reliability of the proposed module of the lift planning system."],"url":"http://arxiv.org/abs/2404.05581v1","category":"cs.RO"}
{"created":"2024-04-08 14:55:35","title":"Robust Data Pruning: Uncovering and Overcoming Implicit Bias","abstract":"In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. At the same time, we argue that random data pruning with appropriate class ratios has potential to improve the worst-class performance. We propose a \"fairness-aware\" approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving robustness at a tolerable drop of average performance as we prune more from the datasets. We present theoretical analysis of the classification risk in a mixture of Gaussians to further motivate our algorithm and support our findings.","sentences":["In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning.","Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws.","However, little is known about its impact on classification bias of the trained models.","We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers.","At the same time, we argue that random data pruning with appropriate class ratios has potential to improve the worst-class performance.","We propose a \"fairness-aware\" approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks.","In sharp contrast to existing algorithms, our proposed method continues improving robustness at a tolerable drop of average performance as we prune more from the datasets.","We present theoretical analysis of the classification risk in a mixture of Gaussians to further motivate our algorithm and support our findings."],"url":"http://arxiv.org/abs/2404.05579v1","category":"cs.LG"}
{"created":"2024-04-08 14:28:41","title":"Bathymetry reconstruction from experimental data using PDE-constrained optimisation","abstract":"Knowledge of the bottom topography, also called bathymetry, of rivers, seas or the ocean is important for many areas of maritime science and civil engineering. While direct measurements are possible, they are time consuming and expensive. Therefore, many approaches have been proposed how to infer the bathymetry from measurements of surface waves. Mathematically, this is an inverse problem where an unknown system state needs to be reconstructed from observations with a suitable model for the flow as constraint. In many cases, the shallow water equations can be used to describe the flow. While theoretical studies of the efficacy of such a PDE-constrained optimisation approach for bathymetry reconstruction exist, there seem to be few publications that study its application to data obtained from real-world measurements. This paper shows that the approach can, at least qualitatively, reconstruct a Gaussian-shaped bathymetry in a wave flume from measurements of the water height at up to three points. Achieved normalized root mean square errors (NRMSE) are in line with other approaches.","sentences":["Knowledge of the bottom topography, also called bathymetry, of rivers, seas or the ocean is important for many areas of maritime science and civil engineering.","While direct measurements are possible, they are time consuming and expensive.","Therefore, many approaches have been proposed how to infer the bathymetry from measurements of surface waves.","Mathematically, this is an inverse problem where an unknown system state needs to be reconstructed from observations with a suitable model for the flow as constraint.","In many cases, the shallow water equations can be used to describe the flow.","While theoretical studies of the efficacy of such a PDE-constrained optimisation approach for bathymetry reconstruction exist, there seem to be few publications that study its application to data obtained from real-world measurements.","This paper shows that the approach can, at least qualitatively, reconstruct a Gaussian-shaped bathymetry in a wave flume from measurements of the water height at up to three points.","Achieved normalized root mean square errors (NRMSE) are in line with other approaches."],"url":"http://arxiv.org/abs/2404.05556v1","category":"math.NA"}
{"created":"2024-04-08 14:21:28","title":"Partial balayage for the Helmholtz equation","abstract":"Kow, Larson, Salo and Shahgholian recently initiated the study of quadrature domains for the Helmholtz equation and developed an associated theory of partial balayage of measures. The present paper offers an alternative approach to partial balayage in this context that yields stronger results. Applications are given to quadrature domains and to a domain evolution question that is analogous to Hele-Shaw flow.","sentences":["Kow, Larson, Salo and Shahgholian recently initiated the study of quadrature domains for the Helmholtz equation and developed an associated theory of partial balayage of measures.","The present paper offers an alternative approach to partial balayage in this context that yields stronger results.","Applications are given to quadrature domains and to a domain evolution question that is analogous to Hele-Shaw flow."],"url":"http://arxiv.org/abs/2404.05552v1","category":"math.AP"}
{"created":"2024-04-09 17:14:41","title":"Hyperparameter Selection in Continual Learning","abstract":"In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has prompted the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.","sentences":["In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time.","This has prompted the development of CL-specific HPO frameworks.","The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings.","However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once.","Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality?","This paper answers this question by evaluating several realistic HPO frameworks.","We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly.","We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training."],"url":"http://arxiv.org/abs/2404.06466v1","category":"cs.LG"}
{"created":"2024-04-09 14:22:50","title":"Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences","abstract":"Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches.","sentences":["Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences.","Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale.","Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale.","We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space.","By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements.","Depth measurements are also not required for training, nor are scene reconstructions or image overlap information.","MicKey is supervised only by pairs of images and their relative poses.","MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches."],"url":"http://arxiv.org/abs/2404.06337v1","category":"cs.CV"}
{"created":"2024-04-09 12:44:34","title":"Spatial-Temporal Multi-level Association for Video Object Segmentation","abstract":"Existing semi-supervised video object segmentation methods either focus on temporal feature matching or spatial-temporal feature modeling. However, they do not address the issues of sufficient target interaction and efficient parallel processing simultaneously, thereby constraining the learning of dynamic, target-aware features. To tackle these limitations, this paper proposes a spatial-temporal multi-level association framework, which jointly associates reference frame, test frame, and object features to achieve sufficient interaction and parallel target ID association with a spatial-temporal memory bank for efficient video object segmentation. Specifically, we construct a spatial-temporal multi-level feature association module to learn better target-aware features, which formulates feature extraction and interaction as the efficient operations of object self-attention, reference object enhancement, and test reference correlation. In addition, we propose a spatial-temporal memory to assist feature association and temporal ID assignment and correlation. We evaluate the proposed method by conducting extensive experiments on numerous video object segmentation datasets, including DAVIS 2016/2017 val, DAVIS 2017 test-dev, and YouTube-VOS 2018/2019 val. The favorable performance against the state-of-the-art methods demonstrates the effectiveness of our approach. All source code and trained models will be made publicly available.","sentences":["Existing semi-supervised video object segmentation methods either focus on temporal feature matching or spatial-temporal feature modeling.","However, they do not address the issues of sufficient target interaction and efficient parallel processing simultaneously, thereby constraining the learning of dynamic, target-aware features.","To tackle these limitations, this paper proposes a spatial-temporal multi-level association framework, which jointly associates reference frame, test frame, and object features to achieve sufficient interaction and parallel target ID association with a spatial-temporal memory bank for efficient video object segmentation.","Specifically, we construct a spatial-temporal multi-level feature association module to learn better target-aware features, which formulates feature extraction and interaction as the efficient operations of object self-attention, reference object enhancement, and test reference correlation.","In addition, we propose a spatial-temporal memory to assist feature association and temporal ID assignment and correlation.","We evaluate the proposed method by conducting extensive experiments on numerous video object segmentation datasets, including DAVIS 2016/2017 val, DAVIS 2017 test-dev, and YouTube-VOS 2018/2019 val.","The favorable performance against the state-of-the-art methods demonstrates the effectiveness of our approach.","All source code and trained models will be made publicly available."],"url":"http://arxiv.org/abs/2404.06265v1","category":"cs.CV"}
{"created":"2024-04-09 10:21:32","title":"MLatom software ecosystem for surface hopping dynamics in Python with quantum mechanical and machine learning methods","abstract":"We present an open-source MLatom@XACS software ecosystem for on-the-fly surface hopping nonadiabatic dynamics based on the Landau-Zener-Belyaev-Lebedev (LZBL) algorithm. The dynamics can be performed via Python API with a wide range of quantum mechanical (QM) and machine learning (ML) methods, including ab initio QM (CASSCF and ADC(2)), semi-empirical QM methods (e.g., AM1, PM3, OMx, and ODMx), and many types of machine learning potentials (e.g., KREG, ANI, and MACE). Combinations of QM and ML methods can also be used. While the user can build their own combinations, we provide AIQM1, which is based on {\\Delta}-learning and can be used out of the box. We showcase how AIQM1 reproduces the isomerization quantum yield of trans-azobenzene at a low cost. We provide example scripts that, in a dozen lines, enable the user to obtain the final population plots by simply providing the initial geometry of a molecule. Thus, those scripts perform geometry optimization, normal mode calculations, initial condition sampling, parallel trajectories propagation, population analysis, and final result plotting. Given the capabilities of MLatom to be used for training different ML models, this ecosystem can be seamlessly integrated into the protocols building ML models for nonadiabatic dynamics. In the future, a deeper and more efficient integration of MLatom with Newton-X will enable vast range of functionalities for surface hopping dynamics, such as fewest-switches surface hopping, to facilitate similar workflows via the Python API.","sentences":["We present an open-source MLatom@XACS software ecosystem for on-the-fly surface hopping nonadiabatic dynamics based on the Landau-Zener-Belyaev-Lebedev (LZBL) algorithm.","The dynamics can be performed via Python API with a wide range of quantum mechanical (QM) and machine learning (ML) methods, including ab initio QM (CASSCF and ADC(2)), semi-empirical QM methods (e.g., AM1, PM3, OMx, and ODMx), and many types of machine learning potentials (e.g., KREG, ANI, and MACE).","Combinations of QM and ML methods can also be used.","While the user can build their own combinations, we provide AIQM1, which is based on {\\Delta}-learning and can be used out of the box.","We showcase how AIQM1 reproduces the isomerization quantum yield of trans-azobenzene at a low cost.","We provide example scripts that, in a dozen lines, enable the user to obtain the final population plots by simply providing the initial geometry of a molecule.","Thus, those scripts perform geometry optimization, normal mode calculations, initial condition sampling, parallel trajectories propagation, population analysis, and final result plotting.","Given the capabilities of MLatom to be used for training different ML models, this ecosystem can be seamlessly integrated into the protocols building ML models for nonadiabatic dynamics.","In the future, a deeper and more efficient integration of MLatom with Newton-X will enable vast range of functionalities for surface hopping dynamics, such as fewest-switches surface hopping, to facilitate similar workflows via the Python API."],"url":"http://arxiv.org/abs/2404.06189v1","category":"physics.chem-ph"}
{"created":"2024-04-09 09:54:59","title":"A quantum information theoretic analysis of reinforcement learning-assisted quantum architecture search","abstract":"In the field of quantum computing, variational quantum algorithms (VQAs) represent a pivotal category of quantum solutions across a broad spectrum of applications. These algorithms demonstrate significant potential for realising quantum computational advantage. A fundamental aspect of VQAs involves formulating expressive and efficient quantum circuits (namely ansatz) and automating the search of such ansatz is known as quantum architecture search (QAS). RL-QAS involves optimising QAS using reinforcement learning techniques. This study investigates RL-QAS for crafting ansatzes tailored to the variational quantum state diagonalization problem. Our investigation includes a comprehensive analysis of various dimensions, such as the entanglement thresholds of the resultant states, the impact of initial conditions on the performance of RL-agent, the phase change behavior of correlation in concurrence bounds, and the discrete contributions of qubits in deducing eigenvalues through conditional entropy metrics. We leverage these insights to devise an optimal, admissible QAS to diagonalize random quantum states. Furthermore, the methodologies presented herein offer a generalised framework for constructing reward functions within RL-QAS applicable to variational quantum algorithms.","sentences":["In the field of quantum computing, variational quantum algorithms (VQAs) represent a pivotal category of quantum solutions across a broad spectrum of applications.","These algorithms demonstrate significant potential for realising quantum computational advantage.","A fundamental aspect of VQAs involves formulating expressive and efficient quantum circuits (namely ansatz) and automating the search of such ansatz is known as quantum architecture search (QAS).","RL-QAS involves optimising QAS using reinforcement learning techniques.","This study investigates RL-QAS for crafting ansatzes tailored to the variational quantum state diagonalization problem.","Our investigation includes a comprehensive analysis of various dimensions, such as the entanglement thresholds of the resultant states, the impact of initial conditions on the performance of RL-agent, the phase change behavior of correlation in concurrence bounds, and the discrete contributions of qubits in deducing eigenvalues through conditional entropy metrics.","We leverage these insights to devise an optimal, admissible QAS to diagonalize random quantum states.","Furthermore, the methodologies presented herein offer a generalised framework for constructing reward functions within RL-QAS applicable to variational quantum algorithms."],"url":"http://arxiv.org/abs/2404.06174v1","category":"quant-ph"}
{"created":"2024-04-09 07:27:58","title":"Image and Video Compression using Generative Sparse Representation with Fidelity Controls","abstract":"We propose a framework for learned image and video compression using the generative sparse visual representation (SVR) guided by fidelity-preserving controls. By embedding inputs into a discrete latent space spanned by learned visual codebooks, SVR-based compression transmits integer codeword indices, which is efficient and cross-platform robust. However, high-quality (HQ) reconstruction in the decoder relies on intermediate feature inputs from the encoder via direct connections. Due to the prohibitively high transmission costs, previous SVR-based compression methods remove such feature links, resulting in largely degraded reconstruction quality. In this work, we treat the intermediate features as fidelity-preserving control signals that guide the conditioned generative reconstruction in the decoder. Instead of discarding or directly transferring such signals, we draw them from a low-quality (LQ) fidelity-preserving alternative input that is sent to the decoder with very low bitrate. These control signals provide complementary fidelity cues to improve reconstruction, and their quality is determined by the compression rate of the LQ alternative, which can be tuned to trade off bitrate, fidelity and perceptual quality. Our framework can be conveniently used for both learned image compression (LIC) and learned video compression (LVC). Since SVR is robust against input perturbations, a large portion of codeword indices between adjacent frames can be the same. By only transferring different indices, SVR-based LIC and LVC can share a similar processing pipeline. Experiments over standard image and video compression benchmarks demonstrate the effectiveness of our approach.","sentences":["We propose a framework for learned image and video compression using the generative sparse visual representation (SVR) guided by fidelity-preserving controls.","By embedding inputs into a discrete latent space spanned by learned visual codebooks, SVR-based compression transmits integer codeword indices, which is efficient and cross-platform robust.","However, high-quality (HQ) reconstruction in the decoder relies on intermediate feature inputs from the encoder via direct connections.","Due to the prohibitively high transmission costs, previous SVR-based compression methods remove such feature links, resulting in largely degraded reconstruction quality.","In this work, we treat the intermediate features as fidelity-preserving control signals that guide the conditioned generative reconstruction in the decoder.","Instead of discarding or directly transferring such signals, we draw them from a low-quality (LQ) fidelity-preserving alternative input that is sent to the decoder with very low bitrate.","These control signals provide complementary fidelity cues to improve reconstruction, and their quality is determined by the compression rate of the LQ alternative, which can be tuned to trade off bitrate, fidelity and perceptual quality.","Our framework can be conveniently used for both learned image compression (LIC) and learned video compression (LVC).","Since SVR is robust against input perturbations, a large portion of codeword indices between adjacent frames can be the same.","By only transferring different indices, SVR-based LIC and LVC can share a similar processing pipeline.","Experiments over standard image and video compression benchmarks demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.06076v1","category":"eess.IV"}
{"created":"2024-04-09 06:30:54","title":"Shallow Quantum Circuit Implementation of Symmetric Functions with Limited Ancillary Qubits","abstract":"In quantum computation, optimizing depth and number of ancillary qubits in quantum circuits is crucial due to constraints imposed by current quantum devices. This paper presents an innovative approach to implementing arbitrary symmetric Boolean functions using poly-logarithmic depth quantum circuits with logarithmic number of ancillary qubits. Symmetric functions are those whose outputs rely solely on the Hamming weight of the inputs. These functions find applications across diverse domains, including quantum machine learning, arithmetic circuit synthesis, and quantum algorithm design (e.g., Grover's algorithm). Moreover, by fully leveraging the potential of qutrits (an additional energy level), the ancilla count can be further reduced to 1. The key technique involves a novel poly-logarithmic depth quantum circuit designed to compute Hamming weight without the need for ancillary qubits. The quantum circuit for Hamming weight is of independent interest because of its broad applications, such as quantum memory and quantum machine learning.","sentences":["In quantum computation, optimizing depth and number of ancillary qubits in quantum circuits is crucial due to constraints imposed by current quantum devices.","This paper presents an innovative approach to implementing arbitrary symmetric Boolean functions using poly-logarithmic depth quantum circuits with logarithmic number of ancillary qubits.","Symmetric functions are those whose outputs rely solely on the Hamming weight of the inputs.","These functions find applications across diverse domains, including quantum machine learning, arithmetic circuit synthesis, and quantum algorithm design (e.g., Grover's algorithm).","Moreover, by fully leveraging the potential of qutrits (an additional energy level), the ancilla count can be further reduced to 1.","The key technique involves a novel poly-logarithmic depth quantum circuit designed to compute Hamming weight without the need for ancillary qubits.","The quantum circuit for Hamming weight is of independent interest because of its broad applications, such as quantum memory and quantum machine learning."],"url":"http://arxiv.org/abs/2404.06052v1","category":"quant-ph"}
{"created":"2024-04-09 06:27:35","title":"Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes","abstract":"Dense scene reconstruction for photo-realistic view synthesis has various applications, such as VR/AR, autonomous vehicles. However, most existing methods have difficulties in large-scale scenes due to three core challenges: \\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get in real-world large-scale scenes. \\textit{(b) inaccurate pose estimation.} Most existing approaches rely on accurate pre-estimated camera poses. \\textit{(c) insufficient scene representation capability.} A single global radiance field lacks the capacity to effectively scale to large-scale scenes. To this end, we propose an incremental joint learning framework, which can achieve accurate depth, pose estimation, and large-scale scene reconstruction. A vision transformer-based network is adopted as the backbone to enhance performance in scale information estimation. For pose estimation, a feature-metric bundle adjustment (FBA) method is designed for accurate and robust camera tracking in large-scale scenes. In terms of implicit scene representation, we propose an incremental scene representation method to construct the entire large-scale scene as multiple local radiance fields to enhance the scalability of 3D scene representation. Extended experiments have been conducted to demonstrate the effectiveness and accuracy of our method in depth estimation, pose estimation, and large-scale scene reconstruction.","sentences":["Dense scene reconstruction for photo-realistic view synthesis has various applications, such as VR/AR, autonomous vehicles.","However, most existing methods have difficulties in large-scale scenes due to three core challenges: \\textit{(a) inaccurate depth input.}","Accurate depth input is impossible to get in real-world large-scale scenes.","\\textit{(b) inaccurate pose estimation.}","Most existing approaches rely on accurate pre-estimated camera poses.","\\textit{(c) insufficient scene representation capability.}","A single global radiance field lacks the capacity to effectively scale to large-scale scenes.","To this end, we propose an incremental joint learning framework, which can achieve accurate depth, pose estimation, and large-scale scene reconstruction.","A vision transformer-based network is adopted as the backbone to enhance performance in scale information estimation.","For pose estimation, a feature-metric bundle adjustment (FBA) method is designed for accurate and robust camera tracking in large-scale scenes.","In terms of implicit scene representation, we propose an incremental scene representation method to construct the entire large-scale scene as multiple local radiance fields to enhance the scalability of 3D scene representation.","Extended experiments have been conducted to demonstrate the effectiveness and accuracy of our method in depth estimation, pose estimation, and large-scale scene reconstruction."],"url":"http://arxiv.org/abs/2404.06050v1","category":"cs.CV"}
{"created":"2024-04-09 06:16:42","title":"A Systematic Literature Survey of Sparse Matrix-Vector Multiplication","abstract":"Sparse matrix-vector multiplication (SpMV) is a crucial computing kernel with widespread applications in iterative algorithms. Over the past decades, research on SpMV optimization has made remarkable strides, giving rise to various optimization contributions. However, the comprehensive and systematic literature survey that introduces, analyzes, discusses, and summarizes the advancements of SpMV in recent years is currently lacking. Aiming to fill this gap, this paper compares existing techniques and analyzes their strengths and weaknesses. We begin by highlighting two representative applications of SpMV, then conduct an in-depth overview of the important techniques that optimize SpMV on modern architectures, which we specifically classify as classic, auto-tuning, machine learning, and mixed-precision-based optimization. We also elaborate on the hardware-based architectures, including CPU, GPU, FPGA, processing in Memory, heterogeneous, and distributed platforms. We present a comprehensive experimental evaluation that compares the performance of state-of-the-art SpMV implementations. Based on our findings, we identify several challenges and point out future research directions. This survey is intended to provide researchers with a comprehensive understanding of SpMV optimization on modern architectures and provide guidance for future work.","sentences":["Sparse matrix-vector multiplication (SpMV) is a crucial computing kernel with widespread applications in iterative algorithms.","Over the past decades, research on SpMV optimization has made remarkable strides, giving rise to various optimization contributions.","However, the comprehensive and systematic literature survey that introduces, analyzes, discusses, and summarizes the advancements of SpMV in recent years is currently lacking.","Aiming to fill this gap, this paper compares existing techniques and analyzes their strengths and weaknesses.","We begin by highlighting two representative applications of SpMV, then conduct an in-depth overview of the important techniques that optimize SpMV on modern architectures, which we specifically classify as classic, auto-tuning, machine learning, and mixed-precision-based optimization.","We also elaborate on the hardware-based architectures, including CPU, GPU, FPGA, processing in Memory, heterogeneous, and distributed platforms.","We present a comprehensive experimental evaluation that compares the performance of state-of-the-art SpMV implementations.","Based on our findings, we identify several challenges and point out future research directions.","This survey is intended to provide researchers with a comprehensive understanding of SpMV optimization on modern architectures and provide guidance for future work."],"url":"http://arxiv.org/abs/2404.06047v1","category":"cs.DC"}
{"created":"2024-04-09 05:12:44","title":"Prelimit Coupling and Steady-State Convergence of Constant-stepsize Nonsmooth Contractive SA","abstract":"Motivated by Q-learning, we study nonsmooth contractive stochastic approximation (SA) with constant stepsize. We focus on two important classes of dynamics: 1) nonsmooth contractive SA with additive noise, and 2) synchronous and asynchronous Q-learning, which features both additive and multiplicative noise. For both dynamics, we establish weak convergence of the iterates to a stationary limit distribution in Wasserstein distance. Furthermore, we propose a prelimit coupling technique for establishing steady-state convergence and characterize the limit of the stationary distribution as the stepsize goes to zero. Using this result, we derive that the asymptotic bias of nonsmooth SA is proportional to the square root of the stepsize, which stands in sharp contrast to smooth SA. This bias characterization allows for the use of Richardson-Romberg extrapolation for bias reduction in nonsmooth SA.","sentences":["Motivated by Q-learning, we study nonsmooth contractive stochastic approximation (SA) with constant stepsize.","We focus on two important classes of dynamics: 1) nonsmooth contractive SA with additive noise, and 2) synchronous and asynchronous Q-learning, which features both additive and multiplicative noise.","For both dynamics, we establish weak convergence of the iterates to a stationary limit distribution in Wasserstein distance.","Furthermore, we propose a prelimit coupling technique for establishing steady-state convergence and characterize the limit of the stationary distribution as the stepsize goes to zero.","Using this result, we derive that the asymptotic bias of nonsmooth SA is proportional to the square root of the stepsize, which stands in sharp contrast to smooth SA.","This bias characterization allows for the use of Richardson-Romberg extrapolation for bias reduction in nonsmooth SA."],"url":"http://arxiv.org/abs/2404.06023v1","category":"stat.ML"}
{"created":"2024-04-09 03:58:39","title":"Multi-Agent Coverage Control with Transient Behavior Consideration","abstract":"This paper studies the multi-agent coverage control (MAC) problem where agents must dynamically learn an unknown density function while performing coverage tasks. Unlike many current theoretical frameworks that concentrate solely on the regret occurring at specific targeted sensory locations, our approach additionally considers the regret caused by transient behavior - the path from one location and another. We propose the multi-agent coverage control with the doubling trick (MAC-DT) algorithm and demonstrate that it achieves (approximated) regret of $\\widetilde{O}(\\sqrt{T})$ even when accounting for the transient behavior. Our result is also supported by numerical experiments, showcasing that the proposed algorithm manages to match or even outperform the baseline algorithms in simulation environments. We also show how our algorithm can be modified to handle safety constraints and further implement the algorithm on a real-robotic testbed.","sentences":["This paper studies the multi-agent coverage control (MAC) problem where agents must dynamically learn an unknown density function while performing coverage tasks.","Unlike many current theoretical frameworks that concentrate solely on the regret occurring at specific targeted sensory locations, our approach additionally considers the regret caused by transient behavior - the path from one location and another.","We propose the multi-agent coverage control with the doubling trick (MAC-DT) algorithm and demonstrate that it achieves (approximated) regret of $\\widetilde{O}(\\sqrt{T})$ even when accounting for the transient behavior.","Our result is also supported by numerical experiments, showcasing that the proposed algorithm manages to match or even outperform the baseline algorithms in simulation environments.","We also show how our algorithm can be modified to handle safety constraints and further implement the algorithm on a real-robotic testbed."],"url":"http://arxiv.org/abs/2404.05995v1","category":"math.OC"}
{"created":"2024-04-09 02:58:05","title":"Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation","abstract":"This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization--one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets.","sentences":["This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains.","We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation.","We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization--one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model.","This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input.","Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets."],"url":"http://arxiv.org/abs/2404.05970v1","category":"cs.CL"}
{"created":"2024-04-09 02:55:31","title":"Resolution enhancement of SOHO/MDI Magnetograms","abstract":"Research on the solar magnetic field and its effects on solar dynamo mechanisms and space weather events has benefited from the continual improvements in instrument resolution and measurement frequency. The augmentation and assimilation of historical observational data timelines also play a significant role in understanding the patterns of solar magnetic field variation. Within the realm of astronomical data processing, superresolution reconstruction refers to the process of using a substantial corpus of training data to learn the nonlinear mapping between low-resolution and high-resolution images,thereby achieving higher-resolution astronomical images. This paper is an application study in highdimensional non-linear regression. Deep learning models were employed to perform SR modeling on SOHO/MDI magnetograms and SDO/HMI magnetograms, thus reliably achieving resolution enhancement of full-disk SOHO/MDI magnetograms and enhancing the image resolution to obtain more detailed information. For this study, a dataset comprising 9717 pairs of data from April 2010 to February 2011 was used as the training set,1332 pairs from March 2011 were used as the validation set, and 1,034 pairs from April 2011 were used as the test set. After data preprocessing, we randomly cropped 128x128 sub-images as the LR from the full-disk MDI magnetograms, and the corresponding 512x512 sub-images as HR from the HMI full-disk magnetograms for model training. The tests conducted have shown that the study successfully produced reliable 4x super-resolution reconstruction of full-disk MDI magnetograms.The MESR model'sresults (0.911) were highly correlated with the target HMI magnetographs as indicated by the correlation coefficient values. Furthermore, the method achieved the best PSNR, SSIM, MAE and RMSE values, indicating that the MESR model can effectively reconstruct magnetog.","sentences":["Research on the solar magnetic field and its effects on solar dynamo mechanisms and space weather events has benefited from the continual improvements in instrument resolution and measurement frequency.","The augmentation and assimilation of historical observational data timelines also play a significant role in understanding the patterns of solar magnetic field variation.","Within the realm of astronomical data processing, superresolution reconstruction refers to the process of using a substantial corpus of training data to learn the nonlinear mapping between low-resolution and high-resolution images,thereby achieving higher-resolution astronomical images.","This paper is an application study in highdimensional non-linear regression.","Deep learning models were employed to perform SR modeling on SOHO/MDI magnetograms and SDO/HMI magnetograms, thus reliably achieving resolution enhancement of full-disk SOHO/MDI magnetograms and enhancing the image resolution to obtain more detailed information.","For this study, a dataset comprising 9717 pairs of data from April 2010 to February 2011 was used as the training set,1332 pairs from March 2011 were used as the validation set, and 1,034 pairs from April 2011 were used as the test set.","After data preprocessing, we randomly cropped 128x128 sub-images as the LR from the full-disk MDI magnetograms, and the corresponding 512x512 sub-images as HR from the HMI full-disk magnetograms for model training.","The tests conducted have shown that the study successfully produced reliable 4x super-resolution reconstruction of full-disk MDI magnetograms.","The MESR model'sresults (0.911) were highly correlated with the target HMI magnetographs as indicated by the correlation coefficient values.","Furthermore, the method achieved the best PSNR, SSIM, MAE and RMSE values, indicating that the MESR model can effectively reconstruct magnetog."],"url":"http://arxiv.org/abs/2404.05968v1","category":"astro-ph.SR"}
{"created":"2024-04-09 02:47:52","title":"EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker","abstract":"Most of 3D single object trackers (SOT) in point clouds follow the two-stream multi-stage 3D Siamese or motion tracking paradigms, which process the template and search area point clouds with two parallel branches, built on supervised point cloud backbones. In this work, beyond typical 3D Siamese or motion tracking, we propose a neat and compact one-stream transformer 3D SOT paradigm from the novel perspective, termed as \\textbf{EasyTrack}, which consists of three special designs: 1) A 3D point clouds tracking feature pre-training module is developed to exploit the masked autoencoding for learning 3D point clouds tracking representations. 2) A unified 3D tracking feature learning and fusion network is proposed to simultaneously learns target-aware 3D features, and extensively captures mutual correlation through the flexible self-attention mechanism. 3) A target location network in the dense bird's eye view (BEV) feature space is constructed for target classification and regression. Moreover, we develop an enhanced version named EasyTrack++, which designs the center points interaction (CPI) strategy to reduce the ambiguous targets caused by the noise point cloud background information. The proposed EasyTrack and EasyTrack++ set a new state-of-the-art performance ($\\textbf{18\\%}$, $\\textbf{40\\%}$ and $\\textbf{3\\%}$ success gains) in KITTI, NuScenes, and Waymo while runing at \\textbf{52.6fps} with few parameters (\\textbf{1.3M}). The code will be available at https://github.com/KnightApple427/Easytrack.","sentences":["Most of 3D single object trackers (SOT) in point clouds follow the two-stream multi-stage 3D Siamese or motion tracking paradigms, which process the template and search area point clouds with two parallel branches, built on supervised point cloud backbones.","In this work, beyond typical 3D Siamese or motion tracking, we propose a neat and compact one-stream transformer 3D SOT paradigm from the novel perspective, termed as \\textbf{EasyTrack}, which consists of three special designs: 1) A 3D point clouds tracking feature pre-training module is developed to exploit the masked autoencoding for learning 3D point clouds tracking representations.","2) A unified 3D tracking feature learning and fusion network is proposed to simultaneously learns target-aware 3D features, and extensively captures mutual correlation through the flexible self-attention mechanism.","3) A target location network in the dense bird's eye view (BEV) feature space is constructed for target classification and regression.","Moreover, we develop an enhanced version named EasyTrack++, which designs the center points interaction (CPI) strategy to reduce the ambiguous targets caused by the noise point cloud background information.","The proposed EasyTrack and EasyTrack++ set a new state-of-the-art performance ($\\textbf{18\\%}$, $\\textbf{40\\%}$ and $\\textbf{3\\%}$ success gains) in KITTI, NuScenes, and Waymo while runing at \\textbf{52.6fps} with few parameters (\\textbf{1.3M}).","The code will be available at https://github.com/KnightApple427/Easytrack."],"url":"http://arxiv.org/abs/2404.05960v1","category":"cs.CV"}
{"created":"2024-04-09 01:32:35","title":"Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot","abstract":"In this study, a chair-type asymmetric tripedal low-rigidity robot was designed based on the three-legged chair character in the movie \"Suzume\" and its gait was generated. Its body structure consists of three legs that are asymmetric to the body, so it cannot be easily balanced. In addition, the actuator is a servo motor that can only feed-forward rotational angle commands and the sensor can only sense the robot's posture quaternion. In such an asymmetric and imperfect body structure, we analyzed how gait is generated in walking and stand-up motions by generating gaits with two different methods: a method using linear completion to connect the postures necessary for the gait discovered through trial and error using the actual robot, and a method using the gait generated by reinforcement learning in the simulator and reflecting it to the actual robot. Both methods were able to generate gait that realized walking and stand-up motions, and interesting gait patterns were observed, which differed depending on the method, and were confirmed on the actual robot. Our code and demonstration videos are available here: https://github.com/shin0805/Chair-TypeAsymmetricalTripedalRobot.git","sentences":["In this study, a chair-type asymmetric tripedal low-rigidity robot was designed based on the three-legged chair character in the movie \"Suzume\" and its gait was generated.","Its body structure consists of three legs that are asymmetric to the body, so it cannot be easily balanced.","In addition, the actuator is a servo motor that can only feed-forward rotational angle commands and the sensor can only sense the robot's posture quaternion.","In such an asymmetric and imperfect body structure, we analyzed how gait is generated in walking and stand-up motions by generating gaits with two different methods: a method using linear completion to connect the postures necessary for the gait discovered through trial and error using the actual robot, and a method using the gait generated by reinforcement learning in the simulator and reflecting it to the actual robot.","Both methods were able to generate gait that realized walking and stand-up motions, and interesting gait patterns were observed, which differed depending on the method, and were confirmed on the actual robot.","Our code and demonstration videos are available here: https://github.com/shin0805/Chair-TypeAsymmetricalTripedalRobot.git"],"url":"http://arxiv.org/abs/2404.05932v1","category":"cs.RO"}
{"created":"2024-04-09 00:30:16","title":"Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis","abstract":"Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to the variability in image quality and the necessity to process scans from various standard views. While current automated segmentation methods in echocardiography show promising performance, they are trained on specific scan views to analyze corresponding data. However, this solution has a limitation as the number of required models increases with the number of standard views. To address this, in this paper, we present a prompt-driven universal method for view-agnostic echocardiography analysis. Considering the domain shift between standard views, we first introduce a method called prompt matching, aimed at learning prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model. Then, we utilized a pre-trained medical language model to align textual information with pixel data for accurate segmentation. Extensive experiments on three standard views showed that our approach significantly outperforms the state-of-the-art universal methods and achieves comparable or even better performances over the segmentation model trained and tested on same views.","sentences":["Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to the variability in image quality and the necessity to process scans from various standard views.","While current automated segmentation methods in echocardiography show promising performance, they are trained on specific scan views to analyze corresponding data.","However, this solution has a limitation as the number of required models increases with the number of standard views.","To address this, in this paper, we present a prompt-driven universal method for view-agnostic echocardiography analysis.","Considering the domain shift between standard views, we first introduce a method called prompt matching, aimed at learning prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model.","Then, we utilized a pre-trained medical language model to align textual information with pixel data for accurate segmentation.","Extensive experiments on three standard views showed that our approach significantly outperforms the state-of-the-art universal methods and achieves comparable or even better performances over the segmentation model trained and tested on same views."],"url":"http://arxiv.org/abs/2404.05916v1","category":"cs.CV"}
{"created":"2024-04-08 22:01:28","title":"A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit","abstract":"Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment. Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction. In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids. The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR). To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting. These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation.","sentences":["Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment.","Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction.","In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids.","The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR).","To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting.","These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation."],"url":"http://arxiv.org/abs/2404.05888v1","category":"cs.RO"}
{"created":"2024-04-08 21:58:25","title":"On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality","abstract":"Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons' fatigue and improving patients' outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios.","sentences":["Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons' fatigue and improving patients' outcomes.","These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation.","However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator.","In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS.","The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface.","These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions.","To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty.","Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios."],"url":"http://arxiv.org/abs/2404.05887v1","category":"cs.RO"}
{"created":"2024-04-08 21:08:13","title":"CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation","abstract":"Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx. 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT.","sentences":["Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies.","However, classical industrial robots struggle to cope with product variation and dynamic environments.","In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees.","CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times.","The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx.","93% success rate overall with an average of 7.5s programming time.","We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT."],"url":"http://arxiv.org/abs/2404.05870v1","category":"cs.RO"}
{"created":"2024-04-08 20:58:06","title":"GeniL: A Multilingual Dataset on Generalizing Language","abstract":"LLMs are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups. While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step. Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in. We argue that understanding the sentential context is crucial for detecting instances of generalization. We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization (\"people think the French are very rude\"), and (2) language that reinforces such a generalization (\"as French they must be rude\"), from non-generalizing context (\"My French friends think I am rude\"). For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations. We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations. We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes. We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages. Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies.","sentences":["LLMs are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups.","While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step.","Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in.","We argue that understanding the sentential context is crucial for detecting instances of generalization.","We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization (\"people think the French are very rude\"), and (2) language that reinforces such a generalization (\"as French they must be rude\"), from non-generalizing context (\"My French friends think I am rude\").","For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations.","We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations.","We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes.","We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages.","Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies."],"url":"http://arxiv.org/abs/2404.05866v1","category":"cs.CL"}
{"created":"2024-04-08 20:51:30","title":"Towards Improved Semiconductor Defect Inspection for high-NA EUVL based on SEMI-SuperYOLO-NAS","abstract":"Due to potential pitch reduction, the semiconductor industry is adopting High-NA EUVL technology. However, its low depth of focus presents challenges for High Volume Manufacturing. To address this, suppliers are exploring thinner photoresists and new underlayers/hardmasks. These may suffer from poor SNR, complicating defect detection. Vision-based ML algorithms offer a promising solution for semiconductor defect inspection. However, developing a robust ML model across various image resolutions without explicit training remains a challenge for nano-scale defect inspection. This research's goal is to propose a scale-invariant ADCD framework capable to upscale images, addressing this issue. We propose an improvised ADCD framework as SEMI-SuperYOLO-NAS, which builds upon the baseline YOLO-NAS architecture. This framework integrates a SR assisted branch to aid in learning HR features by the defect detection backbone, particularly for detecting nano-scale defect instances from LR images. Additionally, the SR-assisted branch can recursively generate upscaled images from their corresponding downscaled counterparts, enabling defect detection inference across various image resolutions without requiring explicit training. Moreover, we investigate improved data augmentation strategy aimed at generating diverse and realistic training datasets to enhance model performance. We have evaluated our proposed approach using two original FAB datasets obtained from two distinct processes and captured using two different imaging tools. Finally, we demonstrate zero-shot inference for our model on a new, originating from a process condition distinct from the training dataset and possessing different Pitch characteristics. Experimental validation demonstrates that our proposed ADCD framework aids in increasing the throughput of imaging tools for defect inspection by reducing the required image pixel resolutions.","sentences":["Due to potential pitch reduction, the semiconductor industry is adopting High-NA EUVL technology.","However, its low depth of focus presents challenges for High Volume Manufacturing.","To address this, suppliers are exploring thinner photoresists and new underlayers/hardmasks.","These may suffer from poor SNR, complicating defect detection.","Vision-based ML algorithms offer a promising solution for semiconductor defect inspection.","However, developing a robust ML model across various image resolutions without explicit training remains a challenge for nano-scale defect inspection.","This research's goal is to propose a scale-invariant ADCD framework capable to upscale images, addressing this issue.","We propose an improvised ADCD framework as SEMI-SuperYOLO-NAS, which builds upon the baseline YOLO-NAS architecture.","This framework integrates a SR assisted branch to aid in learning HR features by the defect detection backbone, particularly for detecting nano-scale defect instances from LR images.","Additionally, the SR-assisted branch can recursively generate upscaled images from their corresponding downscaled counterparts, enabling defect detection inference across various image resolutions without requiring explicit training.","Moreover, we investigate improved data augmentation strategy aimed at generating diverse and realistic training datasets to enhance model performance.","We have evaluated our proposed approach using two original FAB datasets obtained from two distinct processes and captured using two different imaging tools.","Finally, we demonstrate zero-shot inference for our model on a new, originating from a process condition distinct from the training dataset and possessing different Pitch characteristics.","Experimental validation demonstrates that our proposed ADCD framework aids in increasing the throughput of imaging tools for defect inspection by reducing the required image pixel resolutions."],"url":"http://arxiv.org/abs/2404.05862v1","category":"cs.CV"}
{"created":"2024-04-08 20:31:27","title":"Localizing Moments of Actions in Untrimmed Videos of Infants with Autism Spectrum Disorder","abstract":"Autism Spectrum Disorder (ASD) presents significant challenges in early diagnosis and intervention, impacting children and their families. With prevalence rates rising, there is a critical need for accessible and efficient screening tools. Leveraging machine learning (ML) techniques, in particular Temporal Action Localization (TAL), holds promise for automating ASD screening. This paper introduces a self-attention based TAL model designed to identify ASD-related behaviors in infant videos. Unlike existing methods, our approach simplifies complex modeling and emphasizes efficiency, which is essential for practical deployment in real-world scenarios. Importantly, this work underscores the importance of developing computer vision methods capable of operating in naturilistic environments with little equipment control, addressing key challenges in ASD screening. This study is the first to conduct end-to-end temporal action localization in untrimmed videos of infants with ASD, offering promising avenues for early intervention and support. We report baseline results of behavior detection using our TAL model. We achieve 70% accuracy for look face, 79% accuracy for look object, 72% for smile and 65% for vocalization.","sentences":["Autism Spectrum Disorder (ASD) presents significant challenges in early diagnosis and intervention, impacting children and their families.","With prevalence rates rising, there is a critical need for accessible and efficient screening tools.","Leveraging machine learning (ML) techniques, in particular Temporal Action Localization (TAL), holds promise for automating ASD screening.","This paper introduces a self-attention based TAL model designed to identify ASD-related behaviors in infant videos.","Unlike existing methods, our approach simplifies complex modeling and emphasizes efficiency, which is essential for practical deployment in real-world scenarios.","Importantly, this work underscores the importance of developing computer vision methods capable of operating in naturilistic environments with little equipment control, addressing key challenges in ASD screening.","This study is the first to conduct end-to-end temporal action localization in untrimmed videos of infants with ASD, offering promising avenues for early intervention and support.","We report baseline results of behavior detection using our TAL model.","We achieve 70% accuracy for look face, 79% accuracy for look object, 72% for smile and 65% for vocalization."],"url":"http://arxiv.org/abs/2404.05849v1","category":"cs.CV"}
{"created":"2024-04-08 20:14:10","title":"Softmax Attention with Constant Cost per Token","abstract":"We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Attention becomes expressible as a composition of log-sums of exponentials that is linearizable, with a latent space of constant size, enabling sequential application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.","sentences":["We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials.","Attention becomes expressible as a composition of log-sums of exponentials that is linearizable, with a latent space of constant size, enabling sequential application with constant time and space complexity per token.","We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention."],"url":"http://arxiv.org/abs/2404.05843v1","category":"cs.LG"}
{"created":"2024-04-08 20:05:25","title":"\u00daFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin","abstract":"We present LatinPipe, the winning submission to the EvaLatin 2024 Dependency Parsing shared task. Our system consists of a fine-tuned concatenation of base and large pre-trained LMs, with a dot-product attention head for parsing and softmax classification heads for morphology to jointly learn both dependency parsing and morphological analysis. It is trained by sampling from seven publicly available Latin corpora, utilizing additional harmonization of annotations to achieve a more unified annotation style. Before fine-tuning, we train the system for a few initial epochs with frozen weights. We also add additional local relative contextualization by stacking the BiLSTM layers on top of the Transformer(s). Finally, we ensemble output probability distributions from seven randomly instantiated networks for the final submission. The code is available at https://github.com/ufal/evalatin2024-latinpipe.","sentences":["We present LatinPipe, the winning submission to the EvaLatin 2024 Dependency Parsing shared task.","Our system consists of a fine-tuned concatenation of base and large pre-trained LMs, with a dot-product attention head for parsing and softmax classification heads for morphology to jointly learn both dependency parsing and morphological analysis.","It is trained by sampling from seven publicly available Latin corpora, utilizing additional harmonization of annotations to achieve a more unified annotation style.","Before fine-tuning, we train the system for a few initial epochs with frozen weights.","We also add additional local relative contextualization by stacking the BiLSTM layers on top of the Transformer(s).","Finally, we ensemble output probability distributions from seven randomly instantiated networks for the final submission.","The code is available at https://github.com/ufal/evalatin2024-latinpipe."],"url":"http://arxiv.org/abs/2404.05839v1","category":"cs.CL"}
{"created":"2024-04-08 20:03:06","title":"Unveiling Latent Topics in Robotic Process Automation -- an Approach based on Latent Dirichlet Allocation Smart Review","abstract":"Robotic process automation (RPA) is a software technology that in recent years has gained a lot of attention and popularity. By now, research on RPA has spread into multiple research streams. This study aims to create a science map of RPA and its aspects by revealing latent topics related to RPA, their research interest, impact, and time development. We provide a systematic framework that is helpful to develop further research into this technology. By using an unsupervised machine learning method based on Latent Dirichlet Allocation, we were able to analyse over 2000 paper abstracts. Among these, we found 100 distinct study topics, 15 of which have been included in the science map we provide.","sentences":["Robotic process automation (RPA) is a software technology that in recent years has gained a lot of attention and popularity.","By now, research on RPA has spread into multiple research streams.","This study aims to create a science map of RPA and its aspects by revealing latent topics related to RPA, their research interest, impact, and time development.","We provide a systematic framework that is helpful to develop further research into this technology.","By using an unsupervised machine learning method based on Latent Dirichlet Allocation, we were able to analyse over 2000 paper abstracts.","Among these, we found 100 distinct study topics, 15 of which have been included in the science map we provide."],"url":"http://arxiv.org/abs/2404.05836v1","category":"cs.CY"}
{"created":"2024-04-08 19:59:15","title":"Human-Machine Interaction in Automated Vehicles: Reducing Voluntary Driver Intervention","abstract":"This paper develops a novel car-following control method to reduce voluntary driver interventions and improve traffic stability in Automated Vehicles (AVs). Through a combination of experimental and empirical analysis, we show how voluntary driver interventions can instigate substantial traffic disturbances that are amplified along the traffic upstream. Motivated by these findings, we present a framework for driver intervention based on evidence accumulation (EA), which describes the evolution of the driver's distrust in automation, ultimately resulting in intervention. Informed through the EA framework, we propose a deep reinforcement learning (DRL)-based car-following control for AVs that is strategically designed to mitigate unnecessary driver intervention and improve traffic stability. Numerical experiments are conducted to demonstrate the effectiveness of the proposed control model.","sentences":["This paper develops a novel car-following control method to reduce voluntary driver interventions and improve traffic stability in Automated Vehicles (AVs).","Through a combination of experimental and empirical analysis, we show how voluntary driver interventions can instigate substantial traffic disturbances that are amplified along the traffic upstream.","Motivated by these findings, we present a framework for driver intervention based on evidence accumulation (EA), which describes the evolution of the driver's distrust in automation, ultimately resulting in intervention.","Informed through the EA framework, we propose a deep reinforcement learning (DRL)-based car-following control for AVs that is strategically designed to mitigate unnecessary driver intervention and improve traffic stability.","Numerical experiments are conducted to demonstrate the effectiveness of the proposed control model."],"url":"http://arxiv.org/abs/2404.05832v1","category":"cs.HC"}
{"created":"2024-04-08 18:55:07","title":"Just Wing It: Optimal Estimation of Missing Mass in a Markovian Sequence","abstract":"We study the problem of estimating the stationary mass -- also called the unigram mass -- that is missing from a single trajectory of a discrete-time, ergodic Markov chain. This problem has several applications -- for example, estimating the stationary missing mass is critical for accurately smoothing probability estimates in sequence models. While the classical Good--Turing estimator from the 1950s has appealing properties for i.i.d. data, it is known to be biased in the Markov setting, and other heuristic estimators do not come equipped with guarantees. Operating in the general setting in which the size of the state space may be much larger than the length $n$ of the trajectory, we develop a linear-runtime estimator called \\emph{Windowed Good--Turing} (\\textsc{WingIt}) and show that its risk decays as $\\widetilde{\\mathcal{O}}(\\mathsf{T_{mix}}/n)$, where $\\mathsf{T_{mix}}$ denotes the mixing time of the chain in total variation distance. Notably, this rate is independent of the size of the state space and minimax-optimal up to a logarithmic factor in $n / \\mathsf{T_{mix}}$. We also present a bound on the variance of the missing mass random variable, which may be of independent interest. We extend our estimator to approximate the stationary mass placed on elements occurring with small frequency in $X^n$. Finally, we demonstrate the efficacy of our estimators both in simulations on canonical chains and on sequences constructed from a popular natural language corpus.","sentences":["We study the problem of estimating the stationary mass -- also called the unigram mass -- that is missing from a single trajectory of a discrete-time, ergodic Markov chain.","This problem has several applications -- for example, estimating the stationary missing mass is critical for accurately smoothing probability estimates in sequence models.","While the classical Good--Turing estimator from the 1950s has appealing properties for i.i.d. data, it is known to be biased in the Markov setting, and other heuristic estimators do not come equipped with guarantees.","Operating in the general setting in which the size of the state space may be much larger than the length $n$ of the trajectory, we develop a linear-runtime estimator called \\emph{Windowed Good--Turing} (\\textsc{WingIt}) and show that its risk decays as $\\widetilde{\\mathcal{O}}(\\mathsf{T_{mix}}/n)$, where $\\mathsf{T_{mix}}$ denotes the mixing time of the chain in total variation distance.","Notably, this rate is independent of the size of the state space and minimax-optimal up to a logarithmic factor in $n / \\mathsf{T_{mix}}$. We also present a bound on the variance of the missing mass random variable, which may be of independent interest.","We extend our estimator to approximate the stationary mass placed on elements occurring with small frequency in $X^n$. Finally, we demonstrate the efficacy of our estimators both in simulations on canonical chains and on sequences constructed from a popular natural language corpus."],"url":"http://arxiv.org/abs/2404.05819v1","category":"stat.ML"}
{"created":"2024-04-08 18:50:13","title":"Downscaling GRACE-derived ocean bottom pressure anomalies using self-supervised data fusion","abstract":"The gravimetry measurements from the Gravity Recovery and Climate Experiment (GRACE) and its follow-on (GRACE-FO) satellite mission provide an essential way to monitor changes in ocean bottom pressure ($p_b$), which is a critical variable in understanding ocean circulation. However, the coarse spatial resolution of the GRACE(-FO) fields blurs important spatial details, such as $p_b$ gradients. In this study, we employ a self-supervised deep learning algorithm to downscale global monthly $p_b$ anomalies derived from GRACE(-FO) observations to an equal-angle $0.25^\\circ$ grid in the absence of high-resolution ground truth. The optimization process is realized by constraining the outputs to follow the large-scale mass conservation contained in the gravity field estimates while learning the spatial details from two ocean reanalysis products. The downscaled product agrees with GRACE(-FO) solutions over large ocean basins at the millimeter level in terms of equivalent water height and shows signs of outperforming them when evaluating short spatial scale variability. In particular, the downscaled $p_b$ product has more realistic signal content near the coast and exhibits better agreement with tide gauge measurements at around 80% of 465 globally distributed stations. Our method presents a novel way of combining the advantages of satellite measurements and ocean models at the product level, with potential downstream applications for studies of the large-scale ocean circulation, coastal sea level variability, and changes in global geodetic parameters.","sentences":["The gravimetry measurements from the Gravity Recovery and Climate Experiment (GRACE) and its follow-on (GRACE-FO) satellite mission provide an essential way to monitor changes in ocean bottom pressure ($p_b$), which is a critical variable in understanding ocean circulation.","However, the coarse spatial resolution of the GRACE(-FO) fields blurs important spatial details, such as $p_b$ gradients.","In this study, we employ a self-supervised deep learning algorithm to downscale global monthly $p_b$ anomalies derived from GRACE(-FO) observations to an equal-angle $0.25^\\circ$ grid in the absence of high-resolution ground truth.","The optimization process is realized by constraining the outputs to follow the large-scale mass conservation contained in the gravity field estimates while learning the spatial details from two ocean reanalysis products.","The downscaled product agrees with GRACE(-FO) solutions over large ocean basins at the millimeter level in terms of equivalent water height and shows signs of outperforming them when evaluating short spatial scale variability.","In particular, the downscaled $p_b$ product has more realistic signal content near the coast and exhibits better agreement with tide gauge measurements at around 80% of 465 globally distributed stations.","Our method presents a novel way of combining the advantages of satellite measurements and ocean models at the product level, with potential downstream applications for studies of the large-scale ocean circulation, coastal sea level variability, and changes in global geodetic parameters."],"url":"http://arxiv.org/abs/2404.05818v1","category":"physics.geo-ph"}
{"created":"2024-04-08 18:40:25","title":"Centrality Estimators for Probability Density Functions","abstract":"In this report, we explore the data selection leading to a family of estimators maximizing a centrality. The family allows a nice properties leading to accurate and robust probability density function fitting according to some criteria we define. We establish a link between the centrality estimator and the maximum likelihood, showing that the latter is a particular case. Therefore, a new probability interpretation of Fisher maximum likelihood is provided. We will introduce and study two specific centralities that we have named H\\\"older and Lehmer estimators. A numerical simulation is provided showing the effectiveness of the proposed families of estimators opening the door to development of new concepts and algorithms in machine learning, data mining, statistics, and data analysis.","sentences":["In this report, we explore the data selection leading to a family of estimators maximizing a centrality.","The family allows a nice properties leading to accurate and robust probability density function fitting according to some criteria we define.","We establish a link between the centrality estimator and the maximum likelihood, showing that the latter is a particular case.","Therefore, a new probability interpretation of Fisher maximum likelihood is provided.","We will introduce and study two specific centralities that we have named H\\\"older and Lehmer estimators.","A numerical simulation is provided showing the effectiveness of the proposed families of estimators opening the door to development of new concepts and algorithms in machine learning, data mining, statistics, and data analysis."],"url":"http://arxiv.org/abs/2404.05816v1","category":"math.ST"}
{"created":"2024-04-08 17:59:46","title":"Finding Visual Task Vectors","abstract":"Visual Prompting is a technique for teaching models to perform a visual task via in-context examples, without any additional training. In this work, we analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find task vectors, activations that encode task-specific information. Equipped with this insight, we demonstrate that it is possible to identify the task vectors and use them to guide the network towards performing different tasks without providing any input-output examples. To find task vectors, we compute the average intermediate activations per task and use the REINFORCE algorithm to search for the subset of task vectors. The resulting task vectors guide the model towards performing a task better than the original model without the need for input-output examples.","sentences":["Visual Prompting is a technique for teaching models to perform a visual task via in-context examples, without any additional training.","In this work, we analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find task vectors, activations that encode task-specific information.","Equipped with this insight, we demonstrate that it is possible to identify the task vectors and use them to guide the network towards performing different tasks without providing any input-output examples.","To find task vectors, we compute the average intermediate activations per task and use the REINFORCE algorithm to search for the subset of task vectors.","The resulting task vectors guide the model towards performing a task better than the original model without the need for input-output examples."],"url":"http://arxiv.org/abs/2404.05729v1","category":"cs.CV"}
{"created":"2024-04-09 17:29:53","title":"Laue Indexing with Optimal Transport","abstract":"Laue tomography experiments retrieve the positions and orientations of crystal grains in a polycrystalline samples from diffraction patterns recorded at multiple viewing angles. The use of a broad wavelength spectrum beam can greatly reduce the experimental time, but poses a difficult challenge for the indexing of diffraction peaks in polycrystalline samples; the information about the wavelength of these Bragg peaks is absent and the diffraction patterns from multiple grains are superimposed. To date, no algorithms exist capable of indexing samples with more than about 500 grains efficiently. To address this need we present a novel method: Laue indexing with Optimal Transport (LaueOT). We create a probabilistic description of the multi-grain indexing problem and propose a solution based on Sinkhorn Expectation-Maximization method, which allows to efficiently find the maximum of the likelihood thanks to the assignments being calculated using Optimal Transport. This is a non-convex optimization problem, where the orientations and positions of grains are optimized simultaneously with grain-to-spot assignments, while robustly handling the outliers. The selection of initial prototype grains to consider in the optimization problem are also calculated within the Optimal Transport framework. LaueOT can rapidly and effectively index up to 1000 grains on a single large memory GPU within less than 30 minutes. We demonstrate the performance of LaueOT on simulations with variable numbers of grains, spot position measurement noise levels, and outlier fractions. The algorithm recovers the correct number of grains even for high noise levels and up to 70% outliers in our experiments. We compare the results of indexing with LaueOT to existing algorithms both on synthetic and real neutron diffraction data from well-characterized samples.","sentences":["Laue tomography experiments retrieve the positions and orientations of crystal grains in a polycrystalline samples from diffraction patterns recorded at multiple viewing angles.","The use of a broad wavelength spectrum beam can greatly reduce the experimental time, but poses a difficult challenge for the indexing of diffraction peaks in polycrystalline samples; the information about the wavelength of these Bragg peaks is absent and the diffraction patterns from multiple grains are superimposed.","To date, no algorithms exist capable of indexing samples with more than about 500 grains efficiently.","To address this need we present a novel method:","Laue indexing with Optimal Transport (LaueOT).","We create a probabilistic description of the multi-grain indexing problem and propose a solution based on Sinkhorn Expectation-Maximization method, which allows to efficiently find the maximum of the likelihood thanks to the assignments being calculated using Optimal Transport.","This is a non-convex optimization problem, where the orientations and positions of grains are optimized simultaneously with grain-to-spot assignments, while robustly handling the outliers.","The selection of initial prototype grains to consider in the optimization problem are also calculated within the Optimal Transport framework.","LaueOT can rapidly and effectively index up to 1000 grains on a single large memory GPU within less than 30 minutes.","We demonstrate the performance of LaueOT on simulations with variable numbers of grains, spot position measurement noise levels, and outlier fractions.","The algorithm recovers the correct number of grains even for high noise levels and up to 70% outliers in our experiments.","We compare the results of indexing with LaueOT to existing algorithms both on synthetic and real neutron diffraction data from well-characterized samples."],"url":"http://arxiv.org/abs/2404.06478v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 16:25:13","title":"Software-based Security Framework for Edge and Mobile IoT","abstract":"With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative. Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring. This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency. The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources. Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers. This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management.","sentences":["With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative.","Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring.","This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency.","The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources.","Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers.","This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management."],"url":"http://arxiv.org/abs/2404.06435v1","category":"cs.CR"}
{"created":"2024-04-09 16:07:00","title":"Radon-Hurwitz Grassmannian codes","abstract":"Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible. EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing. By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space. We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric.","sentences":["Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible.","EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing.","By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space.","We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric."],"url":"http://arxiv.org/abs/2404.06417v1","category":"cs.IT"}
{"created":"2024-04-09 15:13:50","title":"Enhancing Pharmaceutical Cold Supply Chain: Integrating Medication Synchronization and Diverse Delivery Modes","abstract":"The significance of last-mile logistics in the healthcare supply chain is growing steadily, especially in pharmacies where the growing prevalence of medication delivery to patients' homes is remarkable. This paper proposes a novel mathematical model for the last-mile logistics of the pharmaceutical supply chain and optimizes a pharmacy's logistical financial outcome while considering medication synchronization, different delivery modes, and temperature requirements of medicines. We propose a mathematical formulation of the problem using Mixed Integer Linear Programming (MILP) evolved from the actual problem of an outpatient pharmacy of a Dutch hospital. We create a case study by gathering, preparing, processing, and analyzing the associated data. We find the optimal solution, using Python MIP package and the Gurobi solver, which indicates the number of order batches, the composition of these batches, and the number of staff related to the preparation of the order batches. Our results show that our optimal solution increases the pharmacy's logistical financial outcome by 34 percent. Moreover, we propose other model variations and perform extensive scenario analysis to provide managerial insights applicable to other pharmacies and distributors in the last step of cold supply chains. Based on our scenario analysis, we conclude that improving medication synchronization can significantly enhance the pharmacy's logistical financial outcome.","sentences":["The significance of last-mile logistics in the healthcare supply chain is growing steadily, especially in pharmacies where the growing prevalence of medication delivery to patients' homes is remarkable.","This paper proposes a novel mathematical model for the last-mile logistics of the pharmaceutical supply chain and optimizes a pharmacy's logistical financial outcome while considering medication synchronization, different delivery modes, and temperature requirements of medicines.","We propose a mathematical formulation of the problem using Mixed Integer Linear Programming (MILP) evolved from the actual problem of an outpatient pharmacy of a Dutch hospital.","We create a case study by gathering, preparing, processing, and analyzing the associated data.","We find the optimal solution, using Python MIP package and the Gurobi solver, which indicates the number of order batches, the composition of these batches, and the number of staff related to the preparation of the order batches.","Our results show that our optimal solution increases the pharmacy's logistical financial outcome by 34 percent.","Moreover, we propose other model variations and perform extensive scenario analysis to provide managerial insights applicable to other pharmacies and distributors in the last step of cold supply chains.","Based on our scenario analysis, we conclude that improving medication synchronization can significantly enhance the pharmacy's logistical financial outcome."],"url":"http://arxiv.org/abs/2404.06373v1","category":"math.OC"}
{"created":"2024-04-09 13:45:59","title":"Compensating slice emittance growth in high brightness photoinjectors using sacrificial charge","abstract":"Achieving maximum electron beam brightness in photoinjectors requires detailed control of the 3D bunch shape and precise tuning of the beam focusing. Even in state-of-the-art designs, slice emittance growth due to nonlinear space charge forces and partial nonlaminarity often remains non-negligible. In this work we introduce a new means to linearize the transverse slice phase space: a sacrificial portion of the bunch's own charge distribution, formed into a wavebroken shock front by highly nonlinear space charge forces within the gun, whose downstream purpose is to dynamically linearize the desired bunch core. We show that linearization of an appropriately prepared bunch can be achieved via strongly nonlaminar focusing of the sacrificial shock front, while the inner core focuses laminarly. This leads to a natural spatial separation of the two distributions: a dense core surrounded by a diffuse halo of sacrificial charge that can be collimated. Multi-objective genetic algorithm optimizations of the ultra-compact x-ray free electron laser (UCXFEL) injector employ this concept, and we interpret it with an analytic model that agrees well with the simulations. In simulation we demonstrate a final bunch charge of 100 pC, peak current $\\sim 30$ A, and a sacrificial charge of 150 pC (250 pC total emitted from cathode) with normalized emittance growth of $<20$ nm-rad due to space charge. This implies a maximum achievable brightness approximately an order of magnitude greater than existing FEL injector designs.","sentences":["Achieving maximum electron beam brightness in photoinjectors requires detailed control of the 3D bunch shape and precise tuning of the beam focusing.","Even in state-of-the-art designs, slice emittance growth due to nonlinear space charge forces and partial nonlaminarity often remains non-negligible.","In this work we introduce a new means to linearize the transverse slice phase space: a sacrificial portion of the bunch's own charge distribution, formed into a wavebroken shock front by highly nonlinear space charge forces within the gun, whose downstream purpose is to dynamically linearize the desired bunch core.","We show that linearization of an appropriately prepared bunch can be achieved via strongly nonlaminar focusing of the sacrificial shock front, while the inner core focuses laminarly.","This leads to a natural spatial separation of the two distributions: a dense core surrounded by a diffuse halo of sacrificial charge that can be collimated.","Multi-objective genetic algorithm optimizations of the ultra-compact x-ray free electron laser (UCXFEL) injector employ this concept, and we interpret it with an analytic model that agrees well with the simulations.","In simulation we demonstrate a final bunch charge of 100 pC, peak current $\\sim 30$ A, and a sacrificial charge of 150 pC (250 pC total emitted from cathode) with normalized emittance growth of $<20$ nm-rad due to space charge.","This implies a maximum achievable brightness approximately an order of magnitude greater than existing FEL injector designs."],"url":"http://arxiv.org/abs/2404.06312v1","category":"physics.acc-ph"}
{"created":"2024-04-09 09:29:06","title":"WaSP: Warp Scheduling to Mimic Prefetching in Graphics Workloads","abstract":"Contemporary GPUs are designed to handle long-latency operations effectively; however, challenges such as core occupancy (number of warps in a core) and pipeline width can impede their latency management. This is particularly evident in Tile-Based Rendering (TBR) GPUs, where core occupancy remains low for extended durations. To address this challenge, we introduce WaSP, a lightweight warp scheduler tailored for GPUs in graphics applications. WaSP strategically mimics prefetching by initiating a select subset of warps, termed priority warps, early in execution to reduce memory latency for subsequent warps. This optimization taps into the inherent but underutilized memory parallelism within the GPU core. This underutilization is a consequence of a baseline scheduler that evenly spaces misses throughout execution to exploit the inherent spatial locality in graphics workloads. WaSP improves on this by reducing average memory latency while maintaining locality for the majority of warps. While maximizing memory parallelism utilization, WaSP prevents saturating the caches with misses to avoid filling up the MSHRs (Miss Status Holding Registers). This approach reduces cache stalls that halt further accesses to the cache. Overall, WaSP yields a significant 3.9% performance speedup. Importantly, WaSP accomplishes these enhancements with a negligible overhead, positioning it as a promising solution for enhancing the efficiency of GPUs in managing latency challenges.","sentences":["Contemporary GPUs are designed to handle long-latency operations effectively; however, challenges such as core occupancy (number of warps in a core) and pipeline width can impede their latency management.","This is particularly evident in Tile-Based Rendering (TBR) GPUs, where core occupancy remains low for extended durations.","To address this challenge, we introduce WaSP, a lightweight warp scheduler tailored for GPUs in graphics applications.","WaSP strategically mimics prefetching by initiating a select subset of warps, termed priority warps, early in execution to reduce memory latency for subsequent warps.","This optimization taps into the inherent but underutilized memory parallelism within the GPU core.","This underutilization is a consequence of a baseline scheduler that evenly spaces misses throughout execution to exploit the inherent spatial locality in graphics workloads.","WaSP improves on this by reducing average memory latency while maintaining locality for the majority of warps.","While maximizing memory parallelism utilization, WaSP prevents saturating the caches with misses to avoid filling up the MSHRs (Miss Status Holding Registers).","This approach reduces cache stalls that halt further accesses to the cache.","Overall, WaSP yields a significant 3.9% performance speedup.","Importantly, WaSP accomplishes these enhancements with a negligible overhead, positioning it as a promising solution for enhancing the efficiency of GPUs in managing latency challenges."],"url":"http://arxiv.org/abs/2404.06156v1","category":"cs.AR"}
{"created":"2024-04-09 09:08:24","title":"Highly reflective and high-$Q$ thin resonant subwavelength gratings","abstract":"We theoretically investigate the design of thin subwavelength gratings possessing high-reflectivity and high-$Q$ resonances when illuminated at normal incidence by a Gaussian beam. We compare the performances of single-period and dual-period rectangular gratings using Finite Element Method-based optimization and predict one to two orders of magnitude improvement in their transmission loss-linewidth product, which is the relevant figure of merit for e.g. resonant mirror-based microcavity applications.","sentences":["We theoretically investigate the design of thin subwavelength gratings possessing high-reflectivity and high-$Q$ resonances when illuminated at normal incidence by a Gaussian beam.","We compare the performances of single-period and dual-period rectangular gratings using Finite Element Method-based optimization and predict one to two orders of magnitude improvement in their transmission loss-linewidth product, which is the relevant figure of merit for e.g. resonant mirror-based microcavity applications."],"url":"http://arxiv.org/abs/2404.06143v1","category":"physics.optics"}
{"created":"2024-04-09 07:15:53","title":"Fully Dynamic Matching and Ordered Ruzsa-Szemer\u00e9di Graphs","abstract":"We study the fully dynamic maximum matching problem. In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions. Our focus is particularly on algorithms that maintain the edges of a $(1-\\epsilon)$-approximate maximum matching for an arbitrarily small constant $\\epsilon > 0$. Until recently, the fastest known algorithm for this problem required $\\Theta(n)$ time per update where $n$ is the number of vertices. This bound was slightly improved to $n/(\\log^* n)^{\\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li [STOC'23] and very recently to $n/2^{\\Omega(\\sqrt{\\log n})}$ by Liu [ArXiv'24]. Whether this can be improved to $n^{1-\\Omega(1)}$ remains a major open problem.   In this paper, we present a new algorithm that maintains a $(1-\\epsilon)$-approximate maximum matching. The update-time of our algorithm is parametrized based on the density of a certain class of graphs that we call Ordered Ruzsa-Szemer\\'edi (ORS) graphs, a generalization of the well-known Ruzsa-Szemer\\'edi graphs. While determining the density of ORS (or RS) remains a hard problem in combinatorics, we prove that if the existing constructions of ORS graphs are optimal, then our algorithm runs in $n^{1/2+O(\\epsilon)}$ time for any fixed $\\epsilon > 0$ which would be significantly faster than existing near-linear in $n$ time algorithms.   Our second main contribution is a better upper bound on density of both ORS and RS graphs with linear size matchings. The previous best upper bound was due to a proof of the triangle-removal lemma from more than a decade ago due to Fox [Annals of Mathematics '11].","sentences":["We study the fully dynamic maximum matching problem.","In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions.","Our focus is particularly on algorithms that maintain the edges of a $(1-\\epsilon)$-approximate maximum matching for an arbitrarily small constant $\\epsilon > 0$.","Until recently, the fastest known algorithm for this problem required $\\Theta(n)$ time per update where $n$ is the number of vertices.","This bound was slightly improved to $n/(\\log^* n)^{\\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li","[STOC'23] and very recently to $n/2^{\\Omega(\\sqrt{\\log n})}$ by Liu","[ArXiv'24].","Whether this can be improved to $n^{1-\\Omega(1)}$ remains a major open problem.   ","In this paper, we present a new algorithm that maintains a $(1-\\epsilon)$-approximate maximum matching.","The update-time of our algorithm is parametrized based on the density of a certain class of graphs that we call Ordered Ruzsa-Szemer\\'edi (ORS) graphs, a generalization of the well-known Ruzsa-Szemer\\'edi graphs.","While determining the density of ORS (or RS) remains a hard problem in combinatorics, we prove that if the existing constructions of ORS graphs are optimal, then our algorithm runs in $n^{1/2+O(\\epsilon)}$ time for any fixed $\\epsilon > 0$ which would be significantly faster than existing near-linear in $n$ time algorithms.   ","Our second main contribution is a better upper bound on density of both ORS and RS graphs with linear size matchings.","The previous best upper bound was due to a proof of the triangle-removal lemma from more than a decade ago due to Fox","[Annals of Mathematics '11]."],"url":"http://arxiv.org/abs/2404.06069v1","category":"cs.DS"}
{"created":"2024-04-09 07:05:36","title":"Constructing hierarchical time series through clustering: Is there an optimal way for forecasting?","abstract":"Forecast reconciliation has attracted significant research interest in recent years, with most studies taking the hierarchy of time series as given. We extend existing work that uses time series clustering to construct hierarchies, with the goal of improving forecast accuracy, in three ways. First, we investigate multiple approaches to clustering, including not only different clustering algorithms, but also the way time series are represented and how distance between time series is defined. We find that cluster-based hierarchies lead to improvements in forecast accuracy relative to two-level hierarchies. Second, we devise an approach based on random permutation of hierarchies, keeping the structure of the hierarchy fixed, while time series are randomly allocated to clusters. In doing so, we find that improvements in forecast accuracy that accrue from using clustering do not arise from grouping together similar series but from the structure of the hierarchy. Third, we propose an approach based on averaging forecasts across hierarchies constructed using different clustering methods, that is shown to outperform any single clustering method. All analysis is carried out on two benchmark datasets and a simulated dataset. Our findings provide new insights into the role of hierarchy construction in forecast reconciliation and offer valuable guidance on forecasting practice.","sentences":["Forecast reconciliation has attracted significant research interest in recent years, with most studies taking the hierarchy of time series as given.","We extend existing work that uses time series clustering to construct hierarchies, with the goal of improving forecast accuracy, in three ways.","First, we investigate multiple approaches to clustering, including not only different clustering algorithms, but also the way time series are represented and how distance between time series is defined.","We find that cluster-based hierarchies lead to improvements in forecast accuracy relative to two-level hierarchies.","Second, we devise an approach based on random permutation of hierarchies, keeping the structure of the hierarchy fixed, while time series are randomly allocated to clusters.","In doing so, we find that improvements in forecast accuracy that accrue from using clustering do not arise from grouping together similar series but from the structure of the hierarchy.","Third, we propose an approach based on averaging forecasts across hierarchies constructed using different clustering methods, that is shown to outperform any single clustering method.","All analysis is carried out on two benchmark datasets and a simulated dataset.","Our findings provide new insights into the role of hierarchy construction in forecast reconciliation and offer valuable guidance on forecasting practice."],"url":"http://arxiv.org/abs/2404.06064v1","category":"stat.ME"}
{"created":"2024-04-09 06:47:44","title":"Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning","abstract":"Medical multi-modal pre-training has revealed promise in computer-aided diagnosis by leveraging large-scale unlabeled datasets. However, existing methods based on masked autoencoders mainly rely on data-level reconstruction tasks, but lack high-level semantic information. Furthermore, two significant heterogeneity challenges hinder the transfer of pre-trained knowledge to downstream tasks, \\textit{i.e.}, the distribution heterogeneity between pre-training data and downstream data, and the modality heterogeneity within downstream data. To address these challenges, we propose a Unified Medical Multi-modal Diagnostic (UMD) framework with tailored pre-training and downstream tuning strategies. Specifically, to enhance the representation abilities of vision and language encoders, we propose the Multi-level Reconstruction Pre-training (MR-Pretrain) strategy, including a feature-level and data-level reconstruction, which guides models to capture the semantic information from masked inputs of different modalities. Moreover, to tackle two kinds of heterogeneities during the downstream tuning, we present the heterogeneity-combat downstream tuning strategy, which consists of a Task-oriented Distribution Calibration (TD-Calib) and a Gradient-guided Modality Coordination (GM-Coord). In particular, TD-Calib fine-tunes the pre-trained model regarding the distribution of downstream datasets, and GM-Coord adjusts the gradient weights according to the dynamic optimization status of different modalities. Extensive experiments on five public medical datasets demonstrate the effectiveness of our UMD framework, which remarkably outperforms existing approaches on three kinds of downstream tasks.","sentences":["Medical multi-modal pre-training has revealed promise in computer-aided diagnosis by leveraging large-scale unlabeled datasets.","However, existing methods based on masked autoencoders mainly rely on data-level reconstruction tasks, but lack high-level semantic information.","Furthermore, two significant heterogeneity challenges hinder the transfer of pre-trained knowledge to downstream tasks, \\textit{i.e.}, the distribution heterogeneity between pre-training data and downstream data, and the modality heterogeneity within downstream data.","To address these challenges, we propose a Unified Medical Multi-modal Diagnostic (UMD) framework with tailored pre-training and downstream tuning strategies.","Specifically, to enhance the representation abilities of vision and language encoders, we propose the Multi-level Reconstruction Pre-training (MR-Pretrain) strategy, including a feature-level and data-level reconstruction, which guides models to capture the semantic information from masked inputs of different modalities.","Moreover, to tackle two kinds of heterogeneities during the downstream tuning, we present the heterogeneity-combat downstream tuning strategy, which consists of a Task-oriented Distribution Calibration (TD-Calib) and a Gradient-guided Modality Coordination (GM-Coord).","In particular, TD-Calib fine-tunes the pre-trained model regarding the distribution of downstream datasets, and GM-Coord adjusts the gradient weights according to the dynamic optimization status of different modalities.","Extensive experiments on five public medical datasets demonstrate the effectiveness of our UMD framework, which remarkably outperforms existing approaches on three kinds of downstream tasks."],"url":"http://arxiv.org/abs/2404.06057v1","category":"cs.CV"}
{"created":"2024-04-09 01:33:57","title":"fastcpd: Fast Change Point Detection in R","abstract":"Change point analysis is concerned with detecting and locating structure breaks in the underlying model of a sequence of observations ordered by time, space or other variables. A widely adopted approach for change point analysis is to minimize an objective function with a penalty term on the number of change points. This framework includes several well-established procedures, such as the penalized log-likelihood using the (modified) Bayesian information criterion (BIC) or the minimum description length (MDL). The resulting optimization problem can be solved in polynomial time by dynamic programming or its improved version, such as the Pruned Exact Linear Time (PELT) algorithm (Killick, Fearnhead, and Eckley 2012). However, existing computational methods often suffer from two primary limitations: (1) methods based on direct implementation of dynamic programming or PELT are often time-consuming for long data sequences due to repeated computation of the cost value over different segments of the data sequence; (2) state-of-the-art R packages do not provide enough flexibility for users to handle different change point settings and models. In this work, we present the fastcpd package, aiming to provide an efficient and versatile framework for change point detection in several commonly encountered settings. The core of our algorithm is built upon PELT and the sequential gradient descent method recently proposed by Zhang and Dawn (2023). We illustrate the usage of the fastcpd package through several examples, including mean/variance changes in a (multivariate) Gaussian sequence, parameter changes in regression models, structural breaks in ARMA/GARCH/VAR models, and changes in user-specified models.","sentences":["Change point analysis is concerned with detecting and locating structure breaks in the underlying model of a sequence of observations ordered by time, space or other variables.","A widely adopted approach for change point analysis is to minimize an objective function with a penalty term on the number of change points.","This framework includes several well-established procedures, such as the penalized log-likelihood using the (modified) Bayesian information criterion (BIC) or the minimum description length (MDL).","The resulting optimization problem can be solved in polynomial time by dynamic programming or its improved version, such as the Pruned Exact Linear Time (PELT) algorithm (Killick, Fearnhead, and Eckley 2012).","However, existing computational methods often suffer from two primary limitations: (1) methods based on direct implementation of dynamic programming or PELT are often time-consuming for long data sequences due to repeated computation of the cost value over different segments of the data sequence; (2) state-of-the-art R packages do not provide enough flexibility for users to handle different change point settings and models.","In this work, we present the fastcpd package, aiming to provide an efficient and versatile framework for change point detection in several commonly encountered settings.","The core of our algorithm is built upon PELT and the sequential gradient descent method recently proposed by Zhang and Dawn (2023).","We illustrate the usage of the fastcpd package through several examples, including mean/variance changes in a (multivariate) Gaussian sequence, parameter changes in regression models, structural breaks in ARMA/GARCH/VAR models, and changes in user-specified models."],"url":"http://arxiv.org/abs/2404.05933v1","category":"stat.ME"}
{"created":"2024-04-09 01:14:39","title":"A note on trigonometric polynomials for lower bounds of $\u03b6(s)$","abstract":"Non-negative trigonometric polynomials satisfying certain properties are employed when studying a number of aspects of the Riemann zeta function. When establishing zero-free regions in the critical strip, the classical polynomial $3+4\\cos(\\theta)+\\cos(2\\theta)$ used by de la Vall\\'ee Poussin has since been replaced by more beneficial polynomials with larger degree. The classical polynomial was also employed by Titchmarsh to provide a lower bound on $\\abs{\\zeta(\\sigma+it)}$ when $\\sigma>1$. We show that this polynomial is optimal for this purpose.","sentences":["Non-negative trigonometric polynomials satisfying certain properties are employed when studying a number of aspects of the Riemann zeta function.","When establishing zero-free regions in the critical strip, the classical polynomial $3+4\\cos(\\theta)+\\cos(2\\theta)$ used by de la Vall\\'ee","Poussin has since been replaced by more beneficial polynomials with larger degree.","The classical polynomial was also employed by Titchmarsh to provide a lower bound on $\\abs{\\zeta(\\sigma+it)}$ when $\\sigma>1$. We show that this polynomial is optimal for this purpose."],"url":"http://arxiv.org/abs/2404.05928v1","category":"math.NT"}
{"created":"2024-04-08 22:45:27","title":"Interference Reduction Design for Improved Multitarget Detection in ISAC Systems","abstract":"The advancement of wireless communication systems toward 5G and beyond is spurred by the demand for high data rates, exceedingly dependable low-latency communication, and extensive connectivity that aligns with sensing requisites such as advanced high-resolution sensing and target detection. Consequently, embedding sensing into communication has gained considerable attention. In this work, we propose an alternative approach for optimizing integrated sensing and communication (ISAC) waveform for target detection by concurrently maximizing the power of the communication signal at an intended user and minimizing the multi-user and sensing interference. We formulate the problem as a non-disciplined convex programming (NDCP) optimization and we use a distribution-based approach for interference cancellation. Precisely, we establish the distribution of the communication signal and the multi-user communication interference received by the intended user, and thereafter, we establish that the sensing interference can be distributed as a centralized Chi-squared if the sensing covariance matrix is idempotent. We design such a matrix based on the symmetrical idempotent property. Additionally, we propose a disciplined convex programming (DCP) form of the problem, and using successive convex approximation (SCA), we show that the solutions can reach a stable waveform for efficient target detection. Furthermore, we compare the proposed waveform with state of the art radar-communication waveform designs and demonstrate its superior performance by computer simulations.","sentences":["The advancement of wireless communication systems toward 5G and beyond is spurred by the demand for high data rates, exceedingly dependable low-latency communication, and extensive connectivity that aligns with sensing requisites such as advanced high-resolution sensing and target detection.","Consequently, embedding sensing into communication has gained considerable attention.","In this work, we propose an alternative approach for optimizing integrated sensing and communication (ISAC) waveform for target detection by concurrently maximizing the power of the communication signal at an intended user and minimizing the multi-user and sensing interference.","We formulate the problem as a non-disciplined convex programming (NDCP) optimization and we use a distribution-based approach for interference cancellation.","Precisely, we establish the distribution of the communication signal and the multi-user communication interference received by the intended user, and thereafter, we establish that the sensing interference can be distributed as a centralized Chi-squared if the sensing covariance matrix is idempotent.","We design such a matrix based on the symmetrical idempotent property.","Additionally, we propose a disciplined convex programming (DCP) form of the problem, and using successive convex approximation (SCA), we show that the solutions can reach a stable waveform for efficient target detection.","Furthermore, we compare the proposed waveform with state of the art radar-communication waveform designs and demonstrate its superior performance by computer simulations."],"url":"http://arxiv.org/abs/2404.05895v1","category":"cs.IT"}
{"created":"2024-04-08 21:50:23","title":"Design of Transit-Centric Multimodal Urban Mobility System with Autonomous Mobility-on-Demand","abstract":"This paper addresses the pressing challenge of urban mobility in the context of growing urban populations, changing demand patterns for urban mobility, and emerging technologies like Mobility-on-Demand (MoD) platforms and Autonomous Vehicle (AV). As urban areas swell and demand pattern changes, the integration of Autonomous Mobility-on-Demand (AMoD) systems with existing public transit (PT) networks presents great opportunities to enhancing urban mobility. We propose a novel optimization framework for solving the Transit-Centric Multimodal Urban Mobility with Autonomous Mobility-on-Demand (TCMUM-AMoD) at scale. The system operator (public transit agency) determines the network design and frequency settings of the PT network, fleet sizing and allocations of AMoD system, and the pricing for using the multimodal system with the goal of minimizing passenger disutility. Passengers' mode and route choice behaviors are modeled explicitly using discrete choice models. A first-order approximation algorithm is introduced to solve the problem at scale. Using a case study in Chicago, we showcase the potential to optimize urban mobility across different demand scenarios. To our knowledge, ours is the first paper to jointly optimize transit network design, fleet sizing, and pricing for the multimodal mobility system while considering passengers' mode and route choices.","sentences":["This paper addresses the pressing challenge of urban mobility in the context of growing urban populations, changing demand patterns for urban mobility, and emerging technologies like Mobility-on-Demand (MoD) platforms and Autonomous Vehicle (AV).","As urban areas swell and demand pattern changes, the integration of Autonomous Mobility-on-Demand (AMoD) systems with existing public transit (PT) networks presents great opportunities to enhancing urban mobility.","We propose a novel optimization framework for solving the Transit-Centric Multimodal Urban Mobility with Autonomous Mobility-on-Demand (TCMUM-AMoD) at scale.","The system operator (public transit agency) determines the network design and frequency settings of the PT network, fleet sizing and allocations of AMoD system, and the pricing for using the multimodal system with the goal of minimizing passenger disutility.","Passengers' mode and route choice behaviors are modeled explicitly using discrete choice models.","A first-order approximation algorithm is introduced to solve the problem at scale.","Using a case study in Chicago, we showcase the potential to optimize urban mobility across different demand scenarios.","To our knowledge, ours is the first paper to jointly optimize transit network design, fleet sizing, and pricing for the multimodal mobility system while considering passengers' mode and route choices."],"url":"http://arxiv.org/abs/2404.05885v1","category":"cs.SI"}
{"created":"2024-04-08 20:42:20","title":"Box Filtration","abstract":"We define a new framework that unifies the filtration and mapper approaches from TDA, and present efficient algorithms to compute it. Termed the box filtration of a PCD, we grow boxes (hyperrectangles) that are not necessarily centered at each point (in place of balls centered at points). We grow the boxes non-uniformly and asymmetrically in different dimensions based on the distribution of points. We present two approaches to handle the boxes: a point cover where each point is assigned its own box at start, and a pixel cover that works with a pixelization of the space of the PCD. Any box cover in either setting automatically gives a mapper of the PCD. We show that the persistence diagrams generated by the box filtration using both point and pixel covers satisfy the classical stability based on the Gromov-Hausdorff distance. Using boxes also implies that the box filtration is identical for pairwise or higher order intersections whereas the VR and Cech filtration are not the same.   Growth in each dimension is computed by solving a linear program (LP) that optimizes a cost functional balancing the cost of expansion and benefit of including more points in the box. The box filtration algorithm runs in $O(m|U(0)|\\log(mn\\pi)L(q))$ time, where $m$ is number of steps of increments considered for box growth, $|U(0)|$ is the number of boxes in the initial cover ($\\leq$ number of points), $\\pi$ is the step length for increasing each box dimension, each LP is solved in $O(L(q))$ time, $n$ is the PCD dimension, and $q = n \\times |X|$. We demonstrate through multiple examples that the box filtration can produce more accurate results to summarize the topology of the PCD than VR and distance-to-measure (DTM) filtrations. Software for our implementation is available at https://github.com/pragup/Box-Filteration.","sentences":["We define a new framework that unifies the filtration and mapper approaches from TDA, and present efficient algorithms to compute it.","Termed the box filtration of a PCD, we grow boxes (hyperrectangles) that are not necessarily centered at each point (in place of balls centered at points).","We grow the boxes non-uniformly and asymmetrically in different dimensions based on the distribution of points.","We present two approaches to handle the boxes: a point cover where each point is assigned its own box at start, and a pixel cover that works with a pixelization of the space of the PCD.","Any box cover in either setting automatically gives a mapper of the PCD.","We show that the persistence diagrams generated by the box filtration using both point and pixel covers satisfy the classical stability based on the Gromov-Hausdorff distance.","Using boxes also implies that the box filtration is identical for pairwise or higher order intersections whereas the VR and Cech filtration are not the same.   ","Growth in each dimension is computed by solving a linear program (LP) that optimizes a cost functional balancing the cost of expansion and benefit of including more points in the box.","The box filtration algorithm runs in $O(m|U(0)|\\log(mn\\pi)L(q))$ time, where $m$ is number of steps of increments considered for box growth, $|U(0)|$ is the number of boxes in the initial cover ($\\leq$ number of points), $\\pi$ is the step length for increasing each box dimension, each LP is solved in $O(L(q))$ time, $n$ is the PCD dimension, and $q = n \\times |X|$.","We demonstrate through multiple examples that the box filtration can produce more accurate results to summarize the topology of the PCD than VR and distance-to-measure (DTM) filtrations.","Software for our implementation is available at https://github.com/pragup/Box-Filteration."],"url":"http://arxiv.org/abs/2404.05859v1","category":"cs.CG"}
{"created":"2024-04-08 20:10:42","title":"General Lotto Games with Scouts: Information versus Strength","abstract":"We introduce General Lotto games with Scouts: a General Lotto game with asymmetric information. There are two players, Red and Blue, who both allocate resources to a field. However, scouting capabilities afford Blue to gain information, with some probability, on the number of Red's resources before allocating his own. We derive optimal strategies for this game in the case of a single field. In addition we provide upper and lower bounds of the value of the game in a multi-stage case with multiple battlefields. We devise several ways to characterise the influence of information versus strength. We conclude by drawing qualitative insights from these characterisations and the game values, and draw parallels with military practice.","sentences":["We introduce General Lotto games with Scouts: a General Lotto game with asymmetric information.","There are two players, Red and Blue, who both allocate resources to a field.","However, scouting capabilities afford Blue to gain information, with some probability, on the number of Red's resources before allocating his own.","We derive optimal strategies for this game in the case of a single field.","In addition we provide upper and lower bounds of the value of the game in a multi-stage case with multiple battlefields.","We devise several ways to characterise the influence of information versus strength.","We conclude by drawing qualitative insights from these characterisations and the game values, and draw parallels with military practice."],"url":"http://arxiv.org/abs/2404.05841v1","category":"cs.GT"}
{"created":"2024-04-08 18:00:00","title":"Hybrid Tree Tensor Networks for quantum simulation","abstract":"Hybrid Tensor Networks (hTN) offer a promising solution for encoding variational quantum states beyond the capabilities of efficient classical methods or noisy quantum computers alone. However, their practical usefulness and many operational aspects of hTN-based algorithms, like the optimization of hTNs, the generalization of standard contraction rules to an hybrid setting, and the design of application-oriented architectures have not been thoroughly investigated yet. In this work, we introduce a novel algorithm to perform ground state optimizations with hybrid Tree Tensor Networks (hTTNs), discussing its advantages and roadblocks, and identifying a set of promising applications. We benchmark our approach on two paradigmatic models, namely the Ising model at the critical point and the Toric code Hamiltonian. In both cases, we successfully demonstrate that hTTNs can improve upon classical equivalents with equal bond dimension in the classical part.","sentences":["Hybrid Tensor Networks (hTN) offer a promising solution for encoding variational quantum states beyond the capabilities of efficient classical methods or noisy quantum computers alone.","However, their practical usefulness and many operational aspects of hTN-based algorithms, like the optimization of hTNs, the generalization of standard contraction rules to an hybrid setting, and the design of application-oriented architectures have not been thoroughly investigated yet.","In this work, we introduce a novel algorithm to perform ground state optimizations with hybrid Tree Tensor Networks (hTTNs), discussing its advantages and roadblocks, and identifying a set of promising applications.","We benchmark our approach on two paradigmatic models, namely the Ising model at the critical point and the Toric code Hamiltonian.","In both cases, we successfully demonstrate that hTTNs can improve upon classical equivalents with equal bond dimension in the classical part."],"url":"http://arxiv.org/abs/2404.05784v1","category":"quant-ph"}
{"created":"2024-04-08 17:50:25","title":"Enhance Low-Carbon Power System Operation via Carbon-Aware Demand Response","abstract":"As the electrification process advances, enormous power flexibility is becoming available on the demand side, which can be harnessed to facilitate power system decarbonization. Hence, this paper studies the carbon-aware demand response (C-DR) paradigm, where individual users aim to minimize their carbon footprints through the optimal scheduling of flexible load devices. The specific operational dynamics and constraints of deferrable loads and thermostatically controlled loads are considered, and the carbon emission flow method is employed to determine users' carbon footprints using nodal carbon intensities. Then, an optimal power dispatch model that integrates the C-DR mechanism is proposed for low-carbon power system operation, based on the carbon-aware optimal power flow (C-OPF) method. Two solution algorithms, including a centralized Karush-Kuhn-Tucker (KKT) reformulation algorithm and an iterative solution algorithm, are developed to solve the bi-level power dispatch optimization model. Numerical simulations on the IEEE New England 39-bus system demonstrate the effectiveness of the proposed methods.","sentences":["As the electrification process advances, enormous power flexibility is becoming available on the demand side, which can be harnessed to facilitate power system decarbonization.","Hence, this paper studies the carbon-aware demand response (C-DR) paradigm, where individual users aim to minimize their carbon footprints through the optimal scheduling of flexible load devices.","The specific operational dynamics and constraints of deferrable loads and thermostatically controlled loads are considered, and the carbon emission flow method is employed to determine users' carbon footprints using nodal carbon intensities.","Then, an optimal power dispatch model that integrates the C-DR mechanism is proposed for low-carbon power system operation, based on the carbon-aware optimal power flow (C-OPF) method.","Two solution algorithms, including a centralized Karush-Kuhn-Tucker (KKT) reformulation algorithm and an iterative solution algorithm, are developed to solve the bi-level power dispatch optimization model.","Numerical simulations on the IEEE New England 39-bus system demonstrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.05713v1","category":"eess.SY"}
{"created":"2024-04-08 17:31:59","title":"Boundary regularity of the free interface in spectral optimal partition problems","abstract":"We consider the problem of optimal partition of a domain with respect to the sum of the principal eigenvalues and we prove for the first time regularity results for the free interface up to fixed boundary. All our results are quantitative and, in particular, we obtain fine estimates on the continuity of the solutions and the oscillation of the free interface (in terms of the modulus of continuity of the normal vector of the fixed boundary), even in the case of domains with low (Dini-type) regularity. Our analysis is based on an Almgren-type monotonicity formula at boundary points and an epiperimetric inequality at points of low frequency, which, together, yield an explicit rate of convergence for blow-up sequences and the boundary strong unique continuation property. Exploiting our quantitative blow-up analysis, we manage to prove clean-up results near one-phase and two-phase points. We define the notion of free interface inside the fixed boundary, and we prove that the subset of points of minimal frequency is regular and that the interior free interface is approaching the boundary orthogonally in a smooth way.","sentences":["We consider the problem of optimal partition of a domain with respect to the sum of the principal eigenvalues and we prove for the first time regularity results for the free interface up to fixed boundary.","All our results are quantitative and, in particular, we obtain fine estimates on the continuity of the solutions and the oscillation of the free interface (in terms of the modulus of continuity of the normal vector of the fixed boundary), even in the case of domains with low (Dini-type) regularity.","Our analysis is based on an Almgren-type monotonicity formula at boundary points and an epiperimetric inequality at points of low frequency, which, together, yield an explicit rate of convergence for blow-up sequences and the boundary strong unique continuation property.","Exploiting our quantitative blow-up analysis, we manage to prove clean-up results near one-phase and two-phase points.","We define the notion of free interface inside the fixed boundary, and we prove that the subset of points of minimal frequency is regular and that the interior free interface is approaching the boundary orthogonally in a smooth way."],"url":"http://arxiv.org/abs/2404.05698v1","category":"math.AP"}
{"created":"2024-04-08 16:59:51","title":"From enrollment to exams: Perceived stress dynamics among first-year physics students","abstract":"The current dropout rate in physics studies in Germany is about 60%, with the majority of dropouts occurring in the first year. Consequently, the physics study entry phase poses a significant challenge for many students. Students' stress perception can provide more profound insights into the processes and challenges during that period. In a panel study featuring 67 measuring points involving up to 128 participants at each point, we investigated the students' stress perception with the Perceived Stress Questionnaire (PSQ), identified underlying sources of stress, and assessed the self-estimated workload across two different cohorts. This examination occurred mostly every week during the first, and for one cohort also in the second semester, yielding a total of 3,206 PSQ data points and 5,823 stressors. The PSQ data indicate a consistent stress trajectory across all three groups studied that is characterized by significant dynamics between measuring points, spanning from $M=20.1, SD=15.9$ to $M=63.6, SD=13.4$ on the scale from 0 to 100. The stress level rises in the first lecture weeks, followed by a stable, elevated stress level until the exams and a relaxation phase afterward during the lecture-free time and Christmas vacation. In the first half of the lecture period, students primarily indicate the weekly exercise sheets, the physics lab course, and math courses as stressors; later on, preparation for exams and the exams themselves emerge as the most important stressors. Together with the students' self-estimated workload that correlates with the PSQ score, we can depict a coherent picture of stress perception among first-year physics students. This study enhances the understanding of stress perception and its potential management.","sentences":["The current dropout rate in physics studies in Germany is about 60%, with the majority of dropouts occurring in the first year.","Consequently, the physics study entry phase poses a significant challenge for many students.","Students' stress perception can provide more profound insights into the processes and challenges during that period.","In a panel study featuring 67 measuring points involving up to 128 participants at each point, we investigated the students' stress perception with the Perceived Stress Questionnaire (PSQ), identified underlying sources of stress, and assessed the self-estimated workload across two different cohorts.","This examination occurred mostly every week during the first, and for one cohort also in the second semester, yielding a total of 3,206 PSQ data points and 5,823 stressors.","The PSQ data indicate a consistent stress trajectory across all three groups studied that is characterized by significant dynamics between measuring points, spanning from $M=20.1, SD=15.9$ to $M=63.6, SD=13.4$ on the scale from 0 to 100.","The stress level rises in the first lecture weeks, followed by a stable, elevated stress level until the exams and a relaxation phase afterward during the lecture-free time and Christmas vacation.","In the first half of the lecture period, students primarily indicate the weekly exercise sheets, the physics lab course, and math courses as stressors; later on, preparation for exams and the exams themselves emerge as the most important stressors.","Together with the students' self-estimated workload that correlates with the PSQ score, we can depict a coherent picture of stress perception among first-year physics students.","This study enhances the understanding of stress perception and its potential management."],"url":"http://arxiv.org/abs/2404.05682v1","category":"physics.ed-ph"}
{"created":"2024-04-08 16:59:39","title":"Even Faster Knapsack via Rectangular Monotone Min-Plus Convolution and Balancing","abstract":"We present a pseudopolynomial-time algorithm for the Knapsack problem that has running time $\\widetilde{O}(n + t\\sqrt{p_{\\max}})$, where $n$ is the number of items, $t$ is the knapsack capacity, and $p_{\\max}$ is the maximum item profit. This improves over the $\\widetilde{O}(n + t \\, p_{\\max})$-time algorithm based on the convolution and prediction technique by Bateni et al.~(STOC 2018). Moreover, we give some evidence, based on a strengthening of the Min-Plus Convolution Hypothesis, that our running time might be optimal.   Our algorithm uses two new technical tools, which might be of independent interest. First, we generalize the $\\widetilde{O}(n^{1.5})$-time algorithm for bounded monotone min-plus convolution by Chi et al.~(STOC 2022) to the \\emph{rectangular} case where the range of entries can be different from the sequence length. Second, we give a reduction from general knapsack instances to \\emph{balanced} instances, where all items have nearly the same profit-to-weight ratio, up to a constant factor.   Using these techniques, we can also obtain algorithms that run in time $\\widetilde{O}(n + OPT\\sqrt{w_{\\max}})$, $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3}t^{2/3})$, and $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3} OPT^{2/3})$, where $OPT$ is the optimal total profit and $w_{\\max}$ is the maximum item weight.","sentences":["We present a pseudopolynomial-time algorithm for the Knapsack problem that has running time $\\widetilde{O}(n + t\\sqrt{p_{\\max}})$, where $n$ is the number of items, $t$ is the knapsack capacity, and $p_{\\max}$ is the maximum item profit.","This improves over the $\\widetilde{O}(n + t \\, p_{\\max})$-time algorithm based on the convolution and prediction technique by Bateni et al.~(STOC 2018).","Moreover, we give some evidence, based on a strengthening of the Min-Plus Convolution Hypothesis, that our running time might be optimal.   ","Our algorithm uses two new technical tools, which might be of independent interest.","First, we generalize the $\\widetilde{O}(n^{1.5})$-time algorithm for bounded monotone min-plus convolution by Chi et al.~(STOC 2022) to the \\emph{rectangular} case where the range of entries can be different from the sequence length.","Second, we give a reduction from general knapsack instances to \\emph{balanced} instances, where all items have nearly the same profit-to-weight ratio, up to a constant factor.   ","Using these techniques, we can also obtain algorithms that run in time $\\widetilde{O}(n + OPT\\sqrt{w_{\\max}})$, $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3}t^{2/3})$, and $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3} OPT^{2/3})$, where $OPT$ is the optimal total profit and $w_{\\max}$ is the maximum item weight."],"url":"http://arxiv.org/abs/2404.05681v1","category":"cs.DS"}
{"created":"2024-04-08 16:56:45","title":"Pulsar Timing Array Harmonic Analysis and Source Angular Correlations","abstract":"Gravitational waves (GW) influence the arrival times of radio signals coming from pulsars. Here, we investigate the harmonic space approach to describing the pulsar response to a GW. We derive and discuss the \"diagonalized form\" of the response, which is a sum of spin-2-weighted spherical harmonics of the GW direction multiplied by normal (spin-weight 0) spherical harmonics of the pulsar direction. We show how this allows many useful objects, for example the Hellings and Downs two-point function, to be easily calculated. The approach also provides a clear description of the gauge dependence. We then employ this harmonic approach to model the effects of angular correlations in the sky locations of GW sources (sometimes called \"statistical isotropy\"). To do this, we construct rotationally invariant ensembles made up of many Gaussian subensembles, each of which breaks rotational invariance. Using harmonic techniques, we compute the cosmic covariance and the total covariance of the Hellings and Downs correlation in these models. The results may be used to assess the impact of angular source correlations on the Hellings and Downs correlation, and for optimal reconstruction of the Hellings and Downs curve in models where GW sources have correlated sky locations.","sentences":["Gravitational waves (GW) influence the arrival times of radio signals coming from pulsars.","Here, we investigate the harmonic space approach to describing the pulsar response to a GW.","We derive and discuss the \"diagonalized form\" of the response, which is a sum of spin-2-weighted spherical harmonics of the GW direction multiplied by normal (spin-weight 0) spherical harmonics of the pulsar direction.","We show how this allows many useful objects, for example the Hellings and Downs two-point function, to be easily calculated.","The approach also provides a clear description of the gauge dependence.","We then employ this harmonic approach to model the effects of angular correlations in the sky locations of GW sources (sometimes called \"statistical isotropy\").","To do this, we construct rotationally invariant ensembles made up of many Gaussian subensembles, each of which breaks rotational invariance.","Using harmonic techniques, we compute the cosmic covariance and the total covariance of the Hellings and Downs correlation in these models.","The results may be used to assess the impact of angular source correlations on the Hellings and Downs correlation, and for optimal reconstruction of the Hellings and Downs curve in models where GW sources have correlated sky locations."],"url":"http://arxiv.org/abs/2404.05677v1","category":"gr-qc"}
{"created":"2024-04-08 16:46:47","title":"Significant Photoluminescence Improvements from Bulk Germanium-Based Thin Films with Ultra-low Threading Dislocation Densities","abstract":"Bulk Ge crystals, characterized by significantly lower threading dislocation densities (TDD) than their epitaxial counterparts, emerge as optimal candidates for studying and improving Ge laser performance. Our study focused on the Ge thickness and TDD impacts on Ge's photoluminescence (PL). The PL peak intensity of a bulk Ge sample (TDD = 6000 cm^(-2), n-doping = 10^16 cm^(-3)) experiences a remarkable 32-fold increase as the thickness is reduced from 535 to 2 micron. This surpasses the PL peak intensity of a 0.75 micron thick epi-Ge on Si (biaxial tensile strain= 0.2 %, n-doping = 10^19 cm^(-3)) by a factor of 2.5. Furthermore, the PL peak intensity of a 405 micron thick zero-TDD bulk Ge sample (n-doping = 5 * 10^17 cm^(-3)) is ten times that of the 0.75 micron thick epi-Ge, rising to twelve times when thinned to 1 micron. The TDD reduction method is highly effective, which relaxes the requirements of high n-doping and stress in enhancing Ge laser performance and thus reduces the side effects of high optical absorption, high non-radiative recombination, bandgap narrowing, and large footprints associated with these two techniques.","sentences":["Bulk Ge crystals, characterized by significantly lower threading dislocation densities (TDD) than their epitaxial counterparts, emerge as optimal candidates for studying and improving Ge laser performance.","Our study focused on the Ge thickness and TDD impacts on Ge's photoluminescence (PL).","The PL peak intensity of a bulk Ge sample (TDD = 6000 cm^(-2), n-doping = 10^16 cm^(-3)) experiences a remarkable 32-fold increase as the thickness is reduced from 535 to 2 micron.","This surpasses the PL peak intensity of a 0.75 micron thick epi-Ge on Si (biaxial tensile strain= 0.2 %, n-doping = 10^19 cm^(-3))","by a factor of 2.5.","Furthermore, the PL peak intensity of a 405 micron thick zero-TDD bulk Ge sample (n-doping = 5 * 10^17 cm^(-3)) is ten times that of the 0.75 micron thick epi-Ge, rising to twelve times when thinned to 1 micron.","The TDD reduction method is highly effective, which relaxes the requirements of high n-doping and stress in enhancing Ge laser performance and thus reduces the side effects of high optical absorption, high non-radiative recombination, bandgap narrowing, and large footprints associated with these two techniques."],"url":"http://arxiv.org/abs/2404.05663v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 16:46:07","title":"Automatic Controllable Colorization via Imagination","abstract":"We propose a framework for automatic colorization that allows for iterative editing and modifications. The core of our framework lies in an imagination module: by understanding the content within a grayscale image, we utilize a pre-trained image generation model to generate multiple images that contain the same content. These images serve as references for coloring, mimicking the process of human experts. As the synthesized images can be imperfect or different from the original grayscale image, we propose a Reference Refinement Module to select the optimal reference composition. Unlike most previous end-to-end automatic colorization algorithms, our framework allows for iterative and localized modifications of the colorization results because we explicitly model the coloring samples. Extensive experiments demonstrate the superiority of our framework over existing automatic colorization algorithms in editability and flexibility. Project page: https://xy-cong.github.io/imagine-colorization.","sentences":["We propose a framework for automatic colorization that allows for iterative editing and modifications.","The core of our framework lies in an imagination module: by understanding the content within a grayscale image, we utilize a pre-trained image generation model to generate multiple images that contain the same content.","These images serve as references for coloring, mimicking the process of human experts.","As the synthesized images can be imperfect or different from the original grayscale image, we propose a Reference Refinement Module to select the optimal reference composition.","Unlike most previous end-to-end automatic colorization algorithms, our framework allows for iterative and localized modifications of the colorization results because we explicitly model the coloring samples.","Extensive experiments demonstrate the superiority of our framework over existing automatic colorization algorithms in editability and flexibility.","Project page: https://xy-cong.github.io/imagine-colorization."],"url":"http://arxiv.org/abs/2404.05661v1","category":"cs.CV"}
{"created":"2024-04-08 16:11:15","title":"Semi-Infinite Programs for Robust Control and Optimization: Efficient Solutions and Extensions to Existence Constraints","abstract":"Discrete-time robust optimal control problems generally take a min-max structure over continuous variable spaces, which can be difficult to solve in practice. In this paper, we extend the class of such problems that can be solved through a previously proposed local reduction method to consider those with existence constraints on the uncountable variables. We also consider the possibility of non-unique trajectories that satisfy equality and inequality constraints. Crucially, we show that the problems of interest can be cast into a standard semi-infinite program and demonstrate how to generate optimal uncertainty scenario sets in order to obtain numerical solutions. We also include examples on model predictive control for obstacle avoidance with logical conditions, control with input saturation affected by uncertainty, and optimal parameter estimation to highlight the need for the proposed extension. Our method solves each of the examples considered, producing violation-free and locally optimal solutions.","sentences":["Discrete-time robust optimal control problems generally take a min-max structure over continuous variable spaces, which can be difficult to solve in practice.","In this paper, we extend the class of such problems that can be solved through a previously proposed local reduction method to consider those with existence constraints on the uncountable variables.","We also consider the possibility of non-unique trajectories that satisfy equality and inequality constraints.","Crucially, we show that the problems of interest can be cast into a standard semi-infinite program and demonstrate how to generate optimal uncertainty scenario sets in order to obtain numerical solutions.","We also include examples on model predictive control for obstacle avoidance with logical conditions, control with input saturation affected by uncertainty, and optimal parameter estimation to highlight the need for the proposed extension.","Our method solves each of the examples considered, producing violation-free and locally optimal solutions."],"url":"http://arxiv.org/abs/2404.05635v1","category":"math.OC"}
{"created":"2024-04-08 15:57:31","title":"Robust Control using Control Lyapunov Function and Hamilton-Jacobi Reachability","abstract":"The paper presents a robust control technique that combines the Control Lyapunov function and Hamilton-Jacobi Reachability to compute a controller and its Region of Attraction (ROA). The Control Lyapunov function uses a linear system model with an assumed additive uncertainty to calculate a control gain and the level sets of the ROA as a function of the uncertainty. Next, Hamilton-Jacobi reachability uses the nonlinear model with the modeled uncertainty, which need not be additive, to compute the backward reachable set (BRS). Finally, by juxtaposing the level sets of the ROA with BRS, we can calculate the worst-case additive disturbance and the ROA of the nonlinear model. We illustrate our approach on a 2D quadcopter tracking trajectory and a 2D quadcopter with height and velocity regulation in simulation.","sentences":["The paper presents a robust control technique that combines the Control Lyapunov function and Hamilton-Jacobi Reachability to compute a controller and its Region of Attraction (ROA).","The Control Lyapunov function uses a linear system model with an assumed additive uncertainty to calculate a control gain and the level sets of the ROA as a function of the uncertainty.","Next, Hamilton-Jacobi reachability uses the nonlinear model with the modeled uncertainty, which need not be additive, to compute the backward reachable set (BRS).","Finally, by juxtaposing the level sets of the ROA with BRS, we can calculate the worst-case additive disturbance and the ROA of the nonlinear model.","We illustrate our approach on a 2D quadcopter tracking trajectory and a 2D quadcopter with height and velocity regulation in simulation."],"url":"http://arxiv.org/abs/2404.05625v1","category":"cs.RO"}
{"created":"2024-04-08 15:33:32","title":"Feedback Stability Under Mixed Gain and Phase Uncertainty","abstract":"In this study, we investigate the robust feedback stability problem for multiple-input-multiple-output linear time-invariant systems involving sectored-disk uncertainty, namely, dynamic uncertainty subject to simultaneous gain and phase constraints. This problem is thereby called a sectored-disk problem. Employing a frequency-wise analysis approach, we derive a fundamental static matrix problem that serves as a key component in addressing the feedback stability. The study of this matrix problem heavily relies on the Davis-Wielandt (DW) shells of matrices, providing a profound insight into matrices subjected to simultaneous gain and phase constraints. This understanding is pivotal for establishing a less conservative sufficient condition for the matrix sectored-disk problem, from which we formulate several robust feedback stability conditions against sectored-disk uncertainty. Finally, several conditions based on linear matrix inequalities are developed for efficient computation and verification of feedback robust stability against sectored-disk uncertainty.","sentences":["In this study, we investigate the robust feedback stability problem for multiple-input-multiple-output linear time-invariant systems involving sectored-disk uncertainty, namely, dynamic uncertainty subject to simultaneous gain and phase constraints.","This problem is thereby called a sectored-disk problem.","Employing a frequency-wise analysis approach, we derive a fundamental static matrix problem that serves as a key component in addressing the feedback stability.","The study of this matrix problem heavily relies on the Davis-Wielandt (DW) shells of matrices, providing a profound insight into matrices subjected to simultaneous gain and phase constraints.","This understanding is pivotal for establishing a less conservative sufficient condition for the matrix sectored-disk problem, from which we formulate several robust feedback stability conditions against sectored-disk uncertainty.","Finally, several conditions based on linear matrix inequalities are developed for efficient computation and verification of feedback robust stability against sectored-disk uncertainty."],"url":"http://arxiv.org/abs/2404.05609v1","category":"math.OC"}
{"created":"2024-04-08 15:14:20","title":"UniFL: Improve Stable Diffusion via Unified Feedback Learning","abstract":"Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff.","sentences":["Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications.","However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight.","To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively.","UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL.","Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed.","In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration.","For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference.","Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff."],"url":"http://arxiv.org/abs/2404.05595v1","category":"cs.CV"}
{"created":"2024-04-08 14:54:54","title":"Social-MAE: Social Masked Autoencoder for Multi-person Motion Representation Learning","abstract":"For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking. Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial. Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks. To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data. The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes. Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain. After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks. Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding. These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose.","sentences":["For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking.","Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial.","Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks.","To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data.","The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes.","Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain.","After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks.","Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding.","These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose."],"url":"http://arxiv.org/abs/2404.05578v1","category":"cs.CV"}
{"created":"2024-04-08 14:54:03","title":"On the Stability of swelling porous elastic soils with a single internal fractional damping","abstract":"We study polynomial stability to the one-dimensional system in the linear isothermal theory of swelling porous elastic soils with an internal fractional damping. We establish an optimal decay result by frequency domain method","sentences":["We study polynomial stability to the one-dimensional system in the linear isothermal theory of swelling porous elastic soils with an internal fractional damping.","We establish an optimal decay result by frequency domain method"],"url":"http://arxiv.org/abs/2404.05577v1","category":"math.AP"}
{"created":"2024-04-08 14:39:36","title":"Hausdorff Distance-Based Record Linkage for Improved Matching of Households and Individuals in Different Databases","abstract":"Matching households and individuals across different databases poses challenges due to the lack of unique identifiers, typographical errors, and changes in attributes over time. Record linkage tools play a crucial role in overcoming these difficulties. This paper presents a multi-step record linkage procedure that incorporates household information to enhance the entity-matching process across multiple databases. Our approach utilizes the Hausdorff distance to estimate the probability of a match between households in multiple files. Subsequently, probabilities of matching individuals within these households are computed using a logistic regression model based on attribute-level distances. These estimated probabilities are then employed in a linear programming optimization framework to infer one-to-one matches between individuals. To assess the efficacy of our method, we apply it to link data from the Italian Survey of Household Income and Wealth across different years. Through internal and external validation procedures, the proposed method is shown to provide a significant enhancement in the quality of the individual matching process, thanks to the incorporation of household information. A comparison with a standard record linkage approach based on direct matching of individuals, which neglects household information, underscores the advantages of accounting for such information.","sentences":["Matching households and individuals across different databases poses challenges due to the lack of unique identifiers, typographical errors, and changes in attributes over time.","Record linkage tools play a crucial role in overcoming these difficulties.","This paper presents a multi-step record linkage procedure that incorporates household information to enhance the entity-matching process across multiple databases.","Our approach utilizes the Hausdorff distance to estimate the probability of a match between households in multiple files.","Subsequently, probabilities of matching individuals within these households are computed using a logistic regression model based on attribute-level distances.","These estimated probabilities are then employed in a linear programming optimization framework to infer one-to-one matches between individuals.","To assess the efficacy of our method, we apply it to link data from the Italian Survey of Household Income and Wealth across different years.","Through internal and external validation procedures, the proposed method is shown to provide a significant enhancement in the quality of the individual matching process, thanks to the incorporation of household information.","A comparison with a standard record linkage approach based on direct matching of individuals, which neglects household information, underscores the advantages of accounting for such information."],"url":"http://arxiv.org/abs/2404.05566v1","category":"stat.AP"}
{"created":"2024-04-08 14:35:09","title":"Local analysis of the Kuznetsov formula and the density conjecture","abstract":"We prove Sarnak's spherical density conjecture for the principal congruence subgroup of SL(n, Z) of arbitrary level. Applications include a complete version of Sarnak's optimal lifting conjecture for principal congruence subgroups of SL(n, Z), as well as a transfer of the density theorem to certain co-compact situations. The main ingredients are new lower bounds for Whittaker functions and strong estimates for the cardinality of ramified Kloosterman sets.","sentences":["We prove Sarnak's spherical density conjecture for the principal congruence subgroup of SL(n, Z) of arbitrary level.","Applications include a complete version of Sarnak's optimal lifting conjecture for principal congruence subgroups of SL(n, Z), as well as a transfer of the density theorem to certain co-compact situations.","The main ingredients are new lower bounds for Whittaker functions and strong estimates for the cardinality of ramified Kloosterman sets."],"url":"http://arxiv.org/abs/2404.05561v1","category":"math.NT"}
{"created":"2024-04-08 14:16:05","title":"Information Sale on Network","abstract":"This paper studies a stylized model of a monopoly data seller when information-sharing network exists among data buyers. We show that, if the buyers' prior information is sufficiently noisy, the optimal selling strategy is characterized by a maximum independent set, which is the largest set of buyers who do not have information-sharing link at all. In addition, the precision of the seller's data decreases in the number of information-sharing links among buyers, but it is higher than the socially efficient level of precision.","sentences":["This paper studies a stylized model of a monopoly data seller when information-sharing network exists among data buyers.","We show that, if the buyers' prior information is sufficiently noisy, the optimal selling strategy is characterized by a maximum independent set, which is the largest set of buyers who do not have information-sharing link at all.","In addition, the precision of the seller's data decreases in the number of information-sharing links among buyers, but it is higher than the socially efficient level of precision."],"url":"http://arxiv.org/abs/2404.05546v1","category":"econ.TH"}
{"created":"2024-04-08 14:10:14","title":"Optimal Allocation of Tasks and Price of Anarchy of Distributed Optimization in Networked Computing Facilities","abstract":"The allocation of computing tasks for networked distributed services poses a question to service providers on whether centralized allocation management be worth its cost. Existing analytical models were conceived for users accessing computing resources with practically indistinguishable (hence irrelevant for the allocation decision) delays, which is typical of services located in the same distant data center. However, with the rise of the edge-cloud continuum, a simple analysis of the sojourn time that computing tasks observe at the server misses the impact of diverse latency values imposed by server locations. We therefore study the optimization of computing task allocation with a new model that considers both distance of servers and sojourn time in servers. We derive exact algorithms to optimize the system and we show, through numerical analysis and real experiments, that differences in server location in the edge-cloud continuum cannot be neglected. By means of algorithmic game theory, we study the price of anarchy of a distributed implementation of the computing task allocation problem and unveil important practical properties such as the fact that the price of anarchy tends to be small -- except when the system is overloaded -- and its maximum can be computed with low complexity.","sentences":["The allocation of computing tasks for networked distributed services poses a question to service providers on whether centralized allocation management be worth its cost.","Existing analytical models were conceived for users accessing computing resources with practically indistinguishable (hence irrelevant for the allocation decision) delays, which is typical of services located in the same distant data center.","However, with the rise of the edge-cloud continuum, a simple analysis of the sojourn time that computing tasks observe at the server misses the impact of diverse latency values imposed by server locations.","We therefore study the optimization of computing task allocation with a new model that considers both distance of servers and sojourn time in servers.","We derive exact algorithms to optimize the system and we show, through numerical analysis and real experiments, that differences in server location in the edge-cloud continuum cannot be neglected.","By means of algorithmic game theory, we study the price of anarchy of a distributed implementation of the computing task allocation problem and unveil important practical properties such as the fact that the price of anarchy tends to be small -- except when the system is overloaded -- and its maximum can be computed with low complexity."],"url":"http://arxiv.org/abs/2404.05543v1","category":"cs.GT"}
{"created":"2024-04-08 14:03:37","title":"On the Optimal MMSE Channel Estimation for One-Bit Quantized MIMO Systems","abstract":"This paper focuses on the minimum mean squared error (MMSE) channel estimator for multiple-input multiple-output (MIMO) systems with one-bit quantization at the receiver side. Despite its optimality and significance in estimation theory, the MMSE channel estimator has not been fully investigated in this context due to its general non-linearity and computational complexity. Instead, the typically suboptimal Bussgang linear MMSE (BLMMSE) estimator has been widely adopted. In this work, we develop a new framework to compute the MMSE channel estimator that hinges on computation of the orthant probability of the multivariate normal distribution. Based on this framework, we determine a necessary and sufficient condition for the BLMMSE channel estimator to be optimal and equivalent to the MMSE estimator. Under the assumption of specific channel correlation or pilot symbols, we further utilize the framework to derive analytical expressions for the MMSE channel estimator that are particularly convenient for computation when certain system dimensions become large, thereby enabling a comparison between the BLMMSE and MMSE channel estimators in these cases.","sentences":["This paper focuses on the minimum mean squared error (MMSE) channel estimator for multiple-input multiple-output (MIMO) systems with one-bit quantization at the receiver side.","Despite its optimality and significance in estimation theory, the MMSE channel estimator has not been fully investigated in this context due to its general non-linearity and computational complexity.","Instead, the typically suboptimal Bussgang linear MMSE (BLMMSE) estimator has been widely adopted.","In this work, we develop a new framework to compute the MMSE channel estimator that hinges on computation of the orthant probability of the multivariate normal distribution.","Based on this framework, we determine a necessary and sufficient condition for the BLMMSE channel estimator to be optimal and equivalent to the MMSE estimator.","Under the assumption of specific channel correlation or pilot symbols, we further utilize the framework to derive analytical expressions for the MMSE channel estimator that are particularly convenient for computation when certain system dimensions become large, thereby enabling a comparison between the BLMMSE and MMSE channel estimators in these cases."],"url":"http://arxiv.org/abs/2404.05536v1","category":"cs.IT"}
{"created":"2024-04-08 13:41:32","title":"The Fact Selection Problem in LLM-Based Program Repair","abstract":"Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.","sentences":["Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs).","Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs?","To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark.","Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial.","Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it.","Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes.","These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance.","We found that there is no one-size-fits-all set of facts for bug repair.","Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt.","This model significantly surpasses the performance of the best generic fact set.","To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods.","On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration."],"url":"http://arxiv.org/abs/2404.05520v2","category":"cs.SE"}
{"created":"2024-04-08 13:36:29","title":"Quantum Optimization Methods for Satellite Mission Planning","abstract":"Satellite mission planning for Earth observation satellites is a combinatorial optimization problem that consists of selecting the optimal subset of imaging requests, subject to constraints, to be fulfilled during an orbit pass of a satellite. The ever-growing amount of satellites in orbit underscores the need to operate them efficiently, which requires solving many instances of the problem in short periods of time. However, current classical algorithms often fail to find the global optimum or take too long to execute. Here, we approach the problem from a quantum computing point of view, which offers a promising alternative that could lead to significant improvements in solution quality or execution speed in the future. To this end, we study a planning problem with a variety of intricate constraints and discuss methods to encode them for quantum computers. Additionally, we experimentally assess the performance of quantum annealing and the quantum approximate optimization algorithm on a realistic and diverse dataset. Our results identify key aspects like graph connectivity and constraint structure that influence the performance of the methods. We explore the limits of today's quantum algorithms and hardware, providing bounds on the problems that can be currently solved successfully and showing how the solution degrades as the complexity grows. This work aims to serve as a baseline for further research in the field and establish realistic expectations on current quantum optimization capabilities.","sentences":["Satellite mission planning for Earth observation satellites is a combinatorial optimization problem that consists of selecting the optimal subset of imaging requests, subject to constraints, to be fulfilled during an orbit pass of a satellite.","The ever-growing amount of satellites in orbit underscores the need to operate them efficiently, which requires solving many instances of the problem in short periods of time.","However, current classical algorithms often fail to find the global optimum or take too long to execute.","Here, we approach the problem from a quantum computing point of view, which offers a promising alternative that could lead to significant improvements in solution quality or execution speed in the future.","To this end, we study a planning problem with a variety of intricate constraints and discuss methods to encode them for quantum computers.","Additionally, we experimentally assess the performance of quantum annealing and the quantum approximate optimization algorithm on a realistic and diverse dataset.","Our results identify key aspects like graph connectivity and constraint structure that influence the performance of the methods.","We explore the limits of today's quantum algorithms and hardware, providing bounds on the problems that can be currently solved successfully and showing how the solution degrades as the complexity grows.","This work aims to serve as a baseline for further research in the field and establish realistic expectations on current quantum optimization capabilities."],"url":"http://arxiv.org/abs/2404.05516v1","category":"quant-ph"}
{"created":"2024-04-08 13:34:39","title":"A High-Performant Multi-Parametric Quadratic Programming Solver","abstract":"We propose a combinatorial method for computing explicit solutions to multi-parametric quadratic programs, which can be used to compute explicit control laws for linear model predictive control. In contrast to classical methods, which are based on geometrical adjacency, the proposed method is based on combinatorial adjacency. After introducing the notion of combinatorial adjacency, we show that the explicit solution forms a connected graph in terms of it. We then leverage this connectedness to propose an algorithm that computes the explicit solution. The purely combinatorial nature of the algorithm leads to computational advantages since it enables demanding geometrical operations (such as computing facets of polytopes) to be avoided. Compared with classical combinatorial methods, the proposed method requires fewer combinations to be considered by exploiting combinatorial connectedness. We show that an implementation of the proposed method can yield a speedup of about two orders of magnitude compared with state-of-the-art software packages such as MPT and POP.","sentences":["We propose a combinatorial method for computing explicit solutions to multi-parametric quadratic programs, which can be used to compute explicit control laws for linear model predictive control.","In contrast to classical methods, which are based on geometrical adjacency, the proposed method is based on combinatorial adjacency.","After introducing the notion of combinatorial adjacency, we show that the explicit solution forms a connected graph in terms of it.","We then leverage this connectedness to propose an algorithm that computes the explicit solution.","The purely combinatorial nature of the algorithm leads to computational advantages since it enables demanding geometrical operations (such as computing facets of polytopes) to be avoided.","Compared with classical combinatorial methods, the proposed method requires fewer combinations to be considered by exploiting combinatorial connectedness.","We show that an implementation of the proposed method can yield a speedup of about two orders of magnitude compared with state-of-the-art software packages such as MPT and POP."],"url":"http://arxiv.org/abs/2404.05511v1","category":"math.OC"}
{"created":"2024-04-08 13:34:18","title":"Hardy and Rellich identities and inequalities for Grushin operators via spherical vector fields and Bessel pairs","abstract":"For Grushin vector fields, we prove Hardy, Hardy-Rellich, and Rellich identities and inequalities with sharp constants. Our explicit remainder terms significantly improve those found in the literature. Our arguments build on abstract Hardy-Rellich identities involving the Bessel pair along with the use of spherical harmonics developed by Garofalo-Shen [Ann. Inst. Fourier (1994)]. Furthermore, in the spirit of Bez-Machihara-Ozawa [Math. Z (2023)], we construct spherical vector fields that correspond to the Grushin vector fields and prove identities that, in turn, establish optimal Rellich identities by comparing the Grushin operator with its radial and spherical components. We give alternate proofs of Hardy identities and inequalities with enhanced Hardy constants in some subspaces of the Sobolev space, among other things. Additionally, we compute the deficit involving $L^2$-norm of the Laplacian and radial components of the Laplacian for the Grushin operator with an explicit remainder term, which leads to a comparison of Laplacian with the radial components of the Laplacian. As a consequence of the main results, new second-order Heisenberg-Pauli-Weyl uncertainty principles and Hydrogen uncertainty principles are also derived. Furthermore, we also derive certain symmetrization principles on the Grushin space.","sentences":["For Grushin vector fields, we prove Hardy, Hardy-Rellich, and Rellich identities and inequalities with sharp constants.","Our explicit remainder terms significantly improve those found in the literature.","Our arguments build on abstract Hardy-Rellich identities involving the Bessel pair along with the use of spherical harmonics developed by Garofalo-Shen","[Ann.","Inst.","Fourier (1994)].","Furthermore, in the spirit of Bez-Machihara-Ozawa [Math. Z (2023)], we construct spherical vector fields that correspond to the Grushin vector fields and prove identities that, in turn, establish optimal Rellich identities by comparing the Grushin operator with its radial and spherical components.","We give alternate proofs of Hardy identities and inequalities with enhanced Hardy constants in some subspaces of the Sobolev space, among other things.","Additionally, we compute the deficit involving $L^2$-norm of the Laplacian and radial components of the Laplacian for the Grushin operator with an explicit remainder term, which leads to a comparison of Laplacian with the radial components of the Laplacian.","As a consequence of the main results, new second-order Heisenberg-Pauli-Weyl uncertainty principles and Hydrogen uncertainty principles are also derived.","Furthermore, we also derive certain symmetrization principles on the Grushin space."],"url":"http://arxiv.org/abs/2404.05510v1","category":"math.AP"}
{"created":"2024-04-08 13:12:04","title":"Rewording theoretical predictions at colliders with vacuum amplitudes","abstract":"We propose multiloop vacuum amplitudes as the optimal building blocks for efficiently assembling theoretical predictions at high-energy colliders. This hypothesis is strongly supported by the manifestly causal properties of the loop-tree duality (LTD) representation of a vacuum amplitude. The vacuum amplitude, acting as a kernel, encodes all the final states contributing to a given scattering or decay process through residues in the on-shell energies of the internal propagators. It also naturally implements gauge invariance and the wave function renormalisation of the external legs. This methodological approach, dubbed LTD causal unitary, leads to a novel representation of differential cross sections and decay rates that is locally free of ultraviolet and infrared singularities at all orders in perturbation theory. Unitary threshold singularities also match between different phase-space residues. Most notably, it allows us to conjecture for the first time the local functional form of initial-state collinear singularities. The fulfillment of all these properties provides a theoretical description of differential observables at colliders that is well defined in the four physical dimensions of the space-time.","sentences":["We propose multiloop vacuum amplitudes as the optimal building blocks for efficiently assembling theoretical predictions at high-energy colliders.","This hypothesis is strongly supported by the manifestly causal properties of the loop-tree duality (LTD) representation of a vacuum amplitude.","The vacuum amplitude, acting as a kernel, encodes all the final states contributing to a given scattering or decay process through residues in the on-shell energies of the internal propagators.","It also naturally implements gauge invariance and the wave function renormalisation of the external legs.","This methodological approach, dubbed LTD causal unitary, leads to a novel representation of differential cross sections and decay rates that is locally free of ultraviolet and infrared singularities at all orders in perturbation theory.","Unitary threshold singularities also match between different phase-space residues.","Most notably, it allows us to conjecture for the first time the local functional form of initial-state collinear singularities.","The fulfillment of all these properties provides a theoretical description of differential observables at colliders that is well defined in the four physical dimensions of the space-time."],"url":"http://arxiv.org/abs/2404.05491v1","category":"hep-ph"}
{"created":"2024-04-08 13:11:57","title":"Two-Person Interaction Augmentation with Skeleton Priors","abstract":"Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.","sentences":["Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc.","However, acquiring such skeletal motion is challenging.","While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained.","To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies.","Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes.","Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions."],"url":"http://arxiv.org/abs/2404.05490v1","category":"cs.CV"}
{"created":"2024-04-08 12:54:19","title":"Experimental demonstration of improved quantum optimization with linear Ising penalties","abstract":"The standard approach to encoding constraints in quantum optimization is the quadratic penalty method. Quadratic penalties introduce additional couplings and energy scales, which can be detrimental to the performance of a quantum optimizer. In quantum annealing experiments performed on a D-Wave Advantage, we explore an alternative penalty method that only involves linear Ising terms and apply it to a customer data science problem. Our findings support our hypothesis that the linear Ising penalty method should improve the performance of quantum optimization compared to using the quadratic penalty method due to its more efficient use of physical resources. Although the linear Ising penalty method is not guaranteed to exactly implement the desired constraint in all cases, it is able to do so for the majority of problem instances we consider. For problems with many constraints, where making all penalties linear is unlikely to be feasible, we investigate strategies for combining linear Ising penalties with quadratic penalties to satisfy constraints for which the linear method is not well-suited. We find that this strategy is most effective when the penalties that contribute most to limiting the dynamic range are removed.","sentences":["The standard approach to encoding constraints in quantum optimization is the quadratic penalty method.","Quadratic penalties introduce additional couplings and energy scales, which can be detrimental to the performance of a quantum optimizer.","In quantum annealing experiments performed on a D-Wave Advantage, we explore an alternative penalty method that only involves linear Ising terms and apply it to a customer data science problem.","Our findings support our hypothesis that the linear Ising penalty method should improve the performance of quantum optimization compared to using the quadratic penalty method due to its more efficient use of physical resources.","Although the linear Ising penalty method is not guaranteed to exactly implement the desired constraint in all cases, it is able to do so for the majority of problem instances we consider.","For problems with many constraints, where making all penalties linear is unlikely to be feasible, we investigate strategies for combining linear Ising penalties with quadratic penalties to satisfy constraints for which the linear method is not well-suited.","We find that this strategy is most effective when the penalties that contribute most to limiting the dynamic range are removed."],"url":"http://arxiv.org/abs/2404.05476v1","category":"quant-ph"}
{"created":"2024-04-08 12:46:22","title":"Quantum optimization with linear Ising penalty functions for customer data science","abstract":"Constrained combinatorial optimization problems, which are ubiquitous in industry, can be solved by quantum algorithms such as quantum annealing (QA) and the quantum approximate optimization algorithm (QAOA). In these quantum algorithms, constraints are typically implemented with quadratic penalty functions. This penalty method can introduce large energy scales and make interaction graphs much more dense. These effects can result in worse performance of quantum optimization, particularly on near-term devices that have sparse hardware graphs and other physical limitations. In this work, we consider linear Ising penalty functions, which are applied with local fields in the Ising model, as an alternative method for implementing constraints that makes more efficient use of physical resources. We study the behaviour of the penalty method in the context of quantum optimization for customer data science problems. Our theoretical analysis and numerical simulations of QA and the QAOA indicate that this penalty method can lead to better performance in quantum optimization than the quadratic method. However, the linear Ising penalty method is not suitable for all problems as it cannot always exactly implement the desired constraint. In cases where the linear method is not successful in implementing all constraints, we propose that schemes involving both quadratic and linear Ising penalties can be effective.","sentences":["Constrained combinatorial optimization problems, which are ubiquitous in industry, can be solved by quantum algorithms such as quantum annealing (QA) and the quantum approximate optimization algorithm (QAOA).","In these quantum algorithms, constraints are typically implemented with quadratic penalty functions.","This penalty method can introduce large energy scales and make interaction graphs much more dense.","These effects can result in worse performance of quantum optimization, particularly on near-term devices that have sparse hardware graphs and other physical limitations.","In this work, we consider linear Ising penalty functions, which are applied with local fields in the Ising model, as an alternative method for implementing constraints that makes more efficient use of physical resources.","We study the behaviour of the penalty method in the context of quantum optimization for customer data science problems.","Our theoretical analysis and numerical simulations of QA and the QAOA indicate that this penalty method can lead to better performance in quantum optimization than the quadratic method.","However, the linear Ising penalty method is not suitable for all problems as it cannot always exactly implement the desired constraint.","In cases where the linear method is not successful in implementing all constraints, we propose that schemes involving both quadratic and linear Ising penalties can be effective."],"url":"http://arxiv.org/abs/2404.05467v1","category":"quant-ph"}
{"created":"2024-04-08 12:43:32","title":"HAMMR: HierArchical MultiModal React agents for generic VQA","abstract":"Combining Large Language Models (LLMs) with external specialized tools (LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual Question Answering (VQA). While this approach was demonstrated to work well when optimized and evaluated for each individual benchmark, in practice it is crucial for the next generation of real-world AI systems to handle a broad range of multimodal problems. Therefore we pose the VQA problem from a unified perspective and evaluate a single system on a varied suite of VQA tasks including counting, spatial reasoning, OCR-based reasoning, visual pointing, external knowledge, and more. In this setting, we demonstrate that naively applying the LLM+tools approach using the combined set of all tools leads to poor results. This motivates us to introduce HAMMR: HierArchical MultiModal React. We start from a multimodal ReAct-based system and make it hierarchical by enabling our HAMMR agents to call upon other specialized agents. This enhances the compositionality of the LLM+tools approach, which we show to be critical for obtaining high accuracy on generic VQA. Concretely, on our generic VQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%. Additionally, HAMMR achieves state-of-the-art results on this task, outperforming the generic standalone PaLI-X VQA model by 5.0%.","sentences":["Combining Large Language Models (LLMs) with external specialized tools (LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual Question Answering (VQA).","While this approach was demonstrated to work well when optimized and evaluated for each individual benchmark, in practice it is crucial for the next generation of real-world AI systems to handle a broad range of multimodal problems.","Therefore we pose the VQA problem from a unified perspective and evaluate a single system on a varied suite of VQA tasks including counting, spatial reasoning, OCR-based reasoning, visual pointing, external knowledge, and more.","In this setting, we demonstrate that naively applying the LLM+tools approach using the combined set of all tools leads to poor results.","This motivates us to introduce HAMMR:","HierArchical MultiModal React.","We start from a multimodal ReAct-based system and make it hierarchical by enabling our HAMMR agents to call upon other specialized agents.","This enhances the compositionality of the LLM+tools approach, which we show to be critical for obtaining high accuracy on generic VQA.","Concretely, on our generic VQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.","Additionally, HAMMR achieves state-of-the-art results on this task, outperforming the generic standalone PaLI-X VQA model by 5.0%."],"url":"http://arxiv.org/abs/2404.05465v1","category":"cs.CV"}
{"created":"2024-04-08 12:39:35","title":"On Optimal Transport Maps Between 1 /d-Concave Densities","abstract":"In this paper, we extend the scope of Caffarelli's contraction theorem, which provides a measure of the Lipschitz constant for optimal transport maps between log-concave probability densities in $\\R^d$. Our focus is on a broader category of densities, specifically those that are $\\nicefrac{1}{d}$-concave and can be represented as $V^{-d}$, where $V$ is convex. By setting appropriate conditions, we derive linear or sublinear limitations for the optimal transport map. This leads us to a comprehensive Lipschitz estimate that aligns with the principles established in Caffarelli's theorem.","sentences":["In this paper, we extend the scope of Caffarelli's contraction theorem, which provides a measure of the Lipschitz constant for optimal transport maps between log-concave probability densities in $\\R^d$. Our focus is on a broader category of densities, specifically those that are $\\nicefrac{1}{d}$-concave and can be represented as $V^{-d}$, where $V$ is convex.","By setting appropriate conditions, we derive linear or sublinear limitations for the optimal transport map.","This leads us to a comprehensive Lipschitz estimate that aligns with the principles established in Caffarelli's theorem."],"url":"http://arxiv.org/abs/2404.05456v1","category":"math.AP"}
{"created":"2024-04-08 12:38:26","title":"B-ary Tree Push-Pull Method is Provably Efficient for Decentralized Learning on Heterogeneous Data","abstract":"This paper considers the distributed learning problem where a group of agents cooperatively minimizes the summation of their local cost functions based on peer-to-peer communication. Particularly, we propose a highly efficient algorithm, termed ``B-ary Tree Push-Pull'' (BTPP), that employs two B-ary spanning trees for distributing the information related to the parameters and stochastic gradients across the network. The simple method is efficient in communication since each agent interacts with at most $(B+1)$ neighbors per iteration. More importantly, BTPP achieves linear speedup for smooth nonconvex objective functions with only $\\tilde{O}(n)$ transient iterations, significantly outperforming the state-of-the-art results to the best of our knowledge.","sentences":["This paper considers the distributed learning problem where a group of agents cooperatively minimizes the summation of their local cost functions based on peer-to-peer communication.","Particularly, we propose a highly efficient algorithm, termed ``B-ary Tree Push-Pull'' (BTPP), that employs two B-ary spanning trees for distributing the information related to the parameters and stochastic gradients across the network.","The simple method is efficient in communication since each agent interacts with at most $(B+1)$ neighbors per iteration.","More importantly, BTPP achieves linear speedup for smooth nonconvex objective functions with only $\\tilde{O}(n)$ transient iterations, significantly outperforming the state-of-the-art results to the best of our knowledge."],"url":"http://arxiv.org/abs/2404.05454v1","category":"math.OC"}
{"created":"2024-04-08 12:30:07","title":"Efficient Encodings of the Travelling Salesperson Problem for Variational Quantum Algorithms","abstract":"Routing problems are a common optimization problem in industrial applications, which occur on a large scale in supply chain planning. Due to classical limitations for solving NP-hard problems, quantum computing hopes to improve upon speed or solution quality. Several suggestions have been made for encodings of routing problems to solve them with variational quantum algorithms. However, for an end user it is hard to decide a priori which encoding will give the best solutions according to their needs. In this work, we investigate different encodings for the Travelling Salesperson Problem. We compare their scaling and performance when using the Quantum Approximate Optimization Algorithm and the Variational Quantum Eigensolver and provide a clear guide for users when to choose which encoding. For small instances, we find evidence that the permutation encoding can yield good results since it does not suffer from feasibility issues.","sentences":["Routing problems are a common optimization problem in industrial applications, which occur on a large scale in supply chain planning.","Due to classical limitations for solving NP-hard problems, quantum computing hopes to improve upon speed or solution quality.","Several suggestions have been made for encodings of routing problems to solve them with variational quantum algorithms.","However, for an end user it is hard to decide a priori which encoding will give the best solutions according to their needs.","In this work, we investigate different encodings for the Travelling Salesperson Problem.","We compare their scaling and performance when using the Quantum Approximate Optimization Algorithm and the Variational Quantum Eigensolver and provide a clear guide for users when to choose which encoding.","For small instances, we find evidence that the permutation encoding can yield good results since it does not suffer from feasibility issues."],"url":"http://arxiv.org/abs/2404.05448v1","category":"quant-ph"}
{"created":"2024-04-08 12:24:03","title":"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All","abstract":"Quantum annealers (QA), such as D-Wave systems, become increasingly efficient and competitive at solving combinatorial optimization problems. However, solving problems that do not directly map the chip topology remains challenging for this type of quantum computer. The creation of logical qubits as sets of interconnected physical qubits overcomes limitations imposed by the sparsity of the chip at the expense of increasing the problem size and adding new parameters to optimize. This paper explores the advantages and drawbacks provided by the structure of the logical qubits and the impact of the rescaling of coupler strength on the minimum spectral gap of Ising models. We show that densely connected logical qubits require a lower chain strength to maintain the ferromagnetic coupling. We also analyze the optimal chain strength variations considering different minor embeddings of the same instance. This experimental study suggests that the chain strength can be optimized for each instance. We design a heuristic that optimizes the chain strength using a very low number of shots during the pre-processing step. This heuristic outperforms the default method used to initialize the chain strength on D-Wave systems, increasing the quality of the best solution by up to 17.2% for tested instances on the max-cut problem.","sentences":["Quantum annealers (QA), such as D-Wave systems, become increasingly efficient and competitive at solving combinatorial optimization problems.","However, solving problems that do not directly map the chip topology remains challenging for this type of quantum computer.","The creation of logical qubits as sets of interconnected physical qubits overcomes limitations imposed by the sparsity of the chip at the expense of increasing the problem size and adding new parameters to optimize.","This paper explores the advantages and drawbacks provided by the structure of the logical qubits and the impact of the rescaling of coupler strength on the minimum spectral gap of Ising models.","We show that densely connected logical qubits require a lower chain strength to maintain the ferromagnetic coupling.","We also analyze the optimal chain strength variations considering different minor embeddings of the same instance.","This experimental study suggests that the chain strength can be optimized for each instance.","We design a heuristic that optimizes the chain strength using a very low number of shots during the pre-processing step.","This heuristic outperforms the default method used to initialize the chain strength on D-Wave systems, increasing the quality of the best solution by up to 17.2% for tested instances on the max-cut problem."],"url":"http://arxiv.org/abs/2404.05443v1","category":"quant-ph"}
{"created":"2024-04-08 10:58:01","title":"Generating Galaxy Clusters Mass Density Maps from Mock Multiview Images via Deep Learning","abstract":"Galaxy clusters are composed of dark matter, gas and stars. Their dark matter component, which amounts to around 80\\% of the total mass, cannot be directly observed but traced by the distribution of diffused gas and galaxy members. In this work, we aim to infer the cluster's projected total mass distribution from mock observational data, i.e. stars, Sunyaev-Zeldovich, and X-ray, by training deep learning models. To this end, we have created a multiview images dataset from {\\sc{The Three Hundred}} simulation that is optimal for training Machine Learning models. We further study deep learning architectures based on the U-Net to account for single-input and multi-input models. We show that the predicted mass distribution agrees well with the true one.","sentences":["Galaxy clusters are composed of dark matter, gas and stars.","Their dark matter component, which amounts to around 80\\% of the total mass, cannot be directly observed but traced by the distribution of diffused gas and galaxy members.","In this work, we aim to infer the cluster's projected total mass distribution from mock observational data, i.e. stars, Sunyaev-Zeldovich, and X-ray, by training deep learning models.","To this end, we have created a multiview images dataset from {\\sc{The Three Hundred}} simulation that is optimal for training Machine Learning models.","We further study deep learning architectures based on the U-Net to account for single-input and multi-input models.","We show that the predicted mass distribution agrees well with the true one."],"url":"http://arxiv.org/abs/2404.05400v2","category":"astro-ph.CO"}
{"created":"2024-04-08 10:32:30","title":"A Max-Min-Max Algorithm for Large-Scale Robust Optimization","abstract":"Robust optimization (RO) is a powerful paradigm for decision making under uncertainty. Existing algorithms for solving RO, including the reformulation approach and the cutting-plane method, do not scale well, hindering the application of RO to large-scale decision problems. In this paper, we devise a first-order algorithm for solving RO based on a novel max-min-max perspective. Our algorithm operates directly on the model functions and sets through the subgradient and projection oracles, which enables the exploitation of problem structures and is especially suitable for large-scale RO. Theoretically, we prove that the oracle complexity of our algorithm for attaining an $\\varepsilon$-approximate optimal solution is $\\mathcal{O}(\\varepsilon^{-3})$ or $\\mathcal{O}(\\varepsilon^{-2})$, depending on the smoothness of the model functions. The algorithm and its theoretical results are then extended to RO with projection-unfriendly uncertainty sets. We also show via extensive numerical experiments that the proposed algorithm outperforms the reformulation approach, the cutting-plane method and two other recent first-order algorithms.","sentences":["Robust optimization (RO) is a powerful paradigm for decision making under uncertainty.","Existing algorithms for solving RO, including the reformulation approach and the cutting-plane method, do not scale well, hindering the application of RO to large-scale decision problems.","In this paper, we devise a first-order algorithm for solving RO based on a novel max-min-max perspective.","Our algorithm operates directly on the model functions and sets through the subgradient and projection oracles, which enables the exploitation of problem structures and is especially suitable for large-scale RO.","Theoretically, we prove that the oracle complexity of our algorithm for attaining an $\\varepsilon$-approximate optimal solution is $\\mathcal{O}(\\varepsilon^{-3})$ or $\\mathcal{O}(\\varepsilon^{-2})$, depending on the smoothness of the model functions.","The algorithm and its theoretical results are then extended to RO with projection-unfriendly uncertainty sets.","We also show via extensive numerical experiments that the proposed algorithm outperforms the reformulation approach, the cutting-plane method and two other recent first-order algorithms."],"url":"http://arxiv.org/abs/2404.05377v1","category":"math.OC"}
{"created":"2024-04-08 10:28:32","title":"Testing of Pythia modes to study identified particle production in high-multiplicity pp collisions at $\\mathbf{\\sqrt{s}}$ = 7 TeV","abstract":"This study presents a comprehensive analysis of particle production in proton-proton ($pp$) collisions at $\\sqrt{s}$ = 7 TeV using Pythia~8 event generator. We investigate the transverse momentum $p_T$ spectra of light charged hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$), their yield ratios ($\\pi^-/\\pi^+$, $K^-/K^+$ and $\\bar{p}/p$), and $p_T$-differential ratios ($(K^++K^-)/(\\pi^++\\pi^-)$, $(\\overline{p}+p)/(\\pi^++\\pi^-)$) and mean transverse momentum ($\\langle p_\\mathrm{T} \\rangle$). Our analysis employs various Pythia~8 tunes (Simple, Vincia, and Dire) to explore the impact of different model configurations on particle production. We optimize a key parameter ($p_\\mathrm{T}HatMin$) within each tune to achieve the best agreement between the simulated \\ppt spectra and those measured by the CMS collaboration. Interestingly, we find that the optimal values for $p_\\mathrm{T}HatMin$ differ between hadron species, potentially reflecting the influence of particle mass on production mechanisms. It is not possible to simultaneously and qualitatively describe both, the strangeness enhancement and collectivity in $pp$ collisions from \\pythia~8. Further investigation such as final-state effects such as color ropes or junctions may require to explain these effects. These types of studies help us identify limitations in current models and refine their parameters to better explain experimental observations.","sentences":["This study presents a comprehensive analysis of particle production in proton-proton ($pp$) collisions at $\\sqrt{s}$ = 7 TeV using Pythia~8 event generator.","We investigate the transverse momentum $p_T$ spectra of light charged hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$), their yield ratios ($\\pi^-/\\pi^+$, $K^-/K^+$ and $\\bar{p}/p$), and $p_T$-differential ratios ($(K^++K^-)/(\\pi^++\\pi^-)$, $(\\overline{p}+p)/(\\pi^++\\pi^-)$) and mean transverse momentum ($\\langle p_\\mathrm{T} \\rangle$).","Our analysis employs various Pythia~8 tunes (Simple, Vincia, and Dire) to explore the impact of different model configurations on particle production.","We optimize a key parameter ($p_\\mathrm{T}HatMin$) within each tune to achieve the best agreement between the simulated \\ppt spectra and those measured by the CMS collaboration.","Interestingly, we find that the optimal values for $p_\\mathrm{T}HatMin$ differ between hadron species, potentially reflecting the influence of particle mass on production mechanisms.","It is not possible to simultaneously and qualitatively describe both, the strangeness enhancement and collectivity in $pp$ collisions from \\pythia~8.","Further investigation such as final-state effects such as color ropes or junctions may require to explain these effects.","These types of studies help us identify limitations in current models and refine their parameters to better explain experimental observations."],"url":"http://arxiv.org/abs/2404.05373v1","category":"hep-ph"}
{"created":"2024-04-08 10:10:30","title":"Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural Network Accelerators","abstract":"Energy efficiency and memory footprint of a convolutional neural network (CNN) implemented on a CNN inference accelerator depend on many factors, including a weight quantization strategy (i.e., data types and bit-widths) and mapping (i.e., placement and scheduling of DNN elementary operations on hardware units of the accelerator). We show that enabling rich mixed quantization schemes during the implementation can open a previously hidden space of mappings that utilize the hardware resources more effectively. CNNs utilizing quantized weights and activations and suitable mappings can significantly improve trade-offs among the accuracy, energy, and memory requirements compared to less carefully optimized CNN implementations. To find, analyze, and exploit these mappings, we: (i) extend a general-purpose state-of-the-art mapping tool (Timeloop) to support mixed quantization, which is not currently available; (ii) propose an efficient multi-objective optimization algorithm to find the most suitable bit-widths and mapping for each DNN layer executed on the accelerator; and (iii) conduct a detailed experimental evaluation to validate the proposed method. On two CNNs (MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show that for a given quality metric (such as the accuracy on ImageNet), energy savings are up to 37% without any accuracy drop.","sentences":["Energy efficiency and memory footprint of a convolutional neural network (CNN) implemented on a CNN inference accelerator depend on many factors, including a weight quantization strategy (i.e., data types and bit-widths) and mapping (i.e., placement and scheduling of DNN elementary operations on hardware units of the accelerator).","We show that enabling rich mixed quantization schemes during the implementation can open a previously hidden space of mappings that utilize the hardware resources more effectively.","CNNs utilizing quantized weights and activations and suitable mappings can significantly improve trade-offs among the accuracy, energy, and memory requirements compared to less carefully optimized CNN implementations.","To find, analyze, and exploit these mappings, we: (i) extend a general-purpose state-of-the-art mapping tool (Timeloop) to support mixed quantization, which is not currently available; (ii) propose an efficient multi-objective optimization algorithm to find the most suitable bit-widths and mapping for each DNN layer executed on the accelerator; and (iii) conduct a detailed experimental evaluation to validate the proposed method.","On two CNNs (MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show that for a given quality metric (such as the accuracy on ImageNet), energy savings are up to 37% without any accuracy drop."],"url":"http://arxiv.org/abs/2404.05368v1","category":"cs.AR"}
{"created":"2024-04-08 10:09:15","title":"Finite Elements with Switch Detection for Numerical Optimal Control of Projected Dynamical Systems","abstract":"The Finite Elements with Switch Detection (FESD) method is a highly accurate direct transcription method for optimal control of several classes of nonsmooth dynamical systems. This paper extends the FESD method to Projected Dynamical Systems (PDS) and first-order sweeping processes with time-independent sets. This method discretizes an equivalent dynamic complementarity system and exploits the particular structure of the discontinuities present in these systems. In the FESD method, allowing integration step sizes to be degrees of freedom, and introducing additional complementarity constraints, enables the exact detection of nonsmooth events. In contrast to the standard fixed-step Runge-Kutta methods, this approach allows for the recovery of full-order integration accuracy and the correct computation of numerical sensitivities. Numerical examples illustrate the effectiveness of the proposed method in an optimal control context. This method and the examples are included in the open-source software package nosnoc.","sentences":["The Finite Elements with Switch Detection (FESD) method is a highly accurate direct transcription method for optimal control of several classes of nonsmooth dynamical systems.","This paper extends the FESD method to Projected Dynamical Systems (PDS) and first-order sweeping processes with time-independent sets.","This method discretizes an equivalent dynamic complementarity system and exploits the particular structure of the discontinuities present in these systems.","In the FESD method, allowing integration step sizes to be degrees of freedom, and introducing additional complementarity constraints, enables the exact detection of nonsmooth events.","In contrast to the standard fixed-step Runge-Kutta methods, this approach allows for the recovery of full-order integration accuracy and the correct computation of numerical sensitivities.","Numerical examples illustrate the effectiveness of the proposed method in an optimal control context.","This method and the examples are included in the open-source software package nosnoc."],"url":"http://arxiv.org/abs/2404.05367v1","category":"math.OC"}
{"created":"2024-04-08 09:52:19","title":"Improving Algorithm-Selection and Performance-Prediction via Learning Discriminating Training Samples","abstract":"The choice of input-data used to train algorithm-selection models is recognised as being a critical part of the model success. Recently, feature-free methods for algorithm-selection that use short trajectories obtained from running a solver as input have shown promise. However, it is unclear to what extent these trajectories reliably discriminate between solvers. We propose a meta approach to generating discriminatory trajectories with respect to a portfolio of solvers. The algorithm-configuration tool irace is used to tune the parameters of a simple Simulated Annealing algorithm (SA) to produce trajectories that maximise the performance metrics of ML models trained on this data. We show that when the trajectories obtained from the tuned SA algorithm are used in ML models for algorithm-selection and performance prediction, we obtain significantly improved performance metrics compared to models trained both on raw trajectory data and on exploratory landscape features.","sentences":["The choice of input-data used to train algorithm-selection models is recognised as being a critical part of the model success.","Recently, feature-free methods for algorithm-selection that use short trajectories obtained from running a solver as input have shown promise.","However, it is unclear to what extent these trajectories reliably discriminate between solvers.","We propose a meta approach to generating discriminatory trajectories with respect to a portfolio of solvers.","The algorithm-configuration tool irace is used to tune the parameters of a simple Simulated Annealing algorithm (SA) to produce trajectories that maximise the performance metrics of ML models trained on this data.","We show that when the trajectories obtained from the tuned SA algorithm are used in ML models for algorithm-selection and performance prediction, we obtain significantly improved performance metrics compared to models trained both on raw trajectory data and on exploratory landscape features."],"url":"http://arxiv.org/abs/2404.05359v1","category":"cs.NE"}
{"created":"2024-04-08 09:46:01","title":"Nonlinear Model Reduction to Temporally Aperiodic Spectral Submanifolds","abstract":"We extend the theory of spectral submanifolds (SSMs) to general non-autonomous dynamical systems that are either weakly forced or slowly varying. Examples of such systems arise in structural dynamics, fluid-structure interactions and control problems. The time-dependent SSMs we construct under these assumptions are normally hyperbolic and hence will persist for larger forcing and faster time dependence that are beyond the reach of our precise existence theory. For this reason, we also derive formal asymptotic expansions that, under explicitly verifiable nonresonance conditions, approximate SSMs and their aperiodic anchor trajectories accurately for stronger, faster or even temporally discontinuous forcing. Reducing the dynamical system to these persisting SSMs provides a mathematically justified model reduction technique for non-autonomous physical systems whose time dependance is moderate either in magnitude or speed. We illustrate the existence, persistence and computation of temporally aperiodic SSMs in mechanical examples under chaotic forcing.","sentences":["We extend the theory of spectral submanifolds (SSMs) to general non-autonomous dynamical systems that are either weakly forced or slowly varying.","Examples of such systems arise in structural dynamics, fluid-structure interactions and control problems.","The time-dependent SSMs we construct under these assumptions are normally hyperbolic and hence will persist for larger forcing and faster time dependence that are beyond the reach of our precise existence theory.","For this reason, we also derive formal asymptotic expansions that, under explicitly verifiable nonresonance conditions, approximate SSMs and their aperiodic anchor trajectories accurately for stronger, faster or even temporally discontinuous forcing.","Reducing the dynamical system to these persisting SSMs provides a mathematically justified model reduction technique for non-autonomous physical systems whose time dependance is moderate either in magnitude or speed.","We illustrate the existence, persistence and computation of temporally aperiodic SSMs in mechanical examples under chaotic forcing."],"url":"http://arxiv.org/abs/2404.05355v1","category":"math.DS"}
{"created":"2024-04-08 09:28:34","title":"Beyond the Sequence: Statistics-Driven Pre-training for Stabilizing Sequential Recommendation Model","abstract":"The sequential recommendation task aims to predict the item that user is interested in according to his/her historical action sequence. However, inevitable random action, i.e. user randomly accesses an item among multiple candidates or clicks several items at random order, cause the sequence fails to provide stable and high-quality signals. To alleviate the issue, we propose the StatisTics-Driven Pre-traing framework (called STDP briefly). The main idea of the work lies in the exploration of utilizing the statistics information along with the pre-training paradigm to stabilize the optimization of recommendation model. Specifically, we derive two types of statistical information: item co-occurrence across sequence and attribute frequency within the sequence. And we design the following pre-training tasks: 1) The co-occurred items prediction task, which encourages the model to distribute its attention on multiple suitable targets instead of just focusing on the next item that may be unstable. 2) We generate a paired sequence by replacing items with their co-occurred items and enforce its representation close with the original one, thus enhancing the model's robustness to the random noise. 3) To reduce the impact of random on user's long-term preferences, we encourage the model to capture sequence-level frequent attributes. The significant improvement over six datasets demonstrates the effectiveness and superiority of the proposal, and further analysis verified the generalization of the STDP framework on other models.","sentences":["The sequential recommendation task aims to predict the item that user is interested in according to his/her historical action sequence.","However, inevitable random action, i.e. user randomly accesses an item among multiple candidates or clicks several items at random order, cause the sequence fails to provide stable and high-quality signals.","To alleviate the issue, we propose the StatisTics-Driven Pre-traing framework (called STDP briefly).","The main idea of the work lies in the exploration of utilizing the statistics information along with the pre-training paradigm to stabilize the optimization of recommendation model.","Specifically, we derive two types of statistical information: item co-occurrence across sequence and attribute frequency within the sequence.","And we design the following pre-training tasks: 1) The co-occurred items prediction task, which encourages the model to distribute its attention on multiple suitable targets instead of just focusing on the next item that may be unstable.","2) We generate a paired sequence by replacing items with their co-occurred items and enforce its representation close with the original one, thus enhancing the model's robustness to the random noise.","3) To reduce the impact of random on user's long-term preferences, we encourage the model to capture sequence-level frequent attributes.","The significant improvement over six datasets demonstrates the effectiveness and superiority of the proposal, and further analysis verified the generalization of the STDP framework on other models."],"url":"http://arxiv.org/abs/2404.05342v1","category":"cs.IR"}
{"created":"2024-04-08 09:23:52","title":"Jammer-Resilient Time Synchronization in the MIMO Uplink","abstract":"Spatial filtering based on multiple-input multiple-output (MIMO) processing is a promising approach to jammer mitigation. Effective MIMO data detectors that mitigate smart jammers have recently been proposed, but they all assume perfect time synchronization between transmitter(s) and receiver. However, to the best of our knowledge, there are no methods for resilient time synchronization in the presence of smart jammers. To remedy this situation, we propose JASS, the first method that enables reliable time synchronization for the single-user MIMO uplink while mitigating smart jamming attacks. JASS detects a randomized synchronization sequence based on a novel optimization problem that fits a spatial filter to the time-windowed receive signal in order to mitigate the jammer. We underscore the efficacy of the proposed optimization problem by proving that it ensures successful time synchronization under certain intuitive conditions. We then derive an efficient algorithm for approximately solving our optimization problem. Finally, we use simulations to demonstrate the effectiveness of JASS against a wide range of different jammer types.","sentences":["Spatial filtering based on multiple-input multiple-output (MIMO) processing is a promising approach to jammer mitigation.","Effective MIMO data detectors that mitigate smart jammers have recently been proposed, but they all assume perfect time synchronization between transmitter(s) and receiver.","However, to the best of our knowledge, there are no methods for resilient time synchronization in the presence of smart jammers.","To remedy this situation, we propose JASS, the first method that enables reliable time synchronization for the single-user MIMO uplink while mitigating smart jamming attacks.","JASS detects a randomized synchronization sequence based on a novel optimization problem that fits a spatial filter to the time-windowed receive signal in order to mitigate the jammer.","We underscore the efficacy of the proposed optimization problem by proving that it ensures successful time synchronization under certain intuitive conditions.","We then derive an efficient algorithm for approximately solving our optimization problem.","Finally, we use simulations to demonstrate the effectiveness of JASS against a wide range of different jammer types."],"url":"http://arxiv.org/abs/2404.05335v1","category":"eess.SP"}
{"created":"2024-04-08 09:10:20","title":"Unravelling the Power of Single-Pass Look-Ahead in Modern Codecs for Optimized Transcoding Deployment","abstract":"Modern video encoders have evolved into sophisticated pieces of software in which various coding tools interact with each other. In the past, singlepass encoding was not considered for Video-On-Demand (VOD) use cases. In this work, we evaluate production-ready encoders for H.264 (x264), H.265 (HEVC), AV1 (SVT-AV1) along with direct comparisons to the latest AV1 encoder inside NVIDIA GPUs (40 series), and AWS Mediaconvert's AV1 implementation. Our experimental results demonstrate single pass encoding inside modern encoder implementations can give us very good quality at a reasonable compute cost. The results are presented as three different scenarios targeting High, Medium, and Low complexity accounting quality/bitrate/compute load. Finally, a set of recommendations is presented for end-users to help decide which encoder/preset combination might be more suited to their use case.","sentences":["Modern video encoders have evolved into sophisticated pieces of software in which various coding tools interact with each other.","In the past, singlepass encoding was not considered for Video-On-Demand (VOD) use cases.","In this work, we evaluate production-ready encoders for H.264 (x264), H.265 (HEVC), AV1 (SVT-AV1) along with direct comparisons to the latest AV1 encoder inside NVIDIA GPUs (40 series), and AWS Mediaconvert's AV1 implementation.","Our experimental results demonstrate single pass encoding inside modern encoder implementations can give us very good quality at a reasonable compute cost.","The results are presented as three different scenarios targeting High, Medium, and Low complexity accounting quality/bitrate/compute load.","Finally, a set of recommendations is presented for end-users to help decide which encoder/preset combination might be more suited to their use case."],"url":"http://arxiv.org/abs/2404.05321v1","category":"eess.IV"}
{"created":"2024-04-08 09:10:02","title":"Reflected Search Poisoning for Illicit Promotion","abstract":"As an emerging black hat search engine optimization (SEO) technique, reflected search poisoning (RSP) allows a miscreant to free-ride the reputation of high-ranking websites, poisoning search engines with illicit promotion texts (IPTs) in an efficient and stealthy manner, while avoiding the burden of continuous website compromise as required by traditional promotion infections. However, little is known about the security implications of RSP, e.g., what illicit promotion campaigns are being distributed by RSP, and to what extent regular search users can be exposed to illicit promotion texts distributed by RSP. In this study, we conduct the first security study on RSP-based illicit promotion, which is made possible through an end-to-end methodology for capturing, analyzing, and infiltrating IPTs. As a result, IPTs distributed via RSP are found to be large-scale, continuously growing, and diverse in both illicit categories and natural languages. Particularly, we have identified over 11 million distinct IPTs belonging to 14 different illicit categories, with typical examples including drug trading, data theft, counterfeit goods, and hacking services. Also, the underlying RSP cases have abused tens of thousands of high-ranking websites, as well as extensively poisoning all four popular search engines we studied, especially Google Search and Bing. Furthermore, it is observed that benign search users are being exposed to IPTs at a concerning extent. To facilitate interaction with potential customers (victim search users), miscreants tend to embed various types of contacts in IPTs, especially instant messaging accounts. Further infiltration of these IPT contacts reveals that the underlying illicit campaigns are operated on a large scale. All these findings highlight the negative security implications of IPTs and RSPs, and thus call for more efforts to mitigate RSP-driven illicit promotion.","sentences":["As an emerging black hat search engine optimization (SEO) technique, reflected search poisoning (RSP) allows a miscreant to free-ride the reputation of high-ranking websites, poisoning search engines with illicit promotion texts (IPTs) in an efficient and stealthy manner, while avoiding the burden of continuous website compromise as required by traditional promotion infections.","However, little is known about the security implications of RSP, e.g., what illicit promotion campaigns are being distributed by RSP, and to what extent regular search users can be exposed to illicit promotion texts distributed by RSP.","In this study, we conduct the first security study on RSP-based illicit promotion, which is made possible through an end-to-end methodology for capturing, analyzing, and infiltrating IPTs.","As a result, IPTs distributed via RSP are found to be large-scale, continuously growing, and diverse in both illicit categories and natural languages.","Particularly, we have identified over 11 million distinct IPTs belonging to 14 different illicit categories, with typical examples including drug trading, data theft, counterfeit goods, and hacking services.","Also, the underlying RSP cases have abused tens of thousands of high-ranking websites, as well as extensively poisoning all four popular search engines we studied, especially Google Search and Bing.","Furthermore, it is observed that benign search users are being exposed to IPTs at a concerning extent.","To facilitate interaction with potential customers (victim search users), miscreants tend to embed various types of contacts in IPTs, especially instant messaging accounts.","Further infiltration of these IPT contacts reveals that the underlying illicit campaigns are operated on a large scale.","All these findings highlight the negative security implications of IPTs and RSPs, and thus call for more efforts to mitigate RSP-driven illicit promotion."],"url":"http://arxiv.org/abs/2404.05320v1","category":"cs.CR"}
{"created":"2024-04-08 09:08:59","title":"Stochastic Online Optimization for Cyber-Physical and Robotic Systems","abstract":"We propose a novel gradient-based online optimization framework for solving stochastic programming problems that frequently arise in the context of cyber-physical and robotic systems. Our problem formulation accommodates constraints that model the evolution of a cyber-physical system, which has, in general, a continuous state and action space, is nonlinear, and where the state is only partially observed. We also incorporate an approximate model of the dynamics as prior knowledge into the learning process and show that even rough estimates of the dynamics can significantly improve the convergence of our algorithms. Our online optimization framework encompasses both gradient descent and quasi-Newton methods, and we provide a unified convergence analysis of our algorithms in a non-convex setting. We also characterize the impact of modeling errors in the system dynamics on the convergence rate of the algorithms. Finally, we evaluate our algorithms in simulations of a flexible beam, a four-legged walking robot, and in real-world experiments with a ping-pong playing robot.","sentences":["We propose a novel gradient-based online optimization framework for solving stochastic programming problems that frequently arise in the context of cyber-physical and robotic systems.","Our problem formulation accommodates constraints that model the evolution of a cyber-physical system, which has, in general, a continuous state and action space, is nonlinear, and where the state is only partially observed.","We also incorporate an approximate model of the dynamics as prior knowledge into the learning process and show that even rough estimates of the dynamics can significantly improve the convergence of our algorithms.","Our online optimization framework encompasses both gradient descent and quasi-Newton methods, and we provide a unified convergence analysis of our algorithms in a non-convex setting.","We also characterize the impact of modeling errors in the system dynamics on the convergence rate of the algorithms.","Finally, we evaluate our algorithms in simulations of a flexible beam, a four-legged walking robot, and in real-world experiments with a ping-pong playing robot."],"url":"http://arxiv.org/abs/2404.05318v1","category":"cs.LG"}
{"created":"2024-04-08 09:04:40","title":"A measure for the stability of structures immersed in a 2D laminar flow","abstract":"We introduce a new measure for the stability of structures, such as the cross-section of the deck of a suspension bridge, subject to a 2D fluid force, such as the lift exerted by a laminar wind. We consider a wide class of possible flows, as well as a wide class of structural shapes. Within a suitable topological framework, we prove the existence of an optimal shape maximizing the stability. Applications to engineering problems are also discussed.","sentences":["We introduce a new measure for the stability of structures, such as the cross-section of the deck of a suspension bridge, subject to a 2D fluid force, such as the lift exerted by a laminar wind.","We consider a wide class of possible flows, as well as a wide class of structural shapes.","Within a suitable topological framework, we prove the existence of an optimal shape maximizing the stability.","Applications to engineering problems are also discussed."],"url":"http://arxiv.org/abs/2404.05314v1","category":"math.AP"}
{"created":"2024-04-08 08:34:04","title":"Can Edge Computing fulfill the requirements of automated vehicular services using 5G network ?","abstract":"Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability. Fulfilling these requirements is crucial for ensuring road safety and traffic optimization. The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively. Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements. However, it is not yet clear under which conditions MEC can support CAV requirements and for which services. To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO. We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support. We find that such parameters must vary a lot, depending on the service considered. This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV.","sentences":["Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability.","Fulfilling these requirements is crucial for ensuring road safety and traffic optimization.","The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively.","Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements.","However, it is not yet clear under which conditions MEC can support CAV requirements and for which services.","To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO.","We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support.","We find that such parameters must vary a lot, depending on the service considered.","This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV."],"url":"http://arxiv.org/abs/2404.05296v1","category":"cs.NI"}
{"created":"2024-04-08 08:18:40","title":"Characterization of the ESPRESSO Line-Spread Function and Improvement of the Wavelength Calibration Accuracy","abstract":"Achieving a truly accurate wavelength calibration of high-dispersion echelle spectrographs is a challenging task but crucially needed for certain science cases, e.g. to test for a possible variation of the fine-structure constant in quasar spectra. One of the spectrographs best suited for this mission is VLT/ESPRESSO. Nevertheless, previous studies have identified significant discrepancies between the classical wavelength solutions and the one derived independently from the laser frequency comb. The dominant parts of these systematics were intra-order distortions, most-likely related to a deviation of the instrumental line-spread function from the assumed Gaussian shape. Here, we therefore present a study focused on a detailed modeling of the ESPRESSO instrumental line-spread function. We demonstrate that it is strongly asymmetric, non-Gaussian, different for the two slices and fibers, and varies significantly along the spectral orders. Incorporating the determined non-parametric model in the wavelength calibration process drastically improves the wavelength calibration accuracy, reducing the discrepancies between the two independent wavelength solutions from 50m/s to about 10m/s. The most striking success is, however, that the different fibers and slices now provide fully consistent measurements with a scatter of just a couple m/s. This demonstrates that the instrument-related systematics can be nearly eliminated over most of the spectral range by properly taking into account the complex shape of the instrumental line-spread function and paves the way for further optimizations of the wavelength calibration process.","sentences":["Achieving a truly accurate wavelength calibration of high-dispersion echelle spectrographs is a challenging task but crucially needed for certain science cases, e.g. to test for a possible variation of the fine-structure constant in quasar spectra.","One of the spectrographs best suited for this mission is VLT/ESPRESSO.","Nevertheless, previous studies have identified significant discrepancies between the classical wavelength solutions and the one derived independently from the laser frequency comb.","The dominant parts of these systematics were intra-order distortions, most-likely related to a deviation of the instrumental line-spread function from the assumed Gaussian shape.","Here, we therefore present a study focused on a detailed modeling of the ESPRESSO instrumental line-spread function.","We demonstrate that it is strongly asymmetric, non-Gaussian, different for the two slices and fibers, and varies significantly along the spectral orders.","Incorporating the determined non-parametric model in the wavelength calibration process drastically improves the wavelength calibration accuracy, reducing the discrepancies between the two independent wavelength solutions from 50m/s to about 10m/s.","The most striking success is, however, that the different fibers and slices now provide fully consistent measurements with a scatter of just a couple m/s.","This demonstrates that the instrument-related systematics can be nearly eliminated over most of the spectral range by properly taking into account the complex shape of the instrumental line-spread function and paves the way for further optimizations of the wavelength calibration process."],"url":"http://arxiv.org/abs/2404.05283v1","category":"astro-ph.IM"}
{"created":"2024-04-08 08:04:44","title":"Deep Optics for Video Snapshot Compressive Imaging","abstract":"Video snapshot compressive imaging (SCI) aims to capture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational reconstruction algorithm. Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications. Yet, there are two clouds in the sunshine of SCI: i) low dynamic range as a victim of high temporal multiplexing, and ii) existing deep learning algorithms' degradation on real system. To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network. Specifically, we first propose a new type of structural mask to realize motion-aware and full-dynamic-range measurement. Considering the motion awareness property in measurement domain, we develop an efficient network for video SCI reconstruction using Transformer to capture long-term temporal dependencies, dubbed Res2former. Moreover, sensor response is introduced into the forward model of video SCI to guarantee end-to-end model training close to real system. Finally, we implement the learned structural masks on a digital micro-mirror device. Experimental results on synthetic and real data validate the effectiveness of the proposed framework. We believe this is a milestone for real-world video SCI. The source code and data are available at https://github.com/pwangcs/DeepOpticsSCI.","sentences":["Video snapshot compressive imaging (SCI) aims to capture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational reconstruction algorithm.","Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications.","Yet, there are two clouds in the sunshine of SCI: i) low dynamic range as a victim of high temporal multiplexing, and ii) existing deep learning algorithms' degradation on real system.","To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network.","Specifically, we first propose a new type of structural mask to realize motion-aware and full-dynamic-range measurement.","Considering the motion awareness property in measurement domain, we develop an efficient network for video SCI reconstruction using Transformer to capture long-term temporal dependencies, dubbed Res2former.","Moreover, sensor response is introduced into the forward model of video SCI to guarantee end-to-end model training close to real system.","Finally, we implement the learned structural masks on a digital micro-mirror device.","Experimental results on synthetic and real data validate the effectiveness of the proposed framework.","We believe this is a milestone for real-world video SCI.","The source code and data are available at https://github.com/pwangcs/DeepOpticsSCI."],"url":"http://arxiv.org/abs/2404.05274v1","category":"cs.CV"}
{"created":"2024-04-08 08:02:28","title":"Creating highly symmetric qudit heralded entanglement through highly symmetric graphs","abstract":"Recent attention has turned to exploring quantum information within larger Hilbert spaces by utilizing qudits, which offer increased information capacity and potential for robust quantum communications. While the efficient generation of multipartite qudit entanglement is crucial for studying quantum correlations in high-dimensional Hilbert spaces, the increased dimension makes the circuit design challanging, especially when the entanglement is generated by heralding detections. In this work, we demonstrate that the graph picture of linear quantum networks (LQG picture) can provide a simplified method to generate qudit multipartite heralded entanglement of high symmetries. The LQG picture enables the reduction of circuit complexity by directly imposing the state symmetry onto the circuit structure. Leveraging this insight, we propose heralded schemes for generating $N$-partite $N$-level anti-symmetric (singlet) and symmetric (Dicke) states. Our study shed light on the optimal circuit design of high-dimensional entanglement with a systematic graphical strategy.","sentences":["Recent attention has turned to exploring quantum information within larger Hilbert spaces by utilizing qudits, which offer increased information capacity and potential for robust quantum communications.","While the efficient generation of multipartite qudit entanglement is crucial for studying quantum correlations in high-dimensional Hilbert spaces, the increased dimension makes the circuit design challanging, especially when the entanglement is generated by heralding detections.","In this work, we demonstrate that the graph picture of linear quantum networks (LQG picture) can provide a simplified method to generate qudit multipartite heralded entanglement of high symmetries.","The LQG picture enables the reduction of circuit complexity by directly imposing the state symmetry onto the circuit structure.","Leveraging this insight, we propose heralded schemes for generating $N$-partite $N$-level anti-symmetric (singlet) and symmetric (Dicke) states.","Our study shed light on the optimal circuit design of high-dimensional entanglement with a systematic graphical strategy."],"url":"http://arxiv.org/abs/2404.05273v1","category":"quant-ph"}
{"created":"2024-04-09 17:55:41","title":"Reconstructing Hand-Held Objects in 3D","abstract":"Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos. Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels. At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects. With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets. Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs. Subsequently, we use GPT-4(V) to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred geometry; we call this alignment Retrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions.","sentences":["Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos.","Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels.","At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects.","With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets.","Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs.","Subsequently, we use GPT-4(V) to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred geometry; we call this alignment Retrieval-Augmented Reconstruction (RAR).","Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions."],"url":"http://arxiv.org/abs/2404.06507v1","category":"cs.CV"}
{"created":"2024-04-09 17:44:15","title":"The jet of BP Tau","abstract":"A strong global magnetic field of young low-mass stars and a high accretion rate are the necessary conditions for the formation of collimated outflows (jets) from these objects. But it is still unclear whether these conditions are also sufficient. We aim to check whether BP Tau, an actively accreting young star with a strong magnetic field, has a jet. We carried out narrowband SII 672 nm imaging and spectroscopic observations of BP Tau and its vicinity. We find that BP Tau is a source of a Herbig-Haro flow (assigned number HH 1181), which includes two HH objects moving from the star in opposite directions and a micro- (counter-) jet of ~ 1\" projected length. The flow is oriented along position angle $59 \\pm 1$ degree.","sentences":["A strong global magnetic field of young low-mass stars and a high accretion rate are the necessary conditions for the formation of collimated outflows (jets) from these objects.","But it is still unclear whether these conditions are also sufficient.","We aim to check whether BP Tau, an actively accreting young star with a strong magnetic field, has a jet.","We carried out narrowband SII 672 nm imaging and spectroscopic observations of BP Tau and its vicinity.","We find that BP Tau is a source of a Herbig-Haro flow (assigned number HH 1181), which includes two HH objects moving from the star in opposite directions and a micro- (counter-) jet of ~ 1\" projected length.","The flow is oriented along position angle $59 \\pm 1$ degree."],"url":"http://arxiv.org/abs/2404.06491v1","category":"astro-ph.SR"}
{"created":"2024-04-09 17:32:47","title":"Exceptional zeros of Rankin-Selberg $L$-functions and joint Sato-Tate distributions","abstract":"Let $\\chi$ be an idele class character over a number field $F$, and let $\\pi,\\pi'$ be non-dihedral twist-inequivalent cuspidal automorphic representations of $\\mathrm{GL}_2(\\mathbb{A}_F)$. We prove the following results.   1. If $m,n\\geq 0$ are integers, $m+n\\geq 1$, $F$ is totally real, $\\chi$ corresponds with a ray class character, and $\\pi,\\pi'$ correspond with primitive non-CM holomorphic Hilbert cusp forms, then the Rankin-Selberg $L$-function $L(s,\\mathrm{Sym}^m(\\pi)\\times(\\mathrm{Sym}^n(\\pi')\\otimes\\chi))$ has a standard zero-free region with no exceptional Landau-Siegel zero. When $m,n\\geq 1$ and $m+n\\geq 4$, this is new even for $F=\\mathbb{Q}$. As an application, we establish the strongest known unconditional effective rates of convergence in the Sato-Tate distribution for $\\pi$ and the joint Sato-Tate distribution for $\\pi$ and $\\pi'$.   2. The Rankin-Selberg $L$-function $L(s,\\mathrm{Sym}^2(\\pi)\\times(\\mathrm{Sym}^2 (\\pi')\\otimes\\chi))$ has a standard zero-free region with no exceptional Landau-Siegel zero. Until now, this was only known when $\\pi=\\pi'$, $\\pi$ is self-dual, and $\\chi$ is trivial.","sentences":["Let $\\chi$ be an idele class character over a number field $F$, and let $\\pi,\\pi'$ be non-dihedral twist-inequivalent cuspidal automorphic representations of $\\mathrm{GL}_2(\\mathbb{A}_F)$. We prove the following results.   ","1.","If $m,n\\geq 0$ are integers, $m+n\\geq 1$, $F$ is totally real, $\\chi$ corresponds with a ray class character, and $\\pi,\\pi'$ correspond with primitive non-CM holomorphic Hilbert cusp forms, then the Rankin-Selberg $L$-function $L(s,\\mathrm{Sym}^m(\\pi)\\times(\\mathrm{Sym}^n(\\pi')\\otimes\\chi))$ has a standard zero-free region with no exceptional Landau-Siegel zero.","When $m,n\\geq 1$ and $m+n\\geq 4$, this is new even for $F=\\mathbb{Q}$. As an application, we establish the strongest known unconditional effective rates of convergence in the Sato-Tate distribution for $\\pi$ and the joint Sato-Tate distribution for $\\pi$ and $\\pi'$.   2.","The Rankin-Selberg $L$-function $L(s,\\mathrm{Sym}^2(\\pi)\\times(\\mathrm{Sym}^2 (\\pi')\\otimes\\chi))$ has a standard zero-free region with no exceptional Landau-Siegel zero.","Until now, this was only known when $\\pi=\\pi'$, $\\pi$ is self-dual, and $\\chi$ is trivial."],"url":"http://arxiv.org/abs/2404.06482v1","category":"math.NT"}
{"created":"2024-04-09 17:15:47","title":"Constraining electron number density in the Sun via Earth-based neutrino flavor data","abstract":"Neutrino flavor transformation offers a window into the physics of various astrophysical environments, including our Sun and the more exotic environs of core-collapse supernovae and binary neutron-star mergers. Here, we apply an inference framework - specifically: statistical data assimilation (SDA) - to neutrino flavor evolution in the Sun. We take a model for solar neutrino flavor evolution, together with Earth-based neutrino measurements, to infer solar properties. Specifically, we ask what signature of the radially-varying solar electron number density $n_e(r)$ is contained within these Earth-based measurements. Currently, the best estimates of $n_e(r)$ come from the standard solar model. We seek to ascertain, through novel application of the SDA method, whether estimates of the same from neutrino data can serve as independent constraints.","sentences":["Neutrino flavor transformation offers a window into the physics of various astrophysical environments, including our Sun and the more exotic environs of core-collapse supernovae and binary neutron-star mergers.","Here, we apply an inference framework - specifically: statistical data assimilation (SDA) - to neutrino flavor evolution in the Sun.","We take a model for solar neutrino flavor evolution, together with Earth-based neutrino measurements, to infer solar properties.","Specifically, we ask what signature of the radially-varying solar electron number density $n_e(r)$ is contained within these Earth-based measurements.","Currently, the best estimates of $n_e(r)$ come from the standard solar model.","We seek to ascertain, through novel application of the SDA method, whether estimates of the same from neutrino data can serve as independent constraints."],"url":"http://arxiv.org/abs/2404.06468v1","category":"astro-ph.SR"}
{"created":"2024-04-09 17:04:41","title":"On Hasse norm principle for 3-manifolds in arithmetic topology","abstract":"Following the analogies between knots and primes, 3-manifolds and number rings in arithmetic topology, we show a topological analogue of the Hasse norm principle for finite cyclic coverings of 3-manifolds, which was originally stated for finite cyclic extensions of number fields.","sentences":["Following the analogies between knots and primes, 3-manifolds and number rings in arithmetic topology, we show a topological analogue of the Hasse norm principle for finite cyclic coverings of 3-manifolds, which was originally stated for finite cyclic extensions of number fields."],"url":"http://arxiv.org/abs/2404.06464v1","category":"math.GT"}
{"created":"2024-04-09 14:19:21","title":"Calibration of luminosity correlations of gamma-ray bursts using quasars","abstract":"In order to test the efficacy of Gamma-ray Bursts (GRBs) as cosmological probes, we the characterize the scatter in the correlations between six pairs of GRB related observables, which have previously also been studied in arXiv:2011.14040. However, some of these observables depend on the luminosity distance, for which one needs to assume an underlying cosmological model. In order to circumvent this circularity problem, we use X-ray and UV fluxes of quasars as distance anchors to calculate the luminosity distance in a model-independent manner, which in turn gets used to calculate the GRB-related quantities. We find that all the six pairs of regression relations show a high intrinsic scatter for both the low and high redshift sample. This implies that these GRB observables cannot be used as model-independent cosmological probes.","sentences":["In order to test the efficacy of Gamma-ray Bursts (GRBs) as cosmological probes, we the characterize the scatter in the correlations between six pairs of GRB related observables, which have previously also been studied in arXiv:2011.14040.","However, some of these observables depend on the luminosity distance, for which one needs to assume an underlying cosmological model.","In order to circumvent this circularity problem, we use X-ray and UV fluxes of quasars as distance anchors to calculate the luminosity distance in a model-independent manner, which in turn gets used to calculate the GRB-related quantities.","We find that all the six pairs of regression relations show a high intrinsic scatter for both the low and high redshift sample.","This implies that these GRB observables cannot be used as model-independent cosmological probes."],"url":"http://arxiv.org/abs/2404.06334v1","category":"astro-ph.HE"}
{"created":"2024-04-09 14:07:16","title":"Description of the processes $e^+e^- \\to K^+K^-\u03c0^0$ and $e^+e^- \\to K^+K^-\u03b7$ within the extended NJL model","abstract":"In the framework of the $U(3) \\times U(3)$ extended Nambu-Jona-Lasinio model, the processes $e^+e^- \\to K^+K^-\\pi^0$ and $e^+e^- \\to K^+K^-\\eta$ are described taking into account both ground and first radially excited intermediate meson states. It is shown that channels with radially excited $\\phi(1680)$ meson are dominant in both processes. The influence on the results of the appearance of phase factors of radially excited intermediate states, the existence of which is indicated by experimental data, is discussed.","sentences":["In the framework of the $U(3) \\times U(3)$ extended Nambu-Jona-Lasinio model, the processes $e^+e^- \\to K^+K^-\\pi^0$ and $e^+e^- \\to K^+K^-\\eta$ are described taking into account both ground and first radially excited intermediate meson states.","It is shown that channels with radially excited $\\phi(1680)$ meson are dominant in both processes.","The influence on the results of the appearance of phase factors of radially excited intermediate states, the existence of which is indicated by experimental data, is discussed."],"url":"http://arxiv.org/abs/2404.06329v1","category":"hep-ph"}
{"created":"2024-04-09 13:25:41","title":"Quantum Simulating Nature's Fundamental Fields","abstract":"Simulating key static and dynamic properties of matter -- from creation in the Big Bang to evolution into sub-atomic and astrophysical environments -- arising from the underlying fundamental quantum fields of the Standard Model and their effective descriptions, lies beyond the capabilities of classical computation alone. Advances in quantum technologies have improved control over quantum entanglement and coherence to the point where robust simulations are anticipated to be possible in the foreseeable future. We discuss the emerging area of quantum simulations of Standard-Model physics, challenges that lie ahead, and opportunities for progress in the context of nuclear and high-energy physics.","sentences":["Simulating key static and dynamic properties of matter -- from creation in the Big Bang to evolution into sub-atomic and astrophysical environments -- arising from the underlying fundamental quantum fields of the Standard Model and their effective descriptions, lies beyond the capabilities of classical computation alone.","Advances in quantum technologies have improved control over quantum entanglement and coherence to the point where robust simulations are anticipated to be possible in the foreseeable future.","We discuss the emerging area of quantum simulations of Standard-Model physics, challenges that lie ahead, and opportunities for progress in the context of nuclear and high-energy physics."],"url":"http://arxiv.org/abs/2404.06298v1","category":"hep-ph"}
{"created":"2024-04-09 12:44:39","title":"Quantifying the U $5f$ covalence and degree of localization in U intermetallics","abstract":"A procedure for quantifying the U $5f$ electrons' covalence and degree of localization in U intermetallic compounds is presented. To this end, bulk sensitive hard and soft x-ray photoelectron spectroscopy were utilized in combination with density-functional theory (DFT) plus dynamical mean-field theory (DMFT) calculations. The energy dependence of the photoionization cross-sections allows the disentanglement of the U\\,$5f$ contribution to the valence band from the various other atomic subshells so that the computational parameters in the DFT\\,+\\,DMFT can be reliably determined. Applying this method to UGa$_2$ and UB$_2$ as model compounds from opposite ends of the (de)localization range, we have achieved excellent simulations of the valence band and core-level spectra. The width in the distribution of atomic U\\,$5f$ configurations contributing to the ground state, as obtained from the calculations, quantifies the correlated nature and degree of localization of the U\\,5$f$. The findings permit answering the longstanding question why different spectroscopic techniques give seemingly different numbers for the U 5$f$ valence in intermetallic U compounds.","sentences":["A procedure for quantifying the U $5f$ electrons' covalence and degree of localization in U intermetallic compounds is presented.","To this end, bulk sensitive hard and soft x-ray photoelectron spectroscopy were utilized in combination with density-functional theory (DFT) plus dynamical mean-field theory (DMFT) calculations.","The energy dependence of the photoionization cross-sections allows the disentanglement of the U\\,$5f$ contribution to the valence band from the various other atomic subshells so that the computational parameters in the DFT\\,+\\,DMFT can be reliably determined.","Applying this method to UGa$_2$ and UB$_2$ as model compounds from opposite ends of the (de)localization range, we have achieved excellent simulations of the valence band and core-level spectra.","The width in the distribution of atomic U\\,$5f$ configurations contributing to the ground state, as obtained from the calculations, quantifies the correlated nature and degree of localization of the U\\,5$f$. The findings permit answering the longstanding question why different spectroscopic techniques give seemingly different numbers for the U 5$f$ valence in intermetallic U compounds."],"url":"http://arxiv.org/abs/2404.06266v1","category":"cond-mat.str-el"}
{"created":"2024-04-09 12:20:39","title":"Perturbative Construction of Equilibrium States for Interacting Fermionic Field Theories","abstract":"In this paper, we aim to extend to interacting massive and massless fermionic theories the recent perturbative construction of equilibrium states developed within the framework of perturbative algebraic quantum field theory. We analyze the case of interactions which depend on time by a smooth switch-on function and on space by a suitably bounded function that multiplies an interaction Lagrangian density constructed with the field of the theory. The construction is achieved by first considering the case of compact support and, in a second step, by removing the space cutoff with a suitable limit (adiabatic limit). As an application, we consider a Dirac field interacting with a classical stationary background electromagnetic potential, and we compute at first perturbative order (linear response) the expectation value of the conserved current on the equilibrium state for the interacting theory. The resulting expectation value is written as a convolution, in the space coordinates, between the electromagnetic potential and an integral kernel which, at vanishing conjugate momentum, gives the inverse of the square Debye screening length at finite temperature. The corresponding Debye screening effect is visible in the backreaction treated semiclassically of this current on the classical background electromagnetic potential sourced by a classical external current.","sentences":["In this paper, we aim to extend to interacting massive and massless fermionic theories the recent perturbative construction of equilibrium states developed within the framework of perturbative algebraic quantum field theory.","We analyze the case of interactions which depend on time by a smooth switch-on function and on space by a suitably bounded function that multiplies an interaction Lagrangian density constructed with the field of the theory.","The construction is achieved by first considering the case of compact support and, in a second step, by removing the space cutoff with a suitable limit (adiabatic limit).","As an application, we consider a Dirac field interacting with a classical stationary background electromagnetic potential, and we compute at first perturbative order (linear response) the expectation value of the conserved current on the equilibrium state for the interacting theory.","The resulting expectation value is written as a convolution, in the space coordinates, between the electromagnetic potential and an integral kernel which, at vanishing conjugate momentum, gives the inverse of the square Debye screening length at finite temperature.","The corresponding Debye screening effect is visible in the backreaction treated semiclassically of this current on the classical background electromagnetic potential sourced by a classical external current."],"url":"http://arxiv.org/abs/2404.06249v1","category":"math-ph"}
{"created":"2024-04-09 12:14:40","title":"Scalar fields, localized structures and the Starobinsky model","abstract":"This work deals with the presence of localized static structures in the real line, described by relativistic real scalar fields in two spacetime dimensions. We consider models featuring both standard and modified kinematics, where we employ two intriguing potentials supporting defect solutions. The first potential can transform kink into compacton in the standard framework, while the second one is based on the inflationary Starobinsky model. Interesting possibilities unseen in previous investigations are described, in particular, for the case related to the Starobinsky potential. The addressed potentials are inserted into a broader framework, and so the extended models are described by a wider set of solutions. This investigation also reveals the presence of the twinlike behavior for a specific compact configuration, which solves two distinct models.","sentences":["This work deals with the presence of localized static structures in the real line, described by relativistic real scalar fields in two spacetime dimensions.","We consider models featuring both standard and modified kinematics, where we employ two intriguing potentials supporting defect solutions.","The first potential can transform kink into compacton in the standard framework, while the second one is based on the inflationary Starobinsky model.","Interesting possibilities unseen in previous investigations are described, in particular, for the case related to the Starobinsky potential.","The addressed potentials are inserted into a broader framework, and so the extended models are described by a wider set of solutions.","This investigation also reveals the presence of the twinlike behavior for a specific compact configuration, which solves two distinct models."],"url":"http://arxiv.org/abs/2404.06248v1","category":"hep-th"}
{"created":"2024-04-09 11:58:33","title":"Least Squares-Based Permutation Tests in Time Series","abstract":"This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure. In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response. Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail. However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds. We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well.","sentences":["This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure.","In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response.","Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail.","However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds.","We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well."],"url":"http://arxiv.org/abs/2404.06238v1","category":"math.ST"}
{"created":"2024-04-09 11:47:12","title":"Understanding the thermal and magnetic properties of a X-class flare in the low solar atmosphere","abstract":"We analyse the spatial distribution and vertical stratification of the physical parameters of the solar atmosphere when an X-class flare occurs. We made use of observations acquired by the Interferometric Bidimensional Spectropolarimeter instrument when observing the full Stokes parameters for the Fe I 6173 A and Ca II 8542 A transitions. We analysed the observed spectra using the newly developed DeSIRe code to infer the atmospheric parameters at photospheric and chromospheric layers over the entire observed field of view. Our findings reveal that the chromosphere is characterised by temperature enhancements and strong upflows in the flare ribbon area, which indicates that the flaring event is producing hot material that is moving outwards from the Sun. We did not detect any trace of temperature enhancements or strong velocities (of any sign) at photospheric layers, signalling that the impact of the flaring event mainly happens at the middle and upper layers. The information about the magnetic field vector revealed relatively smooth stratifications with height for both magnetic field strength and inclination. Still, when examining the spatial distribution of the magnetic field inclination, we observed the presence of large-scale mixed polarities in the regions where the flare ribbon is located. These results suggest that the interaction between those mixed polarities could be the flare's triggering mechanism.","sentences":["We analyse the spatial distribution and vertical stratification of the physical parameters of the solar atmosphere when an X-class flare occurs.","We made use of observations acquired by the Interferometric Bidimensional Spectropolarimeter instrument when observing the full Stokes parameters for the Fe I 6173","A and Ca II 8542 A transitions.","We analysed the observed spectra using the newly developed DeSIRe code to infer the atmospheric parameters at photospheric and chromospheric layers over the entire observed field of view.","Our findings reveal that the chromosphere is characterised by temperature enhancements and strong upflows in the flare ribbon area, which indicates that the flaring event is producing hot material that is moving outwards from the Sun.","We did not detect any trace of temperature enhancements or strong velocities (of any sign) at photospheric layers, signalling that the impact of the flaring event mainly happens at the middle and upper layers.","The information about the magnetic field vector revealed relatively smooth stratifications with height for both magnetic field strength and inclination.","Still, when examining the spatial distribution of the magnetic field inclination, we observed the presence of large-scale mixed polarities in the regions where the flare ribbon is located.","These results suggest that the interaction between those mixed polarities could be the flare's triggering mechanism."],"url":"http://arxiv.org/abs/2404.06231v1","category":"astro-ph.SR"}
{"created":"2024-04-09 11:39:53","title":"Understanding Cross-Lingual Alignment -- A Survey","abstract":"Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years. We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field. We present different understandings of cross-lingual alignment and their limitations. We provide a qualitative summary of results from a large number of surveyed papers. Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key.","sentences":["Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years.","We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field.","We present different understandings of cross-lingual alignment and their limitations.","We provide a qualitative summary of results from a large number of surveyed papers.","Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key."],"url":"http://arxiv.org/abs/2404.06228v1","category":"cs.CL"}
{"created":"2024-04-09 11:23:49","title":"Lifetimes of excited states in Rh-","abstract":"The radiative decay of excited states of the negative ion of rhodium, Rh$^-$, has been investigated experimentally and theoretically. The experiments were conducted at the Double ElectroStatic Ion Ring Experiment (DESIREE) facility at Stockholm University using selective photodetachment from a stored ion beam to monitor the time evolution of the excited state populations. The lifetimes of the Rh$^-$ $^3F_{3}$ and $^3F_{2}$ fine structure levels were measured to be 3.2(6)~s and 21(4)~s, respectively. An additional, previously unreported, higher-lying bound state of mixed $^1D_2+^3P_2+(4d^95s)^1D_2+^3F_2$ composition was observed and found to have a lifetime of 10.9(8)s. The binding energy of this state was determined to be in the interval $0.1584(2) $ eV $ < E_b < 0.2669(2)$ eV, using laser photodetachment threshold (LPT) spectroscopy. An autodetaching state with a lifetime of 480(10) microseconds was also observed. Theoretical calculations of the excited-state compositions, energies, and magnetic-dipole transition lifetimes were performed using the multiconfiguration Dirac-Hartree-Fock and relativistic configuration interaction methods. The calculated lifetimes of the $^3F_{3}$ and $^3F_{2}$ fine structure levels are in excellent agreement with the measured values. The present study should provide valuable insights into electron correlation effects in negative ions and forbidden radiative transitions.","sentences":["The radiative decay of excited states of the negative ion of rhodium, Rh$^-$, has been investigated experimentally and theoretically.","The experiments were conducted at the Double ElectroStatic Ion Ring Experiment (DESIREE) facility at Stockholm University using selective photodetachment from a stored ion beam to monitor the time evolution of the excited state populations.","The lifetimes of the Rh$^-$ $^3F_{3}$ and $^3F_{2}$ fine structure levels were measured to be 3.2(6)~s and 21(4)~s, respectively.","An additional, previously unreported, higher-lying bound state of mixed $^1D_2+^3P_2+(4d^95s)^1D_2+^3F_2$ composition was observed and found to have a lifetime of 10.9(8)s.","The binding energy of this state was determined to be in the interval $0.1584(2) $ eV $ < E_b < 0.2669(2)$ eV, using laser photodetachment threshold (LPT) spectroscopy.","An autodetaching state with a lifetime of 480(10) microseconds was also observed.","Theoretical calculations of the excited-state compositions, energies, and magnetic-dipole transition lifetimes were performed using the multiconfiguration Dirac-Hartree-Fock and relativistic configuration interaction methods.","The calculated lifetimes of the $^3F_{3}$ and $^3F_{2}$ fine structure levels are in excellent agreement with the measured values.","The present study should provide valuable insights into electron correlation effects in negative ions and forbidden radiative transitions."],"url":"http://arxiv.org/abs/2404.06222v1","category":"physics.atom-ph"}
{"created":"2024-04-09 11:20:59","title":"Polarization and quantum entanglement effects in $B^\\pm_c\\to J/\u03c8+\u03c0^\\pm +\u03c0^0$ process","abstract":"Motivated by the very recent observation of the $B^+_c\\to J/\\psi+\\pi^+ +\\pi^0$ decay using proton-proton collision data by the LHCb collaboration, we study the four-body angular distributions and the quantum entanglement effects in the $B^+_c\\to J/\\psi+\\pi^+ +\\pi^0$ associated with $J/\\psi\\to \\mu^++\\mu^-$. The helicity angular distributions are given in the QCD effective theory and the von Neumann entropy is obtained in $B^\\pm_c\\to J/\\psi(\\to \\mu^+\\mu^-)+\\rho^\\pm(\\to \\pi^\\pm \\pi^0)$ decay process.","sentences":["Motivated by the very recent observation of the $B^+_c\\to J/\\psi+\\pi^+ +\\pi^0$ decay using proton-proton collision data by the LHCb collaboration, we study the four-body angular distributions and the quantum entanglement effects in the $B^+_c\\to J/\\psi+\\pi^+ +\\pi^0$ associated with $J/\\psi\\to \\mu^++\\mu^-$.","The helicity angular distributions are given in the QCD effective theory and the von Neumann entropy is obtained in $B^\\pm_c\\to J/\\psi(\\to \\mu^+\\mu^-)+\\rho^\\pm(\\to \\pi^\\pm \\pi^0)$ decay process."],"url":"http://arxiv.org/abs/2404.06221v1","category":"hep-ph"}
{"created":"2024-04-09 10:25:06","title":"Monoidal Context Theory","abstract":"We universally characterize the produoidal category of monoidal lenses over a monoidal category. In the same way that each category induces a cofree promonoidal category of spliced arrows, each monoidal category induces a cofree produoidal category of monoidal spliced arrows; monoidal lenses are the free normalization of the cofree produoidal category of monoidal spliced arrows. We apply the characterization of symmetric monoidal lenses to the analysis of multi-party message-passing protocols. We introduce a minimalistic axiomatization of message passing -- message theories -- and we construct combinatorially the free message theory over a set. Symmetric monoidal lenses are the derivations of the free message theory over a symmetric monoidal category.","sentences":["We universally characterize the produoidal category of monoidal lenses over a monoidal category.","In the same way that each category induces a cofree promonoidal category of spliced arrows, each monoidal category induces a cofree produoidal category of monoidal spliced arrows; monoidal lenses are the free normalization of the cofree produoidal category of monoidal spliced arrows.","We apply the characterization of symmetric monoidal lenses to the analysis of multi-party message-passing protocols.","We introduce a minimalistic axiomatization of message passing -- message theories -- and we construct combinatorially the free message theory over a set.","Symmetric monoidal lenses are the derivations of the free message theory over a symmetric monoidal category."],"url":"http://arxiv.org/abs/2404.06192v1","category":"math.CT"}
{"created":"2024-04-09 10:10:58","title":"Wess-Zumino-Witten Terms of $Sp$ QCD by Bordism Theory","abstract":"We investigate the four-dimensional Wess-Zumino-Witten (WZW) terms within the framework of $Sp$ quantum chromodynamics (QCD) using invertible field theory through bordism theory. We present a novel approach aimed at circumventing both perturbative and non-perturbative gauge anomalies on spacetime manifolds endowed with spin structures. We study both ungauged and gauged WZW terms including the problems of the topological consistency of gauged WZW terms.","sentences":["We investigate the four-dimensional Wess-Zumino-Witten (WZW) terms within the framework of $Sp$ quantum chromodynamics (QCD) using invertible field theory through bordism theory.","We present a novel approach aimed at circumventing both perturbative and non-perturbative gauge anomalies on spacetime manifolds endowed with spin structures.","We study both ungauged and gauged WZW terms including the problems of the topological consistency of gauged WZW terms."],"url":"http://arxiv.org/abs/2404.06185v1","category":"hep-th"}
{"created":"2024-04-09 10:09:30","title":"Characterising and tackling thermally induced zero-drift in displacement measuring interferometry using temperature-controlled enclosure","abstract":"Our research efforts in displacement measurement interferometry focused on long-term drifts initiated an extended experimental investigation in the interferometric assemblies of our design. We aimed to analyze, characterize and tackle the long-term measurement stability, expressed as the zero-drift, with special attention to the thermal effects. For the experimentation, we developed a thermostatic chamber equipped with an active temperature regulation, array of sensors and control electronics. With either the finely stabilized temperature or with the thermal cycling, we are able to carry out a range of investigations: verification of modified design or prototype interferometers, testing of production pieces, characterization of integrated assemblies and units in terms of the zero drift and the susceptibility to thermal effect - the temperature sensitivity, expressed as $\\delta L / \\delta T$ in nm.K$^{-1}$.   Note: This is the version of the article before peer review or editing, as submitted by an author to Measurement Science and Technology. IOP Publishing Ltd is not responsible for any errors or omissions in this version of the manuscript or any version derived from it.","sentences":["Our research efforts in displacement measurement interferometry focused on long-term drifts initiated an extended experimental investigation in the interferometric assemblies of our design.","We aimed to analyze, characterize and tackle the long-term measurement stability, expressed as the zero-drift, with special attention to the thermal effects.","For the experimentation, we developed a thermostatic chamber equipped with an active temperature regulation, array of sensors and control electronics.","With either the finely stabilized temperature or with the thermal cycling, we are able to carry out a range of investigations: verification of modified design or prototype interferometers, testing of production pieces, characterization of integrated assemblies and units in terms of the zero drift and the susceptibility to thermal effect - the temperature sensitivity, expressed as $\\delta L / \\delta T$ in nm.","K$^{-1}$.   Note: This is the version of the article before peer review or editing, as submitted by an author to Measurement Science and Technology.","IOP Publishing Ltd is not responsible for any errors or omissions in this version of the manuscript or any version derived from it."],"url":"http://arxiv.org/abs/2404.06184v1","category":"physics.ins-det"}
{"created":"2024-04-09 09:33:15","title":"Production of $D^{(*)}\\bar{D}^{(*)}$ near the thresholds in $e^{+}e^{-}$ annihilation","abstract":"It is shown that the nontrivial energy dependencies of $D\\bar{D}$, $D\\bar{D}^{*}$, and $D^{*}\\bar{D}^{*}$ pair production cross sections in $e^{+}e^{- }$ annihilation are well described within the approach based on account for the final-state interaction of produced particles. This statement is valid for production of charged and neutral particles. Interaction of $D^{(*)}$ and $\\bar{D}^{(*)}$ is taken into account using the effective potential method. Its applicability is based on the fact that for near-threshold resonance the characteristic width of peak in the wave function is much larger than the interaction radius. The transition amplitudes between all three channels play an important role in the description of cross sections. These transitions are possible since all channels have the same quantum numbers $J^{PC}=1^{--}$.","sentences":["It is shown that the nontrivial energy dependencies of $D\\bar{D}$, $D\\bar{D}^{*}$, and $D^{*}\\bar{D}^{*}$ pair production cross sections in $e^{+}e^{- }$ annihilation are well described within the approach based on account for the final-state interaction of produced particles.","This statement is valid for production of charged and neutral particles.","Interaction of $D^{(*)}$ and $\\bar{D}^{(*)}$ is taken into account using the effective potential method.","Its applicability is based on the fact that for near-threshold resonance the characteristic width of peak in the wave function is much larger than the interaction radius.","The transition amplitudes between all three channels play an important role in the description of cross sections.","These transitions are possible since all channels have the same quantum numbers $J^{PC}=1^{--}$."],"url":"http://arxiv.org/abs/2404.06160v1","category":"hep-ph"}
{"created":"2024-04-09 08:49:53","title":"Quark flavor violation and axion-like particles from top-quark decays at the LHC","abstract":"We study axion-like particles (ALPs) with quark-flavor-violating couplings at the LHC. Specifically, we focus on the theoretical scenario with ALP-top-up and ALP-top-charm interactions, in addition to the more common quark-flavor-diagonal couplings. The ALPs can thus originate from decays of top quarks which are pair produced in large numbers at the LHC, and then decay to jets. If these couplings to the quarks are tiny and the ALPs have $\\mathcal{O}(10)$ GeV masses, they are long-lived, leading to signatures of displaced vertex plus multiple jets, which have the advantage of suppression of background events at the LHC. We recast a recent ATLAS search for the same signature and reinterpret the results in terms of bounds on the long-lived ALP in our theoretical scenario. We find that the LHC with the full Run 2 dataset can place stringent limits, while at the future high-luminosity LHC with 3 ab$^{-1}$ integrated luminosity stronger sensitivities are expected.","sentences":["We study axion-like particles (ALPs) with quark-flavor-violating couplings at the LHC.","Specifically, we focus on the theoretical scenario with ALP-top-up and ALP-top-charm interactions, in addition to the more common quark-flavor-diagonal couplings.","The ALPs can thus originate from decays of top quarks which are pair produced in large numbers at the LHC, and then decay to jets.","If these couplings to the quarks are tiny and the ALPs have $\\mathcal{O}(10)$ GeV masses, they are long-lived, leading to signatures of displaced vertex plus multiple jets, which have the advantage of suppression of background events at the LHC.","We recast a recent ATLAS search for the same signature and reinterpret the results in terms of bounds on the long-lived ALP in our theoretical scenario.","We find that the LHC with the full Run 2 dataset can place stringent limits, while at the future high-luminosity LHC with 3 ab$^{-1}$ integrated luminosity stronger sensitivities are expected."],"url":"http://arxiv.org/abs/2404.06126v1","category":"hep-ph"}
{"created":"2024-04-09 08:40:36","title":"Aspects of confinement within non-Abelian gauge theories","abstract":"These lectures cover two aspects: first, irrespectively of the particular approach followed to tackle the QCD phase diagram, we introduce some tools that help discussing the confinement/deconfinement transition in the continuum; second, within one particular continuum approach, based on the Curci-Ferrari model, we illustrate the use of these various notions in order the describe to the confinement/deconfinement transition.","sentences":["These lectures cover two aspects: first, irrespectively of the particular approach followed to tackle the QCD phase diagram, we introduce some tools that help discussing the confinement/deconfinement transition in the continuum; second, within one particular continuum approach, based on the Curci-Ferrari model, we illustrate the use of these various notions in order the describe to the confinement/deconfinement transition."],"url":"http://arxiv.org/abs/2404.06118v1","category":"hep-ph"}
{"created":"2024-04-09 08:38:54","title":"Soft contributions to the thermal Higgs width across an electroweak phase transition","abstract":"We estimate the equilibration rate of a nearly homogeneous Higgs field, displaced from its ground state during the onset of an electroweak phase transition. The computation is carried out with Hard Thermal Loop resummed perturbation theory, and a significant part of the result originates from Bose-enhanced $t$-channel $2\\leftrightarrow 2$ scatterings. The expression is shown to be IR finite and gauge independent. Possible applications to Langevin simulations of bubble nucleation are mentioned, and we also contrast with the friction affecting bubble growth.","sentences":["We estimate the equilibration rate of a nearly homogeneous Higgs field, displaced from its ground state during the onset of an electroweak phase transition.","The computation is carried out with Hard Thermal Loop resummed perturbation theory, and a significant part of the result originates from Bose-enhanced $t$-channel $2\\leftrightarrow 2$ scatterings.","The expression is shown to be IR finite and gauge independent.","Possible applications to Langevin simulations of bubble nucleation are mentioned, and we also contrast with the friction affecting bubble growth."],"url":"http://arxiv.org/abs/2404.06116v1","category":"hep-ph"}
{"created":"2024-04-09 08:08:22","title":"Estimating the lateral speed of a fast shock driven by a coronal mass ejection at the location of solar radio emissions","abstract":"Fast coronal mass ejections (CMEs) can drive shock waves capable of accelerating electrons to high energies. These shock-accelerated electrons act as sources of electromagnetic radiation, often in the form of solar radio bursts. Recent findings suggest that radio imaging of solar radio bursts can provide a means to estimate the lateral expansion of CMEs and associated shocks in the low corona. Our aim is to estimate the expansion speed of a CME-driven shock at the locations of radio emission using 3D reconstructions of the shock wave from multiple viewpoints. We estimated the 3D location of radio emission using radio imaging from the Nan\\c{c}ay Radioheliograph and the 3D location of the shock. The 3D shock was reconstructed using white-light and extreme ultraviolet images of the CME from the Solar Terrestrial Relations Observatory, Solar Dynamics Observatory, and the Solar and Heliospheric Observatory. The lateral expansion speed of the CME-driven shock at the electron acceleration locations was then estimated using the approximate 3D locations of the radio emission on the surface of the shock. The radio bursts associated with the CME were found to reside at the flank of the expanding CME-driven shock. We identified two prominent radio sources at two different locations and found that the lateral speed of the shock was in the range of $800-1000\\,\\mathrm{km\\,s^{-1}}$ at these locations. Such a high speed during the early stages of the eruption already indicates the presence of a fast shock in the low corona. We also found a larger ratio between the radial and lateral expansion speed compared to values obtained higher up in the corona. The high shock speed obtained is indicative of a fast acceleration during the initial stage of the eruption. This acceleration is most likely one of the key parameters contributing to the presence of metric radio emissions, such as type II radio bursts.","sentences":["Fast coronal mass ejections (CMEs) can drive shock waves capable of accelerating electrons to high energies.","These shock-accelerated electrons act as sources of electromagnetic radiation, often in the form of solar radio bursts.","Recent findings suggest that radio imaging of solar radio bursts can provide a means to estimate the lateral expansion of CMEs and associated shocks in the low corona.","Our aim is to estimate the expansion speed of a CME-driven shock at the locations of radio emission using 3D reconstructions of the shock wave from multiple viewpoints.","We estimated the 3D location of radio emission using radio imaging from the Nan\\c{c}ay Radioheliograph and the 3D location of the shock.","The 3D shock was reconstructed using white-light and extreme ultraviolet images of the CME from the Solar Terrestrial Relations Observatory, Solar Dynamics Observatory, and the Solar and Heliospheric Observatory.","The lateral expansion speed of the CME-driven shock at the electron acceleration locations was then estimated using the approximate 3D locations of the radio emission on the surface of the shock.","The radio bursts associated with the CME were found to reside at the flank of the expanding CME-driven shock.","We identified two prominent radio sources at two different locations and found that the lateral speed of the shock was in the range of $800-1000\\,\\mathrm{km\\,s^{-1}}$ at these locations.","Such a high speed during the early stages of the eruption already indicates the presence of a fast shock in the low corona.","We also found a larger ratio between the radial and lateral expansion speed compared to values obtained higher up in the corona.","The high shock speed obtained is indicative of a fast acceleration during the initial stage of the eruption.","This acceleration is most likely one of the key parameters contributing to the presence of metric radio emissions, such as type II radio bursts."],"url":"http://arxiv.org/abs/2404.06102v1","category":"astro-ph.SR"}
{"created":"2024-04-09 08:05:16","title":"Ginzburg-Landau description for multicritical Yang-Lee models","abstract":"We revisit and extend Fisher's argument for a Ginzburg-Landau description of multicritical Yang-Lee models in terms of a single boson Lagrangian with potential $\\varphi^2 (i \\varphi)^n$. We explicitly study the cases of $n=1,2$ by a Truncated Hamiltonian Approach based on the free massive boson perturbed by $\\boldsymbol P\\boldsymbol T$ symmetric deformations, providing clear evidence of the spontaneous breaking of $\\boldsymbol P \\boldsymbol T$ symmetry. For $n=1$, the symmetric and the broken phases are separated by the critical point corresponding to the minimal model $\\mathcal M(2,5)$, while for $n=2$, they are separated by a critical manifold corresponding to the minimal model $\\mathcal M(2,5)$ with $\\mathcal M(2,7)$ on its boundary. Our numerical analysis strongly supports our Ginzburg-Landau descriptions for multicritical Yang-Lee models.","sentences":["We revisit and extend Fisher's argument for a Ginzburg-Landau description of multicritical Yang-Lee models in terms of a single boson Lagrangian with potential $\\varphi^2 (i \\varphi)^n$. We explicitly study the cases of $n=1,2$ by a Truncated Hamiltonian Approach based on the free massive boson perturbed by $\\boldsymbol P\\boldsymbol T$ symmetric deformations, providing clear evidence of the spontaneous breaking of $\\boldsymbol P \\boldsymbol T$ symmetry.","For $n=1$, the symmetric and the broken phases are separated by the critical point corresponding to the minimal model $\\mathcal M(2,5)$, while for $n=2$, they are separated by a critical manifold corresponding to the minimal model $\\mathcal M(2,5)$ with $\\mathcal M(2,7)$ on its boundary.","Our numerical analysis strongly supports our Ginzburg-Landau descriptions for multicritical Yang-Lee models."],"url":"http://arxiv.org/abs/2404.06100v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-09 08:00:58","title":"Hydrostatic pressure control of the spin-orbit proximity effect and spin relaxation in a phosphorene-WSe$_2$ heterostructure","abstract":"Effective control of interlayer interactions is a key element in modifying the properties of van der Waals heterostructures and the next step toward their practical applications. Focusing on the phosphorene-WSe$_2$ heterostructure, we demonstrate, using first-principles calculations, how the spin-orbit coupling can be transferred from WSe$_2$, a strong spin-orbit coupling material, to phosphorene and further amplified by applying vertical pressure. We simulate external pressure by changing the interlayer distance between bilayer constituents and show that it is possible to tune the spin-orbit field of phosphorene holes in a controllable way. By fitting effective electronic states of the proposed Hamiltonian to the first principles data, we reveal that the spin-orbit coupling in phosphorene hole bands is enhanced more than two times for experimentally accessible pressures up to 17 kbar. Finally, we find that the pressure-enhanced spin-orbit coupling boosts the Dyakonov-Perel spin relaxation mechanism, reducing the spin lifetime of phosphorene holes by factor 4.","sentences":["Effective control of interlayer interactions is a key element in modifying the properties of van der Waals heterostructures and the next step toward their practical applications.","Focusing on the phosphorene-WSe$_2$ heterostructure, we demonstrate, using first-principles calculations, how the spin-orbit coupling can be transferred from WSe$_2$, a strong spin-orbit coupling material, to phosphorene and further amplified by applying vertical pressure.","We simulate external pressure by changing the interlayer distance between bilayer constituents and show that it is possible to tune the spin-orbit field of phosphorene holes in a controllable way.","By fitting effective electronic states of the proposed Hamiltonian to the first principles data, we reveal that the spin-orbit coupling in phosphorene hole bands is enhanced more than two times for experimentally accessible pressures up to 17 kbar.","Finally, we find that the pressure-enhanced spin-orbit coupling boosts the Dyakonov-Perel spin relaxation mechanism, reducing the spin lifetime of phosphorene holes by factor 4."],"url":"http://arxiv.org/abs/2404.06097v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 07:23:19","title":"A Knowledge Producer's View on the Knowledge Commons","abstract":"Hardin introduced the notorious concept of \"tragedy of the commons\". Worrying about the consequences of human overpopulation on the planet, he discussed \"hard problems\": problems with no technical solutions, that can only be addressed by way of an evolving morality. Hardin's tragedy of the commons predicts that the hard problem of human population growth directly implies a hard problem of overuse or pollution of the commons. This paper focuses on the knowledge commons. A technical proposal is presented, based on a JSON schema for structuring pieces of knowledge. This is used to show that even if the knowledge commons satisfies the necessary conditions of the tragedy of the commons, the ensuing problems are not necessarily hard. Some can be made trivial by relying on traditional principles implemented in a technical framework.","sentences":["Hardin introduced the notorious concept of \"tragedy of the commons\".","Worrying about the consequences of human overpopulation on the planet, he discussed \"hard problems\": problems with no technical solutions, that can only be addressed by way of an evolving morality.","Hardin's tragedy of the commons predicts that the hard problem of human population growth directly implies a hard problem of overuse or pollution of the commons.","This paper focuses on the knowledge commons.","A technical proposal is presented, based on a JSON schema for structuring pieces of knowledge.","This is used to show that even if the knowledge commons satisfies the necessary conditions of the tragedy of the commons, the ensuing problems are not necessarily hard.","Some can be made trivial by relying on traditional principles implemented in a technical framework."],"url":"http://arxiv.org/abs/2404.06073v1","category":"cs.DL"}
{"created":"2024-04-09 07:16:18","title":"Complete Active Space Iterative Coupled Cluster Theory","abstract":"In this work, we investigate the possibility of improving multireference-driven coupled cluster (CC) approaches with an algorithm that iteratively combines complete active space (CAS) calculations with tailored CC and externally corrected CC. This is accomplished by establishing a feedback loop between the CC and CAS parts of a calculation through similarity transformation of the Hamiltonian with those CC amplitudes that are not encompassed by the active space. We denote this approach the complete active space iterative coupled cluster (CASiCC) ansatz. We investigate its efficiency and accuracy in the singles and doubles approximation by studying the prototypical molecules H4, H8, H2O, and N2. Our results demonstrate that CASiCC systematically improves on the single-reference CCSD and the ecCCSD methods across entire potential energy curves, while retaining modest computational costs. However, the tailored coupled cluster method shows superior performance in the strong correlation regime suggesting that its accuracy is based on error compensation. We find that the iterative version of externally corrected and tailored coupled cluster methods converge to the same results.","sentences":["In this work, we investigate the possibility of improving multireference-driven coupled cluster (CC) approaches with an algorithm that iteratively combines complete active space (CAS) calculations with tailored CC and externally corrected CC.","This is accomplished by establishing a feedback loop between the CC and CAS parts of a calculation through similarity transformation of the Hamiltonian with those CC amplitudes that are not encompassed by the active space.","We denote this approach the complete active space iterative coupled cluster (CASiCC) ansatz.","We investigate its efficiency and accuracy in the singles and doubles approximation by studying the prototypical molecules H4, H8, H2O, and N2.","Our results demonstrate that CASiCC systematically improves on the single-reference CCSD and the ecCCSD methods across entire potential energy curves, while retaining modest computational costs.","However, the tailored coupled cluster method shows superior performance in the strong correlation regime suggesting that its accuracy is based on error compensation.","We find that the iterative version of externally corrected and tailored coupled cluster methods converge to the same results."],"url":"http://arxiv.org/abs/2404.06070v1","category":"physics.chem-ph"}
{"created":"2024-04-09 05:29:09","title":"Cymatics Cup: Shape-Changing Drinks by Leveraging Cymatics","abstract":"To enhance the dining experience, prior studies in Human-Computer Interaction (HCI) and gastrophysics have demonstrated that modifying the static shape of solid foods can amplify taste perception. However, the exploration of dynamic shape-changing mechanisms in liquid foods remains largely untapped. In the present study, we employ cymatics, a scientific discipline focused on utilizing sound frequencies to generate patterns in liquids and particles to augment the drinking experience. Utilizing speakers, we dynamically reshaped liquids exhibiting five distinct taste profiles and evaluated resultant changes in taste perception and drinking experience. Our research objectives extend beyond merely augmenting taste from visual to tactile sensations; we also prioritize the experiential aspects of drinking. Through a series of experiments and workshops, we revealed a significant impact on taste perception and overall drinking experience when mediated by cymatics effects. Building upon these findings, we designed and developed tableware to integrate cymatics principles into gastronomic experiences.","sentences":["To enhance the dining experience, prior studies in Human-Computer Interaction (HCI) and gastrophysics have demonstrated that modifying the static shape of solid foods can amplify taste perception.","However, the exploration of dynamic shape-changing mechanisms in liquid foods remains largely untapped.","In the present study, we employ cymatics, a scientific discipline focused on utilizing sound frequencies to generate patterns in liquids and particles to augment the drinking experience.","Utilizing speakers, we dynamically reshaped liquids exhibiting five distinct taste profiles and evaluated resultant changes in taste perception and drinking experience.","Our research objectives extend beyond merely augmenting taste from visual to tactile sensations; we also prioritize the experiential aspects of drinking.","Through a series of experiments and workshops, we revealed a significant impact on taste perception and overall drinking experience when mediated by cymatics effects.","Building upon these findings, we designed and developed tableware to integrate cymatics principles into gastronomic experiences."],"url":"http://arxiv.org/abs/2404.06027v1","category":"cs.HC"}
{"created":"2024-04-09 04:55:24","title":"Identifying Shopping Intent in Product QA for Proactive Recommendations","abstract":"Voice assistants have become ubiquitous in smart devices allowing users to instantly access information via voice questions. While extensive research has been conducted in question answering for voice search, little attention has been paid on how to enable proactive recommendations from a voice assistant to its users. This is a highly challenging problem that often leads to user friction, mainly due to recommendations provided to the users at the wrong time. We focus on the domain of e-commerce, namely in identifying Shopping Product Questions (SPQs), where the user asking a product-related question may have an underlying shopping need. Identifying a user's shopping need allows voice assistants to enhance shopping experience by determining when to provide recommendations, such as product or deal recommendations, or proactive shopping actions recommendation. Identifying SPQs is a challenging problem and cannot be done from question text alone, and thus requires to infer latent user behavior patterns inferred from user's past shopping history. We propose features that capture the user's latent shopping behavior from their purchase history, and combine them using a novel Mixture-of-Experts (MoE) model. Our evaluation shows that the proposed approach is able to identify SPQs with a high score of F1=0.91. Furthermore, based on an online evaluation with real voice assistant users, we identify SPQs in real-time and recommend shopping actions to users to add the queried product into their shopping list. We demonstrate that we are able to accurately identify SPQs, as indicated by the significantly higher rate of added products to users' shopping lists when being prompted after SPQs vs random PQs.","sentences":["Voice assistants have become ubiquitous in smart devices allowing users to instantly access information via voice questions.","While extensive research has been conducted in question answering for voice search, little attention has been paid on how to enable proactive recommendations from a voice assistant to its users.","This is a highly challenging problem that often leads to user friction, mainly due to recommendations provided to the users at the wrong time.","We focus on the domain of e-commerce, namely in identifying Shopping Product Questions (SPQs), where the user asking a product-related question may have an underlying shopping need.","Identifying a user's shopping need allows voice assistants to enhance shopping experience by determining when to provide recommendations, such as product or deal recommendations, or proactive shopping actions recommendation.","Identifying SPQs is a challenging problem and cannot be done from question text alone, and thus requires to infer latent user behavior patterns inferred from user's past shopping history.","We propose features that capture the user's latent shopping behavior from their purchase history, and combine them using a novel Mixture-of-Experts (MoE) model.","Our evaluation shows that the proposed approach is able to identify SPQs with a high score of F1=0.91.","Furthermore, based on an online evaluation with real voice assistant users, we identify SPQs in real-time and recommend shopping actions to users to add the queried product into their shopping list.","We demonstrate that we are able to accurately identify SPQs, as indicated by the significantly higher rate of added products to users' shopping lists when being prompted after SPQs vs random PQs."],"url":"http://arxiv.org/abs/2404.06017v1","category":"cs.CL"}
{"created":"2024-04-09 04:51:48","title":"Twisted Kronecker series and periods of modular forms on $\u0393_0(N)$","abstract":"We introduce an infinite family of Kronecker series twisted by characters. As an application, we give a closed formula for the sum of all Hecke eigenforms on ${\\Gamma}_0(N) $ multiplied by their twisted period polynomials in terms of the product of those twisted Kronecker series, when N is square free. This extends an identity of Zagier among period polynomials, Hecke eigenforms and a quotient of Jacobi theta series.","sentences":["We introduce an infinite family of Kronecker series twisted by characters.","As an application, we give a closed formula for the sum of all Hecke eigenforms on ${\\Gamma}_0(N) $ multiplied by their twisted period polynomials in terms of the product of those twisted Kronecker series, when N is square free.","This extends an identity of Zagier among period polynomials, Hecke eigenforms and a quotient of Jacobi theta series."],"url":"http://arxiv.org/abs/2404.06016v1","category":"math.NT"}
{"created":"2024-04-09 04:28:06","title":"Inflation, Proton Decay and Gravitational Waves from Metastable Strings in $SU(4)_C \\times SU(2)_L \\times U(1)_R$ Model","abstract":"We present a realistic supersymmetric $\\mu$-hybrid inflation model within the framework of $SU(4)_C \\times SU(2)_L \\times U(1)_R$ gauge symmetry, wherein the symmetry breaking $SU(4)_C \\times SU(2)_L \\times U(1)_R\\rightarrow SU(3)_C\\times SU(2)_L \\times U(1)_{B-L}\\times U(1)_R$ occurs before observable inflation, effectively eliminating topologically stable primordial monopoles. Subsequent breaking of $U(1)_{B-L} \\times U(1)_R \\rightarrow U(1)_Y$ after inflation leads to the formation of superheavy metastable cosmic strings (CSs), capable of producing a stochastic gravitational wave background (SGWB) consistent with the recent PTA data. Moreover, the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$ align with Planck 2018 observations. A consistent scenario for reheating and non-thermal leptogenesis is employed to explain the observed matter content of the universe. Finally, the embedding of $G_{421}$ into the Pati-Salam gauge symmetry $G_{422}$ is briefly discussed, predicting potentially observable proton decay rates detectable at facilities such as Hyper Kamiokande and DUNE.","sentences":["We present a realistic supersymmetric $\\mu$-hybrid inflation model within the framework of $SU(4)_C \\times SU(2)_L \\times U(1)_R$ gauge symmetry, wherein the symmetry breaking $SU(4)_C \\times SU(2)_L \\times U(1)_R\\rightarrow SU(3)_C\\times SU(2)_L \\times U(1)_{B-L}\\times U(1)_R$ occurs before observable inflation, effectively eliminating topologically stable primordial monopoles.","Subsequent breaking of $U(1)_{B-L} \\times U(1)_R \\rightarrow U(1)_Y$ after inflation leads to the formation of superheavy metastable cosmic strings (CSs), capable of producing a stochastic gravitational wave background (SGWB) consistent with the recent PTA data.","Moreover, the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$ align with Planck 2018 observations.","A consistent scenario for reheating and non-thermal leptogenesis is employed to explain the observed matter content of the universe.","Finally, the embedding of $G_{421}$ into the Pati-Salam gauge symmetry $G_{422}$ is briefly discussed, predicting potentially observable proton decay rates detectable at facilities such as Hyper Kamiokande and DUNE."],"url":"http://arxiv.org/abs/2404.06008v1","category":"hep-ph"}
{"created":"2024-04-09 04:23:18","title":"Large deviation principle for the Airy point process","abstract":"The Airy point process is a determinantal point process that arises from the spectral edge of the Gaussian Unitary Ensemble. In this paper, we establish a large deviation principle for the Airy point process. Our result also extends to point processes arising from the spectrum of the stochastic Airy operator.","sentences":["The Airy point process is a determinantal point process that arises from the spectral edge of the Gaussian Unitary Ensemble.","In this paper, we establish a large deviation principle for the Airy point process.","Our result also extends to point processes arising from the spectrum of the stochastic Airy operator."],"url":"http://arxiv.org/abs/2404.06006v1","category":"math.PR"}
{"created":"2024-04-09 04:20:27","title":"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval","abstract":"In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage. Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage. Our method achieves $\\sim$10 MB memory usage in query search even with billion-scale datasets with minor performance degradation. AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG). This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future.","sentences":["In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage.","Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets.","In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage.","Our method achieves $\\sim$10 MB memory usage in query search even with billion-scale datasets with minor performance degradation.","AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG).","This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future."],"url":"http://arxiv.org/abs/2404.06004v1","category":"cs.IR"}
{"created":"2024-04-09 04:08:16","title":"Explaining the cosmological dark matter coincidence in asymmetric dark QCD","abstract":"To properly solve the coincidence problem ($\\Omega_\\mathrm{DM} \\simeq 5\\Omega_\\mathrm{VM}$) in a model of asymmetric dark matter, one cannot simply relate the number densities of visible and dark matter without also relating their particle masses. Following previous work, we consider a framework where the dark matter is a confined state of a dark QCD gauge group whose confinement scale is dynamically related to the QCD confinement scale by a mechanism utilising infrared fixed points of the two gauge couplings. In this work we present a new, `zero-coupling infrared fixed point' approach, which allows a larger proportion of models in this framework to generically relate the masses of the visible and dark matter particles. Due to the heavy mass scale required for the new field content, we introduce supersymmetry to the theory. We consider how these models may be incorporated in a full theory of asymmetric dark matter, presenting some example leptogenesis-like models. We also discuss the phenomenology of these models; in particular, there are gravitational wave signals which, while weak, may be measurable at future mHz and $\\mu$Hz detectors.","sentences":["To properly solve the coincidence problem ($\\Omega_\\mathrm{DM} \\simeq 5\\Omega_\\mathrm{VM}$) in a model of asymmetric dark matter, one cannot simply relate the number densities of visible and dark matter without also relating their particle masses.","Following previous work, we consider a framework where the dark matter is a confined state of a dark QCD gauge group whose confinement scale is dynamically related to the QCD confinement scale by a mechanism utilising infrared fixed points of the two gauge couplings.","In this work we present a new, `zero-coupling infrared fixed point' approach, which allows a larger proportion of models in this framework to generically relate the masses of the visible and dark matter particles.","Due to the heavy mass scale required for the new field content, we introduce supersymmetry to the theory.","We consider how these models may be incorporated in a full theory of asymmetric dark matter, presenting some example leptogenesis-like models.","We also discuss the phenomenology of these models; in particular, there are gravitational wave signals which, while weak, may be measurable at future mHz and $\\mu$Hz detectors."],"url":"http://arxiv.org/abs/2404.05999v1","category":"hep-ph"}
{"created":"2024-04-09 03:22:36","title":"StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion","abstract":"Story visualization aims to generate a series of realistic and coherent images based on a storyline. Current models adopt a frame-by-frame architecture by transforming the pre-trained text-to-image model into an auto-regressive manner. Although these models have shown notable progress, there are still three flaws. 1) The unidirectional generation of auto-regressive manner restricts the usability in many scenarios. 2) The additional introduced story history encoders bring an extremely high computational cost. 3) The story visualization and continuation models are trained and inferred independently, which is not user-friendly. To these ends, we propose a bidirectional, unified, and efficient framework, namely StoryImager. The StoryImager enhances the storyboard generative ability inherited from the pre-trained text-to-image model for a bidirectional generation. Specifically, we introduce a Target Frame Masking Strategy to extend and unify different story image generation tasks. Furthermore, we propose a Frame-Story Cross Attention Module that decomposes the cross attention for local fidelity and global coherence. Moreover, we design a Contextual Feature Extractor to extract contextual information from the whole storyline. The extensive experimental results demonstrate the excellent performance of our StoryImager. The code is available at https://github.com/tobran/StoryImager.","sentences":["Story visualization aims to generate a series of realistic and coherent images based on a storyline.","Current models adopt a frame-by-frame architecture by transforming the pre-trained text-to-image model into an auto-regressive manner.","Although these models have shown notable progress, there are still three flaws.","1)","The unidirectional generation of auto-regressive manner restricts the usability in many scenarios.","2) The additional introduced story history encoders bring an extremely high computational cost.","3) The story visualization and continuation models are trained and inferred independently, which is not user-friendly.","To these ends, we propose a bidirectional, unified, and efficient framework, namely StoryImager.","The StoryImager enhances the storyboard generative ability inherited from the pre-trained text-to-image model for a bidirectional generation.","Specifically, we introduce a Target Frame Masking Strategy to extend and unify different story image generation tasks.","Furthermore, we propose a Frame-Story Cross Attention Module that decomposes the cross attention for local fidelity and global coherence.","Moreover, we design a Contextual Feature Extractor to extract contextual information from the whole storyline.","The extensive experimental results demonstrate the excellent performance of our StoryImager.","The code is available at https://github.com/tobran/StoryImager."],"url":"http://arxiv.org/abs/2404.05979v1","category":"cs.CV"}
{"created":"2024-04-09 03:01:56","title":"Vacancy enhanced cation ordering enables >15% efficiency in Kesterite solar cells","abstract":"Atomic disorder, a widespread problem in compound crystalline materials, is a imperative affecting the performance of multi-chalcogenide Cu2ZnSn(S, Se)4 (CZTSSe) photovoltaic device known for its low cost and environmental friendliness. Cu-Zn disorder is particularly abundantly present in CZTSSe due to its extraordinarily low formation energy, having induced high-concentration deep defects and severe charge loss, while its regulation remains challenging due to the contradiction between disorder-order phase transition thermodynamics and atom-interchange kinetics. Herein, through introducing more vacancies in the CZTSSe surface, we explored a vacancy-assisted strategy to reduce the atom-interchange barrier limit to facilitate the Cu-Zn ordering kinetic process. The improvement in the Cu-Zn order degree has significantly reduced the charge loss in the device and helped us realize 15.4% (certified at 14.9%) and 13.5% efficiency (certified at 13.3%) in 0.27 cm2 and 1.1 cm2-area CZTSSe solar cells, respectively, thus bringing substantial advancement for emerging inorganic thin-film photovoltaics.","sentences":["Atomic disorder, a widespread problem in compound crystalline materials, is a imperative affecting the performance of multi-chalcogenide Cu2ZnSn(S, Se)4 (CZTSSe) photovoltaic device known for its low cost and environmental friendliness.","Cu-Zn disorder is particularly abundantly present in CZTSSe due to its extraordinarily low formation energy, having induced high-concentration deep defects and severe charge loss, while its regulation remains challenging due to the contradiction between disorder-order phase transition thermodynamics and atom-interchange kinetics.","Herein, through introducing more vacancies in the CZTSSe surface, we explored a vacancy-assisted strategy to reduce the atom-interchange barrier limit to facilitate the Cu-Zn ordering kinetic process.","The improvement in the Cu-Zn order degree has significantly reduced the charge loss in the device and helped us realize 15.4% (certified at 14.9%) and 13.5% efficiency (certified at 13.3%) in 0.27 cm2 and 1.1 cm2-area CZTSSe solar cells, respectively, thus bringing substantial advancement for emerging inorganic thin-film photovoltaics."],"url":"http://arxiv.org/abs/2404.05974v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 02:40:29","title":"Entanglement suppression and low-energy scattering of heavy mesons","abstract":"Recently entanglement suppression was proposed to be one possible origin of emergent symmetries. Here we test this conjecture in the context of heavy meson scatterings. The low-energy interactions of $D^{(*)}\\bar D^{(*)}$ and $D^{(*)} D^{(*)}$ are closely related to the hadronic molecular candidates $X(3872)$ and $T_{cc}(3875)^+$, respectively, and can be described by a nonrelativistic effective Lagrangian manifesting heavy-quark spin symmetry, which includes only constant contact potentials at leading order. We explore entanglement suppression in a tensor-product framework to treat both the isospin and spin degrees of freedom. Using the $X(3872)$ and $T_{cc}(3875)^+$ as inputs, we find that entanglement suppression indeed leads to an emergent symmetry, namely, a light-quark spin symmetry, and as such the $D^{(*)}\\bar D^{(*)}$ or $D^{(*)} D^{(*)}$ interaction strengths for a given total isospin do not depend on the total angular momentum of light (anti)quarks. The $X(3872)$ and $T_{cc}(3875)^+$ are predicted to have five and one isoscalar partner, respectively, while the corresponding partner numbers derived solely from heavy-quark spin symmetry are three and one, respectively. The predictions need to be confronted with experimental data and lattice QCD results to further test the entanglement suppression conjecture.","sentences":["Recently entanglement suppression was proposed to be one possible origin of emergent symmetries.","Here we test this conjecture in the context of heavy meson scatterings.","The low-energy interactions of $D^{(*)}\\bar D^{(*)}$ and $D^{(*)} D^{(*)}$ are closely related to the hadronic molecular candidates $X(3872)$ and $T_{cc}(3875)^+$, respectively, and can be described by a nonrelativistic effective Lagrangian manifesting heavy-quark spin symmetry, which includes only constant contact potentials at leading order.","We explore entanglement suppression in a tensor-product framework to treat both the isospin and spin degrees of freedom.","Using the $X(3872)$ and $T_{cc}(3875)^+$ as inputs, we find that entanglement suppression indeed leads to an emergent symmetry, namely, a light-quark spin symmetry, and as such the $D^{(*)}\\bar D^{(*)}$ or $D^{(*)} D^{(*)}$ interaction strengths for a given total isospin do not depend on the total angular momentum of light (anti)quarks.","The $X(3872)$ and $T_{cc}(3875)^+$ are predicted to have five and one isoscalar partner, respectively, while the corresponding partner numbers derived solely from heavy-quark spin symmetry are three and one, respectively.","The predictions need to be confronted with experimental data and lattice QCD results to further test the entanglement suppression conjecture."],"url":"http://arxiv.org/abs/2404.05958v1","category":"hep-ph"}
{"created":"2024-04-09 01:56:32","title":"Thermoelectric transport and current noise through a multilevel Anderson impurity: Three-body Fermi-liquid corrections in quantum dots and magnetic alloys","abstract":"We present a comprehensive Fermi-liquid description for thermoelectric transport and current noise, applicable to multilevel quantum dots (QD) and magnetic alloys (MA) without electron-hole or time-reversal symmetry. Our formulation for the low-energy transport is based on an Anderson model with $N$ discrete impurity levels, and is asymptotically exact at low energies, up to the next-leading order terms in power expansions with respect to temperature $T$ and bias voltage $eV$. The expansion coefficients can be expressed in terms of the Fermi-liquid parameters, which include the three-body correlation functions defined with respect to the equilibrium ground state in addition to the linear susceptibilities and the occupation number $N_d^{}$ of impurity electrons. We apply this formulation to SU($N$) symmetric QD and MA, and calculate the correlation functions for $N=4$ and $6$, using the numerical renormalization group approach. The three-body correlations are shown to be determined by a single parameter over a wide range of electron fillings $1 \\lesssim N_d^{} \\lesssim N-1$ for strong Coulomb interactions $U$, and they also exhibit the plateau structures due to the SU($N$) Kondo effects at integer values of $N_d^{}$. We find that the Lorenz number $L=\\kappa/(T \\sigma)$ for QD and MA, defined as the ratio of the thermal conductivity $\\kappa$ to the electrical conductivity $\\sigma$, deviates from the universal Wiedemann-Franz value $\\pi^2/(3e^2)$ as the temperature increases from $T=0$, showing the $T^2$ dependence, the coefficient for which depends on the three-body correlations away from half filling. We also demonstrate the role of three-body correlations on the nonlinear current noise and the other transport coefficients.","sentences":["We present a comprehensive Fermi-liquid description for thermoelectric transport and current noise, applicable to multilevel quantum dots (QD) and magnetic alloys (MA) without electron-hole or time-reversal symmetry.","Our formulation for the low-energy transport is based on an Anderson model with $N$ discrete impurity levels, and is asymptotically exact at low energies, up to the next-leading order terms in power expansions with respect to temperature $T$ and bias voltage $eV$. The expansion coefficients can be expressed in terms of the Fermi-liquid parameters, which include the three-body correlation functions defined with respect to the equilibrium ground state in addition to the linear susceptibilities and the occupation number $N_d^{}$ of impurity electrons.","We apply this formulation to SU($N$) symmetric QD and MA, and calculate the correlation functions for $N=4$ and $6$, using the numerical renormalization group approach.","The three-body correlations are shown to be determined by a single parameter over a wide range of electron fillings $1 \\lesssim N_d^{} \\lesssim N-1$ for strong Coulomb interactions $U$, and they also exhibit the plateau structures due to the SU($N$) Kondo effects at integer values of $N_d^{}$. We find that the Lorenz number $L=\\kappa/(T \\sigma)$ for QD and MA, defined as the ratio of the thermal conductivity $\\kappa$ to the electrical conductivity $\\sigma$, deviates from the universal Wiedemann-Franz value $\\pi^2/(3e^2)$ as the temperature increases from $T=0$, showing the $T^2$ dependence, the coefficient for which depends on the three-body correlations away from half filling.","We also demonstrate the role of three-body correlations on the nonlinear current noise and the other transport coefficients."],"url":"http://arxiv.org/abs/2404.05947v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-09 01:55:27","title":"Criticality of central charges for Gauss-Bonnet black holes","abstract":"Employing extended phase space formalism, we study critical phenomenon of A-charge and C-charge for holographic theories dual to Gauss-Bonnet black holes. We find a universal critical Gauss-Bonnet coupling, giving rise to a universal ratio between the two central charges at the critical point. This leads to a new intepretation for critical behavior of Gauss-Bonnet black holes in terms of the boundary degrees of freedoms, although the solutions are electrically neutral. Another novel feature is for either of the central charges, the transition temperature is beyond the critical point but is upper bounded by causality of the boundary theories.","sentences":["Employing extended phase space formalism, we study critical phenomenon of A-charge and C-charge for holographic theories dual to Gauss-Bonnet black holes.","We find a universal critical Gauss-Bonnet coupling, giving rise to a universal ratio between the two central charges at the critical point.","This leads to a new intepretation for critical behavior of Gauss-Bonnet black holes in terms of the boundary degrees of freedoms, although the solutions are electrically neutral.","Another novel feature is for either of the central charges, the transition temperature is beyond the critical point but is upper bounded by causality of the boundary theories."],"url":"http://arxiv.org/abs/2404.05945v1","category":"hep-th"}
{"created":"2024-04-09 01:04:26","title":"The influence of vertical resolution on internal tide energetics and subsequent effects on underwater acoustic propagation","abstract":"Internal tide generation and breaking play a primary role in the vertical transport and mixing of heat and other properties in the ocean interior, thereby influencing climate regulation. Additionally, internal tides increase sound speed variability in the ocean, consequently impacting underwater acoustic propagation. With advancements in large-scale ocean modeling capabilities, it is essential to assess the impact of higher model resolutions (horizontal and vertical) in representing internal tides. This study investigates the influence of vertical resolution on internal tide energetics and its subsequent effects on underwater acoustic propagation in the HYbrid Coordinate Ocean Model (HYCOM). An idealized configuration with a ridge, forced only by semidiurnal tides and having 1-km horizontal grid-spacing, is used to test two different vertical-grid discretizations, defined based on the zero-crossings of horizontal velocity eigenfunctions, with seven distinct numbers of isopycnal layers, ranging from 8 to 128. Analyses reveal that increasing the number of layers up to 48 increases barotropic-to-baroclinic tidal conversion, available potential energy, and vertical kinetic energy, reaching equilibrium afterwards with higher layer counts. Vertical shear exhibits a similar pattern but converging at 96 layers. Simulations with at least 48 layers fully resolve the available potential energy contained in the 3rd to 8th tidal baroclinic modes. Finally, sound speed variability and acoustic parameters differ for simulations with less than 48 layers. Therefore, the study concludes that a minimum vertical resolution (48 layers in this case) is required in isopycnal models to minimize the impact on internal tide properties and associated underwater acoustic propagation.","sentences":["Internal tide generation and breaking play a primary role in the vertical transport and mixing of heat and other properties in the ocean interior, thereby influencing climate regulation.","Additionally, internal tides increase sound speed variability in the ocean, consequently impacting underwater acoustic propagation.","With advancements in large-scale ocean modeling capabilities, it is essential to assess the impact of higher model resolutions (horizontal and vertical) in representing internal tides.","This study investigates the influence of vertical resolution on internal tide energetics and its subsequent effects on underwater acoustic propagation in the HYbrid Coordinate Ocean Model (HYCOM).","An idealized configuration with a ridge, forced only by semidiurnal tides and having 1-km horizontal grid-spacing, is used to test two different vertical-grid discretizations, defined based on the zero-crossings of horizontal velocity eigenfunctions, with seven distinct numbers of isopycnal layers, ranging from 8 to 128.","Analyses reveal that increasing the number of layers up to 48 increases barotropic-to-baroclinic tidal conversion, available potential energy, and vertical kinetic energy, reaching equilibrium afterwards with higher layer counts.","Vertical shear exhibits a similar pattern but converging at 96 layers.","Simulations with at least 48 layers fully resolve the available potential energy contained in the 3rd to 8th tidal baroclinic modes.","Finally, sound speed variability and acoustic parameters differ for simulations with less than 48 layers.","Therefore, the study concludes that a minimum vertical resolution (48 layers in this case) is required in isopycnal models to minimize the impact on internal tide properties and associated underwater acoustic propagation."],"url":"http://arxiv.org/abs/2404.05924v1","category":"physics.ao-ph"}
{"created":"2024-04-09 00:59:34","title":"The Application of Tailored Fields for Studying Chirality and Symmetry","abstract":"Ultrashort laser pulses pose unique tools to trigger and probe the fastest charge dynamics in matter, allowing the investigation of fundamental physical phenomena with unprecedented resolution in space, time, and energy. One of the most fascinating opportunities that ultrashort pulses offer is the possibility of modulating and investigating symmetries by tailoring the properties of the laser beam in the spatial and polarization domains, effectively controlling symmetry breaking on multiple levels. In particular, this allows probing chiral matter and ultrafast chiral dynamics. In recent years, the development of highly sensitive approaches for studying chirality has been a hot topic in physics and chemistry that has developed largely separately from the field of tailored light. This perspective discusses the individual and joint evolution of these fields with an emphasis on how the fields have already cross-fertilized, opening new opportunities in science. We outline a future outlook of how the topics are expected to fully merge and mutually evolve, emphasizing outstanding open issues.","sentences":["Ultrashort laser pulses pose unique tools to trigger and probe the fastest charge dynamics in matter, allowing the investigation of fundamental physical phenomena with unprecedented resolution in space, time, and energy.","One of the most fascinating opportunities that ultrashort pulses offer is the possibility of modulating and investigating symmetries by tailoring the properties of the laser beam in the spatial and polarization domains, effectively controlling symmetry breaking on multiple levels.","In particular, this allows probing chiral matter and ultrafast chiral dynamics.","In recent years, the development of highly sensitive approaches for studying chirality has been a hot topic in physics and chemistry that has developed largely separately from the field of tailored light.","This perspective discusses the individual and joint evolution of these fields with an emphasis on how the fields have already cross-fertilized, opening new opportunities in science.","We outline a future outlook of how the topics are expected to fully merge and mutually evolve, emphasizing outstanding open issues."],"url":"http://arxiv.org/abs/2404.05923v1","category":"physics.optics"}
{"created":"2024-04-09 00:37:23","title":"Thermal Casimir effect for a Dirac field on flat space with a nontrivial circular boundary condition","abstract":"This work investigates the thermal Casimir effect associated with a massive spinor field defined on a four-dimensional flat space with a circularly compactified spatial dimension whose periodicity is oriented along a vector in $xy$-plane. We employ the generalized zeta function method to establish a finite definition for the vacuum free energy density. This definition conveniently separates into the zero-temperature Casimir energy density and additional terms accounting for temperature corrections. The structure of existing divergences is analyzed from the asymptotic behavior of the spinor heat kernel function and removed in the renormalization by subtracting scheme. The only non-null heat coefficient is the one associated with the Euclidean divergence. We also address the need for a finite renormalization to treat the ambiguity in the zeta function regularization prescription \\text{associated} with this Euclidean heat kernel coefficient and ensure that the renormalization procedure is unique. The high- and low-temperature asymptotic limits are also explored. In particular, we explicitly show that free energy density lacks a classical limit at high temperatures, and the entropy density agrees with the Nernst heat theorem at low temperatures.","sentences":["This work investigates the thermal Casimir effect associated with a massive spinor field defined on a four-dimensional flat space with a circularly compactified spatial dimension whose periodicity is oriented along a vector in $xy$-plane.","We employ the generalized zeta function method to establish a finite definition for the vacuum free energy density.","This definition conveniently separates into the zero-temperature Casimir energy density and additional terms accounting for temperature corrections.","The structure of existing divergences is analyzed from the asymptotic behavior of the spinor heat kernel function and removed in the renormalization by subtracting scheme.","The only non-null heat coefficient is the one associated with the Euclidean divergence.","We also address the need for a finite renormalization to treat the ambiguity in the zeta function regularization prescription \\text{associated} with this Euclidean heat kernel coefficient and ensure that the renormalization procedure is unique.","The high- and low-temperature asymptotic limits are also explored.","In particular, we explicitly show that free energy density lacks a classical limit at high temperatures, and the entropy density agrees with the Nernst heat theorem at low temperatures."],"url":"http://arxiv.org/abs/2404.05918v1","category":"hep-th"}
