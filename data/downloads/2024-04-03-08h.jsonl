{"created":"2024-04-02 17:59:10","title":"Segment Any 3D Object with Language","abstract":"In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories. Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or geometry information, leading to sub-optimal performance. Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes. In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both backbone and decoder. In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of multimodal associations as supervision. Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica benchmarks, and the results are even close to the fully-supervised counterpart despite the absence of class annotations in the training. Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions.","sentences":["In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions.","Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories.","Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or geometry information, leading to sub-optimal performance.","Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes.","In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds.","Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both backbone and decoder.","In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of multimodal associations as supervision.","Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica benchmarks, and the results are even close to the fully-supervised counterpart despite the absence of class annotations in the training.","Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions."],"url":"http://arxiv.org/abs/2404.02157v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:49","title":"Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration","abstract":"All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples. We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet.","sentences":["All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation.","The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives.","We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks.","Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training.","This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights.","Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours.","To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples.","We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models.","The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet."],"url":"http://arxiv.org/abs/2404.02154v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:36","title":"Mass calibration of DES Year-3 clusters via SPT-3G CMB cluster lensing","abstract":"We measure the stacked lensing signal in the direction of galaxy clusters in the Dark Energy Survey Year 3 (DES Y3) redMaPPer sample, using cosmic microwave background (CMB) temperature data from SPT-3G, the third-generation CMB camera on the South Pole Telescope (SPT). We estimate the lensing signal using temperature maps constructed from the initial 2 years of data from the SPT-3G 'Main' survey, covering 1500 deg$^2$ of the Southern sky. We then use this signal as a proxy for the mean cluster mass of the DES sample. In this work, we employ three versions of the redMaPPer catalogue: a Flux-Limited sample containing 8865 clusters, a Volume-Limited sample with 5391 clusters, and a Volume&Redshift-Limited sample with 4450 clusters. For the three samples, we find the mean cluster masses to be ${M}_{200{\\rm{m}}}=1.66\\pm0.13$ [stat.]$\\pm0.03$ [sys.], $1.97\\pm0.18$ [stat.]$\\pm0.05$ [sys.], and $2.11\\pm0.20$ [stat.]$\\pm0.05$ [sys.]$\\times{10}^{14}\\ {\\rm{M}}_{\\odot }$, respectively. This is a factor of $\\sim2$ improvement relative to the precision of measurements with previous generations of SPT surveys and the most constraining cluster mass measurements using CMB cluster lensing to date. Overall, we find no significant tensions between our results and masses given by redMaPPer mass-richness scaling relations of previous works, which were calibrated using CMB cluster lensing, optical weak lensing, and velocity dispersion measurements from various combinations of DES, SDSS and Planck data. We then divide our sample into 3 redshift and 3 richness bins, finding no significant tensions with optical weak-lensing calibrated masses in these bins. We forecast a $5.8\\%$ constraint on the mean cluster mass of the DES Y3 sample with the complete SPT-3G surveys when using both temperature and polarization data and including an additional $\\sim1400$ deg$^2$ of observations from the 'Extended' SPT-3G survey.","sentences":["We measure the stacked lensing signal in the direction of galaxy clusters in the Dark Energy Survey Year 3 (DES Y3) redMaPPer sample, using cosmic microwave background (CMB) temperature data from SPT-3G, the third-generation CMB camera on the South Pole Telescope (SPT).","We estimate the lensing signal using temperature maps constructed from the initial 2 years of data from the SPT-3G 'Main' survey, covering 1500 deg$^2$ of the Southern sky.","We then use this signal as a proxy for the mean cluster mass of the DES sample.","In this work, we employ three versions of the redMaPPer catalogue: a Flux-Limited sample containing 8865 clusters, a Volume-Limited sample with 5391 clusters, and a Volume&Redshift-Limited sample with 4450 clusters.","For the three samples, we find the mean cluster masses to be ${M}_{200{\\rm{m}}}=1.66\\pm0.13$","[stat.]$\\pm0.03$ [sys.], $1.97\\pm0.18$ [stat.]$\\pm0.05$ [sys.], and $2.11\\pm0.20$ [stat.]$\\pm0.05$ [sys.]$\\times{10}^{14}\\ {\\rm{M}}_{\\odot }$, respectively.","This is a factor of $\\sim2$ improvement relative to the precision of measurements with previous generations of SPT surveys and the most constraining cluster mass measurements using CMB cluster lensing to date.","Overall, we find no significant tensions between our results and masses given by redMaPPer mass-richness scaling relations of previous works, which were calibrated using CMB cluster lensing, optical weak lensing, and velocity dispersion measurements from various combinations of DES, SDSS and Planck data.","We then divide our sample into 3 redshift and 3 richness bins, finding no significant tensions with optical weak-lensing calibrated masses in these bins.","We forecast a $5.8\\%$ constraint on the mean cluster mass of the DES Y3 sample with the complete SPT-3G surveys when using both temperature and polarization data and including an additional $\\sim1400$ deg$^2$ of observations from the 'Extended' SPT-3G survey."],"url":"http://arxiv.org/abs/2404.02153v1","category":"astro-ph.CO"}
{"created":"2024-04-02 17:58:35","title":"GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image","abstract":"Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/","sentences":["Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars.","However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations.","In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars.","To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field.","To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion.","Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints.","Project page: https://zju3dv.github.io/geneavatar/"],"url":"http://arxiv.org/abs/2404.02152v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:27","title":"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks","abstract":"We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token \"Sure\"), potentially with multiple restarts. In this way, we achieve nearly 100\\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.","sentences":["We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks.","First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token \"Sure\"), potentially with multiple restarts.","In this way, we achieve nearly 100\\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack.","We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\\% success rate.","In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition.","The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection).","We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks."],"url":"http://arxiv.org/abs/2404.02151v1","category":"cs.CR"}
{"created":"2024-04-02 17:58:22","title":"Nambu-Goto equation from three-dimensional gravity","abstract":"We demonstrate that the solutions of three-dimensional gravity obtained by gluing two copies of a spacetime across a junction constituted of a tensile string are in one-to-one correspondence with the solutions of the Nambu-Goto equation in the same spacetime up to a finite number of rigid deformations. The non-linear Nambu-Goto equation satisfied by the average of the embedding coordinates of the junction emerges directly from the junction conditions along with the rigid deformations and corrections due to the tension. Therefore, the equivalence principle generalizes non-trivially to the string. Our results are valid both in three-dimensional flat and AdS spacetimes. In the context of AdS$_3$/CFT$_2$ correspondence, our setup could be used to describe a class of interfaces in the conformal field theory featuring relative time reparametrization at the interface which encodes the solution of the Nambu-Goto equation corresponding to the bulk junction.","sentences":["We demonstrate that the solutions of three-dimensional gravity obtained by gluing two copies of a spacetime across a junction constituted of a tensile string are in one-to-one correspondence with the solutions of the Nambu-Goto equation in the same spacetime up to a finite number of rigid deformations.","The non-linear Nambu-Goto equation satisfied by the average of the embedding coordinates of the junction emerges directly from the junction conditions along with the rigid deformations and corrections due to the tension.","Therefore, the equivalence principle generalizes non-trivially to the string.","Our results are valid both in three-dimensional flat and AdS spacetimes.","In the context of AdS$_3$/CFT$_2$ correspondence, our setup could be used to describe a class of interfaces in the conformal field theory featuring relative time reparametrization at the interface which encodes the solution of the Nambu-Goto equation corresponding to the bulk junction."],"url":"http://arxiv.org/abs/2404.02149v1","category":"hep-th"}
{"created":"2024-04-02 17:58:03","title":"Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models","abstract":"Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts.","sentences":["Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images.","However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly.","Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively.","In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation.","Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated.","Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes.","Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models.","Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts."],"url":"http://arxiv.org/abs/2404.02148v1","category":"cs.CV"}
{"created":"2024-04-02 17:57:57","title":"Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools","abstract":"Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver's seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones. In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools. Visualization has already been shown to be a fundamental component in explainable AI models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users. We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis. We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools.","sentences":["Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver's seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones.","In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools.","Visualization has already been shown to be a fundamental component in explainable AI models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users.","We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis.","We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools."],"url":"http://arxiv.org/abs/2404.02147v1","category":"cs.HC"}
{"created":"2024-04-02 17:57:47","title":"Radiation in Holography","abstract":"We show how to encode the radiative degrees of freedom in $4$-dimensional asymptotically AdS spacetimes, using the boundary Cotton and stress tensors. Background radiation leads to a reduction of the asymptotic symmetry group, in contrast to asymptotically flat spacetimes, where a non-vanishing news tensor does not restrict the asymptotic symmetries. Null gauges, such as $\\Lambda$-BMS, provide a framework for AdS spacetimes that include radiation in the flat limit. We use this to check that the flat limit of the radiative data matches the expected definition in intrinsically asymptotically flat spacetimes. We further dimensionally reduce our construction to the celestial sphere, and show how the $2$-dimensional celestial currents can be extracted from the $3$-dimensional boundary data.","sentences":["We show how to encode the radiative degrees of freedom in $4$-dimensional asymptotically AdS spacetimes, using the boundary Cotton and stress tensors.","Background radiation leads to a reduction of the asymptotic symmetry group, in contrast to asymptotically flat spacetimes, where a non-vanishing news tensor does not restrict the asymptotic symmetries.","Null gauges, such as $\\Lambda$-BMS, provide a framework for AdS spacetimes that include radiation in the flat limit.","We use this to check that the flat limit of the radiative data matches the expected definition in intrinsically asymptotically flat spacetimes.","We further dimensionally reduce our construction to the celestial sphere, and show how the $2$-dimensional celestial currents can be extracted from the $3$-dimensional boundary data."],"url":"http://arxiv.org/abs/2404.02146v1","category":"hep-th"}
{"created":"2024-04-02 17:57:31","title":"Iterated Learning Improves Compositionality in Large Vision-Language Models","abstract":"A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our state-of-the-art vision-language models struggle at compositionality. They are unable to distinguish between images of \" a girl in white facing a man in black\" and \"a girl in black facing a man in white\". Moreover, prior work suggests that compositionality doesn't arise with scale: larger model sizes or training data don't help. This paper develops a new iterated training algorithm that incentivizes compositionality. We draw on decades of cognitive science research that identifies cultural transmission-the need to teach a new generation-as a necessary inductive prior that incentivizes humans to develop compositional languages. Specifically, we reframe vision-language contrastive learning as the Lewis Signaling Game between a vision agent and a language agent, and operationalize cultural transmission by iteratively resetting one of the agent's weights during training. After every iteration, this training paradigm induces representations that become \"easier to learn\", a property of compositional languages: e.g. our model trained on CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the SugarCrepe benchmark.","sentences":["A fundamental characteristic common to both human vision and natural language is their compositional nature.","Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our state-of-the-art vision-language models struggle at compositionality.","They are unable to distinguish between images of \" a girl in white facing a man in black\" and \"a girl in black facing a man in white\".","Moreover, prior work suggests that compositionality doesn't arise with scale: larger model sizes or training data don't help.","This paper develops a new iterated training algorithm that incentivizes compositionality.","We draw on decades of cognitive science research that identifies cultural transmission-the need to teach a new generation-as a necessary inductive prior that incentivizes humans to develop compositional languages.","Specifically, we reframe vision-language contrastive learning as the Lewis Signaling Game between a vision agent and a language agent, and operationalize cultural transmission by iteratively resetting one of the agent's weights during training.","After every iteration, this training paradigm induces representations that become \"easier to learn\", a property of compositional languages: e.g. our model trained on CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the SugarCrepe benchmark."],"url":"http://arxiv.org/abs/2404.02145v1","category":"cs.CV"}
{"created":"2024-04-02 17:57:04","title":"Multiparametric quantification and visualization of liver fat using ultrasound","abstract":"Objectives- Several ultrasound measures have shown promise for assessment of steatosis compared to traditional B-scan, however clinicians may be required to integrate information across the parameters. Here, we propose an integrated multiparametric approach, enabling simple clinical assessment of key information from combined ultrasound parameters. Methods- We have measured 13 parameters related to ultrasound and shear wave elastography. These were measured in 30 human subjects under a study of liver fat. The 13 individual measures are assessed for their predictive value using independent magnetic resonance imaging-derived proton density fat fraction (MRI-PDFF) measurements as a reference standard. In addition, a comprehensive and fine-grain analysis is made of all possible combinations of sub-sets of these parameters to determine if any subset can be efficiently combined to predict fat fraction. Results- We found that as few as four key parameters related to ultrasound propagation are sufficient to generate a linear multiparametric parameter with a correlation against MRI-PDFF values of greater than 0.93. This optimal combination was found to have a classification area under the curve (AUC) approaching 1.0 when applying a threshold for separating steatosis grade zero from higher classes. Furthermore, a strategy is developed for applying local estimates of fat content as a color overlay to produce a visual impression of the extent and distribution of fat within the liver. Conclusion- In principle, this approach can be applied to most clinical ultrasound systems to provide the clinician and patient with a rapid and inexpensive estimate of liver fat content.","sentences":["Objectives- Several ultrasound measures have shown promise for assessment of steatosis compared to traditional B-scan, however clinicians may be required to integrate information across the parameters.","Here, we propose an integrated multiparametric approach, enabling simple clinical assessment of key information from combined ultrasound parameters.","Methods-","We have measured 13 parameters related to ultrasound and shear wave elastography.","These were measured in 30 human subjects under a study of liver fat.","The 13 individual measures are assessed for their predictive value using independent magnetic resonance imaging-derived proton density fat fraction (MRI-PDFF) measurements as a reference standard.","In addition, a comprehensive and fine-grain analysis is made of all possible combinations of sub-sets of these parameters to determine if any subset can be efficiently combined to predict fat fraction.","Results- We found that as few as four key parameters related to ultrasound propagation are sufficient to generate a linear multiparametric parameter with a correlation against MRI-PDFF values of greater than 0.93.","This optimal combination was found to have a classification area under the curve (AUC) approaching 1.0 when applying a threshold for separating steatosis grade zero from higher classes.","Furthermore, a strategy is developed for applying local estimates of fat content as a color overlay to produce a visual impression of the extent and distribution of fat within the liver.","Conclusion-","In principle, this approach can be applied to most clinical ultrasound systems to provide the clinician and patient with a rapid and inexpensive estimate of liver fat content."],"url":"http://arxiv.org/abs/2404.02143v1","category":"physics.med-ph"}
{"created":"2024-04-02 17:55:51","title":"Priority-Neutral Matching Lattices Are Not Distributive","abstract":"Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure. However, in school-choice problems, stable matchings are not Pareto optimal for the students. Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching. Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice.   We study the structure of the priority-neutral lattice. Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice. In particular, we show that the priority-neutral lattice need not be distributive. Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question. This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations. However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle.","sentences":["Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure.","However, in school-choice problems, stable matchings are not Pareto optimal for the students.","Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching.","Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice.   ","We study the structure of the priority-neutral lattice.","Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice.","In particular, we show that the priority-neutral lattice need not be distributive.","Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question.","This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations.","However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle."],"url":"http://arxiv.org/abs/2404.02142v1","category":"econ.TH"}
{"created":"2024-04-02 17:49:40","title":"Topic-based Watermarks for LLM-Generated Text","abstract":"Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \"topic-based watermarking algorithm\" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.","sentences":["Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text.","Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output.","However, current watermarking schemes lack robustness against known attacks against watermarking algorithms.","In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work.","In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \"topic-based watermarking algorithm\" for LLMs.","The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM.","Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM.","Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm.","Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss."],"url":"http://arxiv.org/abs/2404.02138v1","category":"cs.CR"}
{"created":"2024-04-02 17:48:53","title":"On a Conjecture Concerning the Roots of Ehrhart Polynomials of Symmetric Edge Polytopes from Complete Multipartite Graphs","abstract":"In [7], Higashitani, Kummer, and Micha{\\l}ek pose a conjecture about the symmetric edge polytopes of complete multipartite graphs and confirm it for a number of families in the bipartite case. We confirm that conjecture for a number of new classes following the authors' methods and we present a more general result which suggests that the methods in their current form might not be enough to prove the conjecture in full generality.","sentences":["In [7], Higashitani, Kummer, and Micha{\\l}ek pose a conjecture about the symmetric edge polytopes of complete multipartite graphs and confirm it for a number of families in the bipartite case.","We confirm that conjecture for a number of new classes following the authors' methods and we present a more general result which suggests that the methods in their current form might not be enough to prove the conjecture in full generality."],"url":"http://arxiv.org/abs/2404.02136v1","category":"math.CO"}
{"created":"2024-04-02 17:39:31","title":"Detecting Gravitational Wave Memory in the Next Galactic Core-Collapse Supernova","abstract":"We present an approach to detecting (linear) gravitational wave memory in a Galactic core-collapse supernova using current interferometers. Gravitational wave memory is an important prediction of general relativity that has yet to be confirmed. Our approach uses a combination of Linear Prediction Filtering and Matched-Filtering. We present the results of our approach on data from core-collapse supernova simulations that span a range of progenitor mass and metallicity. We are able to detect gravitational wave memory out to 10 kpc. We also present the False Alarm Probabilities assuming an On-Source Window compatible with the presence of a neutrino detection.","sentences":["We present an approach to detecting (linear) gravitational wave memory in a Galactic core-collapse supernova using current interferometers.","Gravitational wave memory is an important prediction of general relativity that has yet to be confirmed.","Our approach uses a combination of Linear Prediction Filtering and Matched-Filtering.","We present the results of our approach on data from core-collapse supernova simulations that span a range of progenitor mass and metallicity.","We are able to detect gravitational wave memory out to 10 kpc.","We also present the False Alarm Probabilities assuming an On-Source Window compatible with the presence of a neutrino detection."],"url":"http://arxiv.org/abs/2404.02131v1","category":"astro-ph.HE"}
{"created":"2024-04-02 17:36:10","title":"Circularly Polarized Luminescence Without External Magnetic Fields from Individual CsPbBr3 Perovskite Quantum Dots","abstract":"Lead halide perovskite quantum dots (QDs), the latest generation of colloidal QD family, exhibit outstanding optical properties which are now exploited as both classical and quantum light sources. Most of their rather exceptional properties are related to the peculiar exciton fine-structure of band-edge states which can support unique bright triplet excitons. The degeneracy of the bright triplet excitons is lifted with energetic splitting in the order of millielectronvolts, which can be resolved by the photoluminescence (PL) measurements of single QDs at cryogenic temperatures. Each bright exciton fine-structure-state (FSS) exhibits a dominantly linear polarization, in line with several theoretical models based on the sole crystal field, exchange interaction and shape anisotropy. Here, we show that in addition to a high degree of linear polarization, the individual exciton FSS can exhibit a non-negligible degree of circular polarization even without external magnetic fields by investigating the four Stokes parameters of the exciton fine-structure in individual CsPbBr3 QDs through Stokes polarimetric measurements. We observe a degree of circular polarization up to ~38%, which could not be detected by using the conventional polarimetric technique. In addition, we found a consistent transition from left- to right-hand circular polarization within the fine-structure triplet manifold, which was observed in magnetic field dependent experiments. Our optical investigation provides deeper insights into the nature of the exciton fine-structures and thereby drives the yet-incomplete understanding of the unique photophysical properties of this novel class of QDs, potentially opening new scenarios in chiral quantum optics.","sentences":["Lead halide perovskite quantum dots (QDs), the latest generation of colloidal QD family, exhibit outstanding optical properties which are now exploited as both classical and quantum light sources.","Most of their rather exceptional properties are related to the peculiar exciton fine-structure of band-edge states which can support unique bright triplet excitons.","The degeneracy of the bright triplet excitons is lifted with energetic splitting in the order of millielectronvolts, which can be resolved by the photoluminescence (PL) measurements of single QDs at cryogenic temperatures.","Each bright exciton fine-structure-state (FSS) exhibits a dominantly linear polarization, in line with several theoretical models based on the sole crystal field, exchange interaction and shape anisotropy.","Here, we show that in addition to a high degree of linear polarization, the individual exciton FSS can exhibit a non-negligible degree of circular polarization even without external magnetic fields by investigating the four Stokes parameters of the exciton fine-structure in individual CsPbBr3 QDs through Stokes polarimetric measurements.","We observe a degree of circular polarization up to ~38%, which could not be detected by using the conventional polarimetric technique.","In addition, we found a consistent transition from left- to right-hand circular polarization within the fine-structure triplet manifold, which was observed in magnetic field dependent experiments.","Our optical investigation provides deeper insights into the nature of the exciton fine-structures and thereby drives the yet-incomplete understanding of the unique photophysical properties of this novel class of QDs, potentially opening new scenarios in chiral quantum optics."],"url":"http://arxiv.org/abs/2404.02130v1","category":"physics.app-ph"}
{"created":"2024-04-02 17:35:58","title":"Solution to the cosmological constant problem","abstract":"I propose an observationally and theoretically consistent resolution of the cosmological constant problem: $\\Lambda$ is a counterterm - with a running coupling - that balances the monopole celestial sky average of the kinetic energy of expansion of inhomogeneously distributed `small' cosmic voids. No other dark energy source is required. This solution relies on the first investigation of void statistics in cosmological simulations in full general relativity (arXiv:2403.15134). Results are consistent with parameters of the Timescape model of cosmological backreaction. Crucially, dynamical spatial curvature arises as time-varying spatial gradients of the kinetic spatial curvature, and depends directly on the void volume fraction. Its monopole average generates the cosmological term. This result potentially resolves the Hubble tension and offers new approaches to tackling other tensions and anomalies.","sentences":["I propose an observationally and theoretically consistent resolution of the cosmological constant problem: $\\Lambda$ is a counterterm - with a running coupling - that balances the monopole celestial sky average of the kinetic energy of expansion of inhomogeneously distributed `small' cosmic voids.","No other dark energy source is required.","This solution relies on the first investigation of void statistics in cosmological simulations in full general relativity (arXiv:2403.15134).","Results are consistent with parameters of the Timescape model of cosmological backreaction.","Crucially, dynamical spatial curvature arises as time-varying spatial gradients of the kinetic spatial curvature, and depends directly on the void volume fraction.","Its monopole average generates the cosmological term.","This result potentially resolves the Hubble tension and offers new approaches to tackling other tensions and anomalies."],"url":"http://arxiv.org/abs/2404.02129v1","category":"gr-qc"}
{"created":"2024-04-02 17:35:28","title":"On factored lifts of graphs and their spectra","abstract":"In this note, we introduce the concept of factored lift, associated with a combined voltage graph, as a generalization of the lift graph. We present a new method for computing the eigenvalues and eigenspaces of factored lifts.","sentences":["In this note, we introduce the concept of factored lift, associated with a combined voltage graph, as a generalization of the lift graph.","We present a new method for computing the eigenvalues and eigenspaces of factored lifts."],"url":"http://arxiv.org/abs/2404.02128v1","category":"math.CO"}
{"created":"2024-04-02 17:33:34","title":"FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning","abstract":"Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.","sentences":["Instruction tuning is an important step in making language models useful for direct user interaction.","However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain.","This critically limits research in this application area.","In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples.","We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the baseline.","However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors.","LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain."],"url":"http://arxiv.org/abs/2404.02127v1","category":"cs.CL"}
{"created":"2024-04-02 17:32:12","title":"3D Congealing: 3D-Aware Image Alignment in the Wild","abstract":"We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects. Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space. We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters. At its core is a canonical 3D representation that encapsulates geometric and semantic information. The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching. The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images. The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model. Our framework can be used for various tasks such as correspondence matching, pose estimation, and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections.","sentences":["We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects.","Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space.","We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters.","At its core is a canonical 3D representation that encapsulates geometric and semantic information.","The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching.","The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images.","The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model.","Our framework can be used for various tasks such as correspondence matching, pose estimation, and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections."],"url":"http://arxiv.org/abs/2404.02125v1","category":"cs.CV"}
{"created":"2024-04-02 17:31:58","title":"Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models","abstract":"Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.","sentences":["Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices.","One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students.","To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability.","In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning.","We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students."],"url":"http://arxiv.org/abs/2404.02124v1","category":"cs.CL"}
{"created":"2024-04-02 17:30:30","title":"Token graphs of Cayley graphs as lifts","abstract":"This paper describes a general method for representing $k$-token graphs of Cayley graphs as lifts of voltage graphs. This allows us to construct line graphs of circulant graphs and Johnson graphs as lift graphs on cyclic groups. As an application of the method, we derive the spectra of the considered token graphs. This method can also be applied to dealing with other matrices, such as the Laplacian or the signless Laplacian, and to construct token digraphs of Cayley digraphs.","sentences":["This paper describes a general method for representing $k$-token graphs of Cayley graphs as lifts of voltage graphs.","This allows us to construct line graphs of circulant graphs and Johnson graphs as lift graphs on cyclic groups.","As an application of the method, we derive the spectra of the considered token graphs.","This method can also be applied to dealing with other matrices, such as the Laplacian or the signless Laplacian, and to construct token digraphs of Cayley digraphs."],"url":"http://arxiv.org/abs/2404.02122v1","category":"math.CO"}
{"created":"2024-04-02 17:28:55","title":"On regularity and rigidity of $2\\times 2$ differential inclusions into non-elliptic curves","abstract":"We study differential inclusions $Du\\in \\Pi$ in an open set $\\Omega\\subset\\mathbb R^2$, where $\\Pi\\subset \\mathbb R^{2\\times 2}$ is a compact connected $C^2$ curve without rank-one connections, but non-elliptic: tangent lines to $\\Pi$ may have rank-one connections, so that classical regularity and rigidity results do not apply. For a wide class of such curves $\\Pi$, we show that $Du$ is locally Lipschitz outside a discrete set, and is rigidly characterized around each singularity. Moreover, in the partially elliptic case where at least one tangent line to $\\Pi$ has no rank-one connections, or under some topological restrictions on the tangent bundle of $\\Pi$, there are no singularities. This goes well beyond previously known particular cases related to Burgers' equation and to the Aviles-Giga functional. The key is the identification and appropriate use of a general underlying structure: an infinite family of conservation laws, called entropy productions in reference to the theory of scalar conservation laws.","sentences":["We study differential inclusions $Du\\in \\Pi$ in an open set $\\Omega\\subset\\mathbb R^2$, where $\\Pi\\subset \\mathbb R^{2\\times 2}$ is a compact connected $C^2$ curve without rank-one connections, but non-elliptic: tangent lines to $\\Pi$ may have rank-one connections, so that classical regularity and rigidity results do not apply.","For a wide class of such curves $\\Pi$, we show that $Du$ is locally Lipschitz outside a discrete set, and is rigidly characterized around each singularity.","Moreover, in the partially elliptic case where at least one tangent line to $\\Pi$ has no rank-one connections, or under some topological restrictions on the tangent bundle of $\\Pi$, there are no singularities.","This goes well beyond previously known particular cases related to Burgers' equation and to the Aviles-Giga functional.","The key is the identification and appropriate use of a general underlying structure: an infinite family of conservation laws, called entropy productions in reference to the theory of scalar conservation laws."],"url":"http://arxiv.org/abs/2404.02121v1","category":"math.AP"}
{"created":"2024-04-02 17:27:51","title":"DEMO: Dose Exploration, Monitoring, and Optimization Using a Biological Mediator for Clinical Outcomes","abstract":"Phase 1-2 designs provide a methodological advance over phase 1 designs for dose finding by using both clinical response and toxicity. A phase 1-2 trial still may fail to select a truly optimal dose. because early response is not a perfect surrogate for long term therapeutic success. To address this problem, a generalized phase 1-2 design first uses a phase 1-2 design's components to identify a set of candidate doses, adaptively randomizes patients among the candidates, and after longer follow up selects a dose to maximize long-term success rate. In this paper, we extend this paradigm by proposing a design that exploits an early treatment-related, real-valued biological outcome, such as pharmacodynamic activity or an immunological effect, that may act as a mediator between dose and clinical outcomes, including tumor response, toxicity, and survival time. We assume multivariate dose-outcome models that include effects appearing in causal pathways from dose to the clinical outcomes. Bayesian model selection is used to identify and eliminate biologically inactive doses. At the end of the trial, a therapeutically optimal dose is chosen from the set of doses that are acceptably safe, clinically effective, and biologically active to maximize restricted mean survival time. Results of a simulation study show that the proposed design may provide substantial improvements over designs that ignore the biological variable.","sentences":["Phase 1-2 designs provide a methodological advance over phase 1 designs for dose finding by using both clinical response and toxicity.","A phase 1-2 trial still may fail to select a truly optimal dose.","because early response is not a perfect surrogate for long term therapeutic success.","To address this problem, a generalized phase 1-2 design first uses a phase 1-2 design's components to identify a set of candidate doses, adaptively randomizes patients among the candidates, and after longer follow up selects a dose to maximize long-term success rate.","In this paper, we extend this paradigm by proposing a design that exploits an early treatment-related, real-valued biological outcome, such as pharmacodynamic activity or an immunological effect, that may act as a mediator between dose and clinical outcomes, including tumor response, toxicity, and survival time.","We assume multivariate dose-outcome models that include effects appearing in causal pathways from dose to the clinical outcomes.","Bayesian model selection is used to identify and eliminate biologically inactive doses.","At the end of the trial, a therapeutically optimal dose is chosen from the set of doses that are acceptably safe, clinically effective, and biologically active to maximize restricted mean survival time.","Results of a simulation study show that the proposed design may provide substantial improvements over designs that ignore the biological variable."],"url":"http://arxiv.org/abs/2404.02120v1","category":"stat.AP"}
{"created":"2024-04-02 17:13:22","title":"Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL","abstract":"In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well. In addition, we find that properties designed to measure the network's ability to learn continually indeed correlate with performance under one-percent tuning.","sentences":["In continual or lifelong reinforcement learning access to the environment should be limited.","If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime.","The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent.","This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies.","In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning.","We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains.","We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well.","In addition, we find that properties designed to measure the network's ability to learn continually indeed correlate with performance under one-percent tuning."],"url":"http://arxiv.org/abs/2404.02113v1","category":"cs.LG"}
{"created":"2024-04-02 17:10:47","title":"Tightening the reins on non-minimal dark sector physics: Interacting Dark Energy with dynamical and non-dynamical equation of state","abstract":"We present a comprehensive reassessment of the state of Interacting Dark Energy (IDE) cosmology, namely models featuring a non-gravitational interaction between Dark Matter (DM) and Dark Energy (DE). To achieve high generality, we extend the dark sector physics by considering two different scenarios: a non-dynamical DE equation of state $w_0\\neq-1$, and a dynamical $w(a)=w_0+w_a(1-a)$. In both cases, we distinguish two different physical regimes resulting from a phantom or quintessence equation of state. To circumvent early-time superhorizon instabilities, the energy-momentum transfer should occur in opposing directions within the two regimes, resulting in distinct phenomenological outcomes. We study quintessence and phantom non-dynamical and dynamical models in light of two independent Cosmic Microwave Background (CMB) experiments - the Planck satellite and the Atacama Cosmology Telescope. We analyze CMB data both independently and in combination with Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog and Baryon Acoustic Oscillations (BAO) from the SDSS-IV eBOSS survey. Our results update and extend the state-of-the-art analyses, significantly narrowing the parameter space allowed for these models and limiting their overall ability to reconcile cosmological tensions. Although considering different combinations of data leaves some freedom to increase $H_0$ towards the value measured by the SH0ES collaboration, our most constraining dataset (CMB+BAO+SN) indicates that fully reconciling the tension solely within the framework of IDE remains challenging.","sentences":["We present a comprehensive reassessment of the state of Interacting Dark Energy (IDE) cosmology, namely models featuring a non-gravitational interaction between Dark Matter (DM) and Dark Energy (DE).","To achieve high generality, we extend the dark sector physics by considering two different scenarios: a non-dynamical DE equation of state $w_0\\neq-1$, and a dynamical $w(a)=w_0+w_a(1-a)$.","In both cases, we distinguish two different physical regimes resulting from a phantom or quintessence equation of state.","To circumvent early-time superhorizon instabilities, the energy-momentum transfer should occur in opposing directions within the two regimes, resulting in distinct phenomenological outcomes.","We study quintessence and phantom non-dynamical and dynamical models in light of two independent Cosmic Microwave Background (CMB) experiments - the Planck satellite and the Atacama Cosmology Telescope.","We analyze CMB data both independently and in combination with Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog and Baryon Acoustic Oscillations (BAO) from the SDSS-IV eBOSS survey.","Our results update and extend the state-of-the-art analyses, significantly narrowing the parameter space allowed for these models and limiting their overall ability to reconcile cosmological tensions.","Although considering different combinations of data leaves some freedom to increase $H_0$ towards the value measured by the SH0ES collaboration, our most constraining dataset (CMB+BAO+SN) indicates that fully reconciling the tension solely within the framework of IDE remains challenging."],"url":"http://arxiv.org/abs/2404.02110v1","category":"astro-ph.CO"}
{"created":"2024-04-02 17:08:23","title":"Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes","abstract":"We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes. The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\\tilde{\\mathcal{O}}(T^{3/4})$.","sentences":["We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes.","The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\\tilde{\\mathcal{O}}(T^{3/4})$."],"url":"http://arxiv.org/abs/2404.02108v1","category":"cs.LG"}
{"created":"2024-04-02 17:00:11","title":"CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems","abstract":"Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq","sentences":["Retrieval Augmented Generation (RAG) has become a popular application for large language models.","It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations.","While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary.","We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline.","ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline.","The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous.","RAG models must adapt to these properties to be successful at ClapNQ.","We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG.","CLAPNQ is publicly available at https://github.com/primeqa/clapnq"],"url":"http://arxiv.org/abs/2404.02103v1","category":"cs.CL"}
{"created":"2024-04-02 16:52:41","title":"CameraCtrl: Enabling Camera Control for Text-to-Video Generation","abstract":"Controllability plays a crucial role in video generation since it allows users to create desired content. However, existing models largely overlooked the precise control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for text-to-video(T2V) models. After precisely parameterizing the camera trajectory, a plug-and-play camera module is then trained on a T2V model, leaving others untouched. Additionally, a comprehensive study on the effect of various datasets is also conducted, suggesting that videos with diverse camera distribution and similar appearances indeed enhance controllability and generalization. Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise and domain-adaptive camera control, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs. Our project website is at: https://hehao13.github.io/projects-CameraCtrl/.","sentences":["Controllability plays a crucial role in video generation since it allows users to create desired content.","However, existing models largely overlooked the precise control of camera pose that serves as a cinematic language to express deeper narrative nuances.","To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for text-to-video(T2V) models.","After precisely parameterizing the camera trajectory, a plug-and-play camera module is then trained on a T2V model, leaving others untouched.","Additionally, a comprehensive study on the effect of various datasets is also conducted, suggesting that videos with diverse camera distribution and similar appearances indeed enhance controllability and generalization.","Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise and domain-adaptive camera control, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs.","Our project website is at: https://hehao13.github.io/projects-CameraCtrl/."],"url":"http://arxiv.org/abs/2404.02101v1","category":"cs.CV"}
{"created":"2024-04-02 16:46:11","title":"Searching for new physics in the solar system with tetrahedral spacecraft formations","abstract":"Tetrahedral configurations of spacecraft on unperturbed heliocentric orbits allow for highly precise observations of small spatial changes in the gravitational field, especially those affecting the gravity gradient tensor (GGT). The resulting high sensitivity may be used to search for new physics that could manifest itself via deviations from general relativistic behavior yielding a non-vanishing trace[GGT]. We study the feasibility of recovering the trace[GGT] with the sensitivity of O(1e-24 s^(-2)) -- the level where some of the recently proposed cosmological models may have observable effects in the solar system. We consider how local measurements provided by precision laser ranging and atom-wave interferometry can be used for that purpose. We report on a preliminary study of such an experiment and precision that may be reached in measuring the trace[GGT], with the assumption of drag-compensated spacecraft by atom interferometer measurements. For that, we study the dynamical behavior of a tetrahedral formation established by four spacecraft on heliocentric nearby elliptical orbits. We formulate the observational equations to measure the trace[GGT] relying only on the observables available within the formation: laser ranging and the Sagnac interferometry. We demonstrate that Sagnac observable is a mission enabling and allows to measure the angular frequency of the tetrahedral rotation with respect to an inertial reference frame with an accuracy much higher than that available from any other modern navigational techniques. We show that the quality of the science measurements is affected by the changes in tetrahedron's orientation and shape as spacecraft follow their orbits. We present the preliminary mission and instrument requirements needed to measure the trace[GGT] to the required accuracy and demonstrate the feasibility of satisfying the science objectives.","sentences":["Tetrahedral configurations of spacecraft on unperturbed heliocentric orbits allow for highly precise observations of small spatial changes in the gravitational field, especially those affecting the gravity gradient tensor (GGT).","The resulting high sensitivity may be used to search for new physics that could manifest itself via deviations from general relativistic behavior yielding a non-vanishing trace[GGT].","We study the feasibility of recovering the trace[GGT] with the sensitivity of O(1e-24 s^(-2)) -- the level where some of the recently proposed cosmological models may have observable effects in the solar system.","We consider how local measurements provided by precision laser ranging and atom-wave interferometry can be used for that purpose.","We report on a preliminary study of such an experiment and precision that may be reached in measuring the trace[GGT], with the assumption of drag-compensated spacecraft by atom interferometer measurements.","For that, we study the dynamical behavior of a tetrahedral formation established by four spacecraft on heliocentric nearby elliptical orbits.","We formulate the observational equations to measure the trace[GGT] relying only on the observables available within the formation: laser ranging and the Sagnac interferometry.","We demonstrate that Sagnac observable is a mission enabling and allows to measure the angular frequency of the tetrahedral rotation with respect to an inertial reference frame with an accuracy much higher than that available from any other modern navigational techniques.","We show that the quality of the science measurements is affected by the changes in tetrahedron's orientation and shape as spacecraft follow their orbits.","We present the preliminary mission and instrument requirements needed to measure the trace[GGT] to the required accuracy and demonstrate the feasibility of satisfying the science objectives."],"url":"http://arxiv.org/abs/2404.02096v1","category":"gr-qc"}
{"created":"2024-04-02 16:45:46","title":"Interplay between the Lyapunov exponents and phase transitions of charged AdS black holes","abstract":"We study the relationship between the standard or extended thermodynamic phase structure of various AdS black holes and the Lyapunov exponents associated with the null and time-like geodesics. We consider dyonic, Bardeen, Gauss-Bonnet, and Lorentz-symmetry breaking massive gravity black holes and calculate the Lyapunov exponents of massless and massive particles in unstable circular geodesics close to the black hole. We find that the thermal profile of the Lyapunov exponents exhibits distinct behaviour in the small and large black hole phases and can encompass certain aspects of the van der Waals type small/large black hole phase transition. We further analyse the properties of Lyapunov exponents as an order parameter and find that its critical exponent is $1/2$, near the critical point for all black holes considered here.","sentences":["We study the relationship between the standard or extended thermodynamic phase structure of various AdS black holes and the Lyapunov exponents associated with the null and time-like geodesics.","We consider dyonic, Bardeen, Gauss-Bonnet, and Lorentz-symmetry breaking massive gravity black holes and calculate the Lyapunov exponents of massless and massive particles in unstable circular geodesics close to the black hole.","We find that the thermal profile of the Lyapunov exponents exhibits distinct behaviour in the small and large black hole phases and can encompass certain aspects of the van der Waals type small/large black hole phase transition.","We further analyse the properties of Lyapunov exponents as an order parameter and find that its critical exponent is $1/2$, near the critical point for all black holes considered here."],"url":"http://arxiv.org/abs/2404.02095v1","category":"hep-th"}
{"created":"2024-04-02 16:40:57","title":"Optimal Bell inequalities for qubit-qudit systems","abstract":"We evaluate the maximal Bell violation for a generic qubit-qudit system, obtaining easily computable expressions in arbitrary qudit dimension. This work generalizes the well-known Horodeckis's result for a qubit-qubit system. We also give simple lower and upper bounds on that violation and study the possibility of improving the amount of Bell-violation by embedding the qudit Hilbert space in one of larger dimension. The results are illustrated with a family of density matrices in the context of a qubit-qutrit system.","sentences":["We evaluate the maximal Bell violation for a generic qubit-qudit system, obtaining easily computable expressions in arbitrary qudit dimension.","This work generalizes the well-known Horodeckis's result for a qubit-qubit system.","We also give simple lower and upper bounds on that violation and study the possibility of improving the amount of Bell-violation by embedding the qudit Hilbert space in one of larger dimension.","The results are illustrated with a family of density matrices in the context of a qubit-qutrit system."],"url":"http://arxiv.org/abs/2404.02092v1","category":"quant-ph"}
{"created":"2024-04-02 16:35:52","title":"Already Moderate Population Sizes Provably Yield Strong Robustness to Noise","abstract":"Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\\lambda)$ and $(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy offspring. We are optimistic that the technical lemmas resulting from this insight will find applications also in future mathematical runtime analyses of evolutionary algorithms.","sentences":["Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   ","In this first mathematical runtime analysis of the $(1+\\lambda)$ and $(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark.","For this, a population size $\\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark.","Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy offspring.","We are optimistic that the technical lemmas resulting from this insight will find applications also in future mathematical runtime analyses of evolutionary algorithms."],"url":"http://arxiv.org/abs/2404.02090v1","category":"cs.NE"}
{"created":"2024-04-02 16:30:12","title":"Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images","abstract":"Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.","sentences":["Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets.","To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning.","Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain.","Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability.","In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks.","Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets."],"url":"http://arxiv.org/abs/2404.02084v1","category":"cs.CV"}
{"created":"2024-04-02 16:28:41","title":"WcDT: World-centric Diffusion Transformer for Traffic Scene Generation","abstract":"In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers. Our proposed framework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference. To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks. Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders. The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories. Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems.","sentences":["In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers.","Our proposed framework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference.","To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks.","Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders.","The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories.","Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems."],"url":"http://arxiv.org/abs/2404.02082v1","category":"cs.CV"}
{"created":"2024-04-02 16:26:33","title":"Generic Properties of Conjugate Points in Optimal Control Problems","abstract":"The first part of the paper studies a class of optimal control problems in Bolza form, where the dynamics is linear w.r.t.~the control function. A necessary condition is derived, for the optimality of a trajectory which starts at a conjugate point. The second part is concerned with a classical problem in the Calculus of Variations, with free terminal point. For a generic terminal cost $\\psi\\in \\C^4(\\mathbb{R}^n)$, applying the previous necessary condition we show that the set of conjugate points is contained in the image of an $(n-2)$-dimensional manifold, and has locally bounded $(n-2)$-dimensional Hausdorff measure.","sentences":["The first part of the paper studies a class of optimal control problems in Bolza form, where the dynamics is linear w.r.t.~the control function.","A necessary condition is derived, for the optimality of a trajectory which starts at a conjugate point.","The second part is concerned with a classical problem in the Calculus of Variations, with free terminal point.","For a generic terminal cost $\\psi\\in \\C^4(\\mathbb{R}^n)$, applying the previous necessary condition we show that the set of conjugate points is contained in the image of an $(n-2)$-dimensional manifold, and has locally bounded $(n-2)$-dimensional Hausdorff measure."],"url":"http://arxiv.org/abs/2404.02080v1","category":"math.OC"}
{"created":"2024-04-02 16:25:35","title":"Coherent Control of an Optical Quantum Dot Using Phonons and Photons","abstract":"Genuine quantum-mechanical effects are readily observable in modern optomechanical systems comprising bosonic (``classical\") optical resonators. Such systems have enabled laser-cooling of mesoscopic objects, generation of squeezed light, and the conversion of photons between microwave and optical modes. Here we demonstrate unique advantages of optical two-level systems, or qubits, for optomechanics. The qubit state can be coherently controlled using an immense variety of driving schemes including both phonons and resonant or detuned photons. We experimentally demonstrate this using charge-controlled InAs quantum dots (QDs) in surface-acoustic-wave resonators. Time-correlated single-photon counting measurements reveal the control of QD population dynamics using engineered optical pulses and mechanical motion. As a first example, we show how this can improve signal-to-background scattering in microwave-to-optical transduction processes. Specifically, we tailor the scheme so that mechanically assisted photon scattering is enhanced over the direct detuned photon scattering from the optical system. These differences are greatly amplified by strategic temporal pulse shaping. Quantum-mechanical calculations show good agreement with our experimental results and provide guidance for adapting these schemes to small phonon occupancies relevant for microwave-to-optical quantum transduction.","sentences":["Genuine quantum-mechanical effects are readily observable in modern optomechanical systems comprising bosonic (``classical\") optical resonators.","Such systems have enabled laser-cooling of mesoscopic objects, generation of squeezed light, and the conversion of photons between microwave and optical modes.","Here we demonstrate unique advantages of optical two-level systems, or qubits, for optomechanics.","The qubit state can be coherently controlled using an immense variety of driving schemes including both phonons and resonant or detuned photons.","We experimentally demonstrate this using charge-controlled InAs quantum dots (QDs) in surface-acoustic-wave resonators.","Time-correlated single-photon counting measurements reveal the control of QD population dynamics using engineered optical pulses and mechanical motion.","As a first example, we show how this can improve signal-to-background scattering in microwave-to-optical transduction processes.","Specifically, we tailor the scheme so that mechanically assisted photon scattering is enhanced over the direct detuned photon scattering from the optical system.","These differences are greatly amplified by strategic temporal pulse shaping.","Quantum-mechanical calculations show good agreement with our experimental results and provide guidance for adapting these schemes to small phonon occupancies relevant for microwave-to-optical quantum transduction."],"url":"http://arxiv.org/abs/2404.02079v1","category":"quant-ph"}
{"created":"2024-04-02 16:25:30","title":"Advancing LLM Reasoning Generalists with Preference Trees","abstract":"We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.","sentences":["We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning.","Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems.","Notably, Eurus-70B beats GPT-3.5","Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%.","The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks.","UltraInteract can be used in both supervised fine-tuning and preference learning.","For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning.","UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks.","Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations.","Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model."],"url":"http://arxiv.org/abs/2404.02078v1","category":"cs.AI"}
{"created":"2024-04-02 16:23:15","title":"Energy-Optimized Planning in Non-Uniform Wind Fields with Fixed-Wing Aerial Vehicles","abstract":"Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances. However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns. Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable. Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency. In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields. We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations. We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time. The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes.","sentences":["Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances.","However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns.","Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable.","Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency.","In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields.","We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations.","We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time.","The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes."],"url":"http://arxiv.org/abs/2404.02077v1","category":"cs.RO"}
{"created":"2024-04-02 16:22:40","title":"Green Measures for a Class of non-Markov Processes","abstract":"In this paper, we investigate the Green measure for a class of non-Gaussian processes in $\\mathbb{R}^{d}$. These measures are associated with the family of generalized grey Brownian motions $B_{\\beta,\\alpha}$, $0<\\beta\\le1$, $0<\\alpha\\le2$. This family includes both fractional Brownian motion, Brownian motion, and other non-Gaussian processes. We show that the perpetual integral exists with probability $1$ for $d\\alpha>2$ and $1<\\alpha\\le2$. The Green measure then generalizes those measures of all these classes.","sentences":["In this paper, we investigate the Green measure for a class of non-Gaussian processes in $\\mathbb{R}^{d}$. These measures are associated with the family of generalized grey Brownian motions $B_{\\beta,\\alpha}$, $0<\\beta\\le1$, $0<\\alpha\\le2$. This family includes both fractional Brownian motion, Brownian motion, and other non-Gaussian processes.","We show that the perpetual integral exists with probability $1$ for $d\\alpha>2$ and $1<\\alpha\\le2$. The Green measure then generalizes those measures of all these classes."],"url":"http://arxiv.org/abs/2404.02076v1","category":"math.PR"}
{"created":"2024-04-02 16:20:02","title":"EGTR: Extracting Graph from Transformer for Scene Graph Generation","abstract":"Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr .","sentences":["Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects.","After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied.","However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected.","We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder.","By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head.","Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects.","By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves.","Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction.","We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets.","Our code is publicly available at https://github.com/naver-ai/egtr ."],"url":"http://arxiv.org/abs/2404.02072v1","category":"cs.CV"}
{"created":"2024-04-02 16:17:41","title":"Multivariate Bernstein-type Polynomials of Finitely Generated D-Modules","abstract":"We generalize the Gr\\\"obner basis method for free D-modules to the case of several term orderings induced by a partition of the set of basic variables. Using this generalized Gr\\\"obner basis technique we prove the existence and give a method of computation of a dimension polynomial in several variables associated with a finitely generated D-module. We show that this dimension polynomial carries more invariants of a D-module than the classical (univariate) Bernstein polynomial. We also show that the introduced multivariate dimension polynomial can be used for characterization of holonomic D-modules.","sentences":["We generalize the Gr\\\"obner basis method for free D-modules to the case of several term orderings induced by a partition of the set of basic variables.","Using this generalized Gr\\\"obner basis technique we prove the existence and give a method of computation of a dimension polynomial in several variables associated with a finitely generated D-module.","We show that this dimension polynomial carries more invariants of a D-module than the classical (univariate) Bernstein polynomial.","We also show that the introduced multivariate dimension polynomial can be used for characterization of holonomic D-modules."],"url":"http://arxiv.org/abs/2404.02069v1","category":"math.AC"}
{"created":"2024-04-02 16:10:29","title":"Using Interpretation Methods for Model Enhancement","abstract":"In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at https://github.com/Chord-Chen-30/UIMER.","sentences":["In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models.","Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales.","However, this intuitive idea has not been fully explored.","In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models.","Our framework is very general in the sense that it can incorporate various interpretation methods.","Previously proposed gradient-based methods can be shown as an instance of our framework.","We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement.","We conduct comprehensive experiments on a variety of tasks.","Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings.","Code is available at https://github.com/Chord-Chen-30/UIMER."],"url":"http://arxiv.org/abs/2404.02068v1","category":"cs.CL"}
{"created":"2024-04-02 16:07:50","title":"Red-Teaming Segment Anything Model","abstract":"Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation.","sentences":["Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications.","The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks.","This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks.","(2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task.","(3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts.","We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels.","All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation."],"url":"http://arxiv.org/abs/2404.02067v1","category":"cs.CV"}
{"created":"2024-04-02 16:07:44","title":"On the stability of $\\ddot x(t)+\u03b1(t)\\dot x(t)+\u03b2(t) x(t)=0$","abstract":"Our main goal is to understand the stability of second order linear homogeneous differential equations $\\ddot x(t)+\\alpha(t)\\dot x(t)+\\beta(t)x(t)=0$ for $C^0$-generic values of the variable parameters $\\alpha(t)$ and $\\beta(t)$. For that we embed the problem into the framework of the general theory of continuous-time linear cocycles induced by the random ODE $\\ddot x(t)+\\alpha(\\varphi^t(\\omega))\\dot x(t)+\\beta(\\varphi^t(\\omega))x(t)=0$, where the coefficients $\\alpha$ and $\\beta$ evolve along the $\\varphi^t$-orbit for $\\omega\\in M$, and $\\varphi^t: M\\to M$ is a flow defined on a compact Hausdorff space $M$ preserving a probability measure $\\mu$. Considering $y=\\dot x$, the above random ODE can be rewritten as $\\dot X=A(\\varphi^t (\\omega))X$, with $X=(x,y)^\\top$, having a kinetic linear cocycle as fundamental solution.   We prove that for a $C^0$-generic choice of parameters $\\alpha$ and $\\beta$ and for $\\mu$-almost all $\\omega\\in M$ either the Lyapunov exponents of the linear cocycle are equal ($\\lambda_1(\\omega)=\\lambda_2(\\omega)$), or else the orbit of $\\omega$ displays a dominated splitting. Applying to dissipative systems ($\\alpha<0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)<0$, attesting the stability of the solution of the random ODE above, or else the orbit of $\\omega$ displays a dominated splitting. Applying to frictionless systems ($\\alpha=0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)=0$, attesting the asymptotic neutrality of the solution of the random ODE above, or else the orbit of $\\omega$ displays a hyperbolic splitting attesting the \\emph{uniform} instability of the solution of the ODE above. This last result implies also an analog result for the 1-d continuous aperiodic Schr\\\"odinger equation. Furthermore, all results hold for $L^\\infty$-generic parameters $\\alpha$ and $\\beta$.","sentences":["Our main goal is to understand the stability of second order linear homogeneous differential equations $\\ddot x(t)+\\alpha(t)\\dot x(t)+\\beta(t)x(t)=0$ for $C^0$-generic values of the variable parameters $\\alpha(t)$ and $\\beta(t)$. For that we embed the problem into the framework of the general theory of continuous-time linear cocycles induced by the random ODE $\\ddot x(t)+\\alpha(\\varphi^t(\\omega))\\dot x(t)+\\beta(\\varphi^t(\\omega))x(t)=0$, where the coefficients $\\alpha$ and $\\beta$ evolve along the $\\varphi^t$-orbit for $\\omega\\in M$, and $\\varphi^t: M\\to M$ is a flow defined on a compact Hausdorff space $M$ preserving a probability measure $\\mu$. Considering $y=\\dot x$, the above random ODE can be rewritten as $\\dot X=A(\\varphi^t (\\omega))X$, with $X=(x,y)^\\top$, having a kinetic linear cocycle as fundamental solution.   ","We prove that for a $C^0$-generic choice of parameters $\\alpha$ and $\\beta$ and for $\\mu$-almost all $\\omega\\in M$ either the Lyapunov exponents of the linear cocycle are equal ($\\lambda_1(\\omega)=\\lambda_2(\\omega)$), or else the orbit of $\\omega$ displays a dominated splitting.","Applying to dissipative systems ($\\alpha<0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)<0$, attesting the stability of the solution of the random ODE above, or else the orbit of $\\omega$ displays a dominated splitting.","Applying to frictionless systems ($\\alpha=0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)=0$, attesting the asymptotic neutrality of the solution of the random ODE above, or else the orbit of $\\omega$ displays a hyperbolic splitting attesting the \\emph{uniform} instability of the solution of the ODE above.","This last result implies also an analog result for the 1-d continuous aperiodic Schr\\\"odinger equation.","Furthermore, all results hold for $L^\\infty$-generic parameters $\\alpha$ and $\\beta$."],"url":"http://arxiv.org/abs/2404.02066v1","category":"math.DS"}
{"created":"2024-04-02 16:04:31","title":"SPMamba: State-space model is all you need in speech separation","abstract":"In speech separation, both CNN- and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performance aspects of Mamba-based models. SPMamba demonstrates superior performance with a significant advantage over existing separation models in a dataset built on Librispeech. Notably, SPMamba achieves a substantial improvement in separation quality, with a 2.42 dB enhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba is publicly accessible at https://github.com/JusperLee/SPMamba .","sentences":["In speech separation, both CNN- and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community.","However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance.","Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity.","Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements.","In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba.","We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information.","Our experimental results reveal an important role in the performance aspects of Mamba-based models.","SPMamba demonstrates superior performance with a significant advantage over existing separation models in a dataset built on Librispeech.","Notably, SPMamba achieves a substantial improvement in separation quality, with a 2.42 dB enhancement in SI-SNRi compared to the TF-GridNet.","The source code for SPMamba is publicly accessible at https://github.com/JusperLee/SPMamba ."],"url":"http://arxiv.org/abs/2404.02063v1","category":"cs.SD"}
{"created":"2024-04-02 16:01:18","title":"Digital Forgetting in Large Language Models: A Survey of Unlearning Methods","abstract":"The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, we introduce the approaches to digital forgetting in LLMs, among which unlearning methodologies stand out as the state of the art. Fourth, we provide a detailed taxonomy of machine unlearning methods for LLMs, and we survey and compare current approaches. Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime. Sixth, we discuss challenges in the area. Finally, we provide some concluding remarks.","sentences":["The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present.","The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation.","Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained).","This survey focuses on forgetting in large language models (LLMs).","We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline.","Second, we describe the motivations, types, and desired properties of digital forgetting.","Third, we introduce the approaches to digital forgetting in LLMs, among which unlearning methodologies stand out as the state of the art.","Fourth, we provide a detailed taxonomy of machine unlearning methods for LLMs, and we survey and compare current approaches.","Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime.","Sixth, we discuss challenges in the area.","Finally, we provide some concluding remarks."],"url":"http://arxiv.org/abs/2404.02062v1","category":"cs.CR"}
{"created":"2024-04-02 15:59:11","title":"Long-context LLMs Struggle with Long In-context Learning","abstract":"Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.","sentences":["Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens.","However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios.","This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification.","We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction.","We evaluate 13 long-context LLMs on our benchmarks.","We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window.","However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically.","This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences.","Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence.","Their ability to reason over multiple pieces in the long sequence is yet to be improved.","Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs.","We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs."],"url":"http://arxiv.org/abs/2404.02060v1","category":"cs.CL"}
{"created":"2024-04-02 15:57:32","title":"Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks","abstract":"Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.","sentences":["Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest.","This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize.","Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable.","The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time.","fastprop is freely available on github at github.com/JacksonBurns/fastprop."],"url":"http://arxiv.org/abs/2404.02058v1","category":"cs.LG"}
{"created":"2024-04-02 15:52:05","title":"Multitask-based Evaluation of Open-Source LLM on Software Vulnerability","abstract":"This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of LLMs based on this dataset. We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection. Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities.","sentences":["This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets.","We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks.","We evaluate the multitask and multilingual aspects of LLMs based on this dataset.","We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection.","Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types.","In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types.","Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting.","Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential.","Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities."],"url":"http://arxiv.org/abs/2404.02056v1","category":"cs.SE"}
{"created":"2024-04-02 15:50:55","title":"Deconstructing In-Context Learning: Understanding Prompts via Corruption","abstract":"The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models ($\\geq$30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.","sentences":["The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard.","These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback.","In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect.","Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation.","Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples.","Prior work has examined how modifying different elements of the prompt can affect model performance.","However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results.","Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging.","In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration.","We investigate the effects of structural and semantic corruptions of these elements on model performance.","We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks.","We find that repeating text within the prompt boosts model performance, and bigger models ($\\geq$30B) are more sensitive to the semantics of the prompt.","Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted."],"url":"http://arxiv.org/abs/2404.02054v1","category":"cs.CL"}
{"created":"2024-04-02 15:41:07","title":"Small-scale magnetohydrodynamic dynamos: from deterministic chaos to turbulence","abstract":"It is shown, using results of numerical simulations, and geophysical and solar observations, that the transition from deterministic chaos to hard turbulence in the magnetic field generated by the small-scale MHD dynamos occurs through a randomization process. This randomization process has been described using the notion of distributed chaos and the main parameter of distributed chaos has been used for quantifying the degree of randomization. The dissipative (Loitsianskii and Birkhoff-Saffman integrals) and ideal (magnetic helicity) magnetohydrodynamic invariants control the randomization process and determine the degree of randomization in different MHD flows, directly or through the Kolmogorov-Iroshnikov phenomenology (the magneto-inertial range of scales as a precursor of hard turbulence). Despite the considerable differences in the scales and physical parameters, the results of numerical simulations are in quantitative agreement with the geophysical and solar observations in the frames of this approach. The Hall magnetohydrodynamic dynamo has been also briefly discussed in this context.","sentences":["It is shown, using results of numerical simulations, and geophysical and solar observations, that the transition from deterministic chaos to hard turbulence in the magnetic field generated by the small-scale MHD dynamos occurs through a randomization process.","This randomization process has been described using the notion of distributed chaos and the main parameter of distributed chaos has been used for quantifying the degree of randomization.","The dissipative (Loitsianskii and Birkhoff-Saffman integrals) and ideal (magnetic helicity) magnetohydrodynamic invariants control the randomization process and determine the degree of randomization in different MHD flows, directly or through the Kolmogorov-Iroshnikov phenomenology (the magneto-inertial range of scales as a precursor of hard turbulence).","Despite the considerable differences in the scales and physical parameters, the results of numerical simulations are in quantitative agreement with the geophysical and solar observations in the frames of this approach.","The Hall magnetohydrodynamic dynamo has been also briefly discussed in this context."],"url":"http://arxiv.org/abs/2404.02049v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 15:39:14","title":"Universal representations for financial transactional data: embracing local, global, and external contexts","abstract":"Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\\% for the next MCC prediction task and up to 46\\% for downstream tasks from existing contrastive baselines. Incorporating external information improves the scores by an additional 20\\%.","sentences":["Effective processing of financial transactions is essential for banking data analysis.","However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems.","We present a representation learning framework that addresses diverse business challenges.","We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions.","Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time.","Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\\% for the next MCC prediction task and up to 46\\% for downstream tasks from existing contrastive baselines.","Incorporating external information improves the scores by an additional 20\\%."],"url":"http://arxiv.org/abs/2404.02047v1","category":"cs.LG"}
{"created":"2024-04-02 15:38:18","title":"Causality-based Transfer of Driving Scenarios to Unseen Intersections","abstract":"Scenario-based testing of automated driving functions has become a promising method to reduce time and cost compared to real-world testing. In scenario-based testing automated functions are evaluated in a set of pre-defined scenarios. These scenarios provide information about vehicle behaviors, environmental conditions, or road characteristics using parameters. To create realistic scenarios, parameters and parameter dependencies have to be fitted utilizing real-world data. However, due to the large variety of intersections and movement constellations found in reality, data may not be available for certain scenarios. This paper proposes a methodology to systematically analyze relations between parameters of scenarios. Bayesian networks are utilized to analyze causal dependencies in order to decrease the amount of required data and to transfer causal patterns creating unseen scenarios. Thereby, infrastructural influences on movement patterns are investigated to generate realistic scenarios on unobserved intersections. For evaluation, scenarios and underlying parameters are extracted from the inD dataset. Movement patterns are estimated, transferred and checked against recorded data from those initially unseen intersections.","sentences":["Scenario-based testing of automated driving functions has become a promising method to reduce time and cost compared to real-world testing.","In scenario-based testing automated functions are evaluated in a set of pre-defined scenarios.","These scenarios provide information about vehicle behaviors, environmental conditions, or road characteristics using parameters.","To create realistic scenarios, parameters and parameter dependencies have to be fitted utilizing real-world data.","However, due to the large variety of intersections and movement constellations found in reality, data may not be available for certain scenarios.","This paper proposes a methodology to systematically analyze relations between parameters of scenarios.","Bayesian networks are utilized to analyze causal dependencies in order to decrease the amount of required data and to transfer causal patterns creating unseen scenarios.","Thereby, infrastructural influences on movement patterns are investigated to generate realistic scenarios on unobserved intersections.","For evaluation, scenarios and underlying parameters are extracted from the inD dataset.","Movement patterns are estimated, transferred and checked against recorded data from those initially unseen intersections."],"url":"http://arxiv.org/abs/2404.02046v1","category":"cs.CV"}
{"created":"2024-04-02 15:37:31","title":"Impact of repetitive, ultra-short soft X-ray pulses from processing of steel with ultrafast lasers on human cell cultures","abstract":"Ultrafast lasers, with pulse durations below a few picoseconds, are of significant interest to the industry, offering a cutting-edge approach to enhancing manufacturing processes and enabling the fabrication of intricate components with unparalleled accuracy. When processing metals at irradiances exceeding the evaporation threshold of about 10^10 W/cm2 these processes can generate ultra-short, soft X-ray pulses with photon energies above 5 keV. This has prompted extensive discussions and regulatory measures on radiation safety. However, the impact of these ultra-short X-ray pulses on molecular pathways in the context of living cells, has not been investigated so far. Using laser pulses of 250 fs and 6.7 ps, along with pulse repetition rates exceeding 10 kHz, we conducted the first molecular characterization of epithelial cell responses to ultra-short soft X-ray pulses generated during processing of steel with ultrafast lasers. Ambient exposure of vitro human cell cultures, followed by imaging of the DNA damage response and fitting of the data to an experimentally calibrated model of dose rate estimation, revealed a linear increase in the DNA damage response relative to the exposure dose. This is in line with findings from work using continuous wave soft X-ray sources and suggests that the ultra-short X-ray pulses do not generate additional hazard. This research contributes valuable insights into the biological effects of ultrafast laser processes and their potential implications for user safety.","sentences":["Ultrafast lasers, with pulse durations below a few picoseconds, are of significant interest to the industry, offering a cutting-edge approach to enhancing manufacturing processes and enabling the fabrication of intricate components with unparalleled accuracy.","When processing metals at irradiances exceeding the evaporation threshold of about 10^10 W/cm2 these processes can generate ultra-short, soft X-ray pulses with photon energies above 5 keV.","This has prompted extensive discussions and regulatory measures on radiation safety.","However, the impact of these ultra-short X-ray pulses on molecular pathways in the context of living cells, has not been investigated so far.","Using laser pulses of 250 fs and 6.7 ps, along with pulse repetition rates exceeding 10 kHz, we conducted the first molecular characterization of epithelial cell responses to ultra-short soft X-ray pulses generated during processing of steel with ultrafast lasers.","Ambient exposure of vitro human cell cultures, followed by imaging of the DNA damage response and fitting of the data to an experimentally calibrated model of dose rate estimation, revealed a linear increase in the DNA damage response relative to the exposure dose.","This is in line with findings from work using continuous wave soft X-ray sources and suggests that the ultra-short X-ray pulses do not generate additional hazard.","This research contributes valuable insights into the biological effects of ultrafast laser processes and their potential implications for user safety."],"url":"http://arxiv.org/abs/2404.02044v1","category":"physics.bio-ph"}
{"created":"2024-04-02 15:37:09","title":"Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches","abstract":"Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the \"recipe\" for the optimal setups.","sentences":["Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident.","Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies.","Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks.","In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters.","We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the \"recipe\" for the optimal setups."],"url":"http://arxiv.org/abs/2404.02043v1","category":"cs.CL"}
{"created":"2024-04-02 15:34:52","title":"SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation","abstract":"We present a new self-supervised approach, SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views. Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator. We propose two self-supervised learning objectives: self-supervised person localization in 3d space and self-supervised 3d pose estimation. We achieve self-supervised 3d person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views. We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner. Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning. To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision. Our experiments and analysis on three public benchmark datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods. Code is available at \\url{https://github.com/CAMMA-public/SelfPose3D}","sentences":["We present a new self-supervised approach, SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views.","Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator.","We propose two self-supervised learning objectives: self-supervised person localization in 3d space and self-supervised 3d pose estimation.","We achieve self-supervised 3d person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views.","We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner.","Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning.","To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision.","Our experiments and analysis on three public benchmark datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods.","Code is available at \\url{https://github.com/CAMMA-public/SelfPose3D}"],"url":"http://arxiv.org/abs/2404.02041v1","category":"cs.CV"}
{"created":"2024-04-02 15:34:18","title":"A Survey on Large Language Model-Based Game Agents","abstract":"The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.","sentences":["The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI).","The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments.","This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint.","First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning.","Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games.","Finally, we present an outlook of future research and development directions in this burgeoning field.","A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers."],"url":"http://arxiv.org/abs/2404.02039v1","category":"cs.AI"}
{"created":"2024-04-02 15:32:32","title":"MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages","abstract":"Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models -- from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora -- showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.","sentences":["Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register.","Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023).","All these applications are extremely important to ensure safe communication in modern digital worlds.","However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup.","In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language.","Then, we experiment with different text detoxification models -- from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora -- showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language."],"url":"http://arxiv.org/abs/2404.02037v1","category":"cs.CL"}
{"created":"2024-04-02 15:23:08","title":"How much symmetry do symmetric measurements need for efficient operational applications?","abstract":"We introduce a generalization of symmetric measurements to collections of unequinumerous positive, operator-valued measures (POVMs). For informationally complete sets, we propose construction methods from orthonormal Hermitian operator bases. The correspondence between operator bases and measurements can be as high as four-to-four, with a one-to-one correspondence following only under additional assumptions. Importantly, it turns out that some of the symmetry properties, lost in the process of generalization, can be recovered without fixing the same number of elements for all POVMs. In particular, for a wide class of unequinumerous symmetric measurements that are conical 2-designs, we derive the index of coincidence, entropic uncertainty relations, and separability criteria for bipartite quantum states.","sentences":["We introduce a generalization of symmetric measurements to collections of unequinumerous positive, operator-valued measures (POVMs).","For informationally complete sets, we propose construction methods from orthonormal Hermitian operator bases.","The correspondence between operator bases and measurements can be as high as four-to-four, with a one-to-one correspondence following only under additional assumptions.","Importantly, it turns out that some of the symmetry properties, lost in the process of generalization, can be recovered without fixing the same number of elements for all POVMs.","In particular, for a wide class of unequinumerous symmetric measurements that are conical 2-designs, we derive the index of coincidence, entropic uncertainty relations, and separability criteria for bipartite quantum states."],"url":"http://arxiv.org/abs/2404.02034v1","category":"quant-ph"}
{"created":"2024-04-02 15:17:12","title":"Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework","abstract":"This study presents an innovative approach to portfolio optimization by integrating Transformer models with Generative Adversarial Networks (GANs) within the Black-Litterman (BL) framework. Capitalizing on Transformers' ability to discern long-range dependencies and GANs' proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations. This fusion of our model with BL's structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods. Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization.","sentences":["This study presents an innovative approach to portfolio optimization by integrating Transformer models with Generative Adversarial Networks (GANs) within the Black-Litterman (BL) framework.","Capitalizing on Transformers' ability to discern long-range dependencies and GANs' proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations.","This fusion of our model with BL's structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods.","Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization."],"url":"http://arxiv.org/abs/2404.02029v1","category":"cs.CE"}
{"created":"2024-04-02 15:17:09","title":"QUSL: Quantum Unsupervised Image Similarity Learning with Enhanced Performance","abstract":"Leveraging quantum advantages to enhance machine learning capabilities has become a primary focus of research, particularly for complex tasks such as image similarity detection. To fully exploit the potential of quantum computing, it is essential to design quantum circuits tailored to the specific characteristics of the task at hand. In response to this challenge, we propose a novel quantum unsupervised similarity learning method, QUSL. Building upon the foundation of similarity detection triplets and generating positive samples through perturbations of anchor images, QUSL operates independently of classical oracles. By leveraging the performance of triplets and the characteristics of quantum circuits, QUSL systematically explores high-performance quantum circuit architectures customized for dataset features using metaheuristic algorithms, thereby achieving efficient quantum feature extraction with reduced circuit costs. Comprehensive experiments demonstrate QUSL's remarkable performance compared to state-of-the-art quantum methods. QUSL achieves reductions exceeding 50% in critical quantum resource utilization while also realizing an enhancement of approximately 20% in similarity detection correlation across the DISC21, COCO, and landscape datasets. This enables efficient quantum similarity modeling for large-scale unlabeled image data with reduced quantum resource utilization.","sentences":["Leveraging quantum advantages to enhance machine learning capabilities has become a primary focus of research, particularly for complex tasks such as image similarity detection.","To fully exploit the potential of quantum computing, it is essential to design quantum circuits tailored to the specific characteristics of the task at hand.","In response to this challenge, we propose a novel quantum unsupervised similarity learning method, QUSL.","Building upon the foundation of similarity detection triplets and generating positive samples through perturbations of anchor images, QUSL operates independently of classical oracles.","By leveraging the performance of triplets and the characteristics of quantum circuits, QUSL systematically explores high-performance quantum circuit architectures customized for dataset features using metaheuristic algorithms, thereby achieving efficient quantum feature extraction with reduced circuit costs.","Comprehensive experiments demonstrate QUSL's remarkable performance compared to state-of-the-art quantum methods.","QUSL achieves reductions exceeding 50% in critical quantum resource utilization while also realizing an enhancement of approximately 20% in similarity detection correlation across the DISC21, COCO, and landscape datasets.","This enables efficient quantum similarity modeling for large-scale unlabeled image data with reduced quantum resource utilization."],"url":"http://arxiv.org/abs/2404.02028v1","category":"quant-ph"}
{"created":"2024-04-02 15:10:11","title":"Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts","abstract":"In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.","sentences":["In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems.","Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains.","This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks.","It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs.","With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline.","Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings."],"url":"http://arxiv.org/abs/2404.02022v1","category":"cs.CL"}
{"created":"2024-04-02 15:09:54","title":"On off-diagonal hypergraph Ramsey numbers","abstract":"A fundamental problem in Ramsey theory is to determine the growth rate in terms of $n$ of the Ramsey number $r(H, K_n^{(3)})$ of a fixed $3$-uniform hypergraph $H$ versus the complete $3$-uniform hypergraph with $n$ vertices. We study this problem, proving two main results. First, we show that for a broad class of $H$, including links of odd cycles and tight cycles of length not divisible by three, $r(H, K_n^{(3)}) \\ge 2^{\\Omega_H(n \\log n)}$. This significantly generalizes and simplifies an earlier construction of Fox and He which handled the case of links of odd cycles and is sharp both in this case and for all but finitely many tight cycles of length not divisible by three. Second, disproving a folklore conjecture in the area, we show that there exists a linear hypergraph $H$ for which $r(H, K_n^{(3)})$ is superpolynomial in $n$. This provides the first example of a separation between $r(H,K_n^{(3)})$ and $r(H,K_{n,n,n}^{(3)})$, since the latter is known to be polynomial in $n$ when $H$ is linear.","sentences":["A fundamental problem in Ramsey theory is to determine the growth rate in terms of $n$ of the Ramsey number $r(H, K_n^{(3)})$ of a fixed $3$-uniform hypergraph $H$ versus the complete $3$-uniform hypergraph with $n$ vertices.","We study this problem, proving two main results.","First, we show that for a broad class of $H$, including links of odd cycles and tight cycles of length not divisible by three, $r(H, K_n^{(3)})","\\ge 2^{\\Omega_H(n \\log n)}$. This significantly generalizes and simplifies an earlier construction of Fox and He which handled the case of links of odd cycles and is sharp both in this case and for all but finitely many tight cycles of length not divisible by three.","Second, disproving a folklore conjecture in the area, we show that there exists a linear hypergraph $H$ for which $r(H, K_n^{(3)})$ is superpolynomial in $n$. This provides the first example of a separation between $r(H,K_n^{(3)})$ and $r(H,K_{n,n,n}^{(3)})$, since the latter is known to be polynomial in $n$ when $H$ is linear."],"url":"http://arxiv.org/abs/2404.02021v1","category":"math.CO"}
{"created":"2024-04-02 15:09:22","title":"Aspects of the geometry and topology of expanding horizons","abstract":"The aim of this paper is to extend some basic results about marginally outer trapped surfaces to the context of surfaces having general null expansion. Motivated in part by recent work of Chai-Wan, we introduce the notion of $\\mathfrak{g}$-stability for a general closed hypersurface $\\Sigma$ in an ambient initial data set and prove that, under natural energy conditions, $\\Sigma$ has positive Yamabe type, that is, $\\Sigma$ admits a metric of positive scalar curvature, provided $\\Sigma$ is $\\mathfrak{g}$-stable. Similar results are obtained when $\\Sigma$ is embedded in a spacelike, or null, hypersurface of a spacetime satisfying the dominant energy condition. Conditions implying $\\mathfrak{g}$-stability are also discussed. Finally, we obtain a spacetime positive mass theorem for initial data sets with compact boundary $\\Sigma$ of positive null expansion, assuming that the dominant energy condition is sufficiently strict near $\\Sigma$. This extends recent results of Galloway-Lee and Lee-Lesourd-Unger.","sentences":["The aim of this paper is to extend some basic results about marginally outer trapped surfaces to the context of surfaces having general null expansion.","Motivated in part by recent work of Chai-Wan, we introduce the notion of $\\mathfrak{g}$-stability for a general closed hypersurface $\\Sigma$ in an ambient initial data set and prove that, under natural energy conditions, $\\Sigma$ has positive Yamabe type, that is, $\\Sigma$ admits a metric of positive scalar curvature, provided $\\Sigma$ is $\\mathfrak{g}$-stable.","Similar results are obtained when $\\Sigma$ is embedded in a spacelike, or null, hypersurface of a spacetime satisfying the dominant energy condition.","Conditions implying $\\mathfrak{g}$-stability are also discussed.","Finally, we obtain a spacetime positive mass theorem for initial data sets with compact boundary $\\Sigma$ of positive null expansion, assuming that the dominant energy condition is sufficiently strict near $\\Sigma$. This extends recent results of Galloway-Lee and Lee-Lesourd-Unger."],"url":"http://arxiv.org/abs/2404.02019v1","category":"gr-qc"}
{"created":"2024-04-02 15:08:35","title":"Large Language Models for Orchestrating Bimanual Robots","abstract":"Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have taken control of a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge for the first time by an LLM, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. In the simulated environment, the LABOR agent is evaluated through several everyday tasks on the NICOL humanoid robot. Reported success rates indicate that overall coordination efficiency is close to optimal performance, while the analysis of failure causes, classified into spatial and temporal coordination and skill selection, shows that these vary over tasks. The project website can be found at http://labor-agent.github.io","sentences":["Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination.","With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have taken control of a variety of robotic tasks.","However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks.","To tackle this challenge for the first time by an LLM, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks.","In the simulated environment, the LABOR agent is evaluated through several everyday tasks on the NICOL humanoid robot.","Reported success rates indicate that overall coordination efficiency is close to optimal performance, while the analysis of failure causes, classified into spatial and temporal coordination and skill selection, shows that these vary over tasks.","The project website can be found at http://labor-agent.github.io"],"url":"http://arxiv.org/abs/2404.02018v1","category":"cs.RO"}
{"created":"2024-04-02 14:55:47","title":"Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces","abstract":"Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each language dataset comprising over 7,600 crowdsourced annotations across labels for explicit abuse, targeted minority attacks and general offences. The validation scores showed strong performance across f1-measures, especially for English 0.84. Our experiments reveal how customizing embeddings and model hyperparameters can improve detection capability. The proposed architecture ranked 1st in the competition, proving its ability to handle real-world noisy text with code-switching. This technique has a promising scope as platforms aim to combat cyber harassment facing Indic language internet users. Our Code is at https://github.com/advaithavetagiri/CNLP-NITS-PP","sentences":["Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces.","Detecting such abusive content can enable platforms to curb this menace.","We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse.","Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data.","The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text.","To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases.","Multiple variations were trained using FastText and GloVe word embeddings for each language dataset comprising over 7,600 crowdsourced annotations across labels for explicit abuse, targeted minority attacks and general offences.","The validation scores showed strong performance across f1-measures, especially for English 0.84.","Our experiments reveal how customizing embeddings and model hyperparameters can improve detection capability.","The proposed architecture ranked 1st in the competition, proving its ability to handle real-world noisy text with code-switching.","This technique has a promising scope as platforms aim to combat cyber harassment facing Indic language internet users.","Our Code is at https://github.com/advaithavetagiri/CNLP-NITS-PP"],"url":"http://arxiv.org/abs/2404.02013v1","category":"cs.CL"}
{"created":"2024-04-02 14:55:35","title":"Superionic Fluoride Gate Dielectrics with Low Diffusion Barrier for Advanced Electronics","abstract":"Exploration of new dielectrics with large capacitive coupling is an essential topic in modern electronics when conventional dielectrics suffer from the leakage issue near breakdown limit. To address this looming challenge, we demonstrate that rare-earth-metal fluorides with extremely-low ion migration barriers can generally exhibit an excellent capacitive coupling over 20 $\\mu$F cm$^{-2}$ (with an equivalent oxide thickness of ~0.15 nm and a large effective dielectric constant near 30) and great compatibility with scalable device manufacturing processes. Such static dielectric capability of superionic fluorides is exemplified by MoS$_2$ transistors exhibiting high on/off current ratios over 10$^8$, ultralow subthreshold swing of 65 mV dec$^{-1}$, and ultralow leakage current density of ~10$^{-6}$ A cm$^{-2}$. Therefore, the fluoride-gated logic inverters can achieve significantly higher static voltage gain values, surpassing ~167, compared to conventional dielectric. Furthermore, the application of fluoride gating enables the demonstration of NAND, NOR, AND, and OR logic circuits with low static energy consumption. Notably, the superconductor-to-insulator transition at the clean-limit Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ can also be realized through fluoride gating. Our findings highlight fluoride dielectrics as a pioneering platform for advanced electronics applications and for tailoring emergent electronic states in condensed matters.","sentences":["Exploration of new dielectrics with large capacitive coupling is an essential topic in modern electronics when conventional dielectrics suffer from the leakage issue near breakdown limit.","To address this looming challenge, we demonstrate that rare-earth-metal fluorides with extremely-low ion migration barriers can generally exhibit an excellent capacitive coupling over 20 $\\mu$F cm$^{-2}$","(with an equivalent oxide thickness of ~0.15 nm and a large effective dielectric constant near 30) and great compatibility with scalable device manufacturing processes.","Such static dielectric capability of superionic fluorides is exemplified by MoS$_2$ transistors exhibiting high on/off current ratios over 10$^8$, ultralow subthreshold swing of 65 mV dec$^{-1}$, and ultralow leakage current density of ~10$^{-6}$ A cm$^{-2}$.","Therefore, the fluoride-gated logic inverters can achieve significantly higher static voltage gain values, surpassing ~167, compared to conventional dielectric.","Furthermore, the application of fluoride gating enables the demonstration of NAND, NOR, AND, and OR logic circuits with low static energy consumption.","Notably, the superconductor-to-insulator transition at the clean-limit Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ can also be realized through fluoride gating.","Our findings highlight fluoride dielectrics as a pioneering platform for advanced electronics applications and for tailoring emergent electronic states in condensed matters."],"url":"http://arxiv.org/abs/2404.02011v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 14:48:20","title":"An MILP-Based Solution Scheme for Factored and Robust Factored Markov Decision Processes","abstract":"Factored Markov decision processes (MDPs) are a prominent paradigm within the artificial intelligence community for modeling and solving large-scale MDPs whose rewards and dynamics decompose into smaller, loosely interacting components. Through the use of dynamic Bayesian networks and context-specific independence, factored MDPs can achieve an exponential reduction in the state space of an MDP and thus scale to problem sizes that are beyond the reach of classical MDP algorithms. However, factored MDPs are typically solved using custom-designed algorithms that can require meticulous implementations and considerable fine-tuning. In this paper, we propose a mathematical programming approach to solving factored MDPs. In contrast to existing solution schemes, our approach leverages off-the-shelf solvers, which allows for a streamlined implementation and maintenance; it effectively capitalizes on the factored structure present in both state and action spaces; and it readily extends to the largely unexplored class of robust factored MDPs, whose transition kernels are only known to reside in a pre-specified ambiguity set. Our numerical experiments demonstrate the potential of our approach.","sentences":["Factored Markov decision processes (MDPs) are a prominent paradigm within the artificial intelligence community for modeling and solving large-scale MDPs whose rewards and dynamics decompose into smaller, loosely interacting components.","Through the use of dynamic Bayesian networks and context-specific independence, factored MDPs can achieve an exponential reduction in the state space of an MDP and thus scale to problem sizes that are beyond the reach of classical MDP algorithms.","However, factored MDPs are typically solved using custom-designed algorithms that can require meticulous implementations and considerable fine-tuning.","In this paper, we propose a mathematical programming approach to solving factored MDPs.","In contrast to existing solution schemes, our approach leverages off-the-shelf solvers, which allows for a streamlined implementation and maintenance; it effectively capitalizes on the factored structure present in both state and action spaces; and it readily extends to the largely unexplored class of robust factored MDPs, whose transition kernels are only known to reside in a pre-specified ambiguity set.","Our numerical experiments demonstrate the potential of our approach."],"url":"http://arxiv.org/abs/2404.02006v1","category":"math.OC"}
{"created":"2024-04-02 14:44:02","title":"AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design","abstract":"Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.","sentences":["Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success.","However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles.","To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model.","Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling.","In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical.","Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity."],"url":"http://arxiv.org/abs/2404.02003v1","category":"cs.LG"}
{"created":"2024-04-02 14:42:52","title":"Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning","abstract":"Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether reinforcement learning can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners' training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the reinforcement learning algorithm to explain how the agents perform their tasks. To this end, we identify three emerging dominant strategies and several rare approaches taken. These strategies, whilst producing almost identical trajectories in simulation, are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions.","sentences":["Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments.","Here we investigate whether reinforcement learning can provide insights into biological systems when trained to perform chemotaxis.","Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target.","We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners' training fails.","We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment.","We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds.","Finally, we study the strategy adopted by the reinforcement learning algorithm to explain how the agents perform their tasks.","To this end, we identify three emerging dominant strategies and several rare approaches taken.","These strategies, whilst producing almost identical trajectories in simulation, are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions."],"url":"http://arxiv.org/abs/2404.01999v1","category":"physics.bio-ph"}
{"created":"2024-04-02 14:41:42","title":"Specularity Factorization for Low-Light Enhancement","abstract":"We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet. The code and data is released for reproducibility on the project homepage.","sentences":["We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition.","Our model-driven {\\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned.","The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion.","Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision.","Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets.","We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet.","The code and data is released for reproducibility on the project homepage."],"url":"http://arxiv.org/abs/2404.01998v1","category":"cs.CV"}
{"created":"2024-04-02 14:40:04","title":"DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning","abstract":"Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.","sentences":["Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction.","For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history.","Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective.","Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision.","To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning.","This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making.","Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations.","We also reconstruct a dual-level instruction for adaptation to the dual-level alignment.","As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities.","Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN."],"url":"http://arxiv.org/abs/2404.01994v1","category":"cs.CV"}
{"created":"2024-04-02 14:22:54","title":"Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues","abstract":"For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person's gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system's ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.","sentences":["For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience.","We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way.","Our main contribution is a study of the benefit of features representing the person's gaze in this context.","Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system's ability to adapt to new environments without external supervision.","Qualitative experiments show practical applications with a waiter robot."],"url":"http://arxiv.org/abs/2404.01986v1","category":"cs.RO"}
{"created":"2024-04-02 14:22:04","title":"Fashion Style Editing with Generative Human Prior","abstract":"Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications. Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited. In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions. Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space. We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing. Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field.","sentences":["Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications.","Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited.","In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions.","Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space.","We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing.","Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field."],"url":"http://arxiv.org/abs/2404.01984v1","category":"cs.CV"}
{"created":"2024-04-02 14:19:30","title":"Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials","abstract":"Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize to clinical speakers, with less than 2.7% EER for European Languages and 8.26% EER for Arabic. This represents a significant step in developing more versatile and efficient speaker verification systems for cognitive and mental health clinical trials that can be used across a wide range of languages and dialects, substantially reducing the effort required to develop speaker verification systems for multiple languages. We also evaluate how speech tasks and number of speakers involved in the trial influence the performance and show that the type of speech tasks impacts the model performance.","sentences":["Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge.","In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders.","We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial.","Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative.","We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages.","Our results demonstrate that tested models can effectively generalize to clinical speakers, with less than 2.7% EER for European Languages and 8.26% EER for Arabic.","This represents a significant step in developing more versatile and efficient speaker verification systems for cognitive and mental health clinical trials that can be used across a wide range of languages and dialects, substantially reducing the effort required to develop speaker verification systems for multiple languages.","We also evaluate how speech tasks and number of speakers involved in the trial influence the performance and show that the type of speech tasks impacts the model performance."],"url":"http://arxiv.org/abs/2404.01981v1","category":"cs.LG"}
{"created":"2024-04-02 14:16:59","title":"Joint-Task Regularization for Partially Labeled Multi-Task Learning","abstract":"Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets. Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks. Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image. With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks. JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs -- therefore, it achieves linear complexity relative to the number of tasks while previous methods scale quadratically. To demonstrate the validity of our approach, we extensively benchmark our method across a wide variety of partially labeled scenarios based on NYU-v2, Cityscapes, and Taskonomy.","sentences":["Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets.","Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks.","Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image.","With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks.","JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs -- therefore, it achieves linear complexity relative to the number of tasks while previous methods scale quadratically.","To demonstrate the validity of our approach, we extensively benchmark our method across a wide variety of partially labeled scenarios based on NYU-v2, Cityscapes, and Taskonomy."],"url":"http://arxiv.org/abs/2404.01976v1","category":"cs.CV"}
{"created":"2024-04-02 14:16:57","title":"DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation","abstract":"Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public. Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions. To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology). Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data). The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way. Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids. In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid graphs and images. Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE.","sentences":["Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public.","Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions.","To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology).","Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data).","The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way.","Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids.","In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid graphs and images.","Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE."],"url":"http://arxiv.org/abs/2404.01975v1","category":"cs.LG"}
{"created":"2024-04-02 14:16:56","title":"Gaia23bab : a new EXor","abstract":"On March 6 2023, the Gaia telescope has alerted a 2-magnitude burst from Gaia23bab, a Young Stellar Object in the Galactic plane. We observed Gaia23bab with the Large Binocular Telescope obtaining optical and near-infrared spectra close in time to the peak of the burst, and collected all public multi-band photometry to reconstruct the historical light curve. This latter shows three bursts in ten years (2013, 2017 and 2023), whose duration and amplitude are typical of EXor variables.   We estimate that, due to the bursts, the mass accumulated on the star is about twice greater than if the source had remained quiescent for the same period of time. Photometric analysis indicates that Gaia23bab is a Class,II source with age < 1 Myr, spectral type G3-K0, stellar luminosity 4.0 L_sun, and mass 1.6 M_sun. The optical/near infrared spectrum is rich in emission lines. From the analysis of these lines we measured the accretion luminosity and the mass accretion rate L_acc(burst)=3.7 L_sun, M_acc(burst) 2.0 10 $^(-7) M_sun/yr, consistent with those of EXors. More generally, we derive the relationships between accretion and stellar parameters in a sample of EXors. We find that, when in burst, the accretion parameters become almost independent of the stellar parameters and that EXors, even in quiescence, are more efficient than classical T Tauri stars in assembling mass.","sentences":["On March 6 2023, the Gaia telescope has alerted a 2-magnitude burst from Gaia23bab, a Young Stellar Object in the Galactic plane.","We observed Gaia23bab with the Large Binocular Telescope obtaining optical and near-infrared spectra close in time to the peak of the burst, and collected all public multi-band photometry to reconstruct the historical light curve.","This latter shows three bursts in ten years (2013, 2017 and 2023), whose duration and amplitude are typical of EXor variables.   ","We estimate that, due to the bursts, the mass accumulated on the star is about twice greater than if the source had remained quiescent for the same period of time.","Photometric analysis indicates that Gaia23bab is a Class,II source with age <","1 Myr, spectral type G3-K0, stellar luminosity 4.0 L_sun, and mass 1.6 M_sun.","The optical/near infrared spectrum is rich in emission lines.","From the analysis of these lines we measured the accretion luminosity and the mass accretion rate L_acc(burst)=3.7 L_sun, M_acc(burst) 2.0 10 $^(-7) M_sun/yr, consistent with those of EXors.","More generally, we derive the relationships between accretion and stellar parameters in a sample of EXors.","We find that, when in burst, the accretion parameters become almost independent of the stellar parameters and that EXors, even in quiescence, are more efficient than classical T Tauri stars in assembling mass."],"url":"http://arxiv.org/abs/2404.01974v1","category":"astro-ph.SR"}
{"created":"2024-04-02 14:14:55","title":"Hook-Lengths, Symplectic/Orthogonal Contents and Amdeberhan's Conjectures","abstract":"The symplectic/orthogonal contents of partitions are related to the dimensions of irreducible representations of symplectic/orthogonal groups. In 2012, motivated by Nekrasov-Okounkov's hook-length formula and Stanley's hook-content formula, Amdeberhan proposed several conjectures about infinite product formulas for certain generating functions of hook-lengths and symplectic/orthogonal contents. Some special cases of his conjectures were recently proved by Amdeberhan, Andrews and Ballantine. In this paper, we prove the general cases of Amdeberhan's conjectures.","sentences":["The symplectic/orthogonal contents of partitions are related to the dimensions of irreducible representations of symplectic/orthogonal groups.","In 2012, motivated by Nekrasov-Okounkov's hook-length formula and Stanley's hook-content formula, Amdeberhan proposed several conjectures about infinite product formulas for certain generating functions of hook-lengths and symplectic/orthogonal contents.","Some special cases of his conjectures were recently proved by Amdeberhan, Andrews and Ballantine.","In this paper, we prove the general cases of Amdeberhan's conjectures."],"url":"http://arxiv.org/abs/2404.01973v1","category":"math.CO"}
{"created":"2024-04-02 14:14:29","title":"A novel analysis method for calculating nonlinear Frequency Response Functions","abstract":"The Frequency Response Functions (FRFs) are the most widely used functions to characterise the dynamic behaviour of structures. The natural frequencies and damping behaviour can be easily and quickly detected from a Bode diagram. The modal properties of FRFs can be evaluated using modal analysis methods, and as the last step, frequency response models can synthesise response functions to verify the robustness of the modal parameters identified by the analysis. The circularity between 1) measurement, 2) identification, 3) regeneration and 4) comparison is ensured on the assumption that transfer functions are measured under linear vibrations, even though mechanical systems are intrinsically non-linear. Some sources of nonlinearity might be excited and revealed, and others not for various reasons. Anyhow, it is unavoidable to measure non-linear vibrations when vibration tests are executed at various levels of excitation forces. Eventually, linear and non-linear vibrations are processed to obtain linear and non-linear FRFs. The linear FRFs are processed using the existing identification methods. The non-linear FRFs are archived or blandly processed to evaluate the level and the type of nonlinearity, such as hardening or softening behaviour. This research aims to (i) formulate a new analysis method to generate nonlinear frequency responses and (ii) formulate a new identification method for extracting amplitude-dependent modal parameters. The first objective will demonstrate that a nonlinear frequency response surface generated by linear FRFs is the solution space of nonlinear FRFs. The second objective will demonstrate that a linear modal analysis method called line-fit, based on the Dobson formulation, allows extracting amplitude-dependent modal parameters from non-linear FRFs.","sentences":["The Frequency Response Functions (FRFs) are the most widely used functions to characterise the dynamic behaviour of structures.","The natural frequencies and damping behaviour can be easily and quickly detected from a Bode diagram.","The modal properties of FRFs can be evaluated using modal analysis methods, and as the last step, frequency response models can synthesise response functions to verify the robustness of the modal parameters identified by the analysis.","The circularity between 1) measurement, 2) identification, 3) regeneration and 4) comparison is ensured on the assumption that transfer functions are measured under linear vibrations, even though mechanical systems are intrinsically non-linear.","Some sources of nonlinearity might be excited and revealed, and others not for various reasons.","Anyhow, it is unavoidable to measure non-linear vibrations when vibration tests are executed at various levels of excitation forces.","Eventually, linear and non-linear vibrations are processed to obtain linear and non-linear FRFs.","The linear FRFs are processed using the existing identification methods.","The non-linear FRFs are archived or blandly processed to evaluate the level and the type of nonlinearity, such as hardening or softening behaviour.","This research aims to (i) formulate a new analysis method to generate nonlinear frequency responses and (ii) formulate a new identification method for extracting amplitude-dependent modal parameters.","The first objective will demonstrate that a nonlinear frequency response surface generated by linear FRFs is the solution space of nonlinear FRFs.","The second objective will demonstrate that a linear modal analysis method called line-fit, based on the Dobson formulation, allows extracting amplitude-dependent modal parameters from non-linear FRFs."],"url":"http://arxiv.org/abs/2404.01972v1","category":"physics.class-ph"}
{"created":"2024-04-02 14:14:09","title":"Combinatorial flag arrangements","abstract":"We introduce combinatorial objects named matricubes that provide a generalization of the theory of matroids. As matroids provide a combinatorial axiomatization of hyperplane arrangements, matricubes provide a combinatorial axiomatization of arrangements of initial flags in a vector space. We give cryptomorphic axiomatic systems in terms of rank function, flats, circuits, and independent sets, and formulate a duality concept. We also provide precise links between matricubes, permutation arrays and matroids, and raise several open questions.","sentences":["We introduce combinatorial objects named matricubes that provide a generalization of the theory of matroids.","As matroids provide a combinatorial axiomatization of hyperplane arrangements, matricubes provide a combinatorial axiomatization of arrangements of initial flags in a vector space.","We give cryptomorphic axiomatic systems in terms of rank function, flats, circuits, and independent sets, and formulate a duality concept.","We also provide precise links between matricubes, permutation arrays and matroids, and raise several open questions."],"url":"http://arxiv.org/abs/2404.01971v1","category":"math.CO"}
{"created":"2024-04-02 14:14:04","title":"Anisotropic quark stars in $f(R,L_m,T)$ gravity","abstract":"We investigate the impact of $f(R,L_m,T)$ gravity on the internal structure of compact stars, expecting this theory to manifest prominently in the high-density cores of such stars. In this study, we begin by considering the algebraic function $f(R,L_m,T) = R + \\alpha T L_m$, where $\\alpha$ represents the matter-geometry coupling constant. We specifically choose the matter Lagrangian density $L_m= -\\rho$ to explore compact stars with anisotropic pressure. To this end, we employ the MIT bag model as an equation of state. We then numerically solve the hydrostatic equilibrium equations to obtain mass-radius relations for quark stars, examining static stability criteria, the adiabatic index, and the speed of sound. Finally, we use recent astrophysical data to constrain the coupling parameter $\\alpha$, which may lead to either larger or smaller masses for quark stars compared to their counterparts in general relativity.","sentences":["We investigate the impact of $f(R,L_m,T)$ gravity on the internal structure of compact stars, expecting this theory to manifest prominently in the high-density cores of such stars.","In this study, we begin by considering the algebraic function $f(R,L_m,T) = R + \\alpha T L_m$, where $\\alpha$ represents the matter-geometry coupling constant.","We specifically choose the matter Lagrangian density $L_m= -\\rho$ to explore compact stars with anisotropic pressure.","To this end, we employ the MIT bag model as an equation of state.","We then numerically solve the hydrostatic equilibrium equations to obtain mass-radius relations for quark stars, examining static stability criteria, the adiabatic index, and the speed of sound.","Finally, we use recent astrophysical data to constrain the coupling parameter $\\alpha$, which may lead to either larger or smaller masses for quark stars compared to their counterparts in general relativity."],"url":"http://arxiv.org/abs/2404.01970v1","category":"gr-qc"}
{"created":"2024-04-02 14:03:37","title":"Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks","abstract":"Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\\% in accuracy and low computational cost. Overall, our method accelerates efficient model development while enabling sustainable AI applications.","sentences":["Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets.","However, the computational demands of DL models pose environmental and resource challenges.","Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference.","Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques.","We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption.","Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization.","Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\\% in accuracy and low computational cost.","Overall, our method accelerates efficient model development while enabling sustainable AI applications."],"url":"http://arxiv.org/abs/2404.01965v1","category":"cs.LG"}
{"created":"2024-04-02 13:55:39","title":"Existence of solutions to the generalized dual Minkowski problem","abstract":"Given a real number $q$ and a star body in the $n$-dimensional Euclidean space, the generalized dual curvature measure of a convex body was introduced by Lutwak-Yang-Zhang [43]. The corresponding generalized dual Minkowski problem is studied in this paper. By using variational methods, we solve the generalized dual Minkowski problem for $q<0$, and the even generalized dual Minkowski problem for $0\\leq q\\leq1$. We also obtain a sufficient condition for the existence of solutions to the even generalized dual Minkowski problem for $1<q<n$.","sentences":["Given a real number $q$ and a star body in the $n$-dimensional Euclidean space, the generalized dual curvature measure of a convex body was introduced by Lutwak-Yang-Zhang","[43].","The corresponding generalized dual Minkowski problem is studied in this paper.","By using variational methods, we solve the generalized dual Minkowski problem for $q<0$, and the even generalized dual Minkowski problem for $0\\leq q\\leq1$. We also obtain a sufficient condition for the existence of solutions to the even generalized dual Minkowski problem for $1<q<n$."],"url":"http://arxiv.org/abs/2404.01962v1","category":"math.AP"}
{"created":"2024-04-02 13:54:22","title":"Bi-LORA: A Vision-Language Approach for Synthetic Image Detection","abstract":"Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.","sentences":["Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images.","While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts.","This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs).","We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images.","The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2).","Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs.","The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models.","The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT."],"url":"http://arxiv.org/abs/2404.01959v1","category":"cs.CV"}
{"created":"2024-04-02 13:49:10","title":"Contact germs and partial differential equations","abstract":"The article introduces contact germs that transform solutions of some partial differential equations into solutions of other equations. Parametric symmetries of differential equations generalizing point and contact symmetries are defined. New transformations and symmetries may depend on derivatives of arbitrary but finite order. The stationary Schr\\\"odinger equations, acoustics and gas dynamics equations are considered as examples.","sentences":["The article introduces contact germs that transform solutions of some partial differential equations into solutions of other equations.","Parametric symmetries of differential equations generalizing point and contact symmetries are defined.","New transformations and symmetries may depend on derivatives of arbitrary but finite order.","The stationary Schr\\\"odinger equations, acoustics and gas dynamics equations are considered as examples."],"url":"http://arxiv.org/abs/2404.01955v1","category":"nlin.SI"}
{"created":"2024-04-02 13:48:49","title":"HyperCLOVA X Technical Report","abstract":"We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.","sentences":["We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding.","HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI.","The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English.","HyperCLOVA","X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances.","Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks.","We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs."],"url":"http://arxiv.org/abs/2404.01954v1","category":"cs.CL"}
{"created":"2024-04-02 13:46:45","title":"Near-unity indistinguishability of single photons emitted from dissimilar and independent atomic quantum nodes","abstract":"Generating indistinguishable photons from independent nodes is an important challenge for the development of quantum networks. In this work, we demonstrate the generation of highly indistinguishable single photons from two dissimilar atomic quantum nodes. One node is based on a fully blockaded cold Rydberg ensemble and generates on-demand single photons. The other node is a quantum repeater node based on a DLCZ quantum memory and emits heralded single photons after a controllable memory time that is used to synchronize the two sources. We demonstrate an indistinguishability of ${94.6 \\pm 5.2 \\%}$ for a temporal window including ${90\\%}$ of the photons. This advancement opens new possibilities for interconnecting quantum repeater and processing nodes with high fidelity Bell-state measurement without sacrificing its efficiency.","sentences":["Generating indistinguishable photons from independent nodes is an important challenge for the development of quantum networks.","In this work, we demonstrate the generation of highly indistinguishable single photons from two dissimilar atomic quantum nodes.","One node is based on a fully blockaded cold Rydberg ensemble and generates on-demand single photons.","The other node is a quantum repeater node based on a DLCZ quantum memory and emits heralded single photons after a controllable memory time that is used to synchronize the two sources.","We demonstrate an indistinguishability of ${94.6 \\pm 5.2 \\%}$ for a temporal window including ${90\\%}$ of the photons.","This advancement opens new possibilities for interconnecting quantum repeater and processing nodes with high fidelity Bell-state measurement without sacrificing its efficiency."],"url":"http://arxiv.org/abs/2404.01951v1","category":"quant-ph"}
{"created":"2024-04-02 13:42:46","title":"Unique continuation of Schr\u00f6dinger-type equations for $\\bar\\partial$","abstract":"The purpose of this paper is to study the unique continuation property for a Schr\\\"odinger-type equation $ \\bar\\partial u = Vu$ on a domain in $\\mathbb C^n$, where the solution $u$ may be a scalar function, or a vector-valued function. While simple examples show that the unique continuation property fails in general if the potential $V\\in L^{p}, p<2n$, we first prove that, in the case when $u $ is a scalar function, the unique continuation property holds when $V\\in L_{loc}^{2n}$ and is $\\bar\\partial$-closed. For vector-valued smooth solutions, we establish the unique continuation property either when $V\\in L_{loc}^p $, $ p>2n$ for $n\\ge 3$, or when $V\\in L_{loc}^{2n}$ for $n = 2$. Finally, we discuss the unique continuation property for some special cases where $V\\notin L_{loc}^{2n}$, for instance, $V $ is a constant multiple of $ \\frac{1}{|z|}$.","sentences":["The purpose of this paper is to study the unique continuation property for a Schr\\\"odinger-type equation $ \\bar\\partial u = Vu$ on a domain in $\\mathbb C^n$, where the solution $u$ may be a scalar function, or a vector-valued function.","While simple examples show that the unique continuation property fails in general if the potential $V\\in L^{p}, p<2n$, we first prove that, in the case when $u $ is a scalar function, the unique continuation property holds when $V\\in L_{loc}^{2n}$ and is $\\bar\\partial$-closed.","For vector-valued smooth solutions, we establish the unique continuation property either when $V\\in L_{loc}^p $, $ p>2n$ for $n\\ge 3$, or when $V\\in L_{loc}^{2n}$ for $n = 2$.","Finally, we discuss the unique continuation property for some special cases where $V\\notin L_{loc}^{2n}$, for instance, $V $ is a constant multiple of $ \\frac{1}{|z|}$."],"url":"http://arxiv.org/abs/2404.01947v1","category":"math.CV"}
{"created":"2024-04-02 13:38:08","title":"Simulation and time series analysis of responsive active Brownian particles (rABPs) with memory","abstract":"To realise the goals of active matter at the micro- and nano-scale, the next generation of microrobots must be capable of autonomously sensing and responding to their environment to carry out pre-programmed tasks. Memory effects are proposed to have a significant effect on the dynamics of responsive robotic systems, drawing parallels to strategies used in nature across all length-scales. Inspired by the integral feedback control mechanism by which E.coli are proposed to sense their environment, we develop a numerical responsive active Brownian particle (rABP) model in which the rABPs continuously react to changes in the physical parameters dictated by their local environment. The resulting generated time series are then used to classify and characterise their response, leading to the identification of conditional heteroscedasticity in their physics. We then train recurrent neural networks (RNNs) capable of quantitatively describing the responsiveness of rABPs using their 2-D trajectories. We believe that our proposed strategy to determine the parameters governing the dynamics of rABPs can be applied to guide the design of microrobots with physical intelligence encoded during their fabrication.","sentences":["To realise the goals of active matter at the micro- and nano-scale, the next generation of microrobots must be capable of autonomously sensing and responding to their environment to carry out pre-programmed tasks.","Memory effects are proposed to have a significant effect on the dynamics of responsive robotic systems, drawing parallels to strategies used in nature across all length-scales.","Inspired by the integral feedback control mechanism by which E.coli are proposed to sense their environment, we develop a numerical responsive active Brownian particle (rABP) model in which the rABPs continuously react to changes in the physical parameters dictated by their local environment.","The resulting generated time series are then used to classify and characterise their response, leading to the identification of conditional heteroscedasticity in their physics.","We then train recurrent neural networks (RNNs) capable of quantitatively describing the responsiveness of rABPs using their 2-D trajectories.","We believe that our proposed strategy to determine the parameters governing the dynamics of rABPs can be applied to guide the design of microrobots with physical intelligence encoded during their fabrication."],"url":"http://arxiv.org/abs/2404.01944v1","category":"cond-mat.soft"}
{"created":"2024-04-02 13:33:23","title":"Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation","abstract":"Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.","sentences":["Understanding cybercrime communications is paramount for cybersecurity defence.","This often involves translating communications into English for processing, interpreting, and generating timely intelligence.","The problem is that translation is hard.","Human translation is slow, expensive, and scarce.","Machine translation is inaccurate and biased.","We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language.","We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group.","Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language.","Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator."],"url":"http://arxiv.org/abs/2404.01940v1","category":"cs.CL"}
{"created":"2024-04-02 13:31:19","title":"Settling Time vs. Accuracy Tradeoffs for Clustering Big Data","abstract":"We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. Any approach that significantly improves on this must then resort to practical heuristics, leading us to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings. Through this, we show the conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient. As a result, we provide a comprehensive theoretical and practical blueprint for effective clustering regardless of data size. Our code is publicly available and has scripts to recreate the experiments.","sentences":["We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets.","Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation.","Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow.","Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size.","We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data.","Any approach that significantly improves on this must then resort to practical heuristics, leading us to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings.","Through this, we show the conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient.","As a result, we provide a comprehensive theoretical and practical blueprint for effective clustering regardless of data size.","Our code is publicly available and has scripts to recreate the experiments."],"url":"http://arxiv.org/abs/2404.01936v1","category":"cs.LG"}
{"created":"2024-04-02 13:29:38","title":"Towards a Completeness Argumentation for Scenario Concepts","abstract":"Scenario-based testing has become a promising approach to overcome the complexity of real-world traffic for safety assurance of automated vehicles. Within scenario-based testing, a system under test is confronted with a set of predefined scenarios. This set shall ensure more efficient testing of an automated vehicle operating in an open context compared to real-world testing. However, the question arises if a scenario catalog can cover the open context sufficiently to allow an argumentation for sufficiently safe driving functions and how this can be proven. Within this paper, a methodology is proposed to argue a sufficient completeness of a scenario concept using a goal structured notation. Thereby, the distinction between completeness and coverage is discussed. For both, methods are proposed for a streamlined argumentation and regarding evidence. These methods are applied to a scenario concept and the inD dataset to prove the usability.","sentences":["Scenario-based testing has become a promising approach to overcome the complexity of real-world traffic for safety assurance of automated vehicles.","Within scenario-based testing, a system under test is confronted with a set of predefined scenarios.","This set shall ensure more efficient testing of an automated vehicle operating in an open context compared to real-world testing.","However, the question arises if a scenario catalog can cover the open context sufficiently to allow an argumentation for sufficiently safe driving functions and how this can be proven.","Within this paper, a methodology is proposed to argue a sufficient completeness of a scenario concept using a goal structured notation.","Thereby, the distinction between completeness and coverage is discussed.","For both, methods are proposed for a streamlined argumentation and regarding evidence.","These methods are applied to a scenario concept and the inD dataset to prove the usability."],"url":"http://arxiv.org/abs/2404.01934v1","category":"cs.SE"}
{"created":"2024-04-02 13:19:59","title":"Synchro-curvature description of $\u03b3$-ray light curves and spectra of pulsars: global properties","abstract":"This work presents a methodological approach to generate realistic $\\gamma$-ray light curves of pulsars, resembling reasonably well the observational ones observed by the Fermi-Large Area Telescope instrument, fitting at the same time their high-energy spectra. The theoretical light curves are obtained from a spectral and geometrical model of the synchro-curvature emission. Despite our model relies on a few effective physical parameters, the synthetic light curves present the same main features observed in the observational $\\gamma$-ray light curve zoo, such as the different shapes, variety in the number of peaks, and a diversity of peak widths. The morphological features of the light curves allows us to statistically compare the observed properties. In particular, we find that the proportion on the number of peaks found in our synthetic light curves is in agreement with the observational one provided by the third Fermi-LAT pulsar catalog. We also found that the detection probability due to beaming is much higher for orthogonal rotators (approaching 100%) than for small inclination angles (less than 20%).The small variation on the synthetic skymaps generated for different pulsars indicates that the geometry dominates over timing and spectral properties in shaping the gamma-ray light curves. This means that geometrical parameters like the inclination angle can be in principle constrained by gamma-ray data alone independently on the specific properties of a pulsar. At the same time, we find that $\\gamma$-ray spectra seen by different observers can slightly differ, opening the door to constraining the viewing angle of a particular pulsar.","sentences":["This work presents a methodological approach to generate realistic $\\gamma$-ray light curves of pulsars, resembling reasonably well the observational ones observed by the Fermi-Large Area Telescope instrument, fitting at the same time their high-energy spectra.","The theoretical light curves are obtained from a spectral and geometrical model of the synchro-curvature emission.","Despite our model relies on a few effective physical parameters, the synthetic light curves present the same main features observed in the observational $\\gamma$-ray light curve zoo, such as the different shapes, variety in the number of peaks, and a diversity of peak widths.","The morphological features of the light curves allows us to statistically compare the observed properties.","In particular, we find that the proportion on the number of peaks found in our synthetic light curves is in agreement with the observational one provided by the third Fermi-LAT pulsar catalog.","We also found that the detection probability due to beaming is much higher for orthogonal rotators (approaching 100%) than for small inclination angles (less than 20%).The small variation on the synthetic skymaps generated for different pulsars indicates that the geometry dominates over timing and spectral properties in shaping the gamma-ray light curves.","This means that geometrical parameters like the inclination angle can be in principle constrained by gamma-ray data alone independently on the specific properties of a pulsar.","At the same time, we find that $\\gamma$-ray spectra seen by different observers can slightly differ, opening the door to constraining the viewing angle of a particular pulsar."],"url":"http://arxiv.org/abs/2404.01926v1","category":"astro-ph.HE"}
{"created":"2024-04-02 13:19:45","title":"Improving Bird's Eye View Semantic Segmentation by Task Decomposition","abstract":"Semantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at https://github.com/happytianhao/TaDe.","sentences":["Semantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving.","Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs.","However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize.","In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment.","In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns.","The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level.","Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively.","Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps.","Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead.","Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method.","Code is available at https://github.com/happytianhao/TaDe."],"url":"http://arxiv.org/abs/2404.01925v1","category":"cs.CV"}
{"created":"2024-04-02 13:19:06","title":"Toward Efficient Visual Gyroscopes: Spherical Moments, Harmonics Filtering, and Masking Techniques for Spherical Camera Applications","abstract":"Unlike a traditional gyroscope, a visual gyroscope estimates camera rotation through images. The integration of omnidirectional cameras, offering a larger field of view compared to traditional RGB cameras, has proven to yield more accurate and robust results. However, challenges arise in situations that lack features, have substantial noise causing significant errors, and where certain features in the images lack sufficient strength, leading to less precise prediction results.   Here, we address these challenges by introducing a novel visual gyroscope, which combines an analytical method with a neural network approach to provide a more efficient and accurate rotation estimation from spherical images. The presented method relies on three key contributions: an adapted analytical approach to compute the spherical moments coefficients, introduction of masks for better global feature representation, and the use of a multilayer perceptron to adaptively choose the best combination of masks and filters. Experimental results demonstrate superior performance of the proposed approach in terms of accuracy. The paper emphasizes the advantages of integrating machine learning to optimize analytical solutions, discusses limitations, and suggests directions for future research.","sentences":["Unlike a traditional gyroscope, a visual gyroscope estimates camera rotation through images.","The integration of omnidirectional cameras, offering a larger field of view compared to traditional RGB cameras, has proven to yield more accurate and robust results.","However, challenges arise in situations that lack features, have substantial noise causing significant errors, and where certain features in the images lack sufficient strength, leading to less precise prediction results.   ","Here, we address these challenges by introducing a novel visual gyroscope, which combines an analytical method with a neural network approach to provide a more efficient and accurate rotation estimation from spherical images.","The presented method relies on three key contributions: an adapted analytical approach to compute the spherical moments coefficients, introduction of masks for better global feature representation, and the use of a multilayer perceptron to adaptively choose the best combination of masks and filters.","Experimental results demonstrate superior performance of the proposed approach in terms of accuracy.","The paper emphasizes the advantages of integrating machine learning to optimize analytical solutions, discusses limitations, and suggests directions for future research."],"url":"http://arxiv.org/abs/2404.01924v1","category":"cs.CV"}
{"created":"2024-04-02 13:17:36","title":"SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation","abstract":"Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates \"skeleton heuristics\", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input. Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.","sentences":["Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB.","Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge.","With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge.","Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study.","In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG.","The framework incorporates \"skeleton heuristics\", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.","More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input.","Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions.","Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks."],"url":"http://arxiv.org/abs/2404.01923v1","category":"cs.CL"}
{"created":"2024-04-02 13:14:32","title":"Hyperviscosity stabilisation of the RBF-FD solution to natural convection","abstract":"The numerical stability of fluid flow is an important topic in computational fluid dynamics as fluid flow simulations usually become numerically unstable in the turbulent regime. Many mesh-based methods have already established numerical dissipation procedures that dampen the effects of the unstable advection term. When it comes to meshless methods, the prominent stabilisation scheme is hyperviscosity. It introduces numerical dissipation in the form of a higher-order Laplacian operator. Many papers have already discussed the general effects of hyperviscosity and its parameters. However, hyperviscosity in flow problems has not yet been analyzed in depth. In this paper, we discuss the effects of hyperviscosity on natural convection flow problems as we approach the turbulent regime.","sentences":["The numerical stability of fluid flow is an important topic in computational fluid dynamics as fluid flow simulations usually become numerically unstable in the turbulent regime.","Many mesh-based methods have already established numerical dissipation procedures that dampen the effects of the unstable advection term.","When it comes to meshless methods, the prominent stabilisation scheme is hyperviscosity.","It introduces numerical dissipation in the form of a higher-order Laplacian operator.","Many papers have already discussed the general effects of hyperviscosity and its parameters.","However, hyperviscosity in flow problems has not yet been analyzed in depth.","In this paper, we discuss the effects of hyperviscosity on natural convection flow problems as we approach the turbulent regime."],"url":"http://arxiv.org/abs/2404.01919v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 13:12:40","title":"Particle systems for mean reflected BSDEs with jumps","abstract":"In this paper, we study the mean reflected backward stochastic differential equations with jump (BSDEJs) where the generator only depends on $Y$. We extend the work of Briand and Hibon on the propagation of chaos for mean reflected BSDEs \\cite{briand2021particles} to the jump framework. Besides, we study the reflections for the particle system and obtain the rate of of convergence of the particle system towards the deterministic flat solution to the mean reflected BSDEJ.","sentences":["In this paper, we study the mean reflected backward stochastic differential equations with jump (BSDEJs) where the generator only depends on $Y$. We extend the work of Briand and Hibon on the propagation of chaos for mean reflected BSDEs \\cite{briand2021particles} to the jump framework.","Besides, we study the reflections for the particle system and obtain the rate of of convergence of the particle system towards the deterministic flat solution to the mean reflected BSDEJ."],"url":"http://arxiv.org/abs/2404.01916v1","category":"math.PR"}
{"created":"2024-04-02 13:05:41","title":"SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities","abstract":"Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties. Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks. Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks.","sentences":["Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER).","A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations.","To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants.","SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources.","We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities.","Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties.","Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks.","Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks."],"url":"http://arxiv.org/abs/2404.01914v1","category":"cs.CL"}
{"created":"2024-04-02 13:00:19","title":"Why is the universe not frozen by the quantum Zeno effect?","abstract":"We build a discrete model that simulates the ubiquitous competition between the free internal evolution of a two-level system and the decoherence induced by the interaction with its surrounding environment. It is aimed at being as universal as possible, so that no specific Hamiltonian is assumed. This leads to an analytic criterion, depending on the level of short time decoherence, allowing to determine whether the system will freeze due to the Zeno effect. We check this criterion on several classes of functions which correspond to different physical situations. In the most generic case, the free evolution wins over decoherence, thereby explaining why the universe is indeed not frozen.","sentences":["We build a discrete model that simulates the ubiquitous competition between the free internal evolution of a two-level system and the decoherence induced by the interaction with its surrounding environment.","It is aimed at being as universal as possible, so that no specific Hamiltonian is assumed.","This leads to an analytic criterion, depending on the level of short time decoherence, allowing to determine whether the system will freeze due to the Zeno effect.","We check this criterion on several classes of functions which correspond to different physical situations.","In the most generic case, the free evolution wins over decoherence, thereby explaining why the universe is indeed not frozen."],"url":"http://arxiv.org/abs/2404.01913v1","category":"quant-ph"}
{"created":"2024-04-02 12:57:22","title":"VLRM: Vision-Language Models act as Reward Models for Image Captioning","abstract":"In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to generate longer and more comprehensive descriptions. Our model reaches impressive 0.90 R@1 CLIP Recall score on MS-COCO Carpathy Test Split.   Weights are available at https://huggingface.co/sashakunitsyn/vlrm-blip2-opt-2.7b.","sentences":["In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models.","The RL-tuned model is able to generate longer and more comprehensive descriptions.","Our model reaches impressive 0.90 R@1 CLIP Recall score on MS-COCO Carpathy Test Split.   ","Weights are available at https://huggingface.co/sashakunitsyn/vlrm-blip2-opt-2.7b."],"url":"http://arxiv.org/abs/2404.01911v1","category":"cs.CV"}
{"created":"2024-04-02 12:49:22","title":"Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack","abstract":"With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model's robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.","sentences":["With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism.","While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing.","In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection.","We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks.","The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content.","Furthermore, we explore the prospect of improving the model's robustness over iterative adversarial learning.","Although some improvements in model robustness are observed, practical applications still face significant challenges.","These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods."],"url":"http://arxiv.org/abs/2404.01907v1","category":"cs.CL"}
{"created":"2024-04-02 12:40:02","title":"Learning-based model augmentation with LFRs","abstract":"Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems. To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods. These methods have shown better estimation speeds and/or accuracy on unseen data. Among these methods are model augmentation structures. A variety of these structures have been considered in literature, there is however no unifying theory to these. In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure. This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure. Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure. The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example.","sentences":["Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems.","To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods.","These methods have shown better estimation speeds and/or accuracy on unseen data.","Among these methods are model augmentation structures.","A variety of these structures have been considered in literature, there is however no unifying theory to these.","In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure.","This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure.","Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure.","The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example."],"url":"http://arxiv.org/abs/2404.01901v1","category":"eess.SY"}
{"created":"2024-04-02 12:38:15","title":"Non-ultralocal classical r-matrix structure for 1+1 field analogue of elliptic Calogero-Moser model","abstract":"We consider 1+1 field generalization of the elliptic Calogero-Moser model. It is shown that the Lax connection satisfies the classical non-ultralocal $r$-matrix structure of Maillet type. Next, we consider 1+1 field analogue of the spin Calogero-Moser model and its multipole (or multispin) extension. Finally, we discuss the field analogue of the classical IRF-Vertex correspondence, which relates utralocal and non-ultralocal $r$-matrix structures.","sentences":["We consider 1+1 field generalization of the elliptic Calogero-Moser model.","It is shown that the Lax connection satisfies the classical non-ultralocal $r$-matrix structure of Maillet type.","Next, we consider 1+1 field analogue of the spin Calogero-Moser model and its multipole (or multispin) extension.","Finally, we discuss the field analogue of the classical IRF-Vertex correspondence, which relates utralocal and non-ultralocal $r$-matrix structures."],"url":"http://arxiv.org/abs/2404.01898v1","category":"hep-th"}
{"created":"2024-04-02 12:36:40","title":"Continuous Spiking Graph Neural Networks","abstract":"Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.","sentences":["Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics.","They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE).","However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices.","Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN).","We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time.","To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation.","Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes.","Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines."],"url":"http://arxiv.org/abs/2404.01897v1","category":"cs.NE"}
{"created":"2024-04-02 12:34:12","title":"$q$-variational H{\u00f6}rmander functional calculus and Schr{\u00f6}dinger and wave maximal estimates","abstract":"This article is the continuation of the work [DK] where we had proved maximal estimates $$\\left\\|\\sup_{t > 0} |m(tA)f| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f\\|_{L^p(\\Omega,Y)}$$ for sectorial operators $A$ acting on $L^p(\\Omega,Y)$ ($Y$ being a UMD lattice) and admitting a H\\\"ormander functional calculus(a strengthening of the holomorphic $H^\\infty$ calculus to symbols $m$ differentiable on $(0,\\infty)$ in a quantified manner), and $m : (0, \\infty) \\to \\mathbb{C}$ being a H\\\"ormander class symbol with certain decay at $\\infty$.In the present article, we show that under the same conditions as above, the scalar function $t \\mapsto m(tA)f(x,\\omega)$ is of finite $q$-variation with $q > 2$, a.e. $(x,\\omega)$.This extends recent works by [BMSW,HHL,HoMa1,HoMa,JSW,LMX] who have considered among others $m(tA) = e^{-tA}$ the semigroup generated by $-A$.As a consequence, we extend estimates for spherical means in euclidean space from [JSW] to the case of UMD lattice-valued spaces.A second main result yields a maximal estimate $$\\left\\|\\sup_{t > 0} |m(tA) f_t| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f_t\\|_{L^p(\\Omega,Y(\\Lambda^\\beta))}$$ for the same $A$ and similar conditions on $m$ as above but with $f_t$ depending itself on $t$ such that $t \\mapsto f_t(x,\\omega)$ belongs to a Sobolev space $\\Lambda^\\beta$ over $(\\mathbb{R}_+, \\frac{dt}{t})$.We apply this to show a maximal estimate of the Schr\\\"odinger (case $A = -\\Delta$) or wave (case $A = \\sqrt{-\\Delta}$) solution propagator $t \\mapsto \\exp(itA)f$.Then we deduce from it variants of Carleson's problem of pointwise convergence [Car]\\[ \\exp(itA)f(x,\\omega) \\to f(x,\\omega) \\text{ a. e. }(x,\\omega) \\quad (t \\to 0+)\\]for $A$ a Fourier multiplier operator or a differential operator on an open domain $\\Omega \\subseteq \\mathbb{R}^d$ with boundary conditions.","sentences":["This article is the continuation of the work","[DK] where we had proved maximal estimates $$\\left\\|\\sup_{t > 0} |m(tA)f| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f\\|_{L^p(\\Omega,Y)}$$ for sectorial operators $A$ acting on $L^p(\\Omega,Y)$ ($Y$ being a UMD lattice) and admitting a H\\\"ormander functional calculus(a strengthening of the holomorphic $H^\\infty$ calculus to symbols $m$ differentiable on $(0,\\infty)$ in a quantified manner), and $m : (0, \\infty) \\to \\mathbb{C}$ being a H\\\"ormander class symbol with certain decay at $\\infty$.In the present article, we show that under the same conditions as above, the scalar function $t \\mapsto m(tA)f(x,\\omega)$ is of finite $q$-variation with $q > 2$, a.e. $(x,\\omega)$.This extends recent works by [BMSW,HHL,HoMa1,HoMa,JSW,LMX] who have considered among others $m(tA)","= e^{-tA}$ the semigroup generated by $-A$.As a consequence, we extend estimates for spherical means in euclidean space from [JSW] to the case of UMD lattice-valued spaces.","A second main result yields a maximal estimate $$\\left\\|\\sup_{t > 0} |m(tA) f_t| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f_t\\|_{L^p(\\Omega,Y(\\Lambda^\\beta))}$$ for the same $A$ and similar conditions on $m$ as above but with $f_t$ depending itself on $t$ such that $t \\mapsto f_t(x,\\omega)$ belongs to a Sobolev space $\\Lambda^\\beta$ over $(\\mathbb{R}_+, \\frac{dt}{t})$.We apply this to show a maximal estimate of the Schr\\\"odinger (case $A = -\\Delta$) or wave (case $A = \\sqrt{-\\Delta}$) solution propagator $t \\mapsto \\exp(itA)f$.Then we deduce from it variants of Carleson's problem of pointwise convergence [Car]\\[ \\exp(itA)f(x,\\omega) \\to f(x,\\omega) \\text{ a. e. }(x,\\omega) \\quad (t \\to 0+)\\]for $A$ a Fourier multiplier operator or a differential operator on an open domain $\\Omega \\subseteq \\mathbb{R}^d$ with boundary conditions."],"url":"http://arxiv.org/abs/2404.01893v1","category":"math.CA"}
{"created":"2024-04-02 12:28:40","title":"RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement","abstract":"In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP embeddings of backlit and well-lit images from training data, we compute the residual vector in the embedding space as a simple difference between the mean embeddings of the well-lit and backlit images. This vector then guides the enhancement network during training, pushing a backlit image towards the space of well-lit images. This approach further dramatically reduces training time, stabilizes training and produces high quality enhanced images without artifacts, both in supervised and unsupervised training regimes. Additionally, we show that residual vectors can be interpreted, revealing biases in training data, and thereby enabling potential bias correction.","sentences":["In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement.","Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space.","Learned prompts then guide an image enhancement network.","Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance.","First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality.","This accelerates training and potentially enables the use of additional encoders that do not have a text encoder.","Second, we propose a novel approach that does not require any prompt tuning.","Instead, based on CLIP embeddings of backlit and well-lit images from training data, we compute the residual vector in the embedding space as a simple difference between the mean embeddings of the well-lit and backlit images.","This vector then guides the enhancement network during training, pushing a backlit image towards the space of well-lit images.","This approach further dramatically reduces training time, stabilizes training and produces high quality enhanced images without artifacts, both in supervised and unsupervised training regimes.","Additionally, we show that residual vectors can be interpreted, revealing biases in training data, and thereby enabling potential bias correction."],"url":"http://arxiv.org/abs/2404.01889v1","category":"cs.CV"}
{"created":"2024-04-02 12:26:17","title":"3D Scene Generation from Scene Graphs and Self-Attention","abstract":"Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans. We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph. Our experiments shows self-attention layers leads to sparser (HOW MUCH) and more diverse scenes (HOW MUCH)\\. Included in this work, we publish the first large-scale dataset for conditioned scene generation from scene graphs, containing over XXX rooms (of floor plans and scene graphs).","sentences":["Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality.","As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout.","We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans.","We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model.","Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph.","Our experiments shows self-attention layers leads to sparser (HOW MUCH) and more diverse scenes (HOW MUCH)\\. Included in this work, we publish the first large-scale dataset for conditioned scene generation from scene graphs, containing over XXX rooms (of floor plans and scene graphs)."],"url":"http://arxiv.org/abs/2404.01887v1","category":"cs.CV"}
{"created":"2024-04-02 12:16:35","title":"Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles","abstract":"Amorphous silicon is a highly promising anode material for next-generation lithium-ion batteries. Large volume changes of the silicon particle have a critical effect on the surrounding solid-electrolyte interphase (SEI) due to repeated fracture and healing during cycling. Based on a thermodynamically consistent chemo-elasto-plastic continuum model we investigate the stress development inside the particle and the SEI. Using the example of a particle with SEI, we apply a higher order finite element method together with a variable-step, variable-order time integration scheme on a nonlinear system of partial differential equations. Starting from a single silicon particle setting, the surrounding SEI is added in a first step with the typically used elastic Green--St-Venant (GSV) strain definition for a purely elastic deformation. For this type of deformation, the definition of the elastic strain is crucial to get reasonable simulation results. In case of the elastic GSV strain, the simulation aborts. We overcome the simulation failure by using the definition of the logarithmic Hencky strain. However, the particle remains unaffected by the elastic strain definitions in the particle domain. Compared to GSV, plastic deformation with the Hencky strain is straightforward to take into account. For the plastic SEI deformation, a rate-independent and a rate-dependent plastic deformation are newly introduced and numerically compared for three half cycles for the example of a radial symmetric particle.","sentences":["Amorphous silicon is a highly promising anode material for next-generation lithium-ion batteries.","Large volume changes of the silicon particle have a critical effect on the surrounding solid-electrolyte interphase (SEI) due to repeated fracture and healing during cycling.","Based on a thermodynamically consistent chemo-elasto-plastic continuum model we investigate the stress development inside the particle and the SEI.","Using the example of a particle with SEI, we apply a higher order finite element method together with a variable-step, variable-order time integration scheme on a nonlinear system of partial differential equations.","Starting from a single silicon particle setting, the surrounding SEI is added in a first step with the typically used elastic Green--St-Venant (GSV) strain definition for a purely elastic deformation.","For this type of deformation, the definition of the elastic strain is crucial to get reasonable simulation results.","In case of the elastic GSV strain, the simulation aborts.","We overcome the simulation failure by using the definition of the logarithmic Hencky strain.","However, the particle remains unaffected by the elastic strain definitions in the particle domain.","Compared to GSV, plastic deformation with the Hencky strain is straightforward to take into account.","For the plastic SEI deformation, a rate-independent and a rate-dependent plastic deformation are newly introduced and numerically compared for three half cycles for the example of a radial symmetric particle."],"url":"http://arxiv.org/abs/2404.01884v1","category":"math.NA"}
{"created":"2024-04-02 12:13:56","title":"Consistent treatment of quantum systems with a time-dependent Hilbert space","abstract":"We consider some basic problems associated with quantum mechanics of systems having a time-dependent Hilbert space. We provide a consistent treatment of these systems and address the possibility of describing them in terms of a time-independent Hilbert space. We show that in general the Hamiltonian operator does not represent an observable of the system even if it is a self-adjoint operator. This is related to a hidden geometric aspect of quantum mechanics arising from the presence of an operator-valued gauge potential. We also offer a careful treatment of quantum systems whose Hilbert space is obtained by endowing a time-independent vector space with a time-dependent inner product.","sentences":["We consider some basic problems associated with quantum mechanics of systems having a time-dependent Hilbert space.","We provide a consistent treatment of these systems and address the possibility of describing them in terms of a time-independent Hilbert space.","We show that in general the Hamiltonian operator does not represent an observable of the system even if it is a self-adjoint operator.","This is related to a hidden geometric aspect of quantum mechanics arising from the presence of an operator-valued gauge potential.","We also offer a careful treatment of quantum systems whose Hilbert space is obtained by endowing a time-independent vector space with a time-dependent inner product."],"url":"http://arxiv.org/abs/2404.01881v1","category":"quant-ph"}
{"created":"2024-04-02 12:13:40","title":"Making two particle detectors in flat spacetime communicate quantumly","abstract":"A communication protocol with non-zero quantum capacity is found when the two communicating parts are particle detector models in (3+1)-dimensional spacetime. In particular, as detectors, we consider two harmonic oscillators interacting with a scalar field, whose evolution is generalized for whatever background spacetime and whatever spacetime smearing of the detectors. We then specialize to Minkowski spacetime and an initial Minkowski vacuum, considering a rapid interaction between the field and the two detectors, studying the case where the receiver is static and the sender is moving. The possibility to have a quantum capacity greater than zero stems from a relative acceleration between the detectors. Indeed, no reliable quantum communication is possible when the two detectors are static or moving inertially with respect to each other, but a reliable quantum communication can be achieved between a uniformly accelerated sender and an inertial receiver.","sentences":["A communication protocol with non-zero quantum capacity is found when the two communicating parts are particle detector models in (3+1)-dimensional spacetime.","In particular, as detectors, we consider two harmonic oscillators interacting with a scalar field, whose evolution is generalized for whatever background spacetime and whatever spacetime smearing of the detectors.","We then specialize to Minkowski spacetime and an initial Minkowski vacuum, considering a rapid interaction between the field and the two detectors, studying the case where the receiver is static and the sender is moving.","The possibility to have a quantum capacity greater than zero stems from a relative acceleration between the detectors.","Indeed, no reliable quantum communication is possible when the two detectors are static or moving inertially with respect to each other, but a reliable quantum communication can be achieved between a uniformly accelerated sender and an inertial receiver."],"url":"http://arxiv.org/abs/2404.01880v1","category":"gr-qc"}
{"created":"2024-04-02 12:10:03","title":"Monodromy of generalized Lame equations with Darboux-Treibich-Verdier potentials: A universal law","abstract":"The Darboux-Treibich-Verdier (DTV) potential $\\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)$ is well-known as doubly-periodic solutions of the stationary KdV hierarchy (Treibich-Verdier, Duke Math. J. {\\bf 68} (1992), 217-236). In this paper, we study the generalized Lam\\'{e} equation with the DTV potential \\begin{equation*} y^{\\prime \\prime }(z)=\\bigg[ \\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)+B\\bigg] y(z),\\quad n_{k}\\in \\mathbb{N} \\end{equation*} from the monodromy aspect. We prove that the map from $(\\tau, B)$ to the monodromy data $(r,s)$ satisfies a surprising universal law $d\\tau\\wedge dB\\equiv8\\pi^2 dr\\wedge ds.$ Our proof applies Panlev\\'{e} VI equation and modular forms. We also give applications to the algebraic multiplicity of (anti)periodic eigenvalues for the associated Hill operator.","sentences":["The Darboux-Treibich-Verdier (DTV) potential $\\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)$ is well-known as doubly-periodic solutions of the stationary KdV hierarchy (Treibich-Verdier, Duke Math.","J. {\\bf 68} (1992), 217-236).","In this paper, we study the generalized Lam\\'{e} equation with the DTV potential \\begin{equation*} y^{\\prime \\prime }(z)=\\bigg[ \\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)+B\\bigg] y(z),\\quad n_{k}\\in \\mathbb{N} \\end{equation*} from the monodromy aspect.","We prove that the map from $(\\tau, B)$ to the monodromy data $(r,s)$ satisfies a surprising universal law $d\\tau\\wedge dB\\equiv8\\pi^2 dr\\wedge ds.$ Our proof applies Panlev\\'{e} VI equation and modular forms.","We also give applications to the algebraic multiplicity of (anti)periodic eigenvalues for the associated Hill operator."],"url":"http://arxiv.org/abs/2404.01879v1","category":"math.CA"}
{"created":"2024-04-02 12:08:26","title":"Real, fake and synthetic faces - does the coin have three sides?","abstract":"With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage. To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images. The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images. Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image. ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes. From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively. This observation was supported by further analysis of various image properties. We saw noticeable differences across the three category of images. This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes.","sentences":["With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage.","To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images.","The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images.","Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image.","ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes.","From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively.","This observation was supported by further analysis of various image properties.","We saw noticeable differences across the three category of images.","This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes."],"url":"http://arxiv.org/abs/2404.01878v1","category":"cs.CV"}
{"created":"2024-04-02 12:05:02","title":"Procedural Fairness in Machine Learning","abstract":"Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedural unfairness of the model and propose two methods to improve procedural fairness after identifying unfair features. Our experimental results demonstrate that we can accurately identify the features that lead to procedural unfairness in the ML model, and both of our proposed methods can significantly improve procedural fairness with a slight impact on model performance, while also improving distributive fairness.","sentences":["Fairness in machine learning (ML) has received much attention.","However, existing studies have mainly focused on the distributive fairness of ML models.","The other dimension of fairness, i.e., procedural fairness, has been neglected.","In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness.","We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models.","We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets.","Our experiments reveal the relationship between procedural and distributive fairness of the ML model.","Based on our analysis, we propose a method for identifying the features that lead to the procedural unfairness of the model and propose two methods to improve procedural fairness after identifying unfair features.","Our experimental results demonstrate that we can accurately identify the features that lead to procedural unfairness in the ML model, and both of our proposed methods can significantly improve procedural fairness with a slight impact on model performance, while also improving distributive fairness."],"url":"http://arxiv.org/abs/2404.01877v1","category":"cs.LG"}
{"created":"2024-04-02 11:59:58","title":"Satellite Federated Edge Learning: Architecture Design and Convergence Analysis","abstract":"The proliferation of low-earth-orbit (LEO) satellite networks leads to the generation of vast volumes of remote sensing data which is traditionally transferred to the ground server for centralized processing, raising privacy and bandwidth concerns. Federated edge learning (FEEL), as a distributed machine learning approach, has the potential to address these challenges by sharing only model parameters instead of raw data. Although promising, the dynamics of LEO networks, characterized by the high mobility of satellites and short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL. Notably, frequent model transmission between the satellites and ground incurs prolonged waiting time and large transmission latency. This paper introduces a novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation networks. By integrating inter-satellite links (ISL) for intra-orbit model aggregation, the proposed algorithm significantly reduces the usage of low data rate and intermittent GSL. Our proposed method includes a ring all-reduce based intra-orbit aggregation mechanism, coupled with a network flow-based transmission scheme for global model aggregation, which enhances transmission efficiency. Theoretical convergence analysis is provided to characterize the algorithm performance. Extensive simulations show that our FEDMEGA algorithm outperforms existing satellite FEEL algorithms, exhibiting an approximate 30% improvement in convergence rate.","sentences":["The proliferation of low-earth-orbit (LEO) satellite networks leads to the generation of vast volumes of remote sensing data which is traditionally transferred to the ground server for centralized processing, raising privacy and bandwidth concerns.","Federated edge learning (FEEL), as a distributed machine learning approach, has the potential to address these challenges by sharing only model parameters instead of raw data.","Although promising, the dynamics of LEO networks, characterized by the high mobility of satellites and short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL.","Notably, frequent model transmission between the satellites and ground incurs prolonged waiting time and large transmission latency.","This paper introduces a novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation networks.","By integrating inter-satellite links (ISL) for intra-orbit model aggregation, the proposed algorithm significantly reduces the usage of low data rate and intermittent GSL.","Our proposed method includes a ring all-reduce based intra-orbit aggregation mechanism, coupled with a network flow-based transmission scheme for global model aggregation, which enhances transmission efficiency.","Theoretical convergence analysis is provided to characterize the algorithm performance.","Extensive simulations show that our FEDMEGA algorithm outperforms existing satellite FEEL algorithms, exhibiting an approximate 30% improvement in convergence rate."],"url":"http://arxiv.org/abs/2404.01875v1","category":"eess.SP"}
{"created":"2024-04-02 11:46:31","title":"Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey","abstract":"Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on genuine reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.","sentences":["Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans.","However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain.","This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior.","This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes.","Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses.","Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on genuine reasoning abilities.","Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning.","Through this survey, we aim to shed light on the complex reasoning processes within LLMs."],"url":"http://arxiv.org/abs/2404.01869v1","category":"cs.CL"}
{"created":"2024-04-02 11:44:08","title":"Attosecond gamma-ray flashes and electron-positron pairs in dyadic laser interaction with micro-wire","abstract":"Beams of photons and charged particles of high energy and high brightness find application across a range of disciplines in both fundamental and applied sciences. The interaction of an ultra-intense laser with matter has been shown to be an efficient source of high-energy particles, but typical schemes generate spatially broad distributions with high-divergence and low-brightness. In this paper we report on emission of highly collimated, ultrabright, attosecond $\\gamma$-photons and generation of dense electron-positron pairs via a tunable particle generation scheme which utilises the interaction of two high-power lasers with metallic micro-wire target. Irradiating the target with a radially polarised laser pulse first produces a series of high charge, short duration, electron bunches with low transverse momentum. These electron bunches subsequently collide with a counter-propagating high intensity laser. Depending on the intensity of the counter-propagating laser, the scheme can be used to generate either highly collimated ultra-bright MeV-GeV $\\gamma$-beams or electron-positron bunches with densities approaching the solid density level.","sentences":["Beams of photons and charged particles of high energy and high brightness find application across a range of disciplines in both fundamental and applied sciences.","The interaction of an ultra-intense laser with matter has been shown to be an efficient source of high-energy particles, but typical schemes generate spatially broad distributions with high-divergence and low-brightness.","In this paper we report on emission of highly collimated, ultrabright, attosecond $\\gamma$-photons and generation of dense electron-positron pairs via a tunable particle generation scheme which utilises the interaction of two high-power lasers with metallic micro-wire target.","Irradiating the target with a radially polarised laser pulse first produces a series of high charge, short duration, electron bunches with low transverse momentum.","These electron bunches subsequently collide with a counter-propagating high intensity laser.","Depending on the intensity of the counter-propagating laser, the scheme can be used to generate either highly collimated ultra-bright MeV-GeV $\\gamma$-beams or electron-positron bunches with densities approaching the solid density level."],"url":"http://arxiv.org/abs/2404.01865v1","category":"physics.plasm-ph"}
{"created":"2024-04-02 11:41:22","title":"Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems","abstract":"Generating simulated training data needed for constructing sufficiently accurate surrogate models to be used for efficient optimization or parameter identification can incur a huge computational effort in the offline phase. We consider a fully adaptive greedy approach to the computational design of experiments problem using gradient-enhanced Gaussian process regression as surrogates. Designs are incrementally defined by solving an optimization problem for accuracy given a certain computational budget. We address not only the choice of evaluation points but also of required simulation accuracy, both of values and gradients of the forward model. Numerical results show a significant reduction of the computational effort compared to just position-adaptive and static designs as well as a clear benefit of including gradient information into the surrogate training.","sentences":["Generating simulated training data needed for constructing sufficiently accurate surrogate models to be used for efficient optimization or parameter identification can incur a huge computational effort in the offline phase.","We consider a fully adaptive greedy approach to the computational design of experiments problem using gradient-enhanced Gaussian process regression as surrogates.","Designs are incrementally defined by solving an optimization problem for accuracy given a certain computational budget.","We address not only the choice of evaluation points but also of required simulation accuracy, both of values and gradients of the forward model.","Numerical results show a significant reduction of the computational effort compared to just position-adaptive and static designs as well as a clear benefit of including gradient information into the surrogate training."],"url":"http://arxiv.org/abs/2404.01864v1","category":"math.NA"}
{"created":"2024-04-02 11:40:38","title":"Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models","abstract":"Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts. We demonstrate that incorporating the confidence-calibrated rewards in fine-tuning effectively reduces overoptimization, resulting in twice as many wins in human evaluation for text-image alignment compared against the baseline reward models.","sentences":["Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent.","However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization.","To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations.","Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment.","We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective.","To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts.","We demonstrate that incorporating the confidence-calibrated rewards in fine-tuning effectively reduces overoptimization, resulting in twice as many wins in human evaluation for text-image alignment compared against the baseline reward models."],"url":"http://arxiv.org/abs/2404.01863v1","category":"cs.LG"}
{"created":"2024-04-02 11:40:34","title":"Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model","abstract":"Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.","sentences":["Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction.","While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work.","There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information.","2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length.","To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos.","Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information.","Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos.","For better visual perception, we further design a refinement network focusing on missing details of certain areas.","Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations.","Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion."],"url":"http://arxiv.org/abs/2404.01862v1","category":"cs.CV"}
{"created":"2024-04-02 11:34:12","title":"Poro 34B and the Blessing of Multilinguality","abstract":"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B.","sentences":["The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages.","While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages.","We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training.","In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages.","We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B."],"url":"http://arxiv.org/abs/2404.01856v1","category":"cs.CL"}
{"created":"2024-04-02 11:33:04","title":"Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation","abstract":"Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.","sentences":["Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment.","Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources.","Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios.","However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted.","Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions.","Hence, they cannot effectively solve the next POI recommendation task.","To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in.","Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem.","Through extensive experiments on two widely used real-world datasets, we derive several key findings.","Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions.","We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms."],"url":"http://arxiv.org/abs/2404.01855v1","category":"cs.IR"}
{"created":"2024-04-02 11:27:30","title":"Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks","abstract":"The increasing demand for wireless networks of higher capacity requires key-enabling technologies. Optical wireless communication (OWC) arises as a complementary technology to radio frequency (RF) systems that can support high aggregate data rates. However, OWC systems face some challenges including beam-blockage. Intelligent reflecting surfaces (IRSs) can offer alternative pathways for the optical signal, ensuring continuous connectivity. In this work, we investigate the potential of using IRS in an indoor OWC network. In particular, we define a system model of indoor OWC that employs IRS in conjunction with angle diversity transmitters (ADT) using vertical-cavity surface-emitting laser (VCSEL) arrays. The VCSEL beam is narrow, directed, and easy to block, however, it can deliver high data rates under eye safety regulations. Simulation results show that the deployment of IRS can significantly improve the achievable data rates of Laser-based OWC systems.","sentences":["The increasing demand for wireless networks of higher capacity requires key-enabling technologies.","Optical wireless communication (OWC) arises as a complementary technology to radio frequency (RF) systems that can support high aggregate data rates.","However, OWC systems face some challenges including beam-blockage.","Intelligent reflecting surfaces (IRSs) can offer alternative pathways for the optical signal, ensuring continuous connectivity.","In this work, we investigate the potential of using IRS in an indoor OWC network.","In particular, we define a system model of indoor OWC that employs IRS in conjunction with angle diversity transmitters (ADT) using vertical-cavity surface-emitting laser (VCSEL) arrays.","The VCSEL beam is narrow, directed, and easy to block, however, it can deliver high data rates under eye safety regulations.","Simulation results show that the deployment of IRS can significantly improve the achievable data rates of Laser-based OWC systems."],"url":"http://arxiv.org/abs/2404.01850v1","category":"eess.SP"}
{"created":"2024-04-02 11:22:53","title":"EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking","abstract":"As electric vehicle (EV) numbers rise, concerns about the capacity of current charging and power grid infrastructure grow, necessitating the development of smart charging solutions. While many smart charging simulators have been developed in recent years, only a few support the development of Reinforcement Learning (RL) algorithms in the form of a Gym environment, and those that do usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the aforementioned issues, this paper introduces the EV2Gym, a realistic simulator platform for the development and assessment of small and large-scale smart charging algorithms within a standardized platform. The proposed simulator is populated with comprehensive EV, charging station, power transformer, and EV behavior models validated using real data. EV2Gym has a highly customizable interface empowering users to choose from pre-designed case studies or craft their own customized scenarios to suit their specific requirements. Moreover, it incorporates a diverse array of RL, mathematical programming, and heuristic algorithms to speed up the development and benchmarking of new solutions. By offering a unified and standardized platform, EV2Gym aims to provide researchers and practitioners with a robust environment for advancing and assessing smart charging algorithms.","sentences":["As electric vehicle (EV) numbers rise, concerns about the capacity of current charging and power grid infrastructure grow, necessitating the development of smart charging solutions.","While many smart charging simulators have been developed in recent years, only a few support the development of Reinforcement Learning (RL) algorithms in the form of a Gym environment, and those that do usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios.","To address the aforementioned issues, this paper introduces the EV2Gym, a realistic simulator platform for the development and assessment of small and large-scale smart charging algorithms within a standardized platform.","The proposed simulator is populated with comprehensive EV, charging station, power transformer, and EV behavior models validated using real data.","EV2Gym has a highly customizable interface empowering users to choose from pre-designed case studies or craft their own customized scenarios to suit their specific requirements.","Moreover, it incorporates a diverse array of RL, mathematical programming, and heuristic algorithms to speed up the development and benchmarking of new solutions.","By offering a unified and standardized platform, EV2Gym aims to provide researchers and practitioners with a robust environment for advancing and assessing smart charging algorithms."],"url":"http://arxiv.org/abs/2404.01849v1","category":"cs.SE"}
{"created":"2024-04-02 11:08:31","title":"Testing Yukawa cosmology at the Milky Way and M31 galactic scales","abstract":"We address the galaxy rotation curves through the Yukawa gravitational potential emerging as a correction of the Newtonian potential in extended theories of gravity. On the one hand, we consider the contribution of the galactic bulge, galactic disk, and the dark matter halo of the Navarro-Frenk-White profile, in the framework of the standard $\\Lambda$CDM model. On the other hand, we use modified Yukawa gravity to show that the rotational velocity of galaxies can be addressed successfully without the need for dark matter. In Yukawa gravity, we recover MOND and show that dark matter might be seen as an apparent effect due to the modification of the law of gravitation in terms of two parameters: the coupling constant $\\alpha $ and the characteristic length $\\lambda$. We thus test our theoretical scenario using the Milky Way and M31 rotation velocity curves. In particular, we place observational constraints on the free parameters of Yukawa cosmology through the Monte Carlo method and then compare our results with the predictions of the $\\Lambda$CDM paradigm by making use of Bayesian information criteria. Specifically, we find that $\\lambda$ is constrained to be of the order of kpc, while cosmological data suggest $\\lambda$ of the order of Gpc. To explain this discrepancy, we argue that there is a fundamental limitation in measuring $\\lambda$ due to the role of quantum mechanics on cosmological scales.","sentences":["We address the galaxy rotation curves through the Yukawa gravitational potential emerging as a correction of the Newtonian potential in extended theories of gravity.","On the one hand, we consider the contribution of the galactic bulge, galactic disk, and the dark matter halo of the Navarro-Frenk-White profile, in the framework of the standard $\\Lambda$CDM model.","On the other hand, we use modified Yukawa gravity to show that the rotational velocity of galaxies can be addressed successfully without the need for dark matter.","In Yukawa gravity, we recover MOND and show that dark matter might be seen as an apparent effect due to the modification of the law of gravitation in terms of two parameters: the coupling constant $\\alpha $ and the characteristic length $\\lambda$. We thus test our theoretical scenario using the Milky Way and M31 rotation velocity curves.","In particular, we place observational constraints on the free parameters of Yukawa cosmology through the Monte Carlo method and then compare our results with the predictions of the $\\Lambda$CDM paradigm by making use of Bayesian information criteria.","Specifically, we find that $\\lambda$ is constrained to be of the order of kpc, while cosmological data suggest $\\lambda$ of the order of Gpc.","To explain this discrepancy, we argue that there is a fundamental limitation in measuring $\\lambda$ due to the role of quantum mechanics on cosmological scales."],"url":"http://arxiv.org/abs/2404.01846v1","category":"astro-ph.CO"}
{"created":"2024-04-02 11:04:08","title":"Generalized Calogero-Moser system and supergroup gauge origami","abstract":"We study the integrability and the Bethe/Gauge correspondence of the Generalized Calogero-Moser system proposed by Berntson, Langmann and Lenells which we call the elliptic quadruple Calogero-Moser system (eqCM). We write down the Dunkl operators which give commuting Hamiltonians of the quantum integrable system. We identify the gauge theory in correspondence is a supergroup version of the gauge origami, from which we construct the transfer matrix of the eqCM system.","sentences":["We study the integrability and the Bethe/Gauge correspondence of the Generalized Calogero-Moser system proposed by Berntson, Langmann and Lenells which we call the elliptic quadruple Calogero-Moser system (eqCM).","We write down the Dunkl operators which give commuting Hamiltonians of the quantum integrable system.","We identify the gauge theory in correspondence is a supergroup version of the gauge origami, from which we construct the transfer matrix of the eqCM system."],"url":"http://arxiv.org/abs/2404.01844v1","category":"hep-th"}
{"created":"2024-04-02 11:03:24","title":"Sketch3D: Style-Consistent Guidance for Sketch-to-3D Generation","abstract":"Recently, image-to-3D approaches have achieved significant results with a natural image as input. However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available. Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content. To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description. Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process. Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians. Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss. Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input.","sentences":["Recently, image-to-3D approaches have achieved significant results with a natural image as input.","However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available.","Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content.","To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description.","Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process.","Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians.","Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss.","Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input."],"url":"http://arxiv.org/abs/2404.01843v1","category":"cs.CV"}
{"created":"2024-04-02 10:59:17","title":"A zonogon approach for computing small polygons of maximum perimeter","abstract":"We derive a mixed integer nonlinear programming formulation for the problem of finding a convex polygon with a given number of vertices that is small (diameter at most one) and has maximum perimeter. The formulation is based on a geometric construction using zonogons. The resulting zonogons can be characterized by an integer code and we study the number of codes that are distinct under the symmetries of the problem. We propose a two-phase computational approach. Phase I comprises the solution of a purely combinatorial problem. Under assumption of Mossinghoff's conjecture, the Phase I problem can be reduced to a Subset-Sum Problem. Without Mossinghoff's conjecture, a generalized Subset-Sum Problem needs to be solved, which consists of picking $n$ non-opposing $2n$-th roots of unity such that their sum is as small as possible. Phase II consists of a non-combinatorial Nonlinear Programming Problem, which can be solved to high accuracy with arbitrary precision Newton-type methods. We provide extensive numerical results including highly accurate solutions for polygons with 64 and 128 vertices.","sentences":["We derive a mixed integer nonlinear programming formulation for the problem of finding a convex polygon with a given number of vertices that is small (diameter at most one) and has maximum perimeter.","The formulation is based on a geometric construction using zonogons.","The resulting zonogons can be characterized by an integer code and we study the number of codes that are distinct under the symmetries of the problem.","We propose a two-phase computational approach.","Phase I comprises the solution of a purely combinatorial problem.","Under assumption of Mossinghoff's conjecture, the Phase I problem can be reduced to a Subset-Sum Problem.","Without Mossinghoff's conjecture, a generalized Subset-Sum Problem needs to be solved, which consists of picking $n$ non-opposing $2n$-th roots of unity such that their sum is as small as possible.","Phase II consists of a non-combinatorial Nonlinear Programming Problem, which can be solved to high accuracy with arbitrary precision Newton-type methods.","We provide extensive numerical results including highly accurate solutions for polygons with 64 and 128 vertices."],"url":"http://arxiv.org/abs/2404.01841v1","category":"math.OC"}
{"created":"2024-04-02 10:59:01","title":"Particle Creation in a Linear Gravitational Wave Background","abstract":"Inspired by Leonard Parker's pioneering 1968 work demonstrating matter quanta production in a dynamical spacetime background, we consider production of scalar quanta in a gravitational wave background. Choosing the spacetime to be a flat spacetime perturbed linearly by a spherical plane gravitational wave, we show that scalar particles may indeed be produced in a perturbative manner. Our formulation is valid for any linear gravitational wave background profile, and should generalize straightforwardly to production of massless elementary particles of diverse species (photons, neutrinos). In this aspect, irrespective of the astrophysical nature of the binary merger sourcing the gravitational wave signal, one expects the dynamical nature of the spacetime to produce all species of light particles. Thus, any binary coalescence is in effect a source of multimessenger astrophysics.","sentences":["Inspired by Leonard Parker's pioneering 1968 work demonstrating matter quanta production in a dynamical spacetime background, we consider production of scalar quanta in a gravitational wave background.","Choosing the spacetime to be a flat spacetime perturbed linearly by a spherical plane gravitational wave, we show that scalar particles may indeed be produced in a perturbative manner.","Our formulation is valid for any linear gravitational wave background profile, and should generalize straightforwardly to production of massless elementary particles of diverse species (photons, neutrinos).","In this aspect, irrespective of the astrophysical nature of the binary merger sourcing the gravitational wave signal, one expects the dynamical nature of the spacetime to produce all species of light particles.","Thus, any binary coalescence is in effect a source of multimessenger astrophysics."],"url":"http://arxiv.org/abs/2404.01840v1","category":"gr-qc"}
{"created":"2024-04-02 10:54:43","title":"Fourier-Matsubara series expansion for imaginary-time correlation functions","abstract":"A Fourier-Matsubara series expansion is derived for imaginary-time correlation functions that constitutes the imaginary-time generalization of the infinite Matsubara series for equal-time correlation functions. The expansion is consistent with all known exact properties of imaginary-time correlation functions and opens up new avenues for the utilization of quantum Monte Carlo simulation data. Moreover, the expansion drastically simplifies the computation of imaginary-time density-density correlation functions with the finite temperature version of the self-consistent dielectric formalism. Its existence underscores the utility of imaginary-time as a complementary domain for many-body physics.","sentences":["A Fourier-Matsubara series expansion is derived for imaginary-time correlation functions that constitutes the imaginary-time generalization of the infinite Matsubara series for equal-time correlation functions.","The expansion is consistent with all known exact properties of imaginary-time correlation functions and opens up new avenues for the utilization of quantum Monte Carlo simulation data.","Moreover, the expansion drastically simplifies the computation of imaginary-time density-density correlation functions with the finite temperature version of the self-consistent dielectric formalism.","Its existence underscores the utility of imaginary-time as a complementary domain for many-body physics."],"url":"http://arxiv.org/abs/2404.01838v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 10:49:01","title":"Anisotropic Hartle-Thorne formalism in Rastall gravity","abstract":"In this work, we extend the Hartle-Thorne formalism for anisotropic pressure condition in Rastall gravity. As an implication, Tolman-Oppeneimer-Volkoff equation in anisotropic form is modified. The equations in anisotropic Hartle-Thorne formalism are also modified due to the appearance of $\\lambda$, i.e. a constant that quantifies the deviation of Rastall gravity from general relativity. However, our results do not cover $\\kappa\\lambda=1/4$ and $\\lambda=1/6$, since both would mathematically give singularity in the calculation.","sentences":["In this work, we extend the Hartle-Thorne formalism for anisotropic pressure condition in Rastall gravity.","As an implication, Tolman-Oppeneimer-Volkoff equation in anisotropic form is modified.","The equations in anisotropic Hartle-Thorne formalism are also modified due to the appearance of $\\lambda$, i.e. a constant that quantifies the deviation of Rastall gravity from general relativity.","However, our results do not cover $\\kappa\\lambda=1/4$ and $\\lambda=1/6$, since both would mathematically give singularity in the calculation."],"url":"http://arxiv.org/abs/2404.01837v1","category":"gr-qc"}
{"created":"2024-04-02 10:48:36","title":"CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS","abstract":"Future mobility systems and their components are increasingly defined by their software. The complexity of these cooperative intelligent transport systems (C-ITS) and the everchanging requirements posed at the software require continual software updates. The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use simulations as one core methodology. The availability of such simulation architectures is a common interest among many stakeholders, especially in the field of automated driving. That is why we propose CARLOS - an open, modular, and scalable simulation framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems. We provide core building blocks for this framework and explain how it can be used and extended by the community. Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration. In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing. We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwthaachen/carlos","sentences":["Future mobility systems and their components are increasingly defined by their software.","The complexity of these cooperative intelligent transport systems (C-ITS) and the everchanging requirements posed at the software require continual software updates.","The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use simulations as one core methodology.","The availability of such simulation architectures is a common interest among many stakeholders, especially in the field of automated driving.","That is why we propose CARLOS - an open, modular, and scalable simulation framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems.","We provide core building blocks for this framework and explain how it can be used and extended by the community.","Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration.","In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing.","We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwthaachen/carlos"],"url":"http://arxiv.org/abs/2404.01836v1","category":"cs.RO"}
{"created":"2024-04-02 10:47:01","title":"Asymptotic decay and quasinormal frequencies of scalar and Dirac fields around dilaton-de Sitter black holes","abstract":"We study the decay of Dirac and massive scalar fields at asymptotically late times in the background of the charged asymptotically de Sitter dilatonic black holes. It is shown that the asymptotic decay is exponential and oscillatory for large and intermediate mass of the field, while for zero and small mass it is pure exponential without oscillations. This reflects the dominance of quasinormal modes of the empty de Sitter spacetime at asymptotically late times. We also show that the earlier WKB calculation of the massive scalar field spectrum %[S. Fernando, Gen.Rel.Grav. 48 (2016) 3, 24] does not allow one to find the fundamental mode with reasonable accuracy.","sentences":["We study the decay of Dirac and massive scalar fields at asymptotically late times in the background of the charged asymptotically de Sitter dilatonic black holes.","It is shown that the asymptotic decay is exponential and oscillatory for large and intermediate mass of the field, while for zero and small mass it is pure exponential without oscillations.","This reflects the dominance of quasinormal modes of the empty de Sitter spacetime at asymptotically late times.","We also show that the earlier WKB calculation of the massive scalar field spectrum %","[S. Fernando, Gen.Rel.Grav.","48 (2016) 3, 24] does not allow one to find the fundamental mode with reasonable accuracy."],"url":"http://arxiv.org/abs/2404.01834v1","category":"gr-qc"}
{"created":"2024-04-02 10:45:49","title":"Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack","abstract":"Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as \"jailbreaks\", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.","sentences":["Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications.","These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms.","However, a recent line of attacks, known as \"jailbreaks\", seek to overcome this alignment.","Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do.","In this paper, we introduce a novel jailbreak attack called Crescendo.","Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner.","It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak.","We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat.","Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks.","Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models."],"url":"http://arxiv.org/abs/2404.01833v1","category":"cs.CR"}
{"created":"2024-04-02 10:42:22","title":"Synthesizing Control Lyapunov-Value Functions for High-Dimensional Systems Using System Decomposition and Admissible Control Sets","abstract":"Control Lyapunov functions (CLFs) play a vital role in modern control applications, but finding them remains a problem. Recently, the control Lyapunov-value function (CLVF) and robust CLVF have been proposed as solutions for nonlinear time-invariant systems with bounded control and disturbance. However, the CLVF suffers from the ''curse of dimensionality,'' which hinders its application to practical high-dimensional systems. In this paper, we propose a method to decompose systems of a particular coupled nonlinear structure, in order to solve for the CLVF in each low-dimensional subsystem. We then reconstruct the full-dimensional CLVF and provide sufficient conditions for when this reconstruction is exact. Moreover, a point-wise optimal controller can be obtained using a quadratic program. We also show that when the exact reconstruction is impossible, the subsystems' CLVFs and their ``admissible control sets'' can be used to generate a Lipschitz continuous CLF. We provide several numerical examples to validate the theory and show computational efficiency.","sentences":["Control Lyapunov functions (CLFs) play a vital role in modern control applications, but finding them remains a problem.","Recently, the control Lyapunov-value function (CLVF) and robust CLVF have been proposed as solutions for nonlinear time-invariant systems with bounded control and disturbance.","However, the CLVF suffers from the ''curse of dimensionality,'' which hinders its application to practical high-dimensional systems.","In this paper, we propose a method to decompose systems of a particular coupled nonlinear structure, in order to solve for the CLVF in each low-dimensional subsystem.","We then reconstruct the full-dimensional CLVF and provide sufficient conditions for when this reconstruction is exact.","Moreover, a point-wise optimal controller can be obtained using a quadratic program.","We also show that when the exact reconstruction is impossible, the subsystems' CLVFs and their ``admissible control sets'' can be used to generate a Lipschitz continuous CLF.","We provide several numerical examples to validate the theory and show computational efficiency."],"url":"http://arxiv.org/abs/2404.01829v1","category":"math.OC"}
{"created":"2024-04-02 10:41:51","title":"Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay","abstract":"Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks. (3) A straightforward regularizer mitigates the 'plasticity-stability' trade-off by aligning model output between new and old tasks. Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training.","sentences":["Deep neural networks have demonstrated susceptibility to adversarial attacks.","Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack.","However, new attacks can emerge in sequences in real-world deployment scenarios.","As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks.","In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks.","(2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks.","(3) A straightforward regularizer mitigates the 'plasticity-stability' trade-off by aligning model output between new and old tasks.","Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training."],"url":"http://arxiv.org/abs/2404.01828v1","category":"cs.LG"}
{"created":"2024-04-02 10:36:30","title":"Degree $p$ Extensions of Arbitrary Valuation Rings and \"Best $f$\"","abstract":"We prove the explicit characterization of the so-called \"best f\" for degree $p$ Artin-Schreier and degree $p$ Kummer extensions of Henselian valuation rings in residue characteristic $p$. This characterization is mentioned briefly in [Th16, Th18]. Existence of best $f$ is closely related to the defect of such extensions and this characterization plays a crucial role in understanding their intricate structure. We also treat degree $p$ Artin-Schreier defect extensions of higher rank valuation rings, extending the results in [Th16], and thus completing the study of degree $p$ extensions that are the building blocks of the general theory.","sentences":["We prove the explicit characterization of the so-called \"best f\" for degree $p$ Artin-Schreier and degree $p$ Kummer extensions of Henselian valuation rings in residue characteristic $p$. This characterization is mentioned briefly in [Th16, Th18].","Existence of best $f$ is closely related to the defect of such extensions and this characterization plays a crucial role in understanding their intricate structure.","We also treat degree $p$ Artin-Schreier defect extensions of higher rank valuation rings, extending the results in [Th16], and thus completing the study of degree $p$ extensions that are the building blocks of the general theory."],"url":"http://arxiv.org/abs/2404.01825v1","category":"math.AC"}
{"created":"2024-04-02 10:16:30","title":"A neural network-based approach to hybrid systems identification for control","abstract":"We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being well-suited for optimal control design, numerical simulations illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear benchmarks.","sentences":["We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design.","We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures.","We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP).","Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization.","In addition to being well-suited for optimal control design, numerical simulations illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear benchmarks."],"url":"http://arxiv.org/abs/2404.01814v1","category":"eess.SY"}
{"created":"2024-04-02 10:15:06","title":"Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions","abstract":"Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found here: https://actnerf.github.io.","sentences":["Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces.","This requires physical interaction with objects to build their internal representations.","This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations.","We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility.","Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction.","Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods.","The project page can be found here: https://actnerf.github.io."],"url":"http://arxiv.org/abs/2404.01812v1","category":"cs.RO"}
{"created":"2024-04-02 10:07:13","title":"Stacking of charge-density waves in 2H-NbSe$_2$ bilayers","abstract":"We employ ab-initio electronic structure calculations to investigate the charge-density waves and periodic lattice distortions in bilayer 2H-NbSe$_2$. We demonstrate that the vertical stacking can give rise to a variety of patterns that may lower the symmetry of the CDW exhibited separately by the two composing 1H-NbSe$_2$ monolayers. The general tendency to a spontaneous symmetry breaking observed in the ground state and the first excited states is shown to originate from a non-negligible inter-layer coupling. Simulated images for scanning tunnelling microscopy (STM) as well as diffraction/scattering patterns show signatures of the different stacking orders. This may not only be useful to reinterpret past experiments on surfaces and thin films, but may also be exploited to devise ad-hoc experiments for the investigation of the stacking order in 2H-NbSe$_2$. We anticipate that our analysis does not only apply to the 2H-NbSe$_2$ bilayer, but is also relevant for thin films and bulk, whose smallest centro-symmetric component is indeed the bilayer. Finally, our results illustrate clearly that the vertical stacking is not only important for 1T structures, as exemplified by the metal-to-insulator transition observed in 1T-TaS$_2$, but seems to be a general feature of metallic layered transition metal dichalcogenides as well.","sentences":["We employ ab-initio electronic structure calculations to investigate the charge-density waves and periodic lattice distortions in","bilayer 2H-NbSe$_2$.","We demonstrate that the vertical stacking can give rise to a variety of patterns that may lower the symmetry of the CDW exhibited separately by the two composing 1H-NbSe$_2$ monolayers.","The general tendency to a spontaneous symmetry breaking observed in the ground state and the first excited states is shown to originate from a non-negligible inter-layer coupling.","Simulated images for scanning tunnelling microscopy (STM) as well as diffraction/scattering patterns show signatures of the different stacking orders.","This may not only be useful to reinterpret past experiments on surfaces and thin films, but may also be exploited to devise ad-hoc experiments for the investigation of the stacking order in 2H-NbSe$_2$.","We anticipate that our analysis does not only apply to the 2H-NbSe$_2$ bilayer, but is also relevant for thin films and bulk, whose smallest centro-symmetric component is indeed the bilayer.","Finally, our results illustrate clearly that the vertical stacking is not only important for 1T structures, as exemplified by the metal-to-insulator transition observed in 1T-TaS$_2$, but seems to be a general feature of metallic layered transition metal dichalcogenides as well."],"url":"http://arxiv.org/abs/2404.01807v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 10:06:45","title":"Hurewicz and Dranishnikov-Smith theorems for asymptotic dimension of countable approximate groups","abstract":"We establish two main results for the asymptotic dimension of countable approximate groups. The first one is a Hurewicz type formula for a global morphism of countable approximate groups $f:(\\Xi, \\Xi^\\infty) \\to (\\Lambda, \\Lambda^\\infty)$, stating that $\\mathrm{asdim} \\Xi \\leq \\mathrm{asdim} \\Lambda +\\mathrm{asdim} ([\\mathrm{ker} f]_c)$. This is analogous to the Dranishnikov-Smith result for groups, and is relying on another Hurewicz type formula we prove, using a 6-local morphism instead of a global one. The second result is similar to the Dranishnikov-Smith theorem stating that, for a countable group $G$, $\\mathrm{asdim} G$ is equal to the supremum of asymptotic dimensions of finitely generated subgroups of $G$. Our version states that, if $(\\Lambda, \\Lambda^\\infty)$ is a countable approximate group, then $\\mathrm{asdim} \\Lambda$ is equal to the supremum of asymptotic dimensions of approximate subgroups of finitely generated subgroups of $\\Lambda^\\infty$, with these approximate subgroups contained in $\\Lambda^2$.","sentences":["We establish two main results for the asymptotic dimension of countable approximate groups.","The first one is a Hurewicz type formula for a global morphism of countable approximate groups $f:(\\Xi, \\Xi^\\infty) \\to (\\Lambda, \\Lambda^\\infty)$, stating that $\\mathrm{asdim} \\Xi \\leq \\mathrm{asdim} \\Lambda +\\mathrm{asdim} ([\\mathrm{ker} f]_c)$.","This is analogous to the Dranishnikov-Smith result for groups, and is relying on another Hurewicz type formula we prove, using a 6-local morphism instead of a global one.","The second result is similar to the Dranishnikov-Smith theorem stating that, for a countable group $G$, $\\mathrm{asdim} G$ is equal to the supremum of asymptotic dimensions of finitely generated subgroups of $G$. Our version states that, if $(\\Lambda, \\Lambda^\\infty)$ is a countable approximate group, then $\\mathrm{asdim} \\Lambda$ is equal to the supremum of asymptotic dimensions of approximate subgroups of finitely generated subgroups of $\\Lambda^\\infty$, with these approximate subgroups contained in $\\Lambda^2$."],"url":"http://arxiv.org/abs/2404.01806v1","category":"math.GR"}
{"created":"2024-04-02 10:06:21","title":"Neuromorphic Wireless Device-Edge Co-Inference via the Directed Information Bottleneck","abstract":"An important use case of next-generation wireless systems is device-edge co-inference, where a semantic task is partitioned between a device and an edge server. The device carries out data collection and partial processing of the data, while the remote server completes the given task based on information received from the device. It is often required that processing and communication be run as efficiently as possible at the device, while more computing resources are available at the edge. To address such scenarios, we introduce a new system solution, termed neuromorphic wireless device-edge co-inference. According to it, the device runs sensing, processing, and communication units using neuromorphic hardware, while the server employs conventional radio and computing technologies. The proposed system is designed using a transmitter-centric information-theoretic criterion that targets a reduction of the communication overhead, while retaining the most relevant information for the end-to-end semantic task of interest. Numerical results on standard data sets validate the proposed architecture, and a preliminary testbed realization is reported.","sentences":["An important use case of next-generation wireless systems is device-edge co-inference, where a semantic task is partitioned between a device and an edge server.","The device carries out data collection and partial processing of the data, while the remote server completes the given task based on information received from the device.","It is often required that processing and communication be run as efficiently as possible at the device, while more computing resources are available at the edge.","To address such scenarios, we introduce a new system solution, termed neuromorphic wireless device-edge co-inference.","According to it, the device runs sensing, processing, and communication units using neuromorphic hardware, while the server employs conventional radio and computing technologies.","The proposed system is designed using a transmitter-centric information-theoretic criterion that targets a reduction of the communication overhead, while retaining the most relevant information for the end-to-end semantic task of interest.","Numerical results on standard data sets validate the proposed architecture, and a preliminary testbed realization is reported."],"url":"http://arxiv.org/abs/2404.01804v1","category":"cs.LG"}
{"created":"2024-04-02 09:59:49","title":"Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest","abstract":"Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and promoting integrity in scholarly research.","sentences":["Scientific articles play a crucial role in advancing knowledge and informing research directions.","One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works.","This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles.","By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works.","Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation.","This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and promoting integrity in scholarly research."],"url":"http://arxiv.org/abs/2404.01800v1","category":"cs.DL"}
{"created":"2024-04-02 09:56:19","title":"Open Experimental Measurements of Sub-6GHz Reconfigurable Intelligent Surfaces","abstract":"In this paper, we present two datasets that we make publicly available for research. The data is collected in a testbed comprised of a custom-made Reconfigurable Intelligent Surface (RIS) prototype and two regular OFDM transceivers within an anechoic chamber. First, we discuss the details of the testbed and equipment used, including insights about the design and implementation of our RIS prototype. We further present the methodology we employ to gather measurement samples, which consists of letting the RIS electronically steer the signal reflections from an OFDM transmitter toward a specific location. To this end, we evaluate a suitably designed configuration codebook and collect measurement samples of the received power with an OFDM receiver. Finally, we present the resulting datasets, their format, and examples of exploiting this data for research purposes.","sentences":["In this paper, we present two datasets that we make publicly available for research.","The data is collected in a testbed comprised of a custom-made Reconfigurable Intelligent Surface (RIS) prototype and two regular OFDM transceivers within an anechoic chamber.","First, we discuss the details of the testbed and equipment used, including insights about the design and implementation of our RIS prototype.","We further present the methodology we employ to gather measurement samples, which consists of letting the RIS electronically steer the signal reflections from an OFDM transmitter toward a specific location.","To this end, we evaluate a suitably designed configuration codebook and collect measurement samples of the received power with an OFDM receiver.","Finally, we present the resulting datasets, their format, and examples of exploiting this data for research purposes."],"url":"http://arxiv.org/abs/2404.01796v1","category":"cs.IT"}
{"created":"2024-04-02 09:56:04","title":"Long Time Propagation of Chaos in Total Variation Distance for Mean Field Interacting Particle System","abstract":"In this paper, a general result on the long time quantitative propagation of chaos in total variation distance for mean field interacting particle system driven by general L\\'{e}vy noise is derived, where the non-interacting drift is assumed to be dissipative in long distance and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance. Moreover, by using the method of coupling, the results are applied to mean field interacting particle system driven by Brownian motion and $\\alpha(\\alpha>1)$-stable noise respectively.","sentences":["In this paper, a general result on the long time quantitative propagation of chaos in total variation distance for mean field interacting particle system driven by general L\\'{e}vy noise is derived, where the non-interacting drift is assumed to be dissipative in long distance and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance.","Moreover, by using the method of coupling, the results are applied to mean field interacting particle system driven by Brownian motion and $\\alpha(\\alpha>1)$-stable noise respectively."],"url":"http://arxiv.org/abs/2404.01795v1","category":"math.PR"}
{"created":"2024-04-02 09:55:30","title":"Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid","abstract":"Autonomous and learning systems based on Deep Reinforcement Learning have firmly established themselves as a foundation for approaches to creating resilient and efficient Cyber-Physical Energy Systems. However, most current approaches suffer from two distinct problems: Modern model-free algorithms such as Soft Actor Critic need a high number of samples to learn a meaningful policy, as well as a fallback to ward against concept drifts (e. g., catastrophic forgetting). In this paper, we present the work in progress towards a hybrid agent architecture that combines model-based Deep Reinforcement Learning with imitation learning to overcome both problems.","sentences":["Autonomous and learning systems based on Deep Reinforcement Learning have firmly established themselves as a foundation for approaches to creating resilient and efficient Cyber-Physical Energy Systems.","However, most current approaches suffer from two distinct problems: Modern model-free algorithms such as Soft Actor Critic need a high number of samples to learn a meaningful policy, as well as a fallback to ward against concept drifts (e. g., catastrophic forgetting).","In this paper, we present the work in progress towards a hybrid agent architecture that combines model-based Deep Reinforcement Learning with imitation learning to overcome both problems."],"url":"http://arxiv.org/abs/2404.01794v1","category":"cs.AI"}
{"created":"2024-04-02 09:54:19","title":"Realizations and star-product of doubly $\u03ba$-deformed Yang models","abstract":"The Yang algebra was proposed a long time ago as a generalization of the Snyder algebra to the case of curved background spacetime. It includes as subalgebras both the Snyder and the de Sitter algebras and can therefore be viewed as a model of noncommutative curved spacetime. A peculiarity with respect to standard models of noncommutative geometry is that it includes translation and Lorentz generators, so that the definition of a Hopf algebra and the physical interpretation of the variables conjugated to the primary ones is not trivial. In this paper we consider the realizations of the Yang algebra and its $\\kappa$-deformed generalization on an extended phase space and in this way we are able to define a Hopf structure and a twist.","sentences":["The Yang algebra was proposed a long time ago as a generalization of the Snyder algebra to the case of curved background spacetime.","It includes as subalgebras both the Snyder and the de Sitter algebras and can therefore be viewed as a model of noncommutative curved spacetime.","A peculiarity with respect to standard models of noncommutative geometry is that it includes translation and Lorentz generators, so that the definition of a Hopf algebra and the physical interpretation of the variables conjugated to the primary ones is not trivial.","In this paper we consider the realizations of the Yang algebra and its $\\kappa$-deformed generalization on an extended phase space and in this way we are able to define a Hopf structure and a twist."],"url":"http://arxiv.org/abs/2404.01792v1","category":"hep-th"}
{"created":"2024-04-02 09:53:20","title":"Super-Resolution Analysis for Landfill Waste Classification","abstract":"Illegal landfills are a critical issue due to their environmental, economic, and public health impacts. This study leverages aerial imagery for environmental crime monitoring. While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images. Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains. Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills. We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold fine-tuning.","sentences":["Illegal landfills are a critical issue due to their environmental, economic, and public health impacts.","This study leverages aerial imagery for environmental crime monitoring.","While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images.","Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains.","Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills.","We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold fine-tuning."],"url":"http://arxiv.org/abs/2404.01790v1","category":"cs.CV"}
{"created":"2024-04-02 09:50:33","title":"A Kerr kernel quantum learning machine","abstract":"Kernel methods are of current interest in quantum machine learning due to similarities with quantum computing in how they process information in high-dimensional feature (Hilbert) spaces. Kernels are believed to offer particular advantages when they cannot be computed classically, so a kernel matrix with indisputably nonclassical elements is desirable provided it can be generated efficiently in a particular physical machine. Kerr nonlinearities, known to be a route to universal continuous variable (CV) quantum computation, may be able to play this role for quantum machine learning. We propose a quantum hardware kernel implementation scheme based on superconducting quantum circuits. The scheme does not use qubits or quantum circuits but rather exploits the analogue features of Kerr coupled modes. Our approach is more akin to the growing number of analog machine learning schemes based on sampling quantum probabilities directly in an engineered device by stochastic quantum control.","sentences":["Kernel methods are of current interest in quantum machine learning due to similarities with quantum computing in how they process information in high-dimensional feature (Hilbert) spaces.","Kernels are believed to offer particular advantages when they cannot be computed classically, so a kernel matrix with indisputably nonclassical elements is desirable provided it can be generated efficiently in a particular physical machine.","Kerr nonlinearities, known to be a route to universal continuous variable (CV) quantum computation, may be able to play this role for quantum machine learning.","We propose a quantum hardware kernel implementation scheme based on superconducting quantum circuits.","The scheme does not use qubits or quantum circuits but rather exploits the analogue features of Kerr coupled modes.","Our approach is more akin to the growing number of analog machine learning schemes based on sampling quantum probabilities directly in an engineered device by stochastic quantum control."],"url":"http://arxiv.org/abs/2404.01787v1","category":"quant-ph"}
{"created":"2024-04-02 09:49:53","title":"Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model","abstract":"This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic text generation are also identified.","sentences":["This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods.","Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method.","Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches.","Finally, some future directions of research in the field of automatic text generation are also identified."],"url":"http://arxiv.org/abs/2404.01786v1","category":"cs.CL"}
{"created":"2024-04-02 09:45:19","title":"Multicriteria Analysis Model in Sustainable Corn Farming Area Planning","abstract":"This study aims to develop a framework for multicriteria analysis to evaluate alternatives for sustainable corn agricultural area planning, considering the integration of ecological, economic, and social aspects as pillars of sustainability. The research method uses qualitative and quantitative approaches to integrate ecological, economic, and social aspects in the multicriteria analysis. The analysis involves land evaluation, subcriteria identification, and data integration using Multidimensional Scaling and Analytical Hierarchy Process methods to prioritize developing sustainable corn agricultural areas. Based on the results of the RAP-Corn analysis, it indicates that the ecological dimension depicts less sustainability. The AHP results yield weight distribution and highly relevant scores that describe tangible preferences. Priority directions are grouped as strategic steps toward achieving the goals of sustainable corn agricultural area planning.","sentences":["This study aims to develop a framework for multicriteria analysis to evaluate alternatives for sustainable corn agricultural area planning, considering the integration of ecological, economic, and social aspects as pillars of sustainability.","The research method uses qualitative and quantitative approaches to integrate ecological, economic, and social aspects in the multicriteria analysis.","The analysis involves land evaluation, subcriteria identification, and data integration using Multidimensional Scaling and Analytical Hierarchy Process methods to prioritize developing sustainable corn agricultural areas.","Based on the results of the RAP-Corn analysis, it indicates that the ecological dimension depicts less sustainability.","The AHP results yield weight distribution and highly relevant scores that describe tangible preferences.","Priority directions are grouped as strategic steps toward achieving the goals of sustainable corn agricultural area planning."],"url":"http://arxiv.org/abs/2404.01782v1","category":"econ.GN"}
{"created":"2024-04-02 09:44:38","title":"An evaluation of CFEAR Radar Odometry","abstract":"This article describes the method CFEAR Radar odometry, submitted to a competition at the Radar in Robotics workshop, ICRA 2024. CFEAR is an efficient and accurate method for spinning 2D radar odometry that generalizes well across environments. This article presents an overview of the odometry pipeline with new experiments on the public Boreas dataset. We show that a real-time capable configuration of CFEAR -- with its original parameter se -- yields surprisingly low drift in the Boreas dataset. Additionally, we discuss an improved implementation and solving strategy that enables the most accurate configuration to run in real-time with improved robustness, reaching as low as 0.66% translation drift at a frame rate of 68 Hz. A recent release of the source code is available to the community https://github.com/dan11003/CFEAR_Radarodometry_code_public, and we publish the evaluation from this article https://github.com/dan11003/cfear_2024_workshop.","sentences":["This article describes the method CFEAR Radar odometry, submitted to a competition at the Radar in Robotics workshop, ICRA 2024.","CFEAR is an efficient and accurate method for spinning 2D radar odometry that generalizes well across environments.","This article presents an overview of the odometry pipeline with new experiments on the public Boreas dataset.","We show that a real-time capable configuration of CFEAR -- with its original parameter se -- yields surprisingly low drift in the Boreas dataset.","Additionally, we discuss an improved implementation and solving strategy that enables the most accurate configuration to run in real-time with improved robustness, reaching as low as 0.66% translation drift at a frame rate of 68 Hz.","A recent release of the source code is available to the community https://github.com/dan11003/CFEAR_Radarodometry_code_public, and we publish the evaluation from this article https://github.com/dan11003/cfear_2024_workshop."],"url":"http://arxiv.org/abs/2404.01781v1","category":"cs.RO"}
{"created":"2024-04-02 09:44:02","title":"Diagonal Coset Approach to Topological Quantum Computation with Fibonacci Anyons","abstract":"We investigate a promising conformal field theory realization scheme for topological quantum computation based on the Fibonacci anyons, which are believed to be realized as quasiparticle excitations in the $\\mathbb{Z}_3$ parafermion fractional quantum Hall state in the second Landau level with filling factor $\\nu=12/5$. These anyons are non-Abelian and are known to be capable of universal topological quantum computation. The quantum information is encoded in the fusion channels of pairs of such non-Abelian anyons and is protected from noise and decoherence by the topological properties of these systems.The quantum gates are realized by braiding of these anyons. We propose here an implementation of the $n$-qubit topological quantum register in terms of $2n+2$ Fibonacci anyons. The matrices emerging from the anyon exchanges, i.e. the generators of the braid group for one qubit are derived from the coordinate wave functions of a large number of electron holes and 4 Fibonacci anyons which can furthermore be represented as correlation functions in $\\mathbb{Z}_3$ parafermionic two-dimensional conformal field theory. The representations of the braid groups for more than 4 anyons are obtained by fusing pairs of anyons before braiding, thus reducing eventually the system to 4 anyons.","sentences":["We investigate a promising conformal field theory realization scheme for topological quantum computation based on the Fibonacci anyons, which are believed to be realized as quasiparticle excitations in the $\\mathbb{Z}_3$ parafermion fractional quantum Hall state in the second Landau level with filling factor $\\nu=12/5$. These anyons are non-Abelian and are known to be capable of universal topological quantum computation.","The quantum information is encoded in the fusion channels of pairs of such non-Abelian anyons and is protected from noise and decoherence by the topological properties of these systems.","The quantum gates are realized by braiding of these anyons.","We propose here an implementation of the $n$-qubit topological quantum register in terms of $2n+2$ Fibonacci anyons.","The matrices emerging from the anyon exchanges, i.e. the generators of the braid group for one qubit are derived from the coordinate wave functions of a large number of electron holes and 4 Fibonacci anyons which can furthermore be represented as correlation functions in $\\mathbb{Z}_3$ parafermionic two-dimensional conformal field theory.","The representations of the braid groups for more than 4 anyons are obtained by fusing pairs of anyons before braiding, thus reducing eventually the system to 4 anyons."],"url":"http://arxiv.org/abs/2404.01779v1","category":"quant-ph"}
{"created":"2024-04-02 09:43:01","title":"Braiding Fibonacci anyons","abstract":"Fibonacci anyons provide the simplest possible model of non-Abelian fusion rules: [1] x [1] = [0] + [1]. We propose a conformal field theory construction of topological quantum registers based on Fibonacci anyons realized as quasiparticle excitations in the Z_3 parafermion fractional quantum Hall state. To this end, the results of Ardonne and Schoutens for the correlation function of n = 4 Fibonacci fields are extended to the case of arbitrary n (and 3 r electrons). Special attention is paid to the braiding properties of the obtained correlators. We explain in details the construction of a monodromy representation of the Artin braid group acting on n-point conformal blocks of Fibonacci anyons. For low n (up to n = 8), the matrices of braid group generators are displayed explicitly. A simple recursion formula makes it possible to extend without efforts the construction to any n. Finally, we construct N qubit computational spaces in terms of conformal blocks of 2N + 2 Fibonacci anyons.","sentences":["Fibonacci anyons provide the simplest possible model of non-Abelian fusion rules: [1] x [1] =","[0] +","[1].","We propose a conformal field theory construction of topological quantum registers based on Fibonacci anyons realized as quasiparticle excitations in the Z_3 parafermion fractional quantum Hall state.","To this end, the results of Ardonne and Schoutens for the correlation function of n = 4 Fibonacci fields are extended to the case of arbitrary n (and 3 r electrons).","Special attention is paid to the braiding properties of the obtained correlators.","We explain in details the construction of a monodromy representation of the Artin braid group acting on n-point conformal blocks of Fibonacci anyons.","For low n (up to n = 8), the matrices of braid group generators are displayed explicitly.","A simple recursion formula makes it possible to extend without efforts the construction to any n. Finally, we construct N qubit computational spaces in terms of conformal blocks of 2N +","2 Fibonacci anyons."],"url":"http://arxiv.org/abs/2404.01778v1","category":"hep-th"}
{"created":"2024-04-02 09:40:22","title":"A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?","abstract":"The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: https://github.com/glhr/ood-labelnoise","sentences":["The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems.","In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection.","While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset.","In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels).","Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods.","Code: https://github.com/glhr/ood-labelnoise"],"url":"http://arxiv.org/abs/2404.01775v1","category":"cs.CV"}
{"created":"2024-04-02 09:33:31","title":"On the rings whose injective right modules are max-projective","abstract":"Recently, the rings whose injective right modules are R-projective (respectively, max-projective) were investigated and studied in [2]. Such ring are called right almost-QF (respectively, max-QF). In this paper, our aim is to give some further characterization of these rings over more general classes of rings, and address several questions about these rings. We obtain characterizations of max-QF rings over several classes of rings including local, semilocal right semihereditary, right nonsingular right noetherian and right nonsingular right finite dimensional rings. We prove that for a ring R being right almost-QF and right max-QF are not left-right symmetric. We also show that right almost-QF and right max-QF rings are not closed under factor rings. This leads to consider the rings all of whose factor rings are almost-QF and max-QF.","sentences":["Recently, the rings whose injective right modules are R-projective (respectively, max-projective) were investigated and studied in [2].","Such ring are called right almost-QF (respectively, max-QF).","In this paper, our aim is to give some further characterization of these rings over more general classes of rings, and address several questions about these rings.","We obtain characterizations of max-QF rings over several classes of rings including local, semilocal right semihereditary, right nonsingular right noetherian and right nonsingular right finite dimensional rings.","We prove that for a ring R being right almost-QF and right max-QF are not left-right symmetric.","We also show that right almost-QF and right max-QF rings are not closed under factor rings.","This leads to consider the rings all of whose factor rings are almost-QF and max-QF."],"url":"http://arxiv.org/abs/2404.01771v1","category":"math.RA"}
{"created":"2024-04-02 09:32:23","title":"Nanoscale mechanical manipulation of ultrathin SiN membranes enabling infrared near-field microscopy of liquid-immersed samples","abstract":"Scattering scanning near-field optical microscopy (s-SNOM) is a powerful technique for mid-infrared spectroscopy at nanometer length scales. By investigating objects in aqueous environments through ultrathin membranes, s-SNOM has recently been extended towards label-free nanoscopy of the dynamics of living cells and nanoparticles, assessing both the optical and the mechanical interactions between the tip, the membrane and the liquid suspension underneath. Here, we report that the tapping AFM tip induces a reversible nanometric deformation of the membrane manifested as either an indentation or protrusion. This mechanism depends on the driving force of the tapping cantilever, which we exploit to minimize topographical deformations of the membrane to improve optical measurements. Furthermore, we show that the tapping phase, or phase delay between driving signal and tip oscillation, is a highly sensitive observable for quantifying the mechanics of adhering objects, exhibiting highest contrast for low tapping amplitudes where the membrane remains nearly flat. We correlate mechanical responses with simultaneously recorded spectroscopy data to reveal the thickness of nanometric water pockets between membrane and adhering objects. Besides a general applicability of depth profiling, our technique holds great promise for studying mechano-active biopolymers and living cells, biomaterials that exhibit complex behaviors when under a mechanical load.","sentences":["Scattering scanning near-field optical microscopy (s-SNOM) is a powerful technique for mid-infrared spectroscopy at nanometer length scales.","By investigating objects in aqueous environments through ultrathin membranes, s-SNOM has recently been extended towards label-free nanoscopy of the dynamics of living cells and nanoparticles, assessing both the optical and the mechanical interactions between the tip, the membrane and the liquid suspension underneath.","Here, we report that the tapping AFM tip induces a reversible nanometric deformation of the membrane manifested as either an indentation or protrusion.","This mechanism depends on the driving force of the tapping cantilever, which we exploit to minimize topographical deformations of the membrane to improve optical measurements.","Furthermore, we show that the tapping phase, or phase delay between driving signal and tip oscillation, is a highly sensitive observable for quantifying the mechanics of adhering objects, exhibiting highest contrast for low tapping amplitudes where the membrane remains nearly flat.","We correlate mechanical responses with simultaneously recorded spectroscopy data to reveal the thickness of nanometric water pockets between membrane and adhering objects.","Besides a general applicability of depth profiling, our technique holds great promise for studying mechano-active biopolymers and living cells, biomaterials that exhibit complex behaviors when under a mechanical load."],"url":"http://arxiv.org/abs/2404.01770v1","category":"physics.optics"}
{"created":"2024-04-02 09:31:32","title":"Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation","abstract":"Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series of example cases discussing the results. Finally, we develop a series of stereotype elicitation prompts and evaluate the presence of stereotypes in text generation tasks with popular LLMs, using one of our best performing previously presented stereotypes detectors. Our experiments yielded several key findings: i) Training stereotype detectors in a multi-dimension setting yields better results than training multiple single-dimension classifiers.ii) The integrated MGS Dataset enhances both the in-dataset and cross-dataset generalisation ability of stereotype detectors compared to using the datasets separately. iii) There is a reduction in stereotypes in the content generated by GPT Family LLMs with newer versions.","sentences":["Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications.","However, LLMs could reproduce and even exacerbate stereotypical outputs from training data.","This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets.","We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS.","To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series of example cases discussing the results.","Finally, we develop a series of stereotype elicitation prompts and evaluate the presence of stereotypes in text generation tasks with popular LLMs, using one of our best performing previously presented stereotypes detectors.","Our experiments yielded several key findings: i) Training stereotype detectors in a multi-dimension setting yields better results than training multiple single-dimension classifiers.ii) The integrated MGS Dataset enhances both the in-dataset and cross-dataset generalisation ability of stereotype detectors compared to using the datasets separately.","iii)","There is a reduction in stereotypes in the content generated by GPT Family LLMs with newer versions."],"url":"http://arxiv.org/abs/2404.01768v1","category":"cs.CL"}
{"created":"2024-04-02 09:29:44","title":"Experimental Demonstration of Back-Linked Fabry-Perot Interferometer for the Space Gravitational Wave Antenna","abstract":"The back-linked Fabry-Perot interferometer (BLFPI) is an interferometer topology proposed for space gravitational wave antennas with the use of inter-satellite Fabry-Perot interferometers. The BLFPI offers simultaneous and independent control over all interferometer length degrees of freedom by controlling the laser frequencies. Therefore, BLFPI does not require an active control system for the physical lengths of the inter-satellite Fabry-Perot interferometers. To achieve a high sensitivity, the implementation must rely on an offline signal process for subtracting laser frequency noises. However, the subtraction has not been experimentally verified to date. This paper reports a demonstration of the frequency noise subtraction in the frequency band of 100 Hz-50 kHz, including the cavity pole frequency, using Fabry-Perot cavities with a length of 46 cm. The highest reduction ratio of approximately 200 was achieved. This marks the first experimental verification of the critical function in the BLFPI.","sentences":["The back-linked Fabry-Perot interferometer (BLFPI) is an interferometer topology proposed for space gravitational wave antennas with the use of inter-satellite Fabry-Perot interferometers.","The BLFPI offers simultaneous and independent control over all interferometer length degrees of freedom by controlling the laser frequencies.","Therefore, BLFPI does not require an active control system for the physical lengths of the inter-satellite Fabry-Perot interferometers.","To achieve a high sensitivity, the implementation must rely on an offline signal process for subtracting laser frequency noises.","However, the subtraction has not been experimentally verified to date.","This paper reports a demonstration of the frequency noise subtraction in the frequency band of 100 Hz-50 kHz, including the cavity pole frequency, using Fabry-Perot cavities with a length of 46 cm.","The highest reduction ratio of approximately 200 was achieved.","This marks the first experimental verification of the critical function in the BLFPI."],"url":"http://arxiv.org/abs/2404.01764v1","category":"gr-qc"}
{"created":"2024-04-02 09:26:44","title":"A Formal Proof of R(4,5)=25","abstract":"In 1995, McKay and Radziszowski proved that the Ramsey number R(4,5) is equal to 25. Their proof relies on a combination of high-level arguments and computational steps. The authors have performed the computational parts of the proof with different implementations in order to reduce the possibility of an error in their programs. In this work, we prove this theorem in the interactive theorem prover HOL4 limiting the uncertainty to the small HOL4 kernel. Instead of verifying their algorithms directly, we rely on a verified SAT solver to prove gluing lemmas. To reduce the number of such lemmas and thus make the computational part of the proof feasible, we implement a generalization algorithm and verify that its output covers all the possible cases.","sentences":["In 1995, McKay and Radziszowski proved that the Ramsey number R(4,5) is equal to 25.","Their proof relies on a combination of high-level arguments and computational steps.","The authors have performed the computational parts of the proof with different implementations in order to reduce the possibility of an error in their programs.","In this work, we prove this theorem in the interactive theorem prover HOL4 limiting the uncertainty to the small HOL4 kernel.","Instead of verifying their algorithms directly, we rely on a verified SAT solver to prove gluing lemmas.","To reduce the number of such lemmas and thus make the computational part of the proof feasible, we implement a generalization algorithm and verify that its output covers all the possible cases."],"url":"http://arxiv.org/abs/2404.01761v1","category":"cs.LO"}
{"created":"2024-04-02 09:18:52","title":"GEARS: Local Geometry-aware Hand-object Interaction Synthesis","abstract":"Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless, these methods show limited generalizability across object categories, shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data. To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions. Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model's generalization capability. We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually.","sentences":["Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans.","Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features.","Nonetheless, these methods show limited generalizability across object categories, shapes and sizes.","We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data.","To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions.","The sensor queries for object surface points in the neighbourhood of each hand joint.","As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint.","This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions.","Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples.","This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model's generalization capability.","We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually."],"url":"http://arxiv.org/abs/2404.01758v1","category":"cs.CV"}
{"created":"2024-04-02 09:17:20","title":"Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs","abstract":"Neural Networks (NNs) are increasingly used in the last decade in several demanding applications, such as object detection and classification, autonomous driving, etc. Among different computing platforms for implementing NNs, FPGAs have multiple advantages due to design flexibility and high performance-to-watt ratio. Moreover, approximation techniques, such as quantization, have been introduced, which reduce the computational and storage requirements, thus enabling the integration of larger NNs into FPGA devices. On the other hand, FPGAs are sensitive to radiation-induced Single Event Upsets (SEUs). In this work, we perform an in-depth reliability analysis in an FPGA-based Binarized Fully Connected Neural Network (BNN) accelerator running a statistical fault injection campaign. The BNN benchmark has been produced by FINN, an open-source framework that provides an end-to-end flow from abstract level to design, making it easy to design customized FPGA NN accelerators, while it also supports various approximation techniques. The campaign includes the injection of faults in the configuration memory of a state-of-the-art Xilinx Ultrascale+ FPGA running the BNN, as well an exhaustive fault injection in the user flip flops. We have analyzed the fault injection results characterizing the SEU vulnerability of the circuit per network layer, per clock cycle, and register. In general, the results show that the BNNs are inherently resilient to soft errors, since a low portion of SEUs in the configuration memory and the flip flops, cause system crashes or misclassification errors.","sentences":["Neural Networks (NNs) are increasingly used in the last decade in several demanding applications, such as object detection and classification, autonomous driving, etc.","Among different computing platforms for implementing NNs, FPGAs have multiple advantages due to design flexibility and high performance-to-watt ratio.","Moreover, approximation techniques, such as quantization, have been introduced, which reduce the computational and storage requirements, thus enabling the integration of larger NNs into FPGA devices.","On the other hand, FPGAs are sensitive to radiation-induced Single Event Upsets (SEUs).","In this work, we perform an in-depth reliability analysis in an FPGA-based Binarized Fully Connected Neural Network (BNN) accelerator running a statistical fault injection campaign.","The BNN benchmark has been produced by FINN, an open-source framework that provides an end-to-end flow from abstract level to design, making it easy to design customized FPGA NN accelerators, while it also supports various approximation techniques.","The campaign includes the injection of faults in the configuration memory of a state-of-the-art Xilinx Ultrascale+ FPGA running the BNN, as well an exhaustive fault injection in the user flip flops.","We have analyzed the fault injection results characterizing the SEU vulnerability of the circuit per network layer, per clock cycle, and register.","In general, the results show that the BNNs are inherently resilient to soft errors, since a low portion of SEUs in the configuration memory and the flip flops, cause system crashes or misclassification errors."],"url":"http://arxiv.org/abs/2404.01757v1","category":"cs.AR"}
{"created":"2024-04-02 09:12:21","title":"Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments","abstract":"Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively","sentences":["Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments.","Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments.","However, the programs used for evaluation are relatively simple.","It remains unclear how existing approaches perform in repairing programs from higher-level programming courses.","To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course.","Subsequently, we identify the challenges related to fixing bugs in advanced assignments.","Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases:","Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair.","Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria.","Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage.","The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively"],"url":"http://arxiv.org/abs/2404.01754v1","category":"cs.SE"}
{"created":"2024-04-02 09:07:12","title":"Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space","abstract":"In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces higher-quality solutions compared to the state-of-the-art planners for continuous space. Compared to the most scalable existing algorithm, SI-CPP achieves a success rate that is up to 94% higher with 100 robots while maintaining solution quality (i.e., flowtime, the sum of travel times of all robots) without significant compromise. SI-CPP also decreases the makespan up to 45%. SI-CCBS decreases the flowtime by 9% compared to the competitor, albeit exhibiting a 14% lower success rate.","sentences":["In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths.","The difficulty of the problem arises from two primary factors.","First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially.","Second, the continuous space presents potentially infinite states and actions.","For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots.","The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS).","Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples.","SI-CPP exhibits improved scalability while SI-CCBS produces higher-quality solutions compared to the state-of-the-art planners for continuous space.","Compared to the most scalable existing algorithm, SI-CPP achieves a success rate that is up to 94% higher with 100 robots while maintaining solution quality (i.e., flowtime, the sum of travel times of all robots) without significant compromise.","SI-CPP also decreases the makespan up to 45%.","SI-CCBS decreases the flowtime by 9% compared to the competitor, albeit exhibiting a 14% lower success rate."],"url":"http://arxiv.org/abs/2404.01752v1","category":"cs.RO"}
{"created":"2024-04-02 09:05:47","title":"Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder","abstract":"Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process.   In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.","sentences":["Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature.","Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features.","Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images.","By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process.   ","In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder.","The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior.","Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent."],"url":"http://arxiv.org/abs/2404.01750v1","category":"cs.CV"}
{"created":"2024-04-02 09:05:43","title":"Curvature conditions, Liouville-type theorems and Harnack inequalities for a nonlinear parabolic equation on smooth metric measure spaces","abstract":"In this paper we prove gradient estimates of both elliptic and parabolic types, specifically, of Souplet-Zhang, Hamilton and Li-Yau types for positive smooth solutions to a class of nonlinear parabolic equations involving the Witten or drifting Laplacian on smooth metric measure spaces. These estimates are established under various curvature conditions and lower bounds on the generalised Bakry-\\'Emery Ricci tensor and find utility in proving elliptic and parabolic Harnack-type inequalities as well as general elliptic and parabolic Liouville-type and other global constancy results. Several applications and consequences are presented and discussed.","sentences":["In this paper we prove gradient estimates of both elliptic and parabolic types, specifically, of Souplet-Zhang, Hamilton and Li-Yau types for positive smooth solutions to a class of nonlinear parabolic equations involving the Witten or drifting Laplacian on smooth metric measure spaces.","These estimates are established under various curvature conditions and lower bounds on the generalised Bakry-\\'Emery Ricci tensor and find utility in proving elliptic and parabolic Harnack-type inequalities as well as general elliptic and parabolic Liouville-type and other global constancy results.","Several applications and consequences are presented and discussed."],"url":"http://arxiv.org/abs/2404.01749v1","category":"math.AP"}
{"created":"2024-04-02 09:04:06","title":"Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation","abstract":"Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem-solving efficacy of larger models while significantly accelerating optimization. Specifically, in the domain of interaction-aware trajectory planning for autonomous vehicles, we illustrate that training a smaller prediction network using knowledge distillation speeds up optimization without sacrificing accuracy.","sentences":["Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios.","Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making.","These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control.","However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods.","This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity.","We demonstrate that these refined networks maintain the problem-solving efficacy of larger models while significantly accelerating optimization.","Specifically, in the domain of interaction-aware trajectory planning for autonomous vehicles, we illustrate that training a smaller prediction network using knowledge distillation speeds up optimization without sacrificing accuracy."],"url":"http://arxiv.org/abs/2404.01746v1","category":"cs.RO"}
{"created":"2024-04-02 09:01:58","title":"Unleash the Potential of CLIP for Video Highlight Detection","abstract":"Multimodal and large language models (LLMs) have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications. Among these domains, the video domain has notably benefited from their capabilities. In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in multimodal models. By simply fine-tuning the multimodal encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight Benchmark, to the best of our knowledge.","sentences":["Multimodal and large language models (LLMs) have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications.","Among these domains, the video domain has notably benefited from their capabilities.","In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in multimodal models.","By simply fine-tuning the multimodal encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight Benchmark, to the best of our knowledge."],"url":"http://arxiv.org/abs/2404.01745v1","category":"cs.CV"}
{"created":"2024-04-02 09:01:21","title":"Atom-Level Optical Chemical Structure Recognition with Limited Supervision","abstract":"Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development. Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images. To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision. Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds. Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision. Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction.","sentences":["Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development.","Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images.","To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision.","Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds.","Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision.","Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction."],"url":"http://arxiv.org/abs/2404.01743v1","category":"cs.CV"}
{"created":"2024-04-02 09:00:50","title":"Cosmological study of a symmetric teleparallel gravity model","abstract":"We study a symmetric teleparallel gravity with a Lagrangian of logarithmic form. The full model leads to an accelerated universe and for specific values of the free parameters the Hubble rate reduces to the well-known Dvali-Gabadadze-Porrati model, though the evolution of the gravitational potentials are different. We consider different branches of the logarithmic model, among which are self-accelerated branch and normal branch. The phenomenology of both the background and linear perturbations is discussed, including all the relevant effects on cosmic microwave background radiation (CMB) angular power spectrum, lensing and matter power spectra. To this purpose, we modified the Einstein-Boltzmann code mgcamb. Finally, we derive bounds on the free parameters which are in agreement with early dark energy constraint from CMB and big bang nucleosynthesis constraint on the helium abundance.","sentences":["We study a symmetric teleparallel gravity with a Lagrangian of logarithmic form.","The full model leads to an accelerated universe and for specific values of the free parameters the Hubble rate reduces to the well-known Dvali-Gabadadze-Porrati model, though the evolution of the gravitational potentials are different.","We consider different branches of the logarithmic model, among which are self-accelerated branch and normal branch.","The phenomenology of both the background and linear perturbations is discussed, including all the relevant effects on cosmic microwave background radiation (CMB) angular power spectrum, lensing and matter power spectra.","To this purpose, we modified the Einstein-Boltzmann code mgcamb.","Finally, we derive bounds on the free parameters which are in agreement with early dark energy constraint from CMB and big bang nucleosynthesis constraint on the helium abundance."],"url":"http://arxiv.org/abs/2404.01742v1","category":"gr-qc"}
{"created":"2024-04-02 09:00:45","title":"Intrusion Tolerance for Networked Systems through Two-Level Feedback Control","abstract":"We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.","sentences":["We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem.","On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor.","The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem.","Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems.","We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them.","We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions.","The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems."],"url":"http://arxiv.org/abs/2404.01741v1","category":"cs.DC"}
{"created":"2024-04-02 08:59:58","title":"Weakly-supervised Audio Separation via Bi-modal Semantic Similarity","abstract":"Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training. We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP). Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance. First, we show that our proposed methodology significantly improves the performance of purely unsupervised baselines by reducing the distribution shift between training and test samples. In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the supervised learning performance. Second, we show that we can further improve the performance of the supervised learning itself by 17% if we augment it by our proposed weakly-supervised framework, that enables a powerful semi-supervised framework for audio separation.","sentences":["Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge.","Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training.","However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality.","To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training.","We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP).","Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance.","First, we show that our proposed methodology significantly improves the performance of purely unsupervised baselines by reducing the distribution shift between training and test samples.","In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the supervised learning performance.","Second, we show that we can further improve the performance of the supervised learning itself by 17% if we augment it by our proposed weakly-supervised framework, that enables a powerful semi-supervised framework for audio separation."],"url":"http://arxiv.org/abs/2404.01740v1","category":"cs.SD"}
{"created":"2024-04-02 08:53:51","title":"Transfer Learning from Whisper for Microscopic Intelligibility Prediction","abstract":"Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\\% when fine-tuned to predict listeners' responses. Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction.","sentences":["Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus.","In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses.","State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling.","In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses.","Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\\% when fine-tuned to predict listeners' responses.","Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction."],"url":"http://arxiv.org/abs/2404.01737v1","category":"eess.AS"}
{"created":"2024-04-02 08:50:55","title":"CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling","abstract":"Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations. However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks. However, the cross-item relations have been under-explored in the current multimodal pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling. Specifically, we employ a multimodal encoder to generate image and text representations. Then we leverage both the cross-item contrastive loss (CIC) and individual item's image-text contrastive loss (ITC) as the pre-train objectives. Our method seeks to integrate cross-item relation modeling capability into the multimodal encoder, while preserving the in-depth aligned multimodal semantics. Therefore, even for cold-start items that have no relations, their representations are still relation-aware. Furthermore, to eliminate the potential noise and reduce the computational cost, we harness a relation pruning module to remove the noisy and redundant relations. We apply the item representations extracted by CIRP to the product bundling model ItemKNN, and experiments on three e-commerce datasets demonstrate that CIRP outperforms various leading representation learning methods.","sentences":["Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario.","Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations.","However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items.","Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks.","However, the cross-item relations have been under-explored in the current multimodal pre-train models.","To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling.","Specifically, we employ a multimodal encoder to generate image and text representations.","Then we leverage both the cross-item contrastive loss (CIC) and individual item's image-text contrastive loss (ITC) as the pre-train objectives.","Our method seeks to integrate cross-item relation modeling capability into the multimodal encoder, while preserving the in-depth aligned multimodal semantics.","Therefore, even for cold-start items that have no relations, their representations are still relation-aware.","Furthermore, to eliminate the potential noise and reduce the computational cost, we harness a relation pruning module to remove the noisy and redundant relations.","We apply the item representations extracted by CIRP to the product bundling model ItemKNN, and experiments on three e-commerce datasets demonstrate that CIRP outperforms various leading representation learning methods."],"url":"http://arxiv.org/abs/2404.01735v1","category":"cs.IR"}
{"created":"2024-04-02 08:42:19","title":"A simple collocation-type approach to numerical stochastic homogenization","abstract":"This paper proposes a novel collocation-type numerical stochastic homogenization method for prototypical stochastic homogenization problems with random coefficient fields of small correlation lengths. The presented method is based on a recently introduced localization technique that enforces a super-exponential decay of the basis functions relative to the underlying coarse mesh, resulting in considerable computational savings during the sampling phase. More generally, the collocation-type structure offers a particularly simple and computationally efficient construction in the stochastic setting with minimized communication between the patches where the basis functions of the method are computed. An error analysis that bridges numerical homogenization and the quantitative theory of stochastic homogenization is performed. In a series of numerical experiments, we study the effect of the correlation length and the discretization parameters on the approximation quality of the method.","sentences":["This paper proposes a novel collocation-type numerical stochastic homogenization method for prototypical stochastic homogenization problems with random coefficient fields of small correlation lengths.","The presented method is based on a recently introduced localization technique that enforces a super-exponential decay of the basis functions relative to the underlying coarse mesh, resulting in considerable computational savings during the sampling phase.","More generally, the collocation-type structure offers a particularly simple and computationally efficient construction in the stochastic setting with minimized communication between the patches where the basis functions of the method are computed.","An error analysis that bridges numerical homogenization and the quantitative theory of stochastic homogenization is performed.","In a series of numerical experiments, we study the effect of the correlation length and the discretization parameters on the approximation quality of the method."],"url":"http://arxiv.org/abs/2404.01732v1","category":"math.NA"}
{"created":"2024-04-02 08:40:07","title":"Asymptotics of Language Model Alignment","abstract":"Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\\phi$ that results in a higher expected reward while keeping $\\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\\phi_\\Delta$ that maximizes $E_{\\phi_{\\Delta}} r(y)$ subject to a relative entropy constraint $KL(\\phi_\\Delta || p) \\leq \\Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in terms of relative entropy. To further analyze the properties of alignment methods, we introduce two simplifying assumptions: we let the language model be memoryless, and the reward model be linear. Although these assumptions may not reflect complex real-world scenarios, they enable a precise characterization of the asymptotic behavior of both the best-of-$N$ alignment, and the KL-constrained RL method, in terms of information-theoretic quantities. We prove that the reward of the optimal KL-constrained RL solution satisfies a large deviation principle, and we fully characterize its rate function. We also show that the rate of growth of the scaled cumulants of the reward is characterized by a proper Renyi cross entropy. Finally, we show that best-of-$N$ is asymptotically equivalent to KL-constrained RL solution by proving that their expected rewards are asymptotically equal, and concluding that the two distributions must be close in KL divergence.","sentences":["Let $p$ denote a generative language model.","Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred.","The goal of language model alignment is to alter $p$ to a new distribution $\\phi$ that results in a higher expected reward while keeping $\\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\\phi_\\Delta$ that maximizes $E_{\\phi_{\\Delta}} r(y)$ subject to a relative entropy constraint $KL(\\phi_\\Delta || p)","\\leq \\Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected.","In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution.","We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in terms of relative entropy.","To further analyze the properties of alignment methods, we introduce two simplifying assumptions: we let the language model be memoryless, and the reward model be linear.","Although these assumptions may not reflect complex real-world scenarios, they enable a precise characterization of the asymptotic behavior of both the best-of-$N$ alignment, and the KL-constrained RL method, in terms of information-theoretic quantities.","We prove that the reward of the optimal KL-constrained RL solution satisfies a large deviation principle, and we fully characterize its rate function.","We also show that the rate of growth of the scaled cumulants of the reward is characterized by a proper Renyi cross entropy.","Finally, we show that best-of-$N$ is asymptotically equivalent to KL-constrained RL solution by proving that their expected rewards are asymptotically equal, and concluding that the two distributions must be close in KL divergence."],"url":"http://arxiv.org/abs/2404.01730v1","category":"cs.LG"}
{"created":"2024-04-02 08:38:16","title":"Photodriven Mott insulating heterostructures: A steady-state study of impact ionization processes","abstract":"We investigate the photocurrent and spectral features in a simplified model of a Mott photovoltaic system consisting of a multilayered insulating heterostructure. The central correlated region is coupled to two metallic leads kept at different chemical potentials. A periodic drive applied to the correlated region produces excited doublons and holons across the Mott gap which are then separated by a potential gradient, which mimics the polarization-induced electric field present in oxyde heterostructures. The nonequilibrium Floquet steady-state is addressed by means of dynamical mean-field theory and its Floquet extension, while the so-called auxiliary master equation approach is employed as impurity solver. We find that impact ionization, identified by a kink in the photocurrent as function of the driving frequency, becomes significant and is generally favoured by weak, narrow-band hybridizations to the leads beyond a certain strength of the driving field. On the other hand, in the case of a direct coupling to metallic leads with a flat band, we observe a drastic reduction of impact ionization and of the photocurrent itself.","sentences":["We investigate the photocurrent and spectral features in a simplified model of a Mott photovoltaic system consisting of a multilayered insulating heterostructure.","The central correlated region is coupled to two metallic leads kept at different chemical potentials.","A periodic drive applied to the correlated region produces excited doublons and holons across the Mott gap which are then separated by a potential gradient, which mimics the polarization-induced electric field present in oxyde heterostructures.","The nonequilibrium Floquet steady-state is addressed by means of dynamical mean-field theory and its Floquet extension, while the so-called auxiliary master equation approach is employed as impurity solver.","We find that impact ionization, identified by a kink in the photocurrent as function of the driving frequency, becomes significant and is generally favoured by weak, narrow-band hybridizations to the leads beyond a certain strength of the driving field.","On the other hand, in the case of a direct coupling to metallic leads with a flat band, we observe a drastic reduction of impact ionization and of the photocurrent itself."],"url":"http://arxiv.org/abs/2404.01729v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 08:33:21","title":"Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge","abstract":"We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences. More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping. For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios. Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method.","sentences":["We focus on the generalization ability of the 6-DoF grasp detection method in this paper.","While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures.","To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences.","More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping.","For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios.","Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method."],"url":"http://arxiv.org/abs/2404.01727v1","category":"cs.RO"}
{"created":"2024-04-02 08:21:16","title":"Disentangled Pre-training for Human-Object Interaction Detection","abstract":"Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available. Recent approaches address this issue by pre-training according to pseudo-labels, which align object regions with HOI triplets parsed from image captions. However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process. Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem. First, DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers, respectively. Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task. This facilitates efficient knowledge transfer. Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification. Next, we combine the human instance verb predictions in the same image and impose image-level supervision. The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization. Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories. The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI.","sentences":["Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available.","Recent approaches address this issue by pre-training according to pseudo-labels, which align object regions with HOI triplets parsed from image captions.","However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process.","Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem.","First, DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers, respectively.","Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task.","This facilitates efficient knowledge transfer.","Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification.","Next, we combine the human instance verb predictions in the same image and impose image-level supervision.","The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization.","Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories.","The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI."],"url":"http://arxiv.org/abs/2404.01725v1","category":"cs.CV"}
{"created":"2024-04-02 08:14:27","title":"Self-Improvement Programming for Temporal Knowledge Graph Question Answering","abstract":"Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.","sentences":["Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs).","The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions.","Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively.","Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA).","Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given.","Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers.","To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts.","Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric."],"url":"http://arxiv.org/abs/2404.01720v1","category":"cs.CL"}
{"created":"2024-04-02 08:07:38","title":"AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation","abstract":"Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs. However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps. Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet. Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost. Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation. Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD. Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR).","sentences":["Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs.","However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps.","Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet.","Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost.","Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation.","Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD.","Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR)."],"url":"http://arxiv.org/abs/2404.01717v1","category":"cs.CV"}
{"created":"2024-04-02 08:01:05","title":"Effective internal language model training and fusion for factorized transducer model","abstract":"The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T baseline enhanced with external LM fusion, the proposed model yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction for rare words. The proposed model can achieve superior performance without relying on external language models, rendering it highly efficient for production use-cases. To further improve the performance, we propose a novel and memory-efficient ILM-fusion-aware minimum word error rate (MWER) training method which improves ILM integration significantly.","sentences":["The internal language model (ILM) of the neural transducer has been widely studied.","In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models.","Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction.","However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion.","In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores.","Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets.","Furthermore, when compared to a strong RNN-T baseline enhanced with external LM fusion, the proposed model yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction for rare words.","The proposed model can achieve superior performance without relying on external language models, rendering it highly efficient for production use-cases.","To further improve the performance, we propose a novel and memory-efficient ILM-fusion-aware minimum word error rate (MWER) training method which improves ILM integration significantly."],"url":"http://arxiv.org/abs/2404.01716v1","category":"eess.AS"}
{"created":"2024-04-02 07:57:17","title":"Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning","abstract":"Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.","sentences":["Training deep neural networks is a challenging task.","In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning.","Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like.","Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased.","Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset."],"url":"http://arxiv.org/abs/2404.01714v1","category":"cs.LG"}
{"created":"2024-04-02 07:57:05","title":"Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G","abstract":"Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.","sentences":["Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging.","Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses.","This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications.","This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI).","The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme.","Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories."],"url":"http://arxiv.org/abs/2404.01713v1","category":"cs.CL"}
{"created":"2024-04-02 07:54:18","title":"Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics","abstract":"Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the proposed method significantly reduces the unlearning runtime. Experimental studies demonstrate that the proposed scheme surpasses existing results by orders of magnitude in terms of time and memory costs, while also enhancing accuracy.","sentences":["Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data.","Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency.","However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks.","In this work, we propose a Hessian-free online unlearning method.","We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models.","Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation.","Based on the strategy that recollecting statistics for forgetting data, the proposed method significantly reduces the unlearning runtime.","Experimental studies demonstrate that the proposed scheme surpasses existing results by orders of magnitude in terms of time and memory costs, while also enhancing accuracy."],"url":"http://arxiv.org/abs/2404.01712v1","category":"cs.LG"}
{"created":"2024-04-02 07:49:08","title":"Upsample Guidance: Scale Up Diffusion Models without Training","abstract":"Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.","sentences":["Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio.","However, they encounter difficulties in directly generating high-resolution samples.","Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages.","These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work.","In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process.","Remarkably, this technique does not necessitate any additional training or relying on external models.","We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models.","We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment."],"url":"http://arxiv.org/abs/2404.01709v1","category":"cs.CV"}
{"created":"2024-04-02 07:46:19","title":"Equilibrium and Non-Equilibrium Molecular Dynamics Simulation of Thermo-Osmosis: Enhanced Effects on Polarized Graphene Surfaces","abstract":"Thermo-osmotic flows, generated by applying a thermal gradient along a liquid-solid interface, could be harnessed to convert waste heat into electricity. While this phenomenon has been known for almost a century, there is a crucial need to gain a better understanding of the molecular origins of thermo-osmosis. In this paper, we start by detailing the multiple contributions to thermo-osmosis. We then showcase three approaches to compute the thermo-osmotic coefficient using molecular dynamics; a first method based on the computation of the interfacial enthalpy excess and Derjaguin's theoretical framework, a second approach based on the computation of the interfacial entropy excess using the so-called dry-surface method, and a novel non-equilibrium method to compute the thermo-osmotic coefficient in a periodic channel. We show that the three methods align with each other, in particular for smooth surfaces. In addition, for a polarized graphene-water interface, we observe large variations of thermo-osmotic responses, and multiple changes in flow direction with increasing surface charge. Overall, this study showcases the versatility of osmotic flows and calls for experimental investigation of thermo-osmotic behavior in the vicinity of charged surfaces.","sentences":["Thermo-osmotic flows, generated by applying a thermal gradient along a liquid-solid interface, could be harnessed to convert waste heat into electricity.","While this phenomenon has been known for almost a century, there is a crucial need to gain a better understanding of the molecular origins of thermo-osmosis.","In this paper, we start by detailing the multiple contributions to thermo-osmosis.","We then showcase three approaches to compute the thermo-osmotic coefficient using molecular dynamics; a first method based on the computation of the interfacial enthalpy excess and Derjaguin's theoretical framework, a second approach based on the computation of the interfacial entropy excess using the so-called dry-surface method, and a novel non-equilibrium method to compute the thermo-osmotic coefficient in a periodic channel.","We show that the three methods align with each other, in particular for smooth surfaces.","In addition, for a polarized graphene-water interface, we observe large variations of thermo-osmotic responses, and multiple changes in flow direction with increasing surface charge.","Overall, this study showcases the versatility of osmotic flows and calls for experimental investigation of thermo-osmotic behavior in the vicinity of charged surfaces."],"url":"http://arxiv.org/abs/2404.01708v1","category":"cond-mat.soft"}
{"created":"2024-04-02 07:43:12","title":"Polarity Calibration for Opinion Summarization","abstract":"Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarity Calibration model (PoCa) on two types of opinions summarization tasks: summarizing product reviews and political opinions articles. Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality.","sentences":["Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions.","The challenge of opinions summarization lies in presenting divergent or even conflicting opinions.","We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions.","To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text.","Specifically, we develop a reinforcement training approach for polarity calibration.","This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality.","We evaluate our Polarity Calibration model (PoCa) on two types of opinions summarization tasks: summarizing product reviews and political opinions articles.","Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality."],"url":"http://arxiv.org/abs/2404.01706v1","category":"cs.CL"}
{"created":"2024-04-02 07:16:56","title":"Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior","abstract":"The environmental perception of autonomous vehicles in normal conditions have achieved considerable success in the past decade. However, various unfavourable conditions such as fog, low-light, and motion blur will degrade image quality and pose tremendous threats to the safety of autonomous driving. That is, when applied to degraded images, state-of-the-art visual models often suffer performance decline due to the feature content loss and artifact interference caused by statistical and structural properties disruption of captured images. To address this problem, this work proposes a novel Deep Channel Prior (DCP) for degraded visual recognition. Specifically, we observe that, in the deep representation space of pre-trained models, the channel correlations of degraded features with the same degradation type have uniform distribution even if they have different content and semantics, which can facilitate the mapping relationship learning between degraded and clear representations in high-sparsity feature space. Based on this, a novel plug-and-play Unsupervised Feature Enhancement Module (UFEM) is proposed to achieve unsupervised feature correction, where the multi-adversarial mechanism is introduced in the first stage of UFEM to achieve the latent content restoration and artifact removal in high-sparsity feature space. Then, the generated features are transferred to the second stage for global correlation modulation under the guidance of DCP to obtain high-quality and recognition-friendly features. Evaluations of three tasks and eight benchmark datasets demonstrate that our proposed method can comprehensively improve the performance of pre-trained models in real degradation conditions. The source code is available at https://github.com/liyuhang166/Deep_Channel_Prior","sentences":["The environmental perception of autonomous vehicles in normal conditions have achieved considerable success in the past decade.","However, various unfavourable conditions such as fog, low-light, and motion blur will degrade image quality and pose tremendous threats to the safety of autonomous driving.","That is, when applied to degraded images, state-of-the-art visual models often suffer performance decline due to the feature content loss and artifact interference caused by statistical and structural properties disruption of captured images.","To address this problem, this work proposes a novel Deep Channel","Prior (DCP) for degraded visual recognition.","Specifically, we observe that, in the deep representation space of pre-trained models, the channel correlations of degraded features with the same degradation type have uniform distribution even if they have different content and semantics, which can facilitate the mapping relationship learning between degraded and clear representations in high-sparsity feature space.","Based on this, a novel plug-and-play Unsupervised Feature Enhancement Module (UFEM) is proposed to achieve unsupervised feature correction, where the multi-adversarial mechanism is introduced in the first stage of UFEM to achieve the latent content restoration and artifact removal in high-sparsity feature space.","Then, the generated features are transferred to the second stage for global correlation modulation under the guidance of DCP to obtain high-quality and recognition-friendly features.","Evaluations of three tasks and eight benchmark datasets demonstrate that our proposed method can comprehensively improve the performance of pre-trained models in real degradation conditions.","The source code is available at https://github.com/liyuhang166/Deep_Channel_Prior"],"url":"http://arxiv.org/abs/2404.01703v1","category":"cs.CV"}
{"created":"2024-04-02 07:09:44","title":"On the Role of Summary Content Units in Text Summarization Evaluation","abstract":"At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when ranking short summaries, but may not help as much when ranking systems or longer summaries.","sentences":["At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs).","These SCUs are concise sentences that decompose a summary into small facts.","Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems.","Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs).","However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages?","ii) Under which conditions are SCUs (or their approximations) offering the most value?","In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively.","We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs.","We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when ranking short summaries, but may not help as much when ranking systems or longer summaries."],"url":"http://arxiv.org/abs/2404.01701v1","category":"cs.CL"}
{"created":"2024-04-02 07:09:29","title":"MotionChain: Conversational Motion Controllers via Multimodal Prompts","abstract":"Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.","sentences":["Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context.","However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models.","By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems.","In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts.","Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model.","By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts.","Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans."],"url":"http://arxiv.org/abs/2404.01700v1","category":"cs.CV"}
{"created":"2024-04-02 06:53:45","title":"HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning","abstract":"Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks. While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task. However, each of the tasks is of small size, and such a single-task strategy hinders the models' performance and generalization ability. As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem. In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures. In particular, we first construct a standard structure-based multi-task benchmark called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public datasets. Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture heterogeneous relationships between different atoms. Besides, HeMeNet can achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations on our benchmark verify the effectiveness of multi-task learning, and our model generally surpasses state-of-the-art models.","sentences":["Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks.","While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task.","However, each of the tasks is of small size, and such a single-task strategy hinders the models' performance and generalization ability.","As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem.","In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures.","In particular, we first construct a standard structure-based multi-task benchmark called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public datasets.","Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture heterogeneous relationships between different atoms.","Besides, HeMeNet can achieve task-specific learning via the task-aware readout mechanism.","Extensive evaluations on our benchmark verify the effectiveness of multi-task learning, and our model generally surpasses state-of-the-art models."],"url":"http://arxiv.org/abs/2404.01693v1","category":"cs.LG"}
{"created":"2024-04-02 06:52:31","title":"Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss","abstract":"In real-world scenarios, image recognition tasks, such as semantic segmentation and object detection, often pose greater challenges due to the lack of information available within low-resolution (LR) content. Image super-resolution (SR) is one of the promising solutions for addressing the challenges. However, due to the ill-posed property of SR, it is challenging for typical SR methods to restore task-relevant high-frequency contents, which may dilute the advantage of utilizing the SR method. Therefore, in this paper, we propose Super-Resolution for Image Recognition (SR4IR) that effectively guides the generation of SR images beneficial to achieving satisfactory image recognition performance when processing LR images. The critical component of our SR4IR is the task-driven perceptual (TDP) loss that enables the SR network to acquire task-specific knowledge from a network tailored for a specific task. Moreover, we propose a cross-quality patch mix and an alternate training framework that significantly enhances the efficacy of the TDP loss by addressing potential problems when employing the TDP loss. Through extensive experiments, we demonstrate that our SR4IR achieves outstanding task performance by generating SR images useful for a specific image recognition task, including semantic segmentation, object detection, and image classification. The implementation code is available at https://github.com/JaehaKim97/SR4IR.","sentences":["In real-world scenarios, image recognition tasks, such as semantic segmentation and object detection, often pose greater challenges due to the lack of information available within low-resolution (LR) content.","Image super-resolution (SR) is one of the promising solutions for addressing the challenges.","However, due to the ill-posed property of SR, it is challenging for typical SR methods to restore task-relevant high-frequency contents, which may dilute the advantage of utilizing the SR method.","Therefore, in this paper, we propose Super-Resolution for Image Recognition (SR4IR) that effectively guides the generation of SR images beneficial to achieving satisfactory image recognition performance when processing LR images.","The critical component of our SR4IR is the task-driven perceptual (TDP) loss that enables the SR network to acquire task-specific knowledge from a network tailored for a specific task.","Moreover, we propose a cross-quality patch mix and an alternate training framework that significantly enhances the efficacy of the TDP loss by addressing potential problems when employing the TDP loss.","Through extensive experiments, we demonstrate that our SR4IR achieves outstanding task performance by generating SR images useful for a specific image recognition task, including semantic segmentation, object detection, and image classification.","The implementation code is available at https://github.com/JaehaKim97/SR4IR."],"url":"http://arxiv.org/abs/2404.01692v1","category":"cs.CV"}
{"created":"2024-04-02 06:48:33","title":"A Lightweight Security Solution for Mitigation of Hatchetman Attack in RPL-based 6LoWPAN","abstract":"In recent times, the Internet of Things (IoT) has a significant rise in industries, and we live in the era of Industry 4.0, where each device is connected to the Internet from small to big. These devices are Artificial Intelligence (AI) enabled and are capable of perspective analytics. By 2023, it's anticipated that over 14 billion smart devices will be available on the Internet. These applications operate in a wireless environment where memory, power, and other resource limitations apply to the nodes. In addition, the conventional routing method is ineffective in networks with limited resource devices, lossy links, and slow data rates. Routing Protocol for Low Power and Lossy Networks (RPL), a new routing protocol for such networks, was proposed by the IETF's ROLL group. RPL operates in two modes: Storing and Non-Storing. In Storing mode, each node have the information to reach to other node. In Non-Storing mode, the routing information lies with the root node only. The attacker may exploit the Non-Storing feature of the RPL. When the root node transmits User Datagram Protocol~(UDP) or control message packet to the child nodes, the routing information is stored in the extended header of the IPv6 packet. The attacker may modify the address from the source routing header which leads to Denial of Service (DoS) attack. This attack is RPL specific which is known as Hatchetman attack. This paper shows significant degradation in terms of network performance when an attacker exploits this feature. We also propose a lightweight mitigation of Hatchetman attack using game theoretic approach to detect the Hatchetman attack in IoT.","sentences":["In recent times, the Internet of Things (IoT) has a significant rise in industries, and we live in the era of Industry 4.0, where each device is connected to the Internet from small to big.","These devices are Artificial Intelligence (AI) enabled and are capable of perspective analytics.","By 2023, it's anticipated that over 14 billion smart devices will be available on the Internet.","These applications operate in a wireless environment where memory, power, and other resource limitations apply to the nodes.","In addition, the conventional routing method is ineffective in networks with limited resource devices, lossy links, and slow data rates.","Routing Protocol for Low Power and Lossy Networks (RPL), a new routing protocol for such networks, was proposed by the IETF's ROLL group.","RPL operates in two modes: Storing and Non-Storing.","In Storing mode, each node have the information to reach to other node.","In Non-Storing mode, the routing information lies with the root node only.","The attacker may exploit the Non-Storing feature of the RPL.","When the root node transmits User Datagram Protocol~(UDP) or control message packet to the child nodes, the routing information is stored in the extended header of the IPv6 packet.","The attacker may modify the address from the source routing header which leads to Denial of Service (DoS) attack.","This attack is RPL specific which is known as Hatchetman attack.","This paper shows significant degradation in terms of network performance when an attacker exploits this feature.","We also propose a lightweight mitigation of Hatchetman attack using game theoretic approach to detect the Hatchetman attack in IoT."],"url":"http://arxiv.org/abs/2404.01689v1","category":"cs.CR"}
{"created":"2024-04-02 06:42:14","title":"A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling","abstract":"Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70.84% accuracy for CIFAR100) with less than 10M parameters and up to 3.45x speed-up of searching time, thereby making it suitable for embedded applications.","sentences":["Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations.","Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications.","Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed.","Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling.","Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection.","The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70.84% accuracy for CIFAR100) with less than 10M parameters and up to 3.45x speed-up of searching time, thereby making it suitable for embedded applications."],"url":"http://arxiv.org/abs/2404.01685v1","category":"cs.NE"}
{"created":"2024-04-02 06:28:44","title":"Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation","abstract":"Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios. Besides, we observe that GFaiR is faithful to its reasoning process.","sentences":["Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks.","However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language.","This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue.","As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability.","To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation.","Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation.","Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios.","Besides, we observe that GFaiR is faithful to its reasoning process."],"url":"http://arxiv.org/abs/2404.01677v1","category":"cs.AI"}
{"created":"2024-04-02 06:24:21","title":"A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed. However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning. The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world. Therefore, we propose a universal knowledge embedded contrastive learning framework (KnowCL) for supervised, unsupervised, and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones. We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning. The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time. Furthermore, we design a new loss function, which can adaptively fuse the supervised loss and unsupervised loss, enhancing the learning performance. This proposed new classification paradigm shows great potentials in exploring for HSI classification technology. The code can be accessed at https://github.com/quanweiliu/KnowCL.","sentences":["Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed.","However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning.","The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world.","Therefore, we propose a universal knowledge embedded contrastive learning framework (KnowCL) for supervised, unsupervised, and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones.","We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning.","The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time.","Furthermore, we design a new loss function, which can adaptively fuse the supervised loss and unsupervised loss, enhancing the learning performance.","This proposed new classification paradigm shows great potentials in exploring for HSI classification technology.","The code can be accessed at https://github.com/quanweiliu/KnowCL."],"url":"http://arxiv.org/abs/2404.01673v1","category":"cs.CV"}
{"created":"2024-04-02 06:18:41","title":"How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era","abstract":"The debate around vaccines has been going on for decades, but the COVID-19 pandemic showed how crucial it is to understand and mitigate anti-vaccine sentiments. While the pandemic may be over, it is still important to understand how the pandemic affected the anti-vaccine discourse, and whether the arguments against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also changed due to the pandemic. This study attempts to answer these questions through a large-scale study of anti-vaccine posts on Twitter. Almost all prior works that utilized social media to understand anti-vaccine opinions considered only the three broad stances of Anti-Vax, Pro-Vax, and Neutral. There has not been any effort to identify the specific reasons/concerns behind the anti-vax sentiments (e.g., side-effects, conspiracy theories, political reasons) on social media at scale. In this work, we propose two novel methods for classifying tweets into 11 different anti-vax concerns -- a discriminative approach (entailment-based) and a generative approach (based on instruction tuning of LLMs) -- which outperform several strong baselines. We then apply this classifier on anti-vaccine tweets posted over a 5-year period (Jan 2018 - Jan 2023) to understand how the COVID-19 pandemic has impacted the anti-vaccine concerns among the masses. We find that the pandemic has made the anti-vaccine discourse far more complex than in the pre-COVID times, and increased the variety of concerns being voiced. Alarmingly, we find that concerns about COVID vaccines are now being projected onto the non-COVID vaccines, thus making more people hesitant in taking vaccines in the post-COVID era.","sentences":["The debate around vaccines has been going on for decades, but the COVID-19 pandemic showed how crucial it is to understand and mitigate anti-vaccine sentiments.","While the pandemic may be over, it is still important to understand how the pandemic affected the anti-vaccine discourse, and whether the arguments against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also changed due to the pandemic.","This study attempts to answer these questions through a large-scale study of anti-vaccine posts on Twitter.","Almost all prior works that utilized social media to understand anti-vaccine opinions considered only the three broad stances of Anti-Vax, Pro-Vax, and Neutral.","There has not been any effort to identify the specific reasons/concerns behind the anti-vax sentiments (e.g., side-effects, conspiracy theories, political reasons) on social media at scale.","In this work, we propose two novel methods for classifying tweets into 11 different anti-vax concerns -- a discriminative approach (entailment-based) and a generative approach (based on instruction tuning of LLMs) -- which outperform several strong baselines.","We then apply this classifier on anti-vaccine tweets posted over a 5-year period (Jan 2018 - Jan 2023) to understand how the COVID-19 pandemic has impacted the anti-vaccine concerns among the masses.","We find that the pandemic has made the anti-vaccine discourse far more complex than in the pre-COVID times, and increased the variety of concerns being voiced.","Alarmingly, we find that concerns about COVID vaccines are now being projected onto the non-COVID vaccines, thus making more people hesitant in taking vaccines in the post-COVID era."],"url":"http://arxiv.org/abs/2404.01669v1","category":"cs.SI"}
{"created":"2024-04-02 17:49:00","title":"Emergence of collective spectral features in finite arrays of dielectric rods","abstract":"Periodic optical structures, such as diffraction grating and numerous photonic crystals, are one of the staples of modern nanophotonics for the manipulation of electromagnetic radiation. The array of subwavelength dielectric rods is one of the simplest platforms, which, despite its simplicity exhibits extraordinary wave phenomena, such as diffraction anomalies and narrow reflective resonances. Despite the well-documented properties of infinite periodic systems, the behavior of these diffractive effects in systems incorporating a finite number of elements is studied to a far lesser extent. Here we study theoretically and numerically the evolution of collective spectral features in finite arrays of dielectric rods. We develop an analytical model of light scattering by a finite array of circular rods based on the coupled dipoles approximation and analyze the spectral features of finite arrays within the developed model. Finally, we validate the results of the analytical model using full-wave numerical simulations.","sentences":["Periodic optical structures, such as diffraction grating and numerous photonic crystals, are one of the staples of modern nanophotonics for the manipulation of electromagnetic radiation.","The array of subwavelength dielectric rods is one of the simplest platforms, which, despite its simplicity exhibits extraordinary wave phenomena, such as diffraction anomalies and narrow reflective resonances.","Despite the well-documented properties of infinite periodic systems, the behavior of these diffractive effects in systems incorporating a finite number of elements is studied to a far lesser extent.","Here we study theoretically and numerically the evolution of collective spectral features in finite arrays of dielectric rods.","We develop an analytical model of light scattering by a finite array of circular rods based on the coupled dipoles approximation and analyze the spectral features of finite arrays within the developed model.","Finally, we validate the results of the analytical model using full-wave numerical simulations."],"url":"http://arxiv.org/abs/2404.02137v1","category":"physics.optics"}
{"created":"2024-04-02 17:44:45","title":"Quantum bistability at the interplay between collective and individual decay","abstract":"We study driven collective radiation of an ensemble of atoms placed inside a cavity, accounting for individual-atom emission to free space modes. We find that the steady state exhibits a dissipative phase transition, formed by a mixture of two collective quantum states corresponding to a bistable mean-field solution. One of these states is entangled and closely resembles a coherently radiating spin state (CRSS) -- the solution obtained by neglecting individual decay (Dicke superradiance) -- allowing us to analytically find the optimally achievable spin squeezing. We predict quantum switching between the two states, verified by quantum trajectories simulations. The switching rate tends to vanish with the atom number, as the Liouvillan gap closes. Remarkably, this suggests that the system may reside in an entangled CRSS-like state associated with correlated Dicke physics, even in the presence of decorrelating individual decay. This opens a path for a systematic study of the interplay between collective and individual decay, in both experiments and theory.","sentences":["We study driven collective radiation of an ensemble of atoms placed inside a cavity, accounting for individual-atom emission to free space modes.","We find that the steady state exhibits a dissipative phase transition, formed by a mixture of two collective quantum states corresponding to a bistable mean-field solution.","One of these states is entangled and closely resembles a coherently radiating spin state (CRSS) -- the solution obtained by neglecting individual decay (Dicke superradiance) -- allowing us to analytically find the optimally achievable spin squeezing.","We predict quantum switching between the two states, verified by quantum trajectories simulations.","The switching rate tends to vanish with the atom number, as the Liouvillan gap closes.","Remarkably, this suggests that the system may reside in an entangled CRSS-like state associated with correlated Dicke physics, even in the presence of decorrelating individual decay.","This opens a path for a systematic study of the interplay between collective and individual decay, in both experiments and theory."],"url":"http://arxiv.org/abs/2404.02134v1","category":"quant-ph"}
{"created":"2024-04-02 17:18:48","title":"GINopic: Topic Modeling with Graph Isomorphism Network","abstract":"Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.","sentences":["Topic modeling is a widely used approach for analyzing and exploring large document collections.","Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling.","However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words.","In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words.","By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling."],"url":"http://arxiv.org/abs/2404.02115v1","category":"cs.CL"}
{"created":"2024-04-02 16:52:03","title":"Analysis Facilities White Paper","abstract":"This white paper presents the current status of the R&D for Analysis Facilities (AFs) and attempts to summarize the views on the future direction of these facilities. These views have been collected through the High Energy Physics (HEP) Software Foundation's (HSF) Analysis Facilities forum, established in March 2022, the Analysis Ecosystems II workshop, that took place in May 2022, and the WLCG/HSF pre-CHEP workshop, that took place in May 2023. The paper attempts to cover all the aspects of an analysis facility.","sentences":["This white paper presents the current status of the R&D for Analysis Facilities (AFs) and attempts to summarize the views on the future direction of these facilities.","These views have been collected through the High Energy Physics (HEP) Software Foundation's (HSF) Analysis Facilities forum, established in March 2022, the Analysis Ecosystems II workshop, that took place in May 2022, and the WLCG/HSF pre-CHEP workshop, that took place in May 2023.","The paper attempts to cover all the aspects of an analysis facility."],"url":"http://arxiv.org/abs/2404.02100v1","category":"hep-ex"}
{"created":"2024-04-02 16:32:36","title":"Extreme plasmons","abstract":"Nanosciences largely rely on plasmons which are quasiparticles constituted by collective oscillations of quantum electron gas composed of conduction band electrons that occupy discrete quantum states. Our work has introduced non-perturbative plasmons with oscillation amplitudes that approach the extreme limit set by breakdown in characteristic coherence. In contrast, conventional plasmons are small-amplitude oscillations. Controlled excitation of extreme plasmons modeled in our work unleashes unprecedented Petavolts per meter fields. In this work, an analytical model of this new class of plasmons is developed based on quantum kinetic framework. A controllable extreme plasmon, the surface \"crunch-in\" plasmon, is modeled here using a modified independent electron approximation which takes into account the quantum oscillation frequency. Key characteristics of such realizable extreme plasmons that unlock unparalleled possibilities, are obtained.","sentences":["Nanosciences largely rely on plasmons which are quasiparticles constituted by collective oscillations of quantum electron gas composed of conduction band electrons that occupy discrete quantum states.","Our work has introduced non-perturbative plasmons with oscillation amplitudes that approach the extreme limit set by breakdown in characteristic coherence.","In contrast, conventional plasmons are small-amplitude oscillations.","Controlled excitation of extreme plasmons modeled in our work unleashes unprecedented Petavolts per meter fields.","In this work, an analytical model of this new class of plasmons is developed based on quantum kinetic framework.","A controllable extreme plasmon, the surface \"crunch-in\" plasmon, is modeled here using a modified independent electron approximation which takes into account the quantum oscillation frequency.","Key characteristics of such realizable extreme plasmons that unlock unparalleled possibilities, are obtained."],"url":"http://arxiv.org/abs/2404.02087v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 16:21:42","title":"The Online Observation Quality System Implementation for the ASTRI Mini-Array Project","abstract":"The ASTRI Mini-Array project, led by the Italian National Institute for Astrophysics, aims to construct and operate nine Imaging Atmospheric Cherenkov Telescopes for high-energy gamma-ray source study and stellar intensity interferometry. Located at the Teide Astronomical Observatory in Tenerife, the project's software is essential for remote operation, emphasizing the need for prompt feedback on observations. This contribution introduces the Online Observation Quality System (OOQS) as part of the Supervisory Control And Data Acquisition (SCADA) software. OOQS performs real-time data quality checks on data from Cherenkov cameras and Intensity Interferometry instruments. It provides feedback to SCADA and operators, highlighting abnormal conditions and ensuring quick corrective actions for optimal observations. Results are archived for operator visualization and further analysis. The OOQS data quality pipeline prototype utilizes a distributed application with three main components to handle the maximum array data rate of 1.15 Gb/s. The first is a Kafka consumer that manages the data stream from the Array Data Acquisition System through Apache Kafka, handling the data serialization and deserialization involved in the transmission. The data stream is divided into batches of data written in files. The second component monitors new files and conducts analyses using the Slurm workload scheduler, leveraging its parallel processing capabilities and scalability. Finally, the process results are collected by the last component and stored in the Quality Archive.","sentences":["The ASTRI Mini-Array project, led by the Italian National Institute for Astrophysics, aims to construct and operate nine Imaging Atmospheric Cherenkov Telescopes for high-energy gamma-ray source study and stellar intensity interferometry.","Located at the Teide Astronomical Observatory in Tenerife, the project's software is essential for remote operation, emphasizing the need for prompt feedback on observations.","This contribution introduces the Online Observation Quality System (OOQS) as part of the Supervisory Control And Data Acquisition (SCADA) software.","OOQS performs real-time data quality checks on data from Cherenkov cameras and Intensity Interferometry instruments.","It provides feedback to SCADA and operators, highlighting abnormal conditions and ensuring quick corrective actions for optimal observations.","Results are archived for operator visualization and further analysis.","The OOQS data quality pipeline prototype utilizes a distributed application with three main components to handle the maximum array data rate of 1.15 Gb/s. The first is a Kafka consumer that manages the data stream from the Array Data Acquisition System through Apache Kafka, handling the data serialization and deserialization involved in the transmission.","The data stream is divided into batches of data written in files.","The second component monitors new files and conducts analyses using the Slurm workload scheduler, leveraging its parallel processing capabilities and scalability.","Finally, the process results are collected by the last component and stored in the Quality Archive."],"url":"http://arxiv.org/abs/2404.02075v1","category":"astro-ph.IM"}
{"created":"2024-04-02 15:21:13","title":"Search for $C$-even states decaying to $D_{s}^{\\pm}D_{s}^{*\\mp}$ with masses between $4.08$ and $4.32$ $\\rm GeV/{\\it c}^{2}$","abstract":"Six $C$-even states, denoted as $X$, with quantum numbers $J^{PC}=0^{-+}$, $1^{\\pm+}$, or $2^{\\pm+}$, are searched for via the $e^+e^-\\to\\gamma D_{s}^{\\pm}D_{s}^{*\\mp}$ process using $(1667.39\\pm8.84)~\\mathrm{pb}^{-1}$ of $e^+e^-$ collision data collected with the BESIII detector operating at the BEPCII storage ring at center-of-mass energy of $\\sqrt{s}=(4681.92\\pm0.30)~\\mathrm{MeV}$. No statistically significant signal is observed in the mass range from $4.08$ to $4.32~\\mathrm{GeV}/c^{2}$. The upper limits of $\\sigma[e^+e^-\\to\\gamma X]\\cdot \\mathcal{B}[X \\to D_{s}^{\\pm}D_{s}^{*\\mp}]$ at a $90\\%$ confidence level are determined.","sentences":["Six $C$-even states, denoted as $X$, with quantum numbers $J^{PC}=0^{-+}$, $1^{\\pm+}$, or $2^{\\pm+}$, are searched for via the $e^+e^-\\to\\gamma D_{s}^{\\pm}D_{s}^{*\\mp}$ process using $(1667.39\\pm8.84)~\\mathrm{pb}^{-1}$ of $e^+e^-$ collision data collected with the BESIII detector operating at the BEPCII storage ring at center-of-mass energy of $\\sqrt{s}=(4681.92\\pm0.30)~\\mathrm{MeV}$. No statistically significant signal is observed in the mass range from $4.08$ to $4.32~\\mathrm{GeV}/c^{2}$. The upper limits of $\\sigma[e^+e^-\\to\\gamma X]\\cdot \\mathcal{B}[X \\to D_{s}^{\\pm}D_{s}^{*\\mp}]$ at a $90\\%$ confidence level are determined."],"url":"http://arxiv.org/abs/2404.02033v1","category":"hep-ex"}
{"created":"2024-04-02 13:22:19","title":"First Light Curve Analysis of NSVS 8294044, V1023 Her, and V1397 Her Contact Binary Systems","abstract":"The first photometric light curve investigation of the NSVS 8294044, V1023 Her, and V1397 Her binary systems is presented. We used ground-based observations for the NSVS 8294044 system and Transiting Exoplanet Survey Satellite (TESS) data for V1023 Her and V1397 Her. The primary and secondary times of minima were extracted from all the data, and by collecting the literature, a new ephemeris was computed for each system. Linear fits for the O-C diagrams were conducted using the Markov chain Monte Carlo (MCMC) method. Light curve solutions were performed using the PHysics Of Eclipsing BinariEs (PHOEBE) Python code and the MCMC approach. The systems were found to be contact binary stars based on the fillout factor and mass ratio. V1023 Her showed the O'Connell effect, and a cold starspot on the secondary component was required for the light curve solution. The absolute parameters of the system were estimated based on an empirical relationship between orbital period and mass. We presented a new T-M equation based on a sample of 428 contact binary systems and found that our three target systems were in good agreement with the fit. The positions of the systems were also depicted on the M-L, M-R, q-L_{ratio}, and M_{tot}-J_0 diagrams in the logarithmic scales.","sentences":["The first photometric light curve investigation of the NSVS 8294044, V1023 Her, and V1397 Her binary systems is presented.","We used ground-based observations for the NSVS 8294044 system and Transiting Exoplanet Survey Satellite (TESS) data for V1023 Her and V1397 Her.","The primary and secondary times of minima were extracted from all the data, and by collecting the literature, a new ephemeris was computed for each system.","Linear fits for the O-C diagrams were conducted using the Markov chain Monte Carlo (MCMC) method.","Light curve solutions were performed using the PHysics Of Eclipsing BinariEs (PHOEBE)","Python code and the MCMC approach.","The systems were found to be contact binary stars based on the fillout factor and mass ratio.","V1023","Her showed the O'Connell effect, and a cold starspot on the secondary component was required for the light curve solution.","The absolute parameters of the system were estimated based on an empirical relationship between orbital period and mass.","We presented a new T-M equation based on a sample of 428 contact binary systems and found that our three target systems were in good agreement with the fit.","The positions of the systems were also depicted on the M-L, M-R, q-L_{ratio}, and M_{tot}-J_0 diagrams in the logarithmic scales."],"url":"http://arxiv.org/abs/2404.01928v1","category":"astro-ph.SR"}
{"created":"2024-04-02 12:55:56","title":"A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball","abstract":"In this study, a temporal graph model is designed to model the behavior of collective sports teams based on the networks of player interactions. The main motivation for the model is to integrate the temporal dimension into the analysis of players' passing networks in order to gain deeper insights into the dynamics of system behavior, particularly how a system exploits the degeneracy property to self-regulate. First, the temporal graph model and the entropy measures used to assess the complexity of the dynamics of the network structure are introduced and illustrated. Second, an experiment using basketball data is conducted to investigate the relationship between the complexity level and team performance. This is accomplished by examining the correlations between the entropy measures in a team's behavior and the team's final performance, as well as the link between the relative score compared to that of the opponent and the entropy in the team's behavior. Results indicate positive correlations between entropy measures and final team performance, and threshold values of relative score associated with changes in team behavior -- thereby revealing common and unique team signatures. From a complexity science perspective, the model proves useful for identifying key performance factors in team sports and for studying the effects of given constraints on the exploitation of degeneracy to organize team behavior through various network structures. Future research can easily extend the model and apply it to other types of social networks.","sentences":["In this study, a temporal graph model is designed to model the behavior of collective sports teams based on the networks of player interactions.","The main motivation for the model is to integrate the temporal dimension into the analysis of players' passing networks in order to gain deeper insights into the dynamics of system behavior, particularly how a system exploits the degeneracy property to self-regulate.","First, the temporal graph model and the entropy measures used to assess the complexity of the dynamics of the network structure are introduced and illustrated.","Second, an experiment using basketball data is conducted to investigate the relationship between the complexity level and team performance.","This is accomplished by examining the correlations between the entropy measures in a team's behavior and the team's final performance, as well as the link between the relative score compared to that of the opponent and the entropy in the team's behavior.","Results indicate positive correlations between entropy measures and final team performance, and threshold values of relative score associated with changes in team behavior -- thereby revealing common and unique team signatures.","From a complexity science perspective, the model proves useful for identifying key performance factors in team sports and for studying the effects of given constraints on the exploitation of degeneracy to organize team behavior through various network structures.","Future research can easily extend the model and apply it to other types of social networks."],"url":"http://arxiv.org/abs/2404.01909v1","category":"cs.DM"}
{"created":"2024-04-02 11:27:54","title":"The orbital parameters of the del Cep inner binary system determined using 2019 HARPS-N spectroscopic data","abstract":"An inner companion has recently been discovered orbiting the prototype of classical Cepheids, delta Cep, whose orbital parameters are still not fully constrained. We collected new precise radial velocity measurements of delta Cep in 2019 using the HARPS-N spectrograph mounted at the Telescopio Nazionale Galileo. Using these radial velocity measurements, we aimed to improve the orbital parameters of the system. We considered a template available in the literature as a reference for the radial velocity curve of the pulsation of the star. We then calculated the residuals between our global dataset (composed of the new 2019 observations plus data from the literature) and the template as a function of the pulsation phase and the barycentric Julian date. This provides the orbital velocity of the Cepheid component. Using a Bayesian tool, we derived the orbital parameters of the system. Considering priors based on already published Gaia constraints, we find for the orbital period a maximum a posteriori probability of Porb=9.32+/-0.03 years (uncertainties correspond to the 95% highest density probability interval), and we obtain an eccentricity e=0.71+/-0.02, a semimajor axis a=0.029 +/-0.003 arcsecond, and a center-of-mass velocity V0=-17.28+/-0.08 km/s, among other parameters. In this short analysis we derive the orbital parameters of the delta Cep inner binary system and provide a cleaned radial velocity curve of the pulsation of the star, which will be used to study its Baade-Wesselink projection factor in a future publication.","sentences":["An inner companion has recently been discovered orbiting the prototype of classical Cepheids, delta Cep, whose orbital parameters are still not fully constrained.","We collected new precise radial velocity measurements of delta Cep in 2019 using the HARPS-N spectrograph mounted at the Telescopio Nazionale Galileo.","Using these radial velocity measurements, we aimed to improve the orbital parameters of the system.","We considered a template available in the literature as a reference for the radial velocity curve of the pulsation of the star.","We then calculated the residuals between our global dataset (composed of the new 2019 observations plus data from the literature) and the template as a function of the pulsation phase and the barycentric Julian date.","This provides the orbital velocity of the Cepheid component.","Using a Bayesian tool, we derived the orbital parameters of the system.","Considering priors based on already published Gaia constraints, we find for the orbital period a maximum a posteriori probability of Porb=9.32+/-0.03 years (uncertainties correspond to the 95% highest density probability interval), and we obtain an eccentricity e=0.71+/-0.02, a semimajor axis a=0.029 +/-0.003 arcsecond, and a center-of-mass velocity V0=-17.28+/-0.08 km/s, among other parameters.","In this short analysis we derive the orbital parameters of the delta Cep inner binary system and provide a cleaned radial velocity curve of the pulsation of the star, which will be used to study its Baade-Wesselink projection factor in a future publication."],"url":"http://arxiv.org/abs/2404.01851v1","category":"astro-ph.SR"}
{"created":"2024-04-02 10:12:37","title":"Spectral Map: Embedding Slow Kinetics in Collective Variables","abstract":"The dynamics of physical systems that require high-dimensional representation can often be captured in a few meaningful degrees of freedom called collective variables (CVs). However, identifying CVs is challenging and constitutes a fundamental problem in physical chemistry. This problem is even more pronounced when CVs information about slow kinetics related to rare transitions between long-lived metastable states. To address this issue, we propose an unsupervised deep-learning method called spectral map. Our method constructs slow CVs by maximizing the spectral gap between slow and fast eigenvalues of a transition matrix estimated by an anisotropic diffusion kernel. We demonstrate our method in several high-dimensional reversible folding processes.","sentences":["The dynamics of physical systems that require high-dimensional representation can often be captured in a few meaningful degrees of freedom called collective variables (CVs).","However, identifying CVs is challenging and constitutes a fundamental problem in physical chemistry.","This problem is even more pronounced when CVs information about slow kinetics related to rare transitions between long-lived metastable states.","To address this issue, we propose an unsupervised deep-learning method called spectral map.","Our method constructs slow CVs by maximizing the spectral gap between slow and fast eigenvalues of a transition matrix estimated by an anisotropic diffusion kernel.","We demonstrate our method in several high-dimensional reversible folding processes."],"url":"http://arxiv.org/abs/2404.01809v1","category":"physics.chem-ph"}
{"created":"2024-04-02 09:52:18","title":"A Feature Dataset of Microservices-based Systems","abstract":"Microservice architecture has become a dominant architectural style in the service-oriented software industry. Poor practices in the design and development of microservices are called microservice bad smells. In microservice bad smells research, the detection of these bad smells relies on feature data from microservices. However, there is a lack of an appropriate open-source microservice feature dataset. The availability of such datasets may contribute to the detection of microservice bad smells unexpectedly. To address this research gap, this paper collects a number of open-source microservice systems utilizing Spring Cloud. Additionally, feature metrics are established based on the architecture and interactions of Spring Boot style microservices. And an extraction program is developed. The program is then applied to the collected open-source microservice systems, extracting the necessary information, and undergoing manual verification to create an open-source feature dataset specific to microservice systems using Spring Cloud. The dataset is made available through a CSV file. We believe that both the extraction program and the dataset have the potential to contribute to the study of micro-service bad smells.","sentences":["Microservice architecture has become a dominant architectural style in the service-oriented software industry.","Poor practices in the design and development of microservices are called microservice bad smells.","In microservice bad smells research, the detection of these bad smells relies on feature data from microservices.","However, there is a lack of an appropriate open-source microservice feature dataset.","The availability of such datasets may contribute to the detection of microservice bad smells unexpectedly.","To address this research gap, this paper collects a number of open-source microservice systems utilizing Spring Cloud.","Additionally, feature metrics are established based on the architecture and interactions of Spring Boot style microservices.","And an extraction program is developed.","The program is then applied to the collected open-source microservice systems, extracting the necessary information, and undergoing manual verification to create an open-source feature dataset specific to microservice systems using Spring Cloud.","The dataset is made available through a CSV file.","We believe that both the extraction program and the dataset have the potential to contribute to the study of micro-service bad smells."],"url":"http://arxiv.org/abs/2404.01789v1","category":"cs.SE"}
{"created":"2024-04-02 09:49:07","title":"Can Humans Identify Domains?","abstract":"Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models.","sentences":["Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance.","The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document.","We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter).","We publish our annotations in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity.","Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task.","With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them.","By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models."],"url":"http://arxiv.org/abs/2404.01785v1","category":"cs.CL"}
{"created":"2024-04-02 09:04:56","title":"Global Mapping of Exposure and Physical Vulnerability Dynamics in Least Developed Countries using Remote Sensing and Machine Learning","abstract":"As the world marked the midterm of the Sendai Framework for Disaster Risk Reduction 2015-2030, many countries are still struggling to monitor their climate and disaster risk because of the expensive large-scale survey of the distribution of exposure and physical vulnerability and, hence, are not on track in reducing risks amidst the intensifying effects of climate change. We present an ongoing effort in mapping this vital information using machine learning and time-series remote sensing from publicly available Sentinel-1 SAR GRD and Sentinel-2 Harmonized MSI. We introduce the development of \"OpenSendaiBench\" consisting of 47 countries wherein most are least developed (LDCs), trained ResNet-50 deep learning models, and demonstrated the region of Dhaka, Bangladesh by mapping the distribution of its informal constructions. As a pioneering effort in auditing global disaster risk over time, this paper aims to advance the area of large-scale risk quantification in informing our collective long-term efforts in reducing climate and disaster risk.","sentences":["As the world marked the midterm of the Sendai Framework for Disaster Risk Reduction 2015-2030, many countries are still struggling to monitor their climate and disaster risk because of the expensive large-scale survey of the distribution of exposure and physical vulnerability and, hence, are not on track in reducing risks amidst the intensifying effects of climate change.","We present an ongoing effort in mapping this vital information using machine learning and time-series remote sensing from publicly available Sentinel-1 SAR GRD and Sentinel-2 Harmonized MSI.","We introduce the development of \"OpenSendaiBench\" consisting of 47 countries wherein most are least developed (LDCs), trained ResNet-50 deep learning models, and demonstrated the region of Dhaka, Bangladesh by mapping the distribution of its informal constructions.","As a pioneering effort in auditing global disaster risk over time, this paper aims to advance the area of large-scale risk quantification in informing our collective long-term efforts in reducing climate and disaster risk."],"url":"http://arxiv.org/abs/2404.01748v1","category":"cs.LG"}
{"created":"2024-04-02 06:43:53","title":"Search for a sub-eV sterile neutrino using Daya Bay's full dataset","abstract":"This Letter presents results of a search for the mixing of a sub-eV sterile neutrino based on the full data sample of the Daya Bay Reactor Neutrino Experiment, collected during 3158 days of detector operation, which contains $5.55 \\times 10^{6}$ reactor \\anue candidates identified as inverse beta-decay interactions followed by neutron-capture on gadolinium. The result was obtained in the minimally extended 3+1 neutrino mixing model. The analysis benefits from a doubling of the statistics of our previous result and from improvements of several important systematic uncertainties. The results are consistent with the standard three-neutrino mixing model and no significant signal of a sub-eV sterile neutrino was found. Exclusion limits are set by both Feldman-Cousins and CLs methods. Light sterile neutrino mixing with $\\sin^2 2\\theta_{14} \\gtrsim 0.01$ can be excluded at 95\\% confidence level in the region of $0.01$ eV$^2 \\lesssim |\\Delta m^{2}_{41}| \\lesssim 0.1 $ eV$^2$. This result represents the world-leading constraints in the region of $2 \\times 10^{-4}$ eV$^2 \\lesssim |\\Delta m^{2}_{41}| \\lesssim 0.2 $ eV$^2$.","sentences":["This Letter presents results of a search for the mixing of a sub-eV sterile neutrino based on the full data sample of the Daya Bay Reactor Neutrino Experiment, collected during 3158 days of detector operation, which contains $5.55 \\times 10^{6}$ reactor \\anue candidates identified as inverse beta-decay interactions followed by neutron-capture on gadolinium.","The result was obtained in the minimally extended 3+1 neutrino mixing model.","The analysis benefits from a doubling of the statistics of our previous result and from improvements of several important systematic uncertainties.","The results are consistent with the standard three-neutrino mixing model and no significant signal of a sub-eV sterile neutrino was found.","Exclusion limits are set by both Feldman-Cousins and CLs methods.","Light sterile neutrino mixing with $\\sin^2 2\\theta_{14} \\gtrsim 0.01$ can be excluded at 95\\% confidence level in the region of $0.01$ eV$^2 \\lesssim |\\Delta m^{2}_{41}| \\lesssim 0.1 $","eV$^2$.","This result represents the world-leading constraints in the region of $2 \\times 10^{-4}$ eV$^2 \\lesssim |\\Delta m^{2}_{41}| \\lesssim 0.2 $ eV$^2$."],"url":"http://arxiv.org/abs/2404.01687v1","category":"hep-ex"}
{"created":"2024-04-02 06:43:22","title":"JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments","abstract":"Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.","sentences":["Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision.","Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings.","Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making.","As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception.","JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation.","Extensive evaluation of leading methods shows significant challenges posed by our dataset."],"url":"http://arxiv.org/abs/2404.01686v1","category":"cs.CV"}
{"created":"2024-04-02 06:14:54","title":"METAL: Towards Multilingual Meta-Evaluation","abstract":"With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.","sentences":["With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent.","Several studies have shown that LLMs excel on many standard NLP benchmarks.","However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics.","Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics.","However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments.","In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios.","We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization.","This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL).","We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2.","Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly.","Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges."],"url":"http://arxiv.org/abs/2404.01667v1","category":"cs.CL"}
{"created":"2024-04-02 06:09:42","title":"Nonreciprocal interactions in crowd dynamics: investigating the impact of moving threats on pedestrian speed preferences","abstract":"Nonreciprocal interaction crowd systems, such as human-human, human-vehicle, and human-robot systems, often have serious impacts on pedestrian safety and social order. A more comprehensive understanding of these systems is needed to optimize system stability and efficiency. Despite the importance of these interactions, empirical research in this area remains limited. Thus, in our study we explore this underresearched area, focusing on scenarios where nonreciprocity plays a critical role, such as mass stabbings, which pose a substantial risk to public safety. We conducted the first experiments on this system and analysed high-accuracy data obtained from these experiments. The extent of the direct threat zone is determined by the speed of the moving threat and the radius of danger occurrence. We further categorize potential threats into direct, adjacent, and rear-view zones, quantifying the level of threat for pedestrians. Our study revealed that a pedestrian's desired velocity correlated positively with potential threat intensity, increasing until near the direct threat zone. An emerging steady state is observed when escape routes are blocked by moving threats. This deviation affects the density-velocity relationship, making it distinct from the general relationship. This deviation signifies unique pedestrian behaviour in the presence of moving threats. Additionally, the rate of change in the angle for pedestrian motion in various desired directions is synchronized. This indicates the emergence of collective intelligence in nonreciprocal interaction crowd systems. As a result, our study may constitute a pioneering step towards understanding nonreciprocal interactions in crowd systems through laboratory experiments. These findings may enhance pedestrian safety and inform not only government crowd management strategies but also individual self-protection measures.","sentences":["Nonreciprocal interaction crowd systems, such as human-human, human-vehicle, and human-robot systems, often have serious impacts on pedestrian safety and social order.","A more comprehensive understanding of these systems is needed to optimize system stability and efficiency.","Despite the importance of these interactions, empirical research in this area remains limited.","Thus, in our study we explore this underresearched area, focusing on scenarios where nonreciprocity plays a critical role, such as mass stabbings, which pose a substantial risk to public safety.","We conducted the first experiments on this system and analysed high-accuracy data obtained from these experiments.","The extent of the direct threat zone is determined by the speed of the moving threat and the radius of danger occurrence.","We further categorize potential threats into direct, adjacent, and rear-view zones, quantifying the level of threat for pedestrians.","Our study revealed that a pedestrian's desired velocity correlated positively with potential threat intensity, increasing until near the direct threat zone.","An emerging steady state is observed when escape routes are blocked by moving threats.","This deviation affects the density-velocity relationship, making it distinct from the general relationship.","This deviation signifies unique pedestrian behaviour in the presence of moving threats.","Additionally, the rate of change in the angle for pedestrian motion in various desired directions is synchronized.","This indicates the emergence of collective intelligence in nonreciprocal interaction crowd systems.","As a result, our study may constitute a pioneering step towards understanding nonreciprocal interactions in crowd systems through laboratory experiments.","These findings may enhance pedestrian safety and inform not only government crowd management strategies but also individual self-protection measures."],"url":"http://arxiv.org/abs/2404.01664v1","category":"physics.soc-ph"}
{"created":"2024-04-02 06:07:35","title":"CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models","abstract":"Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.","sentences":["Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.","Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.","Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset.","We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback.","This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory.","In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors.","Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs."],"url":"http://arxiv.org/abs/2404.01663v1","category":"cs.CL"}
{"created":"2024-04-02 05:59:43","title":"Release of Pre-Trained Models for the Japanese Language","abstract":"AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.","sentences":["AI democratization aims to create a world in which the average person can utilize AI techniques.","To achieve this goal, numerous research institutes have attempted to make their results accessible to the public.","In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact.","However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly.","To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese.","By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI.","Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks."],"url":"http://arxiv.org/abs/2404.01657v1","category":"cs.CL"}
{"created":"2024-04-02 05:57:35","title":"Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies","abstract":"The expansion of artificial intelligence (AI) in pathology tasks has intensified the demand for doctors' annotations in AI development. However, collecting high-quality annotations from doctors is costly and time-consuming, creating a bottleneck in AI progress. This study investigates eye-tracking as a cost-effective technology to collect doctors' behavioral data for AI training with a focus on the pathology task of mitosis detection. One major challenge in using eye-gaze data is the low signal-to-noise ratio, which hinders the extraction of meaningful information. We tackled this by levering the properties of inter-observer eye-gaze consistencies and creating eye-gaze labels from consistent eye-fixations shared by a group of observers. Our study involved 14 non-medical participants, from whom we collected eye-gaze data and generated eye-gaze labels based on varying group sizes. We assessed the efficacy of such eye-gaze labels by training Convolutional Neural Networks (CNNs) and comparing their performance to those trained with ground truth annotations and a heuristic-based baseline. Results indicated that CNNs trained with our eye-gaze labels closely followed the performance of ground-truth-based CNNs, and significantly outperformed the baseline. Although primarily focused on mitosis, we envision that insights from this study can be generalized to other medical imaging tasks.","sentences":["The expansion of artificial intelligence (AI) in pathology tasks has intensified the demand for doctors' annotations in AI development.","However, collecting high-quality annotations from doctors is costly and time-consuming, creating a bottleneck in AI progress.","This study investigates eye-tracking as a cost-effective technology to collect doctors' behavioral data for AI training with a focus on the pathology task of mitosis detection.","One major challenge in using eye-gaze data is the low signal-to-noise ratio, which hinders the extraction of meaningful information.","We tackled this by levering the properties of inter-observer eye-gaze consistencies and creating eye-gaze labels from consistent eye-fixations shared by a group of observers.","Our study involved 14 non-medical participants, from whom we collected eye-gaze data and generated eye-gaze labels based on varying group sizes.","We assessed the efficacy of such eye-gaze labels by training Convolutional Neural Networks (CNNs) and comparing their performance to those trained with ground truth annotations and a heuristic-based baseline.","Results indicated that CNNs trained with our eye-gaze labels closely followed the performance of ground-truth-based CNNs, and significantly outperformed the baseline.","Although primarily focused on mitosis, we envision that insights from this study can be generalized to other medical imaging tasks."],"url":"http://arxiv.org/abs/2404.01656v1","category":"cs.CV"}
{"created":"2024-04-02 05:53:34","title":"AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease","abstract":"Parkinson's Disease (PD) is the second most common neurodegenerative disorder. The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression. However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication. We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering. The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP.","sentences":["Parkinson's Disease (PD) is the second most common neurodegenerative disorder.","The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression.","However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication.","We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering.","The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP."],"url":"http://arxiv.org/abs/2404.01654v1","category":"cs.CV"}
{"created":"2024-04-02 05:44:50","title":"Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization","abstract":"Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model's performance in its original corpus and domain.","sentences":["Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus.","However, real-world knowledge is not static; it updates and evolves continually.","Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate.","In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains.","In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains.","We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus.","We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training.","Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model's performance in its original corpus and domain."],"url":"http://arxiv.org/abs/2404.01652v1","category":"cs.CL"}
{"created":"2024-04-02 05:33:42","title":"Observations of the Crab Nebula with MACE (Major Atmospheric Cherenkov Experiment)","abstract":"The Major Atmospheric Cherenkov Experiment (MACE) is a large size (21m) Imaging Atmospheric Cherenkov Telescope (IACT) installed at an altitude of 4270m above sea level at Hanle, Ladakh in northern India. Here we report the detection of Very High Energy (VHE) gamma-ray emission from Crab Nebula above 80 GeV. We analysed ~15 hours of data collected at low zenith angle between November 2022 and February 2023. The energy spectrum is well described by a log-parabola function with a flux of ~(3.46 +/- 0.26stat) x 10-10 TeV-1 cm-2 s-1, at 400 GeV with spectral index of 2.09 +/- 0.06stat and a curvature parameter of 0.08 +/- 0.07stat. The gamma-rays are detected in an energy range spanning from 80 GeV to ~5 TeV. The energy resolution improves from ~34% at an analysis energy threshold of 80 GeV to ~21% above 1 TeV. The daily light curve and the spectral energy distribution obtained for the Crab Nebula is in agreement with previous measurements, considering statistical and systematic uncertainties.","sentences":["The Major Atmospheric Cherenkov Experiment (MACE) is a large size (21m)","Imaging Atmospheric Cherenkov Telescope (IACT) installed at an altitude of 4270m above sea level at Hanle, Ladakh in northern India.","Here we report the detection of Very High Energy (VHE) gamma-ray emission from Crab Nebula above 80 GeV.","We analysed ~15 hours of data collected at low zenith angle between November 2022 and February 2023.","The energy spectrum is well described by a log-parabola function with a flux of ~(3.46 +/- 0.26stat)","x 10-10 TeV-1 cm-2 s-1, at 400 GeV with spectral index of 2.09 +/- 0.06stat and a curvature parameter of 0.08 +/- 0.07stat.","The gamma-rays are detected in an energy range spanning from 80 GeV to ~5 TeV.","The energy resolution improves from ~34% at an analysis energy threshold of 80 GeV to ~21% above 1 TeV.","The daily light curve and the spectral energy distribution obtained for the Crab Nebula is in agreement with previous measurements, considering statistical and systematic uncertainties."],"url":"http://arxiv.org/abs/2404.01649v1","category":"astro-ph.HE"}
{"created":"2024-04-02 05:19:27","title":"A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection","abstract":"Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of out-of-distribution (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance. As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data. The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024. Our source code will be made available.","sentences":["Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models.","(2) CT-scan contains large number of out-of-distribution (OOD) slices.","The crucial features may only be present in specific spatial regions and slices of the entire CT scan.","How can we effectively figure out where these are located?","To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan.","It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally.","Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance.","As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data.","The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024.","Our source code will be made available."],"url":"http://arxiv.org/abs/2404.01643v1","category":"cs.CV"}
{"created":"2024-04-02 04:53:49","title":"Collaborative Optimization of Wireless Communication and Computing Resource Allocation based on Multi-Agent Federated Weighting Deep Reinforcement Learning","abstract":"As artificial intelligence (AI)-enabled wireless communication systems continue their evolution, distributed learning has gained widespread attention for its ability to offer enhanced data privacy protection, improved resource utilization, and enhanced fault tolerance within wireless communication applications. Federated learning further enhances the ability of resource coordination and model generalization across nodes based on the above foundation, enabling the realization of an AI-driven communication and computing integrated wireless network. This paper proposes a novel wireless communication system to cater to a personalized service needs of both privacy-sensitive and privacy-insensitive users. We design the system based on based on multi-agent federated weighting deep reinforcement learning (MAFWDRL). The system, while fulfilling service requirements for users, facilitates real-time optimization of local communication resources allocation and concurrent decision-making concerning computing resources. Additionally, exploration noise is incorporated to enhance the exploration process of off-policy deep reinforcement learning (DRL) for wireless channels. Federated weighting (FedWgt) effectively compensates for heterogeneous differences in channel status between communication nodes. Extensive simulation experiments demonstrate that the proposed scheme outperforms baseline methods significantly in terms of throughput, calculation latency, and energy consumption improvement.","sentences":["As artificial intelligence (AI)-enabled wireless communication systems continue their evolution, distributed learning has gained widespread attention for its ability to offer enhanced data privacy protection, improved resource utilization, and enhanced fault tolerance within wireless communication applications.","Federated learning further enhances the ability of resource coordination and model generalization across nodes based on the above foundation, enabling the realization of an AI-driven communication and computing integrated wireless network.","This paper proposes a novel wireless communication system to cater to a personalized service needs of both privacy-sensitive and privacy-insensitive users.","We design the system based on based on multi-agent federated weighting deep reinforcement learning (MAFWDRL).","The system, while fulfilling service requirements for users, facilitates real-time optimization of local communication resources allocation and concurrent decision-making concerning computing resources.","Additionally, exploration noise is incorporated to enhance the exploration process of off-policy deep reinforcement learning (DRL) for wireless channels.","Federated weighting (FedWgt) effectively compensates for heterogeneous differences in channel status between communication nodes.","Extensive simulation experiments demonstrate that the proposed scheme outperforms baseline methods significantly in terms of throughput, calculation latency, and energy consumption improvement."],"url":"http://arxiv.org/abs/2404.01638v1","category":"cs.NI"}
{"created":"2024-04-02 04:53:39","title":"Learning to Control Camera Exposure via Reinforcement Learning","abstract":"Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capability, and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild.As a result, our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms). Also, the acquired images are well-exposed and show superiority in various computer vision tasks, such as feature extraction and object detection.","sentences":["Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications.","Poorly adjusted camera exposure often leads to critical failure and performance degradation.","Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions.","In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning.","The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capability, and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild.","As a result, our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms).","Also, the acquired images are well-exposed and show superiority in various computer vision tasks, such as feature extraction and object detection."],"url":"http://arxiv.org/abs/2404.01636v1","category":"cs.CV"}
{"created":"2024-04-02 04:29:01","title":"Learning Equi-angular Representations for Online Continual Learning","abstract":"Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.","sentences":["Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training).","To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon.","In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space.","With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups."],"url":"http://arxiv.org/abs/2404.01628v1","category":"cs.CV"}
{"created":"2024-04-02 04:22:07","title":"AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation","abstract":"Local differential privacy (LDP) is a strong privacy standard that has been adopted by popular software systems. The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator. Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years. Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee. However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand.   In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem. AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) quantized data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion. The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities. We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms. The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets.","sentences":["Local differential privacy (LDP) is a strong privacy standard that has been adopted by popular software systems.","The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator.","Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years.","Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee.","However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand.   ","In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem.","AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) quantized data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion.","The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities.","We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms.","The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2404.01625v1","category":"cs.CR"}
{"created":"2024-04-02 04:16:26","title":"Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning","abstract":"In recent decades, financial quantification has emerged and matured rapidly. For financial institutions such as funds, investment institutions are increasingly dissatisfied with the situation of passively constructing investment portfolios with average market returns, and are paying more and more attention to active quantitative strategy investment portfolios. This requires the introduction of active stock investment fund management models. Currently, in my country's stock fund investment market, there are many active quantitative investment strategies, and the algorithms used vary widely, such as SVM, random forest, RNN recurrent memory network, etc. This article focuses on this trend, using the emerging LSTM-GRU gate-controlled long short-term memory network model in the field of financial stock investment as a basis to build a set of active investment stock strategies, and combining it with SVM, which has been widely used in the field of quantitative stock investment. Comparing models such as RNN, theoretically speaking, compared to SVM that simply relies on kernel functions for high-order mapping and classification of data, neural network algorithms such as RNN and LSTM-GRU have better principles and are more suitable for processing financial stock data. Then, through multiple By comparison, it was finally found that the LSTM- GRU gate-controlled long short-term memory network has a better accuracy. By selecting the LSTM-GRU algorithm to construct a trading strategy based on the Shanghai and Shenzhen 300 Index constituent stocks, the parameters were adjusted and the neural layer connection was adjusted. Finally, It has significantly outperformed the benchmark index CSI 300 over the long term. The conclusion of this article is that the research results can provide certain quantitative strategy references for financial institutions to construct active stock investment portfolios.","sentences":["In recent decades, financial quantification has emerged and matured rapidly.","For financial institutions such as funds, investment institutions are increasingly dissatisfied with the situation of passively constructing investment portfolios with average market returns, and are paying more and more attention to active quantitative strategy investment portfolios.","This requires the introduction of active stock investment fund management models.","Currently, in my country's stock fund investment market, there are many active quantitative investment strategies, and the algorithms used vary widely, such as SVM, random forest, RNN recurrent memory network, etc.","This article focuses on this trend, using the emerging LSTM-GRU gate-controlled long short-term memory network model in the field of financial stock investment as a basis to build a set of active investment stock strategies, and combining it with SVM, which has been widely used in the field of quantitative stock investment.","Comparing models such as RNN, theoretically speaking, compared to SVM that simply relies on kernel functions for high-order mapping and classification of data, neural network algorithms such as RNN and LSTM-GRU have better principles and are more suitable for processing financial stock data.","Then, through multiple By comparison, it was finally found that the LSTM- GRU gate-controlled long short-term memory network has a better accuracy.","By selecting the LSTM-GRU algorithm to construct a trading strategy based on the Shanghai and Shenzhen 300 Index constituent stocks, the parameters were adjusted and the neural layer connection was adjusted.","Finally, It has significantly outperformed the benchmark index CSI 300 over the long term.","The conclusion of this article is that the research results can provide certain quantitative strategy references for financial institutions to construct active stock investment portfolios."],"url":"http://arxiv.org/abs/2404.01624v1","category":"cs.CE"}
{"created":"2024-04-02 04:11:37","title":"Gen4DS: Workshop on Data Storytelling in an Era of Generative AI","abstract":"Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (including hands-on creation, group discussion pods, and debates on controversial issues) for the workshop. We hope that participants will learn about the latest advances and pioneering work in data storytelling, engage in critical conversations with each other, and have an enjoyable, unforgettable, and meaningful experience at the event.","sentences":["Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age.","Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry.","Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions.","These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future.","We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories?","How might generative AI alter the workflow of data storytellers?","What are the pitfalls and risks of incorporating AI in storytelling?","We have designed both paper presentations and interactive activities (including hands-on creation, group discussion pods, and debates on controversial issues) for the workshop.","We hope that participants will learn about the latest advances and pioneering work in data storytelling, engage in critical conversations with each other, and have an enjoyable, unforgettable, and meaningful experience at the event."],"url":"http://arxiv.org/abs/2404.01622v1","category":"cs.HC"}
{"created":"2024-04-02 04:07:22","title":"Voice EHR: Introducing Multimodal Audio Data for Health","abstract":"Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partners for global work, presents the application used for data collection, and showcases the potential of informative voice EHR to advance the scalability and diversity of audio AI.","sentences":["Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection.","Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries.","This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact.","This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application.","This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets.","This report introduces a consortium of partners for global work, presents the application used for data collection, and showcases the potential of informative voice EHR to advance the scalability and diversity of audio AI."],"url":"http://arxiv.org/abs/2404.01620v1","category":"cs.SD"}
{"created":"2024-04-02 03:39:06","title":"Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration","abstract":"Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors. The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time. Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts. These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams.","sentences":["Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each.","In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies.","User trust is essential to creating and maintaining these collaborative relationships.","Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors.","The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time.","Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts.","These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams."],"url":"http://arxiv.org/abs/2404.01615v1","category":"cs.HC"}
{"created":"2024-04-02 02:46:18","title":"Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game","abstract":"Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs and few LLMs possess the capacity for opinion leadership.","sentences":["Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games.","However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings.","Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group.","In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs.","The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader.","We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders.","The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions.","We conduct extensive experiments to evaluate LLMs of different scales.","In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis.","The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs and few LLMs possess the capacity for opinion leadership."],"url":"http://arxiv.org/abs/2404.01602v1","category":"cs.CL"}
{"created":"2024-04-02 02:39:17","title":"Extremum-Seeking Action Selection for Accelerating Policy Optimization","abstract":"Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment. Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments.","sentences":["Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance.","Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence.","In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning.","We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC).","On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal.","Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment.","Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments."],"url":"http://arxiv.org/abs/2404.01598v1","category":"cs.LG"}
{"created":"2024-04-02 02:36:31","title":"PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving","abstract":"Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction.","sentences":["Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain.","Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance.","In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization.","By merging the advantages of both methods, neuro-symbolic approaches present a promising direction.","These methods embed physical laws into neural models, potentially significantly improving generalization capabilities.","However, no prior works were evaluated in real-world settings for off-road driving.","To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving.","Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties.","It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction."],"url":"http://arxiv.org/abs/2404.01596v1","category":"cs.RO"}
{"created":"2024-04-02 02:36:21","title":"Propensity Score Alignment of Unpaired Multimodal Data","abstract":"Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance -- shared nearest neighbours (SNN) and optimal transport (OT) matching -- and find that OT matching results in significant improvements over state-of-the-art alignment approaches in both a synthetic multi-modal setting and in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge.","sentences":["Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples.","This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning.","We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples.","Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples.","We experiment with two alignment techniques that leverage this distance -- shared nearest neighbours (SNN) and optimal transport (OT) matching -- and find that OT matching results in significant improvements over state-of-the-art alignment approaches in both a synthetic multi-modal setting and in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge."],"url":"http://arxiv.org/abs/2404.01595v1","category":"cs.LG"}
{"created":"2024-04-02 02:30:47","title":"Classifying Cancer Stage with Open-Source Clinical Large Language Models","abstract":"Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data. Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achieve comparable performance on Metastasis (M) classification and improved performance on Node (N) classification.","sentences":["Cancer stage classification is important for making treatment and care management plans for oncology patients.","Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain.","To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare.","In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports.","Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data.","Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achieve comparable performance on Metastasis (M) classification and improved performance on Node (N) classification."],"url":"http://arxiv.org/abs/2404.01589v1","category":"cs.CL"}
{"created":"2024-04-02 02:30:27","title":"Hallucination Diversity-Aware Active Learning for Text Summarization","abstract":"Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.","sentences":["Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported.","Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs.","Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs.","To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed.","By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning.","Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations."],"url":"http://arxiv.org/abs/2404.01588v1","category":"cs.CL"}
{"created":"2024-04-02 02:23:35","title":"Defining Problem from Solutions: Inverse Reinforcement Learning (IRL) and Its Applications for Next-Generation Networking","abstract":"Performance optimization is a critical concern in networking, on which Deep Reinforcement Learning (DRL) has achieved great success. Nonetheless, DRL training relies on precisely defined reward functions, which formulate the optimization objective and indicate the positive/negative progress towards the optimal. With the ever-increasing environmental complexity and human participation in Next-Generation Networking (NGN), defining appropriate reward functions become challenging. In this article, we explore the applications of Inverse Reinforcement Learning (IRL) in NGN. Particularly, if DRL aims to find optimal solutions to the problem, IRL finds a problem from the optimal solutions, where the optimal solutions are collected from experts, and the problem is defined by reward inference. Specifically, we first formally introduce the IRL technique, including its fundamentals, workflow, and difference from DRL. Afterward, we present the motivations of IRL applications in NGN and survey existing studies. Furthermore, to demonstrate the process of applying IRL in NGN, we perform a case study about human-centric prompt engineering in Generative AI-enabled networks. We demonstrate the effectiveness of using both DRL and IRL techniques and prove the superiority of IRL.","sentences":["Performance optimization is a critical concern in networking, on which Deep Reinforcement Learning (DRL) has achieved great success.","Nonetheless, DRL training relies on precisely defined reward functions, which formulate the optimization objective and indicate the positive/negative progress towards the optimal.","With the ever-increasing environmental complexity and human participation in Next-Generation Networking (NGN), defining appropriate reward functions become challenging.","In this article, we explore the applications of Inverse Reinforcement Learning (IRL) in NGN.","Particularly, if DRL aims to find optimal solutions to the problem, IRL finds a problem from the optimal solutions, where the optimal solutions are collected from experts, and the problem is defined by reward inference.","Specifically, we first formally introduce the IRL technique, including its fundamentals, workflow, and difference from DRL.","Afterward, we present the motivations of IRL applications in NGN and survey existing studies.","Furthermore, to demonstrate the process of applying IRL in NGN, we perform a case study about human-centric prompt engineering in Generative AI-enabled networks.","We demonstrate the effectiveness of using both DRL and IRL techniques and prove the superiority of IRL."],"url":"http://arxiv.org/abs/2404.01583v1","category":"cs.NI"}
{"created":"2024-04-02 02:03:28","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental Approach","abstract":"In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicating a substantial 17% decline. This outcome led us to conduct a detailed examination of the model's learning behaviors. Following this, we improved the model's resilience by fine-tuning it with a contrast-enhanced training dataset specifically designed for SNLI, which increased its accuracy to 85.5% on the contrast sets. Our findings highlight the importance of incorporating diverse linguistic expressions into datasets for NLI tasks. We hope that our research will encourage the creation of more inclusive datasets, thereby contributing to the development of NLI models that are both more sophisticated and effective.","sentences":["In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement.","However, this metric falls short in effectively evaluating a model's capacity to understand language entailments.","In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset.","Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences.","This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition.","We conducted our analysis using the ELECTRA-small model.","The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicating a substantial 17% decline.","This outcome led us to conduct a detailed examination of the model's learning behaviors.","Following this, we improved the model's resilience by fine-tuning it with a contrast-enhanced training dataset specifically designed for SNLI, which increased its accuracy to 85.5% on the contrast sets.","Our findings highlight the importance of incorporating diverse linguistic expressions into datasets for NLI tasks.","We hope that our research will encourage the creation of more inclusive datasets, thereby contributing to the development of NLI models that are both more sophisticated and effective."],"url":"http://arxiv.org/abs/2404.01569v1","category":"cs.CL"}
{"created":"2024-04-02 01:56:33","title":"Efficient, indistinguishable telecom C-band photons using a tapered nanobeam","abstract":"Telecom C-band single photons exhibit the lowest attenuation in optical fibers, enabling long-haul quantum-secured communication. However, efficient coupling with optical fibers is crucial for these single photons to be effective carriers in long-distance transmission. In this work, we demonstrate an efficient fiber-coupled single photon source at the telecom C-band using InAs/InP quantum dots coupled to a tapered nanobeam. The tapered nanobeam structure facilitates directional emission that is mode-matched to a lensed fiber, resulting in a collection efficiency of up to 65% from the nanobeam to a single-mode fiber. Using this approach, we demonstrate single photon count rates of 575 $\\pm$ 5 Kcps and a single photon purity of $g^2$ (0) = 0.015 $\\pm$ 0.003. Additionally, we demonstrate Hong-Ou Mandel interference from the emitted photons with a visibility of 0.84 $\\pm$ 0.06. From these measurements, we determine a photon coherence time of 450 $\\pm$ 20 ps, a factor of just 8.3 away from the lifetime limit. This work represents an important step towards the development of telecom C-band single-photon sources emitting bright, pure, and indistinguishable photons, which are necessary to realize fiber-based long-distance quantum networks","sentences":["Telecom C-band single photons exhibit the lowest attenuation in optical fibers, enabling long-haul quantum-secured communication.","However, efficient coupling with optical fibers is crucial for these single photons to be effective carriers in long-distance transmission.","In this work, we demonstrate an efficient fiber-coupled single photon source at the telecom C-band using InAs/InP quantum dots coupled to a tapered nanobeam.","The tapered nanobeam structure facilitates directional emission that is mode-matched to a lensed fiber, resulting in a collection efficiency of up to 65% from the nanobeam to a single-mode fiber.","Using this approach, we demonstrate single photon count rates of 575 $\\pm$ 5 Kcps and a single photon purity of $g^2$ (0) = 0.015 $\\pm$ 0.003.","Additionally, we demonstrate Hong-Ou Mandel interference from the emitted photons with a visibility of 0.84 $\\pm$ 0.06.","From these measurements, we determine a photon coherence time of 450 $\\pm$ 20 ps, a factor of just 8.3 away from the lifetime limit.","This work represents an important step towards the development of telecom C-band single-photon sources emitting bright, pure, and indistinguishable photons, which are necessary to realize fiber-based long-distance quantum networks"],"url":"http://arxiv.org/abs/2404.01562v1","category":"quant-ph"}
{"created":"2024-04-02 01:45:57","title":"Automated User Story Generation with Test Case Specification Using Large Language Model","abstract":"Modern Software Engineering era is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating many parts of the software development workflow. Requirements Engineering (RE) is a crucial phase that begins the software development cycle through multiple discussions on a proposed scope of work documented in different forms. RE phase ends with a list of user-stories for each unit task identified through discussions and usually these are created and tracked on a project management tool such as Jira, AzurDev etc. In this research we developed a tool \"GeneUS\" using GPT-4.0 to automatically create user stories from requirements document which is the outcome of the RE phase. The output is provided in JSON format leaving the possibilities open for downstream integration to the popular project management tools. Analyzing requirements documents takes significant effort and multiple meetings with stakeholders. We believe, automating this process will certainly reduce additional load off the software engineers, and increase the productivity since they will be able to utilize their time on other prioritized tasks.","sentences":["Modern Software Engineering era is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM).","Researchers have already started automating many parts of the software development workflow.","Requirements Engineering (RE) is a crucial phase that begins the software development cycle through multiple discussions on a proposed scope of work documented in different forms.","RE phase ends with a list of user-stories for each unit task identified through discussions and usually these are created and tracked on a project management tool such as Jira, AzurDev etc.","In this research we developed a tool \"GeneUS\" using GPT-4.0 to automatically create user stories from requirements document which is the outcome of the RE phase.","The output is provided in JSON format leaving the possibilities open for downstream integration to the popular project management tools.","Analyzing requirements documents takes significant effort and multiple meetings with stakeholders.","We believe, automating this process will certainly reduce additional load off the software engineers, and increase the productivity since they will be able to utilize their time on other prioritized tasks."],"url":"http://arxiv.org/abs/2404.01558v1","category":"cs.SE"}
{"created":"2024-04-02 01:45:03","title":"Distributed Autonomous Swarm Formation for Dynamic Network Bridging","abstract":"Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications. In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents' actions heavily relying on the communication between them and the underlying network. In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets. Furthermore, we propose a Multi-Agent Reinforcement Learning (MARL) approach for the problem based on Graph Convolutional Reinforcement Learning (DGN) which naturally applies to the networked, distributed nature of the task. The proposed method is evaluated in a simulated environment and compared to a centralized heuristic baseline showing promising results. Moreover, a further step in the direction of sim-to-real transfer is presented, by additionally evaluating the proposed approach in a near Live Virtual Constructive (LVC) UAV framework.","sentences":["Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications.","In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents' actions heavily relying on the communication between them and the underlying network.","In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets.","Furthermore, we propose a Multi-Agent Reinforcement Learning (MARL) approach for the problem based on Graph Convolutional Reinforcement Learning (DGN) which naturally applies to the networked, distributed nature of the task.","The proposed method is evaluated in a simulated environment and compared to a centralized heuristic baseline showing promising results.","Moreover, a further step in the direction of sim-to-real transfer is presented, by additionally evaluating the proposed approach in a near Live Virtual Constructive (LVC) UAV framework."],"url":"http://arxiv.org/abs/2404.01557v1","category":"cs.MA"}
{"created":"2024-04-02 01:30:41","title":"Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging","abstract":"Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability. This work introduces a hybrid approach that integrates Multi-Agent Reinforcement Learning with control-theoretic methods to ensure safe and efficient distributed strategies. Our contributions include a novel setpoint update algorithm that dynamically adjusts agents' positions to preserve safety conditions without compromising the mission's objectives. Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations. Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives.","sentences":["Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability.","This work introduces a hybrid approach that integrates Multi-Agent Reinforcement Learning with control-theoretic methods to ensure safe and efficient distributed strategies.","Our contributions include a novel setpoint update algorithm that dynamically adjusts agents' positions to preserve safety conditions without compromising the mission's objectives.","Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations.","Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives."],"url":"http://arxiv.org/abs/2404.01551v1","category":"cs.MA"}
{"created":"2024-04-02 01:30:17","title":"Perfecting Periodic Trajectory Tracking: Model Predictive Control with a Periodic Observer ($\u03a0$-MPC)","abstract":"In Model Predictive Control (MPC), discrepancies between the actual system and the predictive model can lead to substantial tracking errors and significantly degrade performance and reliability. While such discrepancies can be alleviated with more complex models, this often complicates controller design and implementation. By leveraging the fact that many trajectories of interest are periodic, we show that perfect tracking is possible when incorporating a simple observer that estimates and compensates for periodic disturbances. We present the design of the observer and the accompanying tracking MPC scheme, proving that their combination achieves zero tracking error asymptotically, regardless of the complexity of the unmodelled dynamics. We validate the effectiveness of our method, demonstrating asymptotically perfect tracking on a high-dimensional soft robot with nearly 10,000 states and a fivefold reduction in tracking errors compared to a baseline MPC on small-scale autonomous race car experiments.","sentences":["In Model Predictive Control (MPC), discrepancies between the actual system and the predictive model can lead to substantial tracking errors and significantly degrade performance and reliability.","While such discrepancies can be alleviated with more complex models, this often complicates controller design and implementation.","By leveraging the fact that many trajectories of interest are periodic, we show that perfect tracking is possible when incorporating a simple observer that estimates and compensates for periodic disturbances.","We present the design of the observer and the accompanying tracking MPC scheme, proving that their combination achieves zero tracking error asymptotically, regardless of the complexity of the unmodelled dynamics.","We validate the effectiveness of our method, demonstrating asymptotically perfect tracking on a high-dimensional soft robot with nearly 10,000 states and a fivefold reduction in tracking errors compared to a baseline MPC on small-scale autonomous race car experiments."],"url":"http://arxiv.org/abs/2404.01550v1","category":"cs.RO"}
{"created":"2024-04-02 01:29:28","title":"Octopus: On-device language model for function calling of software APIs","abstract":"In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \\textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.","sentences":["In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities.","This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs.","We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions.","Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls.","Additionally, we propose \\textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds.","We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research.","Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling.","This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications."],"url":"http://arxiv.org/abs/2404.01549v1","category":"cs.CL"}
{"created":"2024-04-02 01:28:44","title":"mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning","abstract":"In the fields of computer vision and natural language processing, multimodal chart question-answering, especially involving color, structure, and textless charts, poses significant challenges. Traditional methods, which typically involve either direct multimodal processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios. This paper introduces a novel multimodal chart question-answering model, specifically designed to address these intricate tasks. Our model integrates visual and linguistic processing, overcoming the constraints of existing methods. We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model's interpretative and analytical abilities in chart-related queries. This approach has demonstrated superior performance on multiple public datasets, particularly in handling color, structure, and textless chart questions, indicating its effectiveness in complex multimodal tasks.","sentences":["In the fields of computer vision and natural language processing, multimodal chart question-answering, especially involving color, structure, and textless charts, poses significant challenges.","Traditional methods, which typically involve either direct multimodal processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios.","This paper introduces a novel multimodal chart question-answering model, specifically designed to address these intricate tasks.","Our model integrates visual and linguistic processing, overcoming the constraints of existing methods.","We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model's interpretative and analytical abilities in chart-related queries.","This approach has demonstrated superior performance on multiple public datasets, particularly in handling color, structure, and textless chart questions, indicating its effectiveness in complex multimodal tasks."],"url":"http://arxiv.org/abs/2404.01548v1","category":"cs.CV"}
{"created":"2024-04-02 00:02:00","title":"Laying Anchors: Semantically Priming Numerals in Language Modeling","abstract":"Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.","sentences":["Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks.","However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension.","We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens.","We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals.","Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings."],"url":"http://arxiv.org/abs/2404.01536v1","category":"cs.CL"}
{"created":"2024-04-01 23:52:48","title":"Strange Sounds","abstract":"This note addresses the effects of long-ranged and/or retarded interactions on the bosonic collective modes in the so-called 'strange metals'. Recently, there have been conflicting reports on the very existence of such stable collective excitations and their properties. Extending a number of approaches that were previously used in the analyses of the standard Fermi liquid one finds evidence for, both, conventional and novel behaviors of the non-Fermi-liquid counterparts of the zero-, shear-, and other 'sounds'.","sentences":["This note addresses the effects of long-ranged and/or retarded interactions on the bosonic collective modes in the so-called 'strange metals'.","Recently, there have been conflicting reports on the very existence of such stable collective excitations and their properties.","Extending a number of approaches that were previously used in the analyses of the standard Fermi liquid one finds evidence for, both, conventional and novel behaviors of the non-Fermi-liquid counterparts of the zero-, shear-, and other 'sounds'."],"url":"http://arxiv.org/abs/2404.01534v1","category":"cond-mat.str-el"}
{"created":"2024-04-01 23:20:08","title":"PlayFutures: Imagining Civic Futures with AI and Puppets","abstract":"Children are the builders of the future and crucial to how the technologies around us develop. They are not voters but are participants in how the public spaces in a city are used. Through a workshop designed around kids of age 9-12, we investigate if novel technologies like artificial intelligence can be integrated in existing ways of play and performance to 1) re-imagine the future of civic spaces, 2) reflect on these novel technologies in the process and 3) build ways of civic engagement through play. We do this using a blend AI image generation and Puppet making to ultimately build future scenarios, perform debate and discussion around the futures and reflect on AI, its role and potential in their process. We present our findings of how AI helped envision these futures, aid performances, and report some initial reflections from children about the technology.","sentences":["Children are the builders of the future and crucial to how the technologies around us develop.","They are not voters but are participants in how the public spaces in a city are used.","Through a workshop designed around kids of age 9-12, we investigate if novel technologies like artificial intelligence can be integrated in existing ways of play and performance to 1) re-imagine the future of civic spaces, 2) reflect on these novel technologies in the process and 3) build ways of civic engagement through play.","We do this using a blend AI image generation and Puppet making to ultimately build future scenarios, perform debate and discussion around the futures and reflect on AI, its role and potential in their process.","We present our findings of how AI helped envision these futures, aid performances, and report some initial reflections from children about the technology."],"url":"http://arxiv.org/abs/2404.01527v1","category":"cs.HC"}
{"created":"2024-04-01 23:19:01","title":"Categorical semiotics: Foundations for Knowledge Integration","abstract":"The integration of knowledge extracted from diverse models, whether described by domain experts or generated by machine learning algorithms, has historically been challenged by the absence of a suitable framework for specifying and integrating structures, learning processes, data transformations, and data models or rules. In this work, we extend algebraic specification methods to address these challenges within such a framework.   In our work, we tackle the challenging task of developing a comprehensive framework for defining and analyzing deep learning architectures. We believe that previous efforts have fallen short by failing to establish a clear connection between the constraints a model must adhere to and its actual implementation.   Our methodology employs graphical structures that resemble Ehresmann's sketches, interpreted within a universe of fuzzy sets. This approach offers a unified theory that elegantly encompasses both deterministic and non-deterministic neural network designs. Furthermore, we highlight how this theory naturally incorporates fundamental concepts from computer science and automata theory. Our extended algebraic specification framework, grounded in graphical structures akin to Ehresmann's sketches, offers a promising solution for integrating knowledge across disparate models and domains. By bridging the gap between domain-specific expertise and machine-generated insights, we pave the way for more comprehensive, collaborative, and effective approaches to knowledge integration and modeling.","sentences":["The integration of knowledge extracted from diverse models, whether described by domain experts or generated by machine learning algorithms, has historically been challenged by the absence of a suitable framework for specifying and integrating structures, learning processes, data transformations, and data models or rules.","In this work, we extend algebraic specification methods to address these challenges within such a framework.   ","In our work, we tackle the challenging task of developing a comprehensive framework for defining and analyzing deep learning architectures.","We believe that previous efforts have fallen short by failing to establish a clear connection between the constraints a model must adhere to and its actual implementation.   ","Our methodology employs graphical structures that resemble Ehresmann's sketches, interpreted within a universe of fuzzy sets.","This approach offers a unified theory that elegantly encompasses both deterministic and non-deterministic neural network designs.","Furthermore, we highlight how this theory naturally incorporates fundamental concepts from computer science and automata theory.","Our extended algebraic specification framework, grounded in graphical structures akin to Ehresmann's sketches, offers a promising solution for integrating knowledge across disparate models and domains.","By bridging the gap between domain-specific expertise and machine-generated insights, we pave the way for more comprehensive, collaborative, and effective approaches to knowledge integration and modeling."],"url":"http://arxiv.org/abs/2404.01526v1","category":"cs.AI"}
{"created":"2024-04-01 23:11:15","title":"On Train-Test Class Overlap and Detection for Image Retrieval","abstract":"How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean. Our dataset is available at https://github.com/dealicious-inc/RGLDv2-clean.","sentences":["How important is it for training and evaluation sets to not have class overlap in image retrieval?","We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris","[34], the most popular evaluation set.","By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking.","Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.","What does it take to focus on objects or interest and ignore background clutter when indexing?","Do we need to train an object detector and the representation separately?","Do we need location supervision?","We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation.","We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean.","Our dataset is available at https://github.com/dealicious-inc/RGLDv2-clean."],"url":"http://arxiv.org/abs/2404.01524v1","category":"cs.CV"}
{"created":"2024-04-01 22:53:09","title":"Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers","abstract":"The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we propose the use of personalization layers for load forecasting in a general framework called PL-FL. We show that PL-FL outperforms FL and purely local training, while requiring lower communication bandwidth than FL. This is done through extensive simulations on three different datasets from the NREL ComStock repository.","sentences":["The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting models.","In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous.","In this paper we propose the use of personalization layers for load forecasting in a general framework called PL-FL.","We show that PL-FL outperforms FL and purely local training, while requiring lower communication bandwidth than FL.","This is done through extensive simulations on three different datasets from the NREL ComStock repository."],"url":"http://arxiv.org/abs/2404.01517v1","category":"cs.LG"}
{"created":"2024-04-01 22:25:48","title":"Can Biases in ImageNet Models Explain Generalization?","abstract":"The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization","sentences":["The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods.","For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches.","The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations.","Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization.","We take a step back and sanity-check these attempts.","Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization.","Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically.","We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization"],"url":"http://arxiv.org/abs/2404.01509v1","category":"cs.CV"}
{"created":"2024-04-01 22:10:12","title":"Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning","abstract":"The ability to generate multiple plans is central to using planning in real-life applications. Top-quality planners generate sets of such top-cost plans, allowing flexibility in determining equivalent ones. In terms of the order between actions in a plan, the literature only considers two extremes -- either all orders are important, making each plan unique, or all orders are unimportant, treating two plans differing only in the order of actions as equivalent. To allow flexibility in selecting important orders, we propose specifying a subset of actions the orders between which are important, interpolating between the top-quality and unordered top-quality planning problems. We explore the ways of adapting partial order reduction search pruning techniques to address this new computational problem and present experimental evaluations demonstrating the benefits of exploiting such techniques in this setting.","sentences":["The ability to generate multiple plans is central to using planning in real-life applications.","Top-quality planners generate sets of such top-cost plans, allowing flexibility in determining equivalent ones.","In terms of the order between actions in a plan, the literature only considers two extremes -- either all orders are important, making each plan unique, or all orders are unimportant, treating two plans differing only in the order of actions as equivalent.","To allow flexibility in selecting important orders, we propose specifying a subset of actions the orders between which are important, interpolating between the top-quality and unordered top-quality planning problems.","We explore the ways of adapting partial order reduction search pruning techniques to address this new computational problem and present experimental evaluations demonstrating the benefits of exploiting such techniques in this setting."],"url":"http://arxiv.org/abs/2404.01503v1","category":"cs.AI"}
{"created":"2024-04-01 21:28:50","title":"Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge","abstract":"A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors that can perform comparably or better than the standard fine-tuning without forgetting the original knowledge. This opens the doors to a more flexible and efficient service-based detection pipeline in which, instead of using a different detector for each modality, a unique and unaltered server is constantly running, where multiple modalities with the corresponding translations can query it. Code: https://github.com/heitorrapela/ModTr.","sentences":["A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks.","While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors.","This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient.","To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models.","ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly.","The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters.","Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors that can perform comparably or better than the standard fine-tuning without forgetting the original knowledge.","This opens the doors to a more flexible and efficient service-based detection pipeline in which, instead of using a different detector for each modality, a unique and unaltered server is constantly running, where multiple modalities with the corresponding translations can query it.","Code: https://github.com/heitorrapela/ModTr."],"url":"http://arxiv.org/abs/2404.01492v1","category":"cs.CV"}
{"created":"2024-04-01 21:13:30","title":"DeLVE into Earth's Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts","abstract":"While previous work has found success in deploying visualizations as museum exhibits, differences in visitor behaviour across varying museum contexts are understudied. We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional reasoning skills through comparison of different time periods. DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units. Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them. We collect data from two sources, an observational study and system trace logs, yielding evidence of successfully meeting our requirements. We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors. We additionally discuss differences in our process from standard design study methodology which is focused on design studies for data analysis purposes, rather than for presentation. Supplemental materials are available at: https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541","sentences":["While previous work has found success in deploying visualizations as museum exhibits, differences in visitor behaviour across varying museum contexts are understudied.","We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional reasoning skills through comparison of different time periods.","DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units.","Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them.","We collect data from two sources, an observational study and system trace logs, yielding evidence of successfully meeting our requirements.","We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors.","We additionally discuss differences in our process from standard design study methodology which is focused on design studies for data analysis purposes, rather than for presentation.","Supplemental materials are available at: https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541"],"url":"http://arxiv.org/abs/2404.01488v1","category":"cs.HC"}
{"created":"2024-04-01 21:12:44","title":"Explainable AI Integrated Feature Engineering for Wildfire Prediction","abstract":"Wildfires present intricate challenges for prediction, necessitating the use of sophisticated machine learning techniques for effective modeling\\cite{jain2020review}. In our research, we conducted a thorough assessment of various machine learning algorithms for both classification and regression tasks relevant to predicting wildfires. We found that for classifying different types or stages of wildfires, the XGBoost model outperformed others in terms of accuracy and robustness. Meanwhile, the Random Forest regression model showed superior results in predicting the extent of wildfire-affected areas, excelling in both prediction error and explained variance. Additionally, we developed a hybrid neural network model that integrates numerical data and image information for simultaneous classification and regression. To gain deeper insights into the decision-making processes of these models and identify key contributing features, we utilized eXplainable Artificial Intelligence (XAI) techniques, including TreeSHAP, LIME, Partial Dependence Plots (PDP), and Gradient-weighted Class Activation Mapping (Grad-CAM). These interpretability tools shed light on the significance and interplay of various features, highlighting the complex factors influencing wildfire predictions. Our study not only demonstrates the effectiveness of specific machine learning models in wildfire-related tasks but also underscores the critical role of model transparency and interpretability in environmental science applications.","sentences":["Wildfires present intricate challenges for prediction, necessitating the use of sophisticated machine learning techniques for effective modeling\\cite{jain2020review}.","In our research, we conducted a thorough assessment of various machine learning algorithms for both classification and regression tasks relevant to predicting wildfires.","We found that for classifying different types or stages of wildfires, the XGBoost model outperformed others in terms of accuracy and robustness.","Meanwhile, the Random Forest regression model showed superior results in predicting the extent of wildfire-affected areas, excelling in both prediction error and explained variance.","Additionally, we developed a hybrid neural network model that integrates numerical data and image information for simultaneous classification and regression.","To gain deeper insights into the decision-making processes of these models and identify key contributing features, we utilized eXplainable Artificial Intelligence (XAI) techniques, including TreeSHAP, LIME, Partial Dependence Plots (PDP), and Gradient-weighted Class Activation Mapping (Grad-CAM).","These interpretability tools shed light on the significance and interplay of various features, highlighting the complex factors influencing wildfire predictions.","Our study not only demonstrates the effectiveness of specific machine learning models in wildfire-related tasks but also underscores the critical role of model transparency and interpretability in environmental science applications."],"url":"http://arxiv.org/abs/2404.01487v1","category":"cs.LG"}
{"created":"2024-04-01 21:11:43","title":"QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving","abstract":"A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on object detection to find the agents in the scene. However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this representation, we evaluate candidate trajectories around key factors such as collision avoidance, comfort, and progress for safety and interpretability. Our approach achieves better highway driving quality than the state-of-the-art in high-fidelity closed-loop simulations.","sentences":["A self-driving vehicle must understand its environment to determine the appropriate action.","Traditional autonomy systems rely on object detection to find the agents in the scene.","However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents.","Alternatively, dense occupancy grid maps have been utilized to understand free-space.","However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle.","We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan.","Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest.","Exploiting this representation, we evaluate candidate trajectories around key factors such as collision avoidance, comfort, and progress for safety and interpretability.","Our approach achieves better highway driving quality than the state-of-the-art in high-fidelity closed-loop simulations."],"url":"http://arxiv.org/abs/2404.01486v1","category":"cs.RO"}
{"created":"2024-04-01 20:58:24","title":"TraveLER: A Multi-LMM Agent Framework for Video Question-Answering","abstract":"Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to \"Traverse\" through the video, ask questions about individual frames to \"Locate\" and store key information, and then \"Evaluate\" if there is enough information to answer the question. Finally, if there is not enough information, our method is able to \"Replan\" based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets.","sentences":["Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner.","While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified.","Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame.","To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question.","Specifically, we propose TraveLER, a model that can create a plan to \"Traverse\" through the video, ask questions about individual frames to \"Locate\" and store key information, and then \"Evaluate\" if there is enough information to answer the question.","Finally, if there is not enough information, our method is able to \"Replan\" based on its collected knowledge.","Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets."],"url":"http://arxiv.org/abs/2404.01476v1","category":"cs.CV"}
{"created":"2024-04-01 20:56:25","title":"Are large language models superhuman chemists?","abstract":"Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. Here, we introduce \"ChemBench,\" an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a wide array of subfields of the chemical sciences, evaluated leading open and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average. The models, however, struggle with some chemical reasoning tasks that are easy for human experts and provide overconfident, misleading predictions, such as about chemicals' safety profiles. These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences. Our findings also indicate a need for adaptations to chemistry curricula and highlight the importance of continuing to develop evaluation frameworks to improve safe and useful LLMs.","sentences":["Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained.","This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text.","LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously.","However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms.","Here, we introduce \"ChemBench,\" an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists.","We curated more than 7,000 question-answer pairs for a wide array of subfields of the chemical sciences, evaluated leading open and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average.","The models, however, struggle with some chemical reasoning tasks that are easy for human experts and provide overconfident, misleading predictions, such as about chemicals' safety profiles.","These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences.","Our findings also indicate a need for adaptations to chemistry curricula and highlight the importance of continuing to develop evaluation frameworks to improve safe and useful LLMs."],"url":"http://arxiv.org/abs/2404.01475v1","category":"cs.LG"}
{"created":"2024-04-01 20:50:13","title":"Finding Replicable Human Evaluations via Stable Ranking Probability","abstract":"Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs.","sentences":["Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult.","Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential.","Without it, there is no reliable foundation for hill-climbing or product launch decisions.","In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions.","We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization.","Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies.","We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs."],"url":"http://arxiv.org/abs/2404.01474v1","category":"cs.CL"}
{"created":"2024-04-01 20:45:18","title":"Measuring the Redundancy of Information from a Source Failure Perspective","abstract":"In this paper, we define a new measure of the redundancy of information from a fault tolerance perspective. The partial information decomposition (PID) emerged last decade as a framework for decomposing the multi-source mutual information $I(T;X_1, ..., X_n)$ into atoms of redundant, synergistic, and unique information. It built upon the notion of redundancy/synergy from McGill's interaction information (McGill 1954). Separately, the redundancy of system components has served as a principle of fault tolerant engineering, for sensing, routing, and control applications. Here, redundancy is understood as the level of duplication necessary for the fault tolerant performance of a system. With these two perspectives in mind, we propose a new PID-based measure of redundancy $I_{\\text{ft}}$, based upon the presupposition that redundant information is robust to individual source failures. We demonstrate that this new measure satisfies the common PID axioms from (Williams 2010). In order to do so, we establish an order-reversing correspondence between collections of source-fallible instantiations of a system, on the one hand, and the PID lattice from (Williams 2010), on the other.","sentences":["In this paper, we define a new measure of the redundancy of information from a fault tolerance perspective.","The partial information decomposition (PID) emerged last decade as a framework for decomposing the multi-source mutual information $I(T;X_1, ..., X_n)$ into atoms of redundant, synergistic, and unique information.","It built upon the notion of redundancy/synergy from McGill's interaction information (McGill 1954).","Separately, the redundancy of system components has served as a principle of fault tolerant engineering, for sensing, routing, and control applications.","Here, redundancy is understood as the level of duplication necessary for the fault tolerant performance of a system.","With these two perspectives in mind, we propose a new PID-based measure of redundancy $I_{\\text{ft}}$, based upon the presupposition that redundant information is robust to individual source failures.","We demonstrate that this new measure satisfies the common PID axioms from (Williams 2010).","In order to do so, we establish an order-reversing correspondence between collections of source-fallible instantiations of a system, on the one hand, and the PID lattice from (Williams 2010), on the other."],"url":"http://arxiv.org/abs/2404.01470v1","category":"cs.IT"}
{"created":"2024-04-01 20:25:04","title":"Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images","abstract":"4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression. However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects. Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult. To address this challenge, this paper proposes a simple yet effective Unsupervised Volumetric Interpolation framework, UVI-Net. This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing unsupervised methods. Experiments on benchmark datasets demonstrate significant improvements across diverse evaluation metrics compared to unsupervised and supervised baselines. Remarkably, our approach achieves this superior performance even when trained with a dataset as small as one, highlighting its exceptional robustness and efficiency in scenarios with sparse supervision. This positions UVI-Net as a compelling alternative for 4D medical imaging, particularly in settings where data availability is limited. The source code is available at https://github.com/jungeun122333/UVI-Net.","sentences":["4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression.","However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects.","Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult.","To address this challenge, this paper proposes a simple yet effective Unsupervised Volumetric Interpolation framework, UVI-Net.","This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing unsupervised methods.","Experiments on benchmark datasets demonstrate significant improvements across diverse evaluation metrics compared to unsupervised and supervised baselines.","Remarkably, our approach achieves this superior performance even when trained with a dataset as small as one, highlighting its exceptional robustness and efficiency in scenarios with sparse supervision.","This positions UVI-Net as a compelling alternative for 4D medical imaging, particularly in settings where data availability is limited.","The source code is available at https://github.com/jungeun122333/UVI-Net."],"url":"http://arxiv.org/abs/2404.01464v1","category":"eess.IV"}
{"created":"2024-04-01 20:13:28","title":"Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers","abstract":"Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs. Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem. This work introduces a unique approach combining Game Theory (GT) and Deep Reinforcement Learning (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs. The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints. We conducted extensive experiments comparing our game-theoretic DRL (GT-DRL) approach with current DRL-based and other optimization techniques. The results demonstrate that our strategy outperforms the state-of-the-art in reducing carbon emissions and minimizing cloud operating costs without compromising computational performance. This work has significant implications for achieving sustainability and cost-efficiency in data centers handling AI inference workloads across diverse geographic locations.","sentences":["Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs.","Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem.","This work introduces a unique approach combining Game Theory (GT) and Deep Reinforcement Learning (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs.","The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints.","We conducted extensive experiments comparing our game-theoretic DRL (GT-DRL) approach with current DRL-based and other optimization techniques.","The results demonstrate that our strategy outperforms the state-of-the-art in reducing carbon emissions and minimizing cloud operating costs without compromising computational performance.","This work has significant implications for achieving sustainability and cost-efficiency in data centers handling AI inference workloads across diverse geographic locations."],"url":"http://arxiv.org/abs/2404.01459v1","category":"cs.DC"}
{"created":"2024-04-01 19:56:41","title":"Unveiling Divergent Inductive Biases of LLMs on Temporal Data","abstract":"Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for \"AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards \"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards \"TRUE'', and GPT-4 exhibits a preference for \"FALSE'' in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.","sentences":["Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics.","Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge.","This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data.","Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events.","The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4.","Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for \"AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards \"BEFORE''.","Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards \"TRUE'', and GPT-4 exhibits a preference for \"FALSE'' in the TE format for both implicit and explicit events.","This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity."],"url":"http://arxiv.org/abs/2404.01453v1","category":"cs.CL"}
{"created":"2024-04-01 19:33:41","title":"Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning","abstract":"Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was explored for tumor detection at low magnification levels and TP53 mutations at various levels. Our results show that a novel additive implementation of MIL matched the performance of reference implementation (AUC 0.96), and was only slightly outperformed by Attention MIL (AUC 0.97). More interestingly from the perspective of the molecular pathologist, these different AI architectures identify distinct sensitivities to morphological features (through the detection of Regions of Interest, RoI) at different amplification levels. Tellingly, TP53 mutation was most sensitive to features at the higher applications where cellular morphology is resolved.","sentences":["Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology.","However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level.","It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level.","This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level.","To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC).","This approach was explored for tumor detection at low magnification levels and TP53 mutations at various levels.","Our results show that a novel additive implementation of MIL matched the performance of reference implementation (AUC 0.96), and was only slightly outperformed by Attention MIL (AUC 0.97).","More interestingly from the perspective of the molecular pathologist, these different AI architectures identify distinct sensitivities to morphological features (through the detection of Regions of Interest, RoI) at different amplification levels.","Tellingly, TP53 mutation was most sensitive to features at the higher applications where cellular morphology is resolved."],"url":"http://arxiv.org/abs/2404.01446v1","category":"cs.CV"}
{"created":"2024-04-01 19:28:52","title":"Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing","abstract":"Knowledge management is a critical challenge for enterprises in today's digital world, as the volume and complexity of data being generated and collected continue to grow incessantly. Knowledge graphs (KG) emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data. This paper builds upon a recent survey of the research literature on combining KGs and Natural Language Processing (NLP). Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination. We cover various approaches from the three core areas of KG construction, reasoning as well as KG-based NLP tasks. In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future.","sentences":["Knowledge management is a critical challenge for enterprises in today's digital world, as the volume and complexity of data being generated and collected continue to grow incessantly.","Knowledge graphs (KG) emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data.","This paper builds upon a recent survey of the research literature on combining KGs and Natural Language Processing (NLP).","Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination.","We cover various approaches from the three core areas of KG construction, reasoning as well as KG-based NLP tasks.","In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future."],"url":"http://arxiv.org/abs/2404.01443v1","category":"cs.CL"}
{"created":"2024-04-01 19:23:00","title":"Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects","abstract":"We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt","sentences":["We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states.","We decompose the problem into two stages, each addressing distinct aspects.","Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states.","By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work.","It also handles more than one movable part and does not rely on any object shape or structure priors.","Project page: https://github.com/NVlabs/DigitalTwinArt"],"url":"http://arxiv.org/abs/2404.01440v1","category":"cs.CV"}
{"created":"2024-04-01 19:22:58","title":"Creating emoji lexica from unsupervised sentiment analysis of their descriptions","abstract":"Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the sentiment expressed by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks. This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji sentiment lexicon using an unsupervised sentiment analysis system based on the definitions given by emoji creators in Emojipedia. Additionally, we automatically created lexicon variants by also considering the sentiment distribution of the informal texts accompanying emojis. All these lexica are evaluated and compared regarding the improvement obtained by including them in sentiment analysis of the annotated datasets provided by Kralj Novak et al. (2015). The results confirm the competitiveness of our approach.","sentences":["Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations.","Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics.","So far, the sentiment expressed by emojis has received little attention.","The use of symbols, however, has boomed in the past four years.","About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks.","This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks.","For this purpose, we automatically constructed a novel emoji sentiment lexicon using an unsupervised sentiment analysis system based on the definitions given by emoji creators in Emojipedia.","Additionally, we automatically created lexicon variants by also considering the sentiment distribution of the informal texts accompanying emojis.","All these lexica are evaluated and compared regarding the improvement obtained by including them in sentiment analysis of the annotated datasets provided by Kralj Novak et al.","(2015).","The results confirm the competitiveness of our approach."],"url":"http://arxiv.org/abs/2404.01439v1","category":"cs.CL"}
{"created":"2024-04-01 19:22:43","title":"Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis","abstract":"A question in the realm of deepfakes is slowly emerging pertaining to whether we can go beyond facial deepfakes and whether it would be beneficial to society. Therefore, this research presents a positive application of deepfake technology in upper body generation, while performing sign-language for the Deaf and Hard of Hearing (DHoH) community. The resulting videos are later vetted with a sign language expert. This is particularly helpful, given the intricate nature of sign language, a scarcity of sign language experts, and potential benefits for health and education. The objectives of this work encompass constructing a reliable deepfake dataset, evaluating its technical and visual credibility through computer vision and natural language processing models, and assessing the plausibility of the generated content. With over 1200 videos, featuring both previously seen and unseen individuals for the generation model, using the help of a sign language expert, we establish a deepfake dataset in sign language that can further be utilized to detect fake videos that may target certain people of determination.","sentences":["A question in the realm of deepfakes is slowly emerging pertaining to whether we can go beyond facial deepfakes and whether it would be beneficial to society.","Therefore, this research presents a positive application of deepfake technology in upper body generation, while performing sign-language for the Deaf and Hard of Hearing (DHoH) community.","The resulting videos are later vetted with a sign language expert.","This is particularly helpful, given the intricate nature of sign language, a scarcity of sign language experts, and potential benefits for health and education.","The objectives of this work encompass constructing a reliable deepfake dataset, evaluating its technical and visual credibility through computer vision and natural language processing models, and assessing the plausibility of the generated content.","With over 1200 videos, featuring both previously seen and unseen individuals for the generation model, using the help of a sign language expert, we establish a deepfake dataset in sign language that can further be utilized to detect fake videos that may target certain people of determination."],"url":"http://arxiv.org/abs/2404.01438v1","category":"cs.CV"}
{"created":"2024-04-01 19:20:32","title":"The Radar Ghost Dataset -- An Evaluation of Ghost Objects in Automotive Radar Data","abstract":"Radar sensors have a long tradition in advanced driver assistance systems (ADAS) and also play a major role in current concepts for autonomous vehicles. Their importance is reasoned by their high robustness against meteorological effects, such as rain, snow, or fog, and the radar's ability to measure relative radial velocity differences via the Doppler effect. The cause for these advantages, namely the large wavelength, is also one of the drawbacks of radar sensors. Compared to camera or lidar sensor, a lot more surfaces in a typical traffic scenario appear flat relative to the radar's emitted signal. This results in multi-path reflections or so called ghost detections in the radar signal. Ghost objects pose a major source for potential false positive detections in a vehicle's perception pipeline. Therefore, it is important to be able to segregate multi-path reflections from direct ones. In this article, we present a dataset with detailed manual annotations for different kinds of ghost detections. Moreover, two different approaches for identifying these kinds of objects are evaluated. We hope that our dataset encourages more researchers to engage in the fields of multi-path object suppression or exploitation.","sentences":["Radar sensors have a long tradition in advanced driver assistance systems (ADAS) and also play a major role in current concepts for autonomous vehicles.","Their importance is reasoned by their high robustness against meteorological effects, such as rain, snow, or fog, and the radar's ability to measure relative radial velocity differences via the Doppler effect.","The cause for these advantages, namely the large wavelength, is also one of the drawbacks of radar sensors.","Compared to camera or lidar sensor, a lot more surfaces in a typical traffic scenario appear flat relative to the radar's emitted signal.","This results in multi-path reflections or so called ghost detections in the radar signal.","Ghost objects pose a major source for potential false positive detections in a vehicle's perception pipeline.","Therefore, it is important to be able to segregate multi-path reflections from direct ones.","In this article, we present a dataset with detailed manual annotations for different kinds of ghost detections.","Moreover, two different approaches for identifying these kinds of objects are evaluated.","We hope that our dataset encourages more researchers to engage in the fields of multi-path object suppression or exploitation."],"url":"http://arxiv.org/abs/2404.01437v1","category":"cs.CV"}
{"created":"2024-04-01 19:09:36","title":"Wehnelt Photoemission in an Ultrafast Electron Microscope: Stability and Usability","abstract":"We tested and compared the stability and usability of three different cathode materials and configurations in a thermionic-based ultrafast electron microscope: (1) on-axis thermionic and photoemission from a 0.1-mm diameter LaB6 source with graphite guard ring, (2) off-axis photoemission from the Ni aperture surface of the Wehnelt electrode, and (3) on-axis thermionic and photoemission from a 0.2-mm diameter polycrystalline Ta source. For each cathode type and configuration, we illustrate how the photoelectron beam-current stability is deleteriously impacted by simultaneous cooling of the source following thermionic heating. Further, we demonstrate usability via collection of parallel- and convergent-beam electron diffraction patterns and by formation of optimum probe size. We find that usability of the off-axis Ni Wehnelt-aperture photoemission is at least comparable to on-axis LaB6 thermionic emission, as well as to on-axis photoemission. However, the stability and achievable beam currents for off-axis photoemission from the Wehnelt aperture were superior to that of the other cathode types and configurations, regardless of the electron-emission mechanism. Beam-current stability for this configuration was found to be within 1% of the mean for 70 minutes (longest duration tested), and steady-state beam current was reached within the sampling-time resolution used here (~1 s) for 15 pA beam currents (i.e., 460 electrons per packet for a 200 kHz repetition rate). Repeatability and robustness of the steady-state condition was also found to be within 1% of the mean. We discuss the implications of these findings for UEM imaging and diffraction experiments, for pulsed-beam damage measurements, and for practical switching between optimum conventional TEM and UEM operation within the same instrument.","sentences":["We tested and compared the stability and usability of three different cathode materials and configurations in a thermionic-based ultrafast electron microscope: (1) on-axis thermionic and photoemission from a 0.1-mm diameter LaB6 source with graphite guard ring, (2) off-axis photoemission from the Ni aperture surface of the Wehnelt electrode, and (3) on-axis thermionic and photoemission from a 0.2-mm diameter polycrystalline Ta source.","For each cathode type and configuration, we illustrate how the photoelectron beam-current stability is deleteriously impacted by simultaneous cooling of the source following thermionic heating.","Further, we demonstrate usability via collection of parallel- and convergent-beam electron diffraction patterns and by formation of optimum probe size.","We find that usability of the off-axis Ni Wehnelt-aperture photoemission is at least comparable to on-axis LaB6 thermionic emission, as well as to on-axis photoemission.","However, the stability and achievable beam currents for off-axis photoemission from the Wehnelt aperture were superior to that of the other cathode types and configurations, regardless of the electron-emission mechanism.","Beam-current stability for this configuration was found to be within 1% of the mean for 70 minutes (longest duration tested), and steady-state beam current was reached within the sampling-time resolution used here (~1 s) for 15 pA beam currents (i.e., 460 electrons per packet for a 200 kHz repetition rate).","Repeatability and robustness of the steady-state condition was also found to be within 1% of the mean.","We discuss the implications of these findings for UEM imaging and diffraction experiments, for pulsed-beam damage measurements, and for practical switching between optimum conventional TEM and UEM operation within the same instrument."],"url":"http://arxiv.org/abs/2404.01434v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-01 19:04:17","title":"Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs","abstract":"Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.","sentences":["Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts.","This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs.","However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence.","In this study, we conduct extensive experiments to investigate the root causes of positional bias.","Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models.","We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences.","To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context.","Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge."],"url":"http://arxiv.org/abs/2404.01430v1","category":"cs.CL"}
{"created":"2024-04-01 18:31:24","title":"Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data","abstract":"The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this result by proving that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations. We next empirically test whether accumulating data similarly prevents model collapse by pretraining sequences of language models on text corpora. We confirm that replacing data does indeed cause model collapse, then demonstrate that accumulating data prevents model collapse; these results hold across a range of model sizes, architectures and hyperparameters. We further show that similar results hold for other deep generative models on real data: diffusion models for molecule generation and variational autoencoders for image generation. Our work provides consistent theoretical and empirical evidence that data accumulation mitigates model collapse.","sentences":["The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs?","Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless.","However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time.","In this paper, we compare these two settings and show that accumulating data prevents model collapse.","We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions.","Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this result by proving that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations.","We next empirically test whether accumulating data similarly prevents model collapse by pretraining sequences of language models on text corpora.","We confirm that replacing data does indeed cause model collapse, then demonstrate that accumulating data prevents model collapse; these results hold across a range of model sizes, architectures and hyperparameters.","We further show that similar results hold for other deep generative models on real data: diffusion models for molecule generation and variational autoencoders for image generation.","Our work provides consistent theoretical and empirical evidence that data accumulation mitigates model collapse."],"url":"http://arxiv.org/abs/2404.01413v1","category":"cs.LG"}
{"created":"2024-04-01 18:26:29","title":"OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation","abstract":"In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task. By addressing the deficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9\\% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation.","sentences":["In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets.","Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting.","These methods often fall short in effectively handling the ingredients, particularly new and diverse ones.","In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context.","By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder.","The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation.","The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task.","By addressing the deficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9\\% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation."],"url":"http://arxiv.org/abs/2404.01409v1","category":"cs.CV"}
{"created":"2024-04-01 18:13:02","title":"Towards a potential paradigm shift in health data collection and analysis","abstract":"Industrial Revolution 4.0 transforms healthcare systems. The first three technological revolutions changed the relationship between human and machine interaction due to the exponential growth of machine numbers. The fourth revolution put humans into a situation where heterogeneous data is produced with unmatched quantity and quality not only by traditional methods, enforced by digitization, but also by ubiquitous computing, machine-to-machine interactions and smart environment. The modern cyber-physical space underlines the role of the person in the expanding context of computerization and big data processing. In healthcare, where data collection and analysis particularly depend on human efforts, the disruptive nature of these developments is evident. Adaptation to this process requires deep scrutiny of the trends and recognition of future medical data technologies` evolution. Significant difficulties arise from discrepancies in requirements by healthcare, administrative and technology stakeholders. Black box and grey box decisions made in medical imaging and diagnostic Decision Support Software are often not transparent enough for the professional, social and medico-legal requirements. While Explainable AI proposes a partial solution for AI applications in medicine, the approach has to be wider and multiplex. LLM potential and limitations are also discussed. This paper lists the most significant issues in these topics and describes possible solutions.","sentences":["Industrial Revolution 4.0 transforms healthcare systems.","The first three technological revolutions changed the relationship between human and machine interaction due to the exponential growth of machine numbers.","The fourth revolution put humans into a situation where heterogeneous data is produced with unmatched quantity and quality not only by traditional methods, enforced by digitization, but also by ubiquitous computing, machine-to-machine interactions and smart environment.","The modern cyber-physical space underlines the role of the person in the expanding context of computerization and big data processing.","In healthcare, where data collection and analysis particularly depend on human efforts, the disruptive nature of these developments is evident.","Adaptation to this process requires deep scrutiny of the trends and recognition of future medical data technologies` evolution.","Significant difficulties arise from discrepancies in requirements by healthcare, administrative and technology stakeholders.","Black box and grey box decisions made in medical imaging and diagnostic Decision Support Software are often not transparent enough for the professional, social and medico-legal requirements.","While Explainable AI proposes a partial solution for AI applications in medicine, the approach has to be wider and multiplex.","LLM potential and limitations are also discussed.","This paper lists the most significant issues in these topics and describes possible solutions."],"url":"http://arxiv.org/abs/2404.01403v1","category":"cs.HC"}
{"created":"2024-04-01 18:12:09","title":"ContactHandover: Contact-Guided Robot-to-Human Object Handover","abstract":"Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on https://clairezixiwang.github.io/ContactHandover.github.io","sentences":["Robot-to-human object handover is an important step in many human robot collaboration tasks.","A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner.","We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase.","During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object.","The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp.","During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements.","We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines.","More results can be found on https://clairezixiwang.github.io/ContactHandover.github.io"],"url":"http://arxiv.org/abs/2404.01402v1","category":"cs.RO"}
{"created":"2024-04-01 18:08:58","title":"Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition","abstract":"Nowadays, users demand for increased personalization of vision systems to localize and identify personal instances of objects (e.g., my dog rather than dog) from a few-shot dataset only. Despite outstanding results of deep networks on classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model for standard object detection), they struggle to maintain within-class variability to represent different instances rather than object categories only. We construct an Object-conditioned Bag of Instances (OBoI) based on multi-order statistics of extracted features, where generic object detection models are extended to search and identify personal instances from the OBoI's metric space, without need for backpropagation. By relying on multi-order statistics, OBoI achieves consistent superior accuracy in distinguishing different instances. In the results, we achieve 77.1% personal object recognition accuracy in case of 18 personal instances, showing about 12% relative gain over the state of the art.","sentences":["Nowadays, users demand for increased personalization of vision systems to localize and identify personal instances of objects (e.g., my dog rather than dog) from a few-shot dataset only.","Despite outstanding results of deep networks on classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model for standard object detection), they struggle to maintain within-class variability to represent different instances rather than object categories only.","We construct an Object-conditioned Bag of Instances (OBoI) based on multi-order statistics of extracted features, where generic object detection models are extended to search and identify personal instances from the OBoI's metric space, without need for backpropagation.","By relying on multi-order statistics, OBoI achieves consistent superior accuracy in distinguishing different instances.","In the results, we achieve 77.1% personal object recognition accuracy in case of 18 personal instances, showing about 12% relative gain over the state of the art."],"url":"http://arxiv.org/abs/2404.01397v1","category":"cs.CV"}
{"created":"2024-04-01 17:59:55","title":"NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields","abstract":"Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.","sentences":["Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics.","Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images.","Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs.","We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular.","Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling.","Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches.","In doing so, the model can learn the semantic and spatial structure of complete scenes.","We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images.","Once pretrained, the encoder is used for effective 3D transfer learning.","Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks.","Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection."],"url":"http://arxiv.org/abs/2404.01300v1","category":"cs.CV"}
{"created":"2024-04-01 17:59:53","title":"Noise2Image: Noise-Enabled Static Scene Recovery for Event Cameras","abstract":"Event cameras capture changes of intensity over time as a stream of 'events' and generally cannot measure intensity itself; hence, they are only used for imaging dynamic scenes. However, fluctuations due to random photon arrival inevitably trigger noise events, even for static scenes. While previous efforts have been focused on filtering out these undesirable noise events to improve signal quality, we find that, in the photon-noise regime, these noise events are correlated with the static scene intensity. We analyze the noise event generation and model its relationship to illuminance. Based on this understanding, we propose a method, called Noise2Image, to leverage the illuminance-dependent noise characteristics to recover the static parts of a scene, which are otherwise invisible to event cameras. We experimentally collect a dataset of noise events on static scenes to train and validate Noise2Image. Our results show that Noise2Image can robustly recover intensity images solely from noise events, providing a novel approach for capturing static scenes in event cameras, without additional hardware.","sentences":["Event cameras capture changes of intensity over time as a stream of 'events' and generally cannot measure intensity itself; hence, they are only used for imaging dynamic scenes.","However, fluctuations due to random photon arrival inevitably trigger noise events, even for static scenes.","While previous efforts have been focused on filtering out these undesirable noise events to improve signal quality, we find that, in the photon-noise regime, these noise events are correlated with the static scene intensity.","We analyze the noise event generation and model its relationship to illuminance.","Based on this understanding, we propose a method, called Noise2Image, to leverage the illuminance-dependent noise characteristics to recover the static parts of a scene, which are otherwise invisible to event cameras.","We experimentally collect a dataset of noise events on static scenes to train and validate Noise2Image.","Our results show that Noise2Image can robustly recover intensity images solely from noise events, providing a novel approach for capturing static scenes in event cameras, without additional hardware."],"url":"http://arxiv.org/abs/2404.01298v1","category":"cs.CV"}
{"created":"2024-04-01 17:59:53","title":"CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes","abstract":"Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.","sentences":["Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis.","To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\" cartoon series.","With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships.","These factors allow models to solve more challenging, yet well-defined causal relationships.","We also introduce hard negative mining, including CausalConfusion version.","While models perform well, there is much room for improvement, especially, on open-ended answers.","We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon.","Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field.","We will release our dataset, codes, and models to help future efforts in this domain."],"url":"http://arxiv.org/abs/2404.01299v1","category":"cs.CV"}
{"created":"2024-04-01 17:59:11","title":"MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space","abstract":"We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: https://syntec-research.github.io/MagicMirror","sentences":["We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization.","Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis.","Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation.","Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry.","These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues.","As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts.","You can find more results and videos in our website: https://syntec-research.github.io/MagicMirror"],"url":"http://arxiv.org/abs/2404.01296v1","category":"cs.CV"}
{"created":"2024-04-01 17:59:06","title":"Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models","abstract":"As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.","sentences":["As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience.","A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm.","Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health.","In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM.","We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs.","Our experiments demonstrate that our method can rewind a learned model and unlock its controllability."],"url":"http://arxiv.org/abs/2404.01295v1","category":"cs.CL"}
{"created":"2024-04-01 17:58:06","title":"Evaluating Text-to-Visual Generation with Image-to-Text Generation","abstract":"Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a \"bag of words\", conflating prompts such as \"the horse is eating the grass\" with \"the grass is eating the horse\". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a \"Yes\" answer to a simple \"Does this figure show '{text}'?\" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.","sentences":["Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks.","For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations.","One reason is that text encoders of CLIP can notoriously act as a \"bag of words\", conflating prompts such as \"the horse is eating the grass\" with \"the grass is eating the horse\".","To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a \"Yes\" answer to a simple \"Does this figure show '{text}'?\" question.","Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks.","We also compute VQAScore with an in-house model that follows best practices in the literature.","For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa).","Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models.","VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts.","We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic.","GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2."],"url":"http://arxiv.org/abs/2404.01291v1","category":"cs.CV"}
{"created":"2024-04-01 17:56:06","title":"Prompt-prompted Mixture of Experts for Efficient LLM Generation","abstract":"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50\\% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available at https://github.com/hdong920/GRIFFIN.","sentences":["With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment.","Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements.","However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures.","To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions.","This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking.","Despite our method's simplicity, we show with 50\\% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\\times$ speed-up in Llama 2 13B on an NVIDIA L40).","Code will be available at https://github.com/hdong920/GRIFFIN."],"url":"http://arxiv.org/abs/2404.01365v1","category":"cs.LG"}
{"created":"2024-04-01 17:52:27","title":"Lower bounds and integrality gaps in simplicial decomposition","abstract":"Let $\\mathcal{K}$ be a finite pure simplicial $d$-complex, with oriented facets $\\{F_i\\}$, which is boundaryless in the sense that $\\sum\\partial F_i=0$. We call such a $\\mathcal{K}$ an \\textit{admissible $d$-complex}. Given an admissible $d$-complex, one can ask for the smallest collection $\\{T_i\\}$ of oriented $(d+1)$-simplices on the vertices of $\\mathcal{K}$ which decomposes $\\mathcal{K}$ in the sense that $\\sum \\partial T_i = \\mathcal{K}$. Let the minimum size of such a collection be $V_\\mathbb{Z}(\\mathcal{K})$, and let $V_\\mathbb{Q}(\\mathcal{K})$ be the relaxed analog where fractional $(d+1)$-simplices may be used. We explain how these quantities may be computed via integer and linear programming, and show how lower bounds may be obtained by exploiting LP-duality. We then prove that $V_\\mathbb{Q}$ and $V_\\mathbb{Z}$ are both additive under disjoint union and connected sum along a $d$-simplex. The remainder of the paper explores integrality gaps between $V_\\mathbb{Z}$ and $V_\\mathbb{Q}$ in dimension 1, where we share what we believe is the simplest admissible complex with an integrality gap; and in dimension 2, where we collect some results on integrality gaps for triangulations of the 2-sphere for a companion paper with Zili Wang and Peter Doyle.","sentences":["Let $\\mathcal{K}$ be a finite pure simplicial $d$-complex, with oriented facets $\\{F_i\\}$, which is boundaryless in the sense that $\\sum\\partial F_i=0$. We call such a $\\mathcal{K}$ an \\textit{admissible $d$-complex}.","Given an admissible $d$-complex, one can ask for the smallest collection $\\{T_i\\}$ of oriented $(d+1)$-simplices on the vertices of $\\mathcal{K}$ which decomposes $\\mathcal{K}$ in the sense that $\\sum \\partial T_i = \\mathcal{K}$. Let the minimum size of such a collection be $V_\\mathbb{Z}(\\mathcal{K})$, and let $V_\\mathbb{Q}(\\mathcal{K})$ be the relaxed analog where fractional $(d+1)$-simplices may be used.","We explain how these quantities may be computed via integer and linear programming, and show how lower bounds may be obtained by exploiting LP-duality.","We then prove that $V_\\mathbb{Q}$ and $V_\\mathbb{Z}$ are both additive under disjoint union and connected sum along a $d$-simplex.","The remainder of the paper explores integrality gaps between $V_\\mathbb{Z}$ and $V_\\mathbb{Q}$ in dimension 1, where we share what we believe is the simplest admissible complex with an integrality gap; and in dimension 2, where we collect some results on integrality gaps for triangulations of the 2-sphere for a companion paper with Zili Wang and Peter Doyle."],"url":"http://arxiv.org/abs/2404.01279v1","category":"math.AT"}
{"created":"2024-04-01 17:45:15","title":"Mapping the Increasing Use of LLMs in Scientific Papers","abstract":"Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.","sentences":["Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time.","Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices.","However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs.","To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time.","Our statistical estimation operates on the corpus level and is more robust than inference on individual instances.","Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%).","In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%).","Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths.","Our findings suggests that LLMs are being broadly used in scientific writings."],"url":"http://arxiv.org/abs/2404.01268v1","category":"cs.CL"}
{"created":"2024-04-01 17:43:27","title":"IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations","abstract":"Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.","sentences":["Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs.","But do their capabilities change depending on the input modality?","In this work, we propose $\\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games.","Each example is presented with multiple $\\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations.","IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation.","Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations.","Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse.","Finally, we present two prompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations."],"url":"http://arxiv.org/abs/2404.01266v2","category":"cs.AI"}
{"created":"2024-04-01 17:35:57","title":"Artificial Intelligence and the Spatial Documentation of Languages","abstract":"The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and interactive web maps and streamlining the mapmaking process, despite facing challenges like inconsistencies and difficulties in adding legends. The findings suggest a promising future for AI in generating language maps and enhancing the work of documentary linguists as they collect their data in the field pointing towards the need for further development to fully harness AI potential in this field.","sentences":["The advancement in technology has made interdisciplinary research more accessible.","Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields.","This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation.","The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise.","The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork.","The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps.","The study highlights the two AI models capabilities in generating highquality static and interactive web maps and streamlining the mapmaking process, despite facing challenges like inconsistencies and difficulties in adding legends.","The findings suggest a promising future for AI in generating language maps and enhancing the work of documentary linguists as they collect their data in the field pointing towards the need for further development to fully harness AI potential in this field."],"url":"http://arxiv.org/abs/2404.01263v1","category":"cs.CL"}
{"created":"2024-04-01 17:34:18","title":"Information Plane Analysis Visualization in Deep Learning via Transfer Entropy","abstract":"In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting. In contrast to mutual information, TE can capture temporal relationships between variables. To explore such links, in our novel approach we use TE to quantify information transfer between neural layers and perform Information Plane analysis. We obtained encouraging experimental results, opening the possibility for further investigations.","sentences":["In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training.","According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output.","Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation.","The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting.","In contrast to mutual information, TE can capture temporal relationships between variables.","To explore such links, in our novel approach we use TE to quantify information transfer between neural layers and perform Information Plane analysis.","We obtained encouraging experimental results, opening the possibility for further investigations."],"url":"http://arxiv.org/abs/2404.01364v1","category":"cs.LG"}
{"created":"2024-04-01 17:33:38","title":"FABLES: Evaluating faithfulness and content selection in book-length summarization","abstract":"While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.","sentences":["While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness.","In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books.","Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden.","We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo.","An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate.","While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims.","Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding.","Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book."],"url":"http://arxiv.org/abs/2404.01261v1","category":"cs.CL"}
{"created":"2024-04-01 17:32:22","title":"AIOps Solutions for Incident Management: Technical Guidelines and A Comprehensive Literature Review","abstract":"The management of modern IT systems poses unique challenges, necessitating scalability, reliability, and efficiency in handling extensive data streams. Traditional methods, reliant on manual tasks and rule-based approaches, prove inefficient for the substantial data volumes and alerts generated by IT systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a solution, leveraging advanced analytics like machine learning and big data to enhance incident management. AIOps detects and predicts incidents, identifies root causes, and automates healing actions, improving quality and reducing operational costs. However, despite its potential, the AIOps domain is still in its early stages, decentralized across multiple sectors, and lacking standardized conventions. Research and industrial contributions are distributed without consistent frameworks for data management, target problems, implementation details, requirements, and capabilities. This study proposes an AIOps terminology and taxonomy, establishing a structured incident management procedure and providing guidelines for constructing an AIOps framework. The research also categorizes contributions based on criteria such as incident management tasks, application areas, data sources, and technical approaches. The goal is to provide a comprehensive review of technical and research aspects in AIOps for incident management, aiming to structure knowledge, identify gaps, and establish a foundation for future developments in the field.","sentences":["The management of modern IT systems poses unique challenges, necessitating scalability, reliability, and efficiency in handling extensive data streams.","Traditional methods, reliant on manual tasks and rule-based approaches, prove inefficient for the substantial data volumes and alerts generated by IT systems.","Artificial Intelligence for Operating Systems (AIOps) has emerged as a solution, leveraging advanced analytics like machine learning and big data to enhance incident management.","AIOps detects and predicts incidents, identifies root causes, and automates healing actions, improving quality and reducing operational costs.","However, despite its potential, the AIOps domain is still in its early stages, decentralized across multiple sectors, and lacking standardized conventions.","Research and industrial contributions are distributed without consistent frameworks for data management, target problems, implementation details, requirements, and capabilities.","This study proposes an AIOps terminology and taxonomy, establishing a structured incident management procedure and providing guidelines for constructing an AIOps framework.","The research also categorizes contributions based on criteria such as incident management tasks, application areas, data sources, and technical approaches.","The goal is to provide a comprehensive review of technical and research aspects in AIOps for incident management, aiming to structure knowledge, identify gaps, and establish a foundation for future developments in the field."],"url":"http://arxiv.org/abs/2404.01363v1","category":"cs.OS"}
{"created":"2024-04-01 17:30:56","title":"Bridging Remote Sensors with Multisensor Geospatial Foundation Models","abstract":"In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.","sentences":["In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities.","Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities.","This integration spans an expansive dataset of two million multisensor images.","msGFM is uniquely adept at handling both paired and unpaired sensor data.","For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors.","msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types.","msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks.","These include scene classification, segmentation, cloud removal, and pan-sharpening.","A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field.","Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities."],"url":"http://arxiv.org/abs/2404.01260v1","category":"cs.CV"}
{"created":"2024-04-01 17:28:16","title":"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward","abstract":"Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.","sentences":["Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM).","However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge.","Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established.","This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions.","Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input.","Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks."],"url":"http://arxiv.org/abs/2404.01258v2","category":"cs.CV"}
{"created":"2024-04-01 16:57:42","title":"Characterization of Atmosphere-Skimming Cosmic-Ray Showers in High-Altitude Experiments","abstract":"Atmosphere-skimming showers are initiated by cosmic rays with incoming directions such that the full development of the cascade occurs inside the atmosphere without ever reaching the ground. This new class of showers has been observed in balloon-borne experiments such as ANITA, but a characterisation of their properties is lacking. The interplay between the Earth's magnetic field, the long distances over which atmosphere-skimming showers develop, and the low density of the atmosphere they traverse gives rise to several effects that are not seen in downward-going cascades, and require detailed modeling. In this article, we used the latest version of the ZHAireS-RASPASS shower simulation program to tackle this problem, and dwell on the particular phenomena that arises from the peculiar environment on which these showers develop. We focus in particular on the properties of the longitudinal profile of the shower and its fluctuations as a function of cosmic-ray energy, direction and primary mass. We have also studied the phase-space of cosmic-ray arrival directions where detection in high-altitude experiments is more likely, and have found that only in a small range of directions the showers are sufficiently developed before reaching the altitude of the detector. Our results are relevant for the design of high-altitude and in particular balloon-borne experiments, and for the interpretation of the data they collect.","sentences":["Atmosphere-skimming showers are initiated by cosmic rays with incoming directions such that the full development of the cascade occurs inside the atmosphere without ever reaching the ground.","This new class of showers has been observed in balloon-borne experiments such as ANITA, but a characterisation of their properties is lacking.","The interplay between the Earth's magnetic field, the long distances over which atmosphere-skimming showers develop, and the low density of the atmosphere they traverse gives rise to several effects that are not seen in downward-going cascades, and require detailed modeling.","In this article, we used the latest version of the ZHAireS-RASPASS shower simulation program to tackle this problem, and dwell on the particular phenomena that arises from the peculiar environment on which these showers develop.","We focus in particular on the properties of the longitudinal profile of the shower and its fluctuations as a function of cosmic-ray energy, direction and primary mass.","We have also studied the phase-space of cosmic-ray arrival directions where detection in high-altitude experiments is more likely, and have found that only in a small range of directions the showers are sufficiently developed before reaching the altitude of the detector.","Our results are relevant for the design of high-altitude and in particular balloon-borne experiments, and for the interpretation of the data they collect."],"url":"http://arxiv.org/abs/2404.01239v1","category":"astro-ph.HE"}
{"created":"2024-04-01 16:31:04","title":"Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing","abstract":"Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/","sentences":["Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes.","Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects.","We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language.","Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries.","Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries.","We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language.","Project website: https://feature-splatting.github.io/"],"url":"http://arxiv.org/abs/2404.01223v1","category":"cs.CV"}
{"created":"2024-04-01 16:25:08","title":"Entity-Centric Reinforcement Learning for Object Manipulation from Pixels","abstract":"Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl","sentences":["Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics.","In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation.","In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations.","In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects.","Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order).","We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects.","Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl"],"url":"http://arxiv.org/abs/2404.01220v1","category":"cs.RO"}
{"created":"2024-04-01 16:17:11","title":"Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy","abstract":"Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.","sentences":["Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management.","With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions.","However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster).","Such challenges are usually classified under domain generalization.","In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs).","We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models.","To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model.","Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns.","We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods."],"url":"http://arxiv.org/abs/2404.01217v1","category":"cs.LG"}
{"created":"2024-04-01 15:49:50","title":"iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer","abstract":"Gastric cancer (GC) is a prevalent malignancy worldwide, ranking as the fifth most common cancer with over 1 million new cases and 700 thousand deaths in 2020. Locally advanced gastric cancer (LAGC) accounts for approximately two-thirds of GC diagnoses, and neoadjuvant chemotherapy (NACT) has emerged as the standard treatment for LAGC. However, the effectiveness of NACT varies significantly among patients, with a considerable subset displaying treatment resistance. Ineffective NACT not only leads to adverse effects but also misses the optimal therapeutic window, resulting in lower survival rate. However, existing multimodal learning methods assume the availability of all modalities for each patient, which does not align with the reality of clinical practice. The limited availability of modalities for each patient would cause information loss, adversely affecting predictive accuracy. In this study, we propose an incomplete multimodal data integration framework for GC (iMD4GC) to address the challenges posed by incomplete multimodal data, enabling precise response prediction and survival analysis. Specifically, iMD4GC incorporates unimodal attention layers for each modality to capture intra-modal information. Subsequently, the cross-modal interaction layers explore potential inter-modal interactions and capture complementary information across modalities, thereby enabling information compensation for missing modalities. To evaluate iMD4GC, we collected three multimodal datasets for GC study: GastricRes (698 cases) for response prediction, GastricSur (801 cases) for survival analysis, and TCGA-STAD (400 cases) for survival analysis. The scale of our datasets is significantly larger than previous studies. The iMD4GC achieved impressive performance with an 80.2% AUC on GastricRes, 71.4% C-index on GastricSur, and 66.1% C-index on TCGA-STAD, significantly surpassing other compared methods.","sentences":["Gastric cancer (GC) is a prevalent malignancy worldwide, ranking as the fifth most common cancer with over 1 million new cases and 700 thousand deaths in 2020.","Locally advanced gastric cancer (LAGC) accounts for approximately two-thirds of GC diagnoses, and neoadjuvant chemotherapy (NACT) has emerged as the standard treatment for LAGC.","However, the effectiveness of NACT varies significantly among patients, with a considerable subset displaying treatment resistance.","Ineffective NACT not only leads to adverse effects but also misses the optimal therapeutic window, resulting in lower survival rate.","However, existing multimodal learning methods assume the availability of all modalities for each patient, which does not align with the reality of clinical practice.","The limited availability of modalities for each patient would cause information loss, adversely affecting predictive accuracy.","In this study, we propose an incomplete multimodal data integration framework for GC (iMD4GC) to address the challenges posed by incomplete multimodal data, enabling precise response prediction and survival analysis.","Specifically, iMD4GC incorporates unimodal attention layers for each modality to capture intra-modal information.","Subsequently, the cross-modal interaction layers explore potential inter-modal interactions and capture complementary information across modalities, thereby enabling information compensation for missing modalities.","To evaluate iMD4GC, we collected three multimodal datasets for GC study: GastricRes (698 cases) for response prediction, GastricSur (801 cases) for survival analysis, and TCGA-STAD (400 cases) for survival analysis.","The scale of our datasets is significantly larger than previous studies.","The iMD4GC achieved impressive performance with an 80.2% AUC on GastricRes, 71.4% C-index on GastricSur, and 66.1% C-index on TCGA-STAD, significantly surpassing other compared methods."],"url":"http://arxiv.org/abs/2404.01192v1","category":"eess.IV"}
{"created":"2024-04-01 15:48:48","title":"A Semiparametric Approach for Robust and Efficient Learning with Biobank Data","abstract":"With the increasing availability of electronic health records (EHR) linked with biobank data for translational research, a critical step in realizing its potential is to accurately classify phenotypes for patients. Existing approaches to achieve this goal are based on error-prone EHR surrogate outcomes, assisted and validated by a small set of labels obtained via medical chart review, which may also be subject to misclassification. Ignoring the noise in these outcomes can induce severe estimation and validation bias to both EHR phenotyping and risking modeling with biomarkers collected in the biobank. To overcome this challenge, we propose a novel unsupervised and semiparametric approach to jointly model multiple noisy EHR outcomes with their linked biobank features. Our approach primarily aims at disease risk modeling with the baseline biomarkers, and is also able to produce a predictive EHR phenotyping model and validate its performance without observations of the true disease outcome. It consists of composite and nonparametric regression steps free of any parametric model specification, followed by a parametric projection step to reduce the uncertainty and improve the estimation efficiency. We show that our method is robust to violations of the parametric assumptions while attaining the desirable root-$n$ convergence rates on risk modeling. Our developed method outperforms existing methods in extensive simulation studies, as well as a real-world application in phenotyping and genetic risk modeling of type II diabetes.","sentences":["With the increasing availability of electronic health records (EHR) linked with biobank data for translational research, a critical step in realizing its potential is to accurately classify phenotypes for patients.","Existing approaches to achieve this goal are based on error-prone EHR surrogate outcomes, assisted and validated by a small set of labels obtained via medical chart review, which may also be subject to misclassification.","Ignoring the noise in these outcomes can induce severe estimation and validation bias to both EHR phenotyping and risking modeling with biomarkers collected in the biobank.","To overcome this challenge, we propose a novel unsupervised and semiparametric approach to jointly model multiple noisy EHR outcomes with their linked biobank features.","Our approach primarily aims at disease risk modeling with the baseline biomarkers, and is also able to produce a predictive EHR phenotyping model and validate its performance without observations of the true disease outcome.","It consists of composite and nonparametric regression steps free of any parametric model specification, followed by a parametric projection step to reduce the uncertainty and improve the estimation efficiency.","We show that our method is robust to violations of the parametric assumptions while attaining the desirable root-$n$ convergence rates on risk modeling.","Our developed method outperforms existing methods in extensive simulation studies, as well as a real-world application in phenotyping and genetic risk modeling of type II diabetes."],"url":"http://arxiv.org/abs/2404.01191v1","category":"stat.ME"}
{"created":"2024-04-01 15:34:11","title":"Constraining Dark Matter Annihilation with Fermi-LAT Observations of Ultra-Faint Compact Stellar Systems","abstract":"Recent results from numerical simulations and models of galaxy formation suggest that recently discovered ultra-faint compact stellar systems (UFCSs) in the halo of the Milky Way (MW) may be some of the smallest and faintest galaxies.   If this is the case, these systems would be attractive targets for indirect searches of weakly interacting massive particle (WIMP) dark matter (DM) annihilation due to their relative proximity and high expected DM content.   In this study, we analyze 14.3 years of gamma-ray data collected by the Fermi-LAT coincident with 26 UFCSs.   No significant excess gamma-ray emission is detected, and we present gamma-ray flux upper limits for these systems.   Assuming that the UFCSs are dark-matter-dominated galaxies consistent with being among the faintest and least massive MW dwarf spheroidal (dSphs) satellite galaxies, we derive the projected sensitivity for a dark matter annihilation signal.   We find that observations of UFCSs have the potential to yield some of the most powerful constraints on DM annihilation, with sensitivity comparable to observations of known dSphs and the Galactic center.   This result emphasizes the importance of precise kinematic studies of UFCSs to empirically determine their DM content.","sentences":["Recent results from numerical simulations and models of galaxy formation suggest that recently discovered ultra-faint compact stellar systems (UFCSs) in the halo of the Milky Way (MW) may be some of the smallest and faintest galaxies.   ","If this is the case, these systems would be attractive targets for indirect searches of weakly interacting massive particle (WIMP) dark matter (DM) annihilation due to their relative proximity and high expected DM content.   ","In this study, we analyze 14.3 years of gamma-ray data collected by the Fermi-LAT coincident with 26 UFCSs.   ","No significant excess gamma-ray emission is detected, and we present gamma-ray flux upper limits for these systems.   ","Assuming that the UFCSs are dark-matter-dominated galaxies consistent with being among the faintest and least massive MW dwarf spheroidal (dSphs) satellite galaxies, we derive the projected sensitivity for a dark matter annihilation signal.   ","We find that observations of UFCSs have the potential to yield some of the most powerful constraints on DM annihilation, with sensitivity comparable to observations of known dSphs and the Galactic center.   ","This result emphasizes the importance of precise kinematic studies of UFCSs to empirically determine their DM content."],"url":"http://arxiv.org/abs/2404.01181v1","category":"astro-ph.HE"}
{"created":"2024-04-01 15:14:15","title":"Scalable Radar-based ITS: Self-localization and Occupancy Heat Map for Traffic Analysis","abstract":"4D mmWave radar sensors are well suited for city scale Intelligent Transportation Systems (ITS) given their long sensing range, weatherproof functionality, simple mechanical design, and low manufacturing cost. In this paper, we investigate radar-based ITS for scalable traffic analysis. Localization of these radar sensors in a city scale range is a fundamental task in ITS. For mobile ITS setups it requires more endeavor. To address this task, we propose a self-localization approach that matches two descriptions of \"road\": the one from the geometry of the motion trajectories of cumulatively observed vehicles, and the other one from the aerial laser scan. An ICP (iterative closest point) algorithm is used to register the motion trajectory into the road section of the laser scan to estimate the sensor pose. We evaluates the results and show that it outperforms other map-based radar localization methods, especially for the orientation estimation. Beyond the localization result, we project radar sensor data onto city scale laser scan and generate an scalable occupancy heat map as a traffic analysis tool. This is demonstrated using two radar sensors monitoring an urban area in the real world.","sentences":["4D mmWave radar sensors are well suited for city scale Intelligent Transportation Systems (ITS) given their long sensing range, weatherproof functionality, simple mechanical design, and low manufacturing cost.","In this paper, we investigate radar-based ITS for scalable traffic analysis.","Localization of these radar sensors in a city scale range is a fundamental task in ITS.","For mobile ITS setups it requires more endeavor.","To address this task, we propose a self-localization approach that matches two descriptions of \"road\": the one from the geometry of the motion trajectories of cumulatively observed vehicles, and the other one from the aerial laser scan.","An ICP (iterative closest point) algorithm is used to register the motion trajectory into the road section of the laser scan to estimate the sensor pose.","We evaluates the results and show that it outperforms other map-based radar localization methods, especially for the orientation estimation.","Beyond the localization result, we project radar sensor data onto city scale laser scan and generate an scalable occupancy heat map as a traffic analysis tool.","This is demonstrated using two radar sensors monitoring an urban area in the real world."],"url":"http://arxiv.org/abs/2404.01166v1","category":"cs.RO"}
{"created":"2024-04-01 15:14:07","title":"LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models","abstract":"The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at https://github.com/hrlics/LITE.","sentences":["The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet.","Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood.","Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables.","However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments.","To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling.","Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images.","Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities.","During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations.","Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction.","Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error.","This justifies its effectiveness.","Our data and code are available at https://github.com/hrlics/LITE."],"url":"http://arxiv.org/abs/2404.01165v1","category":"cs.CL"}
{"created":"2024-04-01 15:13:46","title":"Capturing Shock Waves by Relaxation Neural Networks","abstract":"In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.","sentences":["In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems.","This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN).","It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions.","This ultimately results in the failure of optimization that is based on gradient descent in the training process.","Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective.","Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework.","In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework."],"url":"http://arxiv.org/abs/2404.01163v1","category":"math.NA"}
{"created":"2024-04-01 15:01:45","title":"Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training","abstract":"Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.","sentences":["Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance.","However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs.","Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models.","Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters.","We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions.","Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs.","Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance."],"url":"http://arxiv.org/abs/2404.01157v1","category":"cs.CL"}
{"created":"2024-04-01 15:01:38","title":"SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining","abstract":"Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outperforming existing methods in three downstream tasks.","sentences":["Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets.","However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text.","This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images.","This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features.","Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text.","This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities.","Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets.","Our experiments demonstrate the effectiveness of the proposed approach, outperforming existing methods in three downstream tasks."],"url":"http://arxiv.org/abs/2404.01156v1","category":"cs.CV"}
{"created":"2024-04-01 15:00:29","title":"Research on Mechanism of Voltage Oscillation Caused by Repeated LVRT of Wind Turbine Based on Switched System Theory","abstract":"The electrical distance between the wind power collection sending end grid and the main grid is relatively long, lacking synchronous power supply support, showing the characteristics of weak grid. Therefore, the voltage oscillation phenomenon is easy to happen, threatening the safe and stable operation of the grid. Its dynamic process and evolution mechanism need to be studied urgently. This paper firstly analyzes conditions for voltage oscillations caused by repeated low voltage ride through (LVRT) of wind turbine through steady-state power flow calculation. Then, based on the switched system theory, considering the external connected impedance and internal control dynamics of the wind turbine, the switched system model for the grid-side converter (GSC) of wind turbine is established. After that, the relevant parameters are substituted to analyze the dynamic evolution process of each electrical quantity during the process that wind turbine repeatedly enters and exits LVRT, revealing the evolution mechanism of voltage oscillations. Finally, the voltage oscillation phenomenon of the grid-connected point of wind turbine is reproduced through simulation, verifying the correctness and effectiveness of theoretical analysis results. Furthermore, the main factors which influence characteristics of voltage oscillations are explored as well.","sentences":["The electrical distance between the wind power collection sending end grid and the main grid is relatively long, lacking synchronous power supply support, showing the characteristics of weak grid.","Therefore, the voltage oscillation phenomenon is easy to happen, threatening the safe and stable operation of the grid.","Its dynamic process and evolution mechanism need to be studied urgently.","This paper firstly analyzes conditions for voltage oscillations caused by repeated low voltage ride through (LVRT) of wind turbine through steady-state power flow calculation.","Then, based on the switched system theory, considering the external connected impedance and internal control dynamics of the wind turbine, the switched system model for the grid-side converter (GSC) of wind turbine is established.","After that, the relevant parameters are substituted to analyze the dynamic evolution process of each electrical quantity during the process that wind turbine repeatedly enters and exits LVRT, revealing the evolution mechanism of voltage oscillations.","Finally, the voltage oscillation phenomenon of the grid-connected point of wind turbine is reproduced through simulation, verifying the correctness and effectiveness of theoretical analysis results.","Furthermore, the main factors which influence characteristics of voltage oscillations are explored as well."],"url":"http://arxiv.org/abs/2404.01155v1","category":"eess.SY"}
{"created":"2024-04-01 14:59:13","title":"Uncovering the Text Embedding in Text-to-Image Diffusion Models","abstract":"The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical utility for image editing and semantic discovery. More importantly, we expect the in-depth analyses and findings of the text embedding can enhance the understanding of text-to-image diffusion models.","sentences":["The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image.","While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored.","In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework.","Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing.","Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD).","These uncovered properties offer practical utility for image editing and semantic discovery.","More importantly, we expect the in-depth analyses and findings of the text embedding can enhance the understanding of text-to-image diffusion models."],"url":"http://arxiv.org/abs/2404.01154v1","category":"cs.CV"}
{"created":"2024-04-01 14:58:16","title":"TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression","abstract":"The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method's robustness to covariate shifts.","sentences":["The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions.","In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting.","Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples.","Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts.","We further establish conditions under which the estimator is minimax-optimal.","Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version.","Numerical tests validate our theory, highlighting the method's robustness to covariate shifts."],"url":"http://arxiv.org/abs/2404.01153v1","category":"stat.ML"}
{"created":"2024-04-01 14:53:36","title":"Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs","abstract":"Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce \"Detect2Interact\", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.","sentences":["Localization plays a crucial role in enhancing the practicality and precision of VQA systems.","By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality.","However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses.","In this work, we introduce \"Detect2Interact\", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection.","First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images.","Next, we use Vision Studio to extract semantic object descriptions.","Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map.","As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation."],"url":"http://arxiv.org/abs/2404.01151v1","category":"cs.CV"}
{"created":"2024-04-01 14:49:13","title":"Least Non-Zero Singular Value and the Distribution of Eigenvectors of non-Hermitian Random Matrices","abstract":"We obtain a tail bound for the least non-zero singular value of $A-z$ when $A$ is a random matrix and $z$ is an eigenvalue of $A$ in a neighbourhood of a given point $z_0$ in the bulk of the spectrum. The argument relies on a resolvent comparison and a tail bound for Gauss-divisible matrices. The latter can be obtained by the method of partial Schur decomposition. Using this bound we prove that any finite collection of components of a right eigenvector corresponding to an eigenvalue uniformly sampled from a neighbourhood of a point in the bulk is Gaussian. A byproduct of the calculation is an asymptotic formula for the odd moments of the absolute value of the characteristic polynomial of real Gauss-divisible matrices.","sentences":["We obtain a tail bound for the least non-zero singular value of $A-z$ when $A$ is a random matrix and $z$ is an eigenvalue of $A$ in a neighbourhood of a given point $z_0$ in the bulk of the spectrum.","The argument relies on a resolvent comparison and a tail bound for Gauss-divisible matrices.","The latter can be obtained by the method of partial Schur decomposition.","Using this bound we prove that any finite collection of components of a right eigenvector corresponding to an eigenvalue uniformly sampled from a neighbourhood of a point in the bulk is Gaussian.","A byproduct of the calculation is an asymptotic formula for the odd moments of the absolute value of the characteristic polynomial of real Gauss-divisible matrices."],"url":"http://arxiv.org/abs/2404.01149v1","category":"math.PR"}
{"created":"2024-04-01 14:46:20","title":"Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit","abstract":"Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.","sentences":["Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse.","However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored.","In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits.","We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location.","We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers.","We present several directions for future research based on our initial findings."],"url":"http://arxiv.org/abs/2404.01147v1","category":"cs.CL"}
{"created":"2024-04-01 14:42:57","title":"Condition-Aware Neural Network for Controlled Image Generation","abstract":"We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.","sentences":["We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models.","In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network.","This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition.","We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO.","CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT.","In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step."],"url":"http://arxiv.org/abs/2404.01143v1","category":"cs.CV"}
{"created":"2024-04-01 14:29:58","title":"Enhancing Reasoning Capacity of SLM using Cognitive Enhancement","abstract":"Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable reasonings and outcomes. This information can be extracted through explicit prompt requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the LLMs. However, such size reductions have notable performance reduction, especially when tasked to provide reasoning explanations. In this paper, we aim to mitigate performance reduction through the integration of cognitive strategies that humans use for problem-solving. We term this as cognitive enhancement through prompts. Our experiments showed significant improvement gains of the SLMs' performances when such enhancements were applied. We believe that our exploration study paves the way for further investigation into the use of cognitive enhancement to optimize SLM for cyber security applications.","sentences":["Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics.","However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations.","Accountability ensures models have the means to provide explainable reasonings and outcomes.","This information can be extracted through explicit prompt requests.","For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well.","One approach to deal with this consideration is to have the data processed locally using a local instance of the model.","Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used.","These SLMs have significantly fewer parameters compared to the LLMs.","However, such size reductions have notable performance reduction, especially when tasked to provide reasoning explanations.","In this paper, we aim to mitigate performance reduction through the integration of cognitive strategies that humans use for problem-solving.","We term this as cognitive enhancement through prompts.","Our experiments showed significant improvement gains of the SLMs' performances when such enhancements were applied.","We believe that our exploration study paves the way for further investigation into the use of cognitive enhancement to optimize SLM for cyber security applications."],"url":"http://arxiv.org/abs/2404.01135v1","category":"cs.CR"}
{"created":"2024-04-01 14:19:00","title":"GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems","abstract":"For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner. Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems.","sentences":["For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem.","However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically.","This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task.","We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage.","We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions.","During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner.","Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems."],"url":"http://arxiv.org/abs/2404.01131v1","category":"cs.MA"}
{"created":"2024-04-01 14:06:48","title":"Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation","abstract":"Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.","sentences":["Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases.","While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes.","To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP).","The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers.","By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks.","Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models.","This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable."],"url":"http://arxiv.org/abs/2404.01127v1","category":"cs.CV"}
{"created":"2024-04-01 13:57:46","title":"CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment","abstract":"Recent image tone adjustment (or enhancement) approaches have predominantly adopted supervised learning for learning human-centric perceptual assessment. However, these approaches are constrained by intrinsic challenges of supervised learning. Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses. Moreover, their coverage of target style is confined to stylistic variants inferred from the training data. To surmount the above challenges, we propose an unsupervised learning-based approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions. Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description. To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception. The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training. Our approach's efficacy is demonstrated through comprehensive experiments, including a user study.","sentences":["Recent image tone adjustment (or enhancement) approaches have predominantly adopted supervised learning for learning human-centric perceptual assessment.","However, these approaches are constrained by intrinsic challenges of supervised learning.","Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses.","Moreover, their coverage of target style is confined to stylistic variants inferred from the training data.","To surmount the above challenges, we propose an unsupervised learning-based approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions.","Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description.","To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception.","The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training.","Our approach's efficacy is demonstrated through comprehensive experiments, including a user study."],"url":"http://arxiv.org/abs/2404.01123v1","category":"cs.CV"}
{"created":"2024-04-01 13:55:40","title":"Motion Blur Decomposition with Cross-shutter Guidance","abstract":"Motion blur is a frequently observed image artifact, especially under insufficient illumination where exposure time has to be prolonged so as to collect more photons for a bright enough image. Rather than simply removing such blurring effects, recent researches have aimed at decomposing a blurry image into multiple sharp images with spatial and temporal coherence. Since motion blur decomposition itself is highly ambiguous, priors from neighbouring frames or human annotation are usually needed for motion disambiguation. In this paper, inspired by the complementary exposure characteristics of a global shutter (GS) camera and a rolling shutter (RS) camera, we propose to utilize the ordered scanline-wise delay in a rolling shutter image to robustify motion decomposition of a single blurry image. To evaluate this novel dual imaging setting, we construct a triaxial system to collect realistic data, as well as a deep network architecture that explicitly addresses temporal and contextual information through reciprocal branches for cross-shutter motion blur decomposition. Experiment results have verified the effectiveness of our proposed algorithm, as well as the validity of our dual imaging setting.","sentences":["Motion blur is a frequently observed image artifact, especially under insufficient illumination where exposure time has to be prolonged so as to collect more photons for a bright enough image.","Rather than simply removing such blurring effects, recent researches have aimed at decomposing a blurry image into multiple sharp images with spatial and temporal coherence.","Since motion blur decomposition itself is highly ambiguous, priors from neighbouring frames or human annotation are usually needed for motion disambiguation.","In this paper, inspired by the complementary exposure characteristics of a global shutter (GS) camera and a rolling shutter (RS) camera, we propose to utilize the ordered scanline-wise delay in a rolling shutter image to robustify motion decomposition of a single blurry image.","To evaluate this novel dual imaging setting, we construct a triaxial system to collect realistic data, as well as a deep network architecture that explicitly addresses temporal and contextual information through reciprocal branches for cross-shutter motion blur decomposition.","Experiment results have verified the effectiveness of our proposed algorithm, as well as the validity of our dual imaging setting."],"url":"http://arxiv.org/abs/2404.01120v1","category":"cs.CV"}
{"created":"2024-04-01 13:46:23","title":"Intelligent Robotic Control System Based on Computer Vision Technology","abstract":"The article explores the intersection of computer vision technology and robotic control, highlighting its importance in various fields such as industrial automation, healthcare, and environmental protection. Computer vision technology, which simulates human visual observation, plays a crucial role in enabling robots to perceive and understand their surroundings, leading to advancements in tasks like autonomous navigation, object recognition, and waste management. By integrating computer vision with robot control, robots gain the ability to interact intelligently with their environment, improving efficiency.","sentences":["The article explores the intersection of computer vision technology and robotic control, highlighting its importance in various fields such as industrial automation, healthcare, and environmental protection.","Computer vision technology, which simulates human visual observation, plays a crucial role in enabling robots to perceive and understand their surroundings, leading to advancements in tasks like autonomous navigation, object recognition, and waste management.","By integrating computer vision with robot control, robots gain the ability to interact intelligently with their environment, improving efficiency."],"url":"http://arxiv.org/abs/2404.01116v1","category":"cs.RO"}
{"created":"2024-04-01 13:33:40","title":"An incremental hybrid adaptive network-based IDS in Software Defined Networks to detect stealth attacks","abstract":"Network attacks have became increasingly more sophisticated and stealthy due to the advances in technologies and the growing sophistication of attackers. Advanced Persistent Threats (APTs) are a type of attack that implement a wide range of strategies to evade detection and be under the defence radar. Software Defined Network (SDN) is a network paradigm that implements dynamic configuration by separating the control plane from the network plane. This approach improves security aspects by facilitating the employment of network intrusion detection systems. Implementing Machine Learning (ML) techniques in Intrusion Detection Systems (IDSs) is widely used to detect such attacks but has a challenge when the data distribution changes. Concept drift is a term that describes the change in the relationship between the input data and the target value (label or class). The model is expected to degrade as certain forms of change occur. In this paper, the primary form of change will be in user behaviour (particularly changes in attacker behaviour). It is essential for a model to adapt itself to deviations in data distribution. SDN can help in monitoring changes in data distribution. This paper discusses changes in stealth attacker behaviour. The work described here investigates various concept drift detection algorithms. An incremental hybrid adaptive Network Intrusion Detection System (NIDS) is proposed to tackle the issue of concept drift in SDN. It can detect known and unknown attacks. The model is evaluated over different datasets showing promising results.","sentences":["Network attacks have became increasingly more sophisticated and stealthy due to the advances in technologies and the growing sophistication of attackers.","Advanced Persistent Threats (APTs) are a type of attack that implement a wide range of strategies to evade detection and be under the defence radar.","Software Defined Network (SDN) is a network paradigm that implements dynamic configuration by separating the control plane from the network plane.","This approach improves security aspects by facilitating the employment of network intrusion detection systems.","Implementing Machine Learning (ML) techniques in Intrusion Detection Systems (IDSs) is widely used to detect such attacks but has a challenge when the data distribution changes.","Concept drift is a term that describes the change in the relationship between the input data and the target value (label or class).","The model is expected to degrade as certain forms of change occur.","In this paper, the primary form of change will be in user behaviour (particularly changes in attacker behaviour).","It is essential for a model to adapt itself to deviations in data distribution.","SDN can help in monitoring changes in data distribution.","This paper discusses changes in stealth attacker behaviour.","The work described here investigates various concept drift detection algorithms.","An incremental hybrid adaptive Network Intrusion Detection System (NIDS) is proposed to tackle the issue of concept drift in SDN.","It can detect known and unknown attacks.","The model is evaluated over different datasets showing promising results."],"url":"http://arxiv.org/abs/2404.01109v1","category":"cs.CR"}
{"created":"2024-04-01 13:16:34","title":"LLM Attributor: Interactive Visual Attribution for LLM Generation","abstract":"While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.","sentences":["While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation.","We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation.","Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text.","We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs.","Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models.","For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution.","The video demo is available at https://youtu.be/mIG2MDQKQxM."],"url":"http://arxiv.org/abs/2404.01361v1","category":"cs.CL"}
{"created":"2024-04-01 13:12:30","title":"What's in Your \"Safe\" Data?: Identifying Benign Data that Breaks Safety","abstract":"Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.","sentences":["Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking.","Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety.","We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking.","First, we represent fine-tuning data through two lenses: representation and gradient spaces.","Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones.","By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning.","Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data.","We further find that selected data are often in the form of lists and bullet points, or math questions."],"url":"http://arxiv.org/abs/2404.01099v1","category":"cs.LG"}
{"created":"2024-04-01 12:43:22","title":"Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On","abstract":"Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the original self-attention layers contained in the diffusion model to achieve efficient and accurate texture transfer. Second, we propose a novel diffusion-based method that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results. In addition, we integrate mask prediction and image synthesis into a single compact model. The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases.","sentences":["Image-based virtual try-on is an increasingly important task for online shopping.","It aims to synthesize images of a specific person wearing a specified garment.","Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks.","However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity.","To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders.","Accordingly, we make contributions from two aspects.","First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet.","This enables the original self-attention layers contained in the diffusion model to achieve efficient and accurate texture transfer.","Second, we propose a novel diffusion-based method that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results.","In addition, we integrate mask prediction and image synthesis into a single compact model.","The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases."],"url":"http://arxiv.org/abs/2404.01089v1","category":"cs.CV"}
{"created":"2024-04-01 12:27:55","title":"AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles","abstract":"In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.","sentences":["In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'.","We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle.","We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning.","Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively.","Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks.","In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively."],"url":"http://arxiv.org/abs/2404.01084v1","category":"cs.CL"}
{"created":"2024-04-01 12:20:34","title":"BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System","abstract":"Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text library and intuitively participate in the plagiarism analysis.","sentences":["Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts.","In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets.","In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research.","Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy.","Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively.","At the end, we also provide a user-friendly demo platform that allows users to upload a text library and intuitively participate in the plagiarism analysis."],"url":"http://arxiv.org/abs/2404.01582v1","category":"cs.CL"}
{"created":"2024-04-01 12:19:33","title":"Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling","abstract":"As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is proposed to improve the generalization ability. It is proved in Theorems 1, 2 and 3 that EmSHAP achieves tighter error bound than state-of-the-art methods like KernelSHAP and VAEAC, leading to higher estimation accuracy. Finally, case studies on a medical application and an industrial application show that the proposed Shapley value-based explainable framework exhibits enhanced estimation accuracy without compromise on efficiency.","sentences":["As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models.","However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features.","Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency.","In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest.","In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated.","In addition, a dynamic masking scheme is proposed to improve the generalization ability.","It is proved in Theorems 1, 2 and 3 that EmSHAP achieves tighter error bound than state-of-the-art methods like KernelSHAP and VAEAC, leading to higher estimation accuracy.","Finally, case studies on a medical application and an industrial application show that the proposed Shapley value-based explainable framework exhibits enhanced estimation accuracy without compromise on efficiency."],"url":"http://arxiv.org/abs/2404.01078v1","category":"cs.LG"}
{"created":"2024-04-01 12:18:52","title":"Debiased calibration estimation using generalized entropy in survey sampling","abstract":"Incorporating the auxiliary information into the survey estimation is a fundamental problem in survey sampling. Calibration weighting is a popular tool for incorporating the auxiliary information. The calibration weighting method of Deville and Sarndal (1992) uses a distance measure between the design weights and the final weights to solve the optimization problem with calibration constraints. This paper introduces a novel framework that leverages generalized entropy as the objective function for optimization, where design weights play a role in the constraints to ensure design consistency, rather than being part of the objective function. This innovative calibration framework is particularly attractive due to its generality and its ability to generate more efficient calibration weights compared to traditional methods based on Deville and Sarndal (1992). Furthermore, we identify the optimal choice of the generalized entropy function that achieves the minimum variance across various choices of the generalized entropy function under the same constraints. Asymptotic properties, such as design consistency and asymptotic normality, are presented rigorously. The results from a limited simulation study are also presented. We demonstrate a real-life application using agricultural survey data collected from Kynetec, Inc.","sentences":["Incorporating the auxiliary information into the survey estimation is a fundamental problem in survey sampling.","Calibration weighting is a popular tool for incorporating the auxiliary information.","The calibration weighting method of Deville and Sarndal (1992) uses a distance measure between the design weights and the final weights to solve the optimization problem with calibration constraints.","This paper introduces a novel framework that leverages generalized entropy as the objective function for optimization, where design weights play a role in the constraints to ensure design consistency, rather than being part of the objective function.","This innovative calibration framework is particularly attractive due to its generality and its ability to generate more efficient calibration weights compared to traditional methods based on Deville and Sarndal (1992).","Furthermore, we identify the optimal choice of the generalized entropy function that achieves the minimum variance across various choices of the generalized entropy function under the same constraints.","Asymptotic properties, such as design consistency and asymptotic normality, are presented rigorously.","The results from a limited simulation study are also presented.","We demonstrate a real-life application using agricultural survey data collected from Kynetec, Inc."],"url":"http://arxiv.org/abs/2404.01076v2","category":"stat.ME"}
{"created":"2024-04-01 12:10:21","title":"Integrable deformations of Rikitake systems, Lie bialgebras and bi-Hamiltonian structures","abstract":"Integrable deformations of a class of Rikitake dynamical systems are constructed by deforming their underlying Lie-Poisson Hamiltonian structures as linearizations of Poisson--Lie structures on certain (dual) Lie groups. By taking into account that there exists a one-to one correspondence between Poisson--Lie groups and Lie bialgebra structures, a number of deformed Poisson coalgebras can be obtained, which allow the construction of integrable deformations of coupled Rikitake systems. Moreover, the integrals of the motion for coupled systems can be explicitly obtained by means of the deformed coproduct map. The same procedure can be also applied when the initial system is bi-Hamiltonian with respect to two different Lie-Poisson algebras. In this case, to preserve a bi-Hamiltonian structure under deformation, a common Lie bialgebra structure for the two Lie-Poisson structures has to be found. Coupled dynamical systems arising from this bi-Hamiltonian deformation scheme are also presented, and the use of collective `cluster variables', turns out to be enlightening in order to analyse their dynamical behaviour. As a general feature, the approach here presented provides a novel connection between Lie bialgebras and integrable dynamical systems.","sentences":["Integrable deformations of a class of Rikitake dynamical systems are constructed by deforming their underlying Lie-Poisson Hamiltonian structures as linearizations of Poisson--Lie structures on certain (dual) Lie groups.","By taking into account that there exists a one-to one correspondence between Poisson--Lie groups and Lie bialgebra structures, a number of deformed Poisson coalgebras can be obtained, which allow the construction of integrable deformations of coupled Rikitake systems.","Moreover, the integrals of the motion for coupled systems can be explicitly obtained by means of the deformed coproduct map.","The same procedure can be also applied when the initial system is bi-Hamiltonian with respect to two different Lie-Poisson algebras.","In this case, to preserve a bi-Hamiltonian structure under deformation, a common Lie bialgebra structure for the two Lie-Poisson structures has to be found.","Coupled dynamical systems arising from this bi-Hamiltonian deformation scheme are also presented, and the use of collective `cluster variables', turns out to be enlightening in order to analyse their dynamical behaviour.","As a general feature, the approach here presented provides a novel connection between Lie bialgebras and integrable dynamical systems."],"url":"http://arxiv.org/abs/2404.01073v1","category":"math.DS"}
{"created":"2024-04-01 12:03:35","title":"Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation","abstract":"This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and humans, underscoring the essential role of human oversight in upholding NMT ethical standards. Incorporating a biblical perspective, we discuss the societal impact of NMT and the broader ethical responsibilities of developers, positing them as stewards accountable for the societal repercussions of their creations.","sentences":["This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity.","We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent.","We identify and address ethical issues through empirical studies.","These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching.","And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content.","Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub.","Additionally, the paper probes the distribution of responsibility between AI systems and humans, underscoring the essential role of human oversight in upholding NMT ethical standards.","Incorporating a biblical perspective, we discuss the societal impact of NMT and the broader ethical responsibilities of developers, positing them as stewards accountable for the societal repercussions of their creations."],"url":"http://arxiv.org/abs/2404.01070v1","category":"cs.CL"}
{"created":"2024-04-01 11:44:14","title":"STAR-RIS Aided Secure MIMO Communication Systems","abstract":"This paper investigates simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) aided physical layer security (PLS) in multiple-input multiple-output (MIMO) systems, where the base station (BS) transmits secrecy information with the aid of STAR-RIS against multiple eavesdroppers equipped with multiple antennas. We aim to maximize the secrecy rate by jointly optimizing the active beamforming at the BS and passive beamforming at the STAR-RIS, subject to the hardware constraint for STAR-RIS. To handle the coupling variables, a minimum mean-square error (MMSE) based alternating optimization (AO) algorithm is applied. In particular, the amplitudes and phases of STAR-RIS are divided into two blocks to simplify the algorithm design. Besides, by applying the Majorization-Minimization (MM) method, we derive a closed-form expression of the STAR-RIS's phase shifts. Numerical results show that the proposed scheme significantly outperforms various benchmark schemes, especially as the number of STAR-RIS elements increases.","sentences":["This paper investigates simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) aided physical layer security (PLS) in multiple-input multiple-output (MIMO) systems, where the base station (BS) transmits secrecy information with the aid of STAR-RIS against multiple eavesdroppers equipped with multiple antennas.","We aim to maximize the secrecy rate by jointly optimizing the active beamforming at the BS and passive beamforming at the STAR-RIS, subject to the hardware constraint for STAR-RIS.","To handle the coupling variables, a minimum mean-square error (MMSE) based alternating optimization (AO) algorithm is applied.","In particular, the amplitudes and phases of STAR-RIS are divided into two blocks to simplify the algorithm design.","Besides, by applying the Majorization-Minimization (MM) method, we derive a closed-form expression of the STAR-RIS's phase shifts.","Numerical results show that the proposed scheme significantly outperforms various benchmark schemes, especially as the number of STAR-RIS elements increases."],"url":"http://arxiv.org/abs/2404.01059v1","category":"cs.IT"}
{"created":"2024-04-02 17:58:57","title":"Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields","abstract":"Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust.","sentences":["Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa.","We call this property alpha invariance.","For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance.","We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling.","We test their behaviors and show our recipe to be more robust."],"url":"http://arxiv.org/abs/2404.02155v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:24","title":"From Seaweed to Security: The Emergence of Alginate in Compromising IoT Fingerprint Sensors","abstract":"The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing. Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems. In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors. Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns. Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints. The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors. This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats.","sentences":["The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing.","Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems.","In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors.","Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns.","Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints.","The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors.","This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats."],"url":"http://arxiv.org/abs/2404.02150v1","category":"cs.CR"}
{"created":"2024-04-02 17:57:08","title":"Probing the Physics of Star-Formation (ProPStar): II. The first systematic search for streamers toward protostars","abstract":"The detection of narrow channels of accretion toward protostellar disks, known as streamers, have increased in number in the last few years. However, it is unclear if streamers are a common feature around protostars that were previously missed, or if they are a rare phenomenon. Our goals are to obtain the incidence of streamers toward a region of clustered star formation and to trace the origins of their gas, to determine if they originate within the filamentary structure of molecular clouds or from beyond. We used combined observations of the nearby NGC 1333 star-forming region, carried out with the NOEMA interferometer and the IRAM 30m single dish. Our observations cover the area between the IRAS 4 and SVS 13 systems. We traced the chemically fresh gas within NGC 1333 with HC3N molecular gas emission and the structure of the fibers in this region with N2H+ emission. We fit multiple velocity components in both maps and used clustering algorithms to recover velocity-coherent structures. We find streamer candidates toward 7 out of 16 young stellar objects within our field of view. This represents an incidence of approximately 40\\% of young stellar objects with streamer candidates when looking at a clustered star forming region. The incidence increases to about 60\\% when we considered only embedded protostars. All streamers are found in HC3N emission. Given the different velocities between HC3N and N2H+ emission, and the fact that, by construction, N2H+ traces the fiber structure, we suggest that the gas that forms the streamers comes from outside the fibers. This implies that streamers can connect cloud material that falls to the filaments with protostellar disk scales.","sentences":["The detection of narrow channels of accretion toward protostellar disks, known as streamers, have increased in number in the last few years.","However, it is unclear if streamers are a common feature around protostars that were previously missed, or if they are a rare phenomenon.","Our goals are to obtain the incidence of streamers toward a region of clustered star formation and to trace the origins of their gas, to determine if they originate within the filamentary structure of molecular clouds or from beyond.","We used combined observations of the nearby NGC 1333 star-forming region, carried out with the NOEMA interferometer and the IRAM 30m single dish.","Our observations cover the area between the IRAS 4 and SVS 13 systems.","We traced the chemically fresh gas within NGC 1333 with HC3N molecular gas emission and the structure of the fibers in this region with N2H+ emission.","We fit multiple velocity components in both maps and used clustering algorithms to recover velocity-coherent structures.","We find streamer candidates toward 7 out of 16 young stellar objects within our field of view.","This represents an incidence of approximately 40\\% of young stellar objects with streamer candidates when looking at a clustered star forming region.","The incidence increases to about 60\\% when we considered only embedded protostars.","All streamers are found in HC3N emission.","Given the different velocities between HC3N and N2H+ emission, and the fact that, by construction, N2H+ traces the fiber structure, we suggest that the gas that forms the streamers comes from outside the fibers.","This implies that streamers can connect cloud material that falls to the filaments with protostellar disk scales."],"url":"http://arxiv.org/abs/2404.02144v1","category":"astro-ph.GA"}
{"created":"2024-04-02 17:51:53","title":"Lensed Type Ia Supernova \"Encore\" at z=2: The First Instance of Two Multiply-Imaged Supernovae in the Same Host Galaxy","abstract":"A bright ($m_{\\rm F150W,AB}$=24 mag), $z=1.95$ supernova (SN) candidate was discovered in JWST/NIRCam imaging acquired on 2023 November 17. The SN is quintuply-imaged as a result of strong gravitational lensing by a foreground galaxy cluster, detected in three locations, and remarkably is the second lensed SN found in the same host galaxy. The previous lensed SN was called \"Requiem\", and therefore the new SN is named \"Encore\". This makes the MACS J0138.0$-$2155 cluster the first known system to produce more than one multiply-imaged SN. Moreover, both SN Requiem and SN Encore are Type Ia SNe (SNe Ia), making this the most distant case of a galaxy hosting two SNe Ia. Using parametric host fitting, we determine the probability of detecting two SNe Ia in this host galaxy over a $\\sim10$ year window to be $\\approx3\\%$. These observations have the potential to yield a Hubble Constant ($H_0$) measurement with $\\sim10\\%$ precision, only the third lensed SN capable of such a result, using the three visible images of the SN. Both SN Requiem and SN Encore have a fourth image that is expected to appear within a few years of $\\sim2030$, providing unprecedented baseline for time-delay cosmography.","sentences":["A bright ($m_{\\rm F150W,AB}$=24 mag), $z=1.95$ supernova (SN) candidate was discovered in JWST/NIRCam imaging acquired on 2023 November 17.","The SN is quintuply-imaged as a result of strong gravitational lensing by a foreground galaxy cluster, detected in three locations, and remarkably is the second lensed SN found in the same host galaxy.","The previous lensed SN was called \"Requiem\", and therefore the new SN is named \"Encore\".","This makes the MACS J0138.0$-$2155 cluster the first known system to produce more than one multiply-imaged SN.","Moreover, both SN Requiem and SN Encore are Type Ia SNe (SNe Ia), making this the most distant case of a galaxy hosting two SNe Ia. Using parametric host fitting, we determine the probability of detecting two SNe Ia in this host galaxy over a $\\sim10$ year window to be $\\approx3\\%$. These observations have the potential to yield a Hubble Constant ($H_0$) measurement with $\\sim10\\%$ precision, only the third lensed SN capable of such a result, using the three visible images of the SN.","Both SN Requiem and SN Encore have a fourth image that is expected to appear within a few years of $\\sim2030$, providing unprecedented baseline for time-delay cosmography."],"url":"http://arxiv.org/abs/2404.02139v1","category":"astro-ph.CO"}
{"created":"2024-04-02 17:42:30","title":"Numerical simulation of the Gross-Pitaevskii equation via vortex tracking","abstract":"This paper deals with the numerical simulation of the Gross-Pitaevskii (GP) equation, for which a well-known feature is the appearance of quantized vortices with core size of the order of a small parameter $\\varepsilon$. Without a magnetic field and with suitable initial conditions, these vortices interact, in the singular limit $\\varepsilon\\to0$, through an explicit Hamiltonian dynamics. Using this analytical framework, we develop and analyze a numerical strategy based on the reduced-order Hamiltonian system to efficiently simulate the infinite-dimensional GP equation for small, but finite, $\\varepsilon$. This method allows us to avoid numerical stability issues in solving the GP equation, where small values of $\\varepsilon$ typically require very fine meshes and time steps. We also provide a mathematical justification of our method in terms of rigorous error estimates of the error in the supercurrent, together with numerical illustrations.","sentences":["This paper deals with the numerical simulation of the Gross-Pitaevskii (GP) equation, for which a well-known feature is the appearance of quantized vortices with core size of the order of a small parameter $\\varepsilon$. Without a magnetic field and with suitable initial conditions, these vortices interact, in the singular limit $\\varepsilon\\to0$, through an explicit Hamiltonian dynamics.","Using this analytical framework, we develop and analyze a numerical strategy based on the reduced-order Hamiltonian system to efficiently simulate the infinite-dimensional GP equation for small, but finite, $\\varepsilon$. This method allows us to avoid numerical stability issues in solving the GP equation, where small values of $\\varepsilon$ typically require very fine meshes and time steps.","We also provide a mathematical justification of our method in terms of rigorous error estimates of the error in the supercurrent, together with numerical illustrations."],"url":"http://arxiv.org/abs/2404.02133v1","category":"math.NA"}
{"created":"2024-04-02 17:23:34","title":"Reaching Curzon-Ahlborn limit in linear response and Whitney limit in nonlinear response in edge mode quantum thermoelectrics and refrigeration","abstract":"Quantum heat engines and quantum refrigerators are proposed in three-terminal quantum Hall (QH) and quantum spin Hall (QSH) setups with a voltage-temperature probe in linear and nonlinear transport regimes. In the linear response regime, we find that efficiency at maximum power approaches the Curzon-Ahlborn limit in both QH and QSH setups. Similarly, in nonlinear response, we find that efficiency at maximum power reaches the Whitney bounds. For the first time, we see that the thermoelectric efficiency limits in linear and nonlinear transport regimes are achieved using quantum point contacts in the same setup.","sentences":["Quantum heat engines and quantum refrigerators are proposed in three-terminal quantum Hall (QH) and quantum spin Hall (QSH) setups with a voltage-temperature probe in linear and nonlinear transport regimes.","In the linear response regime, we find that efficiency at maximum power approaches the Curzon-Ahlborn limit in both QH and QSH setups.","Similarly, in nonlinear response, we find that efficiency at maximum power reaches the Whitney bounds.","For the first time, we see that the thermoelectric efficiency limits in linear and nonlinear transport regimes are achieved using quantum point contacts in the same setup."],"url":"http://arxiv.org/abs/2404.02118v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 17:20:12","title":"The lattice structure of negative Sobolev and extrapolation spaces","abstract":"It is well-known that the Sobolev spaces $W^{k,p}(\\mathbb R^d)$ are vector lattices with respect to the pointwise almost everywhere order if $k \\in \\{0,1\\}$, but not if $k \\ge 2$. In this note, we consider negative $k$ and show that the span of the positive cone in $W^{k,p}(\\mathbb R^d)$ is a vector lattice in this case.   We also prove a related abstract result: if $(T(t))_{t \\in [0,\\infty)}$ is a positive $C_0$-semigroup on a Banach lattice $X$ with order continuous norm, then the span of the cone $X_{-1,+}$ in the extrapolation space $X_{-1}$ is a vector lattice. This complements results obtained by B\\'atkai, Jacob, Wintermayr, and Voigt in the context of perturbation theory and provides additional context for the theory of infinite-dimensional positive systems.","sentences":["It is well-known that the Sobolev spaces $W^{k,p}(\\mathbb R^d)$ are vector lattices with respect to the pointwise almost everywhere order if $k \\in \\{0,1\\}$, but not if $k \\ge 2$.","In this note, we consider negative $k$ and show that the span of the positive cone in $W^{k,p}(\\mathbb R^d)$ is a vector lattice in this case.   ","We also prove a related abstract result: if $(T(t))_{t \\in [0,\\infty)}$ is a positive $C_0$-semigroup on a Banach lattice $X$ with order continuous norm, then the span of the cone $X_{-1,+}$ in the extrapolation space $X_{-1}$ is a vector lattice.","This complements results obtained by B\\'atkai, Jacob, Wintermayr, and Voigt in the context of perturbation theory and provides additional context for the theory of infinite-dimensional positive systems."],"url":"http://arxiv.org/abs/2404.02116v1","category":"math.FA"}
{"created":"2024-04-02 17:12:53","title":"Risk-Aware Real-Time Task Allocation for Stochastic Multi-Agent Systems under STL Specifications","abstract":"This paper addresses the control synthesis of heterogeneous stochastic linear multi-agent systems with real-time allocation of signal temporal logic (STL) specifications. Based on previous work, we decompose specifications into sub-specifications on the individual agent level. To leverage the efficiency of task allocation, a heuristic filter evaluates potential task allocation based on STL robustness. Subsequently, an auctioning algorithm determines the definite allocation of specifications. Finally, a control strategy is synthesized for each agent-specification pair using tube-based Model Predictive Control (MPC), ensuring provable probabilistic satisfaction. We demonstrate the efficacy of the proposed methods using a multi-bus scenario that highlights a promising extension to autonomous driving applications like crossing an intersection.","sentences":["This paper addresses the control synthesis of heterogeneous stochastic linear multi-agent systems with real-time allocation of signal temporal logic (STL) specifications.","Based on previous work, we decompose specifications into sub-specifications on the individual agent level.","To leverage the efficiency of task allocation, a heuristic filter evaluates potential task allocation based on STL robustness.","Subsequently, an auctioning algorithm determines the definite allocation of specifications.","Finally, a control strategy is synthesized for each agent-specification pair using tube-based Model Predictive Control (MPC), ensuring provable probabilistic satisfaction.","We demonstrate the efficacy of the proposed methods using a multi-bus scenario that highlights a promising extension to autonomous driving applications like crossing an intersection."],"url":"http://arxiv.org/abs/2404.02111v1","category":"eess.SY"}
{"created":"2024-04-02 17:06:42","title":"Deep Learning for AGILE Anticoincidence System's Background Prediction from Orbital and Attitude Parameters","abstract":"AGILE is an Italian Space Agency (ASI) space mission launched in 2007 to study X-ray and gamma-ray phenomena in the energy range from $\\sim$20 keV to $\\sim$10 GeV. The AGILE AntiCoincidence System (ACS) detects hard-X photons in the 50 - 200 keV energy range and continuously stores each panel's count rates in the telemetry. We developed a new Deep Learning (DL) model to predict the background of the AGILE ACS top panel using the satellite's orbital and attitude parameters. This model aims to learn how the orbital and spinning modulations of the satellite impact the background level of the ACS top panel. The DL model executes a regression problem, and is trained with a supervised learning technique on a dataset larger than twenty million orbital parameters' configurations. Using a test dataset, we evaluated the trained model by comparison of the predicted count rates with the real ones. The results show that the model can reconstruct the background count rates of the ACS top panel with an accuracy of 96.7\\%, considering the orbital modulation and spinning of the satellite. Starting from these promising results, we are developing an anomaly detection method to detect Gamma-ray Bursts when the differences between predicted and real count rates exceed a predefined threshold.","sentences":["AGILE is an Italian Space Agency (ASI) space mission launched in 2007 to study X-ray and gamma-ray phenomena in the energy range from $\\sim$20 keV to $\\sim$10 GeV.","The AGILE AntiCoincidence System (ACS) detects hard-X photons in the 50 - 200 keV energy range and continuously stores each panel's count rates in the telemetry.","We developed a new Deep Learning (DL) model to predict the background of the AGILE ACS top panel using the satellite's orbital and attitude parameters.","This model aims to learn how the orbital and spinning modulations of the satellite impact the background level of the ACS top panel.","The DL model executes a regression problem, and is trained with a supervised learning technique on a dataset larger than twenty million orbital parameters' configurations.","Using a test dataset, we evaluated the trained model by comparison of the predicted count rates with the real ones.","The results show that the model can reconstruct the background count rates of the ACS top panel with an accuracy of 96.7\\%, considering the orbital modulation and spinning of the satellite.","Starting from these promising results, we are developing an anomaly detection method to detect Gamma-ray Bursts when the differences between predicted and real count rates exceed a predefined threshold."],"url":"http://arxiv.org/abs/2404.02107v1","category":"astro-ph.HE"}
{"created":"2024-04-02 17:04:45","title":"Neural Ordinary Differential Equation based Sequential Image Registration for Dynamic Characterization","abstract":"Deformable image registration (DIR) is crucial in medical image analysis, enabling the exploration of biological dynamics such as organ motions and longitudinal changes in imaging. Leveraging Neural Ordinary Differential Equations (ODE) for registration, this extension work discusses how this framework can aid in the characterization of sequential biological processes. Utilizing the Neural ODE's ability to model state derivatives with neural networks, our Neural Ordinary Differential Equation Optimization-based (NODEO) framework considers voxels as particles within a dynamic system, defining deformation fields through the integration of neural differential equations. This method learns dynamics directly from data, bypassing the need for physical priors, making it exceptionally suitable for medical scenarios where such priors are unavailable or inapplicable. Consequently, the framework can discern underlying dynamics and use sequence data to regularize the transformation trajectory. We evaluated our framework on two clinical datasets: one for cardiac motion tracking and another for longitudinal brain MRI analysis. Demonstrating its efficacy in both 2D and 3D imaging scenarios, our framework offers flexibility and model agnosticism, capable of managing image sequences and facilitating label propagation throughout these sequences. This study provides a comprehensive understanding of how the Neural ODE-based framework uniquely benefits the image registration challenge.","sentences":["Deformable image registration (DIR) is crucial in medical image analysis, enabling the exploration of biological dynamics such as organ motions and longitudinal changes in imaging.","Leveraging Neural Ordinary Differential Equations (ODE) for registration, this extension work discusses how this framework can aid in the characterization of sequential biological processes.","Utilizing the Neural ODE's ability to model state derivatives with neural networks, our Neural Ordinary Differential Equation Optimization-based (NODEO) framework considers voxels as particles within a dynamic system, defining deformation fields through the integration of neural differential equations.","This method learns dynamics directly from data, bypassing the need for physical priors, making it exceptionally suitable for medical scenarios where such priors are unavailable or inapplicable.","Consequently, the framework can discern underlying dynamics and use sequence data to regularize the transformation trajectory.","We evaluated our framework on two clinical datasets: one for cardiac motion tracking and another for longitudinal brain MRI analysis.","Demonstrating its efficacy in both 2D and 3D imaging scenarios, our framework offers flexibility and model agnosticism, capable of managing image sequences and facilitating label propagation throughout these sequences.","This study provides a comprehensive understanding of how the Neural ODE-based framework uniquely benefits the image registration challenge."],"url":"http://arxiv.org/abs/2404.02106v1","category":"cs.CV"}
{"created":"2024-04-02 17:03:57","title":"Individual-Ion Addressing and Readout in a Penning Trap","abstract":"We implement individual addressing and readout of ions in a rigidly rotating planar crystal in a compact, permanent magnet Penning trap. The crystal of $^{40}$Ca$^+$ is trapped and stabilized without defects via a rotating triangular potential. The trapped ion fluorescence is detected in the rotating frame for parallel readout. The qubit is encoded in the metastable D$_{5/2}$ manifold enabling the use of high-power near-infrared laser systems for qubit operations. Addressed $\\sigma_z$ operations are realized with a focused AC Stark shifting laser beam. We demonstrate addressing of ions near the center of the crystal and at large radii. Simulations show that the current addressing operation fidelity is limited to $\\sim 97\\%$ by the ion's thermal extent for the in-plane modes near the Doppler limit, but this could be improved to infidelities $<10^{-3}$ with sub-Doppler cooling. The techniques demonstrated in this paper complete the set of operations for quantum simulation with the platform.","sentences":["We implement individual addressing and readout of ions in a rigidly rotating planar crystal in a compact, permanent magnet Penning trap.","The crystal of $^{40}$Ca$^+$ is trapped and stabilized without defects via a rotating triangular potential.","The trapped ion fluorescence is detected in the rotating frame for parallel readout.","The qubit is encoded in the metastable D$_{5/2}$ manifold enabling the use of high-power near-infrared laser systems for qubit operations.","Addressed $\\sigma_z$ operations are realized with a focused AC Stark shifting laser beam.","We demonstrate addressing of ions near the center of the crystal and at large radii.","Simulations show that the current addressing operation fidelity is limited to $\\sim 97\\%$ by the ion's thermal extent for the in-plane modes near the Doppler limit, but this could be improved to infidelities $<10^{-3}$ with sub-Doppler cooling.","The techniques demonstrated in this paper complete the set of operations for quantum simulation with the platform."],"url":"http://arxiv.org/abs/2404.02105v1","category":"physics.atom-ph"}
{"created":"2024-04-02 17:03:40","title":"Quantum Hall effect in a CVD-grown oxide","abstract":"Two-dimensional electron systems (2DES) are promising for investigating correlated quantum phenomena. In particular, 2D oxides provide a platform that can host various quantum phases such as quantized Hall effect, superconductivity, or magnetism. The realization of such quantum phases in 2D oxides heavily relies on dedicated heterostructure growths. Here we show the integer quantum Hall effect achieved in chemical vapor deposition grown Bi2O2Se - a representative member of a more accessible oxide family. A single or few sub-band 2DES can be prepared in thin films of Bi2O2Se, where the film thickness acts as the sole design parameter and the sub-band occupation is determined by the electric field effect. This new oxide platform exhibits characteristic advantages in structural flexibility due to its layered nature, making it suitable for scalable growth. The unique small mass distinguishes Bi2O2Se from other high-mobility oxides, providing a new platform for exploring quantum Hall physics in 2D oxides.","sentences":["Two-dimensional electron systems (2DES) are promising for investigating correlated quantum phenomena.","In particular, 2D oxides provide a platform that can host various quantum phases such as quantized Hall effect, superconductivity, or magnetism.","The realization of such quantum phases in 2D oxides heavily relies on dedicated heterostructure growths.","Here we show the integer quantum Hall effect achieved in chemical vapor deposition grown Bi2O2Se - a representative member of a more accessible oxide family.","A single or few sub-band 2DES can be prepared in thin films of Bi2O2Se, where the film thickness acts as the sole design parameter and the sub-band occupation is determined by the electric field effect.","This new oxide platform exhibits characteristic advantages in structural flexibility due to its layered nature, making it suitable for scalable growth.","The unique small mass distinguishes Bi2O2Se from other high-mobility oxides, providing a new platform for exploring quantum Hall physics in 2D oxides."],"url":"http://arxiv.org/abs/2404.02104v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 16:45:39","title":"$S$-nodes, factorisation of spectral matrix functions and corresponding inequalities","abstract":"Using factorisation and Arov-Krein inequality results, we derive important inequalities (in terms of $S$-nodes) in interpolation problems.","sentences":["Using factorisation and Arov-Krein inequality results, we derive important inequalities (in terms of $S$-nodes) in interpolation problems."],"url":"http://arxiv.org/abs/2404.02094v1","category":"math.CA"}
{"created":"2024-04-02 16:27:44","title":"Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows","abstract":"Explainable AI (XAI) tools represent a turn to more human-centered and human-in-the-loop AI approaches that emphasize user needs and perspectives in machine learning model development workflows. However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces. In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks. We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch text classification workflow. Finally, we conclude with a discussion of best practices and open questions. Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI.","sentences":["Explainable AI (XAI) tools represent a turn to more human-centered and human-in-the-loop AI approaches that emphasize user needs and perspectives in machine learning model development workflows.","However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces.","In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks.","We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch text classification workflow.","Finally, we conclude with a discussion of best practices and open questions.","Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI."],"url":"http://arxiv.org/abs/2404.02081v1","category":"cs.HC"}
{"created":"2024-04-02 16:21:21","title":"Linear distortion and rescaling for quasiregular values","abstract":"Sobolev mappings exhibiting only pointwise quasiregularity-type bounds have arisen in various applications, leading to a recently developed theory of quasiregular values. In this article, we show that by using rescaling, one obtains a direct bridge between this theory and the classical theory of quasiregular maps. More precisely, we prove that a non-constant mapping $f \\colon \\Omega \\to \\mathbb{R}^n$ with a $(K, \\Sigma)$-quasiregular value at $f(x_0)$ can be rescaled at $x_0$ to a non-constant $K$-quasiregular mapping. Our proof of this fact involves establishing a quasiregular values -version of the linear distortion bound of quasiregular mappings. A quasiregular values variant of the small $K$ -theorem is obtained as an immediate corollary of our main result.","sentences":["Sobolev mappings exhibiting only pointwise quasiregularity-type bounds have arisen in various applications, leading to a recently developed theory of quasiregular values.","In this article, we show that by using rescaling, one obtains a direct bridge between this theory and the classical theory of quasiregular maps.","More precisely, we prove that a non-constant mapping $f \\colon \\Omega \\to \\mathbb{R}^n$ with a $(K, \\Sigma)$-quasiregular value at $f(x_0)$ can be rescaled at $x_0$ to a non-constant $K$-quasiregular mapping.","Our proof of this fact involves establishing a quasiregular values -version of the linear distortion bound of quasiregular mappings.","A quasiregular values variant of the small $K$ -theorem is obtained as an immediate corollary of our main result."],"url":"http://arxiv.org/abs/2404.02073v1","category":"math.CV"}
{"created":"2024-04-02 15:58:36","title":"IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT","abstract":"Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training.   Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that \"parameter efficiency represents overall efficiency\". TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at https://github.com/jjGenAILab/IISAN.","sentences":["Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities.","While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed.","Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   ","IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT.","More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks.","Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT.","This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training.   ","Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that \"parameter efficiency represents overall efficiency\".","TPME provides more comprehensive insights into practical efficiency comparisons between different methods.","Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN.","We release our codes and other materials at https://github.com/jjGenAILab/IISAN."],"url":"http://arxiv.org/abs/2404.02059v1","category":"cs.IR"}
{"created":"2024-04-02 15:45:39","title":"Classification of superpotentials for cohomogeneity one Ricci solitons","abstract":"We classify superpotentials for the Hamiltonian system corresponding to the cohomogeneity one gradient Ricci soliton equations. Aside from recovering known examples of superpotentials for steady solitons, we find a new superpotential on a specific case of the B\\'erard-Bergery-Calabi ansatz. The latter is used to obtain an explicit formula for a steady complete soliton with an equidistant family of hypersurfaces given by circle bundles over $S^2\\times S^2$. There are no superpotentials in the non-steady case in dimensions greater than 2, even if polynomial coefficients are allowed. We also briefly discuss generalised first integrals and the limitations of some known methods of finding them.","sentences":["We classify superpotentials for the Hamiltonian system corresponding to the cohomogeneity one gradient Ricci soliton equations.","Aside from recovering known examples of superpotentials for steady solitons, we find a new superpotential on a specific case of the B\\'erard-Bergery-Calabi ansatz.","The latter is used to obtain an explicit formula for a steady complete soliton with an equidistant family of hypersurfaces given by circle bundles over $S^2\\times S^2$.","There are no superpotentials in the non-steady case in dimensions greater than 2, even if polynomial coefficients are allowed.","We also briefly discuss generalised first integrals and the limitations of some known methods of finding them."],"url":"http://arxiv.org/abs/2404.02050v1","category":"math.DG"}
{"created":"2024-04-02 15:41:00","title":"Dynamical study of $D^{*}DK$ and $D^{*}D \\bar{D}$ systems at quark level","abstract":"Inspired by that Belle\\uppercase\\expandafter{\\romannumeral2} Collaboration recently reported $T_{cc}$, which can be interpreted as a molecular $DD^{*}$, we investigated the trihadron system of $T_{cc}$ partner with $IJ^{P}$=$01^{-}$ in the framework of a chiral quark model. It's widely accepted that the main component of $X(3872)$ contains the molecular $\\bar{D}D^{*}$, while the main component of $D_{s0}^{*}(2317)$ is molecular $DK$. Based on these three well-known exotic states, $T_{cc} (DD^{*})$, $X(3872) (\\bar{D}D^{*})$ and $D_{s0}^{*}(2317) (DK)$, we dynamically investigate $D^{*}DK$ and $DD^{*}\\bar{D}$ systems at quark level to search for possible bound states. The results show that both of them are bound states, in which the binding energy of the molecular state $DD^*K$ is relatively small, only 0.8 MeV, while the binding energy of $DD^*\\bar{D}$ is up to 1.9 MeV. According to the calculation results of the Root-square-mean distances, the spatial structure of the two systems shows obvious ($DD^*$)-($\\bar{D}$/$K$) structure, in which $D$ is close to $D^*$ while $DD^*$ as a whole is relatively distant from the third hadron ($\\bar{D}$/$K$), which are similar to the nucleon-electron structure. As a result, we strongly recommend that these bound states $DD^*\\bar{D}$ and $DD^*K$ are searched for experimentally.","sentences":["Inspired by that Belle\\uppercase\\expandafter{\\romannumeral2} Collaboration recently reported $T_{cc}$, which can be interpreted as a molecular $DD^{*}$, we investigated the trihadron system of $T_{cc}$ partner with $IJ^{P}$=$01^{-}$ in the framework of a chiral quark model.","It's widely accepted that the main component of $X(3872)$ contains the molecular $\\bar{D}D^{*}$, while the main component of $D_{s0}^{*}(2317)$ is molecular $DK$. Based on these three well-known exotic states, $T_{cc} (DD^{*})$, $X(3872) (\\bar{D}D^{*})$ and $D_{s0}^{*}(2317) (DK)$, we dynamically investigate $D^{*}DK$ and $DD^{*}\\bar{D}$ systems at quark level to search for possible bound states.","The results show that both of them are bound states, in which the binding energy of the molecular state $DD^*K$ is relatively small, only 0.8 MeV, while the binding energy of $DD^*\\bar{D}$ is up to 1.9 MeV.","According to the calculation results of the Root-square-mean distances, the spatial structure of the two systems shows obvious ($DD^*$)-($\\bar{D}$/$K$) structure, in which $D$ is close to $D^*$ while $DD^*$ as a whole is relatively distant from the third hadron ($\\bar{D}$/$K$), which are similar to the nucleon-electron structure.","As a result, we strongly recommend that these bound states $DD^*\\bar{D}$ and $DD^*K$ are searched for experimentally."],"url":"http://arxiv.org/abs/2404.02048v1","category":"hep-ph"}
{"created":"2024-04-02 15:34:53","title":"Root Graded Groups","abstract":"We define and study root graded groups, that is, groups graded by finite root systems. This notion generalises several existing concepts in the literature, including in particular Jacques Tits' notion of RGD-systems. The most prominent examples of root graded groups are Chevalley groups over commutative associative rings. Our main result is that every root graded group of rank at least 3 is coordinatised by some algebraic structure satisfying a variation of the Chevalley commutator formula. This result can be regarded as a generalisation of Tits' classification of thick irreducible spherical buildings of rank at least 3 to the case of non-division algebraic structures. All coordinatisation results in this book are proven in a characteristic-free way. This is made possible by a new computational method that we call the blueprint technique.","sentences":["We define and study root graded groups, that is, groups graded by finite root systems.","This notion generalises several existing concepts in the literature, including in particular Jacques Tits' notion of RGD-systems.","The most prominent examples of root graded groups are Chevalley groups over commutative associative rings.","Our main result is that every root graded group of rank at least 3 is coordinatised by some algebraic structure satisfying a variation of the Chevalley commutator formula.","This result can be regarded as a generalisation of Tits' classification of thick irreducible spherical buildings of rank at least 3 to the case of non-division algebraic structures.","All coordinatisation results in this book are proven in a characteristic-free way.","This is made possible by a new computational method that we call the blueprint technique."],"url":"http://arxiv.org/abs/2404.02042v1","category":"math.GR"}
{"created":"2024-04-02 15:32:27","title":"Analytic conjugation between planar differential systems and potential systems","abstract":"In this article it is proved that an analytical planar vector field with a non-degenerate center at $(0,0)$ is analytically conjugate, in a neighborhood of $(0,0)$, to a Hamiltonian vector field of the form $y\\frac{\\partial}{\\partial x}-V'(x)\\frac{\\partial}{\\partial y}$, where $V$ is an analytic function defined in a neighborhood of the origin such that $V(0)=V'(0)=0$ and $V''(0)>0.$","sentences":["In this article it is proved that an analytical planar vector field with a non-degenerate center at $(0,0)$ is analytically conjugate, in a neighborhood of $(0,0)$, to a Hamiltonian vector field of the form $y\\frac{\\partial}{\\partial x}-V'(x)\\frac{\\partial}{\\partial y}$, where $V$ is an analytic function defined in a neighborhood of the origin such that $V(0)=V'(0)=0$ and $V''(0)>0.$"],"url":"http://arxiv.org/abs/2404.02036v1","category":"math.DS"}
{"created":"2024-04-02 15:14:29","title":"Infrared nanosensors of pico- to micro-newton forces","abstract":"Mechanical force is an essential feature for many physical and biological processes.1-12 Remote measurement of mechanical signals with high sensitivity and spatial resolution is needed for diverse applications, including robotics,13 biophysics,14-20 energy storage,21-24 and medicine.25-27 Nanoscale luminescent force sensors excel at measuring piconewton forces,28-32 while larger sensors have proven powerful in probing micronewton forces.33,34 However, large gaps remain in the force magnitudes that can be probed remotely from subsurface or interfacial sites, and no individual, non-invasive sensor is capable of measuring over the large dynamic range needed to understand many systems.35,36 Here, we demonstrate Tm3+-doped avalanching nanoparticle37 force sensors that can be addressed remotely by deeply penetrating near-infrared (NIR) light and can detect piconewton to micronewton forces with a dynamic range spanning more than four orders of magnitude. Using atomic force microscopy coupled with single-nanoparticle optical spectroscopy, we characterize the mechanical sensitivity of the photon avalanching process and reveal its exceptional force responsiveness. By manipulating the Tm3+ concentrations and energy transfer within the nanosensors, we demonstrate different optical force-sensing modalities, including mechanobrightening and mechanochromism. The adaptability of these nanoscale optical force sensors, along with their multiscale sensing capability, enable operation in the dynamic and versatile environments present in real-world, complex structures spanning biological organisms to nanoelectromechanical systems (NEMS).","sentences":["Mechanical force is an essential feature for many physical and biological processes.1-12 Remote measurement of mechanical signals with high sensitivity and spatial resolution is needed for diverse applications, including robotics,13 biophysics,14-20 energy storage,21-24 and","medicine.25-27 Nanoscale luminescent force sensors excel at measuring piconewton forces,28-32 while larger sensors have proven powerful in probing micronewton forces.33,34","However, large gaps remain in the force magnitudes that can be probed remotely from subsurface or interfacial sites, and no individual, non-invasive sensor is capable of measuring over the large dynamic range needed to understand many systems.35,36 Here, we demonstrate Tm3+-doped avalanching nanoparticle37 force sensors that can be addressed remotely by deeply penetrating near-infrared (NIR) light and can detect piconewton to micronewton forces with a dynamic range spanning more than four orders of magnitude.","Using atomic force microscopy coupled with single-nanoparticle optical spectroscopy, we characterize the mechanical sensitivity of the photon avalanching process and reveal its exceptional force responsiveness.","By manipulating the Tm3+ concentrations and energy transfer within the nanosensors, we demonstrate different optical force-sensing modalities, including mechanobrightening and mechanochromism.","The adaptability of these nanoscale optical force sensors, along with their multiscale sensing capability, enable operation in the dynamic and versatile environments present in real-world, complex structures spanning biological organisms to nanoelectromechanical systems (NEMS)."],"url":"http://arxiv.org/abs/2404.02026v1","category":"physics.optics"}
{"created":"2024-04-02 15:12:35","title":"On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty","abstract":"Continuous-time adaptive controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty. However, such methods are often unimplementable, as they depend on an unobserved estimation error. We consider the equivalent discrete-time setting with a causal information structure. We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition. We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition.","sentences":["Continuous-time adaptive controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty.","However, such methods are often unimplementable, as they depend on an unobserved estimation error.","We consider the equivalent discrete-time setting with a causal information structure.","We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition.","We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition."],"url":"http://arxiv.org/abs/2404.02023v1","category":"eess.SY"}
{"created":"2024-04-02 14:56:43","title":"MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving","abstract":"Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization. MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment.","sentences":["Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search.","However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs.","In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving.","The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources.","MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization.","MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing.","Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment."],"url":"http://arxiv.org/abs/2404.02015v1","category":"cs.DC"}
{"created":"2024-04-02 14:55:59","title":"On the Effect of Quantization on Dynamic Mode Decomposition","abstract":"Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.This paper investigates how the estimation process is affected when the data is quantized. Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from quantized data. Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the quantized estimate can be considered a regularized version of the unquantized estimate. This key theoretical finding paves the way to accurately recover the unquantized estimate from quantized data. We also explore the relationship between the two estimates in the finite data regime. The theory is validated through repeated numerical experiments conducted on three different dynamical systems.","sentences":["Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.","This paper investigates how the estimation process is affected when the data is quantized.","Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from quantized data.","Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the quantized estimate can be considered a regularized version of the unquantized estimate.","This key theoretical finding paves the way to accurately recover the unquantized estimate from quantized data.","We also explore the relationship between the two estimates in the finite data regime.","The theory is validated through repeated numerical experiments conducted on three different dynamical systems."],"url":"http://arxiv.org/abs/2404.02014v1","category":"eess.SY"}
{"created":"2024-04-02 14:54:36","title":"Resource-Aware Collaborative Monte Carlo Localization with Distribution Compression","abstract":"Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems. In this paper, we address the task of collaborative global localization under computational and communication constraints. We propose a method which reduces the amount of information exchanged and the computational cost. We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community. We exploit techniques for distribution compression in near-linear time, with error guarantees. We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world. Our approach can run online on an onboard computer. We release an open-source C++/ROS2 implementation of our approach, as well as the baselines","sentences":["Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems.","In this paper, we address the task of collaborative global localization under computational and communication constraints.","We propose a method which reduces the amount of information exchanged and the computational cost.","We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community.","We exploit techniques for distribution compression in near-linear time, with error guarantees.","We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world.","Our approach can run online on an onboard computer.","We release an open-source C++/ROS2 implementation of our approach, as well as the baselines"],"url":"http://arxiv.org/abs/2404.02010v1","category":"cs.RO"}
{"created":"2024-04-02 14:53:41","title":"Preuve de concept d'un bot vocal dialoguant en wolof","abstract":"This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22\\% of WER for the ASR task and 78\\% of F1-score on the NLU task.","sentences":["This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal.","This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal.","The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech.","The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings.","The first results of this proof-of-concept are encouraging as we achieved 22\\% of WER for the ASR task and 78\\% of F1-score on the NLU task."],"url":"http://arxiv.org/abs/2404.02009v1","category":"cs.CL"}
{"created":"2024-04-02 14:52:24","title":"Singularity formation of vortex sheets in 2D Euler equations using the characteristic mapping method","abstract":"The goal of this numerical study is to get insight into singular solutions of the two-dimensional (2D) Euler equations for non-smooth initial data, in particular for vortex sheets. To this end high resolution computations of vortex layers in 2D incompressible Euler flows are performed using the characteristic mapping method (CMM). This semi-Lagrangian method evolves the flow map using the gradient-augmented level set method (GALS). The semi-group structure of the flow map allows its decomposition into sub-maps (each over a finite time interval), and thus the precision can be controlled by choosing appropriate remapping times. Composing the flow map yields exponential resolution in linear time, a unique feature of CMM, and thus fine scale flow structures can be resolved in great detail. Here the roll-up process of vortex layers is studied varying the thickness of the layer showing its impact on the growth of palinstrophy and possible blow up of absolute vorticity. The curvature of the vortex sheet shows a singular-like behavior. The self-similar structure of the vortex core is investigated in the vanishing thickness limit. Conclusions on the non-uniqueness of weak solutions of 2D Euler for non-smooth initial data are drawn and the presence of flow singularities is revealed tracking them in the complex plane.","sentences":["The goal of this numerical study is to get insight into singular solutions of the two-dimensional (2D) Euler equations for non-smooth initial data, in particular for vortex sheets.","To this end high resolution computations of vortex layers in 2D incompressible Euler flows are performed using the characteristic mapping method (CMM).","This semi-Lagrangian method evolves the flow map using the gradient-augmented level set method (GALS).","The semi-group structure of the flow map allows its decomposition into sub-maps (each over a finite time interval), and thus the precision can be controlled by choosing appropriate remapping times.","Composing the flow map yields exponential resolution in linear time, a unique feature of CMM, and thus fine scale flow structures can be resolved in great detail.","Here the roll-up process of vortex layers is studied varying the thickness of the layer showing its impact on the growth of palinstrophy and possible blow up of absolute vorticity.","The curvature of the vortex sheet shows a singular-like behavior.","The self-similar structure of the vortex core is investigated in the vanishing thickness limit.","Conclusions on the non-uniqueness of weak solutions of 2D Euler for non-smooth initial data are drawn and the presence of flow singularities is revealed tracking them in the complex plane."],"url":"http://arxiv.org/abs/2404.02008v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 14:41:05","title":"Cash or Non-Cash? Unveiling Ideators' Incentive Preferences in Crowdsourcing Contests","abstract":"Even though research has repeatedly shown that non-cash incentives can be effective, cash incentives are the de facto standard in crowdsourcing contests. In this multi-study research, we quantify ideators' preferences for non-cash incentives and investigate how allowing ideators to self-select their preferred incentive -- offering ideators a choice between cash and non-cash incentives -- affects their creative performance. We further explore whether the market context of the organization hosting the contest -- social (non-profit) or monetary (for-profit) -- moderates incentive preferences and their effectiveness. We find that individuals exhibit heterogeneous incentive preferences and often prefer non-cash incentives, even in for-profit contexts. Offering ideators a choice of incentives can enhance creative performance. Market context moderates the effect of incentives, such that ideators who receive non-cash incentives in for-profit contexts tend to exert less effort. We show that heterogeneity of ideators' preferences (and the ability to satisfy diverse preferences with suitably diverse incentive options) is a critical boundary condition to realizing benefits from offering ideators a choice of incentives. We provide managers with guidance to design effective incentives by improving incentive-preference fit for ideators.","sentences":["Even though research has repeatedly shown that non-cash incentives can be effective, cash incentives are the de facto standard in crowdsourcing contests.","In this multi-study research, we quantify ideators' preferences for non-cash incentives and investigate how allowing ideators to self-select their preferred incentive -- offering ideators a choice between cash and non-cash incentives -- affects their creative performance.","We further explore whether the market context of the organization hosting the contest -- social (non-profit) or monetary (for-profit) -- moderates incentive preferences and their effectiveness.","We find that individuals exhibit heterogeneous incentive preferences and often prefer non-cash incentives, even in for-profit contexts.","Offering ideators a choice of incentives can enhance creative performance.","Market context moderates the effect of incentives, such that ideators who receive non-cash incentives in for-profit contexts tend to exert less effort.","We show that heterogeneity of ideators' preferences (and the ability to satisfy diverse preferences with suitably diverse incentive options) is a critical boundary condition to realizing benefits from offering ideators a choice of incentives.","We provide managers with guidance to design effective incentives by improving incentive-preference fit for ideators."],"url":"http://arxiv.org/abs/2404.01997v1","category":"cs.HC"}
{"created":"2024-04-02 14:20:03","title":"Norm Inequalities for Hilbert space operators with Applications","abstract":"Several unitarily invariant norm inequalities and numerical radius inequalities for Hilbert space operators are studied. We investigate some necessary and sufficient conditions for the parallelism of two bounded operators. For a finite rank operator $A,$ it is shown that \\begin{eqnarray*} \\|A\\|_{p} &\\leq &\\left(\\textit{rank} \\, A\\right)^{1/{2p}} \\|A\\|_{2p} \\,\\, \\leq \\,\\, \\left(\\textit{rank} \\, A\\right)^{{(2p-1)}/{2p^2}} \\|A\\|_{2p^2}, \\quad \\textit{for all $p\\geq 1 $} \\end{eqnarray*} where $\\|\\cdot\\|_p$ is the Schatten $p$-norm. If $\\{ \\lambda_n(A) \\}$ is a listing of all non-zero eigenvalues (with multiplicity) of a compact operator $A$, then we show that \\begin{eqnarray*} \\sum_{n} \\left|\\lambda_n(A)\\right|^{p} &\\leq& \\frac12 \\| A\\|_{ p}^{ p} + \\frac12 \\| A^2\\|_{p/2}^{p/2}, \\quad \\textit{for all $p\\geq 2$} \\end{eqnarray*} which improves the classical Weyl's inequality $\\sum_{n} \\left|\\lambda_n(A)\\right|^{p} \\leq \\| A\\|_{ p}^{ p}$ [Proc. Nat. Acad. Sci. USA 1949]. For an $n\\times n$ matrix $A$, we show that the function $p\\to n^{-{1}/{p}}\\|A\\|_p$ is monotone increasing on $p\\geq 1,$ complementing the well known decreasing nature of $p\\to \\|A\\|_p.$ \\indent As an application of these inequalities, we provide an upper bound for the sum of the absolute values of the zeros of a complex polynomial. As another application we provide a refined upper bound for the energy of a graph $G$, namely, $\\mathcal{E}(G) \\leq \\sqrt{2m\\left(\\textit{rank Adj(G)} \\right)},$ where $m$ is the number of edges, improving on a bound by McClelland in $1971$.","sentences":["Several unitarily invariant norm inequalities and numerical radius inequalities for Hilbert space operators are studied.","We investigate some necessary and sufficient conditions for the parallelism of two bounded operators.","For a finite rank operator $A,$ it is shown that \\begin{eqnarray*} \\|A\\|_{p} &\\leq &\\left(\\textit{rank} \\, A\\right)^{1/{2p}} \\|A\\|_{2p} \\,\\, \\leq \\,\\, \\left(\\textit{rank} \\, A\\right)^{{(2p-1)}/{2p^2}} \\|A\\|_{2p^2}, \\quad \\textit{for all $p\\geq 1 $} \\end{eqnarray*} where $\\|\\cdot\\|_p$ is the Schatten $p$-norm.","If $\\{ \\lambda_n(A) \\}$ is a listing of all non-zero eigenvalues (with multiplicity) of a compact operator $A$, then we show that \\begin{eqnarray*} \\sum_{n} \\left|\\lambda_n(A)\\right|^{p} &\\leq& \\frac12 \\| A\\|_{ p}^{ p} + \\frac12 \\| A^2\\|_{p/2}^{p/2}, \\quad \\textit{for all $p\\geq 2$} \\end{eqnarray*} which improves the classical Weyl's inequality $\\sum_{n} \\left|\\lambda_n(A)\\right|^{p} \\leq \\| A\\|_{ p}^{ p}$ [Proc.","Nat.","Acad.","Sci.","USA 1949].","For an $n\\times n$ matrix $A$, we show that the function $p\\to n^{-{1}/{p}}\\|A\\|_p$ is monotone increasing on $p\\geq 1,$ complementing the well known decreasing nature of $p\\to \\|A\\|_p.$ \\indent As an application of these inequalities, we provide an upper bound for the sum of the absolute values of the zeros of a complex polynomial.","As another application we provide a refined upper bound for the energy of a graph $G$, namely, $\\mathcal{E}(G) \\leq \\sqrt{2m\\left(\\textit{rank Adj(G)} \\right)},$ where $m$ is the number of edges, improving on a bound by McClelland in $1971$."],"url":"http://arxiv.org/abs/2404.01982v1","category":"math.FA"}
{"created":"2024-04-02 14:19:06","title":"A Simple Ricci Flow Proof of the Uniformization Theorem","abstract":"In this note, we provide a very simple proof of the uniformization theorem of Riemann surfaces by Ricci flow. The argument builds on a refinement of Hamilton's isoperimetric estimate for the Ricci flow on the two-sphere.","sentences":["In this note, we provide a very simple proof of the uniformization theorem of Riemann surfaces by Ricci flow.","The argument builds on a refinement of Hamilton's isoperimetric estimate for the Ricci flow on the two-sphere."],"url":"http://arxiv.org/abs/2404.01980v1","category":"math.DG"}
{"created":"2024-04-02 14:18:24","title":"The ground state of electron-doped $t-t'-J$ model on cylinders","abstract":"We perform a comprehensive study of the electron-doped $t-t'-J$ model on cylinders with Density Matrix Renormalization Group (DMRG). We adopt both periodic and anti-periodic boundary conditions along the circumference direction to explore the finite size effect. We study doping levels of $1/6$, $1/8$, and $1/12$ which represent the most interesting region in the phase diagram of electron-doped cuprates. We find that for width-4 and 6 systems, the ground state for fixed doping switches between anti-ferromagnetic Neel state and stripe state under different boundary conditions and with system widths, indicating the presence of large finite size effect in the $t-t'-J$ model. We also have a careful analysis of the $d$-wave pairing correlations which also changes quantitatively with boundary conditions and widths of the system. However, the pairing correlations are enhanced when the system becomes wider for all dopings, suggesting the existence of possible long-ranged superconducting order in the thermodynamic limit. The width-8 results are found to be dependent on the starting state in the DMRG calculation for the kept states we can reach. For width-8 system only Neel (stripe) state can be stabilized in DMRG calculation for $1/12$ ($1/6$) doping, while both stripe and Neel states are stable in the DMRG sweep for $1/8$ doping, regardless of the boundary conditions. These results indicate that $1/8$ doping is likely to lie in the boundary of a phase transition between the Neel phase with lower doping and the stripe phase with higher doping, consistent with the previous study. The sensitivity of ground state on boundary conditions and size observed in this work is similar to that in the $t'$- Hubbard model.","sentences":["We perform a comprehensive study of the electron-doped $t-t'-J$ model on cylinders with Density Matrix Renormalization Group (DMRG).","We adopt both periodic and anti-periodic boundary conditions along the circumference direction to explore the finite size effect.","We study doping levels of $1/6$, $1/8$, and $1/12$ which represent the most interesting region in the phase diagram of electron-doped cuprates.","We find that for width-4 and 6 systems, the ground state for fixed doping switches between anti-ferromagnetic Neel state and stripe state under different boundary conditions and with system widths, indicating the presence of large finite size effect in the $t-t'-J$ model.","We also have a careful analysis of the $d$-wave pairing correlations which also changes quantitatively with boundary conditions and widths of the system.","However, the pairing correlations are enhanced when the system becomes wider for all dopings, suggesting the existence of possible long-ranged superconducting order in the thermodynamic limit.","The width-8 results are found to be dependent on the starting state in the DMRG calculation for the kept states we can reach.","For width-8 system only Neel (stripe) state can be stabilized in DMRG calculation for $1/12$ ($1/6$) doping, while both stripe and Neel states are stable in the DMRG sweep for $1/8$ doping, regardless of the boundary conditions.","These results indicate that $1/8$ doping is likely to lie in the boundary of a phase transition between the Neel phase with lower doping and the stripe phase with higher doping, consistent with the previous study.","The sensitivity of ground state on boundary conditions and size observed in this work is similar to that in the $t'$- Hubbard model."],"url":"http://arxiv.org/abs/2404.01979v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 14:17:54","title":"Irreversibility as divergence from equilibrium","abstract":"The entropy production is commonly used to gauge a system's distance from equilibrium, but this interpretation lacks formalization due to the absence of a natural equilibrium measure. This analysis shows that the entropy production of a Markov system measures its separation from specific equilibrium dynamics. These reference dynamics correspond to the closest reversible systems in the information-theoretic sense. This result yields new lower bounds for the entropy production and novel links between thermodynamics and information geometry.","sentences":["The entropy production is commonly used to gauge a system's distance from equilibrium, but this interpretation lacks formalization due to the absence of a natural equilibrium measure.","This analysis shows that the entropy production of a Markov system measures its separation from specific equilibrium dynamics.","These reference dynamics correspond to the closest reversible systems in the information-theoretic sense.","This result yields new lower bounds for the entropy production and novel links between thermodynamics and information geometry."],"url":"http://arxiv.org/abs/2404.01978v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-02 14:08:54","title":"A fading radius valley towards M-dwarfs, a persistent density valley across stellar types","abstract":"The radius valley separating super-Earths from mini-Neptunes is a fundamental benchmark for theories of planet formation and evolution. Observations show that the location of the radius valley decreases with decreasing stellar mass and with increasing orbital period. Here, we build from our previous pebble-based formation model, which, combined with photoevaporation after disc dispersal, unveiled the radius valley as a separator between rocky- and water-worlds. We expand our models for a range of stellar masses spanning from 0.1 to 1.5 $M_\\odot$. We find that the location of the radius valley is well described by a power-law in stellar mass as $R_{\\rm valley} = 1.8197 \\, M_{\\star}^{\\!0.14({+0.02}/{-0.01})}$, which is in excellent agreement with observations. We also find very good agreement with the dependence of the radius valley on orbital period, both for FGK- and M-dwarfs. Additionally, we note that the radius valley gets filled towards low stellar masses, particularly at 0.1-0.4 $M_\\odot$, yielding a rather flat slope in $R_{\\rm valley} - P_{\\rm orb}$. This is the result of orbital migration occurring at lower planet mass for less massive stars, which allows for low-mass water-worlds to reach the inner regions of the system, blurring the separation in mass (and size) between rocky- and water-worlds. Furthermore, we find that for planetary equilibrium temperatures above 400 K, the water in the volatile layer exists fully in the form of steam, puffing the planet radius up compared to condensed-water worlds. This produces an increase in planet radii of $\\sim 30\\%$ at 1 $M_\\oplus$, and of $\\sim 15\\%$ at 5 $M_\\oplus$, compared to condensed-water-worlds. As with Sun-like stars, pebble accretion leaves its imprint on the overall exoplanet population as a depletion of planets with intermediate compositions, carving a valley in planet density for all spectral types (abridged).","sentences":["The radius valley separating super-Earths from mini-Neptunes is a fundamental benchmark for theories of planet formation and evolution.","Observations show that the location of the radius valley decreases with decreasing stellar mass and with increasing orbital period.","Here, we build from our previous pebble-based formation model, which, combined with photoevaporation after disc dispersal, unveiled the radius valley as a separator between rocky- and water-worlds.","We expand our models for a range of stellar masses spanning from 0.1 to 1.5 $M_\\odot$. We find that the location of the radius valley is well described by a power-law in stellar mass as $R_{\\rm valley} = 1.8197 \\, M_{\\star}^{\\!0.14({+0.02}/{-0.01})}$, which is in excellent agreement with observations.","We also find very good agreement with the dependence of the radius valley on orbital period, both for FGK- and M-dwarfs.","Additionally, we note that the radius valley gets filled towards low stellar masses, particularly at 0.1-0.4 $M_\\odot$, yielding a rather flat slope in $R_{\\rm valley} - P_{\\rm orb}$.","This is the result of orbital migration occurring at lower planet mass for less massive stars, which allows for low-mass water-worlds to reach the inner regions of the system, blurring the separation in mass (and size) between rocky- and water-worlds.","Furthermore, we find that for planetary equilibrium temperatures above 400 K, the water in the volatile layer exists fully in the form of steam, puffing the planet radius up compared to condensed-water worlds.","This produces an increase in planet radii of $\\sim 30\\%$ at 1 $M_\\oplus$, and of $\\sim 15\\%$ at 5 $M_\\oplus$, compared to condensed-water-worlds.","As with Sun-like stars, pebble accretion leaves its imprint on the overall exoplanet population as a depletion of planets with intermediate compositions, carving a valley in planet density for all spectral types (abridged)."],"url":"http://arxiv.org/abs/2404.01967v1","category":"astro-ph.EP"}
{"created":"2024-04-02 13:55:05","title":"Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4","abstract":"In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.","sentences":["In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge.","Legal argument reasoning is an essential skill that all law students must master.","Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information.","Our system explores a prompt-based solution using GPT4 to reason over legal arguments.","We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning.","Overall, our system results in a Macro F1 of .8095","on the validation dataset and .7315 (5th out of 21 teams) on the final test set.","Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4."],"url":"http://arxiv.org/abs/2404.01961v1","category":"cs.CL"}
{"created":"2024-04-02 13:54:05","title":"MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels","abstract":"Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage. With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples. Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data.","sentences":["Human activity recognition (HAR) will be an essential function of various emerging applications.","However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements.","In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase.","From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage.","With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality.","Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples.","Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data."],"url":"http://arxiv.org/abs/2404.01958v1","category":"cs.LG"}
{"created":"2024-04-02 13:51:24","title":"Isotropic to nematic transition in alcohol ferrofluids of barium hexaferrite nanoplatelets","abstract":"Alcohol ferrofluids made of ferrimagnetic barium hexaferrite (BHF) nanoplatelets (NPLs) form a unique example of a dipolar fluid - a liquid magnet. Its formation is induced at a high enough concentration of the NPLs. The key interactions between the NPLs are long-ranged dipolar magnetic and screened anisotropic electrostatic. Herein, we report the results on tuning the isotropic-nematic phase transition (i.e., the NPLs threshold concentration) in 1-butanol ferrofluids of BHF NPLs by affecting the interactions in the ferrofluids. The threshold concentration was determined by polarizing optical microscopy (POM) combined with a system for magnetic field manipulation and was in the range between 4.6 and 6.6 \\% (v/v) depending on the ferrofluid. We observed that the threshold concentration decreased for 0.6 \\% (v/v) with a larger mean diameter of the NPLs, up to 0.6 \\% (v/v) with increased ionic strength of the ferrofluid, and for $\\sim$ 2 \\% (v/v) with higher saturation magnetization of the NPLs. We showed that by tuning the parameters affecting the ferrofluid's interplatelet interactions, we can alter the threshold concentration for the liquid magnet formation. The results elucidate the importance of a delicate balance between the repulsive screened electrostatic and attractive dipolar magnetic interactions for the liquid magnet formation.","sentences":["Alcohol ferrofluids made of ferrimagnetic barium hexaferrite (BHF) nanoplatelets (NPLs) form a unique example of a dipolar fluid - a liquid magnet.","Its formation is induced at a high enough concentration of the NPLs.","The key interactions between the NPLs are long-ranged dipolar magnetic and screened anisotropic electrostatic.","Herein, we report the results on tuning the isotropic-nematic phase transition (i.e., the NPLs threshold concentration) in 1-butanol ferrofluids of BHF NPLs by affecting the interactions in the ferrofluids.","The threshold concentration was determined by polarizing optical microscopy (POM) combined with a system for magnetic field manipulation and was in the range between 4.6 and 6.6 \\% (v/v) depending on the ferrofluid.","We observed that the threshold concentration decreased for 0.6 \\% (v/v) with a larger mean diameter of the NPLs, up to 0.6 \\% (v/v) with increased ionic strength of the ferrofluid, and for $\\sim$ 2 \\% (v/v) with higher saturation magnetization of the NPLs.","We showed that by tuning the parameters affecting the ferrofluid's interplatelet interactions, we can alter the threshold concentration for the liquid magnet formation.","The results elucidate the importance of a delicate balance between the repulsive screened electrostatic and attractive dipolar magnetic interactions for the liquid magnet formation."],"url":"http://arxiv.org/abs/2404.01956v1","category":"cond-mat.soft"}
{"created":"2024-04-02 13:47:52","title":"Classifying Graphemes in English Words Through the Application of a Fuzzy Inference System","abstract":"In Linguistics, a grapheme is a written unit of a writing system corresponding to a phonological sound. In Natural Language Processing tasks, written language is analysed through two different mediums, word analysis, and character analysis. This paper focuses on a third approach, the analysis of graphemes. Graphemes have advantages over word and character analysis by being self-contained representations of phonetic sounds. Due to the nature of splitting a word into graphemes being based on complex, non-binary rules, the application of fuzzy logic would provide a suitable medium upon which to predict the number of graphemes in a word. This paper proposes the application of a Fuzzy Inference System to split words into their graphemes. This Fuzzy Inference System results in a correct prediction of the number of graphemes in a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the correct classification. Given the variety in language, graphemes are tied with pronunciation and therefore can change depending on a regional accent/dialect, the +- 1 accuracy represents the impreciseness of grapheme classification when regional variances are accounted for. To give a baseline of comparison, a second method involving a recursive IPA mapping exercise using a pronunciation dictionary was developed to allow for comparisons to be made.","sentences":["In Linguistics, a grapheme is a written unit of a writing system corresponding to a phonological sound.","In Natural Language Processing tasks, written language is analysed through two different mediums, word analysis, and character analysis.","This paper focuses on a third approach, the analysis of graphemes.","Graphemes have advantages over word and character analysis by being self-contained representations of phonetic sounds.","Due to the nature of splitting a word into graphemes being based on complex, non-binary rules, the application of fuzzy logic would provide a suitable medium upon which to predict the number of graphemes in a word.","This paper proposes the application of a Fuzzy Inference System to split words into their graphemes.","This Fuzzy Inference System results in a correct prediction of the number of graphemes in a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the correct classification.","Given the variety in language, graphemes are tied with pronunciation and therefore can change depending on a regional accent/dialect, the +- 1 accuracy represents the impreciseness of grapheme classification when regional variances are accounted for.","To give a baseline of comparison, a second method involving a recursive IPA mapping exercise using a pronunciation dictionary was developed to allow for comparisons to be made."],"url":"http://arxiv.org/abs/2404.01953v1","category":"cs.CL"}
{"created":"2024-04-02 13:44:27","title":"Heuristic Optimization of Amplifier Reconfiguration Process for Autonomous Driving Optical Networks","abstract":"We propose a heuristic-based optimization scheme for reliable optical amplifier reconfiguration process in ADON. In the experiment on a commercial testbed, the scheme prevents a 0.48-dB Q-factor degradation and outperforms 97.3% random solutions.","sentences":["We propose a heuristic-based optimization scheme for reliable optical amplifier reconfiguration process in ADON.","In the experiment on a commercial testbed, the scheme prevents a 0.48-dB Q-factor degradation and outperforms 97.3% random solutions."],"url":"http://arxiv.org/abs/2404.01949v1","category":"eess.SY"}
{"created":"2024-04-02 13:33:31","title":"LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging","abstract":"Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device. However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data. In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge. We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction. We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends. Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system.","sentences":["Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device.","However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data.","In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge.","We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction.","We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends.","Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system."],"url":"http://arxiv.org/abs/2404.01941v1","category":"cs.CV"}
{"created":"2024-04-02 13:32:46","title":"Optical properties and dynamics of direct and spatially and momentum indirect excitons in AlGaAs/AlAs quantum wells","abstract":"We present an experimental study on optical properties and dynamics of direct and spatially and momentum indirect excitons in AlGaAs/AlAs quantum wells near the crossover between $\\varGamma$- and $X$-valley confined electron states. The time-integrated photoluminescence experiment at $T=$4.8 K revealed three simultaneously observed optical transitions resulting from (a) a direct exciton recombination, involving an electron and a hole states both located in the $\\varGamma$-valley in the quantum well layer, and (b) two spatially and momentum indirect excitons, comprising of the confined electron states in the $X$-valley in the AlAs barrier with different effective masses and quantum well holes in the $\\varGamma$-valley. The measured spatial extent, density dependence and temperature dependence of the structure photoluminescence revealed characteristics necessary to pinpoint the states' nature and provided their characterization crucial in future device design. Temporal dynamics observed in the time-resolved photoluminescence measurement showed the complexity of the capture and recombination dynamics, largely affected by nonradiative processes, what is additionally critical in the system's use. This solid state platform hosting both direct and indirect excitons in a highly tunable monolithic system can benefit and underline the operation principles of novel electronic and photonic devices.","sentences":["We present an experimental study on optical properties and dynamics of direct and spatially and momentum indirect excitons in AlGaAs/AlAs quantum wells near the crossover between $\\varGamma$- and $X$-valley confined electron states.","The time-integrated photoluminescence experiment at $T=$4.8 K revealed three simultaneously observed optical transitions resulting from (a) a direct exciton recombination, involving an electron and a hole states both located in the $\\varGamma$-valley in the quantum well layer, and (b) two spatially and momentum indirect excitons, comprising of the confined electron states in the $X$-valley in the AlAs barrier with different effective masses and quantum well holes in the $\\varGamma$-valley.","The measured spatial extent, density dependence and temperature dependence of the structure photoluminescence revealed characteristics necessary to pinpoint the states' nature and provided their characterization crucial in future device design.","Temporal dynamics observed in the time-resolved photoluminescence measurement showed the complexity of the capture and recombination dynamics, largely affected by nonradiative processes, what is additionally critical in the system's use.","This solid state platform hosting both direct and indirect excitons in a highly tunable monolithic system can benefit and underline the operation principles of novel electronic and photonic devices."],"url":"http://arxiv.org/abs/2404.01938v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 13:30:17","title":"Critical spin chains and loop models with $U(n)$ symmetry","abstract":"Starting with the Ising model, statistical models with global symmetries provide fruitful approaches to interesting physical systems, for example percolation or polymers. These include the $O(n)$ model (symmetry group $O(n)$) and the Potts model (symmetry group $S_Q$). Both models make sense for $n,Q\\in \\mathbb{C}$ and not just $n,Q\\in \\mathbb{N}$, and both give rise to a conformal field theory in the critical limit.   Here, we study similar models based on the unitary group $U(n)$. We focus on the two-dimensional case, where the models can be described either as gases of non-intersecting orientable loops, or as alternating spin chains. This allows us to determine their spectra either by computing a twisted torus partition function, or by studying representations of the walled Brauer algebra.   In the critical limit, our models give rise to a CFT with global $U(n)$ symmetry, which exists for any $n\\in\\mathbb{C}$. Its spectrum is similar to those of the $O(n)$ and Potts CFTs, but a bit simpler. We conjecture that the $O(n)$ CFT is a $\\mathbb{Z}_2$ orbifold of the $U(n)$ CFT, where $\\mathbb{Z}_2$ acts as complex conjugation.","sentences":["Starting with the Ising model, statistical models with global symmetries provide fruitful approaches to interesting physical systems, for example percolation or polymers.","These include the $O(n)$ model (symmetry group $O(n)$) and the Potts model (symmetry group $S_Q$).","Both models make sense for $n,Q\\in \\mathbb{C}$ and not just $n,Q\\in \\mathbb{N}$, and both give rise to a conformal field theory in the critical limit.   ","Here, we study similar models based on the unitary group $U(n)$. We focus on the two-dimensional case, where the models can be described either as gases of non-intersecting orientable loops, or as alternating spin chains.","This allows us to determine their spectra either by computing a twisted torus partition function, or by studying representations of the walled Brauer algebra.   ","In the critical limit, our models give rise to a CFT with global $U(n)$ symmetry, which exists for any $n\\in\\mathbb{C}$. Its spectrum is similar to those of the $O(n)$ and Potts CFTs, but a bit simpler.","We conjecture that the $O(n)$ CFT is a $\\mathbb{Z}_2$ orbifold of the $U(n)$ CFT, where $\\mathbb{Z}_2$ acts as complex conjugation."],"url":"http://arxiv.org/abs/2404.01935v1","category":"hep-th"}
{"created":"2024-04-02 13:23:21","title":"Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A Semi-Supervised Video Object Detection Method","abstract":"This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas. During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions. However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging. Previous research has lacked the application of object detection models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset. In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions. This study introduces a three-dimensional image-based object detection model. It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames. Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data. To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels.","sentences":["This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas.","During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions.","However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging.","Previous research has lacked the application of object detection models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset.","In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions.","This study introduces a three-dimensional image-based object detection model.","It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames.","Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data.","To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels."],"url":"http://arxiv.org/abs/2404.01929v1","category":"eess.IV"}
{"created":"2024-04-02 13:15:52","title":"A note on limiting Calderon-Zygmund theory for transformed $n$-Laplace systems in divergence form","abstract":"We consider rotated $n$-Laplace systems on the unit ball $B_1 \\subset \\mathbb{R}^n$ of the form   \\begin{align*}   -\\mathrm{div}\\left( Q|\\nabla u|^{n-2} \\nabla u\\right) = \\mathrm{div}(G),   \\end{align*}   where $u\\in W^{1,n}(B_1;\\mathbb{R}^N)$, $Q\\in W^{1,n}(B_1;SO(N))$ and $G\\in L^{\\left( \\frac{n}{n-1},q \\right)}(B_1;\\mathbb{R}^n\\otimes \\mathbb{R}^N)$ for some $0<q<\\frac{n}{n-1}$. We prove that $\\nabla u\\in L^{(n,q(n-1))}_{loc}$ with estimates. As a corollary, we obtain that solutions to $\\Delta_n u \\in \\mathcal{H}^1$, where $\\mathcal{H}^1$ is the Hardy space, have a higher integrability, namely $\\nabla u \\in L^{(n,n-1)}_{loc}$.","sentences":["We consider rotated $n$-Laplace systems on the unit ball $B_1 \\subset \\mathbb{R}^n$ of the form   \\begin{align*}   -\\mathrm{div}\\left( Q|\\nabla u|^{n-2} \\nabla u\\right) = \\mathrm{div}(G),   \\end{align*}   where $u\\in W^{1,n}(B_1;\\mathbb{R}^N)$, $Q\\in W^{1,n}(B_1;SO(N))$ and $G\\in L^{\\left( \\frac{n}{n-1},q \\right)}(B_1;\\mathbb{R}^n\\otimes \\mathbb{R}^N)$ for some $0<q<\\frac{n}{n-1}$. We prove that $\\nabla u\\in L^{(n,q(n-1))}_{loc}$ with estimates.","As a corollary, we obtain that solutions to $\\Delta_n u \\in \\mathcal{H}^1$, where $\\mathcal{H}^1$ is the Hardy space, have a higher integrability, namely $\\nabla u \\in L^{(n,n-1)}_{loc}$."],"url":"http://arxiv.org/abs/2404.01922v1","category":"math.AP"}
{"created":"2024-04-02 13:15:07","title":"A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution","abstract":"Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.","sentences":["Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents.","However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text.","We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task.","Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop.","This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation.","Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios."],"url":"http://arxiv.org/abs/2404.01921v1","category":"cs.CL"}
{"created":"2024-04-02 13:13:19","title":"Frustration-induced quantum criticality in Ni-doped CePdAl as revealed by the $\u03bc$SR technique","abstract":"In CePdAl, the 4$f$ moments of cerium arrange to form a geometrically frustrated kagome lattice. Due to frustration in addition to Kondo- and RKKY- interactions, this metallic system shows a long-range magnetic order (LRO) with a $T_{\\rm N}$ of only 2.7\\,K. Upon Ni doping at the Pd sites, $T_{\\rm N}$ is further suppressed, to reach zero at a critical concentration $x_c \\approx 0.15$. Here, by using muon-spin relaxation and rotation ($\\mu$SR), we investigate at a local level CePd$_{1-x}$Ni$_x$Al for five different Ni-concentrations, both above and below $x_c$. Similar to the parent CePdAl compound, for $x = 0.05$, we observe an incommensurate LRO, which turns into a quasi-static magnetic order for $x = 0.1$ and 0.14. More interestingly, away from $x_c$, for $x = 0.16$ and 0.18, we still observe a non-Fermi liquid regime, evidenced by a power-law divergence of the longitudinal relaxation at low temperatures. In this case, longitudinal field measurements exhibit a time-field scaling, indicative of a cooperative spin dynamics that persists for $x > x_c$. Furthermore, similar to the externally applied pressure, the chemical pressure induced by Ni doping suppresses the region below $T^*$, characterized by a spin-liquid like dynamical behavior. Our results suggest that the magnetic properties of CePdAl are similarly affected by the hydrostatic- and the chemical pressure. We also confirm that the unusual non-Fermi liquid regime (compared to conventional quantum critical systems) is due to the presence of frustration that persists up to the highest Ni concentrations.","sentences":["In CePdAl, the 4$f$ moments of cerium arrange to form a geometrically frustrated kagome lattice.","Due to frustration in addition to Kondo- and RKKY- interactions, this metallic system shows a long-range magnetic order (LRO) with a $T_{\\rm N}$ of only 2.7\\,K. Upon Ni doping at the Pd sites, $T_{\\rm N}$ is further suppressed, to reach zero at a critical concentration $x_c \\approx 0.15$.","Here, by using muon-spin relaxation and rotation ($\\mu$SR), we investigate at a local level CePd$_{1-x}$Ni$_x$Al for five different Ni-concentrations, both above and below $x_c$. Similar to the parent CePdAl","compound, for $x = 0.05$, we observe an incommensurate LRO, which turns into a quasi-static magnetic order for $x = 0.1$ and 0.14.","More interestingly, away from $x_c$, for $x = 0.16$ and 0.18, we still observe a non-Fermi liquid regime, evidenced by a power-law divergence of the longitudinal relaxation at low temperatures.","In this case, longitudinal field measurements exhibit a time-field scaling, indicative of a cooperative spin dynamics that persists for $x > x_c$.","Furthermore, similar to the externally applied pressure, the chemical pressure induced by Ni doping suppresses the region below $T^*$, characterized by a spin-liquid like dynamical behavior.","Our results suggest that the magnetic properties of CePdAl are similarly affected by the hydrostatic- and the chemical pressure.","We also confirm that the unusual non-Fermi liquid regime (compared to conventional quantum critical systems) is due to the presence of frustration that persists up to the highest Ni concentrations."],"url":"http://arxiv.org/abs/2404.01917v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 12:57:20","title":"Multicore DRAM Bank-& Row-Conflict Bomb for Timing Attacks in Mixed-Criticality Systems","abstract":"With the increasing use of multicore platforms to realize mixed-criticality systems, understanding the underlying shared resources, such as the memory hierarchy shared among cores, and achieving isolation between co-executing tasks running on the same platform with different criticality levels becomes relevant. In addition to safety considerations, a malicious entity can exploit shared resources to create timing attacks on critical applications. In this paper, we focus on understanding the shared DRAM dual in-line memory module and created a timing attack, that we named the \"bank & row conflict bomb\", to target a victim task in a multicore platform. We also created a \"navigate\" algorithm to understand how victim requests are managed by the Memory Controller and provide valuable inputs for designing the bank & row conflict bomb. We performed experimental tests on a 2nd Gen Intel Xeon Processor with an 8GB DDR4-2666 DRAM module to show that such an attack can produce a significant increase in the execution time of the victim task by about 150%, motivating the need for proper countermeasures to help ensure the safety and security of critical applications.","sentences":["With the increasing use of multicore platforms to realize mixed-criticality systems, understanding the underlying shared resources, such as the memory hierarchy shared among cores, and achieving isolation between co-executing tasks running on the same platform with different criticality levels becomes relevant.","In addition to safety considerations, a malicious entity can exploit shared resources to create timing attacks on critical applications.","In this paper, we focus on understanding the shared DRAM dual in-line memory module and created a timing attack, that we named the \"bank & row conflict bomb\", to target a victim task in a multicore platform.","We also created a \"navigate\" algorithm to understand how victim requests are managed by the Memory Controller and provide valuable inputs for designing the bank & row conflict bomb.","We performed experimental tests on a 2nd Gen Intel Xeon Processor with an 8GB DDR4-2666 DRAM module to show that such an attack can produce a significant increase in the execution time of the victim task by about 150%, motivating the need for proper countermeasures to help ensure the safety and security of critical applications."],"url":"http://arxiv.org/abs/2404.01910v1","category":"cs.CR"}
{"created":"2024-04-02 12:40:31","title":"Efficient estimation for a smoothing thin plate spline in a two-dimensional space","abstract":"Using a deterministic framework allows us to estimate a function with the purpose of interpolating data in spatial statistics. Radial basis functions are commonly used for scattered data interpolation in a d-dimensional space, however, interpolation problems have to deal with dense matrices. For the case of smoothing thin plate splines, we propose an efficient way to address this problem by compressing the dense matrix by an hierarchical matrix ($\\mathcal{H}$-matrix) and using the conjugate gradient method to solve the linear system of equations. A simulation study was conducted to assess the effectiveness of the spatial interpolation method. The results indicated that employing an $\\mathcal{H}$-matrix along with the conjugate gradient method allows for efficient computations while maintaining a minimal error. We also provide a sensitivity analysis that covers a range of smoothing and compression parameter values, along with a Monte Carlo simulation aimed at quantifying uncertainty in the approximated function. Lastly, we present a comparative study between the proposed approach and thin plate regression using the \"mgcv\" package of the statistical software R. The comparison results demonstrate similar interpolation performance between the two methods.","sentences":["Using a deterministic framework allows us to estimate a function with the purpose of interpolating data in spatial statistics.","Radial basis functions are commonly used for scattered data interpolation in a d-dimensional space, however, interpolation problems have to deal with dense matrices.","For the case of smoothing thin plate splines, we propose an efficient way to address this problem by compressing the dense matrix by an hierarchical matrix ($\\mathcal{H}$-matrix) and using the conjugate gradient method to solve the linear system of equations.","A simulation study was conducted to assess the effectiveness of the spatial interpolation method.","The results indicated that employing an $\\mathcal{H}$-matrix along with the conjugate gradient method allows for efficient computations while maintaining a minimal error.","We also provide a sensitivity analysis that covers a range of smoothing and compression parameter values, along with a Monte Carlo simulation aimed at quantifying uncertainty in the approximated function.","Lastly, we present a comparative study between the proposed approach and thin plate regression using the \"mgcv\" package of the statistical software R. The comparison results demonstrate similar interpolation performance between the two methods."],"url":"http://arxiv.org/abs/2404.01902v1","category":"stat.CO"}
{"created":"2024-04-02 12:35:48","title":"Damage Location in Mechanical Structures by Multi-Objective Pattern Search","abstract":"We propose a multi-objective global pattern search algorithm for the task of locating and quantifying damage in flexible mechanical structures. This is achieved by identifying eigenfrequencies and eigenmodes from measurements and matching them against the results of a finite element simulation model, which leads to a nonsmooth nonlinear bi-objective parameter estimation problem. A derivative-free optimization algorithm is required since the problem is nonsmooth and also because complex mechanical simulation models are often solved using commercial black-box software. Moreover, the entire set of non-dominated solutions is of interest to practitioners. Most solution approaches published to date are based on meta-heuristics such as genetic algorithms. The proposed multi-objective pattern-search algorithm provides a mathematically well-founded alternative. It features a novel sorting procedure that reduces the complexity in our context. Test runs on two experimental structures with multiple damage scenarios are used to validate the approach. The results demonstrate that the proposed algorithm yields accurate damage locations and requires moderate computational resources. From the engineer's perspective it represents a promising tool for structural health monitoring.","sentences":["We propose a multi-objective global pattern search algorithm for the task of locating and quantifying damage in flexible mechanical structures.","This is achieved by identifying eigenfrequencies and eigenmodes from measurements and matching them against the results of a finite element simulation model, which leads to a nonsmooth nonlinear bi-objective parameter estimation problem.","A derivative-free optimization algorithm is required since the problem is nonsmooth and also because complex mechanical simulation models are often solved using commercial black-box software.","Moreover, the entire set of non-dominated solutions is of interest to practitioners.","Most solution approaches published to date are based on meta-heuristics such as genetic algorithms.","The proposed multi-objective pattern-search algorithm provides a mathematically well-founded alternative.","It features a novel sorting procedure that reduces the complexity in our context.","Test runs on two experimental structures with multiple damage scenarios are used to validate the approach.","The results demonstrate that the proposed algorithm yields accurate damage locations and requires moderate computational resources.","From the engineer's perspective it represents a promising tool for structural health monitoring."],"url":"http://arxiv.org/abs/2404.01896v1","category":"math.OC"}
{"created":"2024-04-02 12:26:04","title":"Can My Microservice Tolerate an Unreliable Database? Resilience Testing with Fault Injection and Visualization","abstract":"In microservice applications, ensuring resilience during database or service disruptions constitutes a significant challenge. While several tools address resilience testing for service failures, there is a notable gap in tools specifically designed for resilience testing of database failures. To bridge this gap, we have developed an extension for fault injection in database clients, which we integrated into Filibuster, an existing tool for fault injection in services within microservice applications. Our tool systematically simulates database disruptions, thereby enabling comprehensive testing and evaluation of application resilience. It is versatile, supporting a range of both SQL and NoSQL database systems, such as Redis, Apache Cassandra, CockroachDB, PostgreSQL, and DynamoDB. A defining feature is its integration during the development phase, complemented by an IntelliJ IDE plugin, which offers developers visual feedback on the types, locations, and impacts of injected faults. A video demonstration of the tool's capabilities is accessible at https://youtu.be/bvaUVCy1m1s.","sentences":["In microservice applications, ensuring resilience during database or service disruptions constitutes a significant challenge.","While several tools address resilience testing for service failures, there is a notable gap in tools specifically designed for resilience testing of database failures.","To bridge this gap, we have developed an extension for fault injection in database clients, which we integrated into Filibuster, an existing tool for fault injection in services within microservice applications.","Our tool systematically simulates database disruptions, thereby enabling comprehensive testing and evaluation of application resilience.","It is versatile, supporting a range of both SQL and NoSQL database systems, such as Redis, Apache Cassandra, CockroachDB, PostgreSQL, and DynamoDB.","A defining feature is its integration during the development phase, complemented by an IntelliJ IDE plugin, which offers developers visual feedback on the types, locations, and impacts of injected faults.","A video demonstration of the tool's capabilities is accessible at https://youtu.be/bvaUVCy1m1s."],"url":"http://arxiv.org/abs/2404.01886v1","category":"cs.SE"}
{"created":"2024-04-02 12:25:16","title":"Enhancing Robot Navigation Efficiency Using Cellular Automata with Active Cells","abstract":"Enhancing robot navigation efficiency is a crucial objective in modern robotics. Robots relying on external navigation systems are often susceptible to electromagnetic interference (EMI) and encounter environmental disturbances, resulting in orientation errors within their surroundings. Therefore, the study employed an internal navigation system to enhance robot navigation efficacy under interference conditions, based on the analysis of the internal parameters and the external signals. This article presents details of the robot's autonomous operation, which allows for setting the robot's trajectory using an embedded map. The robot's navigation process involves counting the number of wheel revolutions as well as adjusting wheel orientation after each straight path section. In this article, an autonomous robot navigation system has been presented that leverages an embedded control navigation map utilising cellular automata with active cells which can effectively navigate in an environment containing various types of obstacles. By analysing the neighbouring cells of the active cell, the cellular environment determines which cell should become active during the robot's next movement step. This approach ensures the robot's independence from external control inputs. Furthermore, the accuracy and speed of the robot's movement have been further enhanced using a hexagonal mosaic for navigation surface mapping. This concept of utilising on cellular automata with active cells has been extended to the navigation of a group of robots on a shared navigation surface, taking into account the intersections of the robots' trajectories over time. To achieve this, a distance control module has been used that records the travelled trajectories in terms of wheel turns and revolutions.","sentences":["Enhancing robot navigation efficiency is a crucial objective in modern robotics.","Robots relying on external navigation systems are often susceptible to electromagnetic interference (EMI) and encounter environmental disturbances, resulting in orientation errors within their surroundings.","Therefore, the study employed an internal navigation system to enhance robot navigation efficacy under interference conditions, based on the analysis of the internal parameters and the external signals.","This article presents details of the robot's autonomous operation, which allows for setting the robot's trajectory using an embedded map.","The robot's navigation process involves counting the number of wheel revolutions as well as adjusting wheel orientation after each straight path section.","In this article, an autonomous robot navigation system has been presented that leverages an embedded control navigation map utilising cellular automata with active cells which can effectively navigate in an environment containing various types of obstacles.","By analysing the neighbouring cells of the active cell, the cellular environment determines which cell should become active during the robot's next movement step.","This approach ensures the robot's independence from external control inputs.","Furthermore, the accuracy and speed of the robot's movement have been further enhanced using a hexagonal mosaic for navigation surface mapping.","This concept of utilising on cellular automata with active cells has been extended to the navigation of a group of robots on a shared navigation surface, taking into account the intersections of the robots' trajectories over time.","To achieve this, a distance control module has been used that records the travelled trajectories in terms of wheel turns and revolutions."],"url":"http://arxiv.org/abs/2404.01885v1","category":"cs.RO"}
{"created":"2024-04-02 12:15:25","title":"Scene Adaptive Sparse Transformer for Event-based Object Detection","abstract":"While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST","sentences":["While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras.","Image-based works attempt to reduce these costs by introducing sparse Transformers.","However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency.","Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate.","To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST).","SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead.","Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost.","The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1).","Code: https://github.com/Peterande/SAST"],"url":"http://arxiv.org/abs/2404.01882v1","category":"cs.CV"}
{"created":"2024-04-02 12:00:08","title":"A nonperturbative test of nucleation calculations for strong phase transitions","abstract":"Nucleation rate computations are of broad importance in particle physics and cosmology. Perturbative calculations are often used to compute the nucleation rate $\\Gamma$, but these are incomplete. We perform nonperturbative lattice simulations of nucleation in a scalar field theory with a tree-level barrier, computing a final result extrapolated to the thermodynamic and continuum limits. Although the system in question should be well-described by a complete one-loop perturbative calculation, we find only qualitative agreement with the full perturbative result, with a 20% discrepancy in $|\\log \\Gamma|$. Our result motivates further testing of the current nucleation paradigm.","sentences":["Nucleation rate computations are of broad importance in particle physics and cosmology.","Perturbative calculations are often used to compute the nucleation rate $\\Gamma$, but these are incomplete.","We perform nonperturbative lattice simulations of nucleation in a scalar field theory with a tree-level barrier, computing a final result extrapolated to the thermodynamic and continuum limits.","Although the system in question should be well-described by a complete one-loop perturbative calculation, we find only qualitative agreement with the full perturbative result, with a 20% discrepancy in $|\\log \\Gamma|$. Our result motivates further testing of the current nucleation paradigm."],"url":"http://arxiv.org/abs/2404.01876v1","category":"hep-th"}
{"created":"2024-04-02 11:55:50","title":"Fast and Adaptive Questionnaires for Voting Advice Applications","abstract":"The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version.","sentences":["The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires.","To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire.","However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%.","To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters.","Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations.","Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions.","We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy.","Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version."],"url":"http://arxiv.org/abs/2404.01872v1","category":"cs.LG"}
{"created":"2024-04-02 11:50:03","title":"On the reduction of Linear Parameter-Varying State-Space models","abstract":"This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance. The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions. For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection. For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network. The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model.","sentences":["This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance.","The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions.","For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection.","For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network.","The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model."],"url":"http://arxiv.org/abs/2404.01871v1","category":"eess.SY"}
{"created":"2024-04-02 11:49:11","title":"Multidimensional deconvolution with shared bases","abstract":"We address the estimation of seismic wavefields by means of Multidimensional Deconvolution (MDD) for various redatuming applications. While offering more accuracy than conventional correlation-based redatuming methods, MDD faces challenges due to the ill-posed nature of the underlying inverse problem and the requirement to handle large, dense, complex-valued matrices. These obstacles have long limited the adoption of MDD in the geophysical community. Recent interest in this technology has spurred the development of new strategies to enhance the robustness of the inversion process and reduce its computational overhead. We present a novel approach that extends the concept of block low-rank approximations, usually applied to linear operators, to simultaneously compress the operator, right-hand side, and unknowns. This technique greatly alleviates the data-heavy nature of MDD. Moreover, since in 3d applications the matrices do not lend themselves to global low rank approximations, we introduce a novel H2-like approximation. We aim to streamline MDD implementations, fostering efficiency and controlling accuracy in wavefield reconstruction. This innovation holds potential for broader applications in the geophysical domain, possibly revolutionizing the analysis of multi-dimensional seismic datasets.","sentences":["We address the estimation of seismic wavefields by means of Multidimensional Deconvolution (MDD) for various redatuming applications.","While offering more accuracy than conventional correlation-based redatuming methods, MDD faces challenges due to the ill-posed nature of the underlying inverse problem and the requirement to handle large, dense, complex-valued matrices.","These obstacles have long limited the adoption of MDD in the geophysical community.","Recent interest in this technology has spurred the development of new strategies to enhance the robustness of the inversion process and reduce its computational overhead.","We present a novel approach that extends the concept of block low-rank approximations, usually applied to linear operators, to simultaneously compress the operator, right-hand side, and unknowns.","This technique greatly alleviates the data-heavy nature of MDD.","Moreover, since in 3d applications the matrices do not lend themselves to global low rank approximations, we introduce a novel H2-like approximation.","We aim to streamline MDD implementations, fostering efficiency and controlling accuracy in wavefield reconstruction.","This innovation holds potential for broader applications in the geophysical domain, possibly revolutionizing the analysis of multi-dimensional seismic datasets."],"url":"http://arxiv.org/abs/2404.01870v1","category":"math.NA"}
{"created":"2024-04-02 11:45:16","title":"Integrated ultrafast all-optical polariton transistors","abstract":"The clock speed of electronic circuits has been stagnant at a few gigahertz for almost two decades because of the breakdown of Dennard scaling, which states that by shrinking the size of transistors they can operate faster while maintaining the same power consumption. Optical computing could overcome this roadblock, but the lack of materials with suitably strong nonlinear interactions needed to realize all-optical switches has, so far, precluded the fabrication of scalable architectures. Recently, microcavities in the strong light-matter interaction regime enabled all-optical transistors which, when used with an embedded organic material, can operate even at room temperature with sub-picosecond switching times, down to the single-photon level. However, the vertical cavity geometry prevents complex circuits with on-chip coupled transistors. Here, by leveraging silicon photonics technology, we show exciton-polariton condensation at ambient conditions in micrometer-sized, fully integrated high-index contrast grating microcavities filled with an optically active polymer. By coupling two resonators and exploiting seeded polariton condensation, we demonstrate ultrafast all-optical transistor action and cascadability. Our experimental findings open the way for scalable, compact all-optical integrated logic circuits that could process optical signals two orders of magnitude faster than their electrical counterparts.","sentences":["The clock speed of electronic circuits has been stagnant at a few gigahertz for almost two decades because of the breakdown of Dennard scaling, which states that by shrinking the size of transistors they can operate faster while maintaining the same power consumption.","Optical computing could overcome this roadblock, but the lack of materials with suitably strong nonlinear interactions needed to realize all-optical switches has, so far, precluded the fabrication of scalable architectures.","Recently, microcavities in the strong light-matter interaction regime enabled all-optical transistors which, when used with an embedded organic material, can operate even at room temperature with sub-picosecond switching times, down to the single-photon level.","However, the vertical cavity geometry prevents complex circuits with on-chip coupled transistors.","Here, by leveraging silicon photonics technology, we show exciton-polariton condensation at ambient conditions in micrometer-sized, fully integrated high-index contrast grating microcavities filled with an optically active polymer.","By coupling two resonators and exploiting seeded polariton condensation, we demonstrate ultrafast all-optical transistor action and cascadability.","Our experimental findings open the way for scalable, compact all-optical integrated logic circuits that could process optical signals two orders of magnitude faster than their electrical counterparts."],"url":"http://arxiv.org/abs/2404.01868v1","category":"physics.optics"}
{"created":"2024-04-02 11:44:37","title":"Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation","abstract":"Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL). Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment. Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model. Therefore, the quality of the model is fundamental for the performance of the posterior tasks. In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering. We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration. With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward. In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup. Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps. Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks.","sentences":["Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL).","Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment.","Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model.","Therefore, the quality of the model is fundamental for the performance of the posterior tasks.","In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering.","We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration.","With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward.","In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup.","Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps.","Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks."],"url":"http://arxiv.org/abs/2404.01867v1","category":"cs.RO"}
{"created":"2024-04-02 11:38:18","title":"Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices","abstract":"RISC-V cores have gained a lot of popularity over the last few years. However, being quite a recent and novel technology, there is still a gap in the availability of comprehensive simulation frameworks for RISC-V that cover both the functional and extra-functional aspects. This gap hinders progress in the field, as fast yet accurate system-level simulation is crucial for Design Space Exploration (DSE).   This work presents an open-source framework designed to tackle this challenge, integrating functional RISC-V simulation (achieved with GVSoC) with SystemC-AMS (used to model extra-functional aspects, in detail power storage and distribution). The combination of GVSoC and SystemC-AMS in a single simulation framework allows to perform a DSE that is dependent on the mutual impact between functional and extra-functional aspects. In our experiments, we validate the framework's effectiveness by creating a virtual prototype of a compact, battery-powered embedded system.","sentences":["RISC-V cores have gained a lot of popularity over the last few years.","However, being quite a recent and novel technology, there is still a gap in the availability of comprehensive simulation frameworks for RISC-V that cover both the functional and extra-functional aspects.","This gap hinders progress in the field, as fast yet accurate system-level simulation is crucial for Design Space Exploration (DSE).   ","This work presents an open-source framework designed to tackle this challenge, integrating functional RISC-V simulation (achieved with GVSoC) with SystemC-AMS (used to model extra-functional aspects, in detail power storage and distribution).","The combination of GVSoC and SystemC-AMS in a single simulation framework allows to perform a DSE that is dependent on the mutual impact between functional and extra-functional aspects.","In our experiments, we validate the framework's effectiveness by creating a virtual prototype of a compact, battery-powered embedded system."],"url":"http://arxiv.org/abs/2404.01861v1","category":"eess.SY"}
{"created":"2024-04-02 11:38:11","title":"Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less","abstract":"This paper presents two simple improvements to the Self-Structuring AutoEncoder (Self-StrAE). Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality. Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters. Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven. Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans.","sentences":["This paper presents two simple improvements to the Self-Structuring AutoEncoder (Self-StrAE).","Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality.","Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters.","Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven.","Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans."],"url":"http://arxiv.org/abs/2404.01860v1","category":"cs.CL"}
{"created":"2024-04-02 11:37:40","title":"New polarimetric study of the galactic X-ray burster GX 13+1","abstract":"Weakly magnetized neutron stars (WMNS) are complicated sources with challenging phenomenology. For decades, they have been studied via spectrometry and timing. It has been established that the spectrum of WMNSs consists of several components traditionally associated with the accretion disk, the boundary or spreading layer, and the wind and their interactions with each other. Since 2022, WMNSs have been actively observed with the Imaging X-ray Polarimetry Explorer (IXPE). Polarimetric studies provided new information about the behavior and geometry of these sources. One of the most enigmatic sources of the class, galactic X-ray burster GX 13+1 was first observed with IXPE in October 2023. A strongly variable polarization at the level 2-5$\\%$ was detected with the source showing a rotation of the polarization angle (PA) that hinted towards the misalignment within the system. The second observation was performed in February 2024 with a complementary observation by Swift/XRT. IXPE measured an overall polarization degree (PD) of 2.5$\\%$ and the PA of 24 degrees, and the Swift/XRT data helped us evaluate the galactic absorption and fit the continuum. Here we study the similarities and differences between the polarimetric properties of the source during the two observations. We confirm the expectation of the misalignment in the system and the assignment of the harder component to the boundary layer. We emphasize the importance of the wind in the system. We note the difference in the variation of polarimetric properties with energy and with time.","sentences":["Weakly magnetized neutron stars (WMNS) are complicated sources with challenging phenomenology.","For decades, they have been studied via spectrometry and timing.","It has been established that the spectrum of WMNSs consists of several components traditionally associated with the accretion disk, the boundary or spreading layer, and the wind and their interactions with each other.","Since 2022, WMNSs have been actively observed with the Imaging X-ray Polarimetry Explorer (IXPE).","Polarimetric studies provided new information about the behavior and geometry of these sources.","One of the most enigmatic sources of the class, galactic X-ray burster GX 13+1 was first observed with IXPE in October 2023.","A strongly variable polarization at the level 2-5$\\%$ was detected with the source showing a rotation of the polarization angle (PA) that hinted towards the misalignment within the system.","The second observation was performed in February 2024 with a complementary observation by Swift/XRT.","IXPE measured an overall polarization degree (PD) of 2.5$\\%$ and the PA of 24 degrees, and the Swift/XRT data helped us evaluate the galactic absorption and fit the continuum.","Here we study the similarities and differences between the polarimetric properties of the source during the two observations.","We confirm the expectation of the misalignment in the system and the assignment of the harder component to the boundary layer.","We emphasize the importance of the wind in the system.","We note the difference in the variation of polarimetric properties with energy and with time."],"url":"http://arxiv.org/abs/2404.01859v1","category":"astro-ph.HE"}
{"created":"2024-04-02 11:36:58","title":"Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements","abstract":"One of the benefits of using executable specifications such as Behavioral Programming (BP) is the ability to align the system implementation with its requirements. This is facilitated in BP by a protocol that allows independent implementation modules that specify what the system may, must, and must not do. By that, each module can enforce a single system requirement, including negative specifications such as \"don't do X after Y.\" The existing BP protocol, however, allows only the enforcement of safety requirements and does not support the execution of liveness properties such as \"do X at least three times.\" To model liveness requirements in BP directly and independently, we propose idioms for tagging states with \"must-finish,\" indicating that tasks are yet to be completed. We show that this idiom allows a direct specification of known requirements patterns from the literature. We also offer semantics and two execution mechanisms, one based on a translation to B\\\"uchi automata and the other based on a Markov decision process (MDP). The latter approach offers the possibility of utilizing deep reinforcement learning (DRL) algorithms, which bear the potential to handle large software systems effectively. This paper presents a qualitative and quantitative assessment of the proposed approach using a proof-of-concept tool. A formal analysis of the MDP-based execution mechanism is given in an appendix.","sentences":["One of the benefits of using executable specifications such as Behavioral Programming (BP) is the ability to align the system implementation with its requirements.","This is facilitated in BP by a protocol that allows independent implementation modules that specify what the system may, must, and must not do.","By that, each module can enforce a single system requirement, including negative specifications such as \"don't do X after Y.\" The existing BP protocol, however, allows only the enforcement of safety requirements and does not support the execution of liveness properties such as \"do X at least three times.\"","To model liveness requirements in BP directly and independently, we propose idioms for tagging states with \"must-finish,\" indicating that tasks are yet to be completed.","We show that this idiom allows a direct specification of known requirements patterns from the literature.","We also offer semantics and two execution mechanisms, one based on a translation to B\\\"uchi automata and the other based on a Markov decision process (MDP).","The latter approach offers the possibility of utilizing deep reinforcement learning (DRL) algorithms, which bear the potential to handle large software systems effectively.","This paper presents a qualitative and quantitative assessment of the proposed approach using a proof-of-concept tool.","A formal analysis of the MDP-based execution mechanism is given in an appendix."],"url":"http://arxiv.org/abs/2404.01858v1","category":"cs.SE"}
{"created":"2024-04-02 11:28:28","title":"Identification and characterization of three-dimensional crack propagation mechanism in the Aluminium alloy AA2024-T3 using high-resolution Digital Image Correlation","abstract":"Fatigue crack growth is usually a three-dimensional problem, but it is often simplified to two dimensions to reduce complexity. However, this study investigates the relationships between microscopic effects such as crack kinking, shear lips, and plasticity that are present in reality. Therefore, crack propagation tests were carried out on 2-mm-thick MT-160 specimens of AA2024-T3 sheet material in L-T and T-L orientation. Using high-resolution digital image correlation (DIC), the plastic zone was identified and measured on the samples surface. The fracture surfaces were then digitized and their 3D shape characterized. Finite element simulations confirm the presence of a local mixed-mode I/II/III state along the crack front for a slant or double shear-fracture type. A derived mapping function enables the determination of the fracture type from the surface plastic zone, along with the current crack tip loading during the test. Finally, a transition of the fracture type also leads to a short-term delay in the crack propagation rate. Based on this information crack propagation curves are computed with regards to the local 3D crack orientation.","sentences":["Fatigue crack growth is usually a three-dimensional problem, but it is often simplified to two dimensions to reduce complexity.","However, this study investigates the relationships between microscopic effects such as crack kinking, shear lips, and plasticity that are present in reality.","Therefore, crack propagation tests were carried out on 2-mm-thick MT-160 specimens of AA2024-T3 sheet material in L-T and T-L orientation.","Using high-resolution digital image correlation (DIC), the plastic zone was identified and measured on the samples surface.","The fracture surfaces were then digitized and their 3D shape characterized.","Finite element simulations confirm the presence of a local mixed-mode I/II/III state along the crack front for a slant or double shear-fracture type.","A derived mapping function enables the determination of the fracture type from the surface plastic zone, along with the current crack tip loading during the test.","Finally, a transition of the fracture type also leads to a short-term delay in the crack propagation rate.","Based on this information crack propagation curves are computed with regards to the local 3D crack orientation."],"url":"http://arxiv.org/abs/2404.01852v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 10:36:01","title":"Stability and Bifurcation Analysis of Two-Term Fractional Differential Equation with Delay","abstract":"This manuscript deals with the stability and bifurcation analysis of the equation $D^{2\\alpha}x(t)+c D^{\\alpha}x(t)=a x(t)+b x(t-\\tau)$, where $0<\\alpha<1$ and $\\tau>0$. We sketch the boundaries of various stability regions in the parameter plane under different conditions on $\\alpha$ and $b$. First, we provide the stability analysis of this equation with $\\tau=0$. Change in the stability of the delayed counterpart is possible only when the characteristic roots cross the imaginary axis. This leads to various delay-independent as well as delay-dependent stability results. The stability regions are bifurcated on the basis of the following behaviors with respect to the delay $\\tau$ viz. stable region for all $\\tau>0$, unstable region, single stable region, stability switch, and instability switch.","sentences":["This manuscript deals with the stability and bifurcation analysis of the equation $D^{2\\alpha}x(t)+c D^{\\alpha}x(t)=a x(t)+b x(t-\\tau)$, where $0<\\alpha<1$ and $\\tau>0$. We sketch the boundaries of various stability regions in the parameter plane under different conditions on $\\alpha$ and $b$. First, we provide the stability analysis of this equation with $\\tau=0$. Change in the stability of the delayed counterpart is possible only when the characteristic roots cross the imaginary axis.","This leads to various delay-independent as well as delay-dependent stability results.","The stability regions are bifurcated on the basis of the following behaviors with respect to the delay $\\tau$ viz.","stable region for all $\\tau>0$, unstable region, single stable region, stability switch, and instability switch."],"url":"http://arxiv.org/abs/2404.01824v1","category":"math.DS"}
{"created":"2024-04-02 10:24:11","title":"Model Predictive Control of District Heating Grids Using Stabilizing Terminal Ingredients","abstract":"The transformation of fossil fuel-based district heating grids (DHGs) to CO$_2$-neutral DHGs requires the development of novel operating strategies. Model predictive control (MPC) is a promising approach, as knowledge about future heat demand and heat supply can be incorporated into the control, operating constraints can be ensured and the stability of the closed-loop system can be guaranteed. In this paper, we employ MPC for DHGs to control the system mass flows and injected heat flows. Following common practice, we derive terminal ingredients to stabilize given steady state temperatures and storage masses in the DHG. To apply MPC with terminal ingredients, it is crucial that the system under control is stabilizable. By exploiting the particular system structure, we give a sufficient condition for the stabilizability in terms of the grid topology and hence, for the applicability of the MPC scheme to DHGs. Furthermore, we demonstrate the practicability of the application of MPC to an exemplary DHG in a numerical case study.","sentences":["The transformation of fossil fuel-based district heating grids (DHGs) to CO$_2$-neutral DHGs requires the development of novel operating strategies.","Model predictive control (MPC) is a promising approach, as knowledge about future heat demand and heat supply can be incorporated into the control, operating constraints can be ensured and the stability of the closed-loop system can be guaranteed.","In this paper, we employ MPC for DHGs to control the system mass flows and injected heat flows.","Following common practice, we derive terminal ingredients to stabilize given steady state temperatures and storage masses in the DHG.","To apply MPC with terminal ingredients, it is crucial that the system under control is stabilizable.","By exploiting the particular system structure, we give a sufficient condition for the stabilizability in terms of the grid topology and hence, for the applicability of the MPC scheme to DHGs.","Furthermore, we demonstrate the practicability of the application of MPC to an exemplary DHG in a numerical case study."],"url":"http://arxiv.org/abs/2404.01820v1","category":"eess.SY"}
{"created":"2024-04-02 10:19:04","title":"Neuromorphic Split Computing with Wake-Up Radios: Architecture and Design via Digital Twinning","abstract":"Neuromorphic computing leverages the sparsity of temporal data to reduce processing energy by activating a small subset of neurons and synapses at each time step. When deployed for split computing in edge-based systems, remote neuromorphic processing units (NPUs) can reduce the communication power budget by communicating asynchronously using sparse impulse radio (IR) waveforms. This way, the input signal sparsity translates directly into energy savings both in terms of computation and communication. However, with IR transmission, the main contributor to the overall energy consumption remains the power required to maintain the main radio on. This work proposes a novel architecture that integrates a wake-up radio mechanism within a split computing system consisting of remote, wirelessly connected, NPUs. A key challenge in the design of a wake-up radio-based neuromorphic split computing system is the selection of thresholds for sensing, wake-up signal detection, and decision making. To address this problem, as a second contribution, this work proposes a novel methodology that leverages the use of a digital twin (DT), i.e., a simulator, of the physical system, coupled with a sequential statistical testing approach known as Learn Then Test (LTT) to provide theoretical reliability guarantees. The proposed DT-LTT methodology is broadly applicable to other design problems, and is showcased here for neuromorphic communications. Experimental results validate the design and the analysis, confirming the theoretical reliability guarantees and illustrating trade-offs among reliability, energy consumption, and informativeness of the decisions.","sentences":["Neuromorphic computing leverages the sparsity of temporal data to reduce processing energy by activating a small subset of neurons and synapses at each time step.","When deployed for split computing in edge-based systems, remote neuromorphic processing units (NPUs) can reduce the communication power budget by communicating asynchronously using sparse impulse radio (IR) waveforms.","This way, the input signal sparsity translates directly into energy savings both in terms of computation and communication.","However, with IR transmission, the main contributor to the overall energy consumption remains the power required to maintain the main radio on.","This work proposes a novel architecture that integrates a wake-up radio mechanism within a split computing system consisting of remote, wirelessly connected, NPUs.","A key challenge in the design of a wake-up radio-based neuromorphic split computing system is the selection of thresholds for sensing, wake-up signal detection, and decision making.","To address this problem, as a second contribution, this work proposes a novel methodology that leverages the use of a digital twin (DT), i.e., a simulator, of the physical system, coupled with a sequential statistical testing approach known as Learn Then Test (LTT) to provide theoretical reliability guarantees.","The proposed DT-LTT methodology is broadly applicable to other design problems, and is showcased here for neuromorphic communications.","Experimental results validate the design and the analysis, confirming the theoretical reliability guarantees and illustrating trade-offs among reliability, energy consumption, and informativeness of the decisions."],"url":"http://arxiv.org/abs/2404.01815v1","category":"eess.SP"}
{"created":"2024-04-02 10:15:18","title":"Superhard lon C5 and derived carbon nitrides: C4N and C2N2. Crystal chemistry and first principles DFT studies","abstract":"Super-hard C5 with lon topology (lon: Lonsdaleite hexagonal diamond) and characterized by the presence of sp3 and sp2 -like carbon sites is devised from crystal chemistry and used as template matrix structure for identifying original carbonitrides C4N and C2N2 with lon topology except for the equiatomic belonging to a new topology (3,4L147). The steric effect of N(2s2) lone pair is highlighted in C2N2 in inducing an original structure of largely separated two-layered stacking of tetrahedra. The investigations based on crystal chemistry were backed by computations within the quantum density functional theory DFT. All systems were found cohesive and both mechanically (elastic constants) and dynamically (phonons band structures) stable. Super hardness characterizes the carbon allotrope C5 and the nitrides C4N and C2N2. Metallic-like conductivities and insulating characters were identified.","sentences":["Super-hard C5 with lon topology (lon: Lonsdaleite hexagonal diamond) and characterized by the presence of sp3 and sp2 -like carbon sites is devised from crystal chemistry and used as template matrix structure for identifying original carbonitrides C4N and C2N2 with lon topology except for the equiatomic belonging to a new topology (3,4L147).","The steric effect of N(2s2) lone pair is highlighted in C2N2 in inducing an original structure of largely separated two-layered stacking of tetrahedra.","The investigations based on crystal chemistry were backed by computations within the quantum density functional theory DFT.","All systems were found cohesive and both mechanically (elastic constants) and dynamically (phonons band structures) stable.","Super hardness characterizes the carbon allotrope C5 and the nitrides C4N and C2N2.","Metallic-like conductivities and insulating characters were identified."],"url":"http://arxiv.org/abs/2404.01813v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 10:13:38","title":"Multiple scattering suppression for in vivo optical coherence tomography measurement using B-scan-wise multi-focus averaging method","abstract":"We demonstrate a method that reduces the noise caused by multi-scattering (MS) photons in an \\invivo optical coherence tomography image. This method combines a specially designed image acquisition (i.e., optical coherence tomography scan) scheme and subsequent complex signal processing. For the acquisition, multiple cross-sectional images (frames) are sequentially acquired while the depth position of the focus is altered for each frame by an electrically tunable lens. In the signal processing, the frames are numerically defocus-corrected, and complex averaged. Because of the inconsistency in the MS-photon trajectories among the different electrically tunable lens-induced defocus, this averaging reduces the MS signal. This method was validated using a scattering phantom and in vivo unanesthetized small fish samples, and was found to reduce MS noise even for unanesthetized in vivo measurement.","sentences":["We demonstrate a method that reduces the noise caused by multi-scattering (MS) photons in an \\invivo optical coherence tomography image.","This method combines a specially designed image acquisition (i.e., optical coherence tomography scan) scheme and subsequent complex signal processing.","For the acquisition, multiple cross-sectional images (frames) are sequentially acquired while the depth position of the focus is altered for each frame by an electrically tunable lens.","In the signal processing, the frames are numerically defocus-corrected, and complex averaged.","Because of the inconsistency in the MS-photon trajectories among the different electrically tunable lens-induced defocus, this averaging reduces the MS signal.","This method was validated using a scattering phantom and in vivo unanesthetized small fish samples, and was found to reduce MS noise even for unanesthetized in vivo measurement."],"url":"http://arxiv.org/abs/2404.01811v1","category":"physics.optics"}
{"created":"2024-04-02 10:06:30","title":"Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification","abstract":"Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems. This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions. Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes. We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels. Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales. The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification.","sentences":["Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems.","This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions.","Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance.","We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes.","We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels.","Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales.","The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification."],"url":"http://arxiv.org/abs/2404.01805v1","category":"cs.LG"}
{"created":"2024-04-02 10:05:47","title":"Systematic Solutions to Login and Authentication Security: A Dual-Password Login-Authentication Mechanism","abstract":"Credential theft and remote attacks are the most serious threats to authentication mechanisms. The crux of the problems is that we cannot control such behaviors. However, if a password does not contain user's secrets, stealing it is useless. If unauthorized inputs are disabled, the remote attacks can be invalidated. Thereby, credential secrets and input fields to our accounts can be controlled. Rather than encrypting passwords, we design a dual-password login-authentication mechanism, where a user-selected secret-free login password is converted into an untypable authentication password. Subsequently, the authenticatable functionality of the login password and the typable functionality of the authentication password may be disabled or invalidated so that the credential theft and remote attacks can be prevented. Thus, the usability-security trade-off and password reuse are resolved; local storage of authentication passwords is no longer necessary. More importantly, the password converter acts as an open hash algorithm, meaning that its intermediate elements can be used to define a truly unique identity of the login process to implement a novel dual-identity authentication. Particularly, the elements are concealed, inaccessible, and independent of any personal information, and therefore can be used to define a perfect unforgeable process identifier to identify and disable the unauthorized inputs.","sentences":["Credential theft and remote attacks are the most serious threats to authentication mechanisms.","The crux of the problems is that we cannot control such behaviors.","However, if a password does not contain user's secrets, stealing it is useless.","If unauthorized inputs are disabled, the remote attacks can be invalidated.","Thereby, credential secrets and input fields to our accounts can be controlled.","Rather than encrypting passwords, we design a dual-password login-authentication mechanism, where a user-selected secret-free login password is converted into an untypable authentication password.","Subsequently, the authenticatable functionality of the login password and the typable functionality of the authentication password may be disabled or invalidated so that the credential theft and remote attacks can be prevented.","Thus, the usability-security trade-off and password reuse are resolved; local storage of authentication passwords is no longer necessary.","More importantly, the password converter acts as an open hash algorithm, meaning that its intermediate elements can be used to define a truly unique identity of the login process to implement a novel dual-identity authentication.","Particularly, the elements are concealed, inaccessible, and independent of any personal information, and therefore can be used to define a perfect unforgeable process identifier to identify and disable the unauthorized inputs."],"url":"http://arxiv.org/abs/2404.01803v1","category":"cs.CR"}
{"created":"2024-04-02 10:03:33","title":"Explicit formulas for adiabatic elimination with fast unitary dynamics","abstract":"The so-called ``adiabatic elimination'' of fast decaying degrees of freedom in open quantum systems can be performed with a series expansion in the timescale separation. The associated computations are significantly more difficult when the remaining degrees of freedom (center manifold) follow fast unitary dynamics instead of just being slow. This paper highlights how a formulation with Sylvester's equation and with adjoint dynamics leads to systematic, explicit expressions at high orders for settings of physical interest.","sentences":["The so-called ``adiabatic elimination'' of fast decaying degrees of freedom in open quantum systems can be performed with a series expansion in the timescale separation.","The associated computations are significantly more difficult when the remaining degrees of freedom (center manifold) follow fast unitary dynamics instead of just being slow.","This paper highlights how a formulation with Sylvester's equation and with adjoint dynamics leads to systematic, explicit expressions at high orders for settings of physical interest."],"url":"http://arxiv.org/abs/2404.01802v1","category":"quant-ph"}
{"created":"2024-04-02 09:57:54","title":"On the Algorithmic Recovering of Coefficients in Linearizable Differential Equations","abstract":"We investigate the problem of recovering coefficients in scalar nonlinear ordinary differential equations that can be exactly linearized. This contribution builds upon prior work by Lyakhov, Gerdt, and Michels, which focused on obtaining a linearizability certificate through point transformations. Our focus is on quasi-linear equations, specifically those solved for the highest derivative with a rational dependence on the variables involved. Our novel algorithm for coefficient recovery relies on basic operations on Lie algebras, such as computing the derived algebra and the dimension of the symmetry algebra. This algorithmic approach is efficient, although finding the linearization transformation necessitates computing at least one solution of the corresponding Bluman-Kumei equation system.","sentences":["We investigate the problem of recovering coefficients in scalar nonlinear ordinary differential equations that can be exactly linearized.","This contribution builds upon prior work by Lyakhov, Gerdt, and Michels, which focused on obtaining a linearizability certificate through point transformations.","Our focus is on quasi-linear equations, specifically those solved for the highest derivative with a rational dependence on the variables involved.","Our novel algorithm for coefficient recovery relies on basic operations on Lie algebras, such as computing the derived algebra and the dimension of the symmetry algebra.","This algorithmic approach is efficient, although finding the linearization transformation necessitates computing at least one solution of the corresponding Bluman-Kumei equation system."],"url":"http://arxiv.org/abs/2404.01798v1","category":"cs.SC"}
{"created":"2024-04-02 09:46:20","title":"Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems","abstract":"In this paper, we investigate a multi-receiver communication system enabled by movable antennas (MAs). Specifically, the transmit beamforming and the double-side antenna movement at the transceiver are jointly designed to maximize the sum-rate of all receivers under imperfect channel state information (CSI). Since the formulated problem is non-convex with highly coupled variables, conventional optimization methods cannot solve it efficiently. To address these challenges, an effective learning-based algorithm is proposed, namely heterogeneous multi-agent deep deterministic policy gradient (MADDPG), which incorporates two agents to learn policies for beamforming and movement of MAs, respectively. Based on the offline learning under numerous imperfect CSI, the proposed heterogeneous MADDPG can output the solutions for transmit beamforming and antenna movement in real time. Simulation results validate the effectiveness of the proposed algorithm, and the MA can significantly improve the sum-rate performance of multiple receivers compared to other benchmark schemes.","sentences":["In this paper, we investigate a multi-receiver communication system enabled by movable antennas (MAs).","Specifically, the transmit beamforming and the double-side antenna movement at the transceiver are jointly designed to maximize the sum-rate of all receivers under imperfect channel state information (CSI).","Since the formulated problem is non-convex with highly coupled variables, conventional optimization methods cannot solve it efficiently.","To address these challenges, an effective learning-based algorithm is proposed, namely heterogeneous multi-agent deep deterministic policy gradient (MADDPG), which incorporates two agents to learn policies for beamforming and movement of MAs, respectively.","Based on the offline learning under numerous imperfect CSI, the proposed heterogeneous MADDPG can output the solutions for transmit beamforming and antenna movement in real time.","Simulation results validate the effectiveness of the proposed algorithm, and the MA can significantly improve the sum-rate performance of multiple receivers compared to other benchmark schemes."],"url":"http://arxiv.org/abs/2404.01784v1","category":"cs.IT"}
{"created":"2024-04-02 09:44:30","title":"CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST)","abstract":"Strong gravitational lensing is a powerful tool for investigating dark matter and dark energy properties. With the advent of large-scale sky surveys, we can discover strong lensing systems on an unprecedented scale, which requires efficient tools to extract them from billions of astronomical objects. The existing mainstream lens-finding tools are based on machine learning algorithms and applied to cut-out-centered galaxies. However, according to the design and survey strategy of optical surveys by CSST, preparing cutouts with multiple bands requires considerable efforts. To overcome these challenges, we have developed a framework based on a hierarchical visual Transformer with a sliding window technique to search for strong lensing systems within entire images. Moreover, given that multi-color images of strong lensing systems can provide insights into their physical characteristics, our framework is specifically crafted to identify strong lensing systems in images with any number of channels. As evaluated using CSST mock data based on an Semi-Analytic Model named CosmoDC2, our framework achieves precision and recall rates of 0.98 and 0.90, respectively. To evaluate the effectiveness of our method in real observations, we have applied it to a subset of images from the DESI Legacy Imaging Surveys and media images from Euclid Early Release Observations. 61 new strong lensing system candidates are discovered by our method. However, we also identified false positives arising primarily from the simplified galaxy morphology assumptions within the simulation. This underscores the practical limitations of our approach while simultaneously highlighting potential avenues for future improvements.","sentences":["Strong gravitational lensing is a powerful tool for investigating dark matter and dark energy properties.","With the advent of large-scale sky surveys, we can discover strong lensing systems on an unprecedented scale, which requires efficient tools to extract them from billions of astronomical objects.","The existing mainstream lens-finding tools are based on machine learning algorithms and applied to cut-out-centered galaxies.","However, according to the design and survey strategy of optical surveys by CSST, preparing cutouts with multiple bands requires considerable efforts.","To overcome these challenges, we have developed a framework based on a hierarchical visual Transformer with a sliding window technique to search for strong lensing systems within entire images.","Moreover, given that multi-color images of strong lensing systems can provide insights into their physical characteristics, our framework is specifically crafted to identify strong lensing systems in images with any number of channels.","As evaluated using CSST mock data based on an Semi-Analytic Model named CosmoDC2, our framework achieves precision and recall rates of 0.98 and 0.90, respectively.","To evaluate the effectiveness of our method in real observations, we have applied it to a subset of images from the DESI Legacy Imaging Surveys and media images from Euclid Early Release Observations.","61 new strong lensing system candidates are discovered by our method.","However, we also identified false positives arising primarily from the simplified galaxy morphology assumptions within the simulation.","This underscores the practical limitations of our approach while simultaneously highlighting potential avenues for future improvements."],"url":"http://arxiv.org/abs/2404.01780v1","category":"astro-ph.IM"}
{"created":"2024-04-02 09:31:51","title":"Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems","abstract":"The rapid advance of deep reinforcement learning techniques enables the oversight of safety-critical systems through the utilization of Deep Neural Networks (DNNs). This underscores the pressing need to promptly establish certified safety guarantees for such DNN-controlled systems. Most of the existing verification approaches rely on qualitative approaches, predominantly employing reachability analysis. However, qualitative verification proves inadequate for DNN-controlled systems as their behaviors exhibit stochastic tendencies when operating in open and adversarial environments. In this paper, we propose a novel framework for unifying both qualitative and quantitative safety verification problems of DNN-controlled systems. This is achieved by formulating the verification tasks as the synthesis of valid neural barrier certificates (NBCs). Initially, the framework seeks to establish almost-sure safety guarantees through qualitative verification. In cases where qualitative verification fails, our quantitative verification method is invoked, yielding precise lower and upper bounds on probabilistic safety across both infinite and finite time horizons. To facilitate the synthesis of NBCs, we introduce their $k$-inductive variants. We also devise a simulation-guided approach for training NBCs, aiming to achieve tightness in computing precise certified lower and upper bounds. We prototype our approach into a tool called $\\textsf{UniQQ}$ and showcase its efficacy on four classic DNN-controlled systems.","sentences":["The rapid advance of deep reinforcement learning techniques enables the oversight of safety-critical systems through the utilization of Deep Neural Networks (DNNs).","This underscores the pressing need to promptly establish certified safety guarantees for such DNN-controlled systems.","Most of the existing verification approaches rely on qualitative approaches, predominantly employing reachability analysis.","However, qualitative verification proves inadequate for DNN-controlled systems as their behaviors exhibit stochastic tendencies when operating in open and adversarial environments.","In this paper, we propose a novel framework for unifying both qualitative and quantitative safety verification problems of DNN-controlled systems.","This is achieved by formulating the verification tasks as the synthesis of valid neural barrier certificates (NBCs).","Initially, the framework seeks to establish almost-sure safety guarantees through qualitative verification.","In cases where qualitative verification fails, our quantitative verification method is invoked, yielding precise lower and upper bounds on probabilistic safety across both infinite and finite time horizons.","To facilitate the synthesis of NBCs, we introduce their $k$-inductive variants.","We also devise a simulation-guided approach for training NBCs, aiming to achieve tightness in computing precise certified lower and upper bounds.","We prototype our approach into a tool called $\\textsf{UniQQ}$ and showcase its efficacy on four classic DNN-controlled systems."],"url":"http://arxiv.org/abs/2404.01769v1","category":"cs.LG"}
{"created":"2024-04-02 09:31:14","title":"Class-Incremental Few-Shot Event Detection","abstract":"Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD.","sentences":["Event detection is one of the fundamental tasks in information extraction and knowledge graph.","However, a realistic event detection system often needs to deal with new event classes constantly.","These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances.","Therefore, this paper proposes a new task, called class-incremental few-shot event detection.","Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting.","To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD.","Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation.","On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism.","Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD."],"url":"http://arxiv.org/abs/2404.01767v1","category":"cs.CL"}
{"created":"2024-04-02 09:31:06","title":"Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning","abstract":"Segmentation in medical imaging is an essential and often preliminary task in the image processing chain, driving numerous efforts towards the design of robust segmentation algorithms. Supervised learning methods achieve excellent performances when fed with a sufficient amount of labeled data. However, such labels are typically highly time-consuming, error-prone and expensive to produce. Alternatively, semi-supervised learning approaches leverage both labeled and unlabeled data, and are very useful when only a small fraction of the dataset is labeled. They are particularly useful for cerebrovascular segmentation, given that labeling a single volume requires several hours for an expert. In addition to the challenge posed by insufficient annotations, there are concerns regarding annotation consistency. The task of annotating the cerebrovascular tree is inherently ambiguous. Due to the discrete nature of images, the borders and extremities of vessels are often unclear. Consequently, annotations heavily rely on the expert subjectivity and on the underlying clinical objective. These discrepancies significantly increase the complexity of the segmentation task for the model and consequently impair the results. Consequently, it becomes imperative to provide clinicians with precise guidelines to improve the annotation process and construct more uniform datasets. In this article, we investigate the data dependency of deep learning methods within the context of imperfect data and semi-supervised learning, for cerebrovascular segmentation. Specifically, this study compares various state-of-the-art semi-supervised methods based on unsupervised regularization and evaluates their performance in diverse quantity and quality data scenarios. Based on these experiments, we provide guidelines for the annotation and training of cerebrovascular segmentation models.","sentences":["Segmentation in medical imaging is an essential and often preliminary task in the image processing chain, driving numerous efforts towards the design of robust segmentation algorithms.","Supervised learning methods achieve excellent performances when fed with a sufficient amount of labeled data.","However, such labels are typically highly time-consuming, error-prone and expensive to produce.","Alternatively, semi-supervised learning approaches leverage both labeled and unlabeled data, and are very useful when only a small fraction of the dataset is labeled.","They are particularly useful for cerebrovascular segmentation, given that labeling a single volume requires several hours for an expert.","In addition to the challenge posed by insufficient annotations, there are concerns regarding annotation consistency.","The task of annotating the cerebrovascular tree is inherently ambiguous.","Due to the discrete nature of images, the borders and extremities of vessels are often unclear.","Consequently, annotations heavily rely on the expert subjectivity and on the underlying clinical objective.","These discrepancies significantly increase the complexity of the segmentation task for the model and consequently impair the results.","Consequently, it becomes imperative to provide clinicians with precise guidelines to improve the annotation process and construct more uniform datasets.","In this article, we investigate the data dependency of deep learning methods within the context of imperfect data and semi-supervised learning, for cerebrovascular segmentation.","Specifically, this study compares various state-of-the-art semi-supervised methods based on unsupervised regularization and evaluates their performance in diverse quantity and quality data scenarios.","Based on these experiments, we provide guidelines for the annotation and training of cerebrovascular segmentation models."],"url":"http://arxiv.org/abs/2404.01765v1","category":"eess.IV"}
{"created":"2024-04-02 09:16:18","title":"Quantum charges of harmonic oscillators","abstract":"We discuss Riemannian geometry of one-dimensional quantum harmonic oscillator. Its wavefunction is a holomorphic section of the complex line bundle $L_{\\sf{v}}$ over the phase space $\\mathbb{R}^2$. We show that the energy eigenfunctions $\\psi_n$ with $n\\ge 1$, corresponding to the energy levels $E_n$, are complex coordinates on orbifolds $\\mathbb{R}^2/\\mathbb{Z}_n$ embedded into $L_{\\sf{v}}$, where $\\mathbb{Z}_n$ is the cyclic group of order $n$. In fact, $\\psi_n (t,z)$ is a standing wave on $\\mathbb{R}^2/\\mathbb{Z}_n$, where $z$ is a complex coordinate on the phase space $\\mathbb{R}^2\\cong\\mathbb{C}$. Oscillators are characterized by two quantum charges $(q_l^{}, q_{\\sf{v}})=(n,1)$, where $q_l^{}=n$ is the winding number for the group U(1) acting on $\\mathbb{R}^2/\\mathbb{Z}_n$ and $q_{\\sf{v}}^{}=1$ is the winding number for the U(1)-rotations on fibres of the bundle $L_{\\sf{v}}\\to\\mathbb{R}^2$, and $E_n=\\hbar\\omega(q_l^{}+\\frac{1}{2} q_{\\sf{v}}).$ We also discuss ``antioscillators\" with opposite quantum charges and the same positive energy.","sentences":["We discuss Riemannian geometry of one-dimensional quantum harmonic oscillator.","Its wavefunction is a holomorphic section of the complex line bundle $L_{\\sf{v}}$ over the phase space $\\mathbb{R}^2$. We show that the energy eigenfunctions $\\psi_n$ with $n\\ge 1$, corresponding to the energy levels $E_n$, are complex coordinates on orbifolds $\\mathbb{R}^2/\\mathbb{Z}_n$ embedded into $L_{\\sf{v}}$, where $\\mathbb{Z}_n$ is the cyclic group of order $n$. In fact, $\\psi_n (t,z)$ is a standing wave on $\\mathbb{R}^2/\\mathbb{Z}_n$, where $z$ is a complex coordinate on the phase space $\\mathbb{R}^2\\cong\\mathbb{C}$. Oscillators are characterized by two quantum charges $(q_l^{}, q_{\\sf{v}})=(n,1)$, where $q_l^{}=n$ is the winding number for the group U(1) acting on $\\mathbb{R}^2/\\mathbb{Z}_n$ and $q_{\\sf{v}}^{}=1$ is the winding number for the U(1)-rotations on fibres of the bundle $L_{\\sf{v}}\\to\\mathbb{R}^2$, and $E_n=\\hbar\\omega(q_l^{}+\\frac{1}{2} q_{\\sf{v}}).$","We also discuss ``antioscillators\" with opposite quantum charges and the same positive energy."],"url":"http://arxiv.org/abs/2404.01756v1","category":"math-ph"}
{"created":"2024-04-02 09:07:05","title":"T-VSL: Text-Guided Visual Sound Source Localization in Mixtures","abstract":"Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods.","sentences":["Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video.","Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures.","These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios.","The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization.","To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures.","Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures.","Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding.","This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time.","Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.01751v1","category":"cs.CV"}
{"created":"2024-04-02 08:56:23","title":"Real-space Calculation of Orbital Hall Responses in Disordered Materials","abstract":"We developed an efficient numerical approach to compute the different components of the orbital Hall responses in disordered materials from the Berry phase theory of magnetization. We propose a theoretical framework based on the Chebyshev expansion of Green's functions and the position operator for systems under arbitrary boundary conditions. The capability of this scheme is illustrated by computing the orbital Hall conductivity for gapped graphene and Haldane model in the presence of nonperturbative disorder effects. This methodology opens the door to realistic simulations of orbital Hall responses in arbitrary complex models of disordered materials.","sentences":["We developed an efficient numerical approach to compute the different components of the orbital Hall responses in disordered materials from the Berry phase theory of magnetization.","We propose a theoretical framework based on the Chebyshev expansion of Green's functions and the position operator for systems under arbitrary boundary conditions.","The capability of this scheme is illustrated by computing the orbital Hall conductivity for gapped graphene and Haldane model in the presence of nonperturbative disorder effects.","This methodology opens the door to realistic simulations of orbital Hall responses in arbitrary complex models of disordered materials."],"url":"http://arxiv.org/abs/2404.01739v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 08:43:09","title":"The ZOO of combinatorial Banach spaces","abstract":"We study Banach spaces induced by families of finite sets in the most natural (Schreier-like) way, that is, we consider the completion $X_\\mc{F}$ of $c_{00}$ with respect to the norm $\\sup\\{\\sum_{k\\in F}|x(k)|:F\\in\\mc{F}\\}$ where $\\mc{F}$ is an arbitrary (not necessarily compact) family of finite sets covering $\\mbb{N}$.   Among other results, we discuss the following:   (1) Structure theorems bonding the combinatorics of $\\mc{F}$ and the geometry of $X_\\mc{F}$ including possible characterizations and variants of the Schur property, $\\ell_1$-saturation, and the lack of copies of $c_0$ in $X_\\mc{F}$.   (2) A plethora of examples including a relatively simple $\\ell_1$-saturated combinatorial space which does not satisfy the Schur property, as well as a new presentation of Pe\\l czy\\'nski's universal space.   (3) The complexity of the family $\\{H\\subseteq\\NN:X_{\\mc{F}\\upharpoonright H}$ does not contain $c_0\\}$.","sentences":["We study Banach spaces induced by families of finite sets in the most natural (Schreier-like) way, that is, we consider the completion $X_\\mc{F}$ of $c_{00}$ with respect to the norm $\\sup\\{\\sum_{k\\in F}|x(k)|:F\\in\\mc{F}\\}$ where $\\mc{F}$ is an arbitrary (not necessarily compact) family of finite sets covering $\\mbb{N}$.   Among other results, we discuss the following:   (1) Structure theorems bonding the combinatorics of $\\mc{F}$ and the geometry of $X_\\mc{F}$ including possible characterizations and variants of the Schur property, $\\ell_1$-saturation, and the lack of copies of $c_0$ in $X_\\mc{F}$.   (2) A plethora of examples including a relatively simple $\\ell_1$-saturated combinatorial space which does not satisfy the Schur property, as well as a new presentation of Pe\\l czy\\'nski's universal space.   ","(3) The complexity of the family $\\{H\\subseteq\\NN:X_{\\mc{F}\\upharpoonright H}$ does not contain $c_0\\}$."],"url":"http://arxiv.org/abs/2404.01733v1","category":"math.FA"}
{"created":"2024-04-02 08:27:47","title":"A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions","abstract":"Finite-state abstractions are widely studied for the automated synthesis of correct-by-construction controllers for stochastic dynamical systems. However, existing abstraction methods often lead to prohibitively large finite-state models. To address this issue, we propose a novel abstraction scheme for stochastic linear systems that exploits the system's stability to obtain significantly smaller abstract models. As a unique feature, we first stabilize the open-loop dynamics using a linear feedback gain. We then use a model-based approach to abstract a known part of the stabilized dynamics while using a data-driven method to account for the stochastic uncertainty. We formalize abstractions as Markov decision processes (MDPs) with intervals of transition probabilities. By stabilizing the dynamics, we can further constrain the control input modeled in the abstraction, which leads to smaller abstract models while retaining the correctness of controllers. Moreover, when the stabilizing feedback controller is aligned with the property of interest, then a good trade-off is achieved between the reduction in the abstraction size and the performance loss. The experiments show that our approach can reduce the size of the graph of abstractions by up to 90% with negligible performance loss.","sentences":["Finite-state abstractions are widely studied for the automated synthesis of correct-by-construction controllers for stochastic dynamical systems.","However, existing abstraction methods often lead to prohibitively large finite-state models.","To address this issue, we propose a novel abstraction scheme for stochastic linear systems that exploits the system's stability to obtain significantly smaller abstract models.","As a unique feature, we first stabilize the open-loop dynamics using a linear feedback gain.","We then use a model-based approach to abstract a known part of the stabilized dynamics while using a data-driven method to account for the stochastic uncertainty.","We formalize abstractions as Markov decision processes (MDPs) with intervals of transition probabilities.","By stabilizing the dynamics, we can further constrain the control input modeled in the abstraction, which leads to smaller abstract models while retaining the correctness of controllers.","Moreover, when the stabilizing feedback controller is aligned with the property of interest, then a good trade-off is achieved between the reduction in the abstraction size and the performance loss.","The experiments show that our approach can reduce the size of the graph of abstractions by up to 90% with negligible performance loss."],"url":"http://arxiv.org/abs/2404.01726v1","category":"eess.SY"}
{"created":"2024-04-02 08:20:14","title":"Global existence and boundedness of solutions to a fully parabolic chemotaxis system with indirect signal production in $\\mathbb{R}^4$","abstract":"Global existence and boundedness of solutions to the Cauchy problem for the four dimensional fully parabolic chemotaxis system with indirect signal production are studied. We prove that solutions with initial mass below $(8\\pi)^2$ exist globally in time. This value $(8\\pi)^2$ is known as the four dimensional threshold value of the initial mass determining whether blow-up of solutions occurs or not. Furthermore, some condition on the initial mass guaranteeing that the solution remains uniformly bounded is also obtained.","sentences":["Global existence and boundedness of solutions to the Cauchy problem for the four dimensional fully parabolic chemotaxis system with indirect signal production are studied.","We prove that solutions with initial mass below $(8\\pi)^2$ exist globally in time.","This value $(8\\pi)^2$ is known as the four dimensional threshold value of the initial mass determining whether blow-up of solutions occurs or not.","Furthermore, some condition on the initial mass guaranteeing that the solution remains uniformly bounded is also obtained."],"url":"http://arxiv.org/abs/2404.01724v1","category":"math.AP"}
{"created":"2024-04-02 08:15:51","title":"Dynamics on Markov surfaces: classification of stationary measures","abstract":"Consider the four punctured sphere ${\\mathbb{S}}_4^2$. Each choice of four traces, one for each puncture, determines a relative character variety for the representations of the fundamental group of ${\\mathbb{S}}_4^2$ in ${\\sf{SL}}_2(\\mathbb{C})$. We classify the stationary probability measures for the action of the mapping class group ${\\sf{Mod}}({\\mathbb{S}}_4^2)$ on these character varieties.","sentences":["Consider the four punctured sphere ${\\mathbb{S}}_4^2$.","Each choice of four traces, one for each puncture, determines a relative character variety for the representations of the fundamental group of ${\\mathbb{S}}_4^2$ in ${\\sf{SL}}_2(\\mathbb{C})$. We classify the stationary probability measures for the action of the mapping class group ${\\sf{Mod}}({\\mathbb{S}}_4^2)$ on these character varieties."],"url":"http://arxiv.org/abs/2404.01721v1","category":"math.DS"}
{"created":"2024-04-02 08:08:29","title":"Optical Spectra of Plasmon-Exciton Core-Shell Nanoparticles: An Anisotropic Classical Model Eliminates Discrepancies with Experiments","abstract":"The optical properties of the hybrid core--shell nanostructures composed of a metallic core and an organic shell of molecular J-aggregates are determined by the electromagnetic coupling between plasmons localized at the surface of the metallic core and Frenkel excitons in the shell. In cases of strong and ultra-strong plasmon--exciton coupling, the use of the traditional isotropic classical oscillator model to describe the J-aggregate permittivity may lead to drastic discrepancies between theoretical predictions and the available experimental spectra of hybrid nanoparticles. We show that these discrepancies are not caused by limitations of the classical oscillator model itself, but by considering the organic shell as an optically isotropic material. By assuming a tangential orientation of the classical oscillators of the molecular J-aggregates in a shell, we obtain excellent agreement with experimental extinction spectra of TDBC-coated gold nanorods, which cannot be treated with the conventional isotropic shell model. Our results extend the understanding of the physical effects in the optics of metal--organic nanoparticles and suggest an approach for the theoretical description of such hybrid systems.","sentences":["The optical properties of the hybrid core--shell nanostructures composed of a metallic core and an organic shell of molecular J-aggregates are determined by the electromagnetic coupling between plasmons localized at the surface of the metallic core and Frenkel excitons in the shell.","In cases of strong and ultra-strong plasmon--exciton coupling, the use of the traditional isotropic classical oscillator model to describe the J-aggregate permittivity may lead to drastic discrepancies between theoretical predictions and the available experimental spectra of hybrid nanoparticles.","We show that these discrepancies are not caused by limitations of the classical oscillator model itself, but by considering the organic shell as an optically isotropic material.","By assuming a tangential orientation of the classical oscillators of the molecular J-aggregates in a shell, we obtain excellent agreement with experimental extinction spectra of TDBC-coated gold nanorods, which cannot be treated with the conventional isotropic shell model.","Our results extend the understanding of the physical effects in the optics of metal--organic nanoparticles and suggest an approach for the theoretical description of such hybrid systems."],"url":"http://arxiv.org/abs/2404.01718v1","category":"physics.optics"}
{"created":"2024-04-02 07:10:16","title":"Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot","abstract":"As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.","sentences":["As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot.","This includes combining data from several modalities together with the context of the situation and background knowledge.","Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data.","In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup).","We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations.","Then we implement and evaluate the model on the real setup.","In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification.","For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds."],"url":"http://arxiv.org/abs/2404.01702v1","category":"cs.HC"}
{"created":"2024-04-02 06:54:13","title":"Accurate and precise quantum computation of valence two-neutron systems","abstract":"Developing methods to solve nuclear many-body problems with quantum computers is an imperative pursuit within the nuclear physics community. Here, we introduce a quantum algorithm to accurately and precisely compute the ground state of valence two-neutron systems leveraging presently available Noisy Intermediate-Scale Quantum devices. Our focus lies on the nuclei having a doubly-magic core plus two valence neutrons in the $ p $, $ sd $, and $ pf $ shells, i.e. ${}^6$He, ${}^{18}$O, and ${}^{42}$Ca, respectively. Our ansatz, quantum circuit, is constructed in the pair-wise form, taking into account the symmetries of the system in an explicit manner, and enables us to reduce the number of qubits and the number of CNOT gates required. The results on a real quantum hardware by IBM Quantum Platform show that the proposed method gives very accurate results of the ground-state energies, which are typically within $ 0.1 \\, \\% $ error in the energy for ${}^6$He and ${}^{18}$O and at most $ 1 \\, \\% $ error for ${}^{42}$Ca. Furthermore, our experiments using real quantum devices also show the pivotal role of the circuit layout design, attuned to the connectivity of the qubits, in mitigating errors.","sentences":["Developing methods to solve nuclear many-body problems with quantum computers is an imperative pursuit within the nuclear physics community.","Here, we introduce a quantum algorithm to accurately and precisely compute the ground state of valence two-neutron systems leveraging presently available Noisy Intermediate-Scale Quantum devices.","Our focus lies on the nuclei having a doubly-magic core plus two valence neutrons in the $ p $, $ sd $, and $ pf $ shells, i.e. ${}^6$He, ${}^{18}$O, and ${}^{42}$Ca, respectively.","Our ansatz, quantum circuit, is constructed in the pair-wise form, taking into account the symmetries of the system in an explicit manner, and enables us to reduce the number of qubits and the number of CNOT gates required.","The results on a real quantum hardware by IBM Quantum Platform show that the proposed method gives very accurate results of the ground-state energies, which are typically within $ 0.1 \\, \\% $ error in the energy for ${}^6$He and ${}^{18}$O and at most $ 1 \\, \\% $ error for ${}^{42}$Ca.","Furthermore, our experiments using real quantum devices also show the pivotal role of the circuit layout design, attuned to the connectivity of the qubits, in mitigating errors."],"url":"http://arxiv.org/abs/2404.01694v1","category":"nucl-th"}
{"created":"2024-04-02 06:39:46","title":"Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay","abstract":"The Internet of Things (IoT) is a communication scheme which allows various objects to exchange several types of information, enabling functions such as home automation, production management, healthcare, etc. Moreover, energy-harvesting (EH) technology is considered for IoT environment in order to reduce the need for management and enhance maintainability. However, since environments considering outdoor elements such as pedestrians, vehicles and drones have been on the rise recently, it is important to consider mobility when designing an IoT network management scheme. In order to handle this challenge, prior research has made an attempt to solve this problem via variational autoencoder (VAE) and backward-pass rate evaluation method. In this article, we propose a guided-mutation genetic algorithm (GMGA) to derive a sub-optimal relaying topology for IoT systems considering energy-harvesting. Furthermore, we propose a mobility-aware iterative relaying topology algorithm, which calculates the sub-optimal relaying topology of current time frame using the topology result of the previous one. Simulation results verify that our proposed scheme effectively solves formulated IoT network problems compared to other conventional schemes, and also effectively handles IoT environments in terms of mobility.","sentences":["The Internet of Things (IoT) is a communication scheme which allows various objects to exchange several types of information, enabling functions such as home automation, production management, healthcare, etc.","Moreover, energy-harvesting (EH) technology is considered for IoT environment in order to reduce the need for management and enhance maintainability.","However, since environments considering outdoor elements such as pedestrians, vehicles and drones have been on the rise recently, it is important to consider mobility when designing an IoT network management scheme.","In order to handle this challenge, prior research has made an attempt to solve this problem via variational autoencoder (VAE) and backward-pass rate evaluation method.","In this article, we propose a guided-mutation genetic algorithm (GMGA) to derive a sub-optimal relaying topology for IoT systems considering energy-harvesting.","Furthermore, we propose a mobility-aware iterative relaying topology algorithm, which calculates the sub-optimal relaying topology of current time frame using the topology result of the previous one.","Simulation results verify that our proposed scheme effectively solves formulated IoT network problems compared to other conventional schemes, and also effectively handles IoT environments in terms of mobility."],"url":"http://arxiv.org/abs/2404.01683v1","category":"cs.NI"}
{"created":"2024-04-02 06:39:06","title":"Emergent Simplicities in the Living Histories of Individual Cells","abstract":"Organisms maintain the status quo, holding key physiological variables constant to within an acceptable tolerance, and yet adapt with precision and plasticity to dynamic changes in externalities. What organizational principles ensure such exquisite yet robust control of systems-level \"state variables\" in complex systems with an extraordinary number of moving parts and fluctuating variables? Here we focus on these issues in the specific context of intra- and intergenerational life histories of individual bacterial cells, whose biographies are precisely charted via high-precision dynamic experiments using the SChemostat technology. We highlight intra- and intergenerational scaling laws and other \"emergent simplicities\" revealed by these high-precision data. In turn, these facilitate a principled route to dimensional reduction of the problem, and serve as essential building blocks for phenomenological and mechanistic theory. Parameter-free data-theory matches for multiple organisms validate theory frameworks, and explicate the systems physics of stochastic homeostasis and adaptation.","sentences":["Organisms maintain the status quo, holding key physiological variables constant to within an acceptable tolerance, and yet adapt with precision and plasticity to dynamic changes in externalities.","What organizational principles ensure such exquisite yet robust control of systems-level \"state variables\" in complex systems with an extraordinary number of moving parts and fluctuating variables?","Here we focus on these issues in the specific context of intra- and intergenerational life histories of individual bacterial cells, whose biographies are precisely charted via high-precision dynamic experiments using the SChemostat technology.","We highlight intra- and intergenerational scaling laws and other \"emergent simplicities\" revealed by these high-precision data.","In turn, these facilitate a principled route to dimensional reduction of the problem, and serve as essential building blocks for phenomenological and mechanistic theory.","Parameter-free data-theory matches for multiple organisms validate theory frameworks, and explicate the systems physics of stochastic homeostasis and adaptation."],"url":"http://arxiv.org/abs/2404.01682v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-02 06:31:51","title":"A multicore parallel algorithm for multiscale modelling of an entire human blood circulation network","abstract":"The presented multi-scale, closed-loop blood circulation model includes arterial, venous, and portal venous systems, heart-pulmonary circulation, and micro-circulation in capillaries. One-dimensional models simulate large blood vessel flow, whereas zerodimensional models are used for simulating blood flow in vascular subsystems corresponding to peripheral arteries and organs. Transmission conditions at bifurcation and confluence are solved using Riemann invariants. Blood circulation simulation in the portal venous system and related organs (liver, stomach, spleen, pancreas, intestine) is particularly targeted. Those organs play important roles in metabolic system dynamics. The proposed efficient parallel algorithms for multicore environments solve these equations much faster than serial computations.","sentences":["The presented multi-scale, closed-loop blood circulation model includes arterial, venous, and portal venous systems, heart-pulmonary circulation, and micro-circulation in capillaries.","One-dimensional models simulate large blood vessel flow, whereas zerodimensional models are used for simulating blood flow in vascular subsystems corresponding to peripheral arteries and organs.","Transmission conditions at bifurcation and confluence are solved using Riemann invariants.","Blood circulation simulation in the portal venous system and related organs (liver, stomach, spleen, pancreas, intestine) is particularly targeted.","Those organs play important roles in metabolic system dynamics.","The proposed efficient parallel algorithms for multicore environments solve these equations much faster than serial computations."],"url":"http://arxiv.org/abs/2404.01680v1","category":"physics.med-ph"}
{"created":"2024-04-02 06:25:36","title":"Radiative forcing by super-volcano eruptions","abstract":"We investigate the climatic effects of volcanic eruptions spanning from Mt.\\ Pinatubo-sized events to super-volcanoes. The study is based on ensemble simulations in the Community Earth System Model Version 2 (CESM2) climate model using the Whole Atmosphere Community Climate Model Version 6 (WACCM6) atmosphere model. Our analysis focuses on the impact of different \\ce{SO2}-amount injections on stratospheric aerosol optical depth (AOD), effective radiative forcing (RF), and global temperature anomalies. Unlike the traditional linear models used for smaller eruptions, our results reveal a non-linear relationship between RF and AOD for larger eruptions. We also uncover a notable time-dependent decrease in aerosol forcing efficiency across all eruption magnitudes during the first post-eruption year. In addition, the study reveals that larger as compared to medium-sized eruption events produce a delayed and sharper peak in AOD, and a longer-lasting temperature response while the time evolution of RF remains similar between the two eruption types. When including the results of previous studies, we find that relating \\ce{SO2} to any other parameter is inconsistent across models compared to the relationships between AOD, RF, and temperature anomaly. Thus, we expect the largest uncertainty in model codes to relate to the chemistry and physics of \\ce{SO2} evolution. Finally, we find that the peak RF approaches a limiting value, and that the peak temperature response follows linearly, effectively bounding the temperature anomaly to at most (\\sim\\SI{-12}{\\kelvin}).","sentences":["We investigate the climatic effects of volcanic eruptions spanning from Mt.\\ Pinatubo-sized events to super-volcanoes.","The study is based on ensemble simulations in the Community Earth System Model Version 2 (CESM2) climate model using the Whole Atmosphere Community Climate Model Version 6 (WACCM6) atmosphere model.","Our analysis focuses on the impact of different \\ce{SO2}-amount injections on stratospheric aerosol optical depth (AOD), effective radiative forcing (RF), and global temperature anomalies.","Unlike the traditional linear models used for smaller eruptions, our results reveal a non-linear relationship between RF and AOD for larger eruptions.","We also uncover a notable time-dependent decrease in aerosol forcing efficiency across all eruption magnitudes during the first post-eruption year.","In addition, the study reveals that larger as compared to medium-sized eruption events produce a delayed and sharper peak in AOD, and a longer-lasting temperature response while the time evolution of RF remains similar between the two eruption types.","When including the results of previous studies, we find that relating \\ce{SO2} to any other parameter is inconsistent across models compared to the relationships between AOD, RF, and temperature anomaly.","Thus, we expect the largest uncertainty in model codes to relate to the chemistry and physics of \\ce{SO2} evolution.","Finally, we find that the peak RF approaches a limiting value, and that the peak temperature response follows linearly, effectively bounding the temperature anomaly to at most (\\sim\\SI{-12}{\\kelvin})."],"url":"http://arxiv.org/abs/2404.01675v1","category":"physics.ao-ph"}
{"created":"2024-04-02 06:07:25","title":"Structural and flow approaches to complex network systems lesions scale analysis","abstract":"A comparative analysis of structural and flow approaches to analysis of vulnerability of complex network systems (NS) from targeted attacks and non-target lesions of various types was carried out. Typical structural and functional scenarios of successive targeted attacks on the most important by certain characteristics system elements were considered, and scenarios of simultaneous group attacks on the most significant NS's components were proposed. The problem of system lesions scale from heterogeneous negative impacts was investigated and it was confirmed that the flow approach allows us to obtain a much more realistic picture of consequences of such lesions. It is shown that scenarios of group targeted attacks built on the basis of the NC flow model are more optimal from the point of view of choosing attack targets than structural ones.","sentences":["A comparative analysis of structural and flow approaches to analysis of vulnerability of complex network systems (NS) from targeted attacks and non-target lesions of various types was carried out.","Typical structural and functional scenarios of successive targeted attacks on the most important by certain characteristics system elements were considered, and scenarios of simultaneous group attacks on the most significant NS's components were proposed.","The problem of system lesions scale from heterogeneous negative impacts was investigated and it was confirmed that the flow approach allows us to obtain a much more realistic picture of consequences of such lesions.","It is shown that scenarios of group targeted attacks built on the basis of the NC flow model are more optimal from the point of view of choosing attack targets than structural ones."],"url":"http://arxiv.org/abs/2404.01662v1","category":"physics.soc-ph"}
{"created":"2024-04-02 06:07:18","title":"Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic","abstract":"This paper proposes collision-free optimal trajectory planning for autonomous vehicles in highway traffic, where vehicles need to deal with the interaction among each other. To address this issue, a novel optimal control framework is suggested, which couples the trajectory of surrounding vehicles with collision avoidance constraints. Additionally, we describe a trajectory optimization technique under state constraints, utilizing a planner based on Pontryagin's Minimum Principle, capable of numerically solving collision avoidance scenarios with surrounding vehicles. Simulation results demonstrate the effectiveness of the proposed approach regarding interaction-based motion planning for different scenarios.","sentences":["This paper proposes collision-free optimal trajectory planning for autonomous vehicles in highway traffic, where vehicles need to deal with the interaction among each other.","To address this issue, a novel optimal control framework is suggested, which couples the trajectory of surrounding vehicles with collision avoidance constraints.","Additionally, we describe a trajectory optimization technique under state constraints, utilizing a planner based on Pontryagin's Minimum Principle, capable of numerically solving collision avoidance scenarios with surrounding vehicles.","Simulation results demonstrate the effectiveness of the proposed approach regarding interaction-based motion planning for different scenarios."],"url":"http://arxiv.org/abs/2404.01661v1","category":"cs.RO"}
{"created":"2024-04-02 06:06:35","title":"Study of Decoupled Anisotropic Solutions in $f(R,T,R_{\u03c1\u03b7}T^{\u03c1\u03b7})$ Theory","abstract":"In this paper, we consider isotropic solution and extend it to two different exact well-behaved spherical anisotropic solutions through minimal geometric deformation method in $f(R,T,R_{\\rho\\eta}T^{\\rho\\eta})$ gravity. We only deform the radial metric component that separates the field equations into two sets corresponding to their original sources. The first set corresponds to perfect matter distribution while the other set exhibits the effects of additional source, i.e., anisotropy. The isotropic system is resolved by assuming the metric potentials proposed by Krori-Barua while the second set needs one constraint to be solved. The physical acceptability and consistency of the obtained solutions are analyzed through graphical analysis of effective matter components and energy bounds. We also examine mass, surface redshift and compactness of the resulting solutions. For particular values of the decoupling parameter, our both solutions turn out to be viable and stable. We conclude that this curvature-matter coupling gravity provides more stable solutions corresponding to a self-gravitating geometry.","sentences":["In this paper, we consider isotropic solution and extend it to two different exact well-behaved spherical anisotropic solutions through minimal geometric deformation method in $f(R,T,R_{\\rho\\eta}T^{\\rho\\eta})$ gravity.","We only deform the radial metric component that separates the field equations into two sets corresponding to their original sources.","The first set corresponds to perfect matter distribution while the other set exhibits the effects of additional source, i.e., anisotropy.","The isotropic system is resolved by assuming the metric potentials proposed by Krori-Barua while the second set needs one constraint to be solved.","The physical acceptability and consistency of the obtained solutions are analyzed through graphical analysis of effective matter components and energy bounds.","We also examine mass, surface redshift and compactness of the resulting solutions.","For particular values of the decoupling parameter, our both solutions turn out to be viable and stable.","We conclude that this curvature-matter coupling gravity provides more stable solutions corresponding to a self-gravitating geometry."],"url":"http://arxiv.org/abs/2404.01659v1","category":"gr-qc"}
{"created":"2024-04-02 05:56:17","title":"FashionEngine: Interactive Generation and Editing of 3D Clothed Humans","abstract":"We present FashionEngine, an interactive 3D human generation and editing system that allows us to design 3D digital humans in a way that aligns with how humans interact with the world, such as natural languages, visual perceptions, and hand-drawing. FashionEngine automates the 3D human production with three key components: 1) A pre-trained 3D human diffusion model that learns to model 3D humans in a semantic UV latent space from 2D image training data, which provides strong priors for diverse generation and editing tasks. 2) Multimodality-UV Space encoding the texture appearance, shape topology, and textual semantics of human clothing in a canonical UV-aligned space, which faithfully aligns the user multimodal inputs with the implicit UV latent space for controllable 3D human editing. The multimodality-UV space is shared across different user inputs, such as texts, images, and sketches, which enables various joint multimodal editing tasks. 3) Multimodality-UV Aligned Sampler learns to sample high-quality and diverse 3D humans from the diffusion prior for multimodal user inputs. Extensive experiments validate FashionEngine's state-of-the-art performance for conditional generation/editing tasks. In addition, we present an interactive user interface for our FashionEngine that enables both conditional and unconditional generation tasks, and editing tasks including pose/view/shape control, text-, image-, and sketch-driven 3D human editing and 3D virtual try-on, in a unified framework. Our project page is at: https://taohuumd.github.io/projects/FashionEngine.","sentences":["We present FashionEngine, an interactive 3D human generation and editing system that allows us to design 3D digital humans in a way that aligns with how humans interact with the world, such as natural languages, visual perceptions, and hand-drawing.","FashionEngine automates the 3D human production with three key components: 1) A pre-trained 3D human diffusion model that learns to model 3D humans in a semantic UV latent space from 2D image training data, which provides strong priors for diverse generation and editing tasks.","2) Multimodality-UV Space encoding the texture appearance, shape topology, and textual semantics of human clothing in a canonical UV-aligned space, which faithfully aligns the user multimodal inputs with the implicit UV latent space for controllable 3D human editing.","The multimodality-UV space is shared across different user inputs, such as texts, images, and sketches, which enables various joint multimodal editing tasks.","3) Multimodality-UV Aligned Sampler learns to sample high-quality and diverse 3D humans from the diffusion prior for multimodal user inputs.","Extensive experiments validate FashionEngine's state-of-the-art performance for conditional generation/editing tasks.","In addition, we present an interactive user interface for our FashionEngine that enables both conditional and unconditional generation tasks, and editing tasks including pose/view/shape control, text-, image-, and sketch-driven 3D human editing and 3D virtual try-on, in a unified framework.","Our project page is at: https://taohuumd.github.io/projects/FashionEngine."],"url":"http://arxiv.org/abs/2404.01655v1","category":"cs.CV"}
{"created":"2024-04-02 05:36:41","title":"NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps","abstract":"The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.","sentences":["The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word.","Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online.","Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous).","We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech.","We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors.","Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it."],"url":"http://arxiv.org/abs/2404.01651v1","category":"cs.CL"}
{"created":"2024-04-02 05:33:06","title":"The Significance of Ethnomathematics Learning: A Cross-Cultural Perspectives Between Indonesian and Thailand Educators","abstract":"The field of ethnomathematics holds significance in the pursuit of comprehending how students can grasp, express, manipulate, and ultimately apply mathematical concepts. However, ethnomathematics is also considered a complex concept in Asian countries such as Indonesia and Thailand, which can pose challenges as it needs to be comprehensively understood. This research aims to fill the gap by understanding the cross-cultural perspective of mathematics educators in Indonesia and Thailand. The participants were lecturers, teachers, and pre-service teachers. Data was gathered through questionnaires and interviews. The analytical approach involved were data reduction, data presentation, drawing conclusions or verification, and data validity. Positive responses were indicated by mathematics educators with the average scores of respondents in Indonesia at 4.77 and Thailand at 4.57. This research concludes the importance of integrating ethnomathematics in education, which is closely tied to cultural development, emphasizing the crucial role of employing comprehensive strategies in its implementation.","sentences":["The field of ethnomathematics holds significance in the pursuit of comprehending how students can grasp, express, manipulate, and ultimately apply mathematical concepts.","However, ethnomathematics is also considered a complex concept in Asian countries such as Indonesia and Thailand, which can pose challenges as it needs to be comprehensively understood.","This research aims to fill the gap by understanding the cross-cultural perspective of mathematics educators in Indonesia and Thailand.","The participants were lecturers, teachers, and pre-service teachers.","Data was gathered through questionnaires and interviews.","The analytical approach involved were data reduction, data presentation, drawing conclusions or verification, and data validity.","Positive responses were indicated by mathematics educators with the average scores of respondents in Indonesia at 4.77 and Thailand at 4.57.","This research concludes the importance of integrating ethnomathematics in education, which is closely tied to cultural development, emphasizing the crucial role of employing comprehensive strategies in its implementation."],"url":"http://arxiv.org/abs/2404.01648v1","category":"math.HO"}
{"created":"2024-04-02 05:30:39","title":"ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models","abstract":"The success of Transformer-based models has encouraged many researchers to learn CAD models using sequence-based approaches. However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences. Furthermore, the same CAD model can be expressed using different CAD construction sequences. We propose a novel contrastive learning-based approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model. ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model. We also propose a new CAD data augmentation method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset. Experimental results show that the proposed RRE augmentation method significantly enhances the learning performance of Transformer-based autoencoders, even for complex CAD models having very long construction sequences. The proposed ContrastCAD model is shown to be robust to permutation changes of construction sequences and performs better representation learning by generating representation spaces where similar CAD models are more closely clustered. Our codes are available at https://github.com/cm8908/ContrastCAD.","sentences":["The success of Transformer-based models has encouraged many researchers to learn CAD models using sequence-based approaches.","However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences.","Furthermore, the same CAD model can be expressed using different CAD construction sequences.","We propose a novel contrastive learning-based approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model.","ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model.","We also propose a new CAD data augmentation method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset.","Experimental results show that the proposed RRE augmentation method significantly enhances the learning performance of Transformer-based autoencoders, even for complex CAD models having very long construction sequences.","The proposed ContrastCAD model is shown to be robust to permutation changes of construction sequences and performs better representation learning by generating representation spaces where similar CAD models are more closely clustered.","Our codes are available at https://github.com/cm8908/ContrastCAD."],"url":"http://arxiv.org/abs/2404.01645v1","category":"cs.CV"}
{"created":"2024-04-02 05:20:12","title":"InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis","abstract":"The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs. In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis. Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.","sentences":["The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis.","LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents.","However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations.","This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs.","In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis.","Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process.","Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration.","A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience."],"url":"http://arxiv.org/abs/2404.01644v1","category":"cs.HC"}
{"created":"2024-04-02 05:16:59","title":"ADVREPAIR:Provable Repair of Adversarial Attack","abstract":"Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficiency, scalability and repair success rate. Different from existing DNN repair methods, our repair can generalize to general inputs, thereby improving the robustness of the neural network globally, which indicates a significant breakthrough in the generalization capability of ADVREPAIR.","sentences":["Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks.","Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability.","In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data.","By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood.","Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs.","ADVREPAIR demonstrates superior efficiency, scalability and repair success rate.","Different from existing DNN repair methods, our repair can generalize to general inputs, thereby improving the robustness of the neural network globally, which indicates a significant breakthrough in the generalization capability of ADVREPAIR."],"url":"http://arxiv.org/abs/2404.01642v1","category":"cs.LG"}
{"created":"2024-04-02 05:13:39","title":"The impact of geopolitical risk on the international agricultural market: Empirical analysis based on the GJR-GARCH-MIDAS model","abstract":"The current international landscape is turbulent and unstable, with frequent outbreaks of geopolitical conflicts worldwide. Geopolitical risk has emerged as a significant threat to regional and global peace, stability, and economic prosperity, causing serious disruptions to the global food system and food security. Focusing on the international food market, this paper builds different dimensions of geopolitical risk measures based on the random matrix theory and constructs single- and two-factor GJR-GARCH-MIDAS models with fixed time span and rolling window, respectively, to investigate the impact of geopolitical risk on food market volatility. The findings indicate that modeling based on rolling window performs better in describing the overall volatility of the wheat, maize, soybean, and rice markets, and the two-factor models generally exhibit stronger explanatory power in most cases. In terms of short-term fluctuations, all four staple food markets demonstrate obvious volatility clustering and high volatility persistence, without significant asymmetry. Regarding long-term volatility, the realized volatility of wheat, maize, and soybean significantly exacerbates their long-run market volatility. Additionally, geopolitical risks of different dimensions show varying directions and degrees of effects in explaining the long-term market volatility of the four staple food commodities. This study contributes to the understanding of the macro-drivers of food market fluctuations, provides useful information for investment using agricultural futures, and offers valuable insights into maintaining the stable operation of food markets and safeguarding global food security.","sentences":["The current international landscape is turbulent and unstable, with frequent outbreaks of geopolitical conflicts worldwide.","Geopolitical risk has emerged as a significant threat to regional and global peace, stability, and economic prosperity, causing serious disruptions to the global food system and food security.","Focusing on the international food market, this paper builds different dimensions of geopolitical risk measures based on the random matrix theory and constructs single- and two-factor GJR-GARCH-MIDAS models with fixed time span and rolling window, respectively, to investigate the impact of geopolitical risk on food market volatility.","The findings indicate that modeling based on rolling window performs better in describing the overall volatility of the wheat, maize, soybean, and rice markets, and the two-factor models generally exhibit stronger explanatory power in most cases.","In terms of short-term fluctuations, all four staple food markets demonstrate obvious volatility clustering and high volatility persistence, without significant asymmetry.","Regarding long-term volatility, the realized volatility of wheat, maize, and soybean significantly exacerbates their long-run market volatility.","Additionally, geopolitical risks of different dimensions show varying directions and degrees of effects in explaining the long-term market volatility of the four staple food commodities.","This study contributes to the understanding of the macro-drivers of food market fluctuations, provides useful information for investment using agricultural futures, and offers valuable insights into maintaining the stable operation of food markets and safeguarding global food security."],"url":"http://arxiv.org/abs/2404.01641v1","category":"econ.EM"}
{"created":"2024-04-02 05:09:33","title":"Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk","abstract":"This paper presents a deterministic search algorithm on complete bipartite graphs. Our algorithm adopts the simple form of alternating iterations of an oracle and a continuous-time quantum walk operator, which is a generalization of Grover's search algorithm. We address the most general case of multiple marked states, so there is a problem of estimating the number of marked states. To this end, we construct a quantum counting algorithm based on the spectrum structure of the search operator. To implement the continuous-time quantum walk operator, we perform Hamiltonian simulation in the quantum circuit model. We achieve simulation in constant time, that is, the complexity of the quantum circuit does not scale with the evolution time. Besides, deterministic search serves as a simple tool for perfect state transfer (PST). As an application, we explore the problem of PST on complete bipartite graphs.","sentences":["This paper presents a deterministic search algorithm on complete bipartite graphs.","Our algorithm adopts the simple form of alternating iterations of an oracle and a continuous-time quantum walk operator, which is a generalization of Grover's search algorithm.","We address the most general case of multiple marked states, so there is a problem of estimating the number of marked states.","To this end, we construct a quantum counting algorithm based on the spectrum structure of the search operator.","To implement the continuous-time quantum walk operator, we perform Hamiltonian simulation in the quantum circuit model.","We achieve simulation in constant time, that is, the complexity of the quantum circuit does not scale with the evolution time.","Besides, deterministic search serves as a simple tool for perfect state transfer (PST).","As an application, we explore the problem of PST on complete bipartite graphs."],"url":"http://arxiv.org/abs/2404.01640v1","category":"quant-ph"}
{"created":"2024-04-02 04:33:03","title":"Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning","abstract":"Given the widespread use of safety-critical applications in the automotive field, it is crucial to ensure the Functional Safety (FuSa) of circuits and components within automotive systems. The Analog and Mixed-Signal (AMS) circuits prevalent in these systems are more vulnerable to faults induced by parametric perturbations, noise, environmental stress, and other factors, in comparison to their digital counterparts. However, their continuous signal characteristics present an opportunity for early anomaly detection, enabling the implementation of safety mechanisms to prevent system failure. To address this need, we propose a novel framework based on unsupervised machine learning for early anomaly detection in AMS circuits. The proposed approach involves injecting anomalies at various circuit locations and individual components to create a diverse and comprehensive anomaly dataset, followed by the extraction of features from the observed circuit signals. Subsequently, we employ clustering algorithms to facilitate anomaly detection. Finally, we propose a time series framework to enhance and expedite anomaly detection performance. Our approach encompasses a systematic analysis of anomaly abstraction at multiple levels pertaining to the automotive domain, from hardware- to block-level, where anomalies are injected to create diverse fault scenarios. By monitoring the system behavior under these anomalous conditions, we capture the propagation of anomalies and their effects at different abstraction levels, thereby potentially paving the way for the implementation of reliable safety mechanisms to ensure the FuSa of automotive SoCs. Our experimental findings indicate that our approach achieves 100% anomaly detection accuracy and significantly optimizes the associated latency by 5X, underscoring the effectiveness of our devised solution.","sentences":["Given the widespread use of safety-critical applications in the automotive field, it is crucial to ensure the Functional Safety (FuSa) of circuits and components within automotive systems.","The Analog and Mixed-Signal (AMS) circuits prevalent in these systems are more vulnerable to faults induced by parametric perturbations, noise, environmental stress, and other factors, in comparison to their digital counterparts.","However, their continuous signal characteristics present an opportunity for early anomaly detection, enabling the implementation of safety mechanisms to prevent system failure.","To address this need, we propose a novel framework based on unsupervised machine learning for early anomaly detection in AMS circuits.","The proposed approach involves injecting anomalies at various circuit locations and individual components to create a diverse and comprehensive anomaly dataset, followed by the extraction of features from the observed circuit signals.","Subsequently, we employ clustering algorithms to facilitate anomaly detection.","Finally, we propose a time series framework to enhance and expedite anomaly detection performance.","Our approach encompasses a systematic analysis of anomaly abstraction at multiple levels pertaining to the automotive domain, from hardware- to block-level, where anomalies are injected to create diverse fault scenarios.","By monitoring the system behavior under these anomalous conditions, we capture the propagation of anomalies and their effects at different abstraction levels, thereby potentially paving the way for the implementation of reliable safety mechanisms to ensure the FuSa of automotive SoCs.","Our experimental findings indicate that our approach achieves 100% anomaly detection accuracy and significantly optimizes the associated latency by 5X, underscoring the effectiveness of our devised solution."],"url":"http://arxiv.org/abs/2404.01632v1","category":"cs.LG"}
{"created":"2024-04-02 04:13:09","title":"Ab initio extended Hubbard model of short polyenes for efficient quantum computing","abstract":"We propose introducing an extended Hubbard Hamiltonian derived via the ab initio downfolding method, which was originally formulated for periodic materials, towards efficient quantum computing of molecular electronic structure calculations. By utilizing this method, the first-principles Hamiltonian of chemical systems can be coarse-grained by eliminating the electronic degrees of freedom in higher energy space and reducing the number of terms of electron repulsion integral from $\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. Our approach is validated numerically on the vertical excitation energies and excitation characters of ethylene, butadiene, and hexatriene. The dynamical electron correlation is incorporated within the framework of the constrained random phase approximation in advance of quantum computations, and the constructed models capture the trend of experimental and high-level quantum chemical calculation results. As expected, the $L^1$-norm of the fermion-to-qubit mapped model Hamiltonians is significantly lower than that of conventional ab initio Hamiltonians, suggesting improved scalability of quantum computing. Those numerical outcomes and the results of the simulation of excited-state sampling demonstrate that the ab initio extended Hubbard Hamiltonian may hold significant potential for quantum chemical calculations using quantum computers.","sentences":["We propose introducing an extended Hubbard Hamiltonian derived via the ab initio downfolding method, which was originally formulated for periodic materials, towards efficient quantum computing of molecular electronic structure calculations.","By utilizing this method, the first-principles Hamiltonian of chemical systems can be coarse-grained by eliminating the electronic degrees of freedom in higher energy space and reducing the number of terms of electron repulsion integral from $\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. Our approach is validated numerically on the vertical excitation energies and excitation characters of ethylene, butadiene, and hexatriene.","The dynamical electron correlation is incorporated within the framework of the constrained random phase approximation in advance of quantum computations, and the constructed models capture the trend of experimental and high-level quantum chemical calculation results.","As expected, the $L^1$-norm of the fermion-to-qubit mapped model Hamiltonians is significantly lower than that of conventional ab initio Hamiltonians, suggesting improved scalability of quantum computing.","Those numerical outcomes and the results of the simulation of excited-state sampling demonstrate that the ab initio extended Hubbard Hamiltonian may hold significant potential for quantum chemical calculations using quantum computers."],"url":"http://arxiv.org/abs/2404.01623v1","category":"quant-ph"}
{"created":"2024-04-02 04:09:10","title":"The Fourier Transform and Characteristic Cycles of Monodromic $\\ell$-adic Sheaves","abstract":"Brylinski and Malgrange proved in 1986 that, for a coherent monodromic algebraic D-module on a finite dimensional vector space over the complex numbers, its characteristic cycle is canonically identified with the characteristic cycle of its Fourier transform. We prove the exact analogue of this in the $\\ell$-adic context.","sentences":["Brylinski and Malgrange proved in 1986 that, for a coherent monodromic algebraic D-module on a finite dimensional vector space over the complex numbers, its characteristic cycle is canonically identified with the characteristic cycle of its Fourier transform.","We prove the exact analogue of this in the $\\ell$-adic context."],"url":"http://arxiv.org/abs/2404.01621v1","category":"math.AG"}
{"created":"2024-04-02 04:01:31","title":"Making Privacy-preserving Federated Graph Analytics with Strong Guarantees Practical (for Certain Queries)","abstract":"Privacy-preserving federated graph analytics is an emerging area of research. The goal is to run graph analytics queries over a set of devices that are organized as a graph while keeping the raw data on the devices rather than centralizing it. Further, no entity may learn any new information except for the final query result. For instance, a device may not learn a neighbor's data. The state-of-the-art prior work for this problem provides privacy guarantees for a broad set of queries in a strong threat model where the devices can be malicious. However, it imposes an impractical overhead: each device locally requires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per query. This paper presents Colo, a new, low-cost system for privacy-preserving federated graph analytics that requires minutes of cpu time and a few MiBs in network transfers, for a particular subset of queries. At the heart of Colo is a new secure computation protocol that enables a device to securely and efficiently evaluate a graph query in its local neighborhood while hiding device data, edge data, and topology data. An implementation and evaluation of Colo shows that for running a variety of COVID-19 queries over a population of 1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93 MiBs in network transfers - improvements of up to three orders of magnitude.","sentences":["Privacy-preserving federated graph analytics is an emerging area of research.","The goal is to run graph analytics queries over a set of devices that are organized as a graph while keeping the raw data on the devices rather than centralizing it.","Further, no entity may learn any new information except for the final query result.","For instance, a device may not learn a neighbor's data.","The state-of-the-art prior work for this problem provides privacy guarantees for a broad set of queries in a strong threat model where the devices can be malicious.","However, it imposes an impractical overhead: each device locally requires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per query.","This paper presents Colo, a new, low-cost system for privacy-preserving federated graph analytics that requires minutes of cpu time and a few MiBs in network transfers, for a particular subset of queries.","At the heart of Colo is a new secure computation protocol that enables a device to securely and efficiently evaluate a graph query in its local neighborhood while hiding device data, edge data, and topology data.","An implementation and evaluation of Colo shows that for running a variety of COVID-19 queries over a population of 1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93 MiBs in network transfers - improvements of up to three orders of magnitude."],"url":"http://arxiv.org/abs/2404.01619v1","category":"cs.CR"}
{"created":"2024-04-02 03:44:25","title":"Multi-Robot Collaborative Navigation with Formation Adaptation","abstract":"Multi-robot collaborative navigation is an essential ability where teamwork and synchronization are keys. In complex and uncertain environments, adaptive formation is vital, as rigid formations prove to be inadequate. The ability of robots to dynamically adjust their formation enables navigation through unpredictable spaces, maintaining cohesion, and effectively responding to environmental challenges. In this paper, we introduce a novel approach that uses bi-level learning framework. Specifically, we use graph learning at a high level for group coordination and reinforcement learning for individual navigation. We innovate by integrating a spring-damper model within the reinforcement learning reward mechanism, addressing the rigidity of traditional formation control methods. During execution, our approach enables a team of robots to successfully navigate challenging environments, maintain a desired formation shape, and dynamically adjust their formation scale based on environmental information. We conduct extensive experiments to evaluate our approach across three distinct formation scenarios in multi-robot navigation: circle, line, and wedge. Experimental results show that our approach achieves promising results and scalability on multi-robot navigation with formation adaptation.","sentences":["Multi-robot collaborative navigation is an essential ability where teamwork and synchronization are keys.","In complex and uncertain environments, adaptive formation is vital, as rigid formations prove to be inadequate.","The ability of robots to dynamically adjust their formation enables navigation through unpredictable spaces, maintaining cohesion, and effectively responding to environmental challenges.","In this paper, we introduce a novel approach that uses bi-level learning framework.","Specifically, we use graph learning at a high level for group coordination and reinforcement learning for individual navigation.","We innovate by integrating a spring-damper model within the reinforcement learning reward mechanism, addressing the rigidity of traditional formation control methods.","During execution, our approach enables a team of robots to successfully navigate challenging environments, maintain a desired formation shape, and dynamically adjust their formation scale based on environmental information.","We conduct extensive experiments to evaluate our approach across three distinct formation scenarios in multi-robot navigation: circle, line, and wedge.","Experimental results show that our approach achieves promising results and scalability on multi-robot navigation with formation adaptation."],"url":"http://arxiv.org/abs/2404.01618v1","category":"cs.RO"}
{"created":"2024-04-02 03:43:55","title":"LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models","abstract":"We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics. Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures. We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.","sentences":["We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics.","Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures.","We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms."],"url":"http://arxiv.org/abs/2404.01617v1","category":"cs.NI"}
{"created":"2024-04-02 03:42:28","title":"Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems","abstract":"Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.","sentences":["Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data.","At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining.","To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems.","Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training.","Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages.","Our system outperforms previous systems trained explicitly on all 102 languages.","We achieve a 10% absolute improvement in Recall@1 averaged across these languages.","Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data."],"url":"http://arxiv.org/abs/2404.01616v1","category":"cs.CL"}
{"created":"2024-04-02 03:34:39","title":"Identification of High-dimensional ARMA Models with Binary-Valued Observations","abstract":"This paper studies system identification of high-dimensional ARMA models with binary-valued observations. Compared with existing quantized identification of ARMA models, this problem is more challenging since the accessible information is much less. Different from the identification of FIR models with binary-valued observations, the prediction of original system output and the parameter both need to be estimated in ARMA models. We propose an online identification algorithm consisting of parameter estimation and prediction of original system output. The parameter estimation and the prediction of original output are strongly coupled but mutually reinforcing. By analyzing the two estimates at the same time instead of analyzing separately, we finally prove that the parameter estimate can converge to the true parameter with convergence rate $O(1/k)$ under certain conditions. Simulations are given to demonstrate the theoretical results.","sentences":["This paper studies system identification of high-dimensional ARMA models with binary-valued observations.","Compared with existing quantized identification of ARMA models, this problem is more challenging since the accessible information is much less.","Different from the identification of FIR models with binary-valued observations, the prediction of original system output and the parameter both need to be estimated in ARMA models.","We propose an online identification algorithm consisting of parameter estimation and prediction of original system output.","The parameter estimation and the prediction of original output are strongly coupled but mutually reinforcing.","By analyzing the two estimates at the same time instead of analyzing separately, we finally prove that the parameter estimate can converge to the true parameter with convergence rate $O(1/k)$ under certain conditions.","Simulations are given to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2404.01613v1","category":"math.OC"}
{"created":"2024-04-02 03:29:23","title":"Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo","abstract":"Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods. However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question. Existing works impose strong assumptions on the environment lights and objects' material, restricting the effectiveness in more general scenarios. Alternatively, some methods leverage supervised learning with intricate models while lacking interpretability, resulting in a biased estimation. In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an unsupervised method to tackle NaUPS in various environment lights and objects. The proposed method uses a novel setup that captures the object's images on a rotatable platform, which mitigates NaUPS's ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS's ambiguities. Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost. Experiments have shown that Spin-UP outperforms other supervised / unsupervised NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets. Codes and data are available at https://github.com/LMozart/CVPR2024-SpinUP.","sentences":["Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods.","However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question.","Existing works impose strong assumptions on the environment lights and objects' material, restricting the effectiveness in more general scenarios.","Alternatively, some methods leverage supervised learning with intricate models while lacking interpretability, resulting in a biased estimation.","In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an unsupervised method to tackle NaUPS in various environment lights and objects.","The proposed method uses a novel setup that captures the object's images on a rotatable platform, which mitigates NaUPS's ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS's ambiguities.","Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost.","Experiments have shown that Spin-UP outperforms other supervised / unsupervised NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets.","Codes and data are available at https://github.com/LMozart/CVPR2024-SpinUP."],"url":"http://arxiv.org/abs/2404.01612v1","category":"cs.CV"}
{"created":"2024-04-02 03:09:33","title":"Identifying the Largest RoCoF and Its Implications","abstract":"The rate of change of frequency (RoCoF) is a critical factor in ensuring frequency security, particularly in power systems with low inertia. Currently, most RoCoF security constrained optimal inertia dispatch methods and inertia market mechanisms predominantly rely on the center of inertia (COI) model. This model, however, does not account for the disparities in post-contingency frequency dynamics across different regions of a power system. Specifically, regional buses can exhibit significantly larger RoCoFs than that predicted by the system's COI, particularly in systems characterized by unevenly distributed inertia. In this letter, a post-contingency nodal RoCoF model is established, and the maximal initial RoCoF is further proven to occur at generator buses equipped with inertia, rather than at inertia-less load buses. This finding facilitates the development of the optimal nodal inertia dispatch method and the nodal inertia market mechanism in a convex and concise form. Our argument is further verified by the simulation results of the South East Australia power system under various scenarios.","sentences":["The rate of change of frequency (RoCoF) is a critical factor in ensuring frequency security, particularly in power systems with low inertia.","Currently, most RoCoF security constrained optimal inertia dispatch methods and inertia market mechanisms predominantly rely on the center of inertia (COI) model.","This model, however, does not account for the disparities in post-contingency frequency dynamics across different regions of a power system.","Specifically, regional buses can exhibit significantly larger RoCoFs than that predicted by the system's COI, particularly in systems characterized by unevenly distributed inertia.","In this letter, a post-contingency nodal RoCoF model is established, and the maximal initial RoCoF is further proven to occur at generator buses equipped with inertia, rather than at inertia-less load buses.","This finding facilitates the development of the optimal nodal inertia dispatch method and the nodal inertia market mechanism in a convex and concise form.","Our argument is further verified by the simulation results of the South East Australia power system under various scenarios."],"url":"http://arxiv.org/abs/2404.01609v1","category":"eess.SY"}
{"created":"2024-04-02 02:45:12","title":"What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks","abstract":"We study the capabilities of the transformer architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization. We show a transformer with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex tasks beyond our design. Numerical experiments corroborate our theoretical findings.","sentences":["We study the capabilities of the transformer architecture with varying depth.","Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization.","We show a transformer with only one attention layer can excel in memorization but falls short in other tasks.","Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers.","Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers.","This sheds light on studying more practical and complex tasks beyond our design.","Numerical experiments corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2404.01601v1","category":"cs.LG"}
{"created":"2024-04-02 17:53:28","title":"Robustly estimating heterogeneity in factorial data using Rashomon Partitions","abstract":"Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.","sentences":["Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates?","How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics?","Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool).","Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions.","Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science.","We develop an alternative perspective, called Rashomon Partition Sets (RPSs).","Each item in the RPS partitions the space of covariates using a tree-like geometry.","RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates.","This prior is the $\\ell_0$ prior, which we show is minimax optimal.","Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS.","We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS.","Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques.","We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance."],"url":"http://arxiv.org/abs/2404.02141v1","category":"stat.ME"}
{"created":"2024-04-02 17:40:29","title":"ViTamin: Designing Scalable Vision Models in the Vision-Language Era","abstract":"Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).","sentences":["Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community.","The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs.","However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder.","Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs.","Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased.","In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework.","We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes.","To this end, we introduce ViTamin, a new vision models tailored for VLMs.","ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme.","ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models.","When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B)."],"url":"http://arxiv.org/abs/2404.02132v1","category":"cs.CV"}
{"created":"2024-04-02 17:26:22","title":"Gas kinematics and dynamics of Carina Pillars: A case study of G287.76-0.87","abstract":"We study the kinematics of a pillar, namely G287.76-0.87, using three rotational lines of $^{12}$CO(5-4), $^{12}$CO(8-7), $^{12}$CO(11-10), and a fine structure line of [OI] $63\\,\\mu$m Southern Carina observed by SOFIA/GREAT. This pillar is irradiated by the associated massive star cluster Trumpler 16, which includes $\\eta$~Carina. Our analysis shows that the relative velocity of the pillar with respect to this ionization source is small, $\\sim 1\\,\\rm km\\,s^{-1}$, and the gas motion in the tail is more turbulent than in the head. We also performed analytical calculations to estimate the gas column density in local thermal equilibrium (LTE) conditions, which yields $N_{\\rm CO}$ as $(\\sim 0.2 -5)\\times 10^{17}\\,\\rm cm^{-2}$. We further constrain the gas's physical properties in non-LTE conditions using RADEX. The non-LTE estimations result in $n_{\\rm H_{2}} \\simeq 10^{5}\\,\\rm cm^{-3}$ and $N_{\\rm CO} \\simeq 10^{16}\\,\\rm cm^{-2}$. We found that the thermal pressure within the G287.76-0.87 pillar is sufficiently high to make it stable for the surrounding hot gas and radiation feedback if the winds are not active. While they are active, stellar winds from the clustered stars sculpt the surrounding molecular cloud into pillars within the giant bubble around $\\eta$~Carina.","sentences":["We study the kinematics of a pillar, namely G287.76-0.87, using three rotational lines of $^{12}$CO(5-4), $^{12}$CO(8-7), $^{12}$CO(11-10), and a fine structure line of [OI] $63\\,\\mu$m Southern Carina observed by SOFIA/GREAT.","This pillar is irradiated by the associated massive star cluster Trumpler 16, which includes $\\eta$~Carina.","Our analysis shows that the relative velocity of the pillar with respect to this ionization source is small, $\\sim 1\\,\\rm km\\,s^{-1}$, and the gas motion in the tail is more turbulent than in the head.","We also performed analytical calculations to estimate the gas column density in local thermal equilibrium (LTE) conditions, which yields $N_{\\rm CO}$ as $(\\sim 0.2 -5)\\times","10^{17}\\,\\rm cm^{-2}$.","We further constrain the gas's physical properties in non-LTE conditions using RADEX.","The non-LTE estimations result in $n_{\\rm H_{2}} \\simeq 10^{5}\\,\\rm cm^{-3}$ and $N_{\\rm CO} \\simeq 10^{16}\\,\\rm","cm^{-2}$.","We found that the thermal pressure within the G287.76-0.87 pillar is sufficiently high to make it stable for the surrounding hot gas and radiation feedback if the winds are not active.","While they are active, stellar winds from the clustered stars sculpt the surrounding molecular cloud into pillars within the giant bubble around $\\eta$~Carina."],"url":"http://arxiv.org/abs/2404.02119v1","category":"astro-ph.GA"}
{"created":"2024-04-02 17:13:04","title":"ImageNot: A contrast with ImageNet preserves model rankings","abstract":"We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or fine-tuning them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.","sentences":["We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects.","We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet.","This is true when training models from scratch or fine-tuning them.","Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets.","We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes.","Our work demonstrates a surprising degree of external validity in the relative performance of image classification models.","This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset."],"url":"http://arxiv.org/abs/2404.02112v1","category":"cs.LG"}
{"created":"2024-04-02 16:43:39","title":"High-dimensional covariance regression with application to co-expression QTL detection","abstract":"While covariance matrices have been widely studied in many scientific fields, relatively limited progress has been made on estimating conditional covariances that permits a large covariance matrix to vary with high-dimensional subject-level covariates. In this paper, we present a new sparse multivariate regression framework that models the covariance matrix as a function of subject-level covariates. In the context of co-expression quantitative trait locus (QTL) studies, our method can be used to determine if and how gene co-expressions vary with genetic variations. To accommodate high-dimensional responses and covariates, we stipulate a combined sparsity structure that encourages covariates with non-zero effects and edges that are modulated by these covariates to be simultaneously sparse. We approach parameter estimation with a blockwise coordinate descent algorithm, and investigate the $\\ell_2$ convergence rate of the estimated parameters. In addition, we propose a computationally efficient debiased inference procedure for uncertainty quantification. The efficacy of the proposed method is demonstrated through numerical experiments and an application to a gene co-expression network study with brain cancer patients.","sentences":["While covariance matrices have been widely studied in many scientific fields, relatively limited progress has been made on estimating conditional covariances that permits a large covariance matrix to vary with high-dimensional subject-level covariates.","In this paper, we present a new sparse multivariate regression framework that models the covariance matrix as a function of subject-level covariates.","In the context of co-expression quantitative trait locus (QTL) studies, our method can be used to determine if and how gene co-expressions vary with genetic variations.","To accommodate high-dimensional responses and covariates, we stipulate a combined sparsity structure that encourages covariates with non-zero effects and edges that are modulated by these covariates to be simultaneously sparse.","We approach parameter estimation with a blockwise coordinate descent algorithm, and investigate the $\\ell_2$ convergence rate of the estimated parameters.","In addition, we propose a computationally efficient debiased inference procedure for uncertainty quantification.","The efficacy of the proposed method is demonstrated through numerical experiments and an application to a gene co-expression network study with brain cancer patients."],"url":"http://arxiv.org/abs/2404.02093v1","category":"stat.ME"}
{"created":"2024-04-02 16:31:07","title":"On the model uncertainties for the predicted muon content of extensive air showers","abstract":"Motivated by the excess of the muon content of cosmic ray induced extensive air showers (EAS), relative to EAS modeling, observed by the Pierre Auger Observatory, and by the tension between Auger data and air shower simulations on the maximal muon production depth $X^{\\mu}_{\\max}$, we investigate the possibility to modify the corresponding EAS simulation results, within the Standard Model of particle physics. We start by specifying the kinematic range for secondary hadron production, which is of relevance for such predictions. We further investigate the impact on the predicted EAS muon number and on $X^{\\mu}_{\\max}$ of various modifications of the treatment of hadronic interactions, in the framework of the QGSJET-III model, in particular the model calibration to accelerator data, the amount of the \"glue\" in the pion, and the energy dependence of the pion exchange process. None of the considered modifications of the model allowed us to enhance the EAS muon content by more than 10\\%. On the other hand, for the maximal muon production depth, some of the studied modifications of particle production give rise up to $\\sim 10$ g/cm$^2$ larger $X^{\\mu}_{\\max}$ values, which increases the difference with Auger observations.","sentences":["Motivated by the excess of the muon content of cosmic ray induced extensive air showers (EAS), relative to EAS modeling, observed by the Pierre Auger Observatory, and by the tension between Auger data and air shower simulations on the maximal muon production depth $X^{\\mu}_{\\max}$, we investigate the possibility to modify the corresponding EAS simulation results, within the Standard Model of particle physics.","We start by specifying the kinematic range for secondary hadron production, which is of relevance for such predictions.","We further investigate the impact on the predicted EAS muon number and on $X^{\\mu}_{\\max}$ of various modifications of the treatment of hadronic interactions, in the framework of the QGSJET-III model, in particular the model calibration to accelerator data, the amount of the \"glue\" in the pion, and the energy dependence of the pion exchange process.","None of the considered modifications of the model allowed us to enhance the EAS muon content by more than 10\\%.","On the other hand, for the maximal muon production depth, some of the studied modifications of particle production give rise up to $\\sim 10$ g/cm$^2$ larger $X^{\\mu}_{\\max}$ values, which increases the difference with Auger observations."],"url":"http://arxiv.org/abs/2404.02085v1","category":"hep-ph"}
{"created":"2024-04-02 16:29:28","title":"A Stabilized Parametric Finite Element Method for Surface Diffusion with an Arbitrary Surface Energy","abstract":"We proposed a structure-preserving stabilized parametric finite element method (SPFEM) for the evolution of closed curves under anisotropic surface diffusion with an arbitrary surface energy $\\hat{\\gamma}(\\theta)$. By introducing a non-negative stabilizing function $k(\\theta)$ depending on $\\hat{\\gamma}(\\theta)$, we obtained a novel stabilized conservative weak formulation for the anisotropic surface diffusion. A SPFEM is presented for the discretization of this weak formulation. We construct a comprehensive framework to analyze and prove the unconditional energy stability of the SPFEM under a very mild condition on $\\hat{\\gamma}(\\theta)$. This method can be applied to simulate solid-state dewetting of thin films with arbitrary surface energies, which are characterized by anisotropic surface diffusion and contact line migration. Extensive numerical results are reported to demonstrate the efficiency, accuracy and structure-preserving properties of the proposed SPFEM with anisotropic surface energies $\\hat{\\gamma}(\\theta)$ arising from different applications.","sentences":["We proposed a structure-preserving stabilized parametric finite element method (SPFEM) for the evolution of closed curves under anisotropic surface diffusion with an arbitrary surface energy $\\hat{\\gamma}(\\theta)$. By introducing a non-negative stabilizing function $k(\\theta)$ depending on $\\hat{\\gamma}(\\theta)$, we obtained a novel stabilized conservative weak formulation for the anisotropic surface diffusion.","A SPFEM is presented for the discretization of this weak formulation.","We construct a comprehensive framework to analyze and prove the unconditional energy stability of the SPFEM under a very mild condition on $\\hat{\\gamma}(\\theta)$. This method can be applied to simulate solid-state dewetting of thin films with arbitrary surface energies, which are characterized by anisotropic surface diffusion and contact line migration.","Extensive numerical results are reported to demonstrate the efficiency, accuracy and structure-preserving properties of the proposed SPFEM with anisotropic surface energies $\\hat{\\gamma}(\\theta)$ arising from different applications."],"url":"http://arxiv.org/abs/2404.02083v1","category":"math.NA"}
{"created":"2024-04-02 16:19:46","title":"Granular aqueous suspensions with controlled inter-particular friction and adhesion","abstract":"We present a simple route to obtain large quantities of suspensions of non-Brownian particles with stimuli-responsive surface properties to study the relation between their flow and interparticle inter- actions. We perform an alkaline hydrolysis reaction on poly(methyl methacrylate) (PMMA) particles to obtain poly(sodium methacrylate) (PMAA-Na) particles. We characterize the quasi-static macro- scopic frictional response of their aqueous suspensions using a rotating drum. The suspensions are frictionless when the particles are dispersed in pure water. We relate this state to the presence of elec- trosteric repulsion between the charged surfaces of the ionized PMAA-Na particles in water. Then we add monovalent and multivalent ions (Na+, Ca2+, La3+) and we observe that the suspensions become frictional whatever the valency. For divalent and trivalent ions, the quasi-static avalanche angle {\\theta}c at large ionic strength is greater than that of frictional PMMA particles in water, suggest- ing the presence of adhesion. Finally, a decrease in the pH of the suspending solution leads to a transition between a frictionless plateau and a frictional one. We perform Atomic Force Microscopy (AFM) to relate our macroscopic observations to the surface features of the particles. In particular, we show that the increase in friction in the presence of multivalent ions or under acidic conditions is driven by a nanoscopic phase separation and the bundling of polyelectrolyte chains at the surface of the particle. Our results highlight the importance of surface interactions in the rheology of gran- ular suspensions. Our particles provide a simple, yet flexible platform to study frictional suspension flows.","sentences":["We present a simple route to obtain large quantities of suspensions of non-Brownian particles with stimuli-responsive surface properties to study the relation between their flow and interparticle inter- actions.","We perform an alkaline hydrolysis reaction on poly(methyl methacrylate) (PMMA) particles to obtain poly(sodium methacrylate) (PMAA-Na) particles.","We characterize the quasi-static macro- scopic frictional response of their aqueous suspensions using a rotating drum.","The suspensions are frictionless when the particles are dispersed in pure water.","We relate this state to the presence of elec- trosteric repulsion between the charged surfaces of the ionized PMAA-Na particles in water.","Then we add monovalent and multivalent ions (Na+, Ca2+, La3+) and we observe that the suspensions become frictional whatever the valency.","For divalent and trivalent ions, the quasi-static avalanche angle {\\theta}c at large ionic strength is greater than that of frictional PMMA particles in water, suggest- ing the presence of adhesion.","Finally, a decrease in the pH of the suspending solution leads to a transition between a frictionless plateau and a frictional one.","We perform Atomic Force Microscopy (AFM) to relate our macroscopic observations to the surface features of the particles.","In particular, we show that the increase in friction in the presence of multivalent ions or under acidic conditions is driven by a nanoscopic phase separation and the bundling of polyelectrolyte chains at the surface of the particle.","Our results highlight the importance of surface interactions in the rheology of gran- ular suspensions.","Our particles provide a simple, yet flexible platform to study frictional suspension flows."],"url":"http://arxiv.org/abs/2404.02071v1","category":"cond-mat.soft"}
{"created":"2024-04-02 16:06:20","title":"Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation","abstract":"Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data. Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data. However, unreliable pseudo-labeling can undermine the semi-supervision processes. In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels. With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations. We design an end-to-end network to train and perform this effective label corrections mechanism. Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets. Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols.","sentences":["Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data.","Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data.","However, unreliable pseudo-labeling can undermine the semi-supervision processes.","In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels.","Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels.","With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations.","We design an end-to-end network to train and perform this effective label corrections mechanism.","Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets.","Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols."],"url":"http://arxiv.org/abs/2404.02065v1","category":"cs.CV"}
{"created":"2024-04-02 16:01:01","title":"SRG/ART-XC Galactic Bulge deep survey. I. Maximum likelihood source detection algorithm for X-ray surveys","abstract":"We describe an X-ray source detection method entirely based on the maximum likelihood analysis, in application to observations with the ART-XC telescope onboard the Spectrum Roentgen Gamma observatory. The method optimally combines the data taken at different conditions, a situation commonly found in scanning surveys or mosaic observations with a telescope with a significant off-axis PSF distortion. The method can be naturally extended to include additional information from the X-ray photon energies, detector grades, etc. The likelihood-based source detection naturally results in a stable and uniform definition of detection thresholds under different observing conditions (PSF, background level). This greatly simplifies the statistical calibration of the survey needed to, e.g., obtain the $\\log N - \\log S$ distribution of detected sources or their luminosity function. The method can be applied to the data from any imaging X-ray telescope.","sentences":["We describe an X-ray source detection method entirely based on the maximum likelihood analysis, in application to observations with the ART-XC telescope onboard the Spectrum Roentgen Gamma observatory.","The method optimally combines the data taken at different conditions, a situation commonly found in scanning surveys or mosaic observations with a telescope with a significant off-axis PSF distortion.","The method can be naturally extended to include additional information from the X-ray photon energies, detector grades, etc.","The likelihood-based source detection naturally results in a stable and uniform definition of detection thresholds under different observing conditions (PSF, background level).","This greatly simplifies the statistical calibration of the survey needed to, e.g., obtain the $\\log N - \\log S$ distribution of detected sources or their luminosity function.","The method can be applied to the data from any imaging X-ray telescope."],"url":"http://arxiv.org/abs/2404.02061v1","category":"astro-ph.HE"}
{"created":"2024-04-02 15:37:55","title":"Combining magneto-hydrostatic constraints with Stokes profile inversions. IV. Imposing $\\nabla\\cdot{\\bf B}=0$ condition","abstract":"Inferences of the magnetic field in the solar atmosphere by means of spectropolarimetric inversions (i.e., Stokes inversion codes) yield magnetic fields that are non-solenoidal($\\nabla\\cdot{\\bf B} \\ne 0$). Because of this, results obtained by such methods are sometimes put into question. We aim to develop and implement a new technique that can retrieve magnetic fields that are simultaneously consistent with observed polarization signals and with the null divergence condition. The method used in this work strictly imposes $\\nabla\\cdot{\\bf B}=0$ by determining the vertical component of the magnetic field ($B_{\\rm z}$) from the horizontal ones ($B_{\\rm x},B_{\\rm y}$). We implement this solenoidal inversion into the FIRTEZ Stokes inversion code and apply it to spectropolarimetric observations of a sunspot observed with the Hinode/SP instrument. We show that the solenoidal inversion retrieves a vertical component of the magnetic field that is consistent with the vertical component of the magnetic field inferred from the non-solenoidal one. We demonstrate that the solenoidal inversion is capable of a better overall fitting to the observed Stokes vector than the non-solenoidal inversion. In fact, the solenoidal magnetic field fits Stokes $V$ worse, but this is compensated by a better fit to Stokes $I$. We find a direct correlation between the worsening in the fit to the circular polarization profiles by the solenoidal inversion and the deviations in the inferred $B_{\\rm z}$ with respect to the non-solenoidal inversion. These results support the idea that common Stokes inversion techniques fail to reproduce $\\nabla\\cdot{\\bf B}=0$ mainly as a consequence of the uncertainties in the determination of the individual components of the magnetic field.","sentences":["Inferences of the magnetic field in the solar atmosphere by means of spectropolarimetric inversions (i.e., Stokes inversion codes) yield magnetic fields that are non-solenoidal($\\nabla\\cdot{\\bf B} \\ne 0$).","Because of this, results obtained by such methods are sometimes put into question.","We aim to develop and implement a new technique that can retrieve magnetic fields that are simultaneously consistent with observed polarization signals and with the null divergence condition.","The method used in this work strictly imposes $\\nabla\\cdot{\\bf B}=0$ by determining the vertical component of the magnetic field ($B_{\\rm z}$) from the horizontal ones ($B_{\\rm x},B_{\\rm y}$).","We implement this solenoidal inversion into the FIRTEZ Stokes inversion code and apply it to spectropolarimetric observations of a sunspot observed with the Hinode/SP instrument.","We show that the solenoidal inversion retrieves a vertical component of the magnetic field that is consistent with the vertical component of the magnetic field inferred from the non-solenoidal one.","We demonstrate that the solenoidal inversion is capable of a better overall fitting to the observed Stokes vector than the non-solenoidal inversion.","In fact, the solenoidal magnetic field fits Stokes $V$ worse, but this is compensated by a better fit to Stokes $I$.","We find a direct correlation between the worsening in the fit to the circular polarization profiles by the solenoidal inversion and the deviations in the inferred $B_{\\rm z}$ with respect to the non-solenoidal inversion.","These results support the idea that common Stokes inversion techniques fail to reproduce $\\nabla\\cdot{\\bf B}=0$ mainly as a consequence of the uncertainties in the determination of the individual components of the magnetic field."],"url":"http://arxiv.org/abs/2404.02045v1","category":"astro-ph.SR"}
{"created":"2024-04-02 15:05:39","title":"Combs, Causality and Contractions in Atomic Markov Categories","abstract":"We present a counterexample showing that Markov categories with conditionals (such as BorelStoch) need not validate a natural scheme of axioms which we call contraction identities. These identities hold in every traced monoidal category, so in particular this shows that BorelStoch cannot be embedded in any traced monoidal category. We remedy this under the additional assumption of atomicity: Atomic Markov categories validate all contraction identities, and furthermore admit a notion of trace defined for non-signalling morphisms. We conclude that atomic Markov categories admit an intrinsic calculus of combs without having to assume an embedding into compact-closed categories.","sentences":["We present a counterexample showing that Markov categories with conditionals (such as BorelStoch) need not validate a natural scheme of axioms which we call contraction identities.","These identities hold in every traced monoidal category, so in particular this shows that BorelStoch cannot be embedded in any traced monoidal category.","We remedy this under the additional assumption of atomicity: Atomic Markov categories validate all contraction identities, and furthermore admit a notion of trace defined for non-signalling morphisms.","We conclude that atomic Markov categories admit an intrinsic calculus of combs without having to assume an embedding into compact-closed categories."],"url":"http://arxiv.org/abs/2404.02017v1","category":"math.CT"}
{"created":"2024-04-02 14:49:20","title":"Uncovering the Mechanism of Chiral Three-Nucleon Force in Driving Spin-Orbit Splitting","abstract":"The three-nucleon force (3NF) is crucial in shaping the shell structure of atomic nuclei, particularly impacting the enhancement of spin-orbit (SO) splitting, especially in nuclei with significant deviations from stability. Despite its importance, the specific mechanisms driving this enhancement remain unclear. In this study, we introduce a decomposition scheme based on the rank of irreducible tensors forming the 3NF, derived from chiral effective field theory at next-to-next-to-leading order, to elucidate their influence on SO splitting. Within the shell-model framework, our analysis reveals that the rank-1 component of the 3NF is the primary factor enlarging the energy gap between the $0p_{3/2}$ and $0p_{1/2}$ single-particle levels in $p$-shell nuclei, while the rank-2 component makes a subdominant contribution. We also remark on the antisymmetry of the rank-1 3NF, as a means to clarify the disentanglement of quantum spin states. This study lays the groundwork for further exploration into this field toward a microscopic understanding of the 3NF impact on the nuclear shell structure.","sentences":["The three-nucleon force (3NF) is crucial in shaping the shell structure of atomic nuclei, particularly impacting the enhancement of spin-orbit (SO) splitting, especially in nuclei with significant deviations from stability.","Despite its importance, the specific mechanisms driving this enhancement remain unclear.","In this study, we introduce a decomposition scheme based on the rank of irreducible tensors forming the 3NF, derived from chiral effective field theory at next-to-next-to-leading order, to elucidate their influence on SO splitting.","Within the shell-model framework, our analysis reveals that the rank-1 component of the 3NF is the primary factor enlarging the energy gap between the $0p_{3/2}$ and $0p_{1/2}$ single-particle levels in $p$-shell nuclei, while the rank-2 component makes a subdominant contribution.","We also remark on the antisymmetry of the rank-1 3NF, as a means to clarify the disentanglement of quantum spin states.","This study lays the groundwork for further exploration into this field toward a microscopic understanding of the 3NF impact on the nuclear shell structure."],"url":"http://arxiv.org/abs/2404.02007v1","category":"nucl-th"}
{"created":"2024-04-02 14:46:22","title":"Computable conditions for order-2 $CP$ symmetry in NHDM potentials","abstract":"We derive necessary and sufficient conditions for order-2 $CP$ ($CP2$) symmetry in $N$-Higgs-doublet potentials for $N>2$. The conditions, which are formulated as relations between vectors that transform under the adjoint representation of $\\mathsf{SU}(N)$ under a change of doublet basis, are representation theoretical in nature. Making use of Lie algebra and representation theory we devise an efficient, computable algorithm which may be applied to decide whether or not a given numerical potential is $CP2$ invariant.","sentences":["We derive necessary and sufficient conditions for order-2 $CP$ ($CP2$) symmetry in $N$-Higgs-doublet potentials for $N>2$. The conditions, which are formulated as relations between vectors that transform under the adjoint representation of $\\mathsf{SU}(N)$ under a change of doublet basis, are representation theoretical in nature.","Making use of Lie algebra and representation theory we devise an efficient, computable algorithm which may be applied to decide whether or not a given numerical potential is $CP2$ invariant."],"url":"http://arxiv.org/abs/2404.02004v1","category":"hep-ph"}
{"created":"2024-04-02 14:44:01","title":"A catalog of 1.58 million clusters of galaxies identified from the DESI Legacy Imaging Surveys","abstract":"Based on the DESI Legacy Imaging Surveys released data and available spectroscopic redshifts, we identify 1.58 million clusters of galaxies by searching for the overdensity of stellar mass distribution of galaxies within redshift slices around pre-selected massive galaxies, among which 877,806 clusters are found for the first time. The identified clusters have an equivalent mass of M_{500}> 0.47*10^{14}~Msolar with an uncertainty of 0.2 dex. The redshift distribution of clusters extends to z~1.5, and 338,841 clusters have spectroscopic redshifts. Our cluster sample includes most of the rich optical clusters in previous catalogs, more than 95% massive Sunyaev-Zeldovich clusters and 90% ROSAT and eROSITA X-ray clusters. From the light distributions of member galaxies, we derive the dynamical state parameters for 28,038 rich clusters and find no significant evolution of the dynamical state with redshift. We find that the stellar mass of the brightest cluster galaxies grows by a factor of 2 since z=1.","sentences":["Based on the DESI Legacy Imaging Surveys released data and available spectroscopic redshifts, we identify 1.58 million clusters of galaxies by searching for the overdensity of stellar mass distribution of galaxies within redshift slices around pre-selected massive galaxies, among which 877,806 clusters are found for the first time.","The identified clusters have an equivalent mass of M_{500}> 0.47*10^{14}~Msolar with an uncertainty of 0.2 dex.","The redshift distribution of clusters extends to z~1.5, and 338,841 clusters have spectroscopic redshifts.","Our cluster sample includes most of the rich optical clusters in previous catalogs, more than 95% massive Sunyaev-Zeldovich clusters and 90% ROSAT and eROSITA X-ray clusters.","From the light distributions of member galaxies, we derive the dynamical state parameters for 28,038 rich clusters and find no significant evolution of the dynamical state with redshift.","We find that the stellar mass of the brightest cluster galaxies grows by a factor of 2 since z=1."],"url":"http://arxiv.org/abs/2404.02002v1","category":"astro-ph.CO"}
{"created":"2024-04-02 14:43:41","title":"Liouville Theory: An Introduction to Rigorous Approaches","abstract":"In recent years, a surprisingly direct and simple rigorous understanding of quantum Liouville theory has developed. We aim here to make this material more accessible to physicists working on quantum field theory.","sentences":["In recent years, a surprisingly direct and simple rigorous understanding of quantum Liouville theory has developed.","We aim here to make this material more accessible to physicists working on quantum field theory."],"url":"http://arxiv.org/abs/2404.02001v1","category":"hep-th"}
{"created":"2024-04-02 14:35:08","title":"Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models","abstract":"Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.","sentences":["Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge.","One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects.","Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance.","Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence.","We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases.","These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations.","CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs.","Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts.","In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms."],"url":"http://arxiv.org/abs/2404.01992v1","category":"cs.CL"}
{"created":"2024-04-02 14:31:14","title":"Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal","abstract":"This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed speech dataset containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages. These resources are specifically designed for Automatic Speech Recognition purpose, including traditional approaches. To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset.","sentences":["This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture.","Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers.","However, such technologies are keys to the protection, promotion and teaching of these languages.","Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer.","These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country.","However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector.","We release a transcribed speech dataset containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages.","These resources are specifically designed for Automatic Speech Recognition purpose, including traditional approaches.","To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset."],"url":"http://arxiv.org/abs/2404.01991v1","category":"cs.CL"}
{"created":"2024-04-02 14:29:47","title":"Impact of The Newly Revised Gravitational Redshift of X-ray Burster GS 1826-24 on The Equation of State of Supradense Neutron-Rich Matter","abstract":"Thanks to the recent advancement in producing rare isotopes and measuring their masses with unprecedented precision, the updated nuclear masses around the waiting-point nucleus $^{64}$Ge in the rapid-proton capture process have led to a significant revision of the surface gravitational redshift of the neutron star (NS) in GS 1826-24 by re-fitting its X-ray burst light curve ({\\it X. Zhou et al., Nature Physics {\\bf 19}, 1091 (2023)}) using Modules for Experiments in Stellar Astrophysics (MESA). The resulting NS compactness $\\xi$ is between 0.244 and 0.342 at 95\\% confidence level and its upper boundary is significantly smaller than the maximum $\\xi$ previously known. Incorporating this new data within a comprehensive Bayesian statistical framework, we investigate its impact on the Equation of State (EOS) of supradense neutron-rich matter and the required spin frequency for GW190814's minor $m_2$ with mass $2.59\\pm 0.05$M$_{\\odot}$ to be a rotationally stable pulsar. We found that the EOS of high-density symmetric nuclear matter (SNM) has to be softened significantly while the symmetry energy at supersaturation densities stiffened compared to our prior knowledge from earlier analyses using data from both astrophysical observations and terrestrial nuclear experiments. In particular, the skewness $J_0$ characterizing the stiffness of high-density SNM decreases significantly, while the slope $L$, curvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ of nuclear symmetry energy all increase appreciably compared to their fiducial values. We also found that the most probable spin rate for the $m_2$ to be a stable pulsar is very close to its mass-shedding limit once the revised redshift data from GS 1826-24 is considered, making the $m_2$ unlikely the most massive NS observed so far.","sentences":["Thanks to the recent advancement in producing rare isotopes and measuring their masses with unprecedented precision, the updated nuclear masses around the waiting-point nucleus $^{64}$Ge in the rapid-proton capture process have led to a significant revision of the surface gravitational redshift of the neutron star (NS) in GS 1826-24 by re-fitting its X-ray burst light curve ({\\it X. Zhou et al., Nature Physics {\\bf 19}, 1091 (2023)}) using Modules for Experiments in Stellar Astrophysics (MESA).","The resulting NS compactness $\\xi$ is between 0.244 and 0.342 at 95\\% confidence level and its upper boundary is significantly smaller than the maximum $\\xi$ previously known.","Incorporating this new data within a comprehensive Bayesian statistical framework, we investigate its impact on the Equation of State (EOS) of supradense neutron-rich matter and the required spin frequency for GW190814's minor $m_2$ with mass $2.59\\pm 0.05$M$_{\\odot}$ to be a rotationally stable pulsar.","We found that the EOS of high-density symmetric nuclear matter (SNM) has to be softened significantly while the symmetry energy at supersaturation densities stiffened compared to our prior knowledge from earlier analyses using data from both astrophysical observations and terrestrial nuclear experiments.","In particular, the skewness $J_0$ characterizing the stiffness of high-density SNM decreases significantly, while the slope $L$, curvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ of nuclear symmetry energy all increase appreciably compared to their fiducial values.","We also found that the most probable spin rate for the $m_2$ to be a stable pulsar is very close to its mass-shedding limit once the revised redshift data from GS 1826-24 is considered, making the $m_2$ unlikely the most massive NS observed so far."],"url":"http://arxiv.org/abs/2404.01989v1","category":"astro-ph.HE"}
{"created":"2024-04-02 14:26:18","title":"Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection","abstract":"Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles. To address this problem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts. Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain. Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K, SHIFT, and ACDC datasets, respectively. Code is available at https://github.com/jichengyuan/Cooperitive_Students.","sentences":["Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles.","To address this problem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts.","Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain.","Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K, SHIFT, and ACDC datasets, respectively.","Code is available at https://github.com/jichengyuan/Cooperitive_Students."],"url":"http://arxiv.org/abs/2404.01988v1","category":"cs.CV"}
{"created":"2024-04-02 14:17:32","title":"Least Squares Inference for Data with Network Dependency","abstract":"We address the inference problem concerning regression coefficients in a classical linear regression model using least squares estimates. The analysis is conducted under circumstances where network dependency exists across units in the sample. Neglecting the dependency among observations may lead to biased estimation of the asymptotic variance and often inflates the Type I error in coefficient inference. In this paper, we first establish a central limit theorem for the ordinary least squares estimate, with a verifiable dependence condition alongside corresponding neighborhood growth conditions. Subsequently, we propose a consistent estimator for the asymptotic variance of the estimated coefficients, which employs a data-driven method to balance the bias-variance trade-off. We find that the optimal tuning depends on the linear hypothesis under consideration and must be chosen adaptively. The presented theory and methods are illustrated and supported by numerical experiments and a data example.","sentences":["We address the inference problem concerning regression coefficients in a classical linear regression model using least squares estimates.","The analysis is conducted under circumstances where network dependency exists across units in the sample.","Neglecting the dependency among observations may lead to biased estimation of the asymptotic variance and often inflates the Type I error in coefficient inference.","In this paper, we first establish a central limit theorem for the ordinary least squares estimate, with a verifiable dependence condition alongside corresponding neighborhood growth conditions.","Subsequently, we propose a consistent estimator for the asymptotic variance of the estimated coefficients, which employs a data-driven method to balance the bias-variance trade-off.","We find that the optimal tuning depends on the linear hypothesis under consideration and must be chosen adaptively.","The presented theory and methods are illustrated and supported by numerical experiments and a data example."],"url":"http://arxiv.org/abs/2404.01977v1","category":"stat.ME"}
{"created":"2024-04-02 13:57:30","title":"CAM-Based Methods Can See through Walls","abstract":"CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.","sentences":["CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model.","The saliency map highlights the important areas of the image relevant to the prediction.","In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see.","We show that this phenomenon occurs both theoretically and experimentally.","On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization.","Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image.","This behavior is evaluated quantitatively on two new datasets.","We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior."],"url":"http://arxiv.org/abs/2404.01964v1","category":"cs.CV"}
{"created":"2024-04-02 13:47:15","title":"Automatic Wood Pith Detector: Local Orientation Estimation and Robust Accumulation","abstract":"A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced. The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem. We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns. Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL). All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos). All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications. Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species. Dataset and source code are available at http://github.com/hmarichal93/apd.","sentences":["A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced.","The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem.","We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns.","Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL).","All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos).","All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications.","Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species.","Dataset and source code are available at http://github.com/hmarichal93/apd."],"url":"http://arxiv.org/abs/2404.01952v1","category":"cs.CV"}
{"created":"2024-04-02 13:43:08","title":"Quantifying Noise of Dynamic Vision Sensor","abstract":"Dynamic visual sensors (DVS) are characterized by a large amount of background activity (BA) noise, which it is mixed with the original (cleaned) sensor signal. The dynamic nature of the signal and the absence in practical application of the ground truth, it clearly makes difficult to distinguish between noise and the cleaned sensor signals using standard image processing techniques. In this letter, a new technique is presented to characterise BA noise derived from the Detrended Fluctuation Analysis (DFA). The proposed technique can be used to address an existing DVS issues, which is how to quantitatively characterised noise and signal without ground truth, and how to derive an optimal denoising filter parameters. The solution of the latter problem is demonstrated for the popular real moving-car dataset.","sentences":["Dynamic visual sensors (DVS) are characterized by a large amount of background activity (BA) noise, which it is mixed with the original (cleaned) sensor signal.","The dynamic nature of the signal and the absence in practical application of the ground truth, it clearly makes difficult to distinguish between noise and the cleaned sensor signals using standard image processing techniques.","In this letter, a new technique is presented to characterise BA noise derived from the Detrended Fluctuation Analysis (DFA).","The proposed technique can be used to address an existing DVS issues, which is how to quantitatively characterised noise and signal without ground truth, and how to derive an optimal denoising filter parameters.","The solution of the latter problem is demonstrated for the popular real moving-car dataset."],"url":"http://arxiv.org/abs/2404.01948v1","category":"cs.CV"}
{"created":"2024-04-02 13:41:22","title":"Event-assisted Low-Light Video Object Segmentation","abstract":"In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation. Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions. This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy. Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings. Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events. Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios.","sentences":["In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation.","Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions.","This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy.","Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings.","Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events.","Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios."],"url":"http://arxiv.org/abs/2404.01945v1","category":"cs.CV"}
{"created":"2024-04-02 13:36:03","title":"Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation","abstract":"Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.","sentences":["Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments.","At each navigation step, the agent selects from possible candidate locations and then makes the move.","For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations.","To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost.","To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction.","Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation.","Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.01943v1","category":"cs.CV"}
{"created":"2024-04-02 13:23:54","title":"Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies","abstract":"We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the \"maximal gain ratio\". We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results. In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller. This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization.","sentences":["We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains.","We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage.","We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them.","Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee.","In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior.","Moreover, we discover a new parameter of adaptive selection policies, which we term the \"maximal gain ratio\".","We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results.","In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller.","This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization."],"url":"http://arxiv.org/abs/2404.01930v1","category":"cs.LG"}
{"created":"2024-04-02 12:52:26","title":"Optimizing Offload Performance in Heterogeneous MPSoCs","abstract":"Heterogeneous multi-core architectures combine a few \"host\" cores, optimized for single-thread performance, with many small energy-efficient \"accelerator\" cores for data-parallel processing, on a single chip. Offloading a computation to the many-core acceleration fabric introduces a communication and synchronization cost which reduces the speedup attainable on the accelerator, particularly for small and fine-grained parallel tasks. We demonstrate that by co-designing the hardware and offload routines, we can increase the speedup of an offloaded DAXPY kernel by as much as 47.9%. Furthermore, we show that it is possible to accurately model the runtime of an offloaded application, accounting for the offload overheads, with as low as 1% MAPE error, enabling optimal offload decisions under offload execution time constraints.","sentences":["Heterogeneous multi-core architectures combine a few \"host\" cores, optimized for single-thread performance, with many small energy-efficient \"accelerator\" cores for data-parallel processing, on a single chip.","Offloading a computation to the many-core acceleration fabric introduces a communication and synchronization cost which reduces the speedup attainable on the accelerator, particularly for small and fine-grained parallel tasks.","We demonstrate that by co-designing the hardware and offload routines, we can increase the speedup of an offloaded DAXPY kernel by as much as 47.9%.","Furthermore, we show that it is possible to accurately model the runtime of an offloaded application, accounting for the offload overheads, with as low as 1% MAPE error, enabling optimal offload decisions under offload execution time constraints."],"url":"http://arxiv.org/abs/2404.01908v1","category":"cs.AR"}
{"created":"2024-04-02 12:49:03","title":"Nonlinear stability for active suspensions","abstract":"This paper is devoted to the nonlinear analysis of a kinetic model introduced by Saintillan and Shelley to describe suspensions of active rodlike particles in viscous flows. We investigate the stability of the constant state $\\Psi(t,x,p) = \\frac{1}{4\\pi} $ corresponding to a distribution of particles that is homogeneous in space (variable $x \\in \\mathbb{T}^3$) and uniform in orientation (variable $p \\in \\mathbb{S}^2$). We prove its nonlinear stability under the optimal condition of linearized spectral stability, without any addition of spatial diffusion. The mathematical novelty and difficulty compared to previous linear studies comes from the presence of a quasilinear term in $x$ due to nonlinear convection. A key feature of our work, which we hope to be of independent interest, is an analysis of enhanced dissipation and mixing properties of the advection diffusion operator $$\\partial_t + (p + u(t,x)) \\cdot \\nabla_x - \\nu \\Delta_p $$ on $\\mathbb{T}^3 \\times \\mathbb{S}^2$ for a given appropriately small vector field $u$.","sentences":["This paper is devoted to the nonlinear analysis of a kinetic model introduced by Saintillan and Shelley to describe suspensions of active rodlike particles in viscous flows.","We investigate the stability of the constant state $\\Psi(t,x,p) = \\frac{1}{4\\pi} $ corresponding to a distribution of particles that is homogeneous in space (variable $x \\in \\mathbb{T}^3$) and uniform in orientation (variable $p \\in \\mathbb{S}^2$).","We prove its nonlinear stability under the optimal condition of linearized spectral stability, without any addition of spatial diffusion.","The mathematical novelty and difficulty compared to previous linear studies comes from the presence of a quasilinear term in $x$ due to nonlinear convection.","A key feature of our work, which we hope to be of independent interest, is an analysis of enhanced dissipation and mixing properties of the advection diffusion operator $$\\partial_t + (p + u(t,x))","\\cdot \\nabla_x - \\nu \\Delta_p $$ on $\\mathbb{T}^3 \\times \\mathbb{S}^2$ for a given appropriately small vector field $u$."],"url":"http://arxiv.org/abs/2404.01906v1","category":"math.AP"}
{"created":"2024-04-02 12:44:44","title":"Activation Steering for Robust Type Prediction in CodeLLMs","abstract":"Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa. This result suggests that LLMs may be learning to transfer knowledge of types across programming languages.","sentences":["Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks.","However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints.","We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant.","Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction.","We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits.","In contrast, we construct steering vectors from semantics-preserving code edits.","We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript.","This approach corrects up to 90% of type mispredictions.","Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa.","This result suggests that LLMs may be learning to transfer knowledge of types across programming languages."],"url":"http://arxiv.org/abs/2404.01903v1","category":"cs.CL"}
{"created":"2024-04-02 12:38:55","title":"The glassy structure of reactive supplementary cementitious materials (SCMs) and recycled glass: Contribution of XRD and Raman spectroscopy to their characterization","abstract":"This study compares thirteen natural and industrial samples of supplementary or emerging supplementary cementitious materials (SCMs): slag, fly ashes, pozzolan, obsidian, silica fume, and recycled glass. These materials are used or are under consideration for decarbonization in cement plants. XRF, XRD and Raman microspectroscopy were used in order to achieve a deeper understanding of the structural characterization of SCMs. The changes in position and shape of the XRD diffuse halos were compared. Raman spectroscopy was used to study the glass part of the SCM families, to better understand their structure in terms of depolymerization degree, angle, ring size and incorporations into the glass.The chemical composition of each glassy part was also estimated using reverse Bogue calculations. The hump position is correlated with the Raman shift, and with the XRF bulk or with the calculated glass chemical composition of SCMs, in terms of CaO/(SiO2+Al2O3) or network modifiers to formers ratios.","sentences":["This study compares thirteen natural and industrial samples of supplementary or emerging supplementary cementitious materials (SCMs): slag, fly ashes, pozzolan, obsidian, silica fume, and recycled glass.","These materials are used or are under consideration for decarbonization in cement plants.","XRF, XRD and Raman microspectroscopy were used in order to achieve a deeper understanding of the structural characterization of SCMs.","The changes in position and shape of the XRD diffuse halos were compared.","Raman spectroscopy was used to study the glass part of the SCM families, to better understand their structure in terms of depolymerization degree, angle, ring size and incorporations into the glass.","The chemical composition of each glassy part was also estimated using reverse Bogue calculations.","The hump position is correlated with the Raman shift, and with the XRF bulk or with the calculated glass chemical composition of SCMs, in terms of CaO/(SiO2+Al2O3) or network modifiers to formers ratios."],"url":"http://arxiv.org/abs/2404.01899v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 12:29:04","title":"ASTRA: An Action Spotting TRAnsformer for Soccer Videos","abstract":"In this paper, we introduce ASTRA, a Transformer-based model designed for the task of Action Spotting in soccer matches. ASTRA addresses several challenges inherent in the task and dataset, including the requirement for precise action localization, the presence of a long-tail data distribution, non-visibility in certain actions, and inherent label noise. To do so, ASTRA incorporates (a) a Transformer encoder-decoder architecture to achieve the desired output temporal resolution and to produce precise predictions, (b) a balanced mixup strategy to handle the long-tail distribution of the data, (c) an uncertainty-aware displacement head to capture the label variability, and (d) input audio signal to enhance detection of non-visible actions. Results demonstrate the effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set. Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd position with an Average-mAP of 70.21 on the challenge set.","sentences":["In this paper, we introduce ASTRA, a Transformer-based model designed for the task of Action Spotting in soccer matches.","ASTRA addresses several challenges inherent in the task and dataset, including the requirement for precise action localization, the presence of a long-tail data distribution, non-visibility in certain actions, and inherent label noise.","To do so, ASTRA incorporates (a) a Transformer encoder-decoder architecture to achieve the desired output temporal resolution and to produce precise predictions, (b) a balanced mixup strategy to handle the long-tail distribution of the data, (c) an uncertainty-aware displacement head to capture the label variability, and (d) input audio signal to enhance detection of non-visible actions.","Results demonstrate the effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set.","Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd position with an Average-mAP of 70.21 on the challenge set."],"url":"http://arxiv.org/abs/2404.01891v1","category":"cs.CV"}
{"created":"2024-04-02 12:27:16","title":"Estimation of the atmospheric absorption profile with isotropic background events observed by Imaging Atmospheric Cherenkov Telescopes","abstract":"Atmospheric Cherenkov telescopes rely on the Earth's atmosphere as part of the detector. The presence of clouds affects observations and can introduce biases if not corrected for. Correction methods typically require an atmospheric profile, that can be measured with external atmospheric monitoring devices. We present a novel method for measuring the atmospheric profile using the data from Imaging Atmospheric Cherenkov telescopes directly. The method exploits the comparison of average longitudinal distributions of the registered Cherenkov light between clear atmosphere and cloud presence cases. Using Monte Carlo simulations of a subarray of four Large-Sized Telescopes of the upcoming Cherenkov Telescope Array Observatory and a simple cloud model we evaluate the accuracy of the method in determining the basic cloud parameters. We find that the method can reconstruct the transmission of typical clouds with an absolute accuracy of a few per cent. For low-zenith observations, the height of the cloud centre can be reconstructed with a typical accuracy of a few hundred metres, while the geometrical thickness can be accurately reconstructed only if it is >= 3 km. We also evaluate the robustness of the method against the typical systematic uncertainties affecting atmospheric Cherenkov telescopes.","sentences":["Atmospheric Cherenkov telescopes rely on the Earth's atmosphere as part of the detector.","The presence of clouds affects observations and can introduce biases if not corrected for.","Correction methods typically require an atmospheric profile, that can be measured with external atmospheric monitoring devices.","We present a novel method for measuring the atmospheric profile using the data from Imaging Atmospheric Cherenkov telescopes directly.","The method exploits the comparison of average longitudinal distributions of the registered Cherenkov light between clear atmosphere and cloud presence cases.","Using Monte Carlo simulations of a subarray of four Large-Sized Telescopes of the upcoming Cherenkov Telescope Array Observatory and a simple cloud model we evaluate the accuracy of the method in determining the basic cloud parameters.","We find that the method can reconstruct the transmission of typical clouds with an absolute accuracy of a few per cent.","For low-zenith observations, the height of the cloud centre can be reconstructed with a typical accuracy of a few hundred metres, while the geometrical thickness can be accurately reconstructed only if it is >= 3 km.","We also evaluate the robustness of the method against the typical systematic uncertainties affecting atmospheric Cherenkov telescopes."],"url":"http://arxiv.org/abs/2404.01888v1","category":"astro-ph.IM"}
{"created":"2024-04-02 11:30:22","title":"Pairwise Similarity Distribution Clustering for Noisy Label Learning","abstract":"Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels. Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process. In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf semi-supervised learning regimes to further train networks for different downstream tasks. Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sample can be confidently divided into the clean set or noisy set. Even under severe label noise rate, the resulting data partition mechanism has been proved to be more robust in judging the label confidence in both theory and practice. Experimental results on various benchmark datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate significant improvements over state-of-the-art methods.","sentences":["Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels.","Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process.","In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf semi-supervised learning regimes to further train networks for different downstream tasks.","Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sample can be confidently divided into the clean set or noisy set.","Even under severe label noise rate, the resulting data partition mechanism has been proved to be more robust in judging the label confidence in both theory and practice.","Experimental results on various benchmark datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate significant improvements over state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.01853v1","category":"cs.LG"}
{"created":"2024-04-02 10:44:55","title":"When does Subagging Work?","abstract":"We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform subagging if the size of its individual trees is not optimally chosen. This last result goes against common practice of growing large randomized trees to eliminate bias and then averaging to reduce variance.","sentences":["We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning.","First, we give sufficient conditions for pointwise consistency of trees.","We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance.","While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally.","Second, we compare the performance of subagging to that of trees across different numbers of splits.","We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits.","However, (3) a single tree grown at optimal size can outperform subagging if the size of its individual trees is not optimally chosen.","This last result goes against common practice of growing large randomized trees to eliminate bias and then averaging to reduce variance."],"url":"http://arxiv.org/abs/2404.01832v1","category":"stat.ML"}
{"created":"2024-04-02 10:42:44","title":"Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy","abstract":"We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.","sentences":["We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown.","The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy.","When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators.","When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound.","We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods."],"url":"http://arxiv.org/abs/2404.01830v1","category":"stat.ML"}
{"created":"2024-04-02 10:40:03","title":"Solving a Class of Nonconvex Quadratic Programs by Inertial DC Algorithms","abstract":"Two inertial DC algorithms for indefinite quadratic programs under linear constraints (IQPs) are considered in this paper. Using a qualification condition related to the normal cones of unbounded pseudo-faces of the polyhedral convex constraint set, the recession cones of the corresponding faces, and the quadratic form describing the objective function, we prove that the iteration sequences in question are bounded if the given IQP has a finite optimal value. Any cluster point of such a sequence is a KKT point. The convergence of the members of a DCA sequence produced by one of the two inertial algorithms to just one connected component of the KKT point set is also obtained. To do so, we revisit the inertial algorithm for DC programming of de Oliveira and Tcheou [de Oliveira, W., Tcheou, M.P.: An inertial algorithm for DC programming, Set-Valued and Variational Analysis 2019; 27: 895--919] and give a refined version of Theorem 1 from that paper, which can be used for IQPs with unbounded constraint sets. An illustrative example is proposed.","sentences":["Two inertial DC algorithms for indefinite quadratic programs under linear constraints (IQPs) are considered in this paper.","Using a qualification condition related to the normal cones of unbounded pseudo-faces of the polyhedral convex constraint set, the recession cones of the corresponding faces, and the quadratic form describing the objective function, we prove that the iteration sequences in question are bounded if the given IQP has a finite optimal value.","Any cluster point of such a sequence is a KKT point.","The convergence of the members of a DCA sequence produced by one of the two inertial algorithms to just one connected component of the KKT point set is also obtained.","To do so, we revisit the inertial algorithm for DC programming of de Oliveira and Tcheou [de Oliveira, W., Tcheou, M.P.: An inertial algorithm for DC programming, Set-Valued and Variational Analysis 2019; 27: 895--919] and give a refined version of Theorem 1 from that paper, which can be used for IQPs with unbounded constraint sets.","An illustrative example is proposed."],"url":"http://arxiv.org/abs/2404.01827v1","category":"math.OC"}
{"created":"2024-04-02 10:32:21","title":"A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection","abstract":"Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup. We make our code publicly available.","sentences":["Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets.","Yet, misinformation and hate speech continue to propagate on social media networks.","This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph.","In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach.","This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings.","We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures.","Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup.","We make our code publicly available."],"url":"http://arxiv.org/abs/2404.01822v1","category":"cs.LG"}
{"created":"2024-04-02 10:03:23","title":"EventSleep: Sleep Activity Recognition with Event Cameras","abstract":"Event cameras are a promising technology for activity recognition in dark environments due to their unique properties. However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications. We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis. The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments. Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications. Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures. Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments.","sentences":["Event cameras are a promising technology for activity recognition in dark environments due to their unique properties.","However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications.","We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis.","The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments.","Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications.","Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures.","Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments."],"url":"http://arxiv.org/abs/2404.01801v1","category":"cs.CV"}
{"created":"2024-04-02 09:58:57","title":"PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency","abstract":"Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses the aforementioned limitations, presenting a new direction for LLM benchmark research. Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing benchmarking practices. Third, we release 4 datasets to support measuring and comparing LLM proficiency in grade school mathematics and science against human populations.","sentences":["Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers.","While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?)","and unclear human population reference (e.g., To whom can the model be compared?).","In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking.","We make three primary contributions.","First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs.","PATCH addresses the aforementioned limitations, presenting a new direction for LLM benchmark research.","Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th grade mathematics against 56 human populations.","We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing benchmarking practices.","Third, we release 4 datasets to support measuring and comparing LLM proficiency in grade school mathematics and science against human populations."],"url":"http://arxiv.org/abs/2404.01799v1","category":"cs.CL"}
{"created":"2024-04-02 09:40:37","title":"Diversity in hydrogen-rich envelope mass of type II supernovae (I): V -band light curve modeling","abstract":"We conduct a systematic study on the light curves of type II supernovae (SNe II) to investigate how they are affected by the physical properties of the progenitor and the nature of the explosion. The calculations of pre-supernova evolution, the launch of the explosion, and the light curve are carried out by \\texttt{MESA} and \\texttt{STELLA}. Our study encompasses a wide range of zero-age-main-sequence (ZAMS) masses (10 - 20 $M_{\\rm \\odot}$), wind mass-loss rates (which control the range of hydrogen-rich envelope mass to be 3 - 12 $M_{\\rm \\odot}$), explosion energies (0.1 - 2.5 $\\times$ 10$^{51}$ erg), and $^{56}$Ni masses (0 - 0.15 $M_{\\rm \\odot}$). Our analyses reveal that when the explosion energy is fixed, the light curve is primarily determined by the properties of the hydrogen-rich envelope, despite the large variation in the ZAMS masses. The dependencies of the light-curve characteristics on the envelope mass, radius, and explosion energy are investigated, and we establish the scaling relations connecting these quantities. We further derive a formula that corrects for effects of the $^{56}$Ni heating based on observables. Using the derived equations, we develop a method that can constrain the envelope mass with uncertainty within 1 $M_{\\rm \\odot}$ based on photometry. This method is applied to a large sample of $\\sim$ 100 SNe II, which reveals a considerably broader range of the envelope masses estimated from observables as compared to those predicted by single star models evolving with standard stellar wind; this finding indicates that a large fraction of SNe II experience substantial mass-loss beyond the standard mass-loss prescription prior to their explosions, highlighting the uncertainties involved in the massive star evolution and pre-SN mass-loss mechanism.","sentences":["We conduct a systematic study on the light curves of type II supernovae (SNe II) to investigate how they are affected by the physical properties of the progenitor and the nature of the explosion.","The calculations of pre-supernova evolution, the launch of the explosion, and the light curve are carried out by \\texttt{MESA} and \\texttt{STELLA}.","Our study encompasses a wide range of zero-age-main-sequence (ZAMS) masses (10 - 20 $M_{\\rm \\odot}$), wind mass-loss rates (which control the range of hydrogen-rich envelope mass to be 3 - 12 $M_{\\rm \\odot}$), explosion energies (0.1 - 2.5 $\\times$ 10$^{51}$ erg), and $^{56}$Ni masses (0 - 0.15 $M_{\\rm \\odot}$).","Our analyses reveal that when the explosion energy is fixed, the light curve is primarily determined by the properties of the hydrogen-rich envelope, despite the large variation in the ZAMS masses.","The dependencies of the light-curve characteristics on the envelope mass, radius, and explosion energy are investigated, and we establish the scaling relations connecting these quantities.","We further derive a formula that corrects for effects of the $^{56}$Ni heating based on observables.","Using the derived equations, we develop a method that can constrain the envelope mass with uncertainty within 1 $M_{\\rm \\odot}$ based on photometry.","This method is applied to a large sample of $\\sim$ 100 SNe II, which reveals a considerably broader range of the envelope masses estimated from observables as compared to those predicted by single star models evolving with standard stellar wind; this finding indicates that a large fraction of SNe II experience substantial mass-loss beyond the standard mass-loss prescription prior to their explosions, highlighting the uncertainties involved in the massive star evolution and pre-SN mass-loss mechanism."],"url":"http://arxiv.org/abs/2404.01776v1","category":"astro-ph.HE"}
{"created":"2024-04-02 09:35:44","title":"Measurement of the mesonic decay branch of the $\\bar{K}\\!N\\!N$ quasi-bound state","abstract":"We conducted measurements of $K^- + {^3{\\rm He}} \\to \\pi \\!Y \\!N + N'$ reactions using $1~{\\rm GeV}/c$ $K^-$-beam, aiming to understand the broad decay width of $\\bar{K} \\!N \\!N$ ($\\approx$ twice as broad as that of $\\Lambda(1405)$ considered to be the $\\bar{K} \\!N$ quasi-bound state). We successfully reproduced distributions of the $\\pi \\! Y \\! N$ invariant mass and momentum transfer of $\\pi \\! Y \\! N$ by employing model fitting functions for $\\bar{K} \\! N \\! N$ formation and quasi-free $\\bar{K}$ absorption (${\\rm QF}_{\\bar{K}-{\\rm abs}}$) processes. The model can describe the experimental data quite well, and four $\\bar{K} \\! N \\! N \\to \\pi \\! Y \\! N $ cross-sections were obtained. The result suggests that the mesonic decay is the dominant decay branch of $\\bar{K} \\! N \\! N$. It was also suggested that $\\Gamma_{\\pi \\Lambda N} \\sim \\Gamma_{\\pi \\Sigma N}$, which indicates a significant contribution of the $I_{\\bar{K} \\! N}=1$ absorption channel to the $\\bar{K} \\! N \\! N$ decay as well as the $I_{\\bar{K} \\! N}=0$ makes the $\\bar{K} \\! N \\! N$ state $\\approx$ twice as unstable as $\\Lambda$(1405).","sentences":["We conducted measurements of $K^- + {^3{\\rm He}} \\to \\pi \\!Y \\!N + N'$ reactions using $1~{\\rm GeV}/c$ $K^-$-beam, aiming to understand the broad decay width of $\\bar{K} \\!N \\!N$ ($\\approx$ twice as broad as that of $\\Lambda(1405)$ considered to be the $\\bar{K} \\!N$ quasi-bound state).","We successfully reproduced distributions of the $\\pi \\!","Y \\!","N$ invariant mass and momentum transfer of $\\pi \\!","Y \\!","N$ by employing model fitting functions for $\\bar{K} \\!","N \\!","N$ formation and quasi-free $\\bar{K}$ absorption (${\\rm QF}_{\\bar{K}-{\\rm abs}}$) processes.","The model can describe the experimental data quite well, and four $\\bar{K} \\!","N \\!","N \\to \\pi \\!","Y \\!","N","$ cross-sections were obtained.","The result suggests that the mesonic decay is the dominant decay branch of $\\bar{K} \\!","N \\!","N$.","It was also suggested that $\\Gamma_{\\pi \\Lambda N} \\sim \\Gamma_{\\pi \\Sigma N}$, which indicates a significant contribution of the $I_{\\bar{K} \\!","N}=1$ absorption channel to the $\\bar{K} \\!","N \\!","N$ decay as well as the $I_{\\bar{K} \\!","N}=0$ makes the $\\bar{K} \\!","N \\!","N$ state $\\approx$ twice as unstable as $\\Lambda$(1405)."],"url":"http://arxiv.org/abs/2404.01773v1","category":"nucl-ex"}
{"created":"2024-04-02 08:53:00","title":"Nonparametric efficient causal estimation of the intervention-specific expected number of recurrent events with continuous-time targeted maximum likelihood and highly adaptive lasso estimation","abstract":"Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account. In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework. In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process. We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter. We illustrate the methods in a simulation study.","sentences":["Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account.","In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework.","In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process.","We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter.","We illustrate the methods in a simulation study."],"url":"http://arxiv.org/abs/2404.01736v1","category":"stat.ME"}
{"created":"2024-04-02 06:56:21","title":"Selective Temporal Knowledge Graph Reasoning","abstract":"Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. TKG reasoning aims to predict future facts based on given historical ones. However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions. Experiments with representative TKG reasoning models on two benchmark datasets demonstrate the effectiveness of the proposed CEHis.","sentences":["Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently.","TKG reasoning aims to predict future facts based on given historical ones.","However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications.","Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions.","Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence.","To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions.","Experiments with representative TKG reasoning models on two benchmark datasets demonstrate the effectiveness of the proposed CEHis."],"url":"http://arxiv.org/abs/2404.01695v1","category":"cs.LG"}
{"created":"2024-04-02 06:19:34","title":"Automating Vessel Segmentation in the Heart and Brain: A Trend to Develop Multi-Modality and Label-Efficient Deep Learning Techniques","abstract":"Cardio-cerebrovascular diseases are the leading causes of mortality worldwide, whose accurate blood vessel segmentation is significant for both scientific research and clinical usage. However, segmenting cardio-cerebrovascular structures from medical images is very challenging due to the presence of thin or blurred vascular shapes, imbalanced distribution of vessel and non-vessel pixels, and interference from imaging artifacts. These difficulties make manual or semi-manual segmentation methods highly time-consuming, labor-intensive, and prone to errors with interobserver variability, where different experts may produce different segmentations from a variety of modalities. Consequently, there is a growing interest in developing automated algorithms. This paper provides an up-to-date survey of deep learning techniques, for cardio-cerebrovascular segmentation. It analyzes the research landscape, surveys recent approaches, and discusses challenges such as the scarcity of accurately annotated data and variability. This paper also illustrates the urgent needs for developing multi-modality label-efficient deep learning techniques. To the best of our knowledge, this paper is the first comprehensive survey of deep learning approaches that effectively segment vessels in both the heart and brain. It aims to advance automated segmentation techniques for cardio-cerebrovascular diseases, benefiting researchers and healthcare professionals.","sentences":["Cardio-cerebrovascular diseases are the leading causes of mortality worldwide, whose accurate blood vessel segmentation is significant for both scientific research and clinical usage.","However, segmenting cardio-cerebrovascular structures from medical images is very challenging due to the presence of thin or blurred vascular shapes, imbalanced distribution of vessel and non-vessel pixels, and interference from imaging artifacts.","These difficulties make manual or semi-manual segmentation methods highly time-consuming, labor-intensive, and prone to errors with interobserver variability, where different experts may produce different segmentations from a variety of modalities.","Consequently, there is a growing interest in developing automated algorithms.","This paper provides an up-to-date survey of deep learning techniques, for cardio-cerebrovascular segmentation.","It analyzes the research landscape, surveys recent approaches, and discusses challenges such as the scarcity of accurately annotated data and variability.","This paper also illustrates the urgent needs for developing multi-modality label-efficient deep learning techniques.","To the best of our knowledge, this paper is the first comprehensive survey of deep learning approaches that effectively segment vessels in both the heart and brain.","It aims to advance automated segmentation techniques for cardio-cerebrovascular diseases, benefiting researchers and healthcare professionals."],"url":"http://arxiv.org/abs/2404.01671v1","category":"eess.IV"}
{"created":"2024-04-02 06:16:09","title":"Weak solutions of Anisotropic (and crystalline) inverse mean curvature flow as limits of $p$-capacitary potentials","abstract":"We construct weak solutions of the anisotropic inverse mean curvature flow (A-IMCF) under very mild assumptions both on the anisotropy (which is simply a norm in $\\mathbb R^N$ with no ellip\\-ticity nor smoothness requirements, in order to include the crystalline case) and on the initial data. By means of an approximation procedure introduced by Moser, our solutions are limits of anisotropic $p$-harmonic functions or $p$-capacitary functions (after a change of variable), and we get uniqueness both for the approximating solutions (i.e., uniqueness of $p$-capacitary functions) and the limiting ones. Our notion of weak solution still recovers variational and geometric definitions similar to those introduced by Huisken-Ilmanen, but requires to work within the broader setting of $BV$-functions. Despite of this, we still reach classical results like the continuity and exponential growth of perimeter, as well as outward minimizing properties of the sublevel sets. Moreover, by assuming the extra regularity given by an interior rolling ball condition (where a sliding Wulff shape plays the role of a ball), the solutions are shown to be continuous and satisfy Harnack inequalities. Finally, examples of explicit solutions are built.","sentences":["We construct weak solutions of the anisotropic inverse mean curvature flow (A-IMCF) under very mild assumptions both on the anisotropy (which is simply a norm in $\\mathbb R^N$ with no ellip\\-ticity nor smoothness requirements, in order to include the crystalline case) and on the initial data.","By means of an approximation procedure introduced by Moser, our solutions are limits of anisotropic $p$-harmonic functions or $p$-capacitary functions (after a change of variable), and we get uniqueness both for the approximating solutions (i.e., uniqueness of $p$-capacitary functions) and the limiting ones.","Our notion of weak solution still recovers variational and geometric definitions similar to those introduced by Huisken-Ilmanen, but requires to work within the broader setting of $BV$-functions.","Despite of this, we still reach classical results like the continuity and exponential growth of perimeter, as well as outward minimizing properties of the sublevel sets.","Moreover, by assuming the extra regularity given by an interior rolling ball condition (where a sliding Wulff shape plays the role of a ball), the solutions are shown to be continuous and satisfy Harnack inequalities.","Finally, examples of explicit solutions are built."],"url":"http://arxiv.org/abs/2404.01668v1","category":"math.AP"}
{"created":"2024-04-02 06:04:18","title":"Finite-temperature expansion of the dense-matter equation of state","abstract":"In this work we provide a new, well-controlled expansion of the equation of state of dense matter from zero to finite temperatures ($T$), while covering a wide range of charge fractions ($Y_Q$), from pure neutron to isospin symmetric nuclear matter. Our expansion can be used to describe neutron star mergers and core-collapse supernova explosions using as a starting point neutron star observations, while maintaining agreement with laboratory data, in a model independent way. We suggest new thermodynamic quantities of interest that can be calculated from theoretical models or directly inferred by experimental data that can help constrain the finite $T$ equation of state. With our new method, we can quantify the uncertainty in our finite $T$ and $Y_Q$ expansions in a well-controlled manner without making assumptions about the underlying degrees of freedom. We can reproduce results from a microscopic equation of state up to $T=100$ MeV for baryon chemical potential $\\mu_B\\gtrsim 1100$ MeV ($\\sim1-2 \\ n_{\\rm sat}$) within $5\\%$ error, with even better results for larger $\\mu_B$ and/or lower $T$. We investigate the sources of numerical and theoretical uncertainty and discuss future directions of study.","sentences":["In this work we provide a new, well-controlled expansion of the equation of state of dense matter from zero to finite temperatures ($T$), while covering a wide range of charge fractions ($Y_Q$), from pure neutron to isospin symmetric nuclear matter.","Our expansion can be used to describe neutron star mergers and core-collapse supernova explosions using as a starting point neutron star observations, while maintaining agreement with laboratory data, in a model independent way.","We suggest new thermodynamic quantities of interest that can be calculated from theoretical models or directly inferred by experimental data that can help constrain the finite $T$ equation of state.","With our new method, we can quantify the uncertainty in our finite $T$ and $Y_Q$ expansions in a well-controlled manner without making assumptions about the underlying degrees of freedom.","We can reproduce results from a microscopic equation of state up to $T=100$ MeV for baryon chemical potential $\\mu_B\\gtrsim 1100$ MeV ($\\sim1-2 \\ n_{\\rm sat}$) within $5\\%$ error, with even better results for larger $\\mu_B$ and/or lower $T$. We investigate the sources of numerical and theoretical uncertainty and discuss future directions of study."],"url":"http://arxiv.org/abs/2404.01658v1","category":"astro-ph.HE"}
{"created":"2024-04-02 05:46:01","title":"Investigation of reaction and $\u03b1$ production cross sections with $^9$Be projectile","abstract":"In order to investigate the contribution of $\\alpha$ production in the reaction cross sections, measurements of elastic scattering and inclusive $\\alpha$ particle angular distributions have been carried out with the $^9$Be projectile on $^{89}$Y, $^{124}$Sn, $^{159}$Tb, $^{198}$Pt, and $^{209}$Bi targets over a wide angular range at energies near the Coulomb barrier. The measured elastic scattering angular distributions were fitted with optical model calculations, and reaction cross sections were extracted. The same data were also analysed using both global optical model potentials (Global OMP) and microscopic S$\\tilde{a}$o Paulo potentials (SPP), to obtain the reaction cross sections. The data available in the literature for $^9$Be projectile includes the elastic scattering angular distributions, $\\alpha$ production cross sections, and complete fusion cross sections on these and other targets at several energies are also utilised for comparative studies. The reaction cross section extracted from the three potentials (Best Fit, Global OMP and SPP) are in reasonable agreement for all the targets except for the energies below the barrier where the results from SPP deviate by 30-50 \\%. Inclusive $\\alpha$ particle production cross sections were also extracted by integrating the $\\alpha$ particle angular distributions. The present data and data available from literature of reaction and $\\alpha$-particle production cross sections were utilised to make systematic studies. Systematics of reaction and $\\alpha$-particle production cross sections revealed their universal behaviour.","sentences":["In order to investigate the contribution of $\\alpha$ production in the reaction cross sections, measurements of elastic scattering and inclusive $\\alpha$ particle angular distributions have been carried out with the $^9$Be projectile on $^{89}$Y, $^{124}$Sn, $^{159}$Tb, $^{198}$Pt, and $^{209}$Bi targets over a wide angular range at energies near the Coulomb barrier.","The measured elastic scattering angular distributions were fitted with optical model calculations, and reaction cross sections were extracted.","The same data were also analysed using both global optical model potentials (Global OMP) and microscopic S$\\tilde{a}$o Paulo potentials (SPP), to obtain the reaction cross sections.","The data available in the literature for $^9$Be projectile includes the elastic scattering angular distributions, $\\alpha$ production cross sections, and complete fusion cross sections on these and other targets at several energies are also utilised for comparative studies.","The reaction cross section extracted from the three potentials (Best Fit, Global OMP and SPP) are in reasonable agreement for all the targets except for the energies below the barrier where the results from SPP deviate by 30-50 \\%.","Inclusive $\\alpha$ particle production cross sections were also extracted by integrating the $\\alpha$ particle angular distributions.","The present data and data available from literature of reaction and $\\alpha$-particle production cross sections were utilised to make systematic studies.","Systematics of reaction and $\\alpha$-particle production cross sections revealed their universal behaviour."],"url":"http://arxiv.org/abs/2404.01653v1","category":"nucl-ex"}
{"created":"2024-04-02 05:34:33","title":"Test-Time Model Adaptation with Only Forward Passes","abstract":"Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released.","sentences":["Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts.","However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration.","In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported.","To address this, we propose a test-time Forward-Only Adaptation (FOA) method.","In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy.","To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy.","Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance.","Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released."],"url":"http://arxiv.org/abs/2404.01650v1","category":"cs.LG"}
{"created":"2024-04-02 04:27:54","title":"Entity Disambiguation via Fusion Entity Decoding","abstract":"Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA.","sentences":["Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL).","Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark.","Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation.","Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked.","We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions.","Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate.","The decoder then fuses the representations of entity candidates together and selects the correct entity.","Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE.","Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA."],"url":"http://arxiv.org/abs/2404.01626v1","category":"cs.CL"}
{"created":"2024-04-02 03:06:25","title":"FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality","abstract":"Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.","sentences":["Machine learning methods often assume that the test data have the same distribution as the training data.","However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization.","In this work, we address the problem of fair and generalizable machine learning by invariant principles.","We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition.","We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions.","We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality.","We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts."],"url":"http://arxiv.org/abs/2404.01608v1","category":"stat.ML"}
{"created":"2024-04-02 02:56:27","title":"Haina Storage: A Decentralized Secure Storage Framework Based on Improved Blockchain Structure","abstract":"Although the decentralized storage technology based on the blockchain can effectively realize secure data storage on cloud services. However, there are still some problems in the existing schemes, such as low storage capacity and low efficiency. To address related issues, we propose a novel decentralized storage framework, which mainly includes four aspects: (1) we proposed a Bi-direction Circular Linked Chain Structure (BCLCS), which improves data's storage capacity and applicability in decentralized storage. (2) A Proof of Resources (PoR) decision model is proposed. By introducing the network environment as an essential evaluation parameter of storage right decision, the energy and time consumption of decision-making are reduced, and the fairness of decision-making is improved. (3) A chain structure dynamic locking mechanism (CSDLM) is designed to realize anti-traverse and access control. (4) A Bi-directional data Access Mechanism (BDAM) is proposed, which improves the efficiency of data access and acquisition in decentralized storage mode. The experimental results show that the framework has significantly improved the shortcomings of the current decentralized storage.","sentences":["Although the decentralized storage technology based on the blockchain can effectively realize secure data storage on cloud services.","However, there are still some problems in the existing schemes, such as low storage capacity and low efficiency.","To address related issues, we propose a novel decentralized storage framework, which mainly includes four aspects: (1) we proposed a Bi-direction Circular Linked Chain Structure (BCLCS), which improves data's storage capacity and applicability in decentralized storage.","(2) A Proof of Resources (PoR) decision model is proposed.","By introducing the network environment as an essential evaluation parameter of storage right decision, the energy and time consumption of decision-making are reduced, and the fairness of decision-making is improved.","(3) A chain structure dynamic locking mechanism (CSDLM) is designed to realize anti-traverse and access control.","(4) A Bi-directional data Access Mechanism (BDAM) is proposed, which improves the efficiency of data access and acquisition in decentralized storage mode.","The experimental results show that the framework has significantly improved the shortcomings of the current decentralized storage."],"url":"http://arxiv.org/abs/2404.01606v1","category":"cs.CR"}
{"created":"2024-04-02 02:53:36","title":"Division properties of commuting polynomials","abstract":"Polynomials commute under composition are referred to as commuting polynomials. In this paper, we study division properties for commuting polynomials with rational (and integer) coefficients. As a consequence, we show an algebraic particularity of the commuting polynomials coming from weighted sums for cycle graphs with pendant edges (arXiv:2402.07209v1.).","sentences":["Polynomials commute under composition are referred to as commuting polynomials.","In this paper, we study division properties for commuting polynomials with rational (and integer) coefficients.","As a consequence, we show an algebraic particularity of the commuting polynomials coming from weighted sums for cycle graphs with pendant edges (arXiv:2402.07209v1.)."],"url":"http://arxiv.org/abs/2404.01605v1","category":"math.AC"}
{"created":"2024-04-02 02:42:55","title":"A second-order correction method for loosely coupled discretizations applied to parabolic-parabolic interface problems","abstract":"We consider a parabolic-parabolic interface problem and construct a loosely coupled prediction-correction scheme based on the Robin-Robin splitting method analyzed in [J. Numer. Math., 31(1):59--77, 2023]. We show that the errors of the correction step converge at $\\mathcal O((\\Delta t)^2)$, under suitable convergence rate assumptions on the discrete time derivative of the prediction step, where $\\Delta t$ stands for the time-step length. Numerical results are shown to support our analysis and the assumptions.","sentences":["We consider a parabolic-parabolic interface problem and construct a loosely coupled prediction-correction scheme based on the Robin-Robin splitting method analyzed in [J. Numer.","Math., 31(1):59--77, 2023].","We show that the errors of the correction step converge at $\\mathcal O((\\Delta t)^2)$, under suitable convergence rate assumptions on the discrete time derivative of the prediction step, where $\\Delta t$ stands for the time-step length.","Numerical results are shown to support our analysis and the assumptions."],"url":"http://arxiv.org/abs/2404.01599v1","category":"math.NA"}
{"created":"2024-04-02 02:31:13","title":"Language Model Guided Interpretable Video Action Reasoning","abstract":"While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes. Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning. These models, however, usually fall short in performance compared to their black-box counterparts. In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR). LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models. In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models. Using the logical reasoning captured by the language model, we steer the training of the video model. This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance. Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework. The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.","sentences":["While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes.","Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning.","These models, however, usually fall short in performance compared to their black-box counterparts.","In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR).","LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models.","In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models.","Using the logical reasoning captured by the language model, we steer the training of the video model.","This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance.","Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework.","The code of LaIAR is available at https://github.com/NingWang2049/LaIAR."],"url":"http://arxiv.org/abs/2404.01591v1","category":"cs.CV"}
{"created":"2024-04-02 02:13:00","title":"GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection","abstract":"The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention. Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the performances of 366 models on 457 graphs on these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings. (iii) GLEMOS is designed to be easily extended with new models, new graphs, and new performance records. (iv) Based on the experimental results, we discuss the limitations of existing approaches and highlight future research directions. To promote research on this significant problem, we make the benchmark data and code publicly available at https://github.com/facebookresearch/glemos.","sentences":["The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks.","However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed.","Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention.","Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods.","To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions.","(i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the performances of 366 models on 457 graphs on these tasks.","(ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings.","(iii) GLEMOS is designed to be easily extended with new models, new graphs, and new performance records.","(iv) Based on the experimental results, we discuss the limitations of existing approaches and highlight future research directions.","To promote research on this significant problem, we make the benchmark data and code publicly available at https://github.com/facebookresearch/glemos."],"url":"http://arxiv.org/abs/2404.01578v1","category":"cs.LG"}
{"created":"2024-04-02 02:08:29","title":"Multi-granular Adversarial Attacks against Black-box Neural Ranking Models","abstract":"Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where perturbations in the next attack step are influenced by the perturbed document in the current attack step. Since the attack process can only access the final state without direct intermediate signals, we use reinforcement learning to perform multi-granular attacks. During the reinforcement learning process, two agents work cooperatively to identify multi-granular vulnerabilities as attack targets and organize perturbation candidates into a final perturbation sequence. Experimental results show that our attack method surpasses prevailing baselines in both attack effectiveness and imperceptibility.","sentences":["Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models.","Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document.","However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack.","Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations.","Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces.","To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where perturbations in the next attack step are influenced by the perturbed document in the current attack step.","Since the attack process can only access the final state without direct intermediate signals, we use reinforcement learning to perform multi-granular attacks.","During the reinforcement learning process, two agents work cooperatively to identify multi-granular vulnerabilities as attack targets and organize perturbation candidates into a final perturbation sequence.","Experimental results show that our attack method surpasses prevailing baselines in both attack effectiveness and imperceptibility."],"url":"http://arxiv.org/abs/2404.01574v1","category":"cs.IR"}
{"created":"2024-04-02 02:07:13","title":"Tropical cyclone genesis potential using a ventilated potential intensity","abstract":"Genesis potential indices (GPIs) are widely used to understand the climatology of tropical cyclones (TCs). However, the sign of projected future changes depends on how they incorporate environmental moisture. Recent theory combines potential intensity and mid-tropospheric moisture into a single quantity called the ventilated potential intensity, which removes this ambiguity. This work proposes a new GPI ($GPI_v$) that is proportional to the product of the ventilated potential intensity and the absolute vorticity raised to a power. This power is estimated to be approximately 5 by fitting observed tropical cyclone best-track and ECMWF Reanalysis v5 (ERA5) data. Fitting the model with separate exponents yields nearly identical values, indicating that their product likely constitutes a single joint parameter. Likewise, results are nearly identical for a Poisson model as for the power law. $GPI_v$ performs comparably well to existing indices in reproducing the climatological distribution of tropical cyclone genesis and its covariability with El Ni\\~no-Southern Oscillation, while only requiring a single fitting exponent. When applied to Coupled Model Intercomparison Project Phase 6 (CMIP6) projections, $GPI_v$ predicts that environments globally will become gradually more favorable for TC genesis with warming, consistent with prior work based on the normalized entropy deficit, though significant changes emerge only at higher latitudes under relatively strong warming. $GPI_v$ helps resolve the debate over the treatment of the moisture term and its implication for changes in TC genesis favorability with warming, and its clearer physical interpretation may offer a step forward towards a theory for genesis across climate states.","sentences":["Genesis potential indices (GPIs) are widely used to understand the climatology of tropical cyclones (TCs).","However, the sign of projected future changes depends on how they incorporate environmental moisture.","Recent theory combines potential intensity and mid-tropospheric moisture into a single quantity called the ventilated potential intensity, which removes this ambiguity.","This work proposes a new GPI ($GPI_v$) that is proportional to the product of the ventilated potential intensity and the absolute vorticity raised to a power.","This power is estimated to be approximately 5 by fitting observed tropical cyclone best-track and ECMWF Reanalysis v5 (ERA5) data.","Fitting the model with separate exponents yields nearly identical values, indicating that their product likely constitutes a single joint parameter.","Likewise, results are nearly identical for a Poisson model as for the power law.","$GPI_v$ performs comparably well to existing indices in reproducing the climatological distribution of tropical cyclone genesis and its covariability with El Ni\\~no-Southern Oscillation, while only requiring a single fitting exponent.","When applied to Coupled Model Intercomparison Project Phase 6 (CMIP6) projections, $GPI_v$ predicts that environments globally will become gradually more favorable for TC genesis with warming, consistent with prior work based on the normalized entropy deficit, though significant changes emerge only at higher latitudes under relatively strong warming.","$GPI_v$ helps resolve the debate over the treatment of the moisture term and its implication for changes in TC genesis favorability with warming, and its clearer physical interpretation may offer a step forward towards a theory for genesis across climate states."],"url":"http://arxiv.org/abs/2404.01572v1","category":"physics.ao-ph"}
{"created":"2024-04-02 01:55:27","title":"Coalescing sets preserving cospectrality of graphs arising from block similarity matrices","abstract":"Coalescing involves gluing one or more rooted graphs onto another graph. Under specific conditions, it is possible to start with cospectral graphs that are coalesced in similar ways that will result in new cospectral graphs. We present a sufficient condition for this based on the block structure of similarity matrices, possibly with additional constraints depending on which type of matrix is being considered. The matrices considered in this paper include the adjacency, Laplacian, signless Laplacian, distance, and generalized distance matrix.","sentences":["Coalescing involves gluing one or more rooted graphs onto another graph.","Under specific conditions, it is possible to start with cospectral graphs that are coalesced in similar ways that will result in new cospectral graphs.","We present a sufficient condition for this based on the block structure of similarity matrices, possibly with additional constraints depending on which type of matrix is being considered.","The matrices considered in this paper include the adjacency, Laplacian, signless Laplacian, distance, and generalized distance matrix."],"url":"http://arxiv.org/abs/2404.01561v1","category":"math.CO"}
{"created":"2024-04-02 01:43:14","title":"Blind QSO reconstruction challenge: Exploring methods to reconstruct the Ly$\u03b1$ emission line of QSOs","abstract":"Reconstructing the intrinsic Ly$\\alpha$ line flux from high-$z$ QSOs can place constraints on the neutral hydrogen content of the intergalactic medium during reionisation. There are now $\\gtrsim10$ different Ly$\\alpha$ reconstruction pipelines using different methodologies to predict the Ly$\\alpha$ line flux from correlations with the spectral information redward of Ly$\\alpha$. However, there have been few attempts to directly compare the performance of these pipelines. Therefore, we devised a blind QSO challenge to compare these reconstruction pipelines on a uniform set of objects. Each author was provided de-identified, observed rest-frame QSO spectra with spectral information only redward of 1260\\AA\\ rest-frame to ensure unbiased reconstruction. We constructed two samples of 30 QSOs, from X-Shooter and SDSS both spanning $3.5<z<4.5$. Importantly, the purpose of this comparison study was not to champion a single, best performing reconstruction pipeline but rather to explore the relative performance of these pipelines over a range of QSOs with broad observational characteristics to infer general trends. In summary, we find machine learning approaches in general provide the strongest ``best guesses\" but underestimate the accompanying statistical uncertainty, although these can be recalibrated, whilst pipelines that decompose the spectral information, for example principal component or factor analysis generally perform better at predicting the Ly$\\alpha$ profile. Further, we found that reconstruction pipelines trained on SDSS QSOs performed similarly on average for both the X-Shooter and SDSS samples indicating no discernible biases owing to differences in the observational characteristics of the training set or QSO being reconstructed, although the recovered distributions of reconstructions for X-Shooter were broader likely due to an increased fraction of outliers.","sentences":["Reconstructing the intrinsic Ly$\\alpha$ line flux from high-$z$ QSOs can place constraints on the neutral hydrogen content of the intergalactic medium during reionisation.","There are now $\\gtrsim10$ different Ly$\\alpha$ reconstruction pipelines using different methodologies to predict the Ly$\\alpha$ line flux from correlations with the spectral information redward of Ly$\\alpha$.","However, there have been few attempts to directly compare the performance of these pipelines.","Therefore, we devised a blind QSO challenge to compare these reconstruction pipelines on a uniform set of objects.","Each author was provided de-identified, observed rest-frame QSO spectra with spectral information only redward of 1260\\AA\\ rest-frame to ensure unbiased reconstruction.","We constructed two samples of 30 QSOs, from X-Shooter and SDSS both spanning $3.5<z<4.5$.","Importantly, the purpose of this comparison study was not to champion a single, best performing reconstruction pipeline but rather to explore the relative performance of these pipelines over a range of QSOs with broad observational characteristics to infer general trends.","In summary, we find machine learning approaches in general provide the strongest ``best guesses\" but underestimate the accompanying statistical uncertainty, although these can be recalibrated, whilst pipelines that decompose the spectral information, for example principal component or factor analysis generally perform better at predicting the Ly$\\alpha$ profile.","Further, we found that reconstruction pipelines trained on SDSS QSOs performed similarly on average for both the X-Shooter and SDSS samples indicating no discernible biases owing to differences in the observational characteristics of the training set or QSO being reconstructed, although the recovered distributions of reconstructions for X-Shooter were broader likely due to an increased fraction of outliers."],"url":"http://arxiv.org/abs/2404.01556v1","category":"astro-ph.CO"}
{"created":"2024-04-02 01:38:19","title":"A CT Image Denoising Method with Residual Encoder-Decoder Network","abstract":"Utilizing a low-dose CT approach significantly reduces the radiation exposure for patients, yet it introduces challenges, such as increased noise and artifacts in the resultant images, which can hinder accurate medical diagnostics. Traditional methods for noise reduction struggle with preserving image textures due to the complexity of modeling statistical properties directly within the image domain. To address these limitations, this study introduces an enhanced noise-reduction technique centered around an advanced residual encoder-decoder network. By incorporating recursive processing into the foundational network, this method reduces computational complexity and enhances the effectiveness of noise reduction. Furthermore, the introduction of a root-mean-square error and perceptual loss functions aims to retain the integrity of the images' textural details. The enhanced technique also includes optimized tissue segmentation, improving artifact management post-improvement. Validation using the TCGA-COAD clinical dataset demonstrates superior performance in both noise reduction and image quality, as measured by post-denoising PSNR and SSIM, compared to the existing WGAN approach. This advancement in CT image processing offers a practical solution for clinical applications, achieving lower computational demands and faster processing times without compromising image quality.","sentences":["Utilizing a low-dose CT approach significantly reduces the radiation exposure for patients, yet it introduces challenges, such as increased noise and artifacts in the resultant images, which can hinder accurate medical diagnostics.","Traditional methods for noise reduction struggle with preserving image textures due to the complexity of modeling statistical properties directly within the image domain.","To address these limitations, this study introduces an enhanced noise-reduction technique centered around an advanced residual encoder-decoder network.","By incorporating recursive processing into the foundational network, this method reduces computational complexity and enhances the effectiveness of noise reduction.","Furthermore, the introduction of a root-mean-square error and perceptual loss functions aims to retain the integrity of the images' textural details.","The enhanced technique also includes optimized tissue segmentation, improving artifact management post-improvement.","Validation using the TCGA-COAD clinical dataset demonstrates superior performance in both noise reduction and image quality, as measured by post-denoising PSNR and SSIM, compared to the existing WGAN approach.","This advancement in CT image processing offers a practical solution for clinical applications, achieving lower computational demands and faster processing times without compromising image quality."],"url":"http://arxiv.org/abs/2404.01553v1","category":"eess.IV"}
{"created":"2024-04-02 01:01:38","title":"Time-Varying Matrix Factor Models","abstract":"Matrix-variate data of high dimensions are frequently observed in finance and economics, spanning extended time periods, such as the long-term data on international trade flows among numerous countries. To address potential structural shifts and explore the matrix structure's informational context, we propose a time-varying matrix factor model. This model accommodates changing factor loadings over time, revealing the underlying dynamic structure through nonparametric principal component analysis and facilitating dimension reduction. We establish the consistency and asymptotic normality of our estimators under general conditions that allow for weak correlations across time, rows, or columns of the noise. A novel approach is introduced to overcome rotational ambiguity in the estimators, enhancing the clarity and interpretability of the estimated loading matrices. Our simulation study highlights the merits of the proposed estimators and the effective of the smoothing operation. In an application to international trade flow, we investigate the trading hubs, centrality, patterns, and trends in the trading network.","sentences":["Matrix-variate data of high dimensions are frequently observed in finance and economics, spanning extended time periods, such as the long-term data on international trade flows among numerous countries.","To address potential structural shifts and explore the matrix structure's informational context, we propose a time-varying matrix factor model.","This model accommodates changing factor loadings over time, revealing the underlying dynamic structure through nonparametric principal component analysis and facilitating dimension reduction.","We establish the consistency and asymptotic normality of our estimators under general conditions that allow for weak correlations across time, rows, or columns of the noise.","A novel approach is introduced to overcome rotational ambiguity in the estimators, enhancing the clarity and interpretability of the estimated loading matrices.","Our simulation study highlights the merits of the proposed estimators and the effective of the smoothing operation.","In an application to international trade flow, we investigate the trading hubs, centrality, patterns, and trends in the trading network."],"url":"http://arxiv.org/abs/2404.01546v1","category":"stat.ME"}
{"created":"2024-04-02 00:09:30","title":"Are Doppler Velocity Measurements Useful for Spinning Radar Odometry?","abstract":"Spinning, frequency-modulated continuous-wave (FMCW) radar has been gaining popularity for autonomous vehicle navigation. The spinning radar is chosen over the more classic automotive `fixed' radar as it is able to capture the full 360 degree field of view without requiring multiple sensors and extensive calibration. However, commercially available spinning radar systems have not previously had the ability to extract radial velocities due to the lack of repeated measurements in the same direction and fundamental hardware setup. A new firmware upgrade now makes it possible to alternate the modulation of the radar signal between azimuths. In this paper, we first present a way to use this alternating modulation to extract radial Doppler velocity measurements from single raw radar intensity scans. We then incorporate these measurements in two different modern odometry pipelines and evaluate them in progressively challenging autonomous driving environments. We show that using Doppler velocity measurements enables our odometry to continue functioning at state-of-the-art even in severely geometrically degenerate environments.","sentences":["Spinning, frequency-modulated continuous-wave (FMCW) radar has been gaining popularity for autonomous vehicle navigation.","The spinning radar is chosen over the more classic automotive `fixed' radar as it is able to capture the full 360 degree field of view without requiring multiple sensors and extensive calibration.","However, commercially available spinning radar systems have not previously had the ability to extract radial velocities due to the lack of repeated measurements in the same direction and fundamental hardware setup.","A new firmware upgrade now makes it possible to alternate the modulation of the radar signal between azimuths.","In this paper, we first present a way to use this alternating modulation to extract radial Doppler velocity measurements from single raw radar intensity scans.","We then incorporate these measurements in two different modern odometry pipelines and evaluate them in progressively challenging autonomous driving environments.","We show that using Doppler velocity measurements enables our odometry to continue functioning at state-of-the-art even in severely geometrically degenerate environments."],"url":"http://arxiv.org/abs/2404.01537v1","category":"cs.RO"}
{"created":"2024-04-01 23:46:00","title":"Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation","abstract":"Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularisations designed to alleviate text generation loss penalties associated with the linearised graph edge sequences, thus encouraging the generation of more relation edges. Experimental results show that our framework surpasses existing baselines for event temporal graph generation. Furthermore, under zero-shot settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited.","sentences":["Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text.","Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results.","However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models.","This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences.","To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs).","The framework incorporates data augmentations and set-property regularisations designed to alleviate text generation loss penalties associated with the linearised graph edge sequences, thus encouraging the generation of more relation edges.","Experimental results show that our framework surpasses existing baselines for event temporal graph generation.","Furthermore, under zero-shot settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited."],"url":"http://arxiv.org/abs/2404.01532v1","category":"cs.CL"}
{"created":"2024-04-01 23:16:42","title":"Ancient curve shortening flow in the disc with mixed boundary condition","abstract":"Given any non-central interior point $o$ of the unit disc $D$, the diameter $L$ through $o$ is the union of two linear arcs emanating from $o$ which meet $\\partial D$ orthogonally, the shorter of them stable and the longer unstable (under these boundary conditions). In each of the two half discs bounded by $L$, we construct a convex eternal solution to curve shortening flow which fixes $o$ and meets $\\partial D$ orthogonally, and evolves out of the unstable critical arc at $t=-\\infty$ and into the stable one at $t=+\\infty$. We then prove that these two (congruent) solutions are the only non-flat convex ancient solutions to the curve shortening flow satisfying the specified boundary conditions. We obtain analogous conclusions in the \"degenerate\" case $o\\in\\partial D$ as well, although in this case the solution contracts to the point $o$ at a finite time with asymptotic shape that of a half Grim Reaper, thus providing an interesting example for which an embedded flow develops a collapsing singularity.","sentences":["Given any non-central interior point $o$ of the unit disc $D$, the diameter $L$ through $o$ is the union of two linear arcs emanating from $o$ which meet $\\partial D$ orthogonally, the shorter of them stable and the longer unstable (under these boundary conditions).","In each of the two half discs bounded by $L$, we construct a convex eternal solution to curve shortening flow which fixes $o$ and meets $\\partial D$ orthogonally, and evolves out of the unstable critical arc at $t=-\\infty$ and into the stable one at $t=+\\infty$. We then prove that these two (congruent) solutions are the only non-flat convex ancient solutions to the curve shortening flow satisfying the specified boundary conditions.","We obtain analogous conclusions in the \"degenerate\" case $o\\in\\partial D$ as well, although in this case the solution contracts to the point $o$ at a finite time with asymptotic shape that of a half Grim Reaper, thus providing an interesting example for which an embedded flow develops a collapsing singularity."],"url":"http://arxiv.org/abs/2404.01525v1","category":"math.DG"}
{"created":"2024-04-01 23:10:05","title":"Proactive Service Assurance in 5G and B5G Networks: A Closed-Loop Algorithm for End-to-End Network Slicing","abstract":"The customization of services in Fifth-generation (5G) and Beyond 5G (B5G) networks relies heavily on network slicing, which creates multiple virtual networks on a shared physical infrastructure, tailored to meet specific requirements of distinct applications, using Software Defined Networking (SDN) and Network Function Virtualization (NFV). It is imperative to ensure that network services meet the performance and reliability requirements of various applications and users, thus, service assurance is one of the critical components in network slicing. One of the key functionalities of network slicing is the ability to scale Virtualized Network Functions (VNFs) in response to changing resource demand and to meet Customer Service Level agreements (SLAs).   In this paper, we introduce a proactive closed-loop algorithm for end-to-end network orchestration, designed to provide service assurance in 5G and B5G networks. We focus on dynamically scaling resources to meet key performance indicators (KPIs) specific to each network slice and operate in parallel across multiple slices, making it scalable and capable of managing completely automatically real-time service assurance. Through our experiments, we demonstrate that the proposed algorithm effectively fulfills service assurance requirements for different network slice types, thereby minimizing network resource utilization and reducing the over-provisioning of spare resources.","sentences":["The customization of services in Fifth-generation (5G) and Beyond 5G (B5G) networks relies heavily on network slicing, which creates multiple virtual networks on a shared physical infrastructure, tailored to meet specific requirements of distinct applications, using Software Defined Networking (SDN) and Network Function Virtualization (NFV).","It is imperative to ensure that network services meet the performance and reliability requirements of various applications and users, thus, service assurance is one of the critical components in network slicing.","One of the key functionalities of network slicing is the ability to scale Virtualized Network Functions (VNFs) in response to changing resource demand and to meet Customer Service Level agreements (SLAs).   ","In this paper, we introduce a proactive closed-loop algorithm for end-to-end network orchestration, designed to provide service assurance in 5G and B5G networks.","We focus on dynamically scaling resources to meet key performance indicators (KPIs) specific to each network slice and operate in parallel across multiple slices, making it scalable and capable of managing completely automatically real-time service assurance.","Through our experiments, we demonstrate that the proposed algorithm effectively fulfills service assurance requirements for different network slice types, thereby minimizing network resource utilization and reducing the over-provisioning of spare resources."],"url":"http://arxiv.org/abs/2404.01523v1","category":"cs.NI"}
{"created":"2024-04-01 23:04:13","title":"Watanabe's expansion: A Solution for the convexity conundrum","abstract":"In this paper, we present a new method for pricing CMS derivatives. We use Mallaivin's calculus to establish a model-free connection between the price of a CMS derivative and a quadratic payoff. Then, we apply Watanabe's expansions to quadratic payoffs case under local and stochastic local volatility. Our approximations are generic. To evaluate their accuracy, we will compare the approximations numerically under the normal SABR model against the market standards: Hagan's approximation, and a Monte Carlo simulation.","sentences":["In this paper, we present a new method for pricing CMS derivatives.","We use Mallaivin's calculus to establish a model-free connection between the price of a CMS derivative and a quadratic payoff.","Then, we apply Watanabe's expansions to quadratic payoffs case under local and stochastic local volatility.","Our approximations are generic.","To evaluate their accuracy, we will compare the approximations numerically under the normal SABR model against the market standards: Hagan's approximation, and a Monte Carlo simulation."],"url":"http://arxiv.org/abs/2404.01522v1","category":"q-fin.MF"}
{"created":"2024-04-01 22:17:35","title":"Deep reinforcement learning of airfoil pitch control in a highly disturbed environment using partial observations","abstract":"This study explores the application of deep reinforcement learning (RL) to design an airfoil pitch controller capable of minimizing lift variations in randomly disturbed flows. The controller, treated as an agent in a partially observable Markov decision process, receives non-Markovian observations from the environment, simulating practical constraints where flow information is limited to force and pressure sensors. Deep RL, particularly the TD3 algorithm, is used to approximate an optimal control policy under such conditions. Testing is conducted for a flat plate airfoil in two environments: a classical unsteady environment with vertical acceleration disturbances (i.e., a Wagner setup) and a viscous flow model with pulsed point force disturbances. In both cases, augmenting observations of the lift, pitch angle, and angular velocity with extra wake information (e.g., from pressure sensors) and retaining memory of past observations enhances RL control performance. Results demonstrate the capability of RL control to match or exceed standard linear controllers in minimizing lift variations. Special attention is given to the choice of training data and the generalization to unseen disturbances.","sentences":["This study explores the application of deep reinforcement learning (RL) to design an airfoil pitch controller capable of minimizing lift variations in randomly disturbed flows.","The controller, treated as an agent in a partially observable Markov decision process, receives non-Markovian observations from the environment, simulating practical constraints where flow information is limited to force and pressure sensors.","Deep RL, particularly the TD3 algorithm, is used to approximate an optimal control policy under such conditions.","Testing is conducted for a flat plate airfoil in two environments: a classical unsteady environment with vertical acceleration disturbances (i.e., a Wagner setup) and a viscous flow model with pulsed point force disturbances.","In both cases, augmenting observations of the lift, pitch angle, and angular velocity with extra wake information (e.g., from pressure sensors) and retaining memory of past observations enhances RL control performance.","Results demonstrate the capability of RL control to match or exceed standard linear controllers in minimizing lift variations.","Special attention is given to the choice of training data and the generalization to unseen disturbances."],"url":"http://arxiv.org/abs/2404.01506v1","category":"physics.flu-dyn"}
{"created":"2024-04-01 22:14:13","title":"Permutation symmetric solutions of the incompressible Euler equation","abstract":"In this paper, we study permutation symmetric solutions of the incompressible Euler equation. We show that the dynamics of these solutions can be reduced to an evolution equation on a single vorticity component $\\omega_1$, and we characterize the relevant constraint space for this vorticity component under permutation symmetry. We also give single vorticity component versions of the energy equality, Beale-Kato-Majda criterion, and local wellposedness theory that are specific to the permutation symmetric case. This paper is significantly motivated by recent work of the author [11], which proved finite-time blowup for solutions of a Fourier-restricted Euler model equation, where the Helmholtz projection is replaced by a projection onto a more restrictive constraint space. The blowup solutions for this model equation are odd, permutation symmetric, and mirror symmetric about the plane $x_1+x_2+x_3=0.$ Using the blowup solution introduced by Elgindi in [5], we are able to show that there are $C^{1,\\alpha}$ solutions of the full Euler equation that are odd, permutation symmetric, and mirror symmetric about the plane $x_1+x_2+x_3=0$, that blowup in finite-time.","sentences":["In this paper, we study permutation symmetric solutions of the incompressible Euler equation.","We show that the dynamics of these solutions can be reduced to an evolution equation on a single vorticity component $\\omega_1$, and we characterize the relevant constraint space for this vorticity component under permutation symmetry.","We also give single vorticity component versions of the energy equality, Beale-Kato-Majda criterion, and local wellposedness theory that are specific to the permutation symmetric case.","This paper is significantly motivated by recent work of the author [11], which proved finite-time blowup for solutions of a Fourier-restricted Euler model equation, where the Helmholtz projection is replaced by a projection onto a more restrictive constraint space.","The blowup solutions for this model equation are odd, permutation symmetric, and mirror symmetric about the plane $x_1+x_2+x_3=0.$ Using the blowup solution introduced by Elgindi in [5], we are able to show that there are $C^{1,\\alpha}$ solutions of the full Euler equation that are odd, permutation symmetric, and mirror symmetric about the plane $x_1+x_2+x_3=0$, that blowup in finite-time."],"url":"http://arxiv.org/abs/2404.01505v1","category":"math.AP"}
{"created":"2024-04-01 21:44:36","title":"Effective Categorical Enumerative Invariants","abstract":"We introduce enumerative invariants $F_{g,n}$ $(g\\geq0$, $n \\geq 1)$ associated to a cyclic $A_\\infty$ algebra and a splitting of its non-commutative Hodge filtration. These invariants are defined by explicitly computable Feynman sums, and encode the same information as Costello's partition function of the corresponding field theory.   Our invariants are stable under Morita equivalence, and therefore can be associated to a Calabi-Yau category with splitting data. This justifies the name categorical enumerative invariants (CEI) that we use for them.   CEI conjecturally generalize all known enumerative invariants in symplectic geometry, complex geometry, and singularity theory. They also provide a framework for stating enumerative mirror symmetry predictions in arbitrary genus, whenever homological mirror symmetry holds.","sentences":["We introduce enumerative invariants $F_{g,n}$ $(g\\geq0$, $n \\geq 1)$ associated to a cyclic $A_\\infty$ algebra and a splitting of its non-commutative Hodge filtration.","These invariants are defined by explicitly computable Feynman sums, and encode the same information as Costello's partition function of the corresponding field theory.   ","Our invariants are stable under Morita equivalence, and therefore can be associated to a Calabi-Yau category with splitting data.","This justifies the name categorical enumerative invariants (CEI) that we use for them.   ","CEI conjecturally generalize all known enumerative invariants in symplectic geometry, complex geometry, and singularity theory.","They also provide a framework for stating enumerative mirror symmetry predictions in arbitrary genus, whenever homological mirror symmetry holds."],"url":"http://arxiv.org/abs/2404.01499v1","category":"math.AG"}
{"created":"2024-04-01 21:38:59","title":"Existence, uniqueness, and regularity of solutions to nonlinear and non-smooth parabolic obstacle problems","abstract":"We establish the existence, uniqueness, and $W^{1,2,p}$-regularity of solutions to fully nonlinear parabolic obstacle problems when the obstacle is the pointwise supremum of arbitrary functions in $W^{1,2,p}$ and the operator is only assumed to be measurable in the state and time variables. The results hold for a large class of non-smooth obstacles, including all convex obstacles. Applied to stopping problems, they imply that the decision maker never stops at a convex kink of the stopping payoff. The proof relies on new $W^{1,2,p}$-estimates for obstacle problems where the obstacle is the maximum of finitely many functions in $W^{1,2,p}$.","sentences":["We establish the existence, uniqueness, and $W^{1,2,p}$-regularity of solutions to fully nonlinear parabolic obstacle problems when the obstacle is the pointwise supremum of arbitrary functions in $W^{1,2,p}$ and the operator is only assumed to be measurable in the state and time variables.","The results hold for a large class of non-smooth obstacles, including all convex obstacles.","Applied to stopping problems, they imply that the decision maker never stops at a convex kink of the stopping payoff.","The proof relies on new $W^{1,2,p}$-estimates for obstacle problems where the obstacle is the maximum of finitely many functions in $W^{1,2,p}$."],"url":"http://arxiv.org/abs/2404.01498v1","category":"math.AP"}
{"created":"2024-04-01 21:38:31","title":"First determination of the angular dependence of rise and decay times of solar radio bursts using multi-spacecraft observations","abstract":"Radio photons interact with anisotropic density fluctuations in the heliosphere, which can alter their trajectory and influence properties deduced from observations. This is particularly evident in solar radio observations, where anisotropic scattering leads to highly-directional radio emissions. Consequently, observers at varying locations will measure different properties, including different source sizes, source positions, and intensities. However, it is not known if measurements of the decay time of solar radio bursts are also affected by the observer's position. Decay times are dominated by scattering effects, and so are frequently used as proxies of the level of density fluctuations in the heliosphere, making the identification of any location-related dependence crucial. We combine multi-vantage observations of interplanetary Type III bursts from four non-collinear, angularly-separated spacecraft with simulations, to investigate the dependence of both the decay- and rise-time measurements on the separation of the observer from the source. We propose a function to characterise the entire time profile of radio signals, allowing for the simultaneous estimation of the peak flux, decay time, and rise time, while demonstrating that the rise phase of radio bursts has a non-constant, non-exponential growth rate. We determine that the decay and rise times are independent of the observer's position, identifying them as the only properties to remain unaffected, thus not requiring corrections for the observer's location. Moreover, we examine the ratio between the rise and decay times, finding that it does not depend on the frequency. Therefore, we provide the first evidence that the rise phase is also significantly impacted by scattering effects, adding to our understanding of the plasma emission process.","sentences":["Radio photons interact with anisotropic density fluctuations in the heliosphere, which can alter their trajectory and influence properties deduced from observations.","This is particularly evident in solar radio observations, where anisotropic scattering leads to highly-directional radio emissions.","Consequently, observers at varying locations will measure different properties, including different source sizes, source positions, and intensities.","However, it is not known if measurements of the decay time of solar radio bursts are also affected by the observer's position.","Decay times are dominated by scattering effects, and so are frequently used as proxies of the level of density fluctuations in the heliosphere, making the identification of any location-related dependence crucial.","We combine multi-vantage observations of interplanetary Type III bursts from four non-collinear, angularly-separated spacecraft with simulations, to investigate the dependence of both the decay- and rise-time measurements on the separation of the observer from the source.","We propose a function to characterise the entire time profile of radio signals, allowing for the simultaneous estimation of the peak flux, decay time, and rise time, while demonstrating that the rise phase of radio bursts has a non-constant, non-exponential growth rate.","We determine that the decay and rise times are independent of the observer's position, identifying them as the only properties to remain unaffected, thus not requiring corrections for the observer's location.","Moreover, we examine the ratio between the rise and decay times, finding that it does not depend on the frequency.","Therefore, we provide the first evidence that the rise phase is also significantly impacted by scattering effects, adding to our understanding of the plasma emission process."],"url":"http://arxiv.org/abs/2404.01497v1","category":"astro-ph.SR"}
{"created":"2024-04-01 21:21:15","title":"AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness","abstract":"This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data. Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation. For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer).","sentences":["This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages.","The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages.","In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data.","Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation.","For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer.","We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer)."],"url":"http://arxiv.org/abs/2404.01490v1","category":"cs.CL"}
{"created":"2024-04-01 21:19:24","title":"Fear over Friends: Examining the Perceived Influence of Others on Vaccination Decisions","abstract":"In this study, we examine the perceived influence of others, across both strong and weak social ties, on vaccination decisions in the United States. In particular, we focus on the case study of COVID-19 vaccinations. We add context to social influence by measuring related concepts, such as perceived agreement of others and perceived danger of COVID-19 to others. We find that vaccinated populations perceived more influence from their social circles than unvaccinated populations. This finding holds true across various social groups, including family, close friends, co-workers, and neighbors. Indirect measures of social influence also followed this trend. Vaccinated participants perceived COVID-19 as more dangerous to their social circles than unvaccinated participants and perceived that others agreed with their decision to get vaccinated more than unvaccinated participants perceived others to agree with their decision to not get vaccinated. Despite the clear differences in perceived social influence across the groups, we find through open-ended responses that both vaccinated and unvaccinated participants frequently cited fear as a motivating factor in their decision, rather than social influence: vaccinated participants feared COVID-19, while unvaccinated participants feared the vaccine itself.","sentences":["In this study, we examine the perceived influence of others, across both strong and weak social ties, on vaccination decisions in the United States.","In particular, we focus on the case study of COVID-19 vaccinations.","We add context to social influence by measuring related concepts, such as perceived agreement of others and perceived danger of COVID-19 to others.","We find that vaccinated populations perceived more influence from their social circles than unvaccinated populations.","This finding holds true across various social groups, including family, close friends, co-workers, and neighbors.","Indirect measures of social influence also followed this trend.","Vaccinated participants perceived COVID-19 as more dangerous to their social circles than unvaccinated participants and perceived that others agreed with their decision to get vaccinated more than unvaccinated participants perceived others to agree with their decision to not get vaccinated.","Despite the clear differences in perceived social influence across the groups, we find through open-ended responses that both vaccinated and unvaccinated participants frequently cited fear as a motivating factor in their decision, rather than social influence: vaccinated participants feared COVID-19, while unvaccinated participants feared the vaccine itself."],"url":"http://arxiv.org/abs/2404.01489v1","category":"cs.SI"}
{"created":"2024-04-01 20:33:29","title":"TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data","abstract":"The growing availability and importance of time series data across various domains, including environmental science, epidemiology, and economics, has led to an increasing need for time-series causal discovery methods that can identify the intricate relationships in the non-stationary, non-linear, and often noisy real world data. However, the majority of current time series causal discovery methods assume stationarity and linear relations in data, making them infeasible for the task. Further, the recent deep learning-based methods rely on the traditional causal structure learning approaches making them computationally expensive. In this paper, we propose a Time-Series Causal Neural Network (TS-CausalNN) - a deep learning technique to discover contemporaneous and lagged causal relations simultaneously. Our proposed architecture comprises (i) convolutional blocks comprising parallel custom causal layers, (ii) acyclicity constraint, and (iii) optimization techniques using the augmented Lagrangian approach. In addition to the simple parallel design, an advantage of the proposed model is that it naturally handles the non-stationarity and non-linearity of the data. Through experiments on multiple synthetic and real world datasets, we demonstrate the empirical proficiency of our proposed approach as compared to several state-of-the-art methods. The inferred graphs for the real world dataset are in good agreement with the domain understanding.","sentences":["The growing availability and importance of time series data across various domains, including environmental science, epidemiology, and economics, has led to an increasing need for time-series causal discovery methods that can identify the intricate relationships in the non-stationary, non-linear, and often noisy real world data.","However, the majority of current time series causal discovery methods assume stationarity and linear relations in data, making them infeasible for the task.","Further, the recent deep learning-based methods rely on the traditional causal structure learning approaches making them computationally expensive.","In this paper, we propose a Time-Series Causal Neural Network (TS-CausalNN) - a deep learning technique to discover contemporaneous and lagged causal relations simultaneously.","Our proposed architecture comprises (i) convolutional blocks comprising parallel custom causal layers, (ii) acyclicity constraint, and (iii) optimization techniques using the augmented Lagrangian approach.","In addition to the simple parallel design, an advantage of the proposed model is that it naturally handles the non-stationarity and non-linearity of the data.","Through experiments on multiple synthetic and real world datasets, we demonstrate the empirical proficiency of our proposed approach as compared to several state-of-the-art methods.","The inferred graphs for the real world dataset are in good agreement with the domain understanding."],"url":"http://arxiv.org/abs/2404.01466v1","category":"cs.LG"}
{"created":"2024-04-01 20:19:10","title":"Periodic orbits near collision in a restricted four-body problem for the eight choreography","abstract":"We study orbits near collision in a non-autonomous restricted planar four-body problem. This restricted problem consists of a massless particle moving under the gravitational influence due to three bodies following the figure-eight choreography. We use regularized coordinates in order to deal numerically with motions near collision, and reversing symmetries to study both theoretically and numerically certain type of symmetric periodic orbits of the restricted system. The symmetric periodic orbits (initial conditions) were computed with the help of some boundary-value problems.","sentences":["We study orbits near collision in a non-autonomous restricted planar four-body problem.","This restricted problem consists of a massless particle moving under the gravitational influence due to three bodies following the figure-eight choreography.","We use regularized coordinates in order to deal numerically with motions near collision, and reversing symmetries to study both theoretically and numerically certain type of symmetric periodic orbits of the restricted system.","The symmetric periodic orbits (initial conditions) were computed with the help of some boundary-value problems."],"url":"http://arxiv.org/abs/2404.01463v1","category":"math.DS"}
{"created":"2024-04-01 20:15:06","title":"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs","abstract":"Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.","sentences":["Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so.","Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic.","This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence.","This work investigates the impact of the representativeness heuristic on LLM reasoning.","We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics.","Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases.","We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description.","Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge.","This suggests the uniqueness of the representativeness heuristic compared to traditional biases.","It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap.","This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it."],"url":"http://arxiv.org/abs/2404.01461v1","category":"cs.CL"}
{"created":"2024-04-01 20:04:49","title":"An efficient quantum algorithm for generation of ab initio n-th order susceptibilities for non-linear spectroscpies","abstract":"We develop and analyze a fault-tolerant quantum algorithm for computing $n$-th order response properties necessary for analysis of non-linear spectroscopies of molecular and condensed phase systems. We use a semi-classical description in which the electronic degrees of freedom are treated quantum mechanically and the light is treated as a classical field. The algorithm we present can be viewed as an implementation of standard perturbation theory techniques, focused on {\\it ab initio} calculation of $n$-th order response functions. We provide cost estimates in terms of the number of queries to the block encoding of the unperturbed Hamiltonian, as well as the block encodings of the perturbing dipole operators. Using the technique of eigenstate filtering, we provide an algorithm to extract excitation energies to resolution $\\gamma$, and the corresponding linear response amplitude to accuracy $\\epsilon$ using ${O}\\left(N^{6}\\eta^2{{\\gamma^{-1}}\\epsilon^{-1}}\\log(1/\\epsilon)\\right)$ queries to the block encoding of the unperturbed Hamiltonian $H_0$, in double factorized representation. Thus, our approach saturates the Heisenberg $O(\\gamma^{-1})$ limit for energy estimation and allows for the approximation of relevant transition dipole moments. These quantities, combined with sum-over-states formulation of polarizabilities, can be used to compute the $n$-th order susceptibilities and response functions for non-linear spectroscopies under limited assumptions using $\\widetilde{O}\\left({N^{5n+1}\\eta^{n+1}}/{\\gamma^n\\epsilon}\\right)$ queries to the block encoding of $H_0$.","sentences":["We develop and analyze a fault-tolerant quantum algorithm for computing $n$-th order response properties necessary for analysis of non-linear spectroscopies of molecular and condensed phase systems.","We use a semi-classical description in which the electronic degrees of freedom are treated quantum mechanically and the light is treated as a classical field.","The algorithm we present can be viewed as an implementation of standard perturbation theory techniques, focused on {\\it ab initio} calculation of $n$-th order response functions.","We provide cost estimates in terms of the number of queries to the block encoding of the unperturbed Hamiltonian, as well as the block encodings of the perturbing dipole operators.","Using the technique of eigenstate filtering, we provide an algorithm to extract excitation energies to resolution $\\gamma$, and the corresponding linear response amplitude to accuracy $\\epsilon$ using ${O}\\left(N^{6}\\eta^2{{\\gamma^{-1}}\\epsilon^{-1}}\\log(1/\\epsilon)\\right)$ queries to the block encoding of the unperturbed Hamiltonian $H_0$, in double factorized representation.","Thus, our approach saturates the Heisenberg $O(\\gamma^{-1})$ limit for energy estimation and allows for the approximation of relevant transition dipole moments.","These quantities, combined with sum-over-states formulation of polarizabilities, can be used to compute the $n$-th order susceptibilities and response functions for non-linear spectroscopies under limited assumptions using $\\widetilde{O}\\left({N^{5n+1}\\eta^{n+1}}/{\\gamma^n\\epsilon}\\right)$ queries to the block encoding of $H_0$."],"url":"http://arxiv.org/abs/2404.01454v1","category":"quant-ph"}
{"created":"2024-04-01 19:41:33","title":"Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction","abstract":"Cone-beam computed tomography (CBCT) is widely used in image-guided radiotherapy. Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is highly desired for improved imaging efficiency, dose reduction, and better mechanical clearance. LA-CBCT reconstruction, however, suffers from severe under-sampling artifacts, making it a highly ill-posed inverse problem. Diffusion models can generate data/images by reversing a data-noising process through learned data distributions; and can be incorporated as a denoiser/regularizer in LA-CBCT reconstruction. In this study, we developed a diffusion model-based framework, prior frequency-guided diffusion model (PFGDM), for robust and structure-preserving LA-CBCT reconstruction. PFGDM uses a conditioned diffusion model as a regularizer for LA-CBCT reconstruction, and the condition is based on high-frequency information extracted from patient-specific prior CT scans which provides a strong anatomical prior for LA-CBCT reconstruction. Specifically, we developed two variants of PFGDM (PFGDM-A and PFGDM-B) with different conditioning schemes. PFGDM-A applies the high-frequency CT information condition until a pre-optimized iteration step, and drops it afterwards to enable both similar and differing CT/CBCT anatomies to be reconstructed. PFGDM-B, on the other hand, continuously applies the prior CT information condition in every reconstruction step, while with a decaying mechanism, to gradually phase out the reconstruction guidance from the prior CT scans. The two variants of PFGDM were tested and compared with current available LA-CBCT reconstruction solutions, via metrics including PSNR and SSIM. PFGDM outperformed all traditional and diffusion model-based methods. PFGDM reconstructs high-quality LA-CBCTs under very-limited gantry angles, allowing faster and more flexible CBCT scans with dose reductions.","sentences":["Cone-beam computed tomography (CBCT) is widely used in image-guided radiotherapy.","Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is highly desired for improved imaging efficiency, dose reduction, and better mechanical clearance.","LA-CBCT reconstruction, however, suffers from severe under-sampling artifacts, making it a highly ill-posed inverse problem.","Diffusion models can generate data/images by reversing a data-noising process through learned data distributions; and can be incorporated as a denoiser/regularizer in LA-CBCT reconstruction.","In this study, we developed a diffusion model-based framework, prior frequency-guided diffusion model (PFGDM), for robust and structure-preserving LA-CBCT reconstruction.","PFGDM uses a conditioned diffusion model as a regularizer for LA-CBCT reconstruction, and the condition is based on high-frequency information extracted from patient-specific prior CT scans which provides a strong anatomical prior for LA-CBCT reconstruction.","Specifically, we developed two variants of PFGDM (PFGDM-A and PFGDM-B) with different conditioning schemes.","PFGDM-A applies the high-frequency CT information condition until a pre-optimized iteration step, and drops it afterwards to enable both similar and differing CT/CBCT anatomies to be reconstructed.","PFGDM-B, on the other hand, continuously applies the prior CT information condition in every reconstruction step, while with a decaying mechanism, to gradually phase out the reconstruction guidance from the prior CT scans.","The two variants of PFGDM were tested and compared with current available LA-CBCT reconstruction solutions, via metrics including PSNR and SSIM.","PFGDM outperformed all traditional and diffusion model-based methods.","PFGDM reconstructs high-quality LA-CBCTs under very-limited gantry angles, allowing faster and more flexible CBCT scans with dose reductions."],"url":"http://arxiv.org/abs/2404.01448v1","category":"physics.med-ph"}
{"created":"2024-04-01 19:32:30","title":"Using Dynamic Safety Margins as Control Barrier Functions","abstract":"This paper provides an approach to design control barrier functions (CBFs) using the notion of dynamic safety margins (DSMs). In particular, it is shown that DSMs are CBFs for an augmented system. The proposed approach can handle multiple state and input constraints using the control-sharing property of CBFs. Moreover, it makes no assumption on the relative degree of the constraints. Numerical simulations show that the method outperforms existing DSM-based approaches, while also guaranteeing safety and recursive feasibility.","sentences":["This paper provides an approach to design control barrier functions (CBFs) using the notion of dynamic safety margins (DSMs).","In particular, it is shown that DSMs are CBFs for an augmented system.","The proposed approach can handle multiple state and input constraints using the control-sharing property of CBFs.","Moreover, it makes no assumption on the relative degree of the constraints.","Numerical simulations show that the method outperforms existing DSM-based approaches, while also guaranteeing safety and recursive feasibility."],"url":"http://arxiv.org/abs/2404.01445v1","category":"eess.SY"}
{"created":"2024-04-01 19:17:45","title":"Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance","abstract":"This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum. We develop a new upper bound on the first-order term in the descent lemma, which is also a function of the gradient norm. We show that Adam with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. Our complexity results for both RMSProp and Adam match with the complexity lower bound established in \\cite{arjevani2023lower}.","sentences":["This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance.","We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum.","Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm.","Based on this result, we show that RMSProp with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum.","We develop a new upper bound on the first-order term in the descent lemma, which is also a function of the gradient norm.","We show that Adam with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. Our complexity results for both RMSProp and Adam match with the complexity lower bound established in \\cite{arjevani2023lower}."],"url":"http://arxiv.org/abs/2404.01436v1","category":"stat.ML"}
{"created":"2024-04-01 19:05:05","title":"When are Unbiased Monte Carlo Estimators More Preferable than Biased Ones?","abstract":"Due to the potential benefits of parallelization, designing unbiased Monte Carlo estimators, primarily in the setting of randomized multilevel Monte Carlo, has recently become very popular in operations research and computational statistics. However, existing work primarily substantiates the benefits of unbiased estimators at an intuitive level or using empirical evaluations. The intuition being that unbiased estimators can be replicated in parallel enabling fast estimation in terms of wall-clock time. This intuition ignores that, typically, bias will be introduced due to impatience because most unbiased estimators necesitate random completion times. This paper provides a mathematical framework for comparing these methods under various metrics, such as completion time and overall computational cost. Under practical assumptions, our findings reveal that unbiased methods typically have superior completion times - the degree of superiority being quantifiable through the tail behavior of their running time distribution - but they may not automatically provide substantial savings in overall computational costs. We apply our findings to Markov Chain Monte Carlo and Multilevel Monte Carlo methods to identify the conditions and scenarios where unbiased methods have an advantage, thus assisting practitioners in making informed choices between unbiased and biased methods.","sentences":["Due to the potential benefits of parallelization, designing unbiased Monte Carlo estimators, primarily in the setting of randomized multilevel Monte Carlo, has recently become very popular in operations research and computational statistics.","However, existing work primarily substantiates the benefits of unbiased estimators at an intuitive level or using empirical evaluations.","The intuition being that unbiased estimators can be replicated in parallel enabling fast estimation in terms of wall-clock time.","This intuition ignores that, typically, bias will be introduced due to impatience because most unbiased estimators necesitate random completion times.","This paper provides a mathematical framework for comparing these methods under various metrics, such as completion time and overall computational cost.","Under practical assumptions, our findings reveal that unbiased methods typically have superior completion times - the degree of superiority being quantifiable through the tail behavior of their running time distribution - but they may not automatically provide substantial savings in overall computational costs.","We apply our findings to Markov Chain Monte Carlo and Multilevel Monte Carlo methods to identify the conditions and scenarios where unbiased methods have an advantage, thus assisting practitioners in making informed choices between unbiased and biased methods."],"url":"http://arxiv.org/abs/2404.01431v1","category":"stat.CO"}
{"created":"2024-04-01 18:59:13","title":"DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery","abstract":"The recovery of occluded human meshes presents challenges for current methods due to the difficulty in extracting effective image features under severe occlusion. In this paper, we introduce DPMesh, an innovative framework for occluded human mesh recovery that capitalizes on the profound diffusion prior about object structure and spatial relationships embedded in a pre-trained text-to-image diffusion model. Unlike previous methods reliant on conventional backbones for vanilla feature extraction, DPMesh seamlessly integrates the pre-trained denoising U-Net with potent knowledge as its image backbone and performs a single-step inference to provide occlusion-aware information. To enhance the perception capability for occluded poses, DPMesh incorporates well-designed guidance via condition injection, which produces effective controls from 2D observations for the denoising U-Net. Furthermore, we explore a dedicated noisy key-point reasoning approach to mitigate disturbances arising from occlusion and crowded scenarios. This strategy fully unleashes the perceptual capability of the diffusion prior, thereby enhancing accuracy. Extensive experiments affirm the efficacy of our framework, as we outperform state-of-the-art methods on both occlusion-specific and standard datasets. The persuasive results underscore its ability to achieve precise and robust 3D human mesh recovery, particularly in challenging scenarios involving occlusion and crowded scenes.","sentences":["The recovery of occluded human meshes presents challenges for current methods due to the difficulty in extracting effective image features under severe occlusion.","In this paper, we introduce DPMesh, an innovative framework for occluded human mesh recovery that capitalizes on the profound diffusion prior about object structure and spatial relationships embedded in a pre-trained text-to-image diffusion model.","Unlike previous methods reliant on conventional backbones for vanilla feature extraction, DPMesh seamlessly integrates the pre-trained denoising U-Net with potent knowledge as its image backbone and performs a single-step inference to provide occlusion-aware information.","To enhance the perception capability for occluded poses, DPMesh incorporates well-designed guidance via condition injection, which produces effective controls from 2D observations for the denoising U-Net.","Furthermore, we explore a dedicated noisy key-point reasoning approach to mitigate disturbances arising from occlusion and crowded scenarios.","This strategy fully unleashes the perceptual capability of the diffusion prior, thereby enhancing accuracy.","Extensive experiments affirm the efficacy of our framework, as we outperform state-of-the-art methods on both occlusion-specific and standard datasets.","The persuasive results underscore its ability to achieve precise and robust 3D human mesh recovery, particularly in challenging scenarios involving occlusion and crowded scenes."],"url":"http://arxiv.org/abs/2404.01424v1","category":"cs.CV"}
{"created":"2024-04-01 18:57:50","title":"Retrieved Atmospheres and Inferred Surface Properties for Exoplanets Using Transmission and Reflected Light Spectroscopy","abstract":"Future astrophysics missions will seek extraterrestrial life via transmission and direct imaging observations. To assess habitability and biosignatures, we need robust retrieval tools to analyze observed spectra, and infer surface and atmospheric properties with their uncertainties. We use a novel retrieval tool to assess accuracy in characterizing near-surface habitability and biosignatures via simulated transmission and direct imaging spectra, based on the Origins Space Telescope (Origins) and LUVOIR mission concepts. We assess our ability to discriminate between an Earth-like and a false-positive O$_3$ TRAPPIST-1 e with transmission spectroscopy. In reflected light, we assess the robustness of retrieval results to un-modeled cloud extinction. We find that assessing habitability using transmission spectra may be challenging due to relative insensitivity to surface temperature and near-surface H$_2$O abundances. Nonetheless, our order of magnitude H$_2$O constraints can discriminate extremely desiccated worlds. Direct imaging is insensitive to surface temperature and subject to the radius/albedo degeneracy, but this method proves highly sensitive to surface water abundance, achieving retrieval precision within 0.1% even with partial clouds. Concerning biosignatures, Origins-like transmission observations ($t=40$ hours) may detect the CO$_2$/CH$_4$ pair on M-dwarf planets and differentiate between biological and false positive O$_3$ using H$_2$O and abundant CO. In contrast, direct imaging observations with LUVOIR-A ($t=10$ hours) are better suited to constraining O$_2$ and O$_3$, and may be sensitive to wavelength-dependent water cloud features, but will struggle to detect modern Earth-like abundances of methane. For direct imaging, we weakly detect a stratospheric ozone bulge by fitting the near-UV wings of the Hartley band.","sentences":["Future astrophysics missions will seek extraterrestrial life via transmission and direct imaging observations.","To assess habitability and biosignatures, we need robust retrieval tools to analyze observed spectra, and infer surface and atmospheric properties with their uncertainties.","We use a novel retrieval tool to assess accuracy in characterizing near-surface habitability and biosignatures via simulated transmission and direct imaging spectra, based on the Origins Space Telescope (Origins) and LUVOIR mission concepts.","We assess our ability to discriminate between an Earth-like and a false-positive O$_3$ TRAPPIST-1 e with transmission spectroscopy.","In reflected light, we assess the robustness of retrieval results to un-modeled cloud extinction.","We find that assessing habitability using transmission spectra may be challenging due to relative insensitivity to surface temperature and near-surface H$_2$O abundances.","Nonetheless, our order of magnitude H$_2$O constraints can discriminate extremely desiccated worlds.","Direct imaging is insensitive to surface temperature and subject to the radius/albedo degeneracy, but this method proves highly sensitive to surface water abundance, achieving retrieval precision within 0.1% even with partial clouds.","Concerning biosignatures, Origins-like transmission observations ($t=40$ hours) may detect the CO$_2$/CH$_4$ pair on M-dwarf planets and differentiate between biological and false positive O$_3$ using H$_2$O and abundant CO.","In contrast, direct imaging observations with LUVOIR-A ($t=10$ hours) are better suited to constraining O$_2$ and O$_3$, and may be sensitive to wavelength-dependent water cloud features, but will struggle to detect modern Earth-like abundances of methane.","For direct imaging, we weakly detect a stratospheric ozone bulge by fitting the near-UV wings of the Hartley band."],"url":"http://arxiv.org/abs/2404.01423v1","category":"astro-ph.EP"}
{"created":"2024-04-01 18:57:49","title":"On Strong Bounds for Trotter and Zeno Product Formulas with Bosonic Applications","abstract":"The Trotter product formula and the quantum Zeno effect are both indispensable tools for constructing time-evolutions using experimentally feasible building blocks. In this work, we discuss assumptions under which quantitative bounds can be proven in the strong operator topology on Banach spaces and provide natural bosonic examples. Specially, we assume the existence of a continuously embedded Banach space, which relatively bounds the involved generators and creates an invariant subspace of the limiting semigroup with a stable restriction. The slightly stronger assumption of admissible subspaces is well-recognized in the realm of hyperbolic evolution systems (time-dependent semigroups), to which the results are extended. By assuming access to a hierarchy of continuously embedded Banach spaces, Suzuki-higher-order bounds can be demonstrated. In bosonic applications, these embedded Banach spaces naturally arise through the number operator, leading to a diverse set of examples encompassing notable instances such as the Bose-Hubbard model, the Ornstein-Uhlenbeck semigroup, and multi-photon driven dissipation used in bosonic error correction.","sentences":["The Trotter product formula and the quantum Zeno effect are both indispensable tools for constructing time-evolutions using experimentally feasible building blocks.","In this work, we discuss assumptions under which quantitative bounds can be proven in the strong operator topology on Banach spaces and provide natural bosonic examples.","Specially, we assume the existence of a continuously embedded Banach space, which relatively bounds the involved generators and creates an invariant subspace of the limiting semigroup with a stable restriction.","The slightly stronger assumption of admissible subspaces is well-recognized in the realm of hyperbolic evolution systems (time-dependent semigroups), to which the results are extended.","By assuming access to a hierarchy of continuously embedded Banach spaces, Suzuki-higher-order bounds can be demonstrated.","In bosonic applications, these embedded Banach spaces naturally arise through the number operator, leading to a diverse set of examples encompassing notable instances such as the Bose-Hubbard model, the Ornstein-Uhlenbeck semigroup, and multi-photon driven dissipation used in bosonic error correction."],"url":"http://arxiv.org/abs/2404.01422v1","category":"math-ph"}
{"created":"2024-04-01 18:57:05","title":"Transport Coefficients of relativistic matter: A detailed formalism with a gross knowledge of their magnitude","abstract":"The present review article has attempted a compact formalism description of transport coefficient calculations for relativistic fluid, which is expected in heavy ion collision experiments. Here, we first address the macroscopic description of relativistic fluid dynamics and then its microscopic description based on the kinetic theory framework. We also address different relaxation time approximation-based models in Boltzmann transport equations, which make a sandwich between Macro and Micro frameworks of relativistic fluid dynamics and finally provide different microscopic expressions of transport coefficients like the fluid's shear viscosity and bulk viscosity. In the numeric part of this review article, we put stress on the two gross components of transport coefficient expressions: relaxation time and thermodynamic phase-space part. Then, we try to tune the relaxation time component to cover earlier theoretical estimations and experimental data-driven estimations for RHIC and LHC matter. By this way of numerical understanding, we provide the final comments on the values of transport coefficients and relaxation time in the context of the (nearly) perfect fluid nature of the RHIC or LHC matter.","sentences":["The present review article has attempted a compact formalism description of transport coefficient calculations for relativistic fluid, which is expected in heavy ion collision experiments.","Here, we first address the macroscopic description of relativistic fluid dynamics and then its microscopic description based on the kinetic theory framework.","We also address different relaxation time approximation-based models in Boltzmann transport equations, which make a sandwich between Macro and Micro frameworks of relativistic fluid dynamics and finally provide different microscopic expressions of transport coefficients like the fluid's shear viscosity and bulk viscosity.","In the numeric part of this review article, we put stress on the two gross components of transport coefficient expressions: relaxation time and thermodynamic phase-space part.","Then, we try to tune the relaxation time component to cover earlier theoretical estimations and experimental data-driven estimations for RHIC and LHC matter.","By this way of numerical understanding, we provide the final comments on the values of transport coefficients and relaxation time in the context of the (nearly) perfect fluid nature of the RHIC or LHC matter."],"url":"http://arxiv.org/abs/2404.01421v1","category":"nucl-th"}
{"created":"2024-04-01 18:22:23","title":"Presenting Profunctors","abstract":"Motivated by problems in categorical database theory, we introduce and compare two notions of presentation for profunctors, uncurried and curried, which arise intuitively from thinking of profunctors either as functors $\\mathcal{C}^\\text{op} \\times \\mathcal{D} \\to \\textbf{Set}$ or $\\mathcal{C}^\\text{op} \\to \\textbf{Set}^{\\mathcal{D}}$. Although the Cartesian closure of $\\textbf{Cat}$ means these two perspectives can be used interchangeably at the semantic level, a surprising amount of subtlety is revealed when looking through the lens of syntax. Indeed, we prove that finite uncurried presentations are strictly more expressive than finite curried presentations, hence the two notions do not induce the same class of finitely presentable profunctors. Moreover, an explicit construction for the composite of two curried presentations shows that the class of finitely curried presentable profunctors is closed under composition, in contrast with the larger class of finitely uncurried presentable profunctors, which is not. This shows that curried profunctor presentations are more appropriate for computational tasks that use profunctor composition. We package our results on curried profunctor presentations into a double equivalence from a syntactic double category into the double category of profunctors. Finally, we study the relationship between curried and uncurried presentations, leading to the introduction of curryable presentations. These constitute a subcategory of uncurried presentations which is equivalent to the category of curried presentations, therefore acting as a bridge between the two syntactic choices.","sentences":["Motivated by problems in categorical database theory, we introduce and compare two notions of presentation for profunctors, uncurried and curried, which arise intuitively from thinking of profunctors either as functors $\\mathcal{C}^\\text{op} \\times \\mathcal{D} \\to \\textbf{Set}$ or $\\mathcal{C}^\\text{op} \\to \\textbf{Set}^{\\mathcal{D}}$.","Although the Cartesian closure of $\\textbf{Cat}$ means these two perspectives can be used interchangeably at the semantic level, a surprising amount of subtlety is revealed when looking through the lens of syntax.","Indeed, we prove that finite uncurried presentations are strictly more expressive than finite curried presentations, hence the two notions do not induce the same class of finitely presentable profunctors.","Moreover, an explicit construction for the composite of two curried presentations shows that the class of finitely curried presentable profunctors is closed under composition, in contrast with the larger class of finitely uncurried presentable profunctors, which is not.","This shows that curried profunctor presentations are more appropriate for computational tasks that use profunctor composition.","We package our results on curried profunctor presentations into a double equivalence from a syntactic double category into the double category of profunctors.","Finally, we study the relationship between curried and uncurried presentations, leading to the introduction of curryable presentations.","These constitute a subcategory of uncurried presentations which is equivalent to the category of curried presentations, therefore acting as a bridge between the two syntactic choices."],"url":"http://arxiv.org/abs/2404.01406v1","category":"math.CT"}
{"created":"2024-04-01 18:17:49","title":"The curious case of A31P, a topology-switching mutant of the Repressor of Primer protein : A molecular dynamics study of its folding and misfolding","abstract":"The effect of mutations on protein structures is usually rather localized and minor. Finding a mutation that can single-handedly change the fold and/or topology of a protein structure is a rare exception. The A31P mutant of the homodimeric Repressor of Primer (Rop) protein is one such exception: This single mutation -and as demonstrated by two independent crystal structure determinations- can convert the canonical (left-handed/all-antiparallel) 4-alpha-helical bundle of Rop, to a new form (right-handed/mixed parallel and antiparallel bundle) displaying a previously unobserved 'bisecting U' topology. The main problem with understanding the dramatic effect of this mutation on the folding of Rop is to understand its very existence : Most computational methods appear to agree that the mutation should have had no appreciable effect, with the majority of energy minimization methods and protein structure prediction protocols indicating that this mutation is fully consistent with the native Rop structure, requiring only a local and minor change at the mutation site. Here we use two long (10 us each) molecular dynamics simulations to compare the stability and dynamics of the native Rop versus a hypothetical structure that is identical with the native Rop but is carrying this single Alanine-31 to Proline mutation. Comparative analysis of the two trajectories convincingly shows that in contrast to the indications from energy minimization -but in agreement with the experimental data-, this hypothetical native-like A31P structure is unstable, with its turn regions almost completely unfolding, even under the relatively mild 320K NpT simulations that we have used for this study. We discuss the implication of these findings for the folding of the A31P mutant, especially with respect to the proposed model of a double-funneled energy landscape.","sentences":["The effect of mutations on protein structures is usually rather localized and minor.","Finding a mutation that can single-handedly change the fold and/or topology of a protein structure is a rare exception.","The A31P mutant of the homodimeric Repressor of Primer (Rop) protein is one such exception: This single mutation -and as demonstrated by two independent crystal structure determinations- can convert the canonical (left-handed/all-antiparallel) 4-alpha-helical bundle of Rop, to a new form (right-handed/mixed parallel and antiparallel bundle) displaying a previously unobserved 'bisecting U' topology.","The main problem with understanding the dramatic effect of this mutation on the folding of Rop is to understand its very existence : Most computational methods appear to agree that the mutation should have had no appreciable effect, with the majority of energy minimization methods and protein structure prediction protocols indicating that this mutation is fully consistent with the native Rop structure, requiring only a local and minor change at the mutation site.","Here we use two long (10 us each) molecular dynamics simulations to compare the stability and dynamics of the native Rop versus a hypothetical structure that is identical with the native Rop but is carrying this single Alanine-31 to Proline mutation.","Comparative analysis of the two trajectories convincingly shows that in contrast to the indications from energy minimization -but in agreement with the experimental data-, this hypothetical native-like A31P structure is unstable, with its turn regions almost completely unfolding, even under the relatively mild 320K NpT simulations that we have used for this study.","We discuss the implication of these findings for the folding of the A31P mutant, especially with respect to the proposed model of a double-funneled energy landscape."],"url":"http://arxiv.org/abs/2404.01405v1","category":"q-bio.BM"}
{"created":"2024-04-02 15:01:14","title":"Brownian Particles and Matter Waves","abstract":"In view of the remarkable progress in micro-rheology to monitor the random motion of Brownian particles with size as small as few nanometers, in association that de Broglie matter waves have been experimentally observed for large molecules of comparable nanometer size; we examine whether Brownian particles can manifest a particle-wave duality without employing a priori arguments from quantum decoherence. First, we examine the case where Brownian particles are immersed in a memoryless viscous fluid with a time-independent diffusion coefficient; and the requirement for the Brownian particles to manifest a particle-wave duality leads to the untenable result that the diffusion coefficient has to be proportional to the inverse time; therefore, diverging at early times. This finding agrees with past conclusions--that quantum mechanics is not equivalent to a Markovian diffusion process. Next, we examine the case where the Brownian particle is trapped in a harmonic potential well with and without dissipation. Both solutions of the Fokker-Plank equation for the case with dissipation, and of the Schrodinger equation for the case without dissipation lead to the same physically acceptable result-that for the Brownian particle to manifest a particle-wave duality, its mean kinetic energy needs to be half the ground-state energy of the quantum harmonic oscillator. Our one-dimensional calculations show that for this to happen, the trapping needs to be very strong so that a Brownian nanoparticle needs to be embedded in an extremely stiff solid.","sentences":["In view of the remarkable progress in micro-rheology to monitor the random motion of Brownian particles with size as small as few nanometers, in association that de Broglie matter waves have been experimentally observed for large molecules of comparable nanometer size; we examine whether Brownian particles can manifest a particle-wave duality without employing a priori arguments from quantum decoherence.","First, we examine the case where Brownian particles are immersed in a memoryless viscous fluid with a time-independent diffusion coefficient; and the requirement for the Brownian particles to manifest a particle-wave duality leads to the untenable result that the diffusion coefficient has to be proportional to the inverse time; therefore, diverging at early times.","This finding agrees with past conclusions--that quantum mechanics is not equivalent to a Markovian diffusion process.","Next, we examine the case where the Brownian particle is trapped in a harmonic potential well with and without dissipation.","Both solutions of the Fokker-Plank equation for the case with dissipation, and of the Schrodinger equation for the case without dissipation lead to the same physically acceptable result-that for the Brownian particle to manifest a particle-wave duality, its mean kinetic energy needs to be half the ground-state energy of the quantum harmonic oscillator.","Our one-dimensional calculations show that for this to happen, the trapping needs to be very strong so that a Brownian nanoparticle needs to be embedded in an extremely stiff solid."],"url":"http://arxiv.org/abs/2404.02016v1","category":"quant-ph"}
{"created":"2024-04-02 13:27:28","title":"PREGO: online mistake detection in PRocedural EGOcentric videos","abstract":"Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen. This capability has a wide range of applications across various fields, such as manufacturing and healthcare. The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures. However, no technique can currently detect open-set procedural mistakes online. We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos. PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions. Mistake detection is performed by comparing the recognized current action with the expected future one. We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively.","sentences":["Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen.","This capability has a wide range of applications across various fields, such as manufacturing and healthcare.","The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures.","However, no technique can currently detect open-set procedural mistakes online.","We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos.","PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions.","Mistake detection is performed by comparing the recognized current action with the expected future one.","We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively."],"url":"http://arxiv.org/abs/2404.01933v1","category":"cs.CV"}
{"created":"2024-04-02 11:03:13","title":"Semi-Supervised Domain Adaptation for Wildfire Detection","abstract":"Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change. In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries. Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection. Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires. With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset. Our dataset is available at https://github.com/BloomBerry/LADA.","sentences":["Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change.","In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries.","Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection.","Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires.","With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset.","Our dataset is available at https://github.com/BloomBerry/LADA."],"url":"http://arxiv.org/abs/2404.01842v1","category":"cs.CV"}
{"created":"2024-04-02 10:34:46","title":"Mathematical modeling and numerical multigoal-oriented a posteriori error control and adaptivity for a stationary, nonlinear, coupled flow temperature model with temperature dependent density","abstract":"In this work, we develop adaptive schemes using goal-oriented error control for a highly nonlinear flow temperature model with temperature dependent density. The dual-weighted residual method for computing error indicators to steer mesh refinement and solver control is employed. The error indicators are used to employ adaptive algorithms, which are substantiated with several numerical tests. Therein, error reductions and effectivity indices are consulted to establish the robustness and efficiency of our framework.","sentences":["In this work, we develop adaptive schemes using goal-oriented error control for a highly nonlinear flow temperature model with temperature dependent density.","The dual-weighted residual method for computing error indicators to steer mesh refinement and solver control is employed.","The error indicators are used to employ adaptive algorithms, which are substantiated with several numerical tests.","Therein, error reductions and effectivity indices are consulted to establish the robustness and efficiency of our framework."],"url":"http://arxiv.org/abs/2404.01823v1","category":"math.NA"}
{"created":"2024-04-02 08:55:56","title":"A Posteriori Single- and Multi-Goal Error Control and Adaptivity for Partial Differential Equations","abstract":"This work reviews goal-oriented a posteriori error control, adaptivity and solver control for finite element approximations to boundary and initial-boundary value problems for stationary and non-stationary partial differential equations, respectively. In particular, coupled field problems with different physics may require simultaneously the accurate evaluation of several quantities of interest, which is achieved with multi-goal oriented error control. Sensitivity measures are obtained by solving an adjoint problem. Error localization is achieved with the help of a partition-of-unity. We also review and extend theoretical results for efficiency and reliability by employing a saturation assumption. The resulting adaptive algorithms allow to balance discretization and non-linear iteration errors, and are demonstrated for four applications: Poisson's problem, non-linear elliptic boundary value problems, stationary incompressible Navier-Stokes equations, and regularized parabolic $p$-Laplace initial-boundary value problems. Therein, different finite element discretizations in two different software libraries are utilized, which are partially accompanied with open-source implementations on GitHub.","sentences":["This work reviews goal-oriented a posteriori error control, adaptivity and solver control for finite element approximations to boundary and initial-boundary value problems for stationary and non-stationary partial differential equations, respectively.","In particular, coupled field problems with different physics may require simultaneously the accurate evaluation of several quantities of interest, which is achieved with multi-goal oriented error control.","Sensitivity measures are obtained by solving an adjoint problem.","Error localization is achieved with the help of a partition-of-unity.","We also review and extend theoretical results for efficiency and reliability by employing a saturation assumption.","The resulting adaptive algorithms allow to balance discretization and non-linear iteration errors, and are demonstrated for four applications: Poisson's problem, non-linear elliptic boundary value problems, stationary incompressible Navier-Stokes equations, and regularized parabolic $p$-Laplace initial-boundary value problems.","Therein, different finite element discretizations in two different software libraries are utilized, which are partially accompanied with open-source implementations on GitHub."],"url":"http://arxiv.org/abs/2404.01738v1","category":"math.NA"}
{"created":"2024-04-02 04:30:53","title":"SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies","abstract":"With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale. ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong fairness. We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair. We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments. At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver. We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links. Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks.","sentences":["With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale.","ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong fairness.","We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair.","We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments.","At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver.","We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links.","Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks."],"url":"http://arxiv.org/abs/2404.01630v1","category":"cs.NI"}
{"created":"2024-04-02 02:17:50","title":"Diffusion Deepfake","abstract":"Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection. The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes. Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality. Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets. Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation. Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility. To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods. This involves expanding the diversity of both manipulation techniques and image domains. Our findings underscore that increasing training data diversity results in improved generalizability. Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity. This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples. Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly.","sentences":["Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection.","The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes.","Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality.","Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets.","Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation.","Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility.","To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods.","This involves expanding the diversity of both manipulation techniques and image domains.","Our findings underscore that increasing training data diversity results in improved generalizability.","Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity.","This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples.","Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly."],"url":"http://arxiv.org/abs/2404.01579v1","category":"cs.CV"}
{"created":"2024-04-02 02:05:17","title":"DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones -- Extended Version","abstract":"Recently, swarms or formations of drones have received increased interest both in the literature and in applications. To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks. One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service. In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes. In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications. We describe the involved protocols and provide a mainly simulation-based performance analysis of VarDis.","sentences":["Recently, swarms or formations of drones have received increased interest both in the literature and in applications.","To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks.","One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service.","In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes.","In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications.","We describe the involved protocols and provide a mainly simulation-based performance analysis of VarDis."],"url":"http://arxiv.org/abs/2404.01570v1","category":"cs.NI"}
{"created":"2024-04-01 23:44:14","title":"General relativistic force-free electrodynamics with a discontinuous Galerkin-finite difference hybrid method","abstract":"Relativistic plasmas around compact objects can sometimes be approximated as being force-free. In this limit, the plasma inertia is negligible and the overall dynamics is governed by global electric currents. We present a novel numerical approach for simulating such force-free plasmas, which allows for high accuracy in smooth regions as well as capturing dissipation in current sheets. Using a high-order accurate discontinuous Galerkin method augmented with a conservative finite-difference method, we demonstrate efficient global simulations of black hole and neutron star magnetospheres. In addition to a series of challenging test problems, we show that our approach can-depending on the physical properties of the system and the numerical implementation-be up to 10x more efficient than conventional simulations, with a speedup of 2-3x for most problems we consider in practice.","sentences":["Relativistic plasmas around compact objects can sometimes be approximated as being force-free.","In this limit, the plasma inertia is negligible and the overall dynamics is governed by global electric currents.","We present a novel numerical approach for simulating such force-free plasmas, which allows for high accuracy in smooth regions as well as capturing dissipation in current sheets.","Using a high-order accurate discontinuous Galerkin method augmented with a conservative finite-difference method, we demonstrate efficient global simulations of black hole and neutron star magnetospheres.","In addition to a series of challenging test problems, we show that our approach can-depending on the physical properties of the system and the numerical implementation-be up to 10x more efficient than conventional simulations, with a speedup of 2-3x for most problems we consider in practice."],"url":"http://arxiv.org/abs/2404.01531v1","category":"astro-ph.IM"}
{"created":"2024-04-01 23:01:07","title":"Fair MP-BOOST: Fair and Interpretable Minipatch Boosting","abstract":"Ensemble methods, particularly boosting, have established themselves as highly effective and widely embraced machine learning techniques for tabular data. In this paper, we aim to leverage the robust predictive power of traditional boosting methods while enhancing fairness and interpretability. To achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that balances fairness and accuracy by adaptively learning features and observations during training. Specifically, Fair MP-Boost sequentially samples small subsets of observations and features, termed minipatches (MP), according to adaptively learned feature and observation sampling probabilities. We devise these probabilities by combining loss functions, or by combining feature importance scores to address accuracy and fairness simultaneously. Hence, Fair MP-Boost prioritizes important and fair features along with challenging instances, to select the most relevant minipatches for learning. The learned probability distributions also yield intrinsic interpretations of feature importance and important observations in Fair MP-Boost. Through empirical evaluation of simulated and benchmark datasets, we showcase the interpretability, accuracy, and fairness of Fair MP-Boost.","sentences":["Ensemble methods, particularly boosting, have established themselves as highly effective and widely embraced machine learning techniques for tabular data.","In this paper, we aim to leverage the robust predictive power of traditional boosting methods while enhancing fairness and interpretability.","To achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that balances fairness and accuracy by adaptively learning features and observations during training.","Specifically, Fair MP-Boost sequentially samples small subsets of observations and features, termed minipatches (MP), according to adaptively learned feature and observation sampling probabilities.","We devise these probabilities by combining loss functions, or by combining feature importance scores to address accuracy and fairness simultaneously.","Hence, Fair MP-Boost prioritizes important and fair features along with challenging instances, to select the most relevant minipatches for learning.","The learned probability distributions also yield intrinsic interpretations of feature importance and important observations in Fair MP-Boost.","Through empirical evaluation of simulated and benchmark datasets, we showcase the interpretability, accuracy, and fairness of Fair MP-Boost."],"url":"http://arxiv.org/abs/2404.01521v1","category":"stat.ML"}
{"created":"2024-04-01 20:37:14","title":"Performance triggered adaptive model reduction for soil moisture estimation in precision irrigation","abstract":"Accurate soil moisture information is crucial for developing precise irrigation control strategies to enhance water use efficiency. Soil moisture estimation based on limited soil moisture sensors is crucial for obtaining comprehensive soil moisture information when dealing with large-scale agricultural fields. The major challenge in soil moisture estimation lies in the high dimensionality of the spatially discretized agro-hydrological models. In this work, we propose a performance-triggered adaptive model reduction approach to address this challenge. The pro- posed approach employs a trajectory-based unsupervised machine learning technique, and a prediction performance-based triggering scheme is designed to govern model updates adaptively in a way such that the prediction error between the reduced model and the original model over a prediction horizon is maintained below a predetermined threshold. An adaptive extended Kalman filter (EKF) is designed based on the reduced model for soil moisture estimation. The applicability and performance of the proposed approach are evaluated extensively through the application to a simulated large-scale agricultural field.","sentences":["Accurate soil moisture information is crucial for developing precise irrigation control strategies to enhance water use efficiency.","Soil moisture estimation based on limited soil moisture sensors is crucial for obtaining comprehensive soil moisture information when dealing with large-scale agricultural fields.","The major challenge in soil moisture estimation lies in the high dimensionality of the spatially discretized agro-hydrological models.","In this work, we propose a performance-triggered adaptive model reduction approach to address this challenge.","The pro- posed approach employs a trajectory-based unsupervised machine learning technique, and a prediction performance-based triggering scheme is designed to govern model updates adaptively in a way such that the prediction error between the reduced model and the original model over a prediction horizon is maintained below a predetermined threshold.","An adaptive extended Kalman filter (EKF) is designed based on the reduced model for soil moisture estimation.","The applicability and performance of the proposed approach are evaluated extensively through the application to a simulated large-scale agricultural field."],"url":"http://arxiv.org/abs/2404.01468v1","category":"eess.SY"}
{"created":"2024-04-01 18:28:57","title":"Improving Quantum Approximate Optimization by Noise-Directed Adaptive Remapping","abstract":"We present \\emph{Noise-Directed Adaptive Remapping} (NDAR), a heuristic meta-algorithm for approximately solving binary optimization problems by leveraging certain types of noise. We consider access to a noisy quantum processor with dynamics that features a global attractor state. In a standard setting, such noise can be detrimental to the quantum optimization performance. In NDAR, the algorithm bootstraps the attractor by iteratively gauge-transforming the cost-function Hamiltonian. In each iteration step, the gauge transformation effectively changes the attractor state into a higher-quality solution of the cost Hamiltonian based on the results of variational optimization in the previous step. The end result is that noise aids variational optimization, as opposed to hindering it. We demonstrate the effectiveness of our protocol in Quantum Approximate Optimization Algorithm experiments with subsystems of the newest generation of Rigetti Computing's superconducting device Ankaa-2. We obtain approximation ratios (of best-found solutions) $0.9$-$0.96$ (spread across instances) for multiple instances of random, fully connected graphs (Sherrington-Kirkpatrick model) on $n=82$ qubits, using only depth $p=1$ (noisy) QAOA in conjunction with NDAR. This compares to $0.34$-$0.51$ for vanilla $p=1$ QAOA with the same number of function calls.","sentences":["We present \\emph{Noise-Directed Adaptive Remapping} (NDAR), a heuristic meta-algorithm for approximately solving binary optimization problems by leveraging certain types of noise.","We consider access to a noisy quantum processor with dynamics that features a global attractor state.","In a standard setting, such noise can be detrimental to the quantum optimization performance.","In NDAR, the algorithm bootstraps the attractor by iteratively gauge-transforming the cost-function Hamiltonian.","In each iteration step, the gauge transformation effectively changes the attractor state into a higher-quality solution of the cost Hamiltonian based on the results of variational optimization in the previous step.","The end result is that noise aids variational optimization, as opposed to hindering it.","We demonstrate the effectiveness of our protocol in Quantum Approximate Optimization Algorithm experiments with subsystems of the newest generation of Rigetti Computing's superconducting device Ankaa-2.","We obtain approximation ratios (of best-found solutions) $0.9$-$0.96$ (spread across instances) for multiple instances of random, fully connected graphs (Sherrington-Kirkpatrick model) on $n=82$ qubits, using only depth $p=1$ (noisy) QAOA in conjunction with NDAR.","This compares to $0.34$-$0.51$ for vanilla $p=1$ QAOA with the same number of function calls."],"url":"http://arxiv.org/abs/2404.01412v1","category":"quant-ph"}
{"created":"2024-04-01 18:00:02","title":"Constraints on Dark Matter from Dynamical Heating of Stars in Ultrafaint Dwarfs. Part 2: Substructure and the Primordial Power Spectrum","abstract":"There is a large and growing interest in observations of small-scale structure in dark matter. We propose a new way to probe dark matter structures in the $\\sim 10 - 10^8 \\, M_\\odot$ range. This allows us to constrain the primordial power spectrum over shorter distances scales than possible with direct observations from the CMB. For $k$ in the range $\\sim 10 - 1000 \\, {\\rm Mpc}^{-1}$ our constraints on the power spectrum are orders of magnitude stronger than previous bounds.   We also set some of the strongest constraints on dark matter isocurvature perturbations. Our method relies on the heating effect such dark matter substructures would have on the distribution of stars in an ultra-faint dwarf galaxy. Many models of inflation produce enhanced power at these short distance scales and can thus be constrained by our observation. Further, many dark matter models such as axion dark matter, self-interacting dark matter and dissipative dark matter, produce dense structures which could be constrained this way.","sentences":["There is a large and growing interest in observations of small-scale structure in dark matter.","We propose a new way to probe dark matter structures in the $\\sim 10 - 10^8 \\, M_\\odot$ range.","This allows us to constrain the primordial power spectrum over shorter distances scales than possible with direct observations from the CMB.","For $k$ in the range $\\sim 10 - 1000 \\, {\\rm Mpc}^{-1}$ our constraints on the power spectrum are orders of magnitude stronger than previous bounds.   ","We also set some of the strongest constraints on dark matter isocurvature perturbations.","Our method relies on the heating effect such dark matter substructures would have on the distribution of stars in an ultra-faint dwarf galaxy.","Many models of inflation produce enhanced power at these short distance scales and can thus be constrained by our observation.","Further, many dark matter models such as axion dark matter, self-interacting dark matter and dissipative dark matter, produce dense structures which could be constrained this way."],"url":"http://arxiv.org/abs/2404.01378v1","category":"hep-ph"}
{"created":"2024-04-01 17:56:25","title":"Weak-coupling limits of the quantum Langevin equation for an oscillator","abstract":"The quantum Langevin equation as obtained from the independent-oscillator model describes a strong-coupling situation, devoid of the Born-Markov approximation that is employed in the context of the Gorini-Kossakowski-Sudarshan-Lindblad equation. The question we address is what happens when we implement such `Born-Markov'-like approximations at the level of the quantum Langevin equation for a harmonic oscillator which carries a noise term satisfying a fluctuation-dissipation theorem. In this backdrop, we also comment on the rotating-wave approximation.","sentences":["The quantum Langevin equation as obtained from the independent-oscillator model describes a strong-coupling situation, devoid of the Born-Markov approximation that is employed in the context of the Gorini-Kossakowski-Sudarshan-Lindblad equation.","The question we address is what happens when we implement such `Born-Markov'-like approximations at the level of the quantum Langevin equation for a harmonic oscillator which carries a noise term satisfying a fluctuation-dissipation theorem.","In this backdrop, we also comment on the rotating-wave approximation."],"url":"http://arxiv.org/abs/2404.01285v1","category":"quant-ph"}
{"created":"2024-04-01 17:54:34","title":"LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization","abstract":"Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them beyond head-only transfer learning.","sentences":["Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video.","The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities.","Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL.","To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos.","LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges.","These adapters run parallel to the video backbone to significantly reduce memory footprint.","LoSA also includes Long-Short-range Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head.","Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them beyond head-only transfer learning."],"url":"http://arxiv.org/abs/2404.01282v1","category":"cs.CV"}
{"created":"2024-04-01 17:48:15","title":"Language Guided Domain Generalized Medical Image Segmentation","abstract":"Single source domain generalization (SDG) holds promise for more reliable and consistent image segmentation across real-world clinical settings particularly in the medical domain, where data privacy and acquisition cost constraints often limit the availability of diverse datasets. Depending solely on visual features hampers the model's capacity to adapt effectively to various domains, primarily because of the presence of spurious correlations and domain-specific characteristics embedded within the image features. Incorporating text features alongside visual features is a potential solution to enhance the model's understanding of the data, as it goes beyond pixel-level information to provide valuable context. Textual cues describing the anatomical structures, their appearances, and variations across various imaging modalities can guide the model in domain adaptation, ultimately contributing to more robust and consistent segmentation. In this paper, we propose an approach that explicitly leverages textual information by incorporating a contrastive learning mechanism guided by the text encoder features to learn a more robust feature representation. We assess the effectiveness of our text-guided contrastive feature alignment technique in various scenarios, including cross-modality, cross-sequence, and cross-site settings for different segmentation tasks. Our approach achieves favorable performance against existing methods in literature. Our code and model weights are available at https://github.com/ShahinaKK/LG_SDG.git.","sentences":["Single source domain generalization (SDG) holds promise for more reliable and consistent image segmentation across real-world clinical settings particularly in the medical domain, where data privacy and acquisition cost constraints often limit the availability of diverse datasets.","Depending solely on visual features hampers the model's capacity to adapt effectively to various domains, primarily because of the presence of spurious correlations and domain-specific characteristics embedded within the image features.","Incorporating text features alongside visual features is a potential solution to enhance the model's understanding of the data, as it goes beyond pixel-level information to provide valuable context.","Textual cues describing the anatomical structures, their appearances, and variations across various imaging modalities can guide the model in domain adaptation, ultimately contributing to more robust and consistent segmentation.","In this paper, we propose an approach that explicitly leverages textual information by incorporating a contrastive learning mechanism guided by the text encoder features to learn a more robust feature representation.","We assess the effectiveness of our text-guided contrastive feature alignment technique in various scenarios, including cross-modality, cross-sequence, and cross-site settings for different segmentation tasks.","Our approach achieves favorable performance against existing methods in literature.","Our code and model weights are available at https://github.com/ShahinaKK/LG_SDG.git."],"url":"http://arxiv.org/abs/2404.01272v1","category":"cs.CV"}
{"created":"2024-04-01 17:22:07","title":"UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing","abstract":"Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.","sentences":["Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge.","In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge.","We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts.","We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters.","Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts.","Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models.","Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models."],"url":"http://arxiv.org/abs/2404.01253v1","category":"cs.CL"}
{"created":"2024-04-01 17:12:47","title":"FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Registration","abstract":"Diffeomorphic Image Registration is a critical part of the analysis in various imaging modalities and downstream tasks like image translation, segmentation, and atlas building. Registration algorithms based on optimization have stood the test of time in terms of accuracy, reliability, and robustness across a wide spectrum of modalities and acquisition settings. However, these algorithms converge slowly, are prohibitively expensive to run, and their usage requires a steep learning curve, limiting their scalability to larger clinical and scientific studies. In this paper, we develop multi-scale Adaptive Riemannian Optimization algorithms for diffeomorphic image registration. We demonstrate compelling improvements on image registration across a spectrum of modalities and anatomies by measuring structural and landmark overlap of the registered image volumes. Our proposed framework leads to a consistent improvement in performance, and from 300x up to 2000x speedup over existing algorithms. Our modular library design makes it easy to use and allows customization via user-defined cost functions.","sentences":["Diffeomorphic Image Registration is a critical part of the analysis in various imaging modalities and downstream tasks like image translation, segmentation, and atlas building.","Registration algorithms based on optimization have stood the test of time in terms of accuracy, reliability, and robustness across a wide spectrum of modalities and acquisition settings.","However, these algorithms converge slowly, are prohibitively expensive to run, and their usage requires a steep learning curve, limiting their scalability to larger clinical and scientific studies.","In this paper, we develop multi-scale Adaptive Riemannian Optimization algorithms for diffeomorphic image registration.","We demonstrate compelling improvements on image registration across a spectrum of modalities and anatomies by measuring structural and landmark overlap of the registered image volumes.","Our proposed framework leads to a consistent improvement in performance, and from 300x up to 2000x speedup over existing algorithms.","Our modular library design makes it easy to use and allows customization via user-defined cost functions."],"url":"http://arxiv.org/abs/2404.01249v1","category":"cs.CV"}
{"created":"2024-04-01 17:08:50","title":"An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance","abstract":"Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.","sentences":["Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning.","While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text.","In this work, we take a first step towards translating images to make them culturally relevant.","First, we build three pipelines comprising state-of-the-art generative models to do the task.","Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications.","We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation.","We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop.","Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task.","Our code and data is released here: https://github.com/simran-khanuja/image-transcreation."],"url":"http://arxiv.org/abs/2404.01247v1","category":"cs.CL"}
{"created":"2024-04-01 17:03:16","title":"Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets","abstract":"Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained language model and only update the selected parameters together with prompt-related parameters when adapting to the downstream tasks. We verify the effectiveness of our LTP framework on cross-lingual tasks, specifically targeting low-resource languages. Our approach outperforms the baselines by only updating 20\\% of the original parameters.","sentences":["Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters).","Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation.","In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts.","The LTP offers a simpler implementation and requires only a one-time execution.","We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime.","Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective.","Then, we prepend soft prompts to the original pre-trained language model and only update the selected parameters together with prompt-related parameters when adapting to the downstream tasks.","We verify the effectiveness of our LTP framework on cross-lingual tasks, specifically targeting low-resource languages.","Our approach outperforms the baselines by only updating 20\\% of the original parameters."],"url":"http://arxiv.org/abs/2404.01242v1","category":"cs.CL"}
{"created":"2024-04-01 16:51:13","title":"Open-Vocabulary Federated Learning with Multimodal Prototyping","abstract":"Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechanism. Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.","sentences":["Existing federated learning (FL) studies usually assume the training label space and test label space are identical.","However, in real-world applications, this assumption is too ideal to be true.","A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems.","Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL.","That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes.","To address this problem, we leverage the pre-trained vision-language models (VLMs).","In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP).","Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechanism.","Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories.","Our empirical evaluation on various datasets validates the effectiveness of Fed-MP."],"url":"http://arxiv.org/abs/2404.01232v2","category":"cs.CL"}
{"created":"2024-04-01 16:45:25","title":"Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds","abstract":"The higher-order guaranteed lower eigenvalue bounds of the Laplacian in the recent work by Carstensen, Ern, and Puttkammer [Numer. Math. 149, 2021] require a parameter $C_{\\mathrm{st},1}$ that is found $\\textit{not}$ robust as the polynomial degree $p$ increases. This is related to the $H^1$ stability bound of the $L^2$ projection onto polynomials of degree at most $p$ and its growth $C_{\\rm st, 1}\\propto (p+1)^{1/2}$ as $p \\to \\infty$. A similar estimate for the Galerkin projection holds with a $p$-robust constant $C_{\\mathrm{st},2}$ and $C_{\\mathrm{st},2} \\le 2$ for right-isosceles triangles. This paper utilizes the new inequality with the constant $C_{\\mathrm{st},2}$ to design a modified hybrid high-order (HHO) eigensolver that directly computes guaranteed lower eigenvalue bounds under the idealized hypothesis of exact solve of the generalized algebraic eigenvalue problem and a mild explicit condition on the maximal mesh-size in the simplicial mesh. A key advance is a $p$-robust parameter selection.   The analysis of the new method with a different fine-tuned volume stabilization allows for a priori quasi-best approximation and improved $L^2$ error estimates as well as a stabilization-free reliable and efficient a posteriori error control. The associated adaptive mesh-refining algorithm performs superior in computer benchmarks with striking numerical evidence for optimal higher empirical convergence rates.","sentences":["The higher-order guaranteed lower eigenvalue bounds of the Laplacian in the recent work by Carstensen, Ern, and Puttkammer [Numer.","Math. 149, 2021] require a parameter $C_{\\mathrm{st},1}$ that is found $\\textit{not}$ robust as the polynomial degree $p$ increases.","This is related to the $H^1$ stability bound of the $L^2$ projection onto polynomials of degree at most $p$ and its growth $C_{\\rm st, 1}\\propto (p+1)^{1/2}$ as $p \\to \\infty$. A similar estimate for the Galerkin projection holds with a $p$-robust constant $C_{\\mathrm{st},2}$ and $C_{\\mathrm{st},2} \\le 2$ for right-isosceles triangles.","This paper utilizes the new inequality with the constant $C_{\\mathrm{st},2}$ to design a modified hybrid high-order (HHO) eigensolver that directly computes guaranteed lower eigenvalue bounds under the idealized hypothesis of exact solve of the generalized algebraic eigenvalue problem and a mild explicit condition on the maximal mesh-size in the simplicial mesh.","A key advance is a $p$-robust parameter selection.   ","The analysis of the new method with a different fine-tuned volume stabilization allows for a priori quasi-best approximation and improved $L^2$ error estimates as well as a stabilization-free reliable and efficient a posteriori error control.","The associated adaptive mesh-refining algorithm performs superior in computer benchmarks with striking numerical evidence for optimal higher empirical convergence rates."],"url":"http://arxiv.org/abs/2404.01228v1","category":"math.NA"}
{"created":"2024-04-01 16:10:24","title":"Non-Hermitian unidirectional routing of photonic qubits","abstract":"Efficient and tunable qubit unidirectional routers and spin-wave diodes play an important role in both classical and quantum information processing domains. Here, we reveal that multi-level neutral cold atoms can mediate both dissipative and coherent couplings. Interestingly, we investigate and practically implement this paradigm in experiments, successfully synthesizing a system with dual functionality as both a photonic qubit unidirectional router and a spin-wave diode. By manipulating the helicity of the field, we can effectively balance the coherence coupling and dissipative channel, thereby ensuring the unidirectional transfer of photonic qubits. The qubit fidelity exceeds 97.49%, and the isolation ratio achieves $16.8\\pm0.11$ dB while the insertion loss is lower than 0.36 dB. Furthermore, we show that the spin-wave diode can effectively achieve unidirectional information transfer by appropriately setting the coherent coupling parameters. Our work not only provides new ideas for the design of extensive components in quantum networks, but also opens up new possibilities for non-Hermitian quantum physics, complex quantum networks, and unidirectional quantum information transfer.","sentences":["Efficient and tunable qubit unidirectional routers and spin-wave diodes play an important role in both classical and quantum information processing domains.","Here, we reveal that multi-level neutral cold atoms can mediate both dissipative and coherent couplings.","Interestingly, we investigate and practically implement this paradigm in experiments, successfully synthesizing a system with dual functionality as both a photonic qubit unidirectional router and a spin-wave diode.","By manipulating the helicity of the field, we can effectively balance the coherence coupling and dissipative channel, thereby ensuring the unidirectional transfer of photonic qubits.","The qubit fidelity exceeds 97.49%, and the isolation ratio achieves $16.8\\pm0.11$ dB while the insertion loss is lower than 0.36 dB.","Furthermore, we show that the spin-wave diode can effectively achieve unidirectional information transfer by appropriately setting the coherent coupling parameters.","Our work not only provides new ideas for the design of extensive components in quantum networks, but also opens up new possibilities for non-Hermitian quantum physics, complex quantum networks, and unidirectional quantum information transfer."],"url":"http://arxiv.org/abs/2404.01211v1","category":"quant-ph"}
{"created":"2024-04-01 16:02:21","title":"Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories","abstract":"Cyber resilience is a complementary concept to cybersecurity, focusing on the preparation, response, and recovery from cyber threats that are challenging to prevent. Organizations increasingly face such threats in an evolving cyber threat landscape. Understanding and establishing foundations for cyber resilience provide a quantitative and systematic approach to cyber risk assessment, mitigation policy evaluation, and risk-informed defense design. A systems-scientific view toward cyber risks provides holistic and system-level solutions. This chapter starts with a systemic view toward cyber risks and presents the confluence of game theory, control theory, and learning theories, which are three major pillars for the design of cyber resilience mechanisms to counteract increasingly sophisticated and evolving threats in our networks and organizations. Game and control theoretic methods provide a set of modeling frameworks to capture the strategic and dynamic interactions between defenders and attackers. Control and learning frameworks together provide a feedback-driven mechanism that enables autonomous and adaptive responses to threats. Game and learning frameworks offer a data-driven approach to proactively reason about adversarial behaviors and resilient strategies. The confluence of the three lays the theoretical foundations for the analysis and design of cyber resilience. This chapter presents various theoretical paradigms, including dynamic asymmetric games, moving horizon control, conjectural learning, and meta-learning, as recent advances at the intersection. This chapter concludes with future directions and discussions of the role of neurosymbolic learning and the synergy between foundation models and game models in cyber resilience.","sentences":["Cyber resilience is a complementary concept to cybersecurity, focusing on the preparation, response, and recovery from cyber threats that are challenging to prevent.","Organizations increasingly face such threats in an evolving cyber threat landscape.","Understanding and establishing foundations for cyber resilience provide a quantitative and systematic approach to cyber risk assessment, mitigation policy evaluation, and risk-informed defense design.","A systems-scientific view toward cyber risks provides holistic and system-level solutions.","This chapter starts with a systemic view toward cyber risks and presents the confluence of game theory, control theory, and learning theories, which are three major pillars for the design of cyber resilience mechanisms to counteract increasingly sophisticated and evolving threats in our networks and organizations.","Game and control theoretic methods provide a set of modeling frameworks to capture the strategic and dynamic interactions between defenders and attackers.","Control and learning frameworks together provide a feedback-driven mechanism that enables autonomous and adaptive responses to threats.","Game and learning frameworks offer a data-driven approach to proactively reason about adversarial behaviors and resilient strategies.","The confluence of the three lays the theoretical foundations for the analysis and design of cyber resilience.","This chapter presents various theoretical paradigms, including dynamic asymmetric games, moving horizon control, conjectural learning, and meta-learning, as recent advances at the intersection.","This chapter concludes with future directions and discussions of the role of neurosymbolic learning and the synergy between foundation models and game models in cyber resilience."],"url":"http://arxiv.org/abs/2404.01205v1","category":"eess.SY"}
{"created":"2024-04-01 15:52:14","title":"Adaptive Query Prompting for Multi-Domain Landmark Detection","abstract":"Medical landmark detection is crucial in various medical imaging modalities and procedures. Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks. In this work, we propose a universal model for multi-domain landmark detection by leveraging transformer architecture and developing a prompting component, named as Adaptive Query Prompting (AQP). Instead of embedding additional modules in the backbone network, we design a separate module to generate prompts that can be effectively extended to any other transformer network. In our proposed AQP, prompts are learnable parameters maintained in a memory space called prompt pool. The central idea is to keep the backbone frozen and then optimize prompts to instruct the model inference process. Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD. Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost. It has the potential to be extended to more landmark detection tasks. We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks. Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks.","sentences":["Medical landmark detection is crucial in various medical imaging modalities and procedures.","Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks.","In this work, we propose a universal model for multi-domain landmark detection by leveraging transformer architecture and developing a prompting component, named as Adaptive Query Prompting (AQP).","Instead of embedding additional modules in the backbone network, we design a separate module to generate prompts that can be effectively extended to any other transformer network.","In our proposed AQP, prompts are learnable parameters maintained in a memory space called prompt pool.","The central idea is to keep the backbone frozen and then optimize prompts to instruct the model inference process.","Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD.","Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost.","It has the potential to be extended to more landmark detection tasks.","We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks.","Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks."],"url":"http://arxiv.org/abs/2404.01194v1","category":"cs.CV"}
{"created":"2024-04-01 15:47:21","title":"Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record","abstract":"The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 2023]. These works relied heavily on automatic metrics as human annotations were limited. To fill this gap, in Chapter 4, we conduct a fine-grained expert annotation of system errors in order to meta-evaluate existing metrics and better understand task-specific issues of domain adaptation and source-summary alignments. To learn a metric less correlated to extractiveness (copy-and-paste), we derive noisy faithfulness labels from an ensemble of existing metrics and train a faithfulness classifier on these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that fine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations and cover fewer salient entities. We improve both coverage and faithfulness by performing sentence-level entity planning based on a set of pre-computed salient entities from the source text, which extends our work on entity-guided news summarization [ACL, 2023], [EMNLP, 2023].","sentences":["The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers.","An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout.","In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions.","In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021].","In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 2023].","These works relied heavily on automatic metrics as human annotations were limited.","To fill this gap, in Chapter 4, we conduct a fine-grained expert annotation of system errors in order to meta-evaluate existing metrics and better understand task-specific issues of domain adaptation and source-summary alignments.","To learn a metric less correlated to extractiveness (copy-and-paste), we derive noisy faithfulness labels from an ensemble of existing metrics and train a faithfulness classifier on these pseudo labels [MLHC 2023].","Finally, in Chapter 5, we demonstrate that fine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations and cover fewer salient entities.","We improve both coverage and faithfulness by performing sentence-level entity planning based on a set of pre-computed salient entities from the source text, which extends our work on entity-guided news summarization [ACL, 2023], [EMNLP, 2023]."],"url":"http://arxiv.org/abs/2404.01189v1","category":"cs.CL"}
{"created":"2024-04-01 15:45:58","title":"MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint","abstract":"We propose MonoBox, an innovative box-supervised segmentation method constrained by monotonicity to liberate its training from the user-unfriendly box-tightness assumption. In contrast to conventional box-supervised segmentation, where the box edges must precisely touch the target boundaries, MonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise segmentation. The 'linchpin' is that, within the noisy zones around box edges, MonoBox discards the traditional misguiding multiple-instance learning loss, and instead optimizes a carefully-designed objective, termed monotonicity constraint. Along directions transitioning from the foreground to background, this new constraint steers responses to adhere to a trend of monotonically decreasing values. Consequently, the originally unreliable learning within the noisy zones is transformed into a correct and effective monotonicity optimization. Moreover, an adaptive label correction is introduced, enabling MonoBox to enhance the tightness of box annotations using predicted masks from the previous epoch and dynamically shrink the noisy zones as training progresses. We verify MonoBox in the box-supervised segmentation task of polyps, where satisfying box-tightness is challenging due to the vague boundaries between the polyp and normal tissues. Experiments on both public synthetic and in-house real noisy datasets demonstrate that MonoBox exceeds other anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%, respectively. Codes are at https://github.com/Huster-Hq/MonoBox.","sentences":["We propose MonoBox, an innovative box-supervised segmentation method constrained by monotonicity to liberate its training from the user-unfriendly box-tightness assumption.","In contrast to conventional box-supervised segmentation, where the box edges must precisely touch the target boundaries, MonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise segmentation.","The 'linchpin' is that, within the noisy zones around box edges, MonoBox discards the traditional misguiding multiple-instance learning loss, and instead optimizes a carefully-designed objective, termed monotonicity constraint.","Along directions transitioning from the foreground to background, this new constraint steers responses to adhere to a trend of monotonically decreasing values.","Consequently, the originally unreliable learning within the noisy zones is transformed into a correct and effective monotonicity optimization.","Moreover, an adaptive label correction is introduced, enabling MonoBox to enhance the tightness of box annotations using predicted masks from the previous epoch and dynamically shrink the noisy zones as training progresses.","We verify MonoBox in the box-supervised segmentation task of polyps, where satisfying box-tightness is challenging due to the vague boundaries between the polyp and normal tissues.","Experiments on both public synthetic and in-house real noisy datasets demonstrate that MonoBox exceeds other anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%, respectively.","Codes are at https://github.com/Huster-Hq/MonoBox."],"url":"http://arxiv.org/abs/2404.01188v2","category":"cs.CV"}
{"created":"2024-04-01 15:35:41","title":"Positioning is All You Need","abstract":"One can drive around safely using a GPS without memorizing a world map (not to mention the dark regions that humans have never explored) because we only pay attention to the next instruction of turning. Such simple observation with attention mechanism has a profound implication on our understanding of how the brain works. The nonlinear effects generated by inhibitory networks have long been known to provide a ``winner-take-all'' mechanism for localized attention. This attention mechanism, at the system level, implements the selection/positioning operation needed by the hippocampus to provide spatio-temporal context to the neocortex. The localized receptive field of sensory organs, when inspected under the framework of embodied cognition, turns from a constraint to an advantage. Since we only need access to a local map, positioning works better than reconstruction. Such simple intuition implies that the attention mechanism is all you need to understand the flexible behavior generated by hippocampal-neocortical systems. Geometrically, we present a novel manifold positioning framework to explain the principle of localized embodied cognition. Through the co-evolution of the hippocampus and the neocortex, the positioning operation implemented by the attention mechanism can be interpreted as a nonlinear projection linking the discovery of local subspace structure by the neocortex (a sensorimotor machine generating the world map) with the navigation task in mind and $without$ discovering global manifold topology by the hippocampus (the neocortex's map index).","sentences":["One can drive around safely using a GPS without memorizing a world map (not to mention the dark regions that humans have never explored) because we only pay attention to the next instruction of turning.","Such simple observation with attention mechanism has a profound implication on our understanding of how the brain works.","The nonlinear effects generated by inhibitory networks have long been known to provide a ``winner-take-all'' mechanism for localized attention.","This attention mechanism, at the system level, implements the selection/positioning operation needed by the hippocampus to provide spatio-temporal context to the neocortex.","The localized receptive field of sensory organs, when inspected under the framework of embodied cognition, turns from a constraint to an advantage.","Since we only need access to a local map, positioning works better than reconstruction.","Such simple intuition implies that the attention mechanism is all you need to understand the flexible behavior generated by hippocampal-neocortical systems.","Geometrically, we present a novel manifold positioning framework to explain the principle of localized embodied cognition.","Through the co-evolution of the hippocampus and the neocortex, the positioning operation implemented by the attention mechanism can be interpreted as a nonlinear projection linking the discovery of local subspace structure by the neocortex (a sensorimotor machine generating the world map) with the navigation task in mind and $without$ discovering global manifold topology by the hippocampus (the neocortex's map index)."],"url":"http://arxiv.org/abs/2404.01183v1","category":"q-bio.NC"}
{"created":"2024-04-01 15:30:02","title":"Poisoning Decentralized Collaborative Recommender System and Its Countermeasures","abstract":"To make room for privacy and efficiency, the deployment of many recommender systems is experiencing a shift from central servers to personal devices, where the federated recommender systems (FedRecs) and decentralized collaborative recommender systems (DecRecs) are arguably the two most representative paradigms. While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients. Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item's exposure rate. Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on FedRecs, and are either inapplicable or ineffective for DecRecs. Compared with FedRecs where the tampered information can be universally distributed to all clients once uploaded to the cloud, each adversary in DecRecs can only communicate with neighbor clients of a small size, confining its impact to a limited range. To fill the gap, we present a novel attack method named Poisoning with Adaptive Malicious Neighbors (PAMN). With item promotion in top-K recommendation as the attack objective, PAMN effectively boosts target items' ranks with several adversaries that emulate benign clients and transfers adaptively crafted gradients conditioned on each adversary's neighbors. Moreover, with the vulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on user-level gradient clipping with sparsified updating is proposed. Extensive experiments demonstrate the effectiveness of the poisoning attack and the robustness of our defensive mechanism.","sentences":["To make room for privacy and efficiency, the deployment of many recommender systems is experiencing a shift from central servers to personal devices, where the federated recommender systems (FedRecs) and decentralized collaborative recommender systems (DecRecs) are arguably the two most representative paradigms.","While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients.","Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item's exposure rate.","Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on FedRecs, and are either inapplicable or ineffective for DecRecs.","Compared with FedRecs where the tampered information can be universally distributed to all clients once uploaded to the cloud, each adversary in DecRecs can only communicate with neighbor clients of a small size, confining its impact to a limited range.","To fill the gap, we present a novel attack method named Poisoning with Adaptive Malicious Neighbors (PAMN).","With item promotion in top-K recommendation as the attack objective, PAMN effectively boosts target items' ranks with several adversaries that emulate benign clients and transfers adaptively crafted gradients conditioned on each adversary's neighbors.","Moreover, with the vulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on user-level gradient clipping with sparsified updating is proposed.","Extensive experiments demonstrate the effectiveness of the poisoning attack and the robustness of our defensive mechanism."],"url":"http://arxiv.org/abs/2404.01177v1","category":"cs.CR"}
{"created":"2024-04-01 15:19:28","title":"Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-based Vision Transformer","abstract":"Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for real-time force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.","sentences":["Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects.","While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects.","We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection.","At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments.","In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper.","In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data.","Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for real-time force estimation and meets the requirements of real-world scenarios.","We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches."],"url":"http://arxiv.org/abs/2404.01170v1","category":"cs.RO"}
{"created":"2024-04-01 14:33:13","title":"Improved Moving-Puncture Techniques for Compact Binary Simulations","abstract":"To fully unlock the scientific potential of upcoming gravitational wave (GW) interferometers, numerical relativity (NR) simulation accuracy must be greatly enhanced. We present three infrastructure-agnostic improvements to the moving-puncture approach for binary black hole (BBH) simulations, aimed at significantly reducing constraint violation and improving GW predictions. Although these improvements were developed within the highly efficient NR code BlackHoles@Home, we demonstrate their effectiveness in the widely adopted Einstein Toolkit/Carpet AMR framework. Our improvements include a modified Kreiss-Oliger dissipation prescription, a Hamiltonian-constraint-damping adjustment to the BSSN equations, and an additional term to the 1+log lapse evolution equation that slows the development of the sharp lapse feature, which dominates numerical errors in BBH simulations. With minimal increase in computational cost, these improvements significantly reduce GW noise, enabling the extraction of high-order GW modes previously obscured by numerical noise. They also improve convergence properties, reduce Hamiltonian (momentum) constraint violations in the strong-field region by approximately two (three) orders of magnitude, and in the GW-extraction zone by six (two) orders of magnitude. To promote community adoption, we have open-sourced the improved Einstein Toolkit thorn BaikalVacuum used in this work. Although our focus is on BBH evolutions and the BSSN formulation, these improvements may also benefit compact binary simulations involving matter and other formulations, a focus for future investigations.","sentences":["To fully unlock the scientific potential of upcoming gravitational wave (GW) interferometers, numerical relativity (NR) simulation accuracy must be greatly enhanced.","We present three infrastructure-agnostic improvements to the moving-puncture approach for binary black hole (BBH) simulations, aimed at significantly reducing constraint violation and improving GW predictions.","Although these improvements were developed within the highly efficient NR code BlackHoles@Home, we demonstrate their effectiveness in the widely adopted Einstein Toolkit/Carpet AMR framework.","Our improvements include a modified Kreiss-Oliger dissipation prescription, a Hamiltonian-constraint-damping adjustment to the BSSN equations, and an additional term to the 1+log lapse evolution equation that slows the development of the sharp lapse feature, which dominates numerical errors in BBH simulations.","With minimal increase in computational cost, these improvements significantly reduce GW noise, enabling the extraction of high-order GW modes previously obscured by numerical noise.","They also improve convergence properties, reduce Hamiltonian (momentum) constraint violations in the strong-field region by approximately two (three) orders of magnitude, and in the GW-extraction zone by six (two) orders of magnitude.","To promote community adoption, we have open-sourced the improved Einstein Toolkit thorn BaikalVacuum used in this work.","Although our focus is on BBH evolutions and the BSSN formulation, these improvements may also benefit compact binary simulations involving matter and other formulations, a focus for future investigations."],"url":"http://arxiv.org/abs/2404.01137v1","category":"gr-qc"}
{"created":"2024-04-01 14:24:40","title":"CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians","abstract":"The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at https://dekuliutesla.github.io/citygs/.","sentences":["The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS).","However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging.","This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering.","Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion.","Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy.","Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales.","Our project page is available at https://dekuliutesla.github.io/citygs/."],"url":"http://arxiv.org/abs/2404.01133v1","category":"cs.CV"}
{"created":"2024-04-01 14:12:55","title":"Thermodynamic dissipation does not bound replicator growth and decay rates","abstract":"In a well-known paper, Jeremy England derived a bound on the free energy dissipated by a self-replicating system [England, \"Statistical physics of self-replication\", The Journal of Chemical Physics, 2013]. This bound is usually interpreted as a universal relationship that links thermodynamic dissipation to the replicator's per-capita decay and growth rates. Contrary to this interpretation, we argue from thermodynamic principles that such a relationship cannot exist. In particular, we show that it is impossible for a system to undergo both replication and per-capita decay back into reactants. While it is possible for a system to undergo replication and decay into separate waste products, in that case replication and decay are two independent physical processes, and there is no universal relationship between their thermodynamic and dynamical properties.","sentences":["In a well-known paper, Jeremy England derived a bound on the free energy dissipated by a self-replicating system [England, \"Statistical physics of self-replication\", The Journal of Chemical Physics, 2013].","This bound is usually interpreted as a universal relationship that links thermodynamic dissipation to the replicator's per-capita decay and growth rates.","Contrary to this interpretation, we argue from thermodynamic principles that such a relationship cannot exist.","In particular, we show that it is impossible for a system to undergo both replication and per-capita decay back into reactants.","While it is possible for a system to undergo replication and decay into separate waste products, in that case replication and decay are two independent physical processes, and there is no universal relationship between their thermodynamic and dynamical properties."],"url":"http://arxiv.org/abs/2404.01130v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-01 13:23:04","title":"Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation","abstract":"Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statistical domain, offering diffusion guidance without relying on direct mappings between the source and target domains. This advantage allows our method to adapt to changing source domains without the need for retraining, making it highly practical when sufficient labeled source domain data is not available. The proposed framework is validated in zero-shot cross-modality image segmentation tasks through empirical comparisons with influential generative models, including adversarial-based and diffusion-based models.","sentences":["Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality.","Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation.","However, a vast body of existing cross-modality image translation methods relies on supervised learning.","In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase).","To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method.","The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance.","Our framework captures identical cross-modality features in the statistical domain, offering diffusion guidance without relying on direct mappings between the source and target domains.","This advantage allows our method to adapt to changing source domains without the need for retraining, making it highly practical when sufficient labeled source domain data is not available.","The proposed framework is validated in zero-shot cross-modality image segmentation tasks through empirical comparisons with influential generative models, including adversarial-based and diffusion-based models."],"url":"http://arxiv.org/abs/2404.01102v1","category":"eess.IV"}
{"created":"2024-04-01 13:21:05","title":"UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models","abstract":"Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at https://github.com/GuanZihan/official_UFID.","sentences":["Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage.","This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet.","To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections.","However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored.","Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily.","Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical.","In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis.","Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency.","The code is available at https://github.com/GuanZihan/official_UFID."],"url":"http://arxiv.org/abs/2404.01101v1","category":"cs.CR"}
{"created":"2024-04-01 12:59:49","title":"HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach","abstract":"Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100. Our code is available at https://github.com/AIRI-Institute/HairFastGAN.","sentences":["Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on.","This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics.","The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow.","At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators.","Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently.","In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods.","Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing.","The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred.","In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100.","Our code is available at https://github.com/AIRI-Institute/HairFastGAN."],"url":"http://arxiv.org/abs/2404.01094v1","category":"cs.CV"}
{"created":"2024-04-01 12:19:08","title":"Efficient Prompting Methods for Large Language Models: A Survey","abstract":"Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highlight future research directions.","sentences":["Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks.","While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs.","As a result, the LLM field has seen a remarkable surge in efficient prompting methods.","In this paper, we present a comprehensive overview of these methods.","At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design.","The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization.","We present the basic concepts of prompting, review the advances for efficient prompting, and highlight future research directions."],"url":"http://arxiv.org/abs/2404.01077v1","category":"cs.CL"}
{"created":"2024-04-01 12:16:00","title":"Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images","abstract":"Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning. P2Det contains the sparse prompt coding and cross-attention between the multimodal data. Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings. The image embeddings are generated through the Transformer layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-attention of the two different embeddings. The interaction of image-level and prompt-level features is utilized to address the clutter interference. A shape-adaptive refinement module (SARM) is proposed to reduce the effect of aspect ratio. Extensive experiments demonstrated the effectiveness of the proposed model on high-resolution SAR images. P2Det provides a novel insight for multimodal object detection due to its competitive performance.","sentences":["Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification.","A large number of interfering signals superimposes the return signal from the tower.","We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle.","Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning.","P2Det contains the sparse prompt coding and cross-attention between the multimodal data.","Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings.","The image embeddings are generated through the Transformer layers.","Then a two-way fusion module (TWFM) is proposed to calculate the cross-attention of the two different embeddings.","The interaction of image-level and prompt-level features is utilized to address the clutter interference.","A shape-adaptive refinement module (SARM) is proposed to reduce the effect of aspect ratio.","Extensive experiments demonstrated the effectiveness of the proposed model on high-resolution SAR images.","P2Det provides a novel insight for multimodal object detection due to its competitive performance."],"url":"http://arxiv.org/abs/2404.01074v1","category":"cs.CV"}
{"created":"2024-04-01 12:01:06","title":"Exploring the Mystery of Influential Data for Mathematical Reasoning","abstract":"Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model. Additionally, we showcase the use of QaDS in creating efficient fine-tuning mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets, which can perform as a reference for future works on mathematical reasoning tasks.","sentences":["Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency.","Recent works have shown that training with only limited data can show a superior performance on general tasks.","However, the feasibility on mathematical reasoning tasks has not been validated.","To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition.","For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning.","A comparison with other selection strategies validates the superiority of QaDS.","For the latter one, we first enlarge our setting and explore the influential data composition.","We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful.","Then, we define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by QaDS.","With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model.","Additionally, we showcase the use of QaDS in creating efficient fine-tuning mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets, which can perform as a reference for future works on mathematical reasoning tasks."],"url":"http://arxiv.org/abs/2404.01067v1","category":"cs.CL"}
{"created":"2024-04-01 11:57:40","title":"T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation","abstract":"Efficient tooth segmentation in three-dimensional (3D) imaging, critical for orthodontic diagnosis, remains challenging due to noise, low contrast, and artifacts in CBCT images. Both convolutional Neural Networks (CNNs) and transformers have emerged as popular architectures for image segmentation. However, their efficacy in handling long-range dependencies is limited due to inherent locality or computational complexity. To address this issue, we propose T-Mamba, integrating shared positional encoding and frequency-based features into vision mamba, to address limitations in spatial position preservation and feature enhancement in frequency domain. Besides, we also design a gate selection unit to integrate two features in spatial domain and one feature in frequency domain adaptively. T-Mamba is the first work to introduce frequency-based features into vision mamba. Extensive experiments demonstrate that T-Mamba achieves new SOTA results on the public Tooth CBCT dataset and outperforms previous SOTA methods by a large margin, i.e., IoU + 3.63%, SO + 2.43%, DSC +2.30%, HD -4.39mm, and ASSD -0.37mm. The code and models are publicly available at https://github.com/isbrycee/T-Mamba.","sentences":["Efficient tooth segmentation in three-dimensional (3D) imaging, critical for orthodontic diagnosis, remains challenging due to noise, low contrast, and artifacts in CBCT images.","Both convolutional Neural Networks (CNNs) and transformers have emerged as popular architectures for image segmentation.","However, their efficacy in handling long-range dependencies is limited due to inherent locality or computational complexity.","To address this issue, we propose T-Mamba, integrating shared positional encoding and frequency-based features into vision mamba, to address limitations in spatial position preservation and feature enhancement in frequency domain.","Besides, we also design a gate selection unit to integrate two features in spatial domain and one feature in frequency domain adaptively.","T-Mamba is the first work to introduce frequency-based features into vision mamba.","Extensive experiments demonstrate that T-Mamba achieves new SOTA results on the public Tooth CBCT dataset and outperforms previous SOTA methods by a large margin, i.e., IoU + 3.63%, SO + 2.43%, DSC +2.30%, HD -4.39mm, and ASSD -0.37mm.","The code and models are publicly available at https://github.com/isbrycee/T-Mamba."],"url":"http://arxiv.org/abs/2404.01065v1","category":"cs.CV"}
{"created":"2024-04-01 11:42:43","title":"Harnessing Data and Physics for Deep Learning Phase Recovery","abstract":"Phase recovery, calculating the phase of a light wave from its intensity measurements, is essential for various applications, such as coherent diffraction imaging, adaptive optics, and biomedical imaging. It enables the reconstruction of an object's refractive index distribution or topography as well as the correction of imaging system aberrations. In recent years, deep learning has been proven to be highly effective in addressing phase recovery problems. Two main deep learning phase recovery strategies are data-driven (DD) with supervised learning mode and physics-driven (PD) with self-supervised learning mode. DD and PD achieve the same goal in different ways and lack the necessary study to reveal similarities and differences. Therefore, in this paper, we comprehensively compare these two deep learning phase recovery strategies in terms of time consumption, accuracy, generalization ability, ill-posedness adaptability, and prior capacity. What's more, we propose a co-driven (CD) strategy of combining datasets and physics for the balance of high- and low-frequency information. The codes for DD, PD, and CD are publicly available at https://github.com/kqwang/DLPR.","sentences":["Phase recovery, calculating the phase of a light wave from its intensity measurements, is essential for various applications, such as coherent diffraction imaging, adaptive optics, and biomedical imaging.","It enables the reconstruction of an object's refractive index distribution or topography as well as the correction of imaging system aberrations.","In recent years, deep learning has been proven to be highly effective in addressing phase recovery problems.","Two main deep learning phase recovery strategies are data-driven (DD) with supervised learning mode and physics-driven (PD) with self-supervised learning mode.","DD and PD achieve the same goal in different ways and lack the necessary study to reveal similarities and differences.","Therefore, in this paper, we comprehensively compare these two deep learning phase recovery strategies in terms of time consumption, accuracy, generalization ability, ill-posedness adaptability, and prior capacity.","What's more, we propose a co-driven (CD) strategy of combining datasets and physics for the balance of high- and low-frequency information.","The codes for DD, PD, and CD are publicly available at https://github.com/kqwang/DLPR."],"url":"http://arxiv.org/abs/2404.01360v1","category":"eess.IV"}
{"created":"2024-04-01 10:13:49","title":"Easy-to-configure zero-dimensional valley-chiral modes in a graphene point junction","abstract":"The valley degree of freedom in 2D materials can be manipulated for low-dissipation quantum electronics called valleytronics. At the boundary between two regions of bilayer graphene with different atomic or electrostatic configuration, valley-polarized current has been realized. However, the demanding fabrication and operation requirements limit device reproducibility and scalability toward more advanced valleytronics circuits. We demonstrate a new device architecture of a point junction where a valley-chiral 0D PN junction is easily configured, switchable, and capable of carrying valley current with an estimated polarization of ~80%. This work provides a new building block in manipulating valley quantum numbers and scalable valleytronics.","sentences":["The valley degree of freedom in 2D materials can be manipulated for low-dissipation quantum electronics called valleytronics.","At the boundary between two regions of bilayer graphene with different atomic or electrostatic configuration, valley-polarized current has been realized.","However, the demanding fabrication and operation requirements limit device reproducibility and scalability toward more advanced valleytronics circuits.","We demonstrate a new device architecture of a point junction where a valley-chiral 0D PN junction is easily configured, switchable, and capable of carrying valley current with an estimated polarization of ~80%.","This work provides a new building block in manipulating valley quantum numbers and scalable valleytronics."],"url":"http://arxiv.org/abs/2404.01027v2","category":"cond-mat.mes-hall"}
{"created":"2024-04-01 09:39:15","title":"Accelerate Solving Expensive Scheduling by Leveraging Economical Auxiliary Tasks","abstract":"To fully leverage the multi-task optimization paradigm for accelerating the solution of expensive scheduling problems, this study has effectively tackled three vital concerns. The primary issue is identifying auxiliary tasks that closely resemble the original expensive task. We suggested a sampling strategy based on job importance, creating a compact matrix by extracting crucial rows from the entire problem specification matrix of the expensive task. This matrix serves as an economical auxiliary task. Mathematically, we proved that this economical auxiliary task bears similarity to its corresponding expensive task. The subsequent concern revolves around making auxiliary tasks more cost-effective. We determined the sampling proportions for the entire problem specification matrix through factorial design experiments, resulting in a more compact auxiliary task. With a reduced search space and shorter function evaluation time, it can rapidly furnish high-quality transferable information for the primary task. The last aspect involves designing transferable deep information from auxiliary tasks. We regarded the job priorities in the (sub-) optimal solutions to the economical auxiliary task as transferable invariants. By adopting a partial solution patching strategy, we augmented specificity knowledge onto the common knowledge to adapt to the target expensive task. The strategies devised for constructing task pairs and facilitating knowledge transfer, when incorporated into various evolutionary multitasking algorithms, were utilized to address expensive instances of permutation flow shop scheduling. Extensive experiments and statistical comparisons have validated that, with the collaborative synergy of these strategies, the performance of evolutionary multitasking algorithms is significantly enhanced in handling expensive scheduling tasks.","sentences":["To fully leverage the multi-task optimization paradigm for accelerating the solution of expensive scheduling problems, this study has effectively tackled three vital concerns.","The primary issue is identifying auxiliary tasks that closely resemble the original expensive task.","We suggested a sampling strategy based on job importance, creating a compact matrix by extracting crucial rows from the entire problem specification matrix of the expensive task.","This matrix serves as an economical auxiliary task.","Mathematically, we proved that this economical auxiliary task bears similarity to its corresponding expensive task.","The subsequent concern revolves around making auxiliary tasks more cost-effective.","We determined the sampling proportions for the entire problem specification matrix through factorial design experiments, resulting in a more compact auxiliary task.","With a reduced search space and shorter function evaluation time, it can rapidly furnish high-quality transferable information for the primary task.","The last aspect involves designing transferable deep information from auxiliary tasks.","We regarded the job priorities in the (sub-) optimal solutions to the economical auxiliary task as transferable invariants.","By adopting a partial solution patching strategy, we augmented specificity knowledge onto the common knowledge to adapt to the target expensive task.","The strategies devised for constructing task pairs and facilitating knowledge transfer, when incorporated into various evolutionary multitasking algorithms, were utilized to address expensive instances of permutation flow shop scheduling.","Extensive experiments and statistical comparisons have validated that, with the collaborative synergy of these strategies, the performance of evolutionary multitasking algorithms is significantly enhanced in handling expensive scheduling tasks."],"url":"http://arxiv.org/abs/2404.01018v1","category":"math.OC"}
{"created":"2024-04-01 08:01:13","title":"Long time stability and instability in the two-dimensional Boussinesq system with kinematic viscosity","abstract":"In this paper, we investigate the long-time behavior of the two-dimensional incompressible Boussinesq system with kinematic viscosity in a periodic channel, focusing on instability and asymptotic stability near hydrostatic equilibria. Firstly, we prove that any hydrostatic equilibrium reveals long-time instability when the initial data are perturbed in Sobolev spaces of low regularity. Secondly, we establish asymptotic stability of the stratified density, which is strictly decreasing in the vertical direction, under sufficiently regular perturbations, proving that the solution converges to the unique minimizer of the total energy.   Our analysis is based on the energy method. Although the total energy dissipates due to kinematic viscosity, such mechanism cannot capture the stratification of the density. We overcome this difficulty by discovering another Lyapunov functional which exhibits the density stratification in a quantitative manner.","sentences":["In this paper, we investigate the long-time behavior of the two-dimensional incompressible Boussinesq system with kinematic viscosity in a periodic channel, focusing on instability and asymptotic stability near hydrostatic equilibria.","Firstly, we prove that any hydrostatic equilibrium reveals long-time instability when the initial data are perturbed in Sobolev spaces of low regularity.","Secondly, we establish asymptotic stability of the stratified density, which is strictly decreasing in the vertical direction, under sufficiently regular perturbations, proving that the solution converges to the unique minimizer of the total energy.   ","Our analysis is based on the energy method.","Although the total energy dissipates due to kinematic viscosity, such mechanism cannot capture the stratification of the density.","We overcome this difficulty by discovering another Lyapunov functional which exhibits the density stratification in a quantitative manner."],"url":"http://arxiv.org/abs/2404.00985v1","category":"math.AP"}
{"created":"2024-04-01 07:59:29","title":"Continual Learning for Smart City: A Survey","abstract":"With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends.","sentences":["With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities.","Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time.","Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development.","The content consists of three parts: 1) Methodology-wise.","We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning.","2) Application-wise.","We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing.","3) Challenges.","We discuss current problems and challenges and envision several promising research directions.","We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends."],"url":"http://arxiv.org/abs/2404.00983v1","category":"cs.LG"}
{"created":"2024-04-01 07:49:10","title":"Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation","abstract":"A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as well as complex dynamics in terms of direct stakeholder impacts. A workflow for implementing the algorithm in real city planning scenarios is outlined. This workflow includes machine learning of a suitable set of parameters suggesting best-practice planning to aim at the desired development of the planning process and its output.","sentences":["A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process.","The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions.","The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations.","Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters.","These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as well as complex dynamics in terms of direct stakeholder impacts.","A workflow for implementing the algorithm in real city planning scenarios is outlined.","This workflow includes machine learning of a suitable set of parameters suggesting best-practice planning to aim at the desired development of the planning process and its output."],"url":"http://arxiv.org/abs/2404.00977v1","category":"nlin.AO"}
{"created":"2024-04-01 07:47:09","title":"Yet anOther Dose Algorithm (YODA) for independent computations of dose and dose changes due to anatomical changes","abstract":"$\\textbf{Purpose:}$ To assess the viability of a physics-based, deterministic and adjoint-capable algorithm for performing treatment planning system independent dose calculations and for computing dosimetric differences caused by anatomical changes.   $\\textbf{Methods:}$ A semi-numerical approach is employed to solve two partial differential equations for the proton phase-space density which determines the deposited dose. Lateral hetereogeneities are accounted for by an optimized (Gaussian) beam splitting scheme. Adjoint theory is applied to approximate the change in the deposited dose caused by a new underlying patient anatomy.   $\\textbf{Results:}$ The quality of the dose engine was benchmarked through three-dimensional gamma index comparisons against Monte Carlo simulations done in TOPAS. The worst passing rate for the gamma index with (1 mm, 1 %, 10 % dose cut-off) criteria is 95.62 %. The effect of delivering treatment plans on repeat CTs was also tested. For a non-robustly optimized plan the adjoint component was accurate to 6.2 % while for a robustly optimized plan it was accurate to 1 %.   $\\textbf{Conclusions:}$ YODA is capable of accurate dose computations in both single and multi spot irradiations when compared to TOPAS. Moreover, it is able to compute dosimetric differences due to anatomical changes with small to moderate errors thereby facilitating its use for patient-specific quality assurance in online adaptive proton therapy.","sentences":["$\\textbf{Purpose:}$ To assess the viability of a physics-based, deterministic and adjoint-capable algorithm for performing treatment planning system independent dose calculations and for computing dosimetric differences caused by anatomical changes.   ","$\\textbf{Methods:}$ A semi-numerical approach is employed to solve two partial differential equations for the proton phase-space density which determines the deposited dose.","Lateral hetereogeneities are accounted for by an optimized (Gaussian) beam splitting scheme.","Adjoint theory is applied to approximate the change in the deposited dose caused by a new underlying patient anatomy.   ","$\\textbf{Results:}$ The quality of the dose engine was benchmarked through three-dimensional gamma index comparisons against Monte Carlo simulations done in TOPAS.","The worst passing rate for the gamma index with (1 mm, 1 %, 10 % dose cut-off) criteria is 95.62 %.","The effect of delivering treatment plans on repeat CTs was also tested.","For a non-robustly optimized plan the adjoint component was accurate to 6.2 % while for a robustly optimized plan it was accurate to 1 %.   ","$\\textbf{Conclusions:}$ YODA is capable of accurate dose computations in both single and multi spot irradiations when compared to TOPAS.","Moreover, it is able to compute dosimetric differences due to anatomical changes with small to moderate errors thereby facilitating its use for patient-specific quality assurance in online adaptive proton therapy."],"url":"http://arxiv.org/abs/2404.00976v1","category":"physics.med-ph"}
{"created":"2024-04-01 07:12:27","title":"Diffusion-Driven Domain Adaptation for Generating 3D Molecules","abstract":"Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data? This problem can be cast as the problem of domain adaptive molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule. As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains. These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising. We show that, with these encoded structural-grained domain supervisors, GADM can generate effective molecules within the desired new domains. We conduct extensive experiments across various domain adaptation tasks over benchmarking datasets. We show that our approach can improve up to 65.6% in terms of success rate defined based on molecular validity, uniqueness, and novelty compared to alternative baselines.","sentences":["Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data?","This problem can be cast as the problem of domain adaptive molecule generation.","This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule.","As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties.","In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains.","These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising.","We show that, with these encoded structural-grained domain supervisors, GADM can generate effective molecules within the desired new domains.","We conduct extensive experiments across various domain adaptation tasks over benchmarking datasets.","We show that our approach can improve up to 65.6% in terms of success rate defined based on molecular validity, uniqueness, and novelty compared to alternative baselines."],"url":"http://arxiv.org/abs/2404.00962v1","category":"cs.LG"}
{"created":"2024-04-01 07:03:24","title":"Orchestrating UAVs for Prioritized Data Harvesting: A Cross-Layer Optimization Perspective","abstract":"This work describes the orchestration of a fleet of rotary-wing Unmanned Aerial Vehicles (UAVs) for harvesting prioritized traffic from random distributions of heterogeneous users with Multiple Input Multiple Output (MIMO) capabilities. In a finite-horizon offline setting, the goal is to optimize the beam-forming design, the 3D UAV positioning and trajectory solution, and the user association/scheduling policy, to maximize the cumulative fleet-wide reward obtained by satisfying the quality-of-service mandates imposed on each user uplink request, subject to an average per-UAV mobility power constraint. With a probabilistic air-to-ground channel model, a multi-user MIMO uplink communication model with prioritized traffic, and a novel 3D mobility model for rotary-wing UAVs, the fleet-wide reward maximization problem is solved via a cross-layer optimization framework: first, K-means clustering is employed to obtain user clusters; then, equipped with a zero-forcing beam-forming design, the positions of the UAVs are optimized via two-stage grid search; next, treating these optimal positions as the graph vertices of a fully-connected mesh, the 3D UAV trajectories (i.e., graph edges) are designed via a learning based competitive swarm optimization algorithm, under an average UAV power consumption constraint, coupled with projected subgradient ascent for dual optimization; consequently, the user association/scheduling strategy is solved via a graphical branch-and-bound method on the underlying multiple traveling salesman problem. Numerical evaluations demonstrate that the proposed solution outperforms static UAV deployments, adaptive Voronoi decomposition techniques, and state-of-the-art iterative fleet control algorithms, with respect to user quality-of-service and per-UAV average power consumption.","sentences":["This work describes the orchestration of a fleet of rotary-wing Unmanned Aerial Vehicles (UAVs) for harvesting prioritized traffic from random distributions of heterogeneous users with Multiple Input Multiple Output (MIMO) capabilities.","In a finite-horizon offline setting, the goal is to optimize the beam-forming design, the 3D UAV positioning and trajectory solution, and the user association/scheduling policy, to maximize the cumulative fleet-wide reward obtained by satisfying the quality-of-service mandates imposed on each user uplink request, subject to an average per-UAV mobility power constraint.","With a probabilistic air-to-ground channel model, a multi-user MIMO uplink communication model with prioritized traffic, and a novel 3D mobility model for rotary-wing UAVs, the fleet-wide reward maximization problem is solved via a cross-layer optimization framework: first, K-means clustering is employed to obtain user clusters; then, equipped with a zero-forcing beam-forming design, the positions of the UAVs are optimized via two-stage grid search; next, treating these optimal positions as the graph vertices of a fully-connected mesh, the 3D UAV trajectories (i.e., graph edges) are designed via a learning based competitive swarm optimization algorithm, under an average UAV power consumption constraint, coupled with projected subgradient ascent for dual optimization; consequently, the user association/scheduling strategy is solved via a graphical branch-and-bound method on the underlying multiple traveling salesman problem.","Numerical evaluations demonstrate that the proposed solution outperforms static UAV deployments, adaptive Voronoi decomposition techniques, and state-of-the-art iterative fleet control algorithms, with respect to user quality-of-service and per-UAV average power consumption."],"url":"http://arxiv.org/abs/2404.00961v1","category":"eess.SY"}
{"created":"2024-04-01 06:44:10","title":"Parallel finite-element codes for the Bogoliubov-de Gennes stability analysis of Bose-Einstein condensates","abstract":"We present and distribute a parallel finite-element toolbox written in the free software FreeFem for computing the Bogoliubov-de Gennes (BdG) spectrum of stationary solutions to one- and two-component Gross-Pitaevskii (GP) equations, in two or three spatial dimensions. The parallelization of the toolbox relies exclusively upon the recent interfacing of FreeFem with the PETSc library. The latter contains itself a wide palette of state-of-the-art linear algebra libraries, graph partitioners, mesh generation and domain decomposition tools, as well as a suite of eigenvalue solvers that are embodied in the SLEPc library. Within the present toolbox, stationary states of the GP equations are computed by a Newton method. Branches of solutions are constructed using an adaptive step-size continuation algorithm. The combination of mesh adaptivity tools from FreeFem with the parallelization features from PETSc makes the toolbox efficient and reliable for the computation of stationary states. Their BdG spectrum is computed using the SLEPc eigenvalue solver. We perform extensive tests and validate our programs by comparing the toolbox's results with known theoretical and numerical findings that have been reported in the literature.","sentences":["We present and distribute a parallel finite-element toolbox written in the free software FreeFem for computing the Bogoliubov-de Gennes (BdG) spectrum of stationary solutions to one-","and two-component Gross-Pitaevskii (GP) equations, in two or three spatial dimensions.","The parallelization of the toolbox relies exclusively upon the recent interfacing of FreeFem with the PETSc library.","The latter contains itself a wide palette of state-of-the-art linear algebra libraries, graph partitioners, mesh generation and domain decomposition tools, as well as a suite of eigenvalue solvers that are embodied in the SLEPc library.","Within the present toolbox, stationary states of the GP equations are computed by a Newton method.","Branches of solutions are constructed using an adaptive step-size continuation algorithm.","The combination of mesh adaptivity tools from FreeFem with the parallelization features from PETSc makes the toolbox efficient and reliable for the computation of stationary states.","Their BdG spectrum is computed using the SLEPc eigenvalue solver.","We perform extensive tests and validate our programs by comparing the toolbox's results with known theoretical and numerical findings that have been reported in the literature."],"url":"http://arxiv.org/abs/2404.00956v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-01 06:26:48","title":"Adapting CSI-Guided Imaging Across Diverse Environments: An Experimental Study Leveraging Continuous Learning","abstract":"This study explores the feasibility of adapting CSI-guided imaging across varied environments. Focusing on continuous model learning through continuous updates, we investigate CSI-Imager's adaptability in dynamically changing settings, specifically transitioning from an office to an industrial environment. Unlike traditional approaches that may require retraining for new environments, our experimental study aims to validate the potential of CSI-guided imaging to maintain accurate imaging performance through Continuous Learning (CL). By conducting experiments across different scenarios and settings, this work contributes to understanding the limitations and capabilities of existing CSI-guided imaging systems in adapting to new environmental contexts.","sentences":["This study explores the feasibility of adapting CSI-guided imaging across varied environments.","Focusing on continuous model learning through continuous updates, we investigate CSI-Imager's adaptability in dynamically changing settings, specifically transitioning from an office to an industrial environment.","Unlike traditional approaches that may require retraining for new environments, our experimental study aims to validate the potential of CSI-guided imaging to maintain accurate imaging performance through Continuous Learning (CL).","By conducting experiments across different scenarios and settings, this work contributes to understanding the limitations and capabilities of existing CSI-guided imaging systems in adapting to new environmental contexts."],"url":"http://arxiv.org/abs/2404.00951v1","category":"eess.IV"}
{"created":"2024-04-01 06:22:28","title":"Harnessing The Power of Attention For Patch-Based Biomedical Image Classification","abstract":"Biomedical image analysis can be facilitated by an innovative architecture rooted in self-attention mechanisms. The traditional convolutional neural network (CNN), characterized by fixed-sized windows, needs help capturing intricate spatial and temporal relations at the pixel level. The immutability of CNN filter weights post-training further restricts input fluctuations. Recognizing these limitations, we propose a new paradigm of attention-based models instead of convolutions. As an alternative to traditional CNNs, these models demonstrate robust modelling capabilities and the ability to grasp comprehensive long-range contextual information efficiently. Providing a solution to critical challenges faced by attention-based vision models such as inductive bias, weight sharing, receptive field limitations, and data handling in high resolution, our work combines non-overlapping (vanilla patching) with novel overlapped Shifted Patching Techniques (S.P.T.s) to induce local context that enhances model generalization. Moreover, we examine the novel Lancoz5 interpolation technique, which adapts variable image sizes to higher resolutions. Experimental evidence validates our model's generalization effectiveness, comparing favourably with existing approaches. Attention-based methods are particularly effective with ample data, especially when advanced data augmentation methodologies are integrated to strengthen their robustness.","sentences":["Biomedical image analysis can be facilitated by an innovative architecture rooted in self-attention mechanisms.","The traditional convolutional neural network (CNN), characterized by fixed-sized windows, needs help capturing intricate spatial and temporal relations at the pixel level.","The immutability of CNN filter weights post-training further restricts input fluctuations.","Recognizing these limitations, we propose a new paradigm of attention-based models instead of convolutions.","As an alternative to traditional CNNs, these models demonstrate robust modelling capabilities and the ability to grasp comprehensive long-range contextual information efficiently.","Providing a solution to critical challenges faced by attention-based vision models such as inductive bias, weight sharing, receptive field limitations, and data handling in high resolution, our work combines non-overlapping (vanilla patching) with novel overlapped Shifted Patching Techniques (S.P.T.s) to induce local context that enhances model generalization.","Moreover, we examine the novel Lancoz5 interpolation technique, which adapts variable image sizes to higher resolutions.","Experimental evidence validates our model's generalization effectiveness, comparing favourably with existing approaches.","Attention-based methods are particularly effective with ample data, especially when advanced data augmentation methodologies are integrated to strengthen their robustness."],"url":"http://arxiv.org/abs/2404.00949v1","category":"cs.CV"}
{"created":"2024-04-01 05:55:10","title":"Sequential Decision-Making under Uncertainty: A Robust MDPs review","abstract":"This review paper provides an in-depth overview of the evolution and advancements in Robust Markov Decision Processes (RMDPs), a field of paramount importance for its role in sequential decision-making amidst uncertainty. Fueled by advances in robust optimization theory and the increasing applications of reinforcement learning techniques, RMDPs literature has been enriched extensively. The review focuses on the formulation of RMDPs, particularly ambiguity sets modeling, which is central to hedging uncertainty. The review systematically classifies the extant methodologies for RMDP formulation into three principal categories: parametric, moment-based, and discrepancy-based approaches, and comprehensively dissects them. The review further delves into the rectangular assumption, which is essential for the computational tractability of RMDPs yet noted for its potential to engender overly conservative policy outcomes. The review summarizes three popular rectangular forms and presents new proof attesting to the NP-hardness of non-rectangular RMDPs. Out of traditional RMDPs scope, the review also surveys recent efforts without conventional rectangular assumptions and burgeoning research trends within the RMDP community. These studies foster the development of more flexible and practical modeling frameworks and enhance the adaptability and performance of RMDPs in the face of uncertainty.","sentences":["This review paper provides an in-depth overview of the evolution and advancements in Robust Markov Decision Processes (RMDPs), a field of paramount importance for its role in sequential decision-making amidst uncertainty.","Fueled by advances in robust optimization theory and the increasing applications of reinforcement learning techniques, RMDPs literature has been enriched extensively.","The review focuses on the formulation of RMDPs, particularly ambiguity sets modeling, which is central to hedging uncertainty.","The review systematically classifies the extant methodologies for RMDP formulation into three principal categories: parametric, moment-based, and discrepancy-based approaches, and comprehensively dissects them.","The review further delves into the rectangular assumption, which is essential for the computational tractability of RMDPs yet noted for its potential to engender overly conservative policy outcomes.","The review summarizes three popular rectangular forms and presents new proof attesting to the NP-hardness of non-rectangular RMDPs.","Out of traditional RMDPs scope, the review also surveys recent efforts without conventional rectangular assumptions and burgeoning research trends within the RMDP community.","These studies foster the development of more flexible and practical modeling frameworks and enhance the adaptability and performance of RMDPs in the face of uncertainty."],"url":"http://arxiv.org/abs/2404.00940v2","category":"math.OC"}
{"created":"2024-04-01 04:39:44","title":"Token-Efficient Leverage Learning in Large Language Models","abstract":"Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \\textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.","sentences":["Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios.","Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge.","To address the twin hurdles, we introduce \\textbf{Leverage Learning}.","We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL).","TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens.","It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance.","With the same amount of task data, TELL leads in improving task performance compared to SFT.","We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing."],"url":"http://arxiv.org/abs/2404.00914v1","category":"cs.CL"}
{"created":"2024-04-01 04:39:21","title":"LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction","abstract":"Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark. In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.","sentences":["Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs.","In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information.","Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure.","We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts.","LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets.","Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment.","Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios.","Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark.","In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining."],"url":"http://arxiv.org/abs/2404.00913v1","category":"cs.CV"}
{"created":"2024-04-01 04:21:49","title":"AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation","abstract":"Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with the baselines. We further demonstrate the effectiveness of accuracy estimation with a model recovery case study, showcasing the practicality of our model recovery based on accuracy estimation. The source code is available at https://github.com/taeckyung/AETTA.","sentences":["Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data.","However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios.","Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models.","To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA.","We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences.","We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures.","Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with the baselines.","We further demonstrate the effectiveness of accuracy estimation with a model recovery case study, showcasing the practicality of our model recovery based on accuracy estimation.","The source code is available at https://github.com/taeckyung/AETTA."],"url":"http://arxiv.org/abs/2404.01351v1","category":"cs.LG"}
{"created":"2024-04-01 03:51:38","title":"CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series","abstract":"Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.   We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation. Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction. Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.","sentences":["Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset.","Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets.","However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes.","This bias poses significant challenges when deploying models in real-world applications.","Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field.","In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.   ","We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation.","Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction.","Second, we design the augmentation probability regulation method to minimize class-dependent bias.","Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample.","Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance.","These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications."],"url":"http://arxiv.org/abs/2404.00898v1","category":"cs.LG"}
{"created":"2024-04-01 03:27:46","title":"MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control","abstract":"Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLight is highly adaptable.","sentences":["Traffic signal control has a great impact on alleviating traffic congestion in modern cities.","Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency.","To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators.","Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant.","Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance.","We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLight is highly adaptable."],"url":"http://arxiv.org/abs/2404.00886v1","category":"cs.AI"}
{"created":"2024-04-01 03:25:06","title":"Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models","abstract":"Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.","sentences":["Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations.","However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking.","When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail.","To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation.","The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID.","To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.","Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting.","Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights."],"url":"http://arxiv.org/abs/2404.00884v1","category":"cs.CL"}
{"created":"2024-04-01 03:23:21","title":"Auxiliary-Variable Adaptive Control Lyapunov Barrier Functions for Spatio-Temporally Constrained Safety-Critical Applications","abstract":"Recent work has shown that stabilizing an affine control system while optimizing a quadratic cost subject to state and control constraints can be mapped to a sequence of Quadratic Programs (QPs) using Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). One of the main challenges in this method is that the QPs could easily become infeasible under safety and spatio-temporal constraints with tight control bounds. In our own recent work, we defined Auxiliary-Variable Adaptive CBFs (AVCBFs) to improve the feasibility of the CBF-based QP, while avoiding extensive parameter tuning. In this paper, we consider spatio-temporal constraints as finite-time reachability requirements. In order to satisfy these requirements, we generalize AVCBFs to Auxiliary-Variable Adaptive Control Lyapunov Barrier Functions (AVCLBFs) that work for systems and constraints with arbitrary relative degrees. We show that our method has fewer conflicts with safety and input constraints, and outperforms the state of the art in term of adaptivity and feasibility in solving the QP. We illustrate our approach on an optimal control problem for a unicycle.","sentences":["Recent work has shown that stabilizing an affine control system while optimizing a quadratic cost subject to state and control constraints can be mapped to a sequence of Quadratic Programs (QPs) using Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs).","One of the main challenges in this method is that the QPs could easily become infeasible under safety and spatio-temporal constraints with tight control bounds.","In our own recent work, we defined Auxiliary-Variable Adaptive CBFs (AVCBFs) to improve the feasibility of the CBF-based QP, while avoiding extensive parameter tuning.","In this paper, we consider spatio-temporal constraints as finite-time reachability requirements.","In order to satisfy these requirements, we generalize AVCBFs to Auxiliary-Variable Adaptive Control Lyapunov Barrier Functions (AVCLBFs) that work for systems and constraints with arbitrary relative degrees.","We show that our method has fewer conflicts with safety and input constraints, and outperforms the state of the art in term of adaptivity and feasibility in solving the QP.","We illustrate our approach on an optimal control problem for a unicycle."],"url":"http://arxiv.org/abs/2404.00881v1","category":"math.OC"}
{"created":"2024-04-01 03:18:12","title":"Model-Agnostic Human Preference Inversion in Diffusion Models","abstract":"Efficient text-to-image generation remains a challenging task due to the high computational costs associated with the multi-step sampling in diffusion models. Although distillation of pre-trained diffusion models has been successful in reducing sampling steps, low-step image generation often falls short in terms of quality. In this study, we propose a novel sampling design to achieve high-quality one-step image generation aligning with human preferences, particularly focusing on exploring the impact of the prior noise distribution. Our approach, Prompt Adaptive Human Preference Inversion (PAHI), optimizes the noise distributions for each prompt based on human preferences without the need for fine-tuning diffusion models. Our experiments showcase that the tailored noise distributions significantly improve image quality with only a marginal increase in computational cost. Our findings underscore the importance of noise optimization and pave the way for efficient and high-quality text-to-image synthesis.","sentences":["Efficient text-to-image generation remains a challenging task due to the high computational costs associated with the multi-step sampling in diffusion models.","Although distillation of pre-trained diffusion models has been successful in reducing sampling steps, low-step image generation often falls short in terms of quality.","In this study, we propose a novel sampling design to achieve high-quality one-step image generation aligning with human preferences, particularly focusing on exploring the impact of the prior noise distribution.","Our approach, Prompt Adaptive Human Preference Inversion (PAHI), optimizes the noise distributions for each prompt based on human preferences without the need for fine-tuning diffusion models.","Our experiments showcase that the tailored noise distributions significantly improve image quality with only a marginal increase in computational cost.","Our findings underscore the importance of noise optimization and pave the way for efficient and high-quality text-to-image synthesis."],"url":"http://arxiv.org/abs/2404.00879v1","category":"cs.CV"}
{"created":"2024-04-01 03:15:41","title":"TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for High-Fidelity Virtual Try-On","abstract":"Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment. However, the clothing identity uncontrollability and training inefficiency of existing diffusion-based methods, which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications. In this work, we propose an effective and efficient framework, termed TryOn-Adapter. Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation. Our approach utilizes a pre-trained exemplar-based diffusion model as the fundamental network, whose parameters are frozen except for the attention layers. We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with fine-tuning techniques to enable precise and efficient identity control. Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference. Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used benchmarks. Additionally, compared with recent full-tuning diffusion-based methods, we only use about half of their tunable parameters during training. The code will be made publicly available at https://github.com/jiazheng-xing/TryOn-Adapter.","sentences":["Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment.","However, the clothing identity uncontrollability and training inefficiency of existing diffusion-based methods, which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications.","In this work, we propose an effective and efficient framework, termed TryOn-Adapter.","Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation.","Our approach utilizes a pre-trained exemplar-based diffusion model as the fundamental network, whose parameters are frozen except for the attention layers.","We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with fine-tuning techniques to enable precise and efficient identity control.","Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference.","Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used benchmarks.","Additionally, compared with recent full-tuning diffusion-based methods, we only use about half of their tunable parameters during training.","The code will be made publicly available at https://github.com/jiazheng-xing/TryOn-Adapter."],"url":"http://arxiv.org/abs/2404.00878v1","category":"cs.CV"}
{"created":"2024-04-01 03:10:36","title":"DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly","abstract":"We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views.","sentences":["We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object.","By leveraging differentiable volume rendering, our method does not require 3D supervision.","Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction.","As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering.","Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering.","With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views."],"url":"http://arxiv.org/abs/2404.00875v2","category":"cs.CV"}
{"created":"2024-04-01 01:56:27","title":"Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point Cloud Classification","abstract":"Point cloud classification refers to the process of assigning semantic labels or categories to individual points within a point cloud data structure. Recent works have explored the extension of pre-trained CLIP to 3D recognition. In this direction, CLIP-based point cloud models like PointCLIP, CLIP2Point have become state-of-the-art methods in the few-shot setup. Although these methods show promising performance for some classes like airplanes, desks, guitars, etc, the performance for some classes like the cup, flower pot, sink, nightstand, etc is still far from satisfactory. This is due to the fact that the adapter of CLIP-based models is trained using randomly sampled N-way K-shot data in the standard supervised learning setup. In this paper, we propose a novel meta-episodic learning framework for CLIP-based point cloud classification, addressing the challenges of limited training examples and sampling unknown classes. Additionally, we introduce dynamic task sampling within the episode based on performance memory. This sampling strategy effectively addresses the challenge of sampling unknown classes, ensuring that the model learns from a diverse range of classes and promotes the exploration of underrepresented categories. By dynamically updating the performance memory, we adaptively prioritize the sampling of classes based on their performance, enhancing the model's ability to handle challenging and real-world scenarios. Experiments show an average performance gain of 3-6\\% on ModelNet40 and ScanobjectNN datasets in a few-shot setup.","sentences":["Point cloud classification refers to the process of assigning semantic labels or categories to individual points within a point cloud data structure.","Recent works have explored the extension of pre-trained CLIP to 3D recognition.","In this direction, CLIP-based point cloud models like PointCLIP, CLIP2Point have become state-of-the-art methods in the few-shot setup.","Although these methods show promising performance for some classes like airplanes, desks, guitars, etc, the performance for some classes like the cup, flower pot, sink, nightstand, etc is still far from satisfactory.","This is due to the fact that the adapter of CLIP-based models is trained using randomly sampled N-way K-shot data in the standard supervised learning setup.","In this paper, we propose a novel meta-episodic learning framework for CLIP-based point cloud classification, addressing the challenges of limited training examples and sampling unknown classes.","Additionally, we introduce dynamic task sampling within the episode based on performance memory.","This sampling strategy effectively addresses the challenge of sampling unknown classes, ensuring that the model learns from a diverse range of classes and promotes the exploration of underrepresented categories.","By dynamically updating the performance memory, we adaptively prioritize the sampling of classes based on their performance, enhancing the model's ability to handle challenging and real-world scenarios.","Experiments show an average performance gain of 3-6\\% on ModelNet40 and ScanobjectNN datasets in a few-shot setup."],"url":"http://arxiv.org/abs/2404.00857v1","category":"cs.CV"}
{"created":"2024-04-01 01:42:23","title":"Prompt Learning via Meta-Regularization","abstract":"Pre-trained vision-language models have shown impressive success on various computer vision tasks with their zero-shot generalizability. Recently, prompt learning approaches have been explored to efficiently and effectively adapt the vision-language models to a variety of downstream tasks. However, most existing prompt learning methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the prompts are finetuned on a small data set from a specific target task. To address this issue, we propose a Prompt Meta-Regularization (ProMetaR) to improve the generalizability of prompt learning for vision-language models. Specifically, ProMetaR meta-learns both the regularizer and the soft prompts to harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the vision-language models. Further, ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting. In addition, we provide the analysis to comprehend how ProMetaR improves the generalizability of prompt tuning in the perspective of the gradient alignment. Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional prompt learning methods under base-to-base/base-to-new and domain generalization settings. The code of ProMetaR is available at https://github.com/mlvlab/ProMetaR.","sentences":["Pre-trained vision-language models have shown impressive success on various computer vision tasks with their zero-shot generalizability.","Recently, prompt learning approaches have been explored to efficiently and effectively adapt the vision-language models to a variety of downstream tasks.","However, most existing prompt learning methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the prompts are finetuned on a small data set from a specific target task.","To address this issue, we propose a Prompt Meta-Regularization (ProMetaR) to improve the generalizability of prompt learning for vision-language models.","Specifically, ProMetaR meta-learns both the regularizer and the soft prompts to harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the vision-language models.","Further, ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting.","In addition, we provide the analysis to comprehend how ProMetaR improves the generalizability of prompt tuning in the perspective of the gradient alignment.","Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional prompt learning methods under base-to-base/base-to-new and domain generalization settings.","The code of ProMetaR is available at https://github.com/mlvlab/ProMetaR."],"url":"http://arxiv.org/abs/2404.00851v1","category":"cs.CV"}
{"created":"2024-04-01 00:31:11","title":"3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching","abstract":"Optical-SAR image matching is a fundamental task for image fusion and visual navigation. However, all large-scale open SAR dataset for methods development are collected from single platform, resulting in limited satellite types and spatial resolutions. Since images captured by different sensors vary significantly in both geometric and radiometric appearance, existing methods may fail to match corresponding regions containing the same content. Besides, most of existing datasets have not been categorized based on the characteristics of different scenes. To encourage the design of more general multi-modal image matching methods, we introduce a large-scale Multi-sources,Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching(3MOS). It consists of 155K optical-SAR image pairs, including SAR data from six commercial satellites, with resolutions ranging from 1.25m to 12.5m. The data has been classified into eight scenes including urban, rural, plains, hills, mountains, water, desert, and frozen earth. Extensively experiments show that none of state-of-the-art methods achieve consistently superior performance across different sources, resolutions and scenes. In addition, the distribution of data has a substantial impact on the matching capability of deep learning models, this proposes the domain adaptation challenge in optical-SAR image matching. Our data and code will be available at:https://github.com/3M-OS/3MOS.","sentences":["Optical-SAR image matching is a fundamental task for image fusion and visual navigation.","However, all large-scale open SAR dataset for methods development are collected from single platform, resulting in limited satellite types and spatial resolutions.","Since images captured by different sensors vary significantly in both geometric and radiometric appearance, existing methods may fail to match corresponding regions containing the same content.","Besides, most of existing datasets have not been categorized based on the characteristics of different scenes.","To encourage the design of more general multi-modal image matching methods, we introduce a large-scale Multi-sources,Multi-resolutions, and","Multi-scenes dataset for Optical-SAR image matching(3MOS).","It consists of 155K optical-SAR image pairs, including SAR data from six commercial satellites, with resolutions ranging from 1.25m to 12.5m. The data has been classified into eight scenes including urban, rural, plains, hills, mountains, water, desert, and frozen earth.","Extensively experiments show that none of state-of-the-art methods achieve consistently superior performance across different sources, resolutions and scenes.","In addition, the distribution of data has a substantial impact on the matching capability of deep learning models, this proposes the domain adaptation challenge in optical-SAR image matching.","Our data and code will be available at:https://github.com/3M-OS/3MOS."],"url":"http://arxiv.org/abs/2404.00838v1","category":"cs.CV"}
{"created":"2024-03-31 22:18:56","title":"Towards Realistic Scene Generation with LiDAR Diffusion Models","abstract":"Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts. Our code and pretrained weights are available at https://github.com/hancyran/LiDAR-Diffusion.","sentences":["Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle.","This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power.","In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline.","Our method targets three major desiderata: pattern realism, geometry realism, and object realism.","Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context.","With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\\times$ faster).","Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts.","Our code and pretrained weights are available at https://github.com/hancyran/LiDAR-Diffusion."],"url":"http://arxiv.org/abs/2404.00815v1","category":"cs.CV"}
{"created":"2024-03-31 21:31:07","title":"Higher Dimensional Birkhoff attractors","abstract":"We extend to higher dimensions the notion of Birkhoff attractor of a dissipative map. We prove that this notion coincides with the classical Birkhoff attractor. We prove that for the dissipative system associated to the discounted Hamilton-Jacobi equation the graph of a solution is contained in the Birkhoff attractor. We also study what happens when we perturb a Hamiltonian system to make it dissipative and let the perturbation go to zero. The paper contains two important results on $\\gamma$-supports and elements of the $\\gamma$-completion of the space of exact Lagrangians. Firstly the $\\gamma$-support of a Lagrangian in a cotangent bundle carries the cohomology of the base and secondly given an exact Lagrangian $L$, any Floer theoretic equivalent Lagrangian is the $\\gamma$-limit of Hamiltonian images of $L$.","sentences":["We extend to higher dimensions the notion of Birkhoff attractor of a dissipative map.","We prove that this notion coincides with the classical Birkhoff attractor.","We prove that for the dissipative system associated to the discounted Hamilton-Jacobi equation the graph of a solution is contained in the Birkhoff attractor.","We also study what happens when we perturb a Hamiltonian system to make it dissipative and let the perturbation go to zero.","The paper contains two important results on $\\gamma$-supports and elements of the $\\gamma$-completion of the space of exact Lagrangians.","Firstly the $\\gamma$-support of a Lagrangian in a cotangent bundle carries the cohomology of the base and secondly given an exact Lagrangian $L$, any Floer theoretic equivalent Lagrangian is the $\\gamma$-limit of Hamiltonian images of $L$."],"url":"http://arxiv.org/abs/2404.00804v1","category":"math.SG"}
{"created":"2024-03-31 17:47:48","title":"Imitation dynamics and the replicator equation","abstract":"Evolutionary game theory has impacted many fields of research by providing a mathematical framework for studying the evolution and maintenance of social and moral behaviors. This success is owed in large part to the demonstration that the central equation of this theory - the replicator equation - is the deterministic limit of a stochastic imitation (social learning) dynamics. Here we offer an alternative elementary proof of this result, which holds for the scenario where players compare their instantaneous (not average) payoffs to decide whether to maintain or change their strategies, and only more successful individuals can be imitated.","sentences":["Evolutionary game theory has impacted many fields of research by providing a mathematical framework for studying the evolution and maintenance of social and moral behaviors.","This success is owed in large part to the demonstration that the central equation of this theory - the replicator equation - is the deterministic limit of a stochastic imitation (social learning) dynamics.","Here we offer an alternative elementary proof of this result, which holds for the scenario where players compare their instantaneous (not average) payoffs to decide whether to maintain or change their strategies, and only more successful individuals can be imitated."],"url":"http://arxiv.org/abs/2404.00754v1","category":"physics.soc-ph"}
{"created":"2024-03-31 17:37:28","title":"Causal dependencies and Shannon entropy budget -- Analysis of a reduced order atmospheric model","abstract":"The information entropy budget and the rate of information transfer between variables is studied in the context of a nonlinear reduced-order atmospheric model. The key ingredients of the dynamics are present in this model, namely the baroclinic and barotropic instabilities, the instability related to the presence of an orography, the dissipation related to the surface friction, and the large-scale meridional imbalance of energy. For the parameter chosen, the solutions of this system display a chaotic dynamics reminiscent of the large-scale atmospheric dynamics in the extra-tropics. The detailed information entropy budget analysis of this system reveals that the linear rotation terms plays a minor role in the generation of uncertainties as compared to the orography and the surface friction. Additionally, the dominant contribution comes from the nonlinear advection terms, and their decomposition in synergetic (co-variability) and single (impact of each single variable on the target one) components reveals that for some variables the co-variability dominates the information transfer. The estimation of the rate of information transfer based on time series is also discussed, and an extension of the Liang's approach to nonlinear observables, is proposed.","sentences":["The information entropy budget and the rate of information transfer between variables is studied in the context of a nonlinear reduced-order atmospheric model.","The key ingredients of the dynamics are present in this model, namely the baroclinic and barotropic instabilities, the instability related to the presence of an orography, the dissipation related to the surface friction, and the large-scale meridional imbalance of energy.","For the parameter chosen, the solutions of this system display a chaotic dynamics reminiscent of the large-scale atmospheric dynamics in the extra-tropics.","The detailed information entropy budget analysis of this system reveals that the linear rotation terms plays a minor role in the generation of uncertainties as compared to the orography and the surface friction.","Additionally, the dominant contribution comes from the nonlinear advection terms, and their decomposition in synergetic (co-variability) and single (impact of each single variable on the target one) components reveals that for some variables the co-variability dominates the information transfer.","The estimation of the rate of information transfer based on time series is also discussed, and an extension of the Liang's approach to nonlinear observables, is proposed."],"url":"http://arxiv.org/abs/2404.00749v1","category":"physics.ao-ph"}
{"created":"2024-03-31 17:37:03","title":"Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure","abstract":"In this uncertain world, data uncertainty is inherent in many applications and its importance is growing drastically due to the rapid development of modern technologies. Nowadays, researchers have paid more attention to mine patterns in uncertain databases. A few recent works attempt to mine frequent uncertain sequential patterns. Despite their success, they are incompetent to reduce the number of false-positive pattern generation in their mining process and maintain the patterns efficiently. In this paper, we propose multiple theoretically tightened pruning upper bounds that remarkably reduce the mining space. A novel hierarchical structure is introduced to maintain the patterns in a space-efficient way. Afterward, we develop a versatile framework for mining uncertain sequential patterns that can effectively handle weight constraints as well. Besides, with the advent of incremental uncertain databases, existing works are not scalable. There exist several incremental sequential pattern mining algorithms, but they are limited to mine in precise databases. Therefore, we propose a new technique to adapt our framework to mine patterns when the database is incremental. Finally, we conduct extensive experiments on several real-life datasets and show the efficacy of our framework in different applications.","sentences":["In this uncertain world, data uncertainty is inherent in many applications and its importance is growing drastically due to the rapid development of modern technologies.","Nowadays, researchers have paid more attention to mine patterns in uncertain databases.","A few recent works attempt to mine frequent uncertain sequential patterns.","Despite their success, they are incompetent to reduce the number of false-positive pattern generation in their mining process and maintain the patterns efficiently.","In this paper, we propose multiple theoretically tightened pruning upper bounds that remarkably reduce the mining space.","A novel hierarchical structure is introduced to maintain the patterns in a space-efficient way.","Afterward, we develop a versatile framework for mining uncertain sequential patterns that can effectively handle weight constraints as well.","Besides, with the advent of incremental uncertain databases, existing works are not scalable.","There exist several incremental sequential pattern mining algorithms, but they are limited to mine in precise databases.","Therefore, we propose a new technique to adapt our framework to mine patterns when the database is incremental.","Finally, we conduct extensive experiments on several real-life datasets and show the efficacy of our framework in different applications."],"url":"http://arxiv.org/abs/2404.01347v1","category":"cs.DB"}
{"created":"2024-03-31 17:18:57","title":"Adapting to Length Shift: FlexiLength Network for Trajectory Prediction","abstract":"Trajectory prediction plays an important role in various applications, including autonomous driving, robotics, and scene understanding. Existing approaches mainly focus on developing compact neural networks to increase prediction precision on public datasets, typically employing a standardized input duration. However, a notable issue arises when these models are evaluated with varying observation lengths, leading to a significant performance drop, a phenomenon we term the Observation Length Shift. To address this issue, we introduce a general and effective framework, the FlexiLength Network (FLN), to enhance the robustness of existing trajectory prediction techniques against varying observation periods. Specifically, FLN integrates trajectory data with diverse observation lengths, incorporates FlexiLength Calibration (FLC) to acquire temporal invariant representations, and employs FlexiLength Adaptation (FLA) to further refine these representations for more accurate future trajectory predictions. Comprehensive experiments on multiple datasets, ie, ETH/UCY, nuScenes, and Argoverse 1, demonstrate the effectiveness and flexibility of our proposed FLN framework.","sentences":["Trajectory prediction plays an important role in various applications, including autonomous driving, robotics, and scene understanding.","Existing approaches mainly focus on developing compact neural networks to increase prediction precision on public datasets, typically employing a standardized input duration.","However, a notable issue arises when these models are evaluated with varying observation lengths, leading to a significant performance drop, a phenomenon we term the Observation Length Shift.","To address this issue, we introduce a general and effective framework, the FlexiLength Network (FLN), to enhance the robustness of existing trajectory prediction techniques against varying observation periods.","Specifically, FLN integrates trajectory data with diverse observation lengths, incorporates FlexiLength Calibration (FLC) to acquire temporal invariant representations, and employs FlexiLength Adaptation (FLA) to further refine these representations for more accurate future trajectory predictions.","Comprehensive experiments on multiple datasets, ie, ETH/UCY, nuScenes, and Argoverse 1, demonstrate the effectiveness and flexibility of our proposed FLN framework."],"url":"http://arxiv.org/abs/2404.00742v1","category":"cs.CV"}
{"created":"2024-03-31 15:09:47","title":"Survey of Computerized Adaptive Testing: A Machine Learning Perspective","abstract":"Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of current methods, strengths, limitations, and challenges, we strive to develop robust, fair, and efficient CAT systems. By bridging psychometric-driven CAT research with machine learning, this survey advocates for a more inclusive and interdisciplinary approach to the future of adaptive testing.","sentences":["Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance.","Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices.","While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques.","This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method.","By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality.","Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components.","Through an analysis of current methods, strengths, limitations, and challenges, we strive to develop robust, fair, and efficient CAT systems.","By bridging psychometric-driven CAT research with machine learning, this survey advocates for a more inclusive and interdisciplinary approach to the future of adaptive testing."],"url":"http://arxiv.org/abs/2404.00712v1","category":"cs.LG"}
{"created":"2024-03-31 13:54:18","title":"The biharmonic optimal support problem","abstract":"We establish a $\\Gamma$-convergence result for $h\\to 0$ of a thin nonlinearly elastic 3D-plate of thickness $h>0$ which is assumed to be glued to a support region in the 2D-plane $x_3=0$ over the $h$-2D-neighborhood of a given closed set $K$. In the regime of very small vertical forces we identify the $\\Gamma$-limit as being the bi-harmonic energy, with Dirichlet condition on the gluing region $K$, following a general strategy by Friesecke, James, and M\\\"uller that we have to adapt in presence of the glued region. Then we introduce a shape optimization problem that we call \"optimal support problem\" and which aims to find the best glued plate. In this problem the bi-harmonic energy is optimized among all possible glued regions $K$ that we assume to be connected and for which we penalize the length. By relating the dual problem with Griffith almost-minimizers, we are able to prove that any minimizer is $C^{1,\\alpha}$ regular outside a set of Hausdorff dimension strictly less then one.","sentences":["We establish a $\\Gamma$-convergence result for $h\\to 0$ of a thin nonlinearly elastic 3D-plate of thickness $h>0$ which is assumed to be glued to a support region in the 2D-plane $x_3=0$ over the $h$-2D-neighborhood of a given closed set $K$. In the regime of very small vertical forces we identify the $\\Gamma$-limit as being the bi-harmonic energy, with Dirichlet condition on the gluing region $K$, following a general strategy by Friesecke, James, and M\\\"uller that we have to adapt in presence of the glued region.","Then we introduce a shape optimization problem that we call \"optimal support problem\" and which aims to find the best glued plate.","In this problem the bi-harmonic energy is optimized among all possible glued regions $K$ that we assume to be connected and for which we penalize the length.","By relating the dual problem with Griffith almost-minimizers, we are able to prove that any minimizer is $C^{1,\\alpha}$ regular outside a set of Hausdorff dimension strictly less then one."],"url":"http://arxiv.org/abs/2404.00689v1","category":"math.AP"}
{"created":"2024-03-31 13:07:00","title":"OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees","abstract":"We present a method to reconstruct indoor and outdoor static scene geometry and appearance from an omnidirectional video moving in a small circular sweep. This setting is challenging because of the small baseline and large depth ranges, making it difficult to find ray crossings. To better constrain the optimization, we estimate geometry as a signed distance field within a spherical binoctree data structure and use a complementary efficient tree traversal strategy based on a breadth-first search for sampling. Unlike regular grids or trees, the shape of this structure well-matches the camera setting, creating a better memory-quality trade-off. From an initial depth estimate, the binoctree is adaptively subdivided throughout the optimization; previous methods use a fixed depth that leaves the scene undersampled. In comparison with three neural optimization methods and two non-neural methods, ours shows decreased geometry error on average, especially in a detailed scene, while significantly reducing the required number of voxels to represent such details.","sentences":["We present a method to reconstruct indoor and outdoor static scene geometry and appearance from an omnidirectional video moving in a small circular sweep.","This setting is challenging because of the small baseline and large depth ranges, making it difficult to find ray crossings.","To better constrain the optimization, we estimate geometry as a signed distance field within a spherical binoctree data structure and use a complementary efficient tree traversal strategy based on a breadth-first search for sampling.","Unlike regular grids or trees, the shape of this structure well-matches the camera setting, creating a better memory-quality trade-off.","From an initial depth estimate, the binoctree is adaptively subdivided throughout the optimization; previous methods use a fixed depth that leaves the scene undersampled.","In comparison with three neural optimization methods and two non-neural methods, ours shows decreased geometry error on average, especially in a detailed scene, while significantly reducing the required number of voxels to represent such details."],"url":"http://arxiv.org/abs/2404.00678v1","category":"cs.CV"}
{"created":"2024-03-31 12:48:07","title":"LLM meets Vision-Language Models for Zero-Shot One-Class Classification","abstract":"We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label","sentences":["We consider the problem of zero-shot one-class visual classification.","In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task.","We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification.","By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting.","Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones.","Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label"],"url":"http://arxiv.org/abs/2404.00675v2","category":"cs.CV"}
{"created":"2024-03-31 12:45:23","title":"Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated Objects","abstract":"We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at https://github.com/RussRobin/Knowledge_NeRF.","sentences":["We present Knowledge NeRF to synthesize novel views for dynamic scenes.","Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains.","Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos.","However, qualities of their reconstructed scenes are limited.","To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.","We pretrain a NeRF model for an articulated object.","When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state.","We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states.","Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state.","Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects.","The data and implementation are publicly available at https://github.com/RussRobin/Knowledge_NeRF."],"url":"http://arxiv.org/abs/2404.00674v1","category":"cs.CV"}
{"created":"2024-03-31 12:22:23","title":"Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation","abstract":"Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15\\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.","sentences":["Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches.","However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity.","While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage.","To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images.","To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision.","Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation.","To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection.","The experimental results highlight that, by simply using extremely weak annotation, e.g., 15\\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart.","The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model."],"url":"http://arxiv.org/abs/2404.00667v1","category":"cs.CV"}
{"created":"2024-03-31 12:07:04","title":"DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware Stable Diffusion","abstract":"Diffusion models, known for their powerful generative capabilities, play a crucial role in addressing real-world super-resolution challenges. However, these models often focus on improving local textures while neglecting the impacts of global degradation, which can significantly reduce semantic fidelity and lead to inaccurate reconstructions and suboptimal super-resolution performance. To address this issue, we introduce a novel two-stage, degradation-aware framework that enhances the diffusion model's ability to recognize content and degradation in low-resolution images. In the first stage, we employ unsupervised contrastive learning to obtain representations of image degradations. In the second stage, we integrate a degradation-aware module into a simplified ControlNet, enabling flexible adaptation to various degradations based on the learned representations. Furthermore, we decompose the degradation-aware features into global semantics and local details branches, which are then injected into the diffusion denoising module to modulate the target generation. Our method effectively recovers semantically precise and photorealistic details, particularly under significant degradation conditions, demonstrating state-of-the-art performance across various benchmarks. Codes will be released at https://github.com/bichunyang419/DeeDSR.","sentences":["Diffusion models, known for their powerful generative capabilities, play a crucial role in addressing real-world super-resolution challenges.","However, these models often focus on improving local textures while neglecting the impacts of global degradation, which can significantly reduce semantic fidelity and lead to inaccurate reconstructions and suboptimal super-resolution performance.","To address this issue, we introduce a novel two-stage, degradation-aware framework that enhances the diffusion model's ability to recognize content and degradation in low-resolution images.","In the first stage, we employ unsupervised contrastive learning to obtain representations of image degradations.","In the second stage, we integrate a degradation-aware module into a simplified ControlNet, enabling flexible adaptation to various degradations based on the learned representations.","Furthermore, we decompose the degradation-aware features into global semantics and local details branches, which are then injected into the diffusion denoising module to modulate the target generation.","Our method effectively recovers semantically precise and photorealistic details, particularly under significant degradation conditions, demonstrating state-of-the-art performance across various benchmarks.","Codes will be released at https://github.com/bichunyang419/DeeDSR."],"url":"http://arxiv.org/abs/2404.00661v1","category":"cs.CV"}
{"created":"2024-03-31 12:05:35","title":"Real-space renormalisation approach to the Chalker-Coddington model revisited: improved statistics","abstract":"The real-space renormalisation group method can be applied to the Chalker-Coddington model of the quantum Hall transition to provide a convenient numerical estimation of the localisation critical exponent, $\\nu$. Previous such studies found $\\nu\\sim 2.39$ which falls considerably short of the current best estimates by transfer matrix ($\\nu\\approx 2.593$) and exact-diagonalisation studies ($\\nu=2.58(3)$). By increasing the amount of data $500$ fold we can now measure closer to the critical point and find an improved estimate $\\nu\\approx 2.51$. This deviates only $\\sim 3\\%$ from the previous two values and is already better than the $\\sim 7\\%$ accuracy of the classical small-cell renormalisation approach from which our method is adapted. We also study a previously proposed mixing of the Chalker-Coddington model with a classical scattering model which is meant to provide a route to understanding why experimental estimates give a lower $\\nu\\sim 2.3$. Upon implementing this mixing into our RG unit, we find only further increases to the value of $\\nu$.","sentences":["The real-space renormalisation group method can be applied to the Chalker-Coddington model of the quantum Hall transition to provide a convenient numerical estimation of the localisation critical exponent, $\\nu$. Previous such studies found $\\nu\\sim 2.39$ which falls considerably short of the current best estimates by transfer matrix ($\\nu\\approx 2.593$) and exact-diagonalisation studies ($\\nu=2.58(3)$).","By increasing the amount of data $500$ fold we can now measure closer to the critical point and find an improved estimate $\\nu\\approx 2.51$.","This deviates only $\\sim 3\\%$ from the previous two values and is already better than the $\\sim 7\\%$ accuracy of the classical small-cell renormalisation approach from which our method is adapted.","We also study a previously proposed mixing of the Chalker-Coddington model with a classical scattering model which is meant to provide a route to understanding why experimental estimates give a lower $\\nu\\sim 2.3$. Upon implementing this mixing into our RG unit, we find only further increases to the value of $\\nu$."],"url":"http://arxiv.org/abs/2404.00660v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-31 12:01:32","title":"WavLLM: Towards Robust and Adaptive Speech Large Language Model","abstract":"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\url{aka.ms/wavllm}.","sentences":["The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation.","However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks.","In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach.","Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity.","Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks.","To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage.","We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set.","Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.","Furthermore, our model successfully completes Gaokao tasks without specialized training.","The codes, models, audio, and Gaokao evaluation set can be accessed at \\url{aka.ms/wavllm}."],"url":"http://arxiv.org/abs/2404.00656v1","category":"cs.CL"}
{"created":"2024-03-31 11:43:39","title":"Dual DETRs for Multi-Label Temporal Action Detection","abstract":"Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.","sentences":["Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos.","Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task.","However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization.","To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level.","Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure.","This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level.","On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels.","Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner.","Then, the matched queries are initialized using position and content prior from the matched action proposal.","The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding.","We evaluate DualDETR on three challenging multi-label TAD benchmarks.","The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP."],"url":"http://arxiv.org/abs/2404.00653v1","category":"cs.CV"}
{"created":"2024-03-31 11:33:39","title":"SpiralMLP: A Lightweight Vision MLP Architecture","abstract":"We present SpiralMLP, a novel architecture that introduces a Spiral FC layer as a replacement for the conventional Token Mixing approach. Differing from several existing MLP-based models that primarily emphasize axes, our Spiral FC layer is designed as a deformable convolution layer with spiral-like offsets. We further adapt Spiral FC into two variants: Self-Spiral FC and Cross-Spiral FC, which enable both local and global feature integration seamlessly, eliminating the need for additional processing steps. To thoroughly investigate the effectiveness of the spiral-like offsets and validate our design, we conduct ablation studies and explore optimal configurations. In empirical tests, SpiralMLP reaches state-of-the-art performance, similar to Transformers, CNNs, and other MLPs, benchmarking on ImageNet-1k, COCO and ADE20K. SpiralMLP still maintains linear computational complexity O(HW) and is compatible with varying input image resolutions. Our study reveals that targeting the full receptive field is not essential for achieving high performance, instead, adopting a refined approach offers better results.","sentences":["We present SpiralMLP, a novel architecture that introduces a Spiral FC layer as a replacement for the conventional Token Mixing approach.","Differing from several existing MLP-based models that primarily emphasize axes, our Spiral FC layer is designed as a deformable convolution layer with spiral-like offsets.","We further adapt Spiral FC into two variants: Self-Spiral FC and Cross-Spiral FC, which enable both local and global feature integration seamlessly, eliminating the need for additional processing steps.","To thoroughly investigate the effectiveness of the spiral-like offsets and validate our design, we conduct ablation studies and explore optimal configurations.","In empirical tests, SpiralMLP reaches state-of-the-art performance, similar to Transformers, CNNs, and other MLPs, benchmarking on ImageNet-1k, COCO and ADE20K. SpiralMLP still maintains linear computational complexity O(HW) and is compatible with varying input image resolutions.","Our study reveals that targeting the full receptive field is not essential for achieving high performance, instead, adopting a refined approach offers better results."],"url":"http://arxiv.org/abs/2404.00648v1","category":"cs.CV"}
{"created":"2024-03-31 11:09:19","title":"Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for Enhanced CCTV Security","abstract":"This research introduces an innovative security enhancement approach, employing advanced image analysis and soft computing. The focus is on an intelligent surveillance system that detects unauthorized individuals in restricted areas by analyzing attire. Traditional security measures face challenges in monitoring unauthorized access. Leveraging YOLOv8, an advanced object detection algorithm, our system identifies authorized personnel based on their attire in CCTV footage. The methodology involves training the YOLOv8 model on a comprehensive dataset of uniform patterns, ensuring precise recognition in specific regions. Soft computing techniques enhance adaptability to dynamic environments and varying lighting conditions. This research contributes to image analysis and soft computing, providing a sophisticated security solution. Emphasizing uniform-based anomaly detection, it establishes a foundation for robust security systems in restricted areas. The outcomes highlight the potential of YOLOv8-based surveillance in ensuring safety in sensitive locations.","sentences":["This research introduces an innovative security enhancement approach, employing advanced image analysis and soft computing.","The focus is on an intelligent surveillance system that detects unauthorized individuals in restricted areas by analyzing attire.","Traditional security measures face challenges in monitoring unauthorized access.","Leveraging YOLOv8, an advanced object detection algorithm, our system identifies authorized personnel based on their attire in CCTV footage.","The methodology involves training the YOLOv8 model on a comprehensive dataset of uniform patterns, ensuring precise recognition in specific regions.","Soft computing techniques enhance adaptability to dynamic environments and varying lighting conditions.","This research contributes to image analysis and soft computing, providing a sophisticated security solution.","Emphasizing uniform-based anomaly detection, it establishes a foundation for robust security systems in restricted areas.","The outcomes highlight the potential of YOLOv8-based surveillance in ensuring safety in sensitive locations."],"url":"http://arxiv.org/abs/2404.00645v1","category":"cs.CV"}
{"created":"2024-03-31 09:43:22","title":"Fluid Antenna Relay Assisted Communication Systems Through Antenna Location Optimization","abstract":"In this paper, we investigate the problem of resource allocation for fluid antenna relay (FAR) system with antenna location optimization. In the considered model, each user transmits information to a base station (BS) with help of FAR. The antenna location of the FAR is flexible and can be adapted to dynamic location distribution of the users. We formulate a sum rate maximization problem through jointly optimizing the antenna location and bandwidth allocation with meeting the minimum rate requirements, total bandwidth budget, and feasible antenna region constraints. To solve this problem, we obtain the optimal bandwidth in closed form. Based on the optimal bandwidth, the original problem is reduced to the antenna location optimization problem and an alternating algorithm is proposed. Simulation results verify the effectiveness of the proposed algorithm and the sum rate can be increased by up to 125% compared to the conventional schemes.","sentences":["In this paper, we investigate the problem of resource allocation for fluid antenna relay (FAR) system with antenna location optimization.","In the considered model, each user transmits information to a base station (BS) with help of FAR.","The antenna location of the FAR is flexible and can be adapted to dynamic location distribution of the users.","We formulate a sum rate maximization problem through jointly optimizing the antenna location and bandwidth allocation with meeting the minimum rate requirements, total bandwidth budget, and feasible antenna region constraints.","To solve this problem, we obtain the optimal bandwidth in closed form.","Based on the optimal bandwidth, the original problem is reduced to the antenna location optimization problem and an alternating algorithm is proposed.","Simulation results verify the effectiveness of the proposed algorithm and the sum rate can be increased by up to 125% compared to the conventional schemes."],"url":"http://arxiv.org/abs/2404.00628v1","category":"cs.IT"}
{"created":"2024-03-31 09:32:31","title":"Domain Generalizable Person Search Using Unreal Dataset","abstract":"Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.","sentences":["Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues.","The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited.","We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets.","To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively.","Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features.","Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens."],"url":"http://arxiv.org/abs/2404.00626v1","category":"cs.CV"}
{"created":"2024-03-31 09:25:28","title":"Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance","abstract":"Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability. Within this context, Deep Reinforcement Learning (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation. Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles. However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large. To combat this, our work delves into the application of Variational AutoEncoders (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent. The agent's performance, encompassing path-following and collision avoidance, is systematically tested and evaluated within a stochastic simulation environment, presenting a comprehensive exploration of our proposed approach in maritime control systems.","sentences":["Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability.","Within this context, Deep Reinforcement Learning (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation.","Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles.","However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large.","To combat this, our work delves into the application of Variational AutoEncoders (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent.","The agent's performance, encompassing path-following and collision avoidance, is systematically tested and evaluated within a stochastic simulation environment, presenting a comprehensive exploration of our proposed approach in maritime control systems."],"url":"http://arxiv.org/abs/2404.00623v1","category":"cs.LG"}
{"created":"2024-03-31 09:20:30","title":"Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey","abstract":"Personalized recommendation serves as a ubiquitous channel for users to discover information or items tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in pretrained multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications to recommender systems. Furthermore, we discuss open challenges and opportunities for future research in this domain. We hope that this survey, along with our tutorial materials, will inspire further research efforts to advance this evolving landscape.","sentences":["Personalized recommendation serves as a ubiquitous channel for users to discover information or items tailored to their interests.","However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video.","This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms.","The recent advancements in pretrained multimodal models offer new opportunities and challenges in developing content-aware recommender systems.","This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications to recommender systems.","Furthermore, we discuss open challenges and opportunities for future research in this domain.","We hope that this survey, along with our tutorial materials, will inspire further research efforts to advance this evolving landscape."],"url":"http://arxiv.org/abs/2404.00621v1","category":"cs.IR"}
{"created":"2024-03-31 09:10:03","title":"Geometric and information-theoretic aspects of quantum thermodynamics","abstract":"In this thesis, I investigate various aspects of one of the most fundamental questions in thermodynamics: what state transformations can quantum systems undergo while interacting with a thermal bath under specific constraints? These constraints may involve total energy conservation, memory effects, or finite-size considerations. Addressing this question leads to (i) a characterisation of the structure of the thermodynamic arrow of time, (ii) a framework bridging the gap between memoryless and arbitrarily non-Markovian thermodynamic processes, and (iii) a derivation of the famous fluctuation-dissipation relation within a quantum information framework. Finally, the last part of this thesis focuses on studying a ubiquitous phenomenon in science, so-called catalysis. It involves using an auxiliary system (a catalyst) to enable processes that would otherwise be impossible. Over the last two decades, this notion has spread to the field of quantum physics. However, this effect is typically described within a highly abstract framework. Despite its successes, this approach struggles to fully capture the behaviour of physically realisable systems, thereby limiting the applicability of quantum catalysis in practical scenarios. Strikingly, I will demonstrate this effect in a paradigmatic quantum optics setup, namely the Jaynes-Cummings model, where an atom interacts with an optical cavity. The atom plays the role of the catalyst and allows for the deterministic generation of non-classical light in the cavity, as evidenced by sub-Poissonian statistics or Wigner negativity.","sentences":["In this thesis, I investigate various aspects of one of the most fundamental questions in thermodynamics: what state transformations can quantum systems undergo while interacting with a thermal bath under specific constraints?","These constraints may involve total energy conservation, memory effects, or finite-size considerations.","Addressing this question leads to (i) a characterisation of the structure of the thermodynamic arrow of time, (ii) a framework bridging the gap between memoryless and arbitrarily non-Markovian thermodynamic processes, and (iii) a derivation of the famous fluctuation-dissipation relation within a quantum information framework.","Finally, the last part of this thesis focuses on studying a ubiquitous phenomenon in science, so-called catalysis.","It involves using an auxiliary system (a catalyst) to enable processes that would otherwise be impossible.","Over the last two decades, this notion has spread to the field of quantum physics.","However, this effect is typically described within a highly abstract framework.","Despite its successes, this approach struggles to fully capture the behaviour of physically realisable systems, thereby limiting the applicability of quantum catalysis in practical scenarios.","Strikingly, I will demonstrate this effect in a paradigmatic quantum optics setup, namely the Jaynes-Cummings model, where an atom interacts with an optical cavity.","The atom plays the role of the catalyst and allows for the deterministic generation of non-classical light in the cavity, as evidenced by sub-Poissonian statistics or Wigner negativity."],"url":"http://arxiv.org/abs/2404.00617v1","category":"quant-ph"}
{"created":"2024-03-31 09:04:09","title":"Harmonic chain driven by active Rubin bath: transport properties and steady-state correlations","abstract":"Characterizing the properties of an extended system driven by active reservoirs is a question of increasing importance. Here we address this question in two steps. We start by investigating the dynamics of a probe particle connected to an `active Rubin bath' -- a linear chain of overdamped run-and-tumble particles. We derive exact analytical expressions for the effective noise and dissipation kernels, acting on the probe, and show that the active nature of the bath leads to a modified fluctuation-dissipation relation. In the next step, we study the properties of an activity-driven system, modeled by a chain of harmonic oscillators connected to two such active reservoirs at the two ends. We show that the system reaches a nonequilibrium stationary state (NESS), remarkably different from that generated due to a thermal gradient. We characterize this NESS by computing the kinetic temperature profile, spatial and temporal velocity correlations of the oscillators, and the average energy current flowing through the system. It turns out that, the activity drive leads to the emergence of two characteristic length scales, proportional to the activities of the reservoirs. Strong signatures of activity are also manifest in the anomalous short-time decay of the velocity autocorrelations. Finally, we find that the energy current shows a non-monotonic dependence on the activity drive and reversal in direction, corroborating previous findings.","sentences":["Characterizing the properties of an extended system driven by active reservoirs is a question of increasing importance.","Here we address this question in two steps.","We start by investigating the dynamics of a probe particle connected to an `active Rubin bath' -- a linear chain of overdamped run-and-tumble particles.","We derive exact analytical expressions for the effective noise and dissipation kernels, acting on the probe, and show that the active nature of the bath leads to a modified fluctuation-dissipation relation.","In the next step, we study the properties of an activity-driven system, modeled by a chain of harmonic oscillators connected to two such active reservoirs at the two ends.","We show that the system reaches a nonequilibrium stationary state (NESS), remarkably different from that generated due to a thermal gradient.","We characterize this NESS by computing the kinetic temperature profile, spatial and temporal velocity correlations of the oscillators, and the average energy current flowing through the system.","It turns out that, the activity drive leads to the emergence of two characteristic length scales, proportional to the activities of the reservoirs.","Strong signatures of activity are also manifest in the anomalous short-time decay of the velocity autocorrelations.","Finally, we find that the energy current shows a non-monotonic dependence on the activity drive and reversal in direction, corroborating previous findings."],"url":"http://arxiv.org/abs/2404.00615v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-02 17:59:00","title":"Convergence of overlapping domain decomposition methods with PML transmission conditions applied to nontrapping Helmholtz problems","abstract":"We study overlapping Schwarz methods for the Helmholtz equation posed in any dimension with large, real wavenumber and smooth variable wave speed. The radiation condition is approximated by a Cartesian perfectly-matched layer (PML). The domain-decomposition subdomains are overlapping hyperrectangles with Cartesian PMLs at their boundaries. The overlaps of the subdomains and the widths of the PMLs are all taken to be independent of the wavenumber.   For both parallel (i.e., additive) and sequential (i.e., multiplicative) methods, we show that after a specified number of iterations -- depending on the behaviour of the geometric-optic rays -- the error is smooth and smaller than any negative power of the wavenumber. For the parallel method, the specified number of iterations is less than the maximum number of subdomains, counted with their multiplicity, that a geometric-optic ray can intersect.   These results, which are illustrated by numerical experiments, are the first wavenumber-explicit results about convergence of overlapping Schwarz methods for the Helmholtz equation, and the first wavenumber-explicit results about convergence of any domain-decomposition method for the Helmholtz equation with a non-trivial scatterer (here a variable wave speed).","sentences":["We study overlapping Schwarz methods for the Helmholtz equation posed in any dimension with large, real wavenumber and smooth variable wave speed.","The radiation condition is approximated by a Cartesian perfectly-matched layer (PML).","The domain-decomposition subdomains are overlapping hyperrectangles with Cartesian PMLs at their boundaries.","The overlaps of the subdomains and the widths of the PMLs are all taken to be independent of the wavenumber.   ","For both parallel (i.e., additive) and sequential (i.e., multiplicative) methods, we show that after a specified number of iterations -- depending on the behaviour of the geometric-optic rays -- the error is smooth and smaller than any negative power of the wavenumber.","For the parallel method, the specified number of iterations is less than the maximum number of subdomains, counted with their multiplicity, that a geometric-optic ray can intersect.   ","These results, which are illustrated by numerical experiments, are the first wavenumber-explicit results about convergence of overlapping Schwarz methods for the Helmholtz equation, and the first wavenumber-explicit results about convergence of any domain-decomposition method for the Helmholtz equation with a non-trivial scatterer (here a variable wave speed)."],"url":"http://arxiv.org/abs/2404.02156v1","category":"math.NA"}
{"created":"2024-04-02 17:48:46","title":"ResNet with Integrated Convolutional Block Attention Module for Ship Classification Using Transfer Learning on Optical Satellite Imagery","abstract":"This study proposes a novel transfer learning framework for effective ship classification using high-resolution optical remote sensing satellite imagery. The framework is based on the deep convolutional neural network model ResNet50 and incorporates the Convolutional Block Attention Module (CBAM) to enhance performance. CBAM enables the model to attend to salient features in the images, allowing it to better discriminate between subtle differences between ships and backgrounds. Furthermore, this study adopts a transfer learning approach tailored for accurately classifying diverse types of ships by fine-tuning a pre-trained model for the specific task. Experimental results demonstrate the efficacy of the proposed framework in ship classification using optical remote sensing imagery, achieving a high classification accuracy of 94% across 5 classes, outperforming existing methods. This research holds potential applications in maritime surveillance and management, illegal fishing detection, and maritime traffic monitoring.","sentences":["This study proposes a novel transfer learning framework for effective ship classification using high-resolution optical remote sensing satellite imagery.","The framework is based on the deep convolutional neural network model ResNet50 and incorporates the Convolutional Block Attention Module (CBAM) to enhance performance.","CBAM enables the model to attend to salient features in the images, allowing it to better discriminate between subtle differences between ships and backgrounds.","Furthermore, this study adopts a transfer learning approach tailored for accurately classifying diverse types of ships by fine-tuning a pre-trained model for the specific task.","Experimental results demonstrate the efficacy of the proposed framework in ship classification using optical remote sensing imagery, achieving a high classification accuracy of 94% across 5 classes, outperforming existing methods.","This research holds potential applications in maritime surveillance and management, illegal fishing detection, and maritime traffic monitoring."],"url":"http://arxiv.org/abs/2404.02135v1","category":"cs.CV"}
{"created":"2024-04-02 16:18:49","title":"Asymptotics of resampling without replacement in robust and logistic regression","abstract":"This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order. For a given dataset $(\\bm{X},\\bm{y})\\in\\mathbb{R}^{n\\times p}\\times \\mathbb{R}^n$ and fixed subsample ratio $q\\in(0,1)$, the practitioner samples independently of $(\\bm{X},\\bm{y})$ iid subsets $I_1,...,I_M$ of $\\{1,...,n\\}$ of size $q n$ and trains estimators $\\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$ on the corresponding subsets of rows of $(\\bm{X},\\bm{y})$. Understanding the performance of the bagged estimate $\\bm{\\bar{\\beta}} = \\frac1M\\sum_{m=1}^M \\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\\bm{\\hat{\\beta}}(I_m)$ and $\\bm{\\hat{\\beta}}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$. In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data. The limit is characterized as the unique solution of a simple nonlinear equation. We further provide data-driven estimators that are consistent for estimating this limit. These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\\bm{\\bar{\\beta}}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(\\bm{x}_i^T \\bm{\\hat{\\beta}}(I_m), \\bm{x}_i^T \\bm{\\hat{\\beta}}(I_{m'}))$ for observations $i\\in I_m\\cap I_{m'}$, i.e., for observations used to train both estimates.","sentences":["This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order.","For a given dataset $(\\bm{X},\\bm{y})\\in\\mathbb{R}^{n\\times p}\\times \\mathbb{R}^n$ and fixed subsample ratio $q\\in(0,1)$, the practitioner samples independently of $(\\bm{X},\\bm{y})$ iid subsets $I_1,...,I_M$ of $\\{1,...,n\\}$ of size $q n$ and trains estimators $\\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$ on the corresponding subsets of rows of $(\\bm{X},\\bm{y})$. Understanding the performance of the bagged estimate $\\bm{\\bar{\\beta}} = \\frac1M\\sum_{m=1}^M \\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\\bm{\\hat{\\beta}}(I_m)$ and $\\bm{\\hat{\\beta}}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$. In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data.","The limit is characterized as the unique solution of a simple nonlinear equation.","We further provide data-driven estimators that are consistent for estimating this limit.","These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\\bm{\\bar{\\beta}}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(\\bm{x}_i^T \\bm{\\hat{\\beta}}(I_m), \\bm{x}_i^T \\bm{\\hat{\\beta}}(I_{m'}))$ for observations $i\\in I_m\\cap I_{m'}$, i.e., for observations used to train both estimates."],"url":"http://arxiv.org/abs/2404.02070v1","category":"math.ST"}
{"created":"2024-04-02 15:57:27","title":"Uniformity in nonreduced rings via Noetherian operators","abstract":"We prove a differential version of the Artin-Rees lemma with the use of Noetherian differential operators. As a consequence, we obtain several uniformity results for nonreduced rings.","sentences":["We prove a differential version of the Artin-Rees lemma with the use of Noetherian differential operators.","As a consequence, we obtain several uniformity results for nonreduced rings."],"url":"http://arxiv.org/abs/2404.02057v1","category":"math.AC"}
{"created":"2024-04-02 15:51:52","title":"On Hamiltonian Structures of Partial Difference Equations","abstract":"We first introduce the notion of Hamiltonian structure for a partial difference equation. Then we construct some infinite quivers, and realize the discrete KdV equation, the Hirota-Miwa equation and its various reductions as the mutation relations of the corresponding cluster algebras. Finally, we show that the log-canonical Poisson structures associated to these cluster algebras give the Hamiltonian structures or the bihamiltonian structures of these partial difference equations.","sentences":["We first introduce the notion of Hamiltonian structure for a partial difference equation.","Then we construct some infinite quivers, and realize the discrete KdV equation, the Hirota-Miwa equation and its various reductions as the mutation relations of the corresponding cluster algebras.","Finally, we show that the log-canonical Poisson structures associated to these cluster algebras give the Hamiltonian structures or the bihamiltonian structures of these partial difference equations."],"url":"http://arxiv.org/abs/2404.02055v1","category":"math-ph"}
{"created":"2024-04-02 15:20:25","title":"Exploring Spin Polarization of Heavy Quarks in Magnetic Fields and Hot Medium","abstract":"Relativistic heavy-ion collisions give rise to the formation of both deconfined QCD matter and a strong magnetic field. The spin of heavy quarks is influenced by interactions with the external magnetic field as well as by random scatterings with thermal light partons. The presence of QCD matter comprising charged quarks can extend the lifetime and strength of the magnetic field, thereby enhancing the degree of heavy quark polarization. However, the random scatterings with QCD matter tend to diminish heavy quark polarization. In this study, we utilize the Landau-Lifshitz-Gilbert (LLG) equation to investigate both these contributions. Taking into account the realistic evolutions of medium temperatures and the in-medium magnetic fields at the Relativistic Heavy-Ion Collider (RHIC) and the Large Hadron Collider (LHC), we observe that heavy quark polarization is limited by the short lifetime of the magnetic field and the high temperatures of the medium. Furthermore, we explore the mass dependence of quark polarization, revealing that the polarization degree of strange quarks is much larger than that of charm quarks.","sentences":["Relativistic heavy-ion collisions give rise to the formation of both deconfined QCD matter and a strong magnetic field.","The spin of heavy quarks is influenced by interactions with the external magnetic field as well as by random scatterings with thermal light partons.","The presence of QCD matter comprising charged quarks can extend the lifetime and strength of the magnetic field, thereby enhancing the degree of heavy quark polarization.","However, the random scatterings with QCD matter tend to diminish heavy quark polarization.","In this study, we utilize the Landau-Lifshitz-Gilbert (LLG) equation to investigate both these contributions.","Taking into account the realistic evolutions of medium temperatures and the in-medium magnetic fields at the Relativistic Heavy-Ion Collider (RHIC) and the Large Hadron Collider (LHC), we observe that heavy quark polarization is limited by the short lifetime of the magnetic field and the high temperatures of the medium.","Furthermore, we explore the mass dependence of quark polarization, revealing that the polarization degree of strange quarks is much larger than that of charm quarks."],"url":"http://arxiv.org/abs/2404.02032v1","category":"nucl-th"}
{"created":"2024-04-02 15:16:38","title":"High-energy neutrinos flavour composition as a probe of neutrino magnetic moments","abstract":"Neutrino propagation in the Galactic magnetic field is considered. To describe neutrino flavour and spin oscillations on the galactic scale baselines an approach using wave packets is developed. Evolution equations for the neutrino wave packets in a uniform and non-uniform magnetic field are derived. Analytical expressions for neutrino flavour and spin oscillations probabilities accounting for damping due to wave packet separation are obtained for the case of uniform magnetic field. It is shown that for oscillations on magnetic frequencies $\\omega_i^B = \\mu_i B_\\perp$ the coherence lengths that characterizes the damping scale is proportional to the cube of neutrino average momentum $p_0^3$. Probabilities of flavour and spin oscillations are calculated numerically for neutrino interacting with the non-uniform Galactic magnetic field. Flavour compositions of high-energy neutrino flux coming from the Galactic centre are calculated accounting for neutrino interaction with the magnetic field. It is shown that for neutrino magnetic moments $\\sim 10^{-13} \\mu_B$ and larger these flavour compositions significantly differ from ones predicted by the vacuum neutrino oscillations scenario.","sentences":["Neutrino propagation in the Galactic magnetic field is considered.","To describe neutrino flavour and spin oscillations on the galactic scale baselines an approach using wave packets is developed.","Evolution equations for the neutrino wave packets in a uniform and non-uniform magnetic field are derived.","Analytical expressions for neutrino flavour and spin oscillations probabilities accounting for damping due to wave packet separation are obtained for the case of uniform magnetic field.","It is shown that for oscillations on magnetic frequencies $\\omega_i^B = \\mu_i B_\\perp$ the coherence lengths that characterizes the damping scale is proportional to the cube of neutrino average momentum $p_0^3$.","Probabilities of flavour and spin oscillations are calculated numerically for neutrino interacting with the non-uniform Galactic magnetic field.","Flavour compositions of high-energy neutrino flux coming from the Galactic centre are calculated accounting for neutrino interaction with the magnetic field.","It is shown that for neutrino magnetic moments $\\sim 10^{-13} \\mu_B$ and larger these flavour compositions significantly differ from ones predicted by the vacuum neutrino oscillations scenario."],"url":"http://arxiv.org/abs/2404.02027v1","category":"hep-ph"}
{"created":"2024-04-02 15:09:52","title":"Algebraic structures in Lagrangian Floer cohomology modelled on differential forms","abstract":"We define a structure of an algebra on the Lagrangian Floer cohomology of a Lagrangian submanifold over the quantum cohomology of the ambient symplectic manifold. The structure is analogous to the one defined by Biran-Cornea, but is constructed in the differential forms model. In the spirit of Ganatra and Hugtenburg, we define another such algebra structure using a closed-open map. We show that the two structures coincide. As an application, we show that the module structure for the 2-dimensional Clifford torus is given by multiplication by a Novikov coefficient, similarly to the Biran-Cornea module structure for this case.","sentences":["We define a structure of an algebra on the Lagrangian Floer cohomology of a Lagrangian submanifold over the quantum cohomology of the ambient symplectic manifold.","The structure is analogous to the one defined by Biran-Cornea, but is constructed in the differential forms model.","In the spirit of Ganatra and Hugtenburg, we define another such algebra structure using a closed-open map.","We show that the two structures coincide.","As an application, we show that the module structure for the 2-dimensional Clifford torus is given by multiplication by a Novikov coefficient, similarly to the Biran-Cornea module structure for this case."],"url":"http://arxiv.org/abs/2404.02020v1","category":"math.SG"}
{"created":"2024-04-02 14:55:44","title":"Determining the chemical composition of diamagnetic mixed solids via measurements of the magnetic susceptibility","abstract":"Mixed solid compounds are employed in a vast array of applications so an accurate determination of their chemical compositions is of crucial importance. All current characterization methods require specially-treated samples so the availability of a more practical method with similar accuracy should alleviate the quantification process. In this work, we show how the doping concentration $\\delta$ (or isotope concentration) of a mixed solid compound in powdered form, where both parent compounds are diamagnetic, can be obtained from the measurement of the mass magnetization. We exploit the additive nature of the molar magnetic susceptibility $\\chi_{Mol}$ and molar mass to construct two equations with the same two unknowns in the $\\chi_{Mol}$ vs. $\\delta$ space to simultaneously solve $\\chi_{Mol}$ and $\\delta$ of a mixed solid. Eight examples are provided to show the wide applicability of this method: NH$_{4(1-\\delta)}$D$_{4\\delta}$Br (where D = $^2$H), NH$_4$I$_{1-\\delta}$Br$_\\delta$, (NH$_4$H$_2$)$_{1-\\delta}$(ND$_4$D$_2$)$_\\delta$PO$_4$, C$_{48}$H$_{22+6\\delta}$Br$_{6(1-\\delta)}$O$_{32}$Zr$_6$, [creatine]$_{1-\\delta}$[$_D$-glucose]$_\\delta$, [$_L$-glutamic acid]$_{1-\\delta}$[$_L$-leucine]$_\\delta$, [terephthalic acid]$_{1-\\delta}$[trimesic acid]$_\\delta$ and [p-terphenyl]$_{1-\\delta}$[triphenylphosphine]$_\\delta$. Experimental errors of ~1.2% were obtained for $\\delta$ from average sample masses of 16.6 mg in powdered form rendering the presented approach an attractive choice for characterizing the ratios of mixed solids.","sentences":["Mixed solid compounds are employed in a vast array of applications so an accurate determination of their chemical compositions is of crucial importance.","All current characterization methods require specially-treated samples so the availability of a more practical method with similar accuracy should alleviate the quantification process.","In this work, we show how the doping concentration $\\delta$ (or isotope concentration) of a mixed solid compound in powdered form, where both parent compounds are diamagnetic, can be obtained from the measurement of the mass magnetization.","We exploit the additive nature of the molar magnetic susceptibility $\\chi_{Mol}$ and molar mass to construct two equations with the same two unknowns in the $\\chi_{Mol}$ vs. $\\delta$ space to simultaneously solve $\\chi_{Mol}$ and $\\delta$ of a mixed solid.","Eight examples are provided to show the wide applicability of this method: NH$_{4(1-\\delta)}$D$_{4\\delta}$Br (where D = $^2$H), NH$_4$I$_{1-\\delta}$Br$_\\delta$, (NH$_4$H$_2$)$_{1-\\delta}$(ND$_4$D$_2$)$_\\delta$PO$_4$, C$_{48}$H$_{22+6\\delta}$Br$_{6(1-\\delta)}$O$_{32}$Zr$_6$, [creatine]$_{1-\\delta}$[$_D$-glucose]$_\\delta$, [$_L$-glutamic acid]$_{1-\\delta}$[$_L$-leucine]$_\\delta$, [terephthalic acid]$_{1-\\delta}$[trimesic acid]$_\\delta$ and [p-terphenyl]$_{1-\\delta}$[triphenylphosphine]$_\\delta$. Experimental errors of ~1.2% were obtained for $\\delta$ from average sample masses of 16.6 mg in powdered form rendering the presented approach an attractive choice for characterizing the ratios of mixed solids."],"url":"http://arxiv.org/abs/2404.02012v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 14:20:26","title":"Rigorous derivation of an effective model for coupled Stokes advection, reaction and diffusion with freely evolving microstructure","abstract":"We consider the homogenisation of a coupled Stokes flow and advection--reaction--diffusion problem in a perforated domain with an evolving microstructure of size $\\varepsilon$. Reactions at the boundaries of the microscopic interfaces lead to the formation of a solid layer having a variable, a priori unknown thickness. This results in a growth or shrinkage of the solid phase and, thus, the domain evolution is not known a priori but induced by the advection--reaction--diffusion process. The achievements of this work are the existence and uniqueness of a weak microscopic solution and the rigorous derivation of an effective model for $\\varepsilon \\to 0$, based on $\\varepsilon$-uniform a priori estimates. As a result of the limit passage, the processes on the macroscale are described by an advection--reaction--diffusion problem coupled to Darcy's equation with effective coefficients (porosity, diffusivity and permeability) depending on local cell problems. These local problems are formulated on cells, which depend on the macroscopic position and evolve in time. In particular, the evolution of these cells depends on the macroscopic concentration. Thus, the cell problems (respectively the effective coefficients) are coupled to the macroscopic unknowns and vice versa, leading to a strongly coupled micro--macro model. For pure reactive--diffusive transport coupled with microscopic domain evolution but without advective transport, homogenisation results have recently been presented. We extend these models by advective transport which is driven by the Stokes equation in the a priori unknown evolving pore domain.","sentences":["We consider the homogenisation of a coupled Stokes flow and advection--reaction--diffusion problem in a perforated domain with an evolving microstructure of size $\\varepsilon$. Reactions at the boundaries of the microscopic interfaces lead to the formation of a solid layer having a variable, a priori unknown thickness.","This results in a growth or shrinkage of the solid phase and, thus, the domain evolution is not known a priori but induced by the advection--reaction--diffusion process.","The achievements of this work are the existence and uniqueness of a weak microscopic solution and the rigorous derivation of an effective model for $\\varepsilon \\to 0$, based on $\\varepsilon$-uniform a priori estimates.","As a result of the limit passage, the processes on the macroscale are described by an advection--reaction--diffusion problem coupled to Darcy's equation with effective coefficients (porosity, diffusivity and permeability) depending on local cell problems.","These local problems are formulated on cells, which depend on the macroscopic position and evolve in time.","In particular, the evolution of these cells depends on the macroscopic concentration.","Thus, the cell problems (respectively the effective coefficients) are coupled to the macroscopic unknowns and vice versa, leading to a strongly coupled micro--macro model.","For pure reactive--diffusive transport coupled with microscopic domain evolution but without advective transport, homogenisation results have recently been presented.","We extend these models by advective transport which is driven by the Stokes equation in the a priori unknown evolving pore domain."],"url":"http://arxiv.org/abs/2404.01983v1","category":"math.AP"}
{"created":"2024-04-02 14:11:55","title":"Analytical photoresponses of gated nanowire photoconductors","abstract":"Low-dimensional photoconductors have extraordinarily high photoresponse and gain, which can be modulated by gate voltages as shown in literature. However, the physics of gate modulation remains elusive. In this work, we investigated the physics of gate modulation in silicon nanowire photoconductors with the analytical photoresponse equations. It was found that the impact of gate voltage varies vastly for nanowires with different size. For the wide nanowires that cannot be pinched off by high gate voltage, we found that the photoresponses are enhanced by at least one order of magnitude due to the gate-induced electric passivation. For narrow nanowires that starts with a pinched-off channel, the gate voltage has no electric passivation effect but increases the potential barrier between source and drain, resulting in a decrease in dark and photo current. For the nanowires with an intermediate size, the channel is continuous but can be pinched off by a high gate voltage. The photoresponsivity and photodetectivity is maximized during the transition from the continuous channel to the pinched-off one. This work provides important insights on how to design high-performance photoconductors.","sentences":["Low-dimensional photoconductors have extraordinarily high photoresponse and gain, which can be modulated by gate voltages as shown in literature.","However, the physics of gate modulation remains elusive.","In this work, we investigated the physics of gate modulation in silicon nanowire photoconductors with the analytical photoresponse equations.","It was found that the impact of gate voltage varies vastly for nanowires with different size.","For the wide nanowires that cannot be pinched off by high gate voltage, we found that the photoresponses are enhanced by at least one order of magnitude due to the gate-induced electric passivation.","For narrow nanowires that starts with a pinched-off channel, the gate voltage has no electric passivation effect but increases the potential barrier between source and drain, resulting in a decrease in dark and photo current.","For the nanowires with an intermediate size, the channel is continuous but can be pinched off by a high gate voltage.","The photoresponsivity and photodetectivity is maximized during the transition from the continuous channel to the pinched-off one.","This work provides important insights on how to design high-performance photoconductors."],"url":"http://arxiv.org/abs/2404.01969v1","category":"physics.app-ph"}
{"created":"2024-04-02 13:56:02","title":"Triharmonic curves in the 3-dimensional Sol space","abstract":"The main aim of this paper is to study triharmonic curves in the 3-dimensional homogeneous space Sol. In the first part of the paper we shall obtain a complete classification of proper triharmonic curves with constant geodesic curvature and torsion. In the final section we shall show that these triharmonic curves form a constant angle with a suitable Killing field of constant length along the curve.","sentences":["The main aim of this paper is to study triharmonic curves in the 3-dimensional homogeneous space Sol.","In the first part of the paper we shall obtain a complete classification of proper triharmonic curves with constant geodesic curvature and torsion.","In the final section we shall show that these triharmonic curves form a constant angle with a suitable Killing field of constant length along the curve."],"url":"http://arxiv.org/abs/2404.01963v1","category":"math.DG"}
{"created":"2024-04-02 12:29:31","title":"Minimize Quantization Output Error with Bias Compensation","abstract":"Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment. In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning. Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation. We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning. We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models. Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation.","sentences":["Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment.","In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning.","Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation.","We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning.","We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models.","Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation."],"url":"http://arxiv.org/abs/2404.01892v1","category":"cs.CV"}
{"created":"2024-04-02 11:44:37","title":"Supervised Autoencoder MLP for Financial Time Series Forecasting","abstract":"This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance. It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios. The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022. Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness. However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning. This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling. The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors.","sentences":["This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance.","It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios.","The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022.","Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness.","However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning.","This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling.","The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors."],"url":"http://arxiv.org/abs/2404.01866v1","category":"q-fin.TR"}
{"created":"2024-04-02 11:05:10","title":"Unmasking the Nuances of Loneliness: Using Digital Biomarkers to Understand Social and Emotional Loneliness in College Students","abstract":"Background: Loneliness among students is increasing across the world, with potential consequences for mental health and academic success. To address this growing problem, accurate methods of detection are needed to identify loneliness and to differentiate social and emotional loneliness so that intervention can be personalized to individual need. Passive sensing technology provides a unique technique to capture behavioral patterns linked with distinct loneliness forms, allowing for more nuanced understanding and interventions for loneliness.   Methods: To differentiate between social and emotional loneliness using digital biomarkers, our study included statistical tests, machine learning for predictive modeling, and SHAP values for feature importance analysis, revealing important factors in loneliness classification.   Results: Our analysis revealed significant behavioral differences between socially and emotionally lonely groups, particularly in terms of phone usage and location-based features , with machine learning models demonstrating substantial predictive power in classifying loneliness levels. The XGBoost model, in particular, showed high accuracy and was effective in identifying key digital biomarkers, including phone usage duration and location-based features, as significant predictors of loneliness categories.   Conclusion: This study underscores the potential of passive sensing data, combined with machine learning techniques, to provide insights into the behavioral manifestations of social and emotional loneliness among students. The identification of key digital biomarkers paves the way for targeted interventions aimed at mitigating loneliness in this population.","sentences":["Background: Loneliness among students is increasing across the world, with potential consequences for mental health and academic success.","To address this growing problem, accurate methods of detection are needed to identify loneliness and to differentiate social and emotional loneliness so that intervention can be personalized to individual need.","Passive sensing technology provides a unique technique to capture behavioral patterns linked with distinct loneliness forms, allowing for more nuanced understanding and interventions for loneliness.   ","Methods: To differentiate between social and emotional loneliness using digital biomarkers, our study included statistical tests, machine learning for predictive modeling, and SHAP values for feature importance analysis, revealing important factors in loneliness classification.   ","Results:","Our analysis revealed significant behavioral differences between socially and emotionally lonely groups, particularly in terms of phone usage and location-based features , with machine learning models demonstrating substantial predictive power in classifying loneliness levels.","The XGBoost model, in particular, showed high accuracy and was effective in identifying key digital biomarkers, including phone usage duration and location-based features, as significant predictors of loneliness categories.   ","Conclusion: This study underscores the potential of passive sensing data, combined with machine learning techniques, to provide insights into the behavioral manifestations of social and emotional loneliness among students.","The identification of key digital biomarkers paves the way for targeted interventions aimed at mitigating loneliness in this population."],"url":"http://arxiv.org/abs/2404.01845v1","category":"cs.HC"}
{"created":"2024-04-02 10:47:20","title":"Artin formalism for $p$-adic $L$-functions of modular forms at non-ordinary primes","abstract":"Let $p$ be an odd prime number. Let $f$ be a normalized Hecke eigen-cuspform that is non-ordinary at $p$. Let $K$ be an imaginary quadratic field in which $p$ splits. We study the Artin formalism for the two-variable signed $p$-adic $L$-functions attached to $f$ over $K$. In particular, we give evidence of a prediction made by Castella--Ciperiani--Skinner--Sprung.","sentences":["Let $p$ be an odd prime number.","Let $f$ be a normalized Hecke eigen-cuspform that is non-ordinary at $p$. Let $K$ be an imaginary quadratic field in which $p$ splits.","We study the Artin formalism for the two-variable signed $p$-adic $L$-functions attached to $f$ over $K$. In particular, we give evidence of a prediction made by Castella--Ciperiani--Skinner--Sprung."],"url":"http://arxiv.org/abs/2404.01835v1","category":"math.NT"}
{"created":"2024-04-02 10:42:45","title":"Sub-Riemannian optimal synthesis for Carnot groups with the structure of a path geometry","abstract":"This paper explicitly constructs the complete set of optimal sub-Riemannian geodesics starting from a point for certain Carnot groups of step two. These are groups of dimension 2n+1 equipped with a left-invariant distribution of dimension n+1 such that at each point, there is a unique direction defining a nontrivial Lie bracket. A suitable explicit expression of geodesics, together with symmetries of the structure, allows us to identify the cut time and the cut locus by applying the so-called extended Hadamard technique.","sentences":["This paper explicitly constructs the complete set of optimal sub-Riemannian geodesics starting from a point for certain Carnot groups of step two.","These are groups of dimension 2n+1 equipped with a left-invariant distribution of dimension n+1 such that at each point, there is a unique direction defining a nontrivial Lie bracket.","A suitable explicit expression of geodesics, together with symmetries of the structure, allows us to identify the cut time and the cut locus by applying the so-called extended Hadamard technique."],"url":"http://arxiv.org/abs/2404.01831v1","category":"math.DG"}
{"created":"2024-04-02 10:20:12","title":"Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration","abstract":"The NeuroEvolution of Augmenting Topologies (NEAT) algorithm has received considerable recognition in the field of neuroevolution. Its effectiveness is derived from initiating with simple networks and incrementally evolving both their topologies and weights. Although its capability across various challenges is evident, the algorithm's computational efficiency remains an impediment, limiting its scalability potential. In response, this paper introduces a tensorization method for the NEAT algorithm, enabling the transformation of its diverse network topologies and associated operations into uniformly shaped tensors for computation. This advancement facilitates the execution of the NEAT algorithm in a parallelized manner across the entire population. Furthermore, we develop TensorNEAT, a library that implements the tensorized NEAT algorithm and its variants, such as CPPN and HyperNEAT. Building upon JAX, TensorNEAT promotes efficient parallel computations via automated function vectorization and hardware acceleration. Moreover, the TensorNEAT library supports various benchmark environments including Gym, Brax, and gymnax. Through evaluations across a spectrum of robotics control environments in Brax, TensorNEAT achieves up to 500x speedups compared to the existing implementations such as NEAT-Python. Source codes are available at: https://github.com/EMI-Group/tensorneat.","sentences":["The NeuroEvolution of Augmenting Topologies (NEAT) algorithm has received considerable recognition in the field of neuroevolution.","Its effectiveness is derived from initiating with simple networks and incrementally evolving both their topologies and weights.","Although its capability across various challenges is evident, the algorithm's computational efficiency remains an impediment, limiting its scalability potential.","In response, this paper introduces a tensorization method for the NEAT algorithm, enabling the transformation of its diverse network topologies and associated operations into uniformly shaped tensors for computation.","This advancement facilitates the execution of the NEAT algorithm in a parallelized manner across the entire population.","Furthermore, we develop TensorNEAT, a library that implements the tensorized NEAT algorithm and its variants, such as CPPN and HyperNEAT.","Building upon JAX, TensorNEAT promotes efficient parallel computations via automated function vectorization and hardware acceleration.","Moreover, the TensorNEAT library supports various benchmark environments including Gym, Brax, and gymnax.","Through evaluations across a spectrum of robotics control environments in Brax, TensorNEAT achieves up to 500x speedups compared to the existing implementations such as NEAT-Python.","Source codes are available at: https://github.com/EMI-Group/tensorneat."],"url":"http://arxiv.org/abs/2404.01817v1","category":"cs.NE"}
{"created":"2024-04-02 09:56:48","title":"Excitons, Optical Spectra, and Electronic Properties of Semiconducting Hf-based MXenes","abstract":"Semiconducting MXenes are an intriguing two-dimensional (2D) material class with promising electronic and optoelectronic properties. Here, we focused on recently prepared Hf-based MXenes, namely Hf$_3$C$_2$O$_2$ and Hf$_2$CO$_2$. Using the first-principles calculation and excited state corrections, we proved its dynamical stability, reconciled its semiconducting behavior, and obtained fundamental gaps by the many-body GW method (indirect 1.1 eV and 2.2 eV, respectively, direct 1.4 eV and 3.5 eV, respectively). Using the Bethe-Salpeter equation (BSE) we subsequently provided optical gaps (0.9 eV and 2.7eV, respectively), exciton binding energies, absorption spectra, and other properties of excitons in both Hf-based MXenes. The indirect character of both 2D materials further allowed a significant decrease of excitation energies by considering indirect excitons with exciton momentum along the $\\Gamma$-M path in the Brillouin zone. The first bright excitons are strongly delocalized in real space while contributed by only a limited number of electron-hole pairs around the M point in the k-space from the valence and conduction band. A diverse range of excitonic states in Hf$_3$C$_2$O$_2$ MXene lead to a 4\\% and 13\\% absorptance for the first and second peaks in the infrared region of absorption spectra, respectively. In contrast, a prominent 28\\% absorptance peak in the visible region appears in Hf$_2$CO$_2$ MXene. Results from radiative lifetime calculations indicate the promising potential of these materials in optoelectric devices requiring sustained and efficient exciton behavior.","sentences":["Semiconducting MXenes are an intriguing two-dimensional (2D) material class with promising electronic and optoelectronic properties.","Here, we focused on recently prepared Hf-based MXenes, namely Hf$_3$C$_2$O$_2$ and Hf$_2$CO$_2$. Using the first-principles calculation and excited state corrections, we proved its dynamical stability, reconciled its semiconducting behavior, and obtained fundamental gaps by the many-body GW method (indirect 1.1 eV and 2.2 eV, respectively, direct 1.4 eV and 3.5 eV, respectively).","Using the Bethe-Salpeter equation (BSE) we subsequently provided optical gaps (0.9 eV and 2.7eV, respectively), exciton binding energies, absorption spectra, and other properties of excitons in both Hf-based MXenes.","The indirect character of both 2D materials further allowed a significant decrease of excitation energies by considering indirect excitons with exciton momentum along the $\\Gamma$-M path in the Brillouin zone.","The first bright excitons are strongly delocalized in real space while contributed by only a limited number of electron-hole pairs around the M point in the k-space from the valence and conduction band.","A diverse range of excitonic states in Hf$_3$C$_2$O$_2$ MXene lead to a 4\\% and 13\\% absorptance for the first and second peaks in the infrared region of absorption spectra, respectively.","In contrast, a prominent 28\\% absorptance peak in the visible region appears in Hf$_2$CO$_2$ MXene.","Results from radiative lifetime calculations indicate the promising potential of these materials in optoelectric devices requiring sustained and efficient exciton behavior."],"url":"http://arxiv.org/abs/2404.01797v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 09:33:59","title":"A class of globally analytic hypoelliptic operators on compact Lie groups","abstract":"We obtain global analytic hypoellipticity for a class of differential operators that can be expressed as a zero-order perturbation of a sum of squares of vector fields with real-analytic coefficients on compact Lie groups. The key conditions are: the vector fields must satisfy H\\\"ormander's finite type condition; there exists a closed subgroup whose action leaves the vector fields invariant; and the operator must be elliptic in directions transversal to the action of the subgroup. This paves the way for further studies on the regularity of sums of squares on principal fiber bundles.","sentences":["We obtain global analytic hypoellipticity for a class of differential operators that can be expressed as a zero-order perturbation of a sum of squares of vector fields with real-analytic coefficients on compact Lie groups.","The key conditions are: the vector fields must satisfy H\\\"ormander's finite type condition; there exists a closed subgroup whose action leaves the vector fields invariant; and the operator must be elliptic in directions transversal to the action of the subgroup.","This paves the way for further studies on the regularity of sums of squares on principal fiber bundles."],"url":"http://arxiv.org/abs/2404.01772v1","category":"math.AP"}
{"created":"2024-04-02 09:31:11","title":"Coefficient control of the regularized p-Stokes equations","abstract":"The Antarctic and Greenland ice sheet simulation is challenging due to unknown parameters in the p-Stokes equations. In this work, we prove the existence of a solution to a parameter identification for the ice rheology and the friction coefficient. Additionally, we verify G\\^ateaux differentiability of the control-to-state operator by extending a similar result for distributed control. Moreover, we have more complicated boundary conditions. We only have to add a small diffusion term and assume the nonlinear exponent, which is given in applications, to be small enough to obtain the results. Finally, we state the adjoint equation and prove existence and uniqueness of a solution for this equation.","sentences":["The Antarctic and Greenland ice sheet simulation is challenging due to unknown parameters in the p-Stokes equations.","In this work, we prove the existence of a solution to a parameter identification for the ice rheology and the friction coefficient.","Additionally, we verify G\\^ateaux differentiability of the control-to-state operator by extending a similar result for distributed control.","Moreover, we have more complicated boundary conditions.","We only have to add a small diffusion term and assume the nonlinear exponent, which is given in applications, to be small enough to obtain the results.","Finally, we state the adjoint equation and prove existence and uniqueness of a solution for this equation."],"url":"http://arxiv.org/abs/2404.01766v1","category":"math.OC"}
{"created":"2024-04-02 09:22:18","title":"Maximum principles and moving planes method for the fractional $p(x,\\cdot)$-Laplacian","abstract":"In this paper, we investigate the monotonicity of solutions for a nonlinear equations involving the fractional Laplacian with variable exponent. We first prove different maximum principles involving this operator. Then we employ the direct moving planes method to obtain monotonicity of solutions to a nonlinear equations in which the fractional laplacian with variable exponent is present. Note that, there are no results studying the monotonicity of solutions for local or nonlocal equations with variables exponent. Our results are new in this setting and includes a self-contained techniques.","sentences":["In this paper, we investigate the monotonicity of solutions for a nonlinear equations involving the fractional Laplacian with variable exponent.","We first prove different maximum principles involving this operator.","Then we employ the direct moving planes method to obtain monotonicity of solutions to a nonlinear equations in which the fractional laplacian with variable exponent is present.","Note that, there are no results studying the monotonicity of solutions for local or nonlocal equations with variables exponent.","Our results are new in this setting and includes a self-contained techniques."],"url":"http://arxiv.org/abs/2404.01759v1","category":"math.AP"}
{"created":"2024-04-02 09:12:21","title":"Soliton Amplification in the Korteweg-de Vries Equation by Multiplicative Forcing","abstract":"We study the stability and dynamics of solitons in the Korteweg de-Vries (KdV) equation with small multiplicative forcing. Forcing breaks the conservative structure of the KdV equation, leading to substantial changes in energy over long time. We show that, for small forcing, the inserted energy is almost fully absorbed by the soliton, resulting in a drastically changed amplitude and velocity. We decompose the solution to the forced equation into a modulated soliton and an infinite dimensional perturbation. Assuming slow exponential decay of the forcing, we show that the perturbation decays at the same exponential rate in a weighted Sobolev norm centered around the soliton.","sentences":["We study the stability and dynamics of solitons in the Korteweg de-Vries (KdV) equation with small multiplicative forcing.","Forcing breaks the conservative structure of the KdV equation, leading to substantial changes in energy over long time.","We show that, for small forcing, the inserted energy is almost fully absorbed by the soliton, resulting in a drastically changed amplitude and velocity.","We decompose the solution to the forced equation into a modulated soliton and an infinite dimensional perturbation.","Assuming slow exponential decay of the forcing, we show that the perturbation decays at the same exponential rate in a weighted Sobolev norm centered around the soliton."],"url":"http://arxiv.org/abs/2404.01755v1","category":"math.AP"}
{"created":"2024-04-02 08:17:39","title":"Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation","abstract":"The segmentation of organs in volumetric medical images plays an important role in computer-aided diagnosis and treatment/surgery planning. Conventional 2D convolutional neural networks (CNNs) can hardly exploit the spatial correlation of volumetric data. Current 3D CNNs have the advantage to extract more powerful volumetric representations but they usually suffer from occupying excessive memory and computation nevertheless. In this study we aim to enhance the 2D networks with contextual information for better volumetric image segmentation. Accordingly, we propose a contextual embedding learning approach to facilitate 2D CNNs capturing spatial information properly. Our approach leverages the learned embedding and the slice-wisely neighboring matching as a soft cue to guide the network. In such a way, the contextual information can be transferred slice-by-slice thus boosting the volumetric representation of the network. Experiments on challenging prostate MRI dataset (PROMISE12) and abdominal CT dataset (CHAOS) show that our contextual embedding learning can effectively leverage the inter-slice context and improve segmentation performance. The proposed approach is a plug-and-play, and memory-efficient solution to enhance the 2D networks for volumetric segmentation. The code will be publicly available.","sentences":["The segmentation of organs in volumetric medical images plays an important role in computer-aided diagnosis and treatment/surgery planning.","Conventional 2D convolutional neural networks (CNNs) can hardly exploit the spatial correlation of volumetric data.","Current 3D CNNs have the advantage to extract more powerful volumetric representations but they usually suffer from occupying excessive memory and computation nevertheless.","In this study we aim to enhance the 2D networks with contextual information for better volumetric image segmentation.","Accordingly, we propose a contextual embedding learning approach to facilitate 2D CNNs capturing spatial information properly.","Our approach leverages the learned embedding and the slice-wisely neighboring matching as a soft cue to guide the network.","In such a way, the contextual information can be transferred slice-by-slice thus boosting the volumetric representation of the network.","Experiments on challenging prostate MRI dataset (PROMISE12) and abdominal CT dataset (CHAOS) show that our contextual embedding learning can effectively leverage the inter-slice context and improve segmentation performance.","The proposed approach is a plug-and-play, and memory-efficient solution to enhance the 2D networks for volumetric segmentation.","The code will be publicly available."],"url":"http://arxiv.org/abs/2404.01723v1","category":"eess.IV"}
{"created":"2024-04-02 07:38:16","title":"Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model","abstract":"High-resolution remotely sensed images poses a challenge for commonly used semantic segmentation methods such as Convolutional Neural Network (CNN) and Vision Transformer (ViT). CNN-based methods struggle with handling such high-resolution images due to their limited receptive field, while ViT faces challenges to handle long sequences. Inspired by Mamba, which adopts a State Space Model (SSM) to efficiently capture global semantic information, we propose a semantic segmentation framework for high-resolution remotely sensed images, named Samba. Samba utilizes an encoder-decoder architecture, with Samba blocks serving as the encoder for efficient multi-level semantic information extraction, and UperNet functioning as the decoder. We evaluate Samba on the LoveDA dataset, comparing its performance against top-performing CNN and ViT methods. The results reveal that Samba achieved unparalleled performance on LoveDA. This represents that the proposed Samba is an effective application of the SSM in semantic segmentation of remotely sensed images, setting a new benchmark in performance for Mamba-based techniques in this specific application. The source code and baseline implementations are available at https://github.com/zhuqinfeng1999/Samba.","sentences":["High-resolution remotely sensed images poses a challenge for commonly used semantic segmentation methods such as Convolutional Neural Network (CNN) and Vision Transformer (ViT).","CNN-based methods struggle with handling such high-resolution images due to their limited receptive field, while ViT faces challenges to handle long sequences.","Inspired by Mamba, which adopts a State Space Model (SSM) to efficiently capture global semantic information, we propose a semantic segmentation framework for high-resolution remotely sensed images, named Samba.","Samba utilizes an encoder-decoder architecture, with Samba blocks serving as the encoder for efficient multi-level semantic information extraction, and UperNet functioning as the decoder.","We evaluate Samba on the LoveDA dataset, comparing its performance against top-performing CNN and ViT methods.","The results reveal that Samba achieved unparalleled performance on LoveDA.","This represents that the proposed Samba is an effective application of the SSM in semantic segmentation of remotely sensed images, setting a new benchmark in performance for Mamba-based techniques in this specific application.","The source code and baseline implementations are available at https://github.com/zhuqinfeng1999/Samba."],"url":"http://arxiv.org/abs/2404.01705v1","category":"cs.CV"}
{"created":"2024-04-02 06:58:41","title":"Preventing Model Collapse in Gaussian Process Latent Variable Models","abstract":"Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hyperparameters, projection variance, and latent representations within the variational inference framework. The proposed GPLVM, named advisedRFLVM, is evaluated across diverse datasets and consistently outperforms various salient competing models, including state-of-the-art variational autoencoders (VAEs) and GPLVM variants, in terms of informative latent representations and missing data imputation.","sentences":["Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction.","However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data.","This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM.","Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hyperparameters, projection variance, and latent representations within the variational inference framework.","The proposed GPLVM, named advisedRFLVM, is evaluated across diverse datasets and consistently outperforms various salient competing models, including state-of-the-art variational autoencoders (VAEs) and GPLVM variants, in terms of informative latent representations and missing data imputation."],"url":"http://arxiv.org/abs/2404.01697v1","category":"stat.ML"}
{"created":"2024-04-02 06:52:28","title":"Machine Learning-Based Identification of Contaminated Images in Light Curves Data Preprocessing","abstract":"Attitude is one of the crucial parameters for space objects and plays a vital role in collision prediction and debris removal. Analyzing light curves to determine attitude is the most commonly used method. In photometric observations, outliers may exist in the obtained light curves due to various reasons. Therefore, preprocessing is required to remove these outliers to obtain high quality light curves. Through statistical analysis, the reasons leading to outliers can be categorized into two main types: first, the brightness of the object significantly increases due to the passage of a star nearby, referred to as \"stellar contamination,\" and second, the brightness markedly decreases due to cloudy cover, referred to as \"cloudy contamination.\" Traditional approach of manually inspecting images for contamination is time-consuming and labor-intensive. However, We propose the utilization of machine learning methods as a substitute. Convolutional Neural Networks (CNN) and Support Vector Machines (SVM) are employed to identify cases of stellar contamination and cloudy contamination, achieving F1 scores of 1.00 and 0.98 on test set, respectively. We also explored other machine learning methods such as Residual Network-18 (ResNet-18) and Light Gradient Boosting Machine (lightGBM), then conducted comparative analyses of the results.","sentences":["Attitude is one of the crucial parameters for space objects and plays a vital role in collision prediction and debris removal.","Analyzing light curves to determine attitude is the most commonly used method.","In photometric observations, outliers may exist in the obtained light curves due to various reasons.","Therefore, preprocessing is required to remove these outliers to obtain high quality light curves.","Through statistical analysis, the reasons leading to outliers can be categorized into two main types: first, the brightness of the object significantly increases due to the passage of a star nearby, referred to as \"stellar contamination,\" and second, the brightness markedly decreases due to cloudy cover, referred to as \"cloudy contamination.\"","Traditional approach of manually inspecting images for contamination is time-consuming and labor-intensive.","However, We propose the utilization of machine learning methods as a substitute.","Convolutional Neural Networks (CNN) and Support Vector Machines (SVM) are employed to identify cases of stellar contamination and cloudy contamination, achieving F1 scores of 1.00 and 0.98 on test set, respectively.","We also explored other machine learning methods such as Residual Network-18 (ResNet-18) and Light Gradient Boosting Machine (lightGBM), then conducted comparative analyses of the results."],"url":"http://arxiv.org/abs/2404.01691v1","category":"astro-ph.IM"}
{"created":"2024-04-02 06:28:52","title":"Motivic interpretations for iterated integrals on some specific algebraic curves","abstract":"Multiple zeta values (MZVs for short) can be represented as iterated integrals of $\\mathbb{Q}$-rational algebraic differential forms on $\\mathbb{P}^1(\\mathbb{C})\\setminus\\{0, 1, \\infty\\}$. This interpretation allows us to consider MZVs geometrically, and this is one of the motivations for Deligne--Goncharov, Terasoma et al. to give motivic interpretations of MZVs by using the theory of mixed Tate motives and the motivic fundamental groups.   In this paper, we consider the iterated integrals on some rational curves over $\\mathbb{Q}$ and study their arithmetic properties. They are an extension of MZVs and also include some other known special values such as multiple $\\widetilde{T}$-values. Furthermore, we give motivic interpretations of them by investigating a relationship with motivic iterated integrals given by Goncharov. At this point, it is important to consider the base expansion and the Galois invariant part of the space of motivic iterated integrals. Finally, we denote that a motivic interpretation of the alternating multiple mixed values can be given by the same method. Our results also extend a part of author's previous work.","sentences":["Multiple zeta values (MZVs for short) can be represented as iterated integrals of $\\mathbb{Q}$-rational algebraic differential forms on $\\mathbb{P}^1(\\mathbb{C})\\setminus\\{0, 1, \\infty\\}$. This interpretation allows us to consider MZVs geometrically, and this is one of the motivations for Deligne--Goncharov, Terasoma et al. to give motivic interpretations of MZVs by using the theory of mixed Tate motives and the motivic fundamental groups.   ","In this paper, we consider the iterated integrals on some rational curves over $\\mathbb{Q}$ and study their arithmetic properties.","They are an extension of MZVs and also include some other known special values such as multiple $\\widetilde{T}$-values.","Furthermore, we give motivic interpretations of them by investigating a relationship with motivic iterated integrals given by Goncharov.","At this point, it is important to consider the base expansion and the Galois invariant part of the space of motivic iterated integrals.","Finally, we denote that a motivic interpretation of the alternating multiple mixed values can be given by the same method.","Our results also extend a part of author's previous work."],"url":"http://arxiv.org/abs/2404.01678v1","category":"math.NT"}
{"created":"2024-04-02 06:28:22","title":"Incentives in Private Collaborative Machine Learning","abstract":"Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.","sentences":["Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation.","Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved.","To address this, we introduce differential privacy (DP) as an incentive.","Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly.","The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters.","As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model.","Finally, the mediator rewards each party with different posterior samples of the model parameters.","Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior.","We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2404.01676v1","category":"cs.LG"}
{"created":"2024-04-02 06:25:16","title":"PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching","abstract":"Mapping is one of the crucial tasks enabling autonomous navigation of a mobile robot. Conventional mapping methods output dense geometric map representation, e.g. an occupancy grid, which is not trivial to keep consistent for the prolonged runs covering large environments. Meanwhile, capturing the topological structure of the workspace enables fast path planning, is less prone to odometry error accumulation and does not consume much memory. Following this idea, this paper introduces PRISM-TopoMap -- a topological mapping method that maintains a graph of locally aligned locations not relying on global metric coordinates. The proposed method involves learnable multimodal place recognition paired with the scan matching pipeline for localization and loop closure in the graph of locations. The latter is updated online and the robot is localized in a proper node at each time step. We conduct a broad experimental evaluation of the suggested approach in a range of photo-realistic environments and on a real robot (wheeled differential driven Husky robot), and compare it to state of the art. The results of the empirical evaluation confirm that PRISM-Topomap consistently outperforms competitors across several measures of mapping and navigation efficiency and performs well on a real robot. The code of PRISM-Topomap is open-sourced and available at https://github.com/kirillMouraviev/prism-topomap.","sentences":["Mapping is one of the crucial tasks enabling autonomous navigation of a mobile robot.","Conventional mapping methods output dense geometric map representation, e.g. an occupancy grid, which is not trivial to keep consistent for the prolonged runs covering large environments.","Meanwhile, capturing the topological structure of the workspace enables fast path planning, is less prone to odometry error accumulation and does not consume much memory.","Following this idea, this paper introduces PRISM-TopoMap -- a topological mapping method that maintains a graph of locally aligned locations not relying on global metric coordinates.","The proposed method involves learnable multimodal place recognition paired with the scan matching pipeline for localization and loop closure in the graph of locations.","The latter is updated online and the robot is localized in a proper node at each time step.","We conduct a broad experimental evaluation of the suggested approach in a range of photo-realistic environments and on a real robot (wheeled differential driven Husky robot), and compare it to state of the art.","The results of the empirical evaluation confirm that PRISM-Topomap consistently outperforms competitors across several measures of mapping and navigation efficiency and performs well on a real robot.","The code of PRISM-Topomap is open-sourced and available at https://github.com/kirillMouraviev/prism-topomap."],"url":"http://arxiv.org/abs/2404.01674v1","category":"cs.RO"}
{"created":"2024-04-02 06:07:12","title":"On open manifolds admitting no complete metric with positive scalar curvature","abstract":"In this paper, we investigate the topological obstruction problem for positive scalar curvature and uniformly positive scalar curvature on open manifolds. We present a definition for open Schoen-Yau-Schick manifolds and prove that there is no complete metric with positive scalar curvature on these manifolds. Similarly, we define weak Schoen-Yau-Shick manifolds by analogy, which are expected to admit no complete metrics with uniformly positive scalar curvature.","sentences":["In this paper, we investigate the topological obstruction problem for positive scalar curvature and uniformly positive scalar curvature on open manifolds.","We present a definition for open Schoen-Yau-Schick manifolds and prove that there is no complete metric with positive scalar curvature on these manifolds.","Similarly, we define weak Schoen-Yau-Shick manifolds by analogy, which are expected to admit no complete metrics with uniformly positive scalar curvature."],"url":"http://arxiv.org/abs/2404.01660v1","category":"math.DG"}
{"created":"2024-04-02 04:41:33","title":"Concentration and oscillation analysis of positive solutions to semilinear elliptic equations with exponential growth in a disc","abstract":"We establish a series of concentration and oscillation estimates for elliptic equations with exponential nonlinearity $e^{u^p}$ in a disc. Especially, we show various new results on the supercritical case $p>2$ which are left open in the previous works. We begin with the concentration analysis of blow-up solutions by extending the scaling and pointwise techniques developed in the previous studies. A striking result is that we detect an infinite sequence of bubbles in the supercritical case $p>2$. The precise characterization of the limit profile, energy, and location of each bubble is given. Moreover, we arrive at a natural interpretation, the infinite sequence of bubbles causes the infinite oscillation of the solutions. Based on this idea and our concentration estimates, we next carry out the oscillation analysis. The results allow us to estimate intersection points and numbers between blow-up solutions and singular functions. Applying this, we finally demonstrate the infinite oscillations of the bifurcation diagrams of supercritical equations. In addition, we also discuss what happens on the sequences of bubbles in the limit cases $p\\to 2^+$ and $p\\to \\infty$ respectively. As above, the present work discovers a direct path connecting the concentration and oscillation analyses. It leads to a consistent and straightforward understanding of concentration, oscillation, and bifurcation phenomena on blow-up solutions of supercritical problems.","sentences":["We establish a series of concentration and oscillation estimates for elliptic equations with exponential nonlinearity $e^{u^p}$ in a disc.","Especially, we show various new results on the supercritical case $p>2$ which are left open in the previous works.","We begin with the concentration analysis of blow-up solutions by extending the scaling and pointwise techniques developed in the previous studies.","A striking result is that we detect an infinite sequence of bubbles in the supercritical case $p>2$. The precise characterization of the limit profile, energy, and location of each bubble is given.","Moreover, we arrive at a natural interpretation, the infinite sequence of bubbles causes the infinite oscillation of the solutions.","Based on this idea and our concentration estimates, we next carry out the oscillation analysis.","The results allow us to estimate intersection points and numbers between blow-up solutions and singular functions.","Applying this, we finally demonstrate the infinite oscillations of the bifurcation diagrams of supercritical equations.","In addition, we also discuss what happens on the sequences of bubbles in the limit cases $p\\to 2^+$ and $p\\to \\infty$ respectively.","As above, the present work discovers a direct path connecting the concentration and oscillation analyses.","It leads to a consistent and straightforward understanding of concentration, oscillation, and bifurcation phenomena on blow-up solutions of supercritical problems."],"url":"http://arxiv.org/abs/2404.01634v1","category":"math.AP"}
{"created":"2024-04-02 03:11:43","title":"Fractional mean field equations on finite graphs","abstract":"In this paper, the author considers the fractional mean field equation on a finite graph $G=(V,E)$, say \\begin{equation*} (-\\Delta)^s u=\\rho\\left(\\dfrac{he^u}{\\int_V he^ud\\mu}-\\dfrac{1}{|V|}\\right),\\quad\\forall\\,x\\in V, \\end{equation*} where $s\\in(0,\\,1)$, $\\rho\\in(-\\infty,\\,0)\\cup(0,\\,+\\infty)$ are some fixed parameters, $h$ denotes a given real value function on $V$. Based on the sign of the prescribed function $h$, via the variational method, topological degree and two mean field type heat flows, the author obtains the existence of solutions for the above problem in three cases respectively. These results extend the relevant research of Lin-Yang (Calc. Var., 2021), Sun-Wang (Adv. Math., 2022) and Liu-Zhang (J. Math. Anal. Appl., 2023) in the case of $s=1$.","sentences":["In this paper, the author considers the fractional mean field equation on a finite graph $G=(V,E)$, say \\begin{equation*} (-\\Delta)^s u=\\rho\\left(\\dfrac{he^u}{\\int_V he^ud\\mu}-\\dfrac{1}{|V|}\\right),\\quad\\forall\\,x\\in V, \\end{equation*} where $s\\in(0,\\,1)$, $\\rho\\in(-\\infty,\\,0)\\cup(0,\\,+\\infty)$ are some fixed parameters, $h$ denotes a given real value function on $V$. Based on the sign of the prescribed function $h$, via the variational method, topological degree and two mean field type heat flows, the author obtains the existence of solutions for the above problem in three cases respectively.","These results extend the relevant research of Lin-Yang (Calc.","Var., 2021), Sun-Wang (Adv. Math., 2022) and Liu-Zhang (J. Math.","Anal.","Appl., 2023) in the case of $s=1$."],"url":"http://arxiv.org/abs/2404.01610v1","category":"math.AP"}
{"created":"2024-04-02 02:21:04","title":"Random 3-Manifolds Have No Totally Geodesic Submanifolds","abstract":"Murphy and the second author showed that a generic closed Riemannian manifold has no totally geodesic submanifolds, provided it is at least four dimensional. Lytchak and Petrunin established the same thing in dimension 3. For the higher dimensional result, the generic set is open and dense in the $C^{q}$--topology for any $% q\\geq 2.$ In Lytchak and Petrunin's work, the generic set is a dense $G_{\\delta }$ in the $C^{q}$-topology for any $q\\geq 2.$ Here we show that the set of such metrics on a compact $3$-manifold contains a set that is open and dense in the $C^{q}$-topology for any $q\\geq 3.$","sentences":["Murphy and the second author showed that a generic closed Riemannian manifold has no totally geodesic submanifolds, provided it is at least four dimensional.","Lytchak and Petrunin established the same thing in dimension 3.","For the higher dimensional result, the generic set is open and dense in the $C^{q}$--topology for any $% q\\geq 2.$","In Lytchak and Petrunin's work, the generic set is a dense $G_{\\delta }$ in the $C^{q}$-topology for any $q\\geq 2.$","Here we show that the set of such metrics on a compact $3$-manifold contains a set that is open and dense in the $C^{q}$-topology for any $q\\geq 3.$"],"url":"http://arxiv.org/abs/2404.01581v1","category":"math.DG"}
{"created":"2024-04-02 02:12:00","title":"Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment","abstract":"This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.","sentences":["This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software.","Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data.","Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes.","Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height.","Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods.","Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects.","The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility.","This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks."],"url":"http://arxiv.org/abs/2404.01576v1","category":"cs.CV"}
{"created":"2024-04-02 01:59:16","title":"Accurate determination of thermoelectric figure of merit using ac Harman method with a four-probe configuration","abstract":"The ac Harman method has been used for the direct estimation of dimensionless thermoelectric figure of merit (zT) through ac/dc resistance measurements. However, accurate zT estimation with a four-probe configuration is difficult owing to the occurrence of a thermal phase-delay in the heat flow with a low frequency current. This study reports an exact solution for zT estimation by solving the heat conduction equation. The analysis can explain the reverse heat flow, which is the main source of the error in the four-probe configuration, and the experimentally obtained behavior of the frequency dependence of zT of (Bi,Sb)$_2$Te$_3$. Approximately 20 % of the error is caused by a thermal phase-delay, unless an appropriate current frequency and voltage-terminal position are chosen. Thus, an accurate zT evaluation using a four-probe configuration at any voltage terminal position is achieved. These findings can lead to interesting thermoelectric metrology and could serve as a powerful tool to search for promising thermoelectric materials.","sentences":["The ac Harman method has been used for the direct estimation of dimensionless thermoelectric figure of merit (zT) through ac/dc resistance measurements.","However, accurate zT estimation with a four-probe configuration is difficult owing to the occurrence of a thermal phase-delay in the heat flow with a low frequency current.","This study reports an exact solution for zT estimation by solving the heat conduction equation.","The analysis can explain the reverse heat flow, which is the main source of the error in the four-probe configuration, and the experimentally obtained behavior of the frequency dependence of zT of (Bi,Sb)$_2$Te$_3$. Approximately 20 % of the error is caused by a thermal phase-delay, unless an appropriate current frequency and voltage-terminal position are chosen.","Thus, an accurate zT evaluation using a four-probe configuration at any voltage terminal position is achieved.","These findings can lead to interesting thermoelectric metrology and could serve as a powerful tool to search for promising thermoelectric materials."],"url":"http://arxiv.org/abs/2404.01565v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 01:18:16","title":"Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining","abstract":"How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-fine and fine-to-coarse information communication. Extensive experiments demonstrate that our approach, named as NeRD-Rain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. The source code and trained models are available at https://github.com/cschenxiang/NeRD-Rain.","sentences":["How to effectively explore multi-scale representations of rain streaks is important for image deraining.","In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction.","To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios.","To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-fine and fine-to-coarse information communication.","Extensive experiments demonstrate that our approach, named as NeRD-Rain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets.","The source code and trained models are available at https://github.com/cschenxiang/NeRD-Rain."],"url":"http://arxiv.org/abs/2404.01547v1","category":"cs.CV"}
{"created":"2024-04-02 00:56:16","title":"A new gap in the critical exponent for semi-linear structurally damped evolution equations","abstract":"Our aim in this paper is to discuss the critical exponent in semi-linear structurally damped wave and beam equations with additional dispersion term. The special model we have in mind is $$ u_{tt}(t,x)+(-\\Delta)^{\\sigma}u(t,x)+(-\\Delta)^{2\\delta}u(t,x)+2(-\\Delta)^{\\delta}u_{t}(t,x)=\\left|u(t,x)\\right| ^{p} $$ where the initial displacement $u(0,x)=u_{0}(x)$, the initial velocity $u_{t}(0,x)=u_{1}(x)$ and the parameters $ t\\in [0,\\infty)$, $x\\in \\mathbb{R}^{n}$, $\\sigma\\geq 1$, $\\delta\\in(0,\\frac{\\sigma}{2})$, $p>1$. The solution to the linear equation at low frequency region involves an interplay of diffusion and oscillation phenomena represented by a real-complex Fourier multiplier of the form $$m(t,\\xi)=\\frac{e^{-|\\xi|^{2\\delta}t\\pm i|\\xi|^{\\sigma}t}}{2i|\\xi|^{\\sigma}}, \\ \\ \\xi\\in \\mathbb{R}^{n}, \\ \\ i=\\sqrt{-1}.$$ The scaling argument shows that the diffusive part leads to faster decay rates compared to the oscillatory one. This interplay creates a new gap in the critical exponent between the blow up (in finite time) result when $1<p<1+\\frac{4\\delta}{n-2\\delta}$ (sub-critical case) and the global (in time) existence result when $p>1+\\frac{\\sigma+2\\delta}{n-\\sigma}$ (super-critical case). We leave an open to show if this gap will be closed at least in low or high space dimensions because, to the best of authors knowledge, the necessary Fourier multiplier that leads to the sub-critical case does not explicitly appear in $m(t,\\xi)$.","sentences":["Our aim in this paper is to discuss the critical exponent in semi-linear structurally damped wave and beam equations with additional dispersion term.","The special model we have in mind is $$ u_{tt}(t,x)+(-\\Delta)^{\\sigma}u(t,x)+(-\\Delta)^{2\\delta}u(t,x)+2(-\\Delta)^{\\delta}u_{t}(t,x)=\\left|u(t,x)\\right| ^{p} $$ where the initial displacement $u(0,x)=u_{0}(x)$, the initial velocity $u_{t}(0,x)=u_{1}(x)$ and the parameters $ t\\in [0,\\infty)$, $x\\in \\mathbb{R}^{n}$, $\\sigma\\geq 1$, $\\delta\\in(0,\\frac{\\sigma}{2})$, $p>1$. The solution to the linear equation at low frequency region involves an interplay of diffusion and oscillation phenomena represented by a real-complex Fourier multiplier of the form $$m(t,\\xi)=\\frac{e^{-|\\xi|^{2\\delta}t\\pm i|\\xi|^{\\sigma}t}}{2i|\\xi|^{\\sigma}}, \\ \\ \\xi\\in \\mathbb{R}^{n}, \\ \\ i=\\sqrt{-1}.$$ The scaling argument shows that the diffusive part leads to faster decay rates compared to the oscillatory one.","This interplay creates a new gap in the critical exponent between the blow up (in finite time) result when $1<p<1+\\frac{4\\delta}{n-2\\delta}$ (sub-critical case) and the global (in time) existence result when $p>1+\\frac{\\sigma+2\\delta}{n-\\sigma}$ (super-critical case).","We leave an open to show if this gap will be closed at least in low or high space dimensions because, to the best of authors knowledge, the necessary Fourier multiplier that leads to the sub-critical case does not explicitly appear in $m(t,\\xi)$."],"url":"http://arxiv.org/abs/2404.01544v1","category":"math.AP"}
{"created":"2024-04-02 00:55:50","title":"Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes","abstract":"3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.","sentences":["3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism.","However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing.","While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance.","To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality.","Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model.","These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings.","Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method.","Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions."],"url":"http://arxiv.org/abs/2404.01543v1","category":"cs.CV"}
{"created":"2024-04-02 00:54:38","title":"Predicting the Performance of Foundation Models via Agreement-on-the-Line","abstract":"Estimating the out-of-distribution performance in regimes where labels are scarce is critical to safely deploy foundation models. Recently, it was shown that ensembles of neural networks observe the phenomena ``agreement-on-the-line'', which can be leveraged to reliably predict OOD performance without labels. However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, foundation models undergo minimal finetuning from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line. In our work, we demonstrate that when lightly finetuning multiple runs from a $\\textit{single}$ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble. Surprisingly, only random head initialization is able to reliably induce agreement-on-the-line in finetuned foundation models across vision and language benchmarks. Second, we demonstrate that ensembles of $\\textit{multiple}$ foundation models pretrained on different datasets but finetuned on the same task can also show agreement-on-the-line. In total, by careful construction of a diverse ensemble, we can utilize agreement-on-the-line-based methods to predict the OOD performance of foundation models with high precision.","sentences":["Estimating the out-of-distribution performance in regimes where labels are scarce is critical to safely deploy foundation models.","Recently, it was shown that ensembles of neural networks observe the phenomena ``agreement-on-the-line'', which can be leveraged to reliably predict OOD performance without labels.","However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, foundation models undergo minimal finetuning from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line.","In our work, we demonstrate that when lightly finetuning multiple runs from a $\\textit{single}$ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble.","Surprisingly, only random head initialization is able to reliably induce agreement-on-the-line in finetuned foundation models across vision and language benchmarks.","Second, we demonstrate that ensembles of $\\textit{multiple}$ foundation models pretrained on different datasets but finetuned on the same task can also show agreement-on-the-line.","In total, by careful construction of a diverse ensemble, we can utilize agreement-on-the-line-based methods to predict the OOD performance of foundation models with high precision."],"url":"http://arxiv.org/abs/2404.01542v1","category":"cs.LG"}
{"created":"2024-04-02 00:32:57","title":"Internal null controllability for the one-dimensional heat equation with dynamic boundary conditions","abstract":"The primary focus of this paper is to establish the internal null controllability for the one-dimensional heat equation featuring dynamic boundary conditions. This achievement is realized by introducing a new Carleman estimate and an observability inequality for the corresponding backward system. In conclusion, the paper includes a set of numerical experiments that serve to confirm the validity of the theoretical findings and underscore the effectiveness of the designed control with a minimal L2-norm.","sentences":["The primary focus of this paper is to establish the internal null controllability for the one-dimensional heat equation featuring dynamic boundary conditions.","This achievement is realized by introducing a new Carleman estimate and an observability inequality for the corresponding backward system.","In conclusion, the paper includes a set of numerical experiments that serve to confirm the validity of the theoretical findings and underscore the effectiveness of the designed control with a minimal L2-norm."],"url":"http://arxiv.org/abs/2404.01539v1","category":"math.OC"}
{"created":"2024-04-01 23:55:05","title":"Syntactic Robustness for LLM-based Code Generation","abstract":"Rapid advances in the field of Large Language Models (LLMs) have made LLM-based code generation an important area for investigation. An LLM-based code generator takes a prompt as input and produces code that implements the requirements specified in the prompt. Many software requirements include mathematical formulas that specify the expected behavior of the code to be generated. Given a code generation prompt that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated code for the modified prompt should be semantically equivalent. We formalize this concept as syntactic robustness and investigate the syntactic robustness of GPT-3.5-Turbo and GPT-4 as code generators. To test syntactic robustness, we generate syntactically different but semantically equivalent versions of prompts using a set of mutators that only modify mathematical formulas in prompts. In this paper, we focus on prompts that ask for code that generates solutions to variables in an equation, when given coefficients of the equation as input. Our experimental evaluation demonstrates that GPT-3.5-Turbo and GPT-4 are not syntactically robust for this type of prompts. To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step. Our experimental results indicate that the syntactic robustness of LLM-based code generation can be improved using our approach.","sentences":["Rapid advances in the field of Large Language Models (LLMs) have made LLM-based code generation an important area for investigation.","An LLM-based code generator takes a prompt as input and produces code that implements the requirements specified in the prompt.","Many software requirements include mathematical formulas that specify the expected behavior of the code to be generated.","Given a code generation prompt that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated code for the modified prompt should be semantically equivalent.","We formalize this concept as syntactic robustness and investigate the syntactic robustness of GPT-3.5-Turbo and GPT-4 as code generators.","To test syntactic robustness, we generate syntactically different but semantically equivalent versions of prompts using a set of mutators that only modify mathematical formulas in prompts.","In this paper, we focus on prompts that ask for code that generates solutions to variables in an equation, when given coefficients of the equation as input.","Our experimental evaluation demonstrates that GPT-3.5-Turbo and GPT-4 are not syntactically robust for this type of prompts.","To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step.","Our experimental results indicate that the syntactic robustness of LLM-based code generation can be improved using our approach."],"url":"http://arxiv.org/abs/2404.01535v1","category":"cs.SE"}
{"created":"2024-04-01 23:52:30","title":"CyberShake Earthquake Fault Rupture Modeling and Ground Motion Simulations for the Southwest Iceland Transform Zone","abstract":"CyberShake (CS) is a high-performance computing workflow for Probabilistic Seismic Hazard Assessment (PSHA) developed by the Statewide California Earthquake Center. Here, we employ CS to generate a set of 2103 fault ruptures and simulate the corresponding two horizontal velocity components time histories of ground motion (GM) on a 5-km grid of 625 stations in Southwest Iceland (SI). The ruptures were defined on a new synthetic time-independent 500-year catalog consisting of 223 hypothetical finite-fault sources of 5-7, generated using a new physics-based bookshelf fault system model in the SI transform zone. This fault system model and rupture realizations enable the CS time-independent physics-based approach to PSHA in the region. The study aims to migrate CS to SI and validate its kinematic fault rupture, anelastic wave propagation and ground motion simulations. Toward this goal, we use CS to generate multiple finite-fault rupture variations for each hypothetical fault. CS exploits seismic reciprocity for wave propagation by computing Strain Green Tensors along fault planes, which in turn are convolved with rupture models to generate GM seismograms. For each GM recording station, every adjoint simulation uses a 0-1 Hz Gaussian point source polarized along one horizontal grid direction. Comparison of the results in the form of rotation-invariant synthetic pseudo-acceleration spectral response values at 2, 3 and 5 sec periods are in very good agreement with the Icelandic strong-motion dataset, and a suite of new empirical Bayesian ground motion prediction equations (GMPEs). The vast majority of the CS results fall within one standard deviation of the mean GMPE predictions, previously estimated for the area. Importantly, at large magnitudes for which no data exists in Iceland, the CS dataset may play an important role in constraining the GMPEs for future applications.","sentences":["CyberShake (CS) is a high-performance computing workflow for Probabilistic Seismic Hazard Assessment (PSHA) developed by the Statewide California Earthquake Center.","Here, we employ CS to generate a set of 2103 fault ruptures and simulate the corresponding two horizontal velocity components time histories of ground motion (GM) on a 5-km grid of 625 stations in Southwest Iceland (SI).","The ruptures were defined on a new synthetic time-independent 500-year catalog consisting of 223 hypothetical finite-fault sources of 5-7, generated using a new physics-based bookshelf fault system model in the SI transform zone.","This fault system model and rupture realizations enable the CS time-independent physics-based approach to PSHA in the region.","The study aims to migrate CS to SI and validate its kinematic fault rupture, anelastic wave propagation and ground motion simulations.","Toward this goal, we use CS to generate multiple finite-fault rupture variations for each hypothetical fault.","CS exploits seismic reciprocity for wave propagation by computing Strain Green Tensors along fault planes, which in turn are convolved with rupture models to generate GM seismograms.","For each GM recording station, every adjoint simulation uses a 0-1 Hz Gaussian point source polarized along one horizontal grid direction.","Comparison of the results in the form of rotation-invariant synthetic pseudo-acceleration spectral response values at 2, 3 and 5 sec periods are in very good agreement with the Icelandic strong-motion dataset, and a suite of new empirical Bayesian ground motion prediction equations (GMPEs).","The vast majority of the CS results fall within one standard deviation of the mean GMPE predictions, previously estimated for the area.","Importantly, at large magnitudes for which no data exists in Iceland, the CS dataset may play an important role in constraining the GMPEs for future applications."],"url":"http://arxiv.org/abs/2404.01533v1","category":"physics.geo-ph"}
{"created":"2024-04-01 23:31:22","title":"On universal sets and sumsets","abstract":"We study the concept of universal sets from the additive--combinatorial point of view. Among other results we obtain some applications of this type of uniformity to sets avoiding solutions to linear equations, and get an optimal upper bound for the covering number of general sumsets.","sentences":["We study the concept of universal sets from the additive--combinatorial point of view.","Among other results we obtain some applications of this type of uniformity to sets avoiding solutions to linear equations, and get an optimal upper bound for the covering number of general sumsets."],"url":"http://arxiv.org/abs/2404.01529v1","category":"math.CO"}
{"created":"2024-04-01 23:22:41","title":"Hyperbolic times in Minkowski space","abstract":"Time functions with asymptotically hyperbolic geometry play an increasingly important role in many areas of relativity, from computing black-hole perturbations to analyzing wave equations. Despite their significance, many of their properties remain underexplored. In this expository article, I discuss hyperbolic time functions by considering the hyperbola as the relativistic analog of a circle in two-dimensional Minkowski space and argue that suitably defined hyperboloidal coordinates are as natural in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds. The presence of the null cone adds new elements to this analogy, particularly regarding the smooth foliation of the asymptotic boundary.","sentences":["Time functions with asymptotically hyperbolic geometry play an increasingly important role in many areas of relativity, from computing black-hole perturbations to analyzing wave equations.","Despite their significance, many of their properties remain underexplored.","In this expository article, I discuss hyperbolic time functions by considering the hyperbola as the relativistic analog of a circle in two-dimensional Minkowski space and argue that suitably defined hyperboloidal coordinates are as natural in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds.","The presence of the null cone adds new elements to this analogy, particularly regarding the smooth foliation of the asymptotic boundary."],"url":"http://arxiv.org/abs/2404.01528v1","category":"gr-qc"}
{"created":"2024-04-01 22:19:17","title":"Reduction of Joule Losses in Memristive Switching Using Optimal Control","abstract":"Electricity production from fossil fuels is among the main contributors to global warming. To suppress climate change, energy-efficient systems, devices, and technologies must be implemented. This study investigates strategies for minimizing Joule losses in resistive random access memory (ReRAM) cells, which are also referred to as memristive devices. The basic question that we ask is what is the optimal driving protocol to switch a memristive device from one state to another. In the case of ideal memristors, in the most basic scenario, the optimal protocol is determined by solving a variational problem without constraints with the help of the Euler-Lagrange equation. In the case of memristive systems, for the same situation, the optimal protocol is found using the method of Lagrange multipliers. We demonstrate the advantages of our approaches through specific examples and compare our results with those of switching with constant voltage or current. Our findings suggest that voltage or current control can be used to reduce Joule losses in emerging memory devices.","sentences":["Electricity production from fossil fuels is among the main contributors to global warming.","To suppress climate change, energy-efficient systems, devices, and technologies must be implemented.","This study investigates strategies for minimizing Joule losses in resistive random access memory (ReRAM) cells, which are also referred to as memristive devices.","The basic question that we ask is what is the optimal driving protocol to switch a memristive device from one state to another.","In the case of ideal memristors, in the most basic scenario, the optimal protocol is determined by solving a variational problem without constraints with the help of the Euler-Lagrange equation.","In the case of memristive systems, for the same situation, the optimal protocol is found using the method of Lagrange multipliers.","We demonstrate the advantages of our approaches through specific examples and compare our results with those of switching with constant voltage or current.","Our findings suggest that voltage or current control can be used to reduce Joule losses in emerging memory devices."],"url":"http://arxiv.org/abs/2404.01507v1","category":"cs.ET"}
{"created":"2024-04-01 21:37:02","title":"Exploring the Cauchy-Riemann structure and integrability conditions of $F$-structures satisfying $\u03b1F^{K+1} +\u03b2F^{K} + F=0$","abstract":"This work introduces a new class of $F$-structures satisfying $\\alpha F^{K+1} +\\beta F^{K} + F=0$, where $K$ is a positive integer, $K\\geq3$, and $\\alpha, \\beta$ are real or complex numbers. We investigate the Cauchy-Riemann structure and its link with the $F$-structure. We also address the integrability of this structure, including partial and complete integrability. Finally, we present several examples of the $F$-structure.","sentences":["This work introduces a new class of $F$-structures satisfying $\\alpha F^{K+1} +\\beta F^{K} + F=0$, where $K$ is a positive integer, $K\\geq3$, and $\\alpha, \\beta$ are real or complex numbers.","We investigate the Cauchy-Riemann structure and its link with the $F$-structure.","We also address the integrability of this structure, including partial and complete integrability.","Finally, we present several examples of the $F$-structure."],"url":"http://arxiv.org/abs/2404.01496v1","category":"math.DG"}
{"created":"2024-04-01 21:07:16","title":"Creating Decidable Diophantine Equations","abstract":"Generalizing an argument of Matiyasevich, we illustrate a method to generate infinitely many diophantine equations whose solutions can be completely described by linear recurrences. In particular, we provide an integer-coefficient polynomial $p(x, y, z)$ whose only integer roots are consecutive triples of Tribonacci numbers.","sentences":["Generalizing an argument of Matiyasevich, we illustrate a method to generate infinitely many diophantine equations whose solutions can be completely described by linear recurrences.","In particular, we provide an integer-coefficient polynomial $p(x, y, z)$ whose only integer roots are consecutive triples of Tribonacci numbers."],"url":"http://arxiv.org/abs/2404.01483v1","category":"math.NT"}
{"created":"2024-04-01 21:01:56","title":"Approach and rotation of reconnecting topological defect lines in liquid crystal","abstract":"Topological defects are a universal concept across many disciplines, such as crystallography, liquid-crystalline physics, low-temperature physics, cosmology, and even biology. In nematic liquid crystals, topological defects called disclinations have been widely studied. For their three-dimensional (3D) dynamics, however, only recently have theoretical approaches dealing with fully 3D configurations been reported. Further, recent experiments have observed 3D disclination line reconnections, a phenomenon characteristic of defect line dynamics, but detailed discussions were limited to the case of approximately parallel defects. In this study, we focus on the case of two disclination lines that approach at finite angles and lie in separate planes, a more fundamentally 3D reconnection configuration. Observation and analysis showed the square-root law of the distance between disclinations and the decrease of the inter-disclination angle over time. We compare the experimental results with theory and find qualitative agreement on the scaling of distance and angle with time, but quantitative disagreement on distance and angle relative mobilities. To probe this disagreement, we derive the equations of motion for systems with reduced twist constant and also carry out simulations for this case. These, together with the experimental results, suggest that deformations of disclinations may be responsible for the disagreement.","sentences":["Topological defects are a universal concept across many disciplines, such as crystallography, liquid-crystalline physics, low-temperature physics, cosmology, and even biology.","In nematic liquid crystals, topological defects called disclinations have been widely studied.","For their three-dimensional (3D) dynamics, however, only recently have theoretical approaches dealing with fully 3D configurations been reported.","Further, recent experiments have observed 3D disclination line reconnections, a phenomenon characteristic of defect line dynamics, but detailed discussions were limited to the case of approximately parallel defects.","In this study, we focus on the case of two disclination lines that approach at finite angles and lie in separate planes, a more fundamentally 3D reconnection configuration.","Observation and analysis showed the square-root law of the distance between disclinations and the decrease of the inter-disclination angle over time.","We compare the experimental results with theory and find qualitative agreement on the scaling of distance and angle with time, but quantitative disagreement on distance and angle relative mobilities.","To probe this disagreement, we derive the equations of motion for systems with reduced twist constant and also carry out simulations for this case.","These, together with the experimental results, suggest that deformations of disclinations may be responsible for the disagreement."],"url":"http://arxiv.org/abs/2404.01480v1","category":"cond-mat.soft"}
{"created":"2024-04-01 20:47:28","title":"Hybrid GRMHD and Force-Free Simulations of Black Hole Accretion","abstract":"We present a new approach for stably evolving general relativistic magnetohydrodynamic (GRMHD) simulations in regions where the magnetization $\\sigma=b^2/\\rho c^2$ becomes large. GRMHD codes typically struggle to evolve plasma above $\\sigma\\approx100$ in simulations of black hole accretion. To ensure stability, GRMHD codes will inject mass density artificially to the simulation as necessary to keep the magnetization below a ceiling value $\\sigma_{\\rm max}$. We propose an alternative approach where the simulation transitions to solving the equations of general relativistic force-free electrodynamics (GRFFE) above a magnetization $\\sigma_{\\rm trans}$. We augment the GRFFE equations in the highly magnetized region with approximate equations to evolve the decoupled field-parallel velocity, plasma energy density, and plasma mass density. Our hybrid scheme is explicit and easily added to the framework of standard-volume GRMHD codes. We present a variety of tests of our method, implemented in the GRMHD code KORAL, and we show first results from a 3D hybrid GRMHD+GRFFE simulation of a magnetically arrested disc (MAD) around a spinning black hole. Our hybrid MAD simulation closely matches the average properties of a standard GRMHD MAD simulation with the same initial conditions in low magnetization regions, but it achieves a magnetization $\\sigma\\approx10^6$ in the evacuated jet funnel. We present simulated horizon-scale images of both simulations at 230 GHz with the black hole mass and accretion rate matched to M87*. Images from the hybrid simulation are less affected by the choice of magnetization cutoff $\\sigma_{\\rm cut}$ imposed in radiative transfer than images from the standard GRMHD simulation.","sentences":["We present a new approach for stably evolving general relativistic magnetohydrodynamic (GRMHD) simulations in regions where the magnetization $\\sigma=b^2/\\rho c^2$ becomes large.","GRMHD codes typically struggle to evolve plasma above $\\sigma\\approx100$ in simulations of black hole accretion.","To ensure stability, GRMHD codes will inject mass density artificially to the simulation as necessary to keep the magnetization below a ceiling value $\\sigma_{\\rm","max}$.","We propose an alternative approach where the simulation transitions to solving the equations of general relativistic force-free electrodynamics (GRFFE) above a magnetization $\\sigma_{\\rm","trans}$.","We augment the GRFFE equations in the highly magnetized region with approximate equations to evolve the decoupled field-parallel velocity, plasma energy density, and plasma mass density.","Our hybrid scheme is explicit and easily added to the framework of standard-volume GRMHD codes.","We present a variety of tests of our method, implemented in the GRMHD code KORAL, and we show first results from a 3D hybrid GRMHD+GRFFE simulation of a magnetically arrested disc (MAD) around a spinning black hole.","Our hybrid MAD simulation closely matches the average properties of a standard GRMHD MAD simulation with the same initial conditions in low magnetization regions, but it achieves a magnetization $\\sigma\\approx10^6$ in the evacuated jet funnel.","We present simulated horizon-scale images of both simulations at 230 GHz with the black hole mass and accretion rate matched to M87*.","Images from the hybrid simulation are less affected by the choice of magnetization cutoff $\\sigma_{\\rm cut}$ imposed in radiative transfer than images from the standard GRMHD simulation."],"url":"http://arxiv.org/abs/2404.01471v1","category":"astro-ph.HE"}
{"created":"2024-04-01 20:16:21","title":"OpenChemIE: An Information Extraction Toolkit For Chemistry Literature","abstract":"Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reaction data from reaction condition and substrate scope investigations. Our machine learning models attain state-of-the-art performance when evaluated individually, and we meticulously annotate a challenging dataset of reaction schemes with R-groups to evaluate our pipeline as a whole, achieving an F1 score of 69.5%. Additionally, the reaction extraction results of \\ours attain an accuracy score of 64.3% when directly compared against the Reaxys chemical database. We provide OpenChemIE freely to the public as an open-source package, as well as through a web interface.","sentences":["Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry.","Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities.","In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level.","OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions.","For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures.","We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reaction data from reaction condition and substrate scope investigations.","Our machine learning models attain state-of-the-art performance when evaluated individually, and we meticulously annotate a challenging dataset of reaction schemes with R-groups to evaluate our pipeline as a whole, achieving an F1 score of 69.5%.","Additionally, the reaction extraction results of \\ours attain an accuracy score of 64.3% when directly compared against the Reaxys chemical database.","We provide OpenChemIE freely to the public as an open-source package, as well as through a web interface."],"url":"http://arxiv.org/abs/2404.01462v1","category":"cs.LG"}
{"created":"2024-04-01 19:56:28","title":"The Linear $q$-Hypergraph Process","abstract":"We analyze a random greedy process to construct $q$-uniform linear hypergraphs using the differential equation method. We show for $q=o(\\sqrt{\\log n})$, that this process yields a hypergraph with $\\frac{n(n-1)}{q(q-1)}(1-o(1))$ edges. We also give some bounds for maximal linear hypergraphs.","sentences":["We analyze a random greedy process to construct $q$-uniform linear hypergraphs using the differential equation method.","We show for $q=o(\\sqrt{\\log n})$, that this process yields a hypergraph with $\\frac{n(n-1)}{q(q-1)}(1-o(1))$ edges.","We also give some bounds for maximal linear hypergraphs."],"url":"http://arxiv.org/abs/2404.01452v1","category":"math.CO"}
{"created":"2024-04-01 19:49:42","title":"Tutte polynomials in superspace","abstract":"We associate a quotient of superspace to any hyperplane arrangement by considering the differential closure of an ideal generated by powers of certain homogeneous linear forms. This quotient is a superspace analogue of the external zonotopal algebra, and it further contains the central zonotopal algebra in the appropriate grading. We show that an evaluation of the bivariate Tutte polynomial is the bigraded Hilbert series of this quotient. We then use this fact to construct an explicit basis for the Macaulay inverse. These results generalize those of Ardila-Postnikov and Holtz-Ron. We also discuss enumerative consequences of our results in the setting of hyperplane arrangements.","sentences":["We associate a quotient of superspace to any hyperplane arrangement by considering the differential closure of an ideal generated by powers of certain homogeneous linear forms.","This quotient is a superspace analogue of the external zonotopal algebra, and it further contains the central zonotopal algebra in the appropriate grading.","We show that an evaluation of the bivariate Tutte polynomial is the bigraded Hilbert series of this quotient.","We then use this fact to construct an explicit basis for the Macaulay inverse.","These results generalize those of Ardila-Postnikov and Holtz-Ron.","We also discuss enumerative consequences of our results in the setting of hyperplane arrangements."],"url":"http://arxiv.org/abs/2404.01450v1","category":"math.CO"}
{"created":"2024-04-01 19:49:32","title":"Converse theorems for Hilbert modular forms of higher level","abstract":"We prove two results on converse theorems for Hilbert modular forms of higher level over totally real fields of degree $r>1$. The first result recovers a Hilbert modular form (of some level) from an $L$-series satisfying functional equations twisted by all the Hecke characters that are unramified at all finite places. The second result assumes both the above functional equations and an Euler product, and recovers a Hilbert modular form of the expected level predicted by the shape of the functional equations.","sentences":["We prove two results on converse theorems for Hilbert modular forms of higher level over totally real fields of degree $r>1$. The first result recovers a Hilbert modular form (of some level) from an $L$-series satisfying functional equations twisted by all the Hecke characters that are unramified at all finite places.","The second result assumes both the above functional equations and an Euler product, and recovers a Hilbert modular form of the expected level predicted by the shape of the functional equations."],"url":"http://arxiv.org/abs/2404.01449v1","category":"math.NT"}
{"created":"2024-04-01 19:08:08","title":"Existence and non-existence of ground state solutions for magnetic NLS","abstract":"We show the existence and stability of ground state solutions (g.s.s.) for $L^2$-critical magnetic nonlinear Schr\\\"odinger equations (mNLS) for a class of unbounded electromagnetic potentials. We then give non-existence result by constructing a sequence of vortex type functions in the setting of RNLS with an anisotropic harmonic potential. These generalize the corresponding results in [3] and [20]. The case of an isotropic harmonic potential for rotational NLS has been recently addressed in [10]. Numerical results on the ground state profile near the threshold are also included.","sentences":["We show the existence and stability of ground state solutions (g.s.s.) for $L^2$-critical magnetic nonlinear Schr\\\"odinger equations (mNLS) for a class of unbounded electromagnetic potentials.","We then give non-existence result by constructing a sequence of vortex type functions in the setting of RNLS with an anisotropic harmonic potential.","These generalize the corresponding results in [3] and [20].","The case of an isotropic harmonic potential for rotational NLS has been recently addressed in [10].","Numerical results on the ground state profile near the threshold are also included."],"url":"http://arxiv.org/abs/2404.01433v1","category":"math.AP"}
{"created":"2024-04-01 18:41:30","title":"On the Faithfulness of Vision Transformer Explanations","abstract":"To interpret Vision Transformers, post-hoc explanations assign salience scores to input pixels, providing human-understandable heatmaps. However, whether these interpretations reflect true rationales behind the model's output is still underexplored. To address this gap, we study the faithfulness criterion of explanations: the assigned salience scores should represent the influence of the corresponding input pixels on the model's predictions. To evaluate faithfulness, we introduce Salience-guided Faithfulness Coefficient (SaCo), a novel evaluation metric leveraging essential information of salience distribution. Specifically, we conduct pair-wise comparisons among distinct pixel groups and then aggregate the differences in their salience scores, resulting in a coefficient that indicates the explanation's degree of faithfulness. Our explorations reveal that current metrics struggle to differentiate between advanced explanation methods and Random Attribution, thereby failing to capture the faithfulness property. In contrast, our proposed SaCo offers a reliable faithfulness measurement, establishing a robust metric for interpretations. Furthermore, our SaCo demonstrates that the use of gradient and multi-layer aggregation can markedly enhance the faithfulness of attention-based explanation, shedding light on potential paths for advancing Vision Transformer explainability.","sentences":["To interpret Vision Transformers, post-hoc explanations assign salience scores to input pixels, providing human-understandable heatmaps.","However, whether these interpretations reflect true rationales behind the model's output is still underexplored.","To address this gap, we study the faithfulness criterion of explanations: the assigned salience scores should represent the influence of the corresponding input pixels on the model's predictions.","To evaluate faithfulness, we introduce Salience-guided Faithfulness Coefficient (SaCo), a novel evaluation metric leveraging essential information of salience distribution.","Specifically, we conduct pair-wise comparisons among distinct pixel groups and then aggregate the differences in their salience scores, resulting in a coefficient that indicates the explanation's degree of faithfulness.","Our explorations reveal that current metrics struggle to differentiate between advanced explanation methods and Random Attribution, thereby failing to capture the faithfulness property.","In contrast, our proposed SaCo offers a reliable faithfulness measurement, establishing a robust metric for interpretations.","Furthermore, our SaCo demonstrates that the use of gradient and multi-layer aggregation can markedly enhance the faithfulness of attention-based explanation, shedding light on potential paths for advancing Vision Transformer explainability."],"url":"http://arxiv.org/abs/2404.01415v1","category":"cs.CV"}
{"created":"2024-04-01 18:11:30","title":"NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification","abstract":"In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis. However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems. This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry(VIO) to provide a robust solution for robotic navigation in a real-time. By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability. We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework. Experimental validation in the photorealistic simulation environment demonstrates significant improvements in accuracy compared to a conventional VIO approach.","sentences":["In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis.","However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems.","This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry(VIO) to provide a robust solution for robotic navigation in a real-time.","By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability.","We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework.","Experimental validation in the photorealistic simulation environment demonstrates significant improvements in accuracy compared to a conventional VIO approach."],"url":"http://arxiv.org/abs/2404.01400v1","category":"cs.RO"}
{"created":"2024-04-01 18:07:18","title":"Numerical relativity surrogate models for exotic compact objects: the case of head-on mergers of equal-mass Proca stars","abstract":"We present several high-accuracy surrogate models for gravitational-wave signals from equal-mass head-on mergers of Proca stars, computed through the Newman-Penrose scalar $\\psi_4$. We also discuss the current state of the model extensions to mergers of Proca stars with different masses, and the particular challenges that these present. The models are divided in two main categories: two-stage and monolithic. In the two-stage models, a dimensional reduction algorithm is applied to embed the data in a reduced feature space, which is then interpolated in terms of the physical parameters. For the monolithic models, a single neural network is trained to predict the waveform from the input physical parameter. Our model displays mismatches below $10^{-3}$ with respect to the original numerical waveforms. Finally, we demonstrate the usage of our model in full Bayesian parameter inference through the accurate recovery of numerical relativity signals injected in zero-noise, together with the analysis of GW190521. For the latter, we observe excellent agreement with existing results that make use of full numerical relativity.","sentences":["We present several high-accuracy surrogate models for gravitational-wave signals from equal-mass head-on mergers of Proca stars, computed through the Newman-Penrose scalar $\\psi_4$. We also discuss the current state of the model extensions to mergers of Proca stars with different masses, and the particular challenges that these present.","The models are divided in two main categories: two-stage and monolithic.","In the two-stage models, a dimensional reduction algorithm is applied to embed the data in a reduced feature space, which is then interpolated in terms of the physical parameters.","For the monolithic models, a single neural network is trained to predict the waveform from the input physical parameter.","Our model displays mismatches below $10^{-3}$ with respect to the original numerical waveforms.","Finally, we demonstrate the usage of our model in full Bayesian parameter inference through the accurate recovery of numerical relativity signals injected in zero-noise, together with the analysis of GW190521.","For the latter, we observe excellent agreement with existing results that make use of full numerical relativity."],"url":"http://arxiv.org/abs/2404.01395v1","category":"gr-qc"}
{"created":"2024-04-01 18:01:04","title":"Covariant Guiding Center Equations for Charged Particle Motions in General Relativistic Spacetimes","abstract":"Low density plasmas in curved spacetimes, such as those found in accretion flows around black holes, are challenging to model from first principles, owing to the large scale separation between the characteristic scales of the microscopic processes and large mean-free-paths comparable to the system sizes. Kinetic approaches become necessary to capture the relevant physics but lack the dynamic range to model the global characteristics of the systems. In this paper, we develop new covariant guiding center equations of motion for charges in general relativistic spacetimes that are computationally tractable. We decompose the particle motion into a fast gyration, which we integrate analytically and a slow drift of the guiding center, which can be solved numerically. We derive covariant conservation laws for the motions of the guiding centers and show, through a number of limiting cases, that the equations contain all known drift mechanisms. Finally, we present the general relativistic expressions for the various drift velocities in Schwarzschild spacetimes.","sentences":["Low density plasmas in curved spacetimes, such as those found in accretion flows around black holes, are challenging to model from first principles, owing to the large scale separation between the characteristic scales of the microscopic processes and large mean-free-paths comparable to the system sizes.","Kinetic approaches become necessary to capture the relevant physics but lack the dynamic range to model the global characteristics of the systems.","In this paper, we develop new covariant guiding center equations of motion for charges in general relativistic spacetimes that are computationally tractable.","We decompose the particle motion into a fast gyration, which we integrate analytically and a slow drift of the guiding center, which can be solved numerically.","We derive covariant conservation laws for the motions of the guiding centers and show, through a number of limiting cases, that the equations contain all known drift mechanisms.","Finally, we present the general relativistic expressions for the various drift velocities in Schwarzschild spacetimes."],"url":"http://arxiv.org/abs/2404.01391v1","category":"astro-ph.HE"}
{"created":"2024-04-01 18:00:02","title":"A new theoretical approach to disordered Majorana nanowires: Studying disorder without any disorder","abstract":"The interplay of disorder and short finite wire length is the crucial physics hindering progress in the semiconductor-superconductor nanowire platform for realizing non-Abelian Majorana zero modes (MZM). Disorder effectively segments the nanowire into isolated patches of quantum dots (QD) which act as subgap Andreev bound states often mimicking MZMs. In this work, we propose and develop a new theoretical approach to model disorder, effectively a spatially varying effective mass model, which does not rely on incorporating unknown microscopic details of disorder into the Hamiltonian. This model effectively segments the wire into multiple QDs, characterized by highly enhanced effective mass at impurity sites leading to the segmentation of the wire into effective random QDs. We find that this model can reproduce disorder physics, providing a crystal clear way to understand the effects of disorder by comparing the mean free path to the superconducting coherence length. In addition, this model allows precise control over the disorder regime, enabling us to evaluate the reliability of topological invariants (TI) in predicting MZMs. We find that TIs alone may yield a significant false positive rate as indicators for topology in the actual wire with increasing disorder strength. Therefore, we propose new indicators to characterize the spatial distribution of the zero-energy state, emphasizing the key necessity for isolated MZMs localized at wire ends. Employing this set of new indicators for stringent characterizations, we explore their experimental relevance to the measured differential conductance spectra. Our findings highlight the critical role of isolated localized states, beyond the TI, in identifying topological MZMs. We believe that this approach is a powerful tool for studying realistic Majorana nanowires where disorder and short wire length obfuscate the underlying topological physics.","sentences":["The interplay of disorder and short finite wire length is the crucial physics hindering progress in the semiconductor-superconductor nanowire platform for realizing non-Abelian Majorana zero modes (MZM).","Disorder effectively segments the nanowire into isolated patches of quantum dots (QD) which act as subgap Andreev bound states often mimicking MZMs.","In this work, we propose and develop a new theoretical approach to model disorder, effectively a spatially varying effective mass model, which does not rely on incorporating unknown microscopic details of disorder into the Hamiltonian.","This model effectively segments the wire into multiple QDs, characterized by highly enhanced effective mass at impurity sites leading to the segmentation of the wire into effective random QDs.","We find that this model can reproduce disorder physics, providing a crystal clear way to understand the effects of disorder by comparing the mean free path to the superconducting coherence length.","In addition, this model allows precise control over the disorder regime, enabling us to evaluate the reliability of topological invariants (TI) in predicting MZMs.","We find that TIs alone may yield a significant false positive rate as indicators for topology in the actual wire with increasing disorder strength.","Therefore, we propose new indicators to characterize the spatial distribution of the zero-energy state, emphasizing the key necessity for isolated MZMs localized at wire ends.","Employing this set of new indicators for stringent characterizations, we explore their experimental relevance to the measured differential conductance spectra.","Our findings highlight the critical role of isolated localized states, beyond the TI, in identifying topological MZMs.","We believe that this approach is a powerful tool for studying realistic Majorana nanowires where disorder and short wire length obfuscate the underlying topological physics."],"url":"http://arxiv.org/abs/2404.01379v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-01 17:52:17","title":"BiPer: Binary Neural Networks using a Periodic Function","abstract":"Quantized neural networks employ reduced precision representations for both weights and activations. This quantization process significantly reduces the memory requirements and computational complexity of the network. Binary Neural Networks (BNNs) are the extreme quantization case, representing values with just one bit. Since the sign function is typically used to map real values to binary values, smooth approximations are introduced to mimic the gradients during error backpropagation. Thus, the mismatch between the forward and backward models corrupts the direction of the gradient, causing training inconsistency problems and performance degradation. In contrast to current BNN approaches, we propose to employ a binary periodic (BiPer) function during binarization. Specifically, we use a square wave for the forward pass to obtain the binary values and employ the trigonometric sine function with the same period of the square wave as a differentiable surrogate during the backward pass. We demonstrate that this approach can control the quantization error by using the frequency of the periodic function and improves network performance. Extensive experiments validate the effectiveness of BiPer in benchmark datasets and network architectures, with improvements of up to 1% and 0.69% with respect to state-of-the-art methods in the classification task over CIFAR-10 and ImageNet, respectively. Our code is publicly available at https://github.com/edmav4/BiPer.","sentences":["Quantized neural networks employ reduced precision representations for both weights and activations.","This quantization process significantly reduces the memory requirements and computational complexity of the network.","Binary Neural Networks (BNNs) are the extreme quantization case, representing values with just one bit.","Since the sign function is typically used to map real values to binary values, smooth approximations are introduced to mimic the gradients during error backpropagation.","Thus, the mismatch between the forward and backward models corrupts the direction of the gradient, causing training inconsistency problems and performance degradation.","In contrast to current BNN approaches, we propose to employ a binary periodic (BiPer) function during binarization.","Specifically, we use a square wave for the forward pass to obtain the binary values and employ the trigonometric sine function with the same period of the square wave as a differentiable surrogate during the backward pass.","We demonstrate that this approach can control the quantization error by using the frequency of the periodic function and improves network performance.","Extensive experiments validate the effectiveness of BiPer in benchmark datasets and network architectures, with improvements of up to 1% and 0.69% with respect to state-of-the-art methods in the classification task over CIFAR-10 and ImageNet, respectively.","Our code is publicly available at https://github.com/edmav4/BiPer."],"url":"http://arxiv.org/abs/2404.01278v1","category":"cs.CV"}
{"created":"2024-04-01 17:46:17","title":"Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis","abstract":"This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection. We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of \"pre-trained models,\" we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion. Additionally, we propose a practical metric for monitoring internal privacy breaches during the learning process.","sentences":["This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms.","First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection.","We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison.","Second, considering the widespread use of \"pre-trained models,\" we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally.","We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion.","Additionally, we propose a practical metric for monitoring internal privacy breaches during the learning process."],"url":"http://arxiv.org/abs/2404.01270v1","category":"cs.LG"}
{"created":"2024-04-01 17:30:46","title":"Dynamics and Optimization in Spatially Distributed Electrical Vehicle Charging","abstract":"We consider a spatially distributed demand for electrical vehicle recharging, that must be covered by a fixed set of charging stations. Arriving EVs receive feedback on transport times to each station, and waiting times at congested ones, based on which they make a selfish selection. This selection determines total arrival rates in station queues, which are represented by a fluid state; departure rates are modeled under the assumption that clients have a given sojourn time in the system. The resulting differential equation system is analyzed with tools of optimization. We characterize the equilibrium as the solution to a specific convex program, which has connections to optimal transport problems, and also with road traffic theory. In particular a price of anarchy appears with respect to a social planner's allocation. From a dynamical perspective, global convergence to equilibrium is established, with tools of Lagrange duality and Lyapunov theory. An extension of the model that makes customer demand elastic to observed delays is also presented, and analyzed with extensions of the optimization machinery. Simulations to illustrate the global behavior are presented, which also help validate the model beyond the fluid approximation.","sentences":["We consider a spatially distributed demand for electrical vehicle recharging, that must be covered by a fixed set of charging stations.","Arriving EVs receive feedback on transport times to each station, and waiting times at congested ones, based on which they make a selfish selection.","This selection determines total arrival rates in station queues, which are represented by a fluid state; departure rates are modeled under the assumption that clients have a given sojourn time in the system.","The resulting differential equation system is analyzed with tools of optimization.","We characterize the equilibrium as the solution to a specific convex program, which has connections to optimal transport problems, and also with road traffic theory.","In particular a price of anarchy appears with respect to a social planner's allocation.","From a dynamical perspective, global convergence to equilibrium is established, with tools of Lagrange duality and Lyapunov theory.","An extension of the model that makes customer demand elastic to observed delays is also presented, and analyzed with extensions of the optimization machinery.","Simulations to illustrate the global behavior are presented, which also help validate the model beyond the fluid approximation."],"url":"http://arxiv.org/abs/2404.01259v1","category":"math.OC"}
{"created":"2024-04-01 17:25:27","title":"New logarithmic step size for stochastic gradient descent","abstract":"In this paper, we propose a novel warm restart technique using a new logarithmic step size for the stochastic gradient descent (SGD) approach. For smooth and non-convex functions, we establish an $O(\\frac{1}{\\sqrt{T}})$ convergence rate for the SGD. We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9\\%$ for the CIFAR100 dataset when we utilize a convolutional neural network (CNN) model.","sentences":["In this paper, we propose a novel warm restart technique using a new logarithmic step size for the stochastic gradient descent (SGD) approach.","For smooth and non-convex functions, we establish an $O(\\frac{1}{\\sqrt{T}})$ convergence rate for the SGD.","We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets.","Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9\\%$ for the CIFAR100 dataset when we utilize a convolutional neural network (CNN) model."],"url":"http://arxiv.org/abs/2404.01257v1","category":"cs.LG"}
{"created":"2024-04-01 16:58:32","title":"AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding","abstract":"Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.   In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.","sentences":["Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform.","Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens.","However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps.","Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.   ","In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding.","We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly.","The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned.","As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs.","After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen.","We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store.","Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage.","The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques."],"url":"http://arxiv.org/abs/2404.01240v1","category":"cs.SE"}
{"created":"2024-04-01 16:57:13","title":"Mass Spectra of Full-Heavy and Double-Heavy Tetraquark States in the Conventional Quark Model","abstract":"A comprehensive study of the $S$-wave heavy tetraquark states with identical quarks and antiquarks, specifically $QQ{\\bar Q'}\\bar Q'$ ($Q, Q'=c,b$), $QQ\\bar s\\bar s$/$\\bar Q\\bar Q ss$, and $QQ\\bar q\\bar q$/$\\bar Q\\bar Q qq$ ($q=u,d$), are studied in a unified constituent quark model. This model contains the one-gluon exchange and confinement potentials. The latter is modeled as the sum of all two-body linear potentials. We employ the Gaussian expansion method to solve the full four-body Schr\\\"{o}dinger equations, and search bound and resonant states using the complex-scaling method. We then identify $3$ bound and $62$ resonant states. The bound states are all $QQ\\bar q\\bar q$ states with the isospin and spin-parity quantum numbers $I(J^P)=0(1^+)$: two bound $bb\\bar{q}\\bar{q}$ states with the binding energies, 153 MeV and 4 MeV below the $BB^*$ threshold, and a shallow $cc\\bar{q}\\bar{q}$ state at $-15$ MeV from the $DD^*$ threshold. The deeper $bb\\bar q \\bar q$ bound state aligns with the lattice QCD predictions, while $cc\\bar q\\bar q$ bound state, still has a much larger binding energy than the recently observed $T^+_{cc}$ by LHCb collaboration. No bound states are identified for the $QQ\\bar Q'\\bar Q'$, $QQ\\bar s\\bar s$ and $QQ\\bar q\\bar q$ with $I=1$. Our analysis shows that the bound $QQ\\bar Q'\\bar Q'$ states are more probable with a larger mass ratio, $m_Q/m_{Q'}$. Experimental investigation for these states is desired, which will enrich our understanding of hadron spectroscopy and probe insights into the confinement mechanisms within tetraquarks.","sentences":["A comprehensive study of the $S$-wave heavy tetraquark states with identical quarks and antiquarks, specifically $QQ{\\bar Q'}\\bar Q'$ ($Q, Q'=c,b$), $QQ\\bar s\\bar s$/$\\bar Q\\bar Q ss$, and $QQ\\bar q\\bar q$/$\\bar Q\\bar Q qq$ ($q=u,d$), are studied in a unified constituent quark model.","This model contains the one-gluon exchange and confinement potentials.","The latter is modeled as the sum of all two-body linear potentials.","We employ the Gaussian expansion method to solve the full four-body Schr\\\"{o}dinger equations, and search bound and resonant states using the complex-scaling method.","We then identify $3$ bound and $62$ resonant states.","The bound states are all $QQ\\bar q\\bar q$ states with the isospin and spin-parity quantum numbers $I(J^P)=0(1^+)$: two bound $bb\\bar{q}\\bar{q}$ states with the binding energies, 153 MeV and 4 MeV below the $BB^*$ threshold, and a shallow $cc\\bar{q}\\bar{q}$ state at $-15$ MeV from the $DD^*$ threshold.","The deeper $bb\\bar q \\bar q$ bound state aligns with the lattice QCD predictions, while $cc\\bar q\\bar q$ bound state, still has a much larger binding energy than the recently observed $T^+_{cc}$ by LHCb collaboration.","No bound states are identified for the $QQ\\bar Q'\\bar Q'$, $QQ\\bar s\\bar s$ and $QQ\\bar q\\bar q$ with $I=1$. Our analysis shows that the bound $QQ\\bar Q'\\bar Q'$ states are more probable with a larger mass ratio, $m_Q/m_{Q'}$. Experimental investigation for these states is desired, which will enrich our understanding of hadron spectroscopy and probe insights into the confinement mechanisms within tetraquarks."],"url":"http://arxiv.org/abs/2404.01238v1","category":"hep-ph"}
{"created":"2024-04-01 16:54:58","title":"Cosmic topology. Part IVa. Classification of manifolds using machine learning: a case study with small toroidal universes","abstract":"Non-trivial spatial topology of the Universe may give rise to potentially measurable signatures in the cosmic microwave background. We explore different machine learning approaches to classify harmonic-space realizations of the microwave background in the test case of Euclidean $E_1$ topology (the 3-torus) with a cubic fundamental domain of a size scale significantly smaller than the diameter of the last scattering surface. Different machine learning approaches are capable of classifying the harmonic-space realizations with accuracy greater than 99% if the topology scale is half of the diameter of the last-scattering surface and orientation of the topology is known. For distinguishing random rotations of these sky realizations from realizations of the covering space, the extreme gradient boosting classifier algorithm performs best with an accuracy of 88%. Slightly lower accuracies of 83% to 87% are obtained with the random forest classifier along with one- and two-dimensional convolutional neural networks. The techniques presented here can also accurately classify non-rotated cubic $E_1$ topology realizations with a topology scale slightly larger than the diameter of the last-scattering surface, if provided enough training data. This work identifies the prospects and the main challenges for developing machine learning techniques that are capable of accurately classifying observationally viable topologies.","sentences":["Non-trivial spatial topology of the Universe may give rise to potentially measurable signatures in the cosmic microwave background.","We explore different machine learning approaches to classify harmonic-space realizations of the microwave background in the test case of Euclidean $E_1$ topology (the 3-torus) with a cubic fundamental domain of a size scale significantly smaller than the diameter of the last scattering surface.","Different machine learning approaches are capable of classifying the harmonic-space realizations with accuracy greater than 99% if the topology scale is half of the diameter of the last-scattering surface and orientation of the topology is known.","For distinguishing random rotations of these sky realizations from realizations of the covering space, the extreme gradient boosting classifier algorithm performs best with an accuracy of 88%.","Slightly lower accuracies of 83% to 87% are obtained with the random forest classifier along with one- and two-dimensional convolutional neural networks.","The techniques presented here can also accurately classify non-rotated cubic $E_1$ topology realizations with a topology scale slightly larger than the diameter of the last-scattering surface, if provided enough training data.","This work identifies the prospects and the main challenges for developing machine learning techniques that are capable of accurately classifying observationally viable topologies."],"url":"http://arxiv.org/abs/2404.01236v1","category":"astro-ph.CO"}
{"created":"2024-04-01 16:51:50","title":"GFLean: An Autoformalisation Framework for Lean via GF","abstract":"We present an autoformalisation framework for the Lean theorem prover, called GFLean. GFLean uses a high-level grammar writing tool called Grammatical Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell. We explain the functionalities of GFLean, its inner working and discuss its limitations. We also discuss how we can use neural network based translation programs and rule based translation programs together complimenting each other to build robust autoformalisation frameworks.","sentences":["We present an autoformalisation framework for the Lean theorem prover, called GFLean.","GFLean uses a high-level grammar writing tool called Grammatical Framework (GF) for parsing and linearisation.","GFLean is implemented in Haskell.","We explain the functionalities of GFLean, its inner working and discuss its limitations.","We also discuss how we can use neural network based translation programs and rule based translation programs together complimenting each other to build robust autoformalisation frameworks."],"url":"http://arxiv.org/abs/2404.01234v1","category":"cs.CL"}
{"created":"2024-04-01 16:42:20","title":"Method of similar operators in harmonious Banach spaces","abstract":"We consider similarity transformations of a perturbed linear operator $A-B$ in a complex Banach space $\\mathcal{X}$, where the unperturbed operator $A$ is a generator of a Banach $L_1(\\mathbb{R})$-module and the perturbation operator $B$ is a bounded linear operator. The result of the transformation is a simpler operator $A-B_0$. For example, if $A$ is a differentiation operator and $B$ is an operator of multiplication by an operator-valued function, then $B_0$ is an operator of multiplication by a function that is a restriction of an entire function of exponential type and could be $0$ in some cases. As a consequence, we derive the spectral invariance of the operator $A-B$ in a large class of spaces. The study is based on a widely applicable modification of the method of similar operators that is also presented in the paper. This non-traditional modification is rooted in the spectral theory of Banach $L_1(\\mathbb{R})$-modules.","sentences":["We consider similarity transformations of a perturbed linear operator $A-B$ in a complex Banach space $\\mathcal{X}$, where the unperturbed operator $A$ is a generator of a Banach $L_1(\\mathbb{R})$-module and the perturbation operator $B$ is a bounded linear operator.","The result of the transformation is a simpler operator $A-B_0$.","For example, if $A$ is a differentiation operator and $B$ is an operator of multiplication by an operator-valued function, then $B_0$ is an operator of multiplication by a function that is a restriction of an entire function of exponential type and could be $0$ in some cases.","As a consequence, we derive the spectral invariance of the operator $A-B$ in a large class of spaces.","The study is based on a widely applicable modification of the method of similar operators that is also presented in the paper.","This non-traditional modification is rooted in the spectral theory of Banach $L_1(\\mathbb{R})$-modules."],"url":"http://arxiv.org/abs/2404.01227v1","category":"math.FA"}
{"created":"2024-04-01 16:31:06","title":"Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems","abstract":"Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to efficiently learn the Pareto sets of multiple MOPs in a single run while leveraging the relationships among various MOPs. To further understand these relationships, we experimentally demonstrate that there exist shareable representations among MOPs. Leveraging these collaboratively shared representations can effectively improve the capability to approximate Pareto sets. Extensive experiments underscore the superior efficiency and robustness of CoPSL in approximating Pareto sets compared to state-of-the-art approaches on a variety of synthetic and real-world MOPs. Code is available at https://github.com/ckshang/CoPSL.","sentences":["Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions.","However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time.","When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs.","In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner.","CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP.","This collaborative approach enables CoPSL to efficiently learn the Pareto sets of multiple MOPs in a single run while leveraging the relationships among various MOPs.","To further understand these relationships, we experimentally demonstrate that there exist shareable representations among MOPs.","Leveraging these collaboratively shared representations can effectively improve the capability to approximate Pareto sets.","Extensive experiments underscore the superior efficiency and robustness of CoPSL in approximating Pareto sets compared to state-of-the-art approaches on a variety of synthetic and real-world MOPs.","Code is available at https://github.com/ckshang/CoPSL."],"url":"http://arxiv.org/abs/2404.01224v1","category":"cs.LG"}
{"created":"2024-04-01 16:14:13","title":"Miura transformations and large-time behaviors of the Hirota-Satsuma equation","abstract":"The good Boussinesq equation has several modified versions such as the modified Boussinesq equation, Mikhailov-Lenells equation and Hirota-Satsuma equation. This work builds the full relations among these equations by Miura transformation and invertible linear transformations and draws a pyramid diagram to demonstrate such relations. The direct and inverse spectral analysis shows that the solution of Riemann-Hilbert problem for Hirota-Satsuma equation has simple pole at origin, the solution of Riemann-Hilbert problem for the good Boussinesq equation has double pole at origin, while the solution of Riemann-Hilbert problem for the modified Boussinesq equation and Mikhailov-Lenells equation doesn't have singularity at origin. Further, the large-time asymptotic behaviors of the Hirota-Satsuma equation with Schwartz class initial value is studied by Deift-Zhou nonlinear steepest descent analysis. In such initial condition, the asymptotic expressions of the Hirota-Satsuma equation and good Boussinesq equation away from the origin are proposed and it is displayed that the leading term of asymptotic formulas match well with direct numerical simulations.","sentences":["The good Boussinesq equation has several modified versions such as the modified Boussinesq equation, Mikhailov-Lenells equation and Hirota-Satsuma equation.","This work builds the full relations among these equations by Miura transformation and invertible linear transformations and draws a pyramid diagram to demonstrate such relations.","The direct and inverse spectral analysis shows that the solution of Riemann-Hilbert problem for Hirota-Satsuma equation has simple pole at origin, the solution of Riemann-Hilbert problem for the good Boussinesq equation has double pole at origin, while the solution of Riemann-Hilbert problem for the modified Boussinesq equation and Mikhailov-Lenells equation doesn't have singularity at origin.","Further, the large-time asymptotic behaviors of the Hirota-Satsuma equation with Schwartz class initial value is studied by Deift-Zhou nonlinear steepest descent analysis.","In such initial condition, the asymptotic expressions of the Hirota-Satsuma equation and good Boussinesq equation away from the origin are proposed and it is displayed that the leading term of asymptotic formulas match well with direct numerical simulations."],"url":"http://arxiv.org/abs/2404.01215v1","category":"nlin.SI"}
{"created":"2024-04-01 16:12:56","title":"Bifurcation on Fully Nonlinear Elliptic Equations and Systems","abstract":"In this paper, we study the following fully nonlinear elliptic equations \\begin{equation*} \\left\\{\\begin{array}{rl} \\left(S_{k}(D^{2}u)\\right)^{\\frac1k}=\\lambda f(-u) & in\\quad\\Omega \\\\ u=0 & on\\quad \\partial\\Omega\\\\ \\end{array} \\right. \\end{equation*} and coupled systems \\begin{equation*} \\left\\{\\begin{array}{rl} (S_{k}(D^{2}u))^\\frac1k=\\lambda g(-u,-v) & in\\quad\\Omega \\\\ (S_{k}(D^{2}v))^\\frac1k=\\lambda h(-u,-v) & in\\quad\\Omega \\\\ u=v=0 & on\\quad \\partial\\Omega\\\\ \\end{array} \\right. \\end{equation*} dominated by $k$-Hessian operators, where $\\Omega$ is a $(k$-$1)$-convex bounded domain in $\\mathbb{R}^{N}$, $\\lambda$ is a non-negative parameter, $f:\\left[0,+\\infty\\right)\\rightarrow\\left[0,+\\infty\\right)$ is a continuous function with zeros only at $0$ and $g,h:\\left[0,+\\infty\\right)\\times \\left[0,+\\infty\\right)\\rightarrow \\left[0,+\\infty\\right)$ are continuous functions with zeros only at $(\\cdot,0)$ and $(0,\\cdot)$. We determine the interval of $\\lambda$ about the existence, non-existence, uniqueness and multiplicity of $k$-convex solutions to the above problems according to various cases of $f,g,h$, which is a complete supplement to the known results in previous literature. In particular, the above results are also new for Laplacian and Monge-Amp\\`ere operators. We mainly use bifurcation theory, a-priori estimates, various maximum principles and technical strategies in the proof.","sentences":["In this paper, we study the following fully nonlinear elliptic equations \\begin{equation*} \\left\\{\\begin{array}{rl} \\left(S_{k}(D^{2}u)\\right)^{\\frac1k}=\\lambda f(-u) & in\\quad\\Omega \\\\ u=0 & on\\quad \\partial\\Omega\\\\ \\end{array} \\right.","\\end{equation*} and coupled systems \\begin{equation*} \\left\\{\\begin{array}{rl} (S_{k}(D^{2}u))^\\frac1k=\\lambda g(-u,-v) & in\\quad\\Omega \\\\ (S_{k}(D^{2}v))^\\frac1k=\\lambda h(-u,-v) & in\\quad\\Omega \\\\ u=v=0 & on\\quad \\partial\\Omega\\\\ \\end{array} \\right.","\\end{equation*} dominated by $k$-Hessian operators, where $\\Omega$ is a $(k$-$1)$-convex bounded domain in $\\mathbb{R}^{N}$, $\\lambda$ is a non-negative parameter, $f:\\left[0,+\\infty\\right)\\rightarrow\\left[0,+\\infty\\right)$ is a continuous function with zeros only at $0$ and $g,h:\\left[0,+\\infty\\right)\\times \\left[0,+\\infty\\right)\\rightarrow \\left[0,+\\infty\\right)$ are continuous functions with zeros only at $(\\cdot,0)$ and $(0,\\cdot)$. We determine the interval of $\\lambda$ about the existence, non-existence, uniqueness and multiplicity of $k$-convex solutions to the above problems according to various cases of $f,g,h$, which is a complete supplement to the known results in previous literature.","In particular, the above results are also new for Laplacian and Monge-Amp\\`ere operators.","We mainly use bifurcation theory, a-priori estimates, various maximum principles and technical strategies in the proof."],"url":"http://arxiv.org/abs/2404.01213v1","category":"math.AP"}
{"created":"2024-04-01 15:56:58","title":"Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization","abstract":"Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\\chi^2$-divergences as a special case. We prove that our algorithm finds an $\\epsilon$-stationary point with a computational complexity of $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the Cressie-Read divergence. The numerical results indicate that our method outperforms existing methods.} Our method also applies to the smoothed conditional value at risk (CVaR) DRO.","sentences":["Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts.","This paper focuses on constrained DRO, which has an explicit characterization of the robustness level.","Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network.","This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO.","The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications.","We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\\chi^2$-divergences as a special case.","We prove that our algorithm finds an $\\epsilon$-stationary point with a computational complexity of $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the Cressie-Read divergence.","The numerical results indicate that our method outperforms existing methods.}","Our method also applies to the smoothed conditional value at risk (CVaR) DRO."],"url":"http://arxiv.org/abs/2404.01200v1","category":"stat.ML"}
{"created":"2024-04-01 15:50:09","title":"Inferring parameters and reconstruction of two-dimensional turbulent flows with physics-informed neural networks","abstract":"Solving inverse problems, which means obtaining model parameters from observed data, using conventional computational fluid dynamics solvers is prohibitively expensive. Here we employ machine learning algorithms to overcome the challenge. As an example, we consider a moderately turbulent fluid flow, excited by a stationary force and described by a two-dimensional Navier-Stokes equation with linear bottom friction. Given sparse and probably noisy data for the velocity and the general form of the model, we reconstruct the dense velocity and pressure fields in the observation domain, infer the driving force, and determine the unknown fluid viscosity and friction coefficient. Our approach involves training a physics-informed neural network by minimizing the loss function, which penalizes deviations from the provided data and violations of the Navier-Stokes equation. The suggested technique extracts additional information from experimental and numerical observations, potentially enhancing the capabilities of particle image/tracking velocimetry.","sentences":["Solving inverse problems, which means obtaining model parameters from observed data, using conventional computational fluid dynamics solvers is prohibitively expensive.","Here we employ machine learning algorithms to overcome the challenge.","As an example, we consider a moderately turbulent fluid flow, excited by a stationary force and described by a two-dimensional Navier-Stokes equation with linear bottom friction.","Given sparse and probably noisy data for the velocity and the general form of the model, we reconstruct the dense velocity and pressure fields in the observation domain, infer the driving force, and determine the unknown fluid viscosity and friction coefficient.","Our approach involves training a physics-informed neural network by minimizing the loss function, which penalizes deviations from the provided data and violations of the Navier-Stokes equation.","The suggested technique extracts additional information from experimental and numerical observations, potentially enhancing the capabilities of particle image/tracking velocimetry."],"url":"http://arxiv.org/abs/2404.01193v1","category":"physics.flu-dyn"}
{"created":"2024-04-01 15:36:39","title":"Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller","abstract":"Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at https://mit-realm.github.io/CBF-INC-RRT-website/.","sentences":["Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time.","To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT.","Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT.","CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR.","In the latter case, we also study two different settings: fully and partially observed environmental information.","Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative.","Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases.","Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve the success rate by 10%, compared with planning with other steering controllers.","Our project page with supplementary material is at https://mit-realm.github.io/CBF-INC-RRT-website/."],"url":"http://arxiv.org/abs/2404.01184v1","category":"cs.RO"}
{"created":"2024-04-01 15:27:30","title":"Reconstructing Robust Background IFU spectra using Machine Learning","abstract":"In astronomy, spectroscopy consists of observing an astrophysical source and extracting its spectrum of electromagnetic radiation. Once extracted, a model is fit to the spectra to measure the observables, leading to an understanding of the underlying physics of the emission mechanism. One crucial, and often overlooked, aspect of this model is the background emission, which contains foreground and background astrophysical sources, intervening atmospheric emission, and artifacts related to the instrument such as noise. This paper proposes an algorithmic approach to constructing a background model for SITELLE observations using statistical tools and supervised machine learning algorithms. We apply a segmentation algorithm implemented in photutils to divide the data cube into background and source spaxels. After applying a principal component analysis (PCA) on the background spaxels, we train an artificial neural network to interpolate from the background to the source spaxels in the PCA coefficient space, which allows us to generate a local background model over the entire data cube. We highlight the performance of this methodology by applying it to SITELLE observations obtained of a SIGNALS galaxy, \\NGC4449, and the Perseus galaxy cluster of galaxies, NGC 1275. We discuss the physical interpretation of the principal components and noise reduction in the resulting PCA-based reconstructions. Additionally, we compare the fit results using our new background modeling approach to standard methods used in the literature and find that our method better captures the emission from HII regions in NGC 4449 and the faint emission regions in NGC 1275. These methods also demonstrate that the background does change as a function of the position of the datacube.","sentences":["In astronomy, spectroscopy consists of observing an astrophysical source and extracting its spectrum of electromagnetic radiation.","Once extracted, a model is fit to the spectra to measure the observables, leading to an understanding of the underlying physics of the emission mechanism.","One crucial, and often overlooked, aspect of this model is the background emission, which contains foreground and background astrophysical sources, intervening atmospheric emission, and artifacts related to the instrument such as noise.","This paper proposes an algorithmic approach to constructing a background model for SITELLE observations using statistical tools and supervised machine learning algorithms.","We apply a segmentation algorithm implemented in photutils to divide the data cube into background and source spaxels.","After applying a principal component analysis (PCA) on the background spaxels, we train an artificial neural network to interpolate from the background to the source spaxels in the PCA coefficient space, which allows us to generate a local background model over the entire data cube.","We highlight the performance of this methodology by applying it to SITELLE observations obtained of a SIGNALS galaxy, \\NGC4449, and the Perseus galaxy cluster of galaxies, NGC 1275.","We discuss the physical interpretation of the principal components and noise reduction in the resulting PCA-based reconstructions.","Additionally, we compare the fit results using our new background modeling approach to standard methods used in the literature and find that our method better captures the emission from HII regions in NGC 4449 and the faint emission regions in NGC 1275.","These methods also demonstrate that the background does change as a function of the position of the datacube."],"url":"http://arxiv.org/abs/2404.01175v1","category":"astro-ph.GA"}
{"created":"2024-04-01 15:26:44","title":"SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding","abstract":"Temporal video grounding (TVG) is a critical task in video content understanding. Despite significant advancements, existing methods often limit in capturing the fine-grained relationships between multimodal inputs and the high computational costs with processing long video sequences. To address these limitations, we introduce a novel SpikeMba: multi-modal spiking saliency mamba for temporal video grounding. In our work, we integrate the Spiking Neural Networks (SNNs) and state space models (SSMs) to capture the fine-grained relationships of multimodal features effectively. Specifically, we introduce the relevant slots to enhance the model's memory capabilities, enabling a deeper contextual understanding of video sequences. The contextual moment reasoner leverages these slots to maintain a balance between contextual information preservation and semantic relevance exploration. Simultaneously, the spiking saliency detector capitalizes on the unique properties of SNNs to accurately locate salient proposals. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.","sentences":["Temporal video grounding (TVG) is a critical task in video content understanding.","Despite significant advancements, existing methods often limit in capturing the fine-grained relationships between multimodal inputs and the high computational costs with processing long video sequences.","To address these limitations, we introduce a novel SpikeMba: multi-modal spiking saliency mamba for temporal video grounding.","In our work, we integrate the Spiking Neural Networks (SNNs) and state space models (SSMs) to capture the fine-grained relationships of multimodal features effectively.","Specifically, we introduce the relevant slots to enhance the model's memory capabilities, enabling a deeper contextual understanding of video sequences.","The contextual moment reasoner leverages these slots to maintain a balance between contextual information preservation and semantic relevance exploration.","Simultaneously, the spiking saliency detector capitalizes on the unique properties of SNNs to accurately locate salient proposals.","Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks."],"url":"http://arxiv.org/abs/2404.01174v1","category":"cs.CV"}
{"created":"2024-04-01 15:16:33","title":"Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting","abstract":"3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method's ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research.","sentences":["3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis.","However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes.","This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints.","To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections.","By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings.","Extensive assessments, spanning both synthetic and real-world scenes, showcase our method's ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions.","Our code will be made publicly available for reproducible research."],"url":"http://arxiv.org/abs/2404.01168v1","category":"cs.CV"}
{"created":"2024-04-01 15:08:14","title":"A dynamical systems formulation for inhomogeneous LRS-II spacetimes","abstract":"We present a dynamical system formulation for inhomogeneous LRS-II spacetimes using the covariant 1+1+2 decomposition approach. Our approach describes the LRS-II dynamics from the point of view of a comoving observer. Promoting the covariant radial derivatives of the covariant dynamical quantities to new dynamical variables and utilizing the commutation relation between the covariant temporal and radial derivatives, we were able to construct an autonomous system of first-order ordinary differential equations along with some purely algebraic constraints. Using our dynamical system formulation we found several interesting features in the LRS-II phase space with dust, one of them being that the homogeneous solutions constitute an invariant submanifold. For the particular case of LTB, we were also able to recover the previously known result that an expanding LTB tends to Milne in the absence of a cosmological constant, providing a potential validation of our formalism.","sentences":["We present a dynamical system formulation for inhomogeneous LRS-II spacetimes using the covariant 1+1+2 decomposition approach.","Our approach describes the LRS-II dynamics from the point of view of a comoving observer.","Promoting the covariant radial derivatives of the covariant dynamical quantities to new dynamical variables and utilizing the commutation relation between the covariant temporal and radial derivatives, we were able to construct an autonomous system of first-order ordinary differential equations along with some purely algebraic constraints.","Using our dynamical system formulation we found several interesting features in the LRS-II phase space with dust, one of them being that the homogeneous solutions constitute an invariant submanifold.","For the particular case of LTB, we were also able to recover the previously known result that an expanding LTB tends to Milne in the absence of a cosmological constant, providing a potential validation of our formalism."],"url":"http://arxiv.org/abs/2404.01161v1","category":"gr-qc"}
{"created":"2024-04-01 15:06:20","title":"Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning Models","abstract":"Today, skin cancer is considered as one of the most dangerous and common cancers in the world which demands special attention. Skin cancer may be developed in different types; including melanoma, actinic keratosis, basal cell carcinoma, squamous cell carcinoma, and Merkel cell carcinoma. Among them, melanoma is more unpredictable. Melanoma cancer can be diagnosed at early stages increasing the possibility of disease treatment. Automatic classification of skin lesions is a challenging task due to diverse forms and grades of the disease, demanding the requirement of novel methods implementation. Deep convolution neural networks (CNN) have shown an excellent potential for data and image classification. In this article, we inspect skin lesion classification problem using CNN techniques. Remarkably, we present that prominent classification accuracy of lesion detection can be obtained by proper designing and applying of transfer learning framework on pre-trained neural networks, without any requirement for data enlargement procedures i.e. merging VGG16 and VGG19 architectures pre-trained by a generic dataset with modified AlexNet network, and then, fine-tuned by a subject-specific dataset containing dermatology images. The convolution neural network was trained using 2541 images and, in particular, dropout was used to prevent the network from overfitting. Finally, the validity of the model was checked by applying the K-fold cross validation method. The proposed model increased classification accuracy by 3% (from 94.2% to 98.18%) in comparison with other methods.","sentences":["Today, skin cancer is considered as one of the most dangerous and common cancers in the world which demands special attention.","Skin cancer may be developed in different types; including melanoma, actinic keratosis, basal cell carcinoma, squamous cell carcinoma, and Merkel cell carcinoma.","Among them, melanoma is more unpredictable.","Melanoma cancer can be diagnosed at early stages increasing the possibility of disease treatment.","Automatic classification of skin lesions is a challenging task due to diverse forms and grades of the disease, demanding the requirement of novel methods implementation.","Deep convolution neural networks (CNN) have shown an excellent potential for data and image classification.","In this article, we inspect skin lesion classification problem using CNN techniques.","Remarkably, we present that prominent classification accuracy of lesion detection can be obtained by proper designing and applying of transfer learning framework on pre-trained neural networks, without any requirement for data enlargement procedures i.e. merging VGG16 and VGG19 architectures pre-trained by a generic dataset with modified AlexNet network, and then, fine-tuned by a subject-specific dataset containing dermatology images.","The convolution neural network was trained using 2541 images and, in particular, dropout was used to prevent the network from overfitting.","Finally, the validity of the model was checked by applying the K-fold cross validation method.","The proposed model increased classification accuracy by 3% (from 94.2% to 98.18%) in comparison with other methods."],"url":"http://arxiv.org/abs/2404.01160v1","category":"cs.CV"}
{"created":"2024-04-01 15:04:24","title":"GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA","abstract":"Evolutionary multiobjective optimization has witnessed remarkable progress during the past decades. However, existing algorithms often encounter computational challenges in large-scale scenarios, primarily attributed to the absence of hardware acceleration. In response, we introduce a Tensorized Reference Vector Guided Evolutionary Algorithm (TensorRVEA) for harnessing the advancements of GPU acceleration. In TensorRVEA, the key data structures and operators are fully transformed into tensor forms for leveraging GPU-based parallel computing. In numerical benchmark tests involving large-scale populations and problem dimensions, TensorRVEA consistently demonstrates high computational performance, achieving up to over 1000$\\times$ speedups. Then, we applied TensorRVEA to the domain of multiobjective neuroevolution for addressing complex challenges in robotic control tasks. Furthermore, we assessed TensorRVEA's extensibility by altering several tensorized reproduction operators. Experimental results demonstrate promising scalability and robustness of TensorRVEA. Source codes are available at https://github.com/EMI-Group/tensorrvea.","sentences":["Evolutionary multiobjective optimization has witnessed remarkable progress during the past decades.","However, existing algorithms often encounter computational challenges in large-scale scenarios, primarily attributed to the absence of hardware acceleration.","In response, we introduce a Tensorized Reference Vector Guided Evolutionary Algorithm (TensorRVEA) for harnessing the advancements of GPU acceleration.","In TensorRVEA, the key data structures and operators are fully transformed into tensor forms for leveraging GPU-based parallel computing.","In numerical benchmark tests involving large-scale populations and problem dimensions, TensorRVEA consistently demonstrates high computational performance, achieving up to over 1000$\\times$ speedups.","Then, we applied TensorRVEA to the domain of multiobjective neuroevolution for addressing complex challenges in robotic control tasks.","Furthermore, we assessed TensorRVEA's extensibility by altering several tensorized reproduction operators.","Experimental results demonstrate promising scalability and robustness of TensorRVEA.","Source codes are available at https://github.com/EMI-Group/tensorrvea."],"url":"http://arxiv.org/abs/2404.01159v1","category":"cs.NE"}
{"created":"2024-04-01 14:58:07","title":"Applications of the Stone-Weierstrass theorem in the Calder\u00f3n problem","abstract":"We give examples on the use of the Stone-Weierstrass theorem in inverse problems. We show uniqueness in the linearized Calder\\'on problem on holomorphically separable K\\\"ahler manifolds, and in the Calder\\'on problem for nonlinear equations on conformally transversally anisotropic manifolds. We also study the holomorphic separability condition in terms of plurisubharmonic functions. The Stone-Weierstrass theorem allows us to generalize and simplify earlier results. It also makes it possible to circumvent the use of complex geometrical optics solutions and inversion of explicit transforms in certain cases.","sentences":["We give examples on the use of the Stone-Weierstrass theorem in inverse problems.","We show uniqueness in the linearized Calder\\'on problem on holomorphically separable K\\\"ahler manifolds, and in the Calder\\'on problem for nonlinear equations on conformally transversally anisotropic manifolds.","We also study the holomorphic separability condition in terms of plurisubharmonic functions.","The Stone-Weierstrass theorem allows us to generalize and simplify earlier results.","It also makes it possible to circumvent the use of complex geometrical optics solutions and inversion of explicit transforms in certain cases."],"url":"http://arxiv.org/abs/2404.01152v1","category":"math.CV"}
{"created":"2024-04-01 14:45:16","title":"Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations","abstract":"Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows.","sentences":["Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time.","This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis.","The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting.","Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows."],"url":"http://arxiv.org/abs/2404.01145v1","category":"math.NA"}
{"created":"2024-04-01 14:38:51","title":"SoK: A Review of Differentially Private Linear Models For High-Dimensional Data","abstract":"Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To guarantee the privacy of training data, differential privacy can be used. Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models. Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research. Code for implementing all methods is released online.","sentences":["Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions.","To guarantee the privacy of training data, differential privacy can be used.","Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist.","We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models.","Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research.","Code for implementing all methods is released online."],"url":"http://arxiv.org/abs/2404.01141v1","category":"cs.LG"}
{"created":"2024-04-01 14:34:47","title":"Structured Initialization for Attention in Vision Transformers","abstract":"The training of vision transformer (ViT) networks on small-scale datasets poses a significant challenge. By contrast, convolutional neural networks (CNNs) have an architectural inductive bias enabling them to perform well on such problems. In this paper, we argue that the architectural bias inherent to CNNs can be reinterpreted as an initialization bias within ViT. This insight is significant as it empowers ViTs to perform equally well on small-scale problems while maintaining their flexibility for large-scale applications. Our inspiration for this ``structured'' initialization stems from our empirical observation that random impulse filters can achieve comparable performance to learned filters within CNNs. Our approach achieves state-of-the-art performance for data-efficient ViT learning across numerous benchmarks including CIFAR-10, CIFAR-100, and SVHN.","sentences":["The training of vision transformer (ViT) networks on small-scale datasets poses a significant challenge.","By contrast, convolutional neural networks (CNNs) have an architectural inductive bias enabling them to perform well on such problems.","In this paper, we argue that the architectural bias inherent to CNNs can be reinterpreted as an initialization bias within ViT.","This insight is significant as it empowers ViTs to perform equally well on small-scale problems while maintaining their flexibility for large-scale applications.","Our inspiration for this ``structured'' initialization stems from our empirical observation that random impulse filters can achieve comparable performance to learned filters within CNNs.","Our approach achieves state-of-the-art performance for data-efficient ViT learning across numerous benchmarks including CIFAR-10, CIFAR-100, and SVHN."],"url":"http://arxiv.org/abs/2404.01139v1","category":"cs.CV"}
{"created":"2024-04-01 14:04:01","title":"Hermitian null loci","abstract":"We establish a transcendental generalization of Nakamaye's theorem to compact complex manifolds when the form is not assumed to be closed. We apply the recent analytic technique developed by Collins--Tosatti to show that the non-Hermitian locus of a nef and big $(1,1)$-form, which is not necessarily closed, on a compact complex manifold equals the union of all positive-dimensional analytic subvarieties where the restriction of the form is not big (null locus). As an application, we can give an alternative proof of the Nakai--Moishezon criterion of Buchdahl and Lamari for complex surfaces and generalize this result in higher dimensions. This is also used for studying degenerate complex Monge--Amp\\`ere equations on compact Hermitian manifolds. Finally, we investigate finite time non-collapsing singularities of the Chern--Ricci flow, partially answering a question raised by Tosatti and Weinkove.","sentences":["We establish a transcendental generalization of Nakamaye's theorem to compact complex manifolds when the form is not assumed to be closed.","We apply the recent analytic technique developed by Collins--Tosatti to show that the non-Hermitian locus of a nef and big $(1,1)$-form, which is not necessarily closed, on a compact complex manifold equals the union of all positive-dimensional analytic subvarieties where the restriction of the form is not big (null locus).","As an application, we can give an alternative proof of the Nakai--Moishezon criterion of Buchdahl and Lamari for complex surfaces and generalize this result in higher dimensions.","This is also used for studying degenerate complex Monge--Amp\\`ere equations on compact Hermitian manifolds.","Finally, we investigate finite time non-collapsing singularities of the Chern--Ricci flow, partially answering a question raised by Tosatti and Weinkove."],"url":"http://arxiv.org/abs/2404.01126v1","category":"math.CV"}
{"created":"2024-04-01 13:46:42","title":"Variational approach for the two-body problem in a multiband extended-Hubbard model","abstract":"Considering a spin-up and a spin-down fermion in a generic tight-binding lattice with a multi-site basis, here we study the two-body problem using a multiband extended-Hubbard model with arbitrary but finite-ranged hopping and interaction parameters. We derive a linear eigenvalue problem for the entire two-body spectrum, alongside a nonlinear eigenvalue problem for the bound states in the form of a self-consistency equation. Our results are based on an exact variational approach and their versatility offers practical applications in a broad range of lattice geometries. As an illustration, we apply them to the linear-chain model and demonstrate that the resultant spin singlet and triplet bound states are in perfect agreement with the existing literature.","sentences":["Considering a spin-up and a spin-down fermion in a generic tight-binding lattice with a multi-site basis, here we study the two-body problem using a multiband extended-Hubbard model with arbitrary but finite-ranged hopping and interaction parameters.","We derive a linear eigenvalue problem for the entire two-body spectrum, alongside a nonlinear eigenvalue problem for the bound states in the form of a self-consistency equation.","Our results are based on an exact variational approach and their versatility offers practical applications in a broad range of lattice geometries.","As an illustration, we apply them to the linear-chain model and demonstrate that the resultant spin singlet and triplet bound states are in perfect agreement with the existing literature."],"url":"http://arxiv.org/abs/2404.01117v1","category":"cond-mat.supr-con"}
{"created":"2024-04-01 13:39:35","title":"Holographic reconstruction of flat spacetime","abstract":"The flat/CFT dictionary between the bulk gravitational theory and boundary conformal field theory is systematically developed in this paper. Asymptotically flat spacetime is built up by asymptotically AdS hyperboloid slices in terms of Fefferman Graham coordinates together with soft modes propagating between different slices near the null boundary. Then we construct the flat holography dictionary based on studying Einstein equation at zero and first order and it turns out that these correspond to the description of hard and soft sector for the field theory from the boundary point of view. The explicit expression for energy-stress tensor is also determined by performing holographic renormalisation on the Einstein Hilbert action. By studying the anomalies of the energy-stress tensor, we obtain the leading and subleading contribution to the central charge. Einstein equations in the bulk are related to the Ward identities of the boundary theory and we find that the boundary CFT energy-stress tensor is not conserved due to the existence of radiative soft modes which will generate the energy flow through the null boundary.","sentences":["The flat/CFT dictionary between the bulk gravitational theory and boundary conformal field theory is systematically developed in this paper.","Asymptotically flat spacetime is built up by asymptotically AdS hyperboloid slices in terms of Fefferman Graham coordinates together with soft modes propagating between different slices near the null boundary.","Then we construct the flat holography dictionary based on studying Einstein equation at zero and first order and it turns out that these correspond to the description of hard and soft sector for the field theory from the boundary point of view.","The explicit expression for energy-stress tensor is also determined by performing holographic renormalisation on the Einstein Hilbert action.","By studying the anomalies of the energy-stress tensor, we obtain the leading and subleading contribution to the central charge.","Einstein equations in the bulk are related to the Ward identities of the boundary theory and we find that the boundary CFT energy-stress tensor is not conserved due to the existence of radiative soft modes which will generate the energy flow through the null boundary."],"url":"http://arxiv.org/abs/2404.01113v1","category":"hep-th"}
{"created":"2024-04-01 13:38:16","title":"Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features","abstract":"Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.","sentences":["Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes.","We circumvent the problem by leveraging deep learning methods trained on billions of images.","We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models.","To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision.","In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision.","We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset."],"url":"http://arxiv.org/abs/2404.01112v2","category":"cs.CV"}
{"created":"2024-04-01 13:29:23","title":"Exact moments for trapped active particles: inertial impact on steady-state properties and re-entrance","abstract":"In this study, we investigate the behavior of inertial active Brownian particles in a $d$-dimensional harmonic trap in the presence of translational diffusion. While the solution of the Fokker-Planck equation is generally challenging, it can be utilized to compute the exact time evolution of all time-dependent dynamical moments using a Laplace transform approach. We present the explicit form for several moments of position and velocity in $d$-dimensions. An interplay of time scales assures that the effective diffusivity and steady-state kinetic temperature depend on both inertia and trap strength, unlike passive systems. We present detailed `phase diagrams' using kurtosis of velocity and position showing possibilities of re-entrance.","sentences":["In this study, we investigate the behavior of inertial active Brownian particles in a $d$-dimensional harmonic trap in the presence of translational diffusion.","While the solution of the Fokker-Planck equation is generally challenging, it can be utilized to compute the exact time evolution of all time-dependent dynamical moments using a Laplace transform approach.","We present the explicit form for several moments of position and velocity in $d$-dimensions.","An interplay of time scales assures that the effective diffusivity and steady-state kinetic temperature depend on both inertia and trap strength, unlike passive systems.","We present detailed `phase diagrams' using kurtosis of velocity and position showing possibilities of re-entrance."],"url":"http://arxiv.org/abs/2404.01107v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-01 13:23:57","title":"Second-Order Newton-Based Extremum Seeking for Multivariable Static Maps","abstract":"A second-order Newton-based extremum seeking (SONES) algorithm is presented to estimate directional inflection points for multivariable static maps. The design extends the first-order Newton-based extremum seeking algorithm that drives the system toward its peak point. This work provides perturbation matrices to estimate the second- and third-order derivatives necessary for implementation of the SONES. A set of conditions are provided for the probing frequencies that ensure accurate estimation of the derivatives. A differential Riccati filter is used to calculate the inverse of the third-order derivative. The local stability of the new algorithm is proven for general multivariable static maps using averaging analysis. The proposed algorithm ensures uniform convergence toward directional inflection point without requiring information about the curvature of the map and its gradient. Simulation results show the effectiveness of the proposed algorithm.","sentences":["A second-order Newton-based extremum seeking (SONES) algorithm is presented to estimate directional inflection points for multivariable static maps.","The design extends the first-order Newton-based extremum seeking algorithm that drives the system toward its peak point.","This work provides perturbation matrices to estimate the second- and third-order derivatives necessary for implementation of the SONES.","A set of conditions are provided for the probing frequencies that ensure accurate estimation of the derivatives.","A differential Riccati filter is used to calculate the inverse of the third-order derivative.","The local stability of the new algorithm is proven for general multivariable static maps using averaging analysis.","The proposed algorithm ensures uniform convergence toward directional inflection point without requiring information about the curvature of the map and its gradient.","Simulation results show the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.01103v1","category":"eess.SY"}
{"created":"2024-04-01 12:57:07","title":"Non-existence and multiplicity of positive solutions for Choquard equations with critical combined nonlinearities","abstract":"We study the non-existence and multiplicity of positive solutions of the nonlinear Choquard type equation $$ -\\Delta u+ \\varepsilon u=(I_\\alpha \\ast |u|^{p})|u|^{p-2}u+ |u|^{q-2}u, \\quad {\\rm in} \\ \\mathbb R^N, \\qquad (P_\\varepsilon)$$ where $N\\ge 3$ is an integer, $p\\in [\\frac{N+\\alpha}{N}, \\frac{N+\\alpha}{N-2}]$, $q\\in (2,\\frac{2N}{N-2}]$, $I_\\alpha$ is the Riesz potential of order $\\alpha\\in (0,N)$ and $\\varepsilon>0$ is a parameter. We fix one of $p,q$ as a critical exponent (in the sense of Hardy-Littlewood-Sobolev and Sobolev inequalities ) and view the others in $p,q,\\varepsilon, \\alpha$ as parameters, we find regions in the $(p,q,\\alpha, \\varepsilon)$-parameter space, such that the corresponding equation has no positive ground state or admits multiple positive solutions. This is a counterpart of the Brezis-Nirenberg Conjecture (Brezis-Nirenberg, CPAM, 1983) for nonlocal elliptic equation in the whole space. Particularly, some threshold results for the existence of ground states and some conditions which insure two positive solutions are obtained. These results are quite different in nature from the corresponding local equation with combined powers nonlinearity and reveal the special influence of the nonlocal term. To the best of our knowledge, the only two papers concerning the multiplicity of positive solutions of elliptic equations with critical growth nonlinearity are given by Atkinson, Peletier (Nonlinear Anal, 1986) for elliptic equation on a ball and Juncheng Wei, Yuanze Wu (Proc. Royal Soc. Edinburgh, 2022) for elliptic equation with a combined powers nonlinearity in the whole space. The ODE technique is main ingredient in the proofs of the above mentioned papers, however, ODE technique does not work any more in our model equation due to the presence of the nonlocal term.","sentences":["We study the non-existence and multiplicity of positive solutions of the nonlinear Choquard type equation $$ -\\Delta u+ \\varepsilon u=(I_\\alpha \\ast |u|^{p})|u|^{p-2}u+ |u|^{q-2}u, \\quad {\\rm in} \\ \\mathbb R^N, \\qquad (P_\\varepsilon)$$ where $N\\ge 3$ is an integer, $p\\in [\\frac{N+\\alpha}{N}, \\frac{N+\\alpha}{N-2}]$, $q\\in (2,\\frac{2N}{N-2}]$, $I_\\alpha$ is the Riesz potential of order $\\alpha\\in (0,N)$ and $\\varepsilon>0$ is a parameter.","We fix one of $p,q$ as a critical exponent (in the sense of Hardy-Littlewood-Sobolev and Sobolev inequalities ) and view the others in $p,q,\\varepsilon, \\alpha$ as parameters, we find regions in the $(p,q,\\alpha, \\varepsilon)$-parameter space, such that the corresponding equation has no positive ground state or admits multiple positive solutions.","This is a counterpart of the Brezis-Nirenberg Conjecture (Brezis-Nirenberg, CPAM, 1983) for nonlocal elliptic equation in the whole space.","Particularly, some threshold results for the existence of ground states and some conditions which insure two positive solutions are obtained.","These results are quite different in nature from the corresponding local equation with combined powers nonlinearity and reveal the special influence of the nonlocal term.","To the best of our knowledge, the only two papers concerning the multiplicity of positive solutions of elliptic equations with critical growth nonlinearity are given by Atkinson, Peletier (Nonlinear Anal, 1986) for elliptic equation on a ball and Juncheng Wei, Yuanze Wu (Proc.","Royal Soc.","Edinburgh, 2022) for elliptic equation with a combined powers nonlinearity in the whole space.","The ODE technique is main ingredient in the proofs of the above mentioned papers, however, ODE technique does not work any more in our model equation due to the presence of the nonlocal term."],"url":"http://arxiv.org/abs/2404.01093v1","category":"math.AP"}
{"created":"2024-04-01 12:33:20","title":"Stablity of charged scalar hair on a charged black hole","abstract":"The Israel-Carter theorem (also known as the ``no-hair theorem'') puts a restriction on the existence of parameters other than mass, electric charge, and angular momentum of a black hole. In this context, Bekenstein proposed no-hair theorems in various black hole models with neutral and electrically charged scalar fields.   In this paper, we take the Einstein-Maxwell-charged scalar model with an electrically charged scalar field gauge-coupled to the Maxwell field surrounding a charged black hole in the background of a static spherically symmetric metric. We ascertain the validity of all energy conditions coupled with the causality condition, suggesting the possibility of existence of charged hairy solutions. Consequently, we obtain, by exact numerical integration, detailed solutions of the field equations in this model. The solutions exhibit damped oscillatory behaviours for the charged scalar hair. We also find that the electric potential $V(r)$ is a monotonic function of $r$, as required by electrodynamics.   Furthermore, we find that the charged hairy solutions are stable against time-dependant perturbations about the static solutions. We ascertain their stability by carrying out dynamic stability analyses, employing two different methodologies. The first methodology is based on a Sturm-Liouville equation, whereas the second methodology employs a Schrodinger-like equation for the dynamic perturbations. We find stability of the charged hairy solutions in both stability analyses.","sentences":["The Israel-Carter theorem (also known as the ``no-hair theorem'') puts a restriction on the existence of parameters other than mass, electric charge, and angular momentum of a black hole.","In this context, Bekenstein proposed no-hair theorems in various black hole models with neutral and electrically charged scalar fields.   ","In this paper, we take the Einstein-Maxwell-charged scalar model with an electrically charged scalar field gauge-coupled to the Maxwell field surrounding a charged black hole in the background of a static spherically symmetric metric.","We ascertain the validity of all energy conditions coupled with the causality condition, suggesting the possibility of existence of charged hairy solutions.","Consequently, we obtain, by exact numerical integration, detailed solutions of the field equations in this model.","The solutions exhibit damped oscillatory behaviours for the charged scalar hair.","We also find that the electric potential $V(r)$ is a monotonic function of $r$, as required by electrodynamics.   ","Furthermore, we find that the charged hairy solutions are stable against time-dependant perturbations about the static solutions.","We ascertain their stability by carrying out dynamic stability analyses, employing two different methodologies.","The first methodology is based on a Sturm-Liouville equation, whereas the second methodology employs a Schrodinger-like equation for the dynamic perturbations.","We find stability of the charged hairy solutions in both stability analyses."],"url":"http://arxiv.org/abs/2404.01086v1","category":"gr-qc"}
{"created":"2024-04-01 12:07:06","title":"Machine Learning in High Energy Physics: A review of heavy-flavor jet tagging at the LHC","abstract":"The application of machine learning (ML) in high energy physics (HEP), specifically in heavy-flavor jet tagging at Large Hadron Collider (LHC) experiments, has experienced remarkable growth and innovation in the past decade. This review provides a detailed examination of current and past ML techniques in this domain. It starts by exploring various data representation methods and ML architectures, encompassing traditional ML algorithms and advanced deep learning techniques. Subsequent sections discuss specific instances of successful ML applications in jet flavor tagging in the ATLAS and CMS experiments at the LHC, ranging from basic fully-connected layers to graph neural networks employing attention mechanisms. To systematically categorize the advancements over the LHC's three runs, the paper classifies jet tagging algorithms into three generations, each characterized by specific data representation techniques and ML architectures. This classification aims to provide an overview of the chronological evolution in this field. Finally, a brief discussion about anticipated future developments and potential research directions in the field is presented.","sentences":["The application of machine learning (ML) in high energy physics (HEP), specifically in heavy-flavor jet tagging at Large Hadron Collider (LHC) experiments, has experienced remarkable growth and innovation in the past decade.","This review provides a detailed examination of current and past ML techniques in this domain.","It starts by exploring various data representation methods and ML architectures, encompassing traditional ML algorithms and advanced deep learning techniques.","Subsequent sections discuss specific instances of successful ML applications in jet flavor tagging in the ATLAS and CMS experiments at the LHC, ranging from basic fully-connected layers to graph neural networks employing attention mechanisms.","To systematically categorize the advancements over the LHC's three runs, the paper classifies jet tagging algorithms into three generations, each characterized by specific data representation techniques and ML architectures.","This classification aims to provide an overview of the chronological evolution in this field.","Finally, a brief discussion about anticipated future developments and potential research directions in the field is presented."],"url":"http://arxiv.org/abs/2404.01071v1","category":"hep-ex"}
{"created":"2024-04-01 12:02:51","title":"Dual-Unitary Classical Shadow Tomography","abstract":"We study operator spreading in random dual-unitary circuits within the context of classical shadow tomography. Primarily, we analyze the dynamics of the Pauli weight in one-dimensional qubit systems evolved by random two-local dual-unitary gates arranged in a brick-wall structure, ending with a final measurement layer. We do this by deriving general constraints on the Pauli weight transfer matrix and specializing to the case of dual-unitarity. We first show that dual-unitaries must have a minimal amount of entropy production. Remarkably, we find that operator spreading in these circuits has a rich structure resembling that of relativistic quantum field theories, with massless chiral excitations that can decay or fuse into each other, which we call left- or right-movers. We develop a mean-field description of the Pauli weight in terms of $\\rho(x,t)$, which represents the probability of having nontrivial support at site $x$ and depth $t$ starting from a fixed weight distribution. We develop an equation of state for $\\rho(x,t)$, and simulate it numerically using Monte Carlo simulations. Lastly, we demonstrate that the fast-thermalizing properties of dual-unitary circuits make them better at predicting large operators than shallow brick-wall Clifford circuits.","sentences":["We study operator spreading in random dual-unitary circuits within the context of classical shadow tomography.","Primarily, we analyze the dynamics of the Pauli weight in one-dimensional qubit systems evolved by random two-local dual-unitary gates arranged in a brick-wall structure, ending with a final measurement layer.","We do this by deriving general constraints on the Pauli weight transfer matrix and specializing to the case of dual-unitarity.","We first show that dual-unitaries must have a minimal amount of entropy production.","Remarkably, we find that operator spreading in these circuits has a rich structure resembling that of relativistic quantum field theories, with massless chiral excitations that can decay or fuse into each other, which we call left- or right-movers.","We develop a mean-field description of the Pauli weight in terms of $\\rho(x,t)$, which represents the probability of having nontrivial support at site $x$ and depth $t$ starting from a fixed weight distribution.","We develop an equation of state for $\\rho(x,t)$, and simulate it numerically using Monte Carlo simulations.","Lastly, we demonstrate that the fast-thermalizing properties of dual-unitary circuits make them better at predicting large operators than shallow brick-wall Clifford circuits."],"url":"http://arxiv.org/abs/2404.01068v1","category":"quant-ph"}
{"created":"2024-04-01 11:59:59","title":"Steering game dynamics towards desired outcomes","abstract":"The dynamic behavior of agents in games, which captures how their strategies evolve over time based on past interactions, can lead to a spectrum of undesirable behaviors, ranging from non-convergence to Nash equilibria to the emergence of limit cycles and chaos. To mitigate the effects of selfish behavior, central planners can use dynamic payments to guide strategic multi-agent systems toward stability and socially optimal outcomes. However, the effectiveness of such interventions critically relies on accurately predicting agents' responses to incentives and dynamically adjusting payments so that the system is guided towards the desired outcomes. These challenges are further amplified in real-time applications where the dynamics are unknown and only scarce data is available. To tackle this challenge, in this work we introduce the SIAR-MPC method, combining the recently introduced Side Information Assisted Regression (SIAR) method for system identification with Model Predictive Control (MPC). SIAR utilizes side-information constraints inherent to game theoretic applications to model agent responses to payments from scarce data, while MPC uses this model to facilitate dynamic payment adjustments. Our experiments demonstrate the efficiency of SIAR-MPC in guiding the system towards socially optimal equilibria, stabilizing chaotic behaviors, and avoiding specified regions of the state space. Comparative analyses in data-scarce settings show SIAR-MPC's superior performance over pairing MPC with Physics Informed Neural Networks (PINNs), a powerful system identification method that finds models satisfying specific constraints.","sentences":["The dynamic behavior of agents in games, which captures how their strategies evolve over time based on past interactions, can lead to a spectrum of undesirable behaviors, ranging from non-convergence to Nash equilibria to the emergence of limit cycles and chaos.","To mitigate the effects of selfish behavior, central planners can use dynamic payments to guide strategic multi-agent systems toward stability and socially optimal outcomes.","However, the effectiveness of such interventions critically relies on accurately predicting agents' responses to incentives and dynamically adjusting payments so that the system is guided towards the desired outcomes.","These challenges are further amplified in real-time applications where the dynamics are unknown and only scarce data is available.","To tackle this challenge, in this work we introduce the SIAR-MPC method, combining the recently introduced Side Information Assisted Regression (SIAR) method for system identification with Model Predictive Control (MPC).","SIAR utilizes side-information constraints inherent to game theoretic applications to model agent responses to payments from scarce data, while MPC uses this model to facilitate dynamic payment adjustments.","Our experiments demonstrate the efficiency of SIAR-MPC in guiding the system towards socially optimal equilibria, stabilizing chaotic behaviors, and avoiding specified regions of the state space.","Comparative analyses in data-scarce settings show SIAR-MPC's superior performance over pairing MPC with Physics Informed Neural Networks (PINNs), a powerful system identification method that finds models satisfying specific constraints."],"url":"http://arxiv.org/abs/2404.01066v1","category":"eess.SY"}
{"created":"2024-04-02 17:23:22","title":"Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners","abstract":"Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer during few-shot incremental sessions. In this paper, we argue that large models such as vision and language transformers pre-trained on large datasets can be excellent few-shot incremental learners. To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language transformers with prompting functions and knowledge distillation. Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained knowledge tuning (PKT) and two losses: entropy-based divergence loss and semantic knowledge distillation loss. Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet. Our implementation code is available at https://github.com/KHU-AGI/PriViLege.","sentences":["Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given.","FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18.","Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer during few-shot incremental sessions.","In this paper, we argue that large models such as vision and language transformers pre-trained on large datasets can be excellent few-shot incremental learners.","To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language transformers with prompting functions and knowledge distillation.","Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained knowledge tuning (PKT) and two losses: entropy-based divergence loss and semantic knowledge distillation loss.","Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet.","Our implementation code is available at https://github.com/KHU-AGI/PriViLege."],"url":"http://arxiv.org/abs/2404.02117v1","category":"cs.CV"}
{"created":"2024-04-02 16:48:20","title":"BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition","abstract":"Self-supervision has recently shown great promise for learning visual and auditory speech representations from unlabelled data. In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns speech representations entirely from raw audio-visual data. Our modifications to RAVEn enable BRAVEn to achieve state-of-the-art results among self-supervised methods in various settings. Moreover, we observe favourable scaling behaviour by increasing the amount of unlabelled data well beyond other self-supervised works. In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR on the LRS3 test set, with only 30 hours of labelled data and no external ASR models. Our results suggest that readily available unlabelled audio-visual data can largely replace costly transcribed data.","sentences":["Self-supervision has recently shown great promise for learning visual and auditory speech representations from unlabelled data.","In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns speech representations entirely from raw audio-visual data.","Our modifications to RAVEn enable BRAVEn to achieve state-of-the-art results among self-supervised methods in various settings.","Moreover, we observe favourable scaling behaviour by increasing the amount of unlabelled data well beyond other self-supervised works.","In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR on the LRS3 test set, with only 30 hours of labelled data and no external ASR models.","Our results suggest that readily available unlabelled audio-visual data can largely replace costly transcribed data."],"url":"http://arxiv.org/abs/2404.02098v1","category":"cs.CV"}
{"created":"2024-04-02 15:50:10","title":"BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights","abstract":"This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextual aspects of market sentiment. The integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors.","sentences":["This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction.","We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments.","Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks.","Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models.","The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends.","This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextual aspects of market sentiment.","The integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors."],"url":"http://arxiv.org/abs/2404.02053v1","category":"cs.CL"}
{"created":"2024-04-02 15:49:03","title":"Noise Masking Attacks and Defenses for Pretrained Speech Models","abstract":"Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.","sentences":["Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage.","Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise.","They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript.","In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders.","Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time!","We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks."],"url":"http://arxiv.org/abs/2404.02052v1","category":"cs.LG"}
{"created":"2024-04-02 15:34:47","title":"Transformers as Transducers","abstract":"We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people \"think like transformers,\" as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP. A corollary of our results is a new proof that transformer decoders are Turing-complete.","sentences":["We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions.","We do so using variants of RASP, a programming language designed to help people \"think like transformers,\" as an intermediate representation.","We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation).","Then, we introduce two new extensions.","B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions.","S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions.","Finally, we show that masked average-hard attention transformers can simulate S-RASP.","A corollary of our results is a new proof that transformer decoders are Turing-complete."],"url":"http://arxiv.org/abs/2404.02040v1","category":"cs.FL"}
{"created":"2024-04-02 14:43:36","title":"Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context","abstract":"We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\\%.","sentences":["We present the first self-supervised multilingual speech model trained exclusively on African speech.","The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa.","On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters.","Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\\%."],"url":"http://arxiv.org/abs/2404.02000v1","category":"cs.CL"}
{"created":"2024-04-02 13:42:29","title":"Synthetic Data for Robust Stroke Segmentation","abstract":"Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical imaging analysis in clinical settings, especially for stroke pathology, by enabling reliable segmentation across varied imaging sequences with reduced dependency on large annotated corpora. Code and weights available at https://github.com/liamchalcroft/SynthStroke.","sentences":["Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability.","We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies.","Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data.","Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data.","This contribution holds promise for advancing medical imaging analysis in clinical settings, especially for stroke pathology, by enabling reliable segmentation across varied imaging sequences with reduced dependency on large annotated corpora.","Code and weights available at https://github.com/liamchalcroft/SynthStroke."],"url":"http://arxiv.org/abs/2404.01946v1","category":"eess.IV"}
{"created":"2024-04-02 13:25:16","title":"Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks","abstract":"In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.","sentences":["In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation.","Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task.","However, they are computationally demanding and require careful fine-tuning of the produced outputs.","A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models.","Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment.","Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%.","Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length.","Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language."],"url":"http://arxiv.org/abs/2404.01932v1","category":"cs.RO"}
{"created":"2024-04-02 12:39:44","title":"Automatic Derivation of an Optimal Task Frame for Learning and Controlling Contact-Rich Tasks","abstract":"This study investigates learning from demonstration (LfD) for contact-rich tasks. The procedure for choosing a task frame to express the learned signals for the motion and interaction wrench is often omitted or using expert insight. This article presents a procedure to derive the optimal task frame from motion and wrench data recorded during the demonstration. The procedure is based on two principles that are hypothesized to underpin the control configuration targeted by an expert, and assumes task frame origins and orientations that are fixed to either the world or the robot tool. It is rooted in screw theory, is entirely probabilistic and does not involve any hyperparameters. The procedure was validated by demonstrating several tasks, including surface following and manipulation of articulated objects, showing good agreement between the obtained and the assumed expert task frames. To validate the performance of the learned tasks by a UR10e robot, a constraint-based controller was designed based on the derived task frames and the learned data expressed therein. These experiments showed the effectiveness and versatility of the proposed approach. The task frame derivation approach fills a gap in the state of the art of LfD, bringing LfD for contact-rich tasks closer to practical application.","sentences":["This study investigates learning from demonstration (LfD) for contact-rich tasks.","The procedure for choosing a task frame to express the learned signals for the motion and interaction wrench is often omitted or using expert insight.","This article presents a procedure to derive the optimal task frame from motion and wrench data recorded during the demonstration.","The procedure is based on two principles that are hypothesized to underpin the control configuration targeted by an expert, and assumes task frame origins and orientations that are fixed to either the world or the robot tool.","It is rooted in screw theory, is entirely probabilistic and does not involve any hyperparameters.","The procedure was validated by demonstrating several tasks, including surface following and manipulation of articulated objects, showing good agreement between the obtained and the assumed expert task frames.","To validate the performance of the learned tasks by a UR10e robot, a constraint-based controller was designed based on the derived task frames and the learned data expressed therein.","These experiments showed the effectiveness and versatility of the proposed approach.","The task frame derivation approach fills a gap in the state of the art of LfD, bringing LfD for contact-rich tasks closer to practical application."],"url":"http://arxiv.org/abs/2404.01900v1","category":"cs.RO"}
{"created":"2024-04-02 12:15:37","title":"Adversarial Combinatorial Bandits with Switching Costs","abstract":"We study the problem of adversarial combinatorial bandit with a switching cost $\\lambda$ for a switch of each selected arm in each round, considering both the bandit feedback and semi-bandit feedback settings. In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them. To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014). The lower bound for bandit feedback is $ \\tilde{\\Omega}\\big( (\\lambda K)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$ while that for semi-bandit feedback is $ \\tilde{\\Omega}\\big( (\\lambda K I)^{\\frac{1}{3}} T^{\\frac{2}{3}}\\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round. To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batches to restrict the number of switches between actions. For the bandit feedback setting, where only the total loss of the combinatorial arm is observed, we introduce the Batched-Exp2 algorithm which achieves a regret upper bound of $\\tilde{O}\\big((\\lambda K)^{\\frac{1}{3}}T^{\\frac{2}{3}}I^{\\frac{4}{3}}\\big)$ as $T$ tends to infinity. In the semi-bandit feedback setting, where all losses for the combinatorial arm are observed, we propose the Batched-BROAD algorithm which achieves a regret upper bound of $\\tilde{O}\\big( (\\lambda K)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$.","sentences":["We study the problem of adversarial combinatorial bandit with a switching cost $\\lambda$ for a switch of each selected arm in each round, considering both the bandit feedback and semi-bandit feedback settings.","In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them.","To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014).","The lower bound for bandit feedback is $ \\tilde{\\Omega}\\big( (\\lambda K)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$ while that for semi-bandit feedback is $ \\tilde{\\Omega}\\big( (\\lambda K I)^{\\frac{1}{3}} T^{\\frac{2}{3}}\\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round.","To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batches to restrict the number of switches between actions.","For the bandit feedback setting, where only the total loss of the combinatorial arm is observed, we introduce the Batched-Exp2 algorithm which achieves a regret upper bound of $\\tilde{O}\\big((\\lambda K)^{\\frac{1}{3}}T^{\\frac{2}{3}}I^{\\frac{4}{3}}\\big)$ as $T$ tends to infinity.","In the semi-bandit feedback setting, where all losses for the combinatorial arm are observed, we propose the Batched-BROAD algorithm which achieves a regret upper bound of $\\tilde{O}\\big( (\\lambda K)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$."],"url":"http://arxiv.org/abs/2404.01883v1","category":"stat.ML"}
{"created":"2024-04-02 11:35:05","title":"Detecting Gender Bias in Course Evaluations","abstract":"An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp. We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner. Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found. Here we present the results from the work so far, but this is an ongoing project and there is more work to do.","sentences":["An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp.","We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner.","Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found.","Here we present the results from the work so far, but this is an ongoing project and there is more work to do."],"url":"http://arxiv.org/abs/2404.01857v1","category":"cs.LG"}
{"created":"2024-04-02 11:12:42","title":"Accelerating Transformer Pre-Training with 2:4 Sparsity","abstract":"Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a \"flip rate\" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that a combination of our methods reaches the best performance on multiple Transformers among different 2:4 training methods, while actual acceleration can be observed on different shapes of Transformer block.","sentences":["Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage.","NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent.","In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training.","First, we define a \"flip rate\" to monitor the stability of a 2:4 training process.","Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training.","Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss.","Experiments show that a combination of our methods reaches the best performance on multiple Transformers among different 2:4 training methods, while actual acceleration can be observed on different shapes of Transformer block."],"url":"http://arxiv.org/abs/2404.01847v1","category":"cs.LG"}
{"created":"2024-04-02 09:11:58","title":"M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets","abstract":"In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.","sentences":["In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention.","However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts.","While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process.","Our work opens up new avenues for sentiment-related research within the research community.","Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings.","Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well."],"url":"http://arxiv.org/abs/2404.01753v1","category":"cs.CL"}
{"created":"2024-04-02 07:08:15","title":"Task Integration Distillation for Object Detectors","abstract":"Knowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.","sentences":["Knowledge distillation is a widely adopted technique for model lightening.","However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory.","Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task.","This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects.","Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy.","By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection.","Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas.","By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation.","This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs."],"url":"http://arxiv.org/abs/2404.01699v1","category":"cs.CV"}
{"created":"2024-04-02 06:49:38","title":"RefQSR: Reference-based Quantization for Image Super-Resolution Networks","abstract":"Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation. Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments. As a promising solution for computationally efficient network design, network quantization has been extensively studied. However, existing quantization methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study. We introduce a novel method called reference-based quantization for image super-resolution (RefQSR) that applies high-bit quantization to several representative patches and uses them as references for low-bit quantization of the rest of the patches in an image. To this end, we design dedicated patch clustering and reference-based quantization modules and integrate them into existing SISR network quantization methods. The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and quantization methods.","sentences":["Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation.","Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments.","As a promising solution for computationally efficient network design, network quantization has been extensively studied.","However, existing quantization methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study.","We introduce a novel method called reference-based quantization for image super-resolution (RefQSR) that applies high-bit quantization to several representative patches and uses them as references for low-bit quantization of the rest of the patches in an image.","To this end, we design dedicated patch clustering and reference-based quantization modules and integrate them into existing SISR network quantization methods.","The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and quantization methods."],"url":"http://arxiv.org/abs/2404.01690v1","category":"cs.CV"}
{"created":"2024-04-02 05:32:39","title":"EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis","abstract":"Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: https://tanshuai0219.github.io/EDTalk/","sentences":["Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation.","This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods.","To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk).","Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs.","Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively.","Each space is characterized by a set of learnable bases whose linear combinations define specific motions.","To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge.","The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input.","Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis.","Experiments are conducted to demonstrate the effectiveness of EDTalk.","We recommend watching the project website: https://tanshuai0219.github.io/EDTalk/"],"url":"http://arxiv.org/abs/2404.01647v1","category":"cs.CV"}
{"created":"2024-04-02 05:30:54","title":"Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection","abstract":"Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization. In this paper, we introduce a pioneering methodology that amalgamates Transformer-based forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our approach harnesses the predictive capabilities of Transformers to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products. Through extensive simulations in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets.","sentences":["Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization.","In this paper, we introduce a pioneering methodology that amalgamates Transformer-based forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection.","Our approach harnesses the predictive capabilities of Transformers to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products.","Through extensive simulations in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets."],"url":"http://arxiv.org/abs/2404.01646v1","category":"cs.LG"}
{"created":"2024-04-02 03:36:07","title":"LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network","abstract":"Remote sensing target detection aims to identify and locate critical targets within remote sensing images, finding extensive applications in agriculture and urban planning. Feature pyramid networks (FPNs) are commonly used to extract multi-scale features. However, existing FPNs often overlook extracting low-level positional information and fine-grained context interaction. To address this, we propose a novel location refined feature pyramid network (LR-FPN) to enhance the extraction of shallow positional information and facilitate fine-grained context interaction. The LR-FPN consists of two primary modules: the shallow position information extraction module (SPIEM) and the contextual interaction module (CIM). Specifically, SPIEM first maximizes the retention of solid location information of the target by simultaneously extracting positional and saliency information from the low-level feature map. Subsequently, CIM injects this robust location information into different layers of the original FPN through spatial and channel interaction, explicitly enhancing the object area. Moreover, in spatial interaction, we introduce a simple local and non-local interaction strategy to learn and retain the saliency information of the object. Lastly, the LR-FPN can be readily integrated into common object detection frameworks to improve performance significantly. Extensive experiments on two large-scale remote sensing datasets (i.e., DOTAV1.0 and HRSC2016) demonstrate that the proposed LR-FPN is superior to state-of-the-art object detection approaches. Our code and models will be publicly available.","sentences":["Remote sensing target detection aims to identify and locate critical targets within remote sensing images, finding extensive applications in agriculture and urban planning.","Feature pyramid networks (FPNs) are commonly used to extract multi-scale features.","However, existing FPNs often overlook extracting low-level positional information and fine-grained context interaction.","To address this, we propose a novel location refined feature pyramid network (LR-FPN) to enhance the extraction of shallow positional information and facilitate fine-grained context interaction.","The LR-FPN consists of two primary modules: the shallow position information extraction module (SPIEM) and the contextual interaction module (CIM).","Specifically, SPIEM first maximizes the retention of solid location information of the target by simultaneously extracting positional and saliency information from the low-level feature map.","Subsequently, CIM injects this robust location information into different layers of the original FPN through spatial and channel interaction, explicitly enhancing the object area.","Moreover, in spatial interaction, we introduce a simple local and non-local interaction strategy to learn and retain the saliency information of the object.","Lastly, the LR-FPN can be readily integrated into common object detection frameworks to improve performance significantly.","Extensive experiments on two large-scale remote sensing datasets (i.e., DOTAV1.0 and HRSC2016) demonstrate that the proposed LR-FPN is superior to state-of-the-art object detection approaches.","Our code and models will be publicly available."],"url":"http://arxiv.org/abs/2404.01614v1","category":"cs.CV"}
{"created":"2024-04-02 03:18:28","title":"Audio Simulation for Sound Source Localization in Virtual Evironment","abstract":"Non-line-of-sight localization in signal-deprived environments is a challenging yet pertinent problem. Acoustic methods in such predominantly indoor scenarios encounter difficulty due to the reverberant nature. In this study, we aim to locate sound sources to specific locations within a virtual environment by leveraging physically grounded sound propagation simulations and machine learning methods. This process attempts to overcome the issue of data insufficiency to localize sound sources to their location of occurrence especially in post-event localization. We achieve 0.786+/- 0.0136 F1-score using an audio transformer spectrogram approach.","sentences":["Non-line-of-sight localization in signal-deprived environments is a challenging yet pertinent problem.","Acoustic methods in such predominantly indoor scenarios encounter difficulty due to the reverberant nature.","In this study, we aim to locate sound sources to specific locations within a virtual environment by leveraging physically grounded sound propagation simulations and machine learning methods.","This process attempts to overcome the issue of data insufficiency to localize sound sources to their location of occurrence especially in post-event localization.","We achieve 0.786+/- 0.0136 F1-score using an audio transformer spectrogram approach."],"url":"http://arxiv.org/abs/2404.01611v1","category":"cs.LG"}
{"created":"2024-04-02 02:52:05","title":"WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing","abstract":"The surge in interest regarding image dehazing has led to notable advancements in deep learning-based single image dehazing approaches, exhibiting impressive performance in recent studies. Despite these strides, many existing methods fall short in meeting the efficiency demands of practical applications. In this paper, we introduce WaveDH, a novel and compact ConvNet designed to address this efficiency gap in image dehazing. Our WaveDH leverages wavelet sub-bands for guided up-and-downsampling and frequency-aware feature refinement. The key idea lies in utilizing wavelet decomposition to extract low-and-high frequency components from feature levels, allowing for faster processing while upholding high-quality reconstruction. The downsampling block employs a novel squeeze-and-attention scheme to optimize the feature downsampling process in a structurally compact manner through wavelet domain learning, preserving discriminative features while discarding noise components. In our upsampling block, we introduce a dual-upsample and fusion mechanism to enhance high-frequency component awareness, aiding in the reconstruction of high-frequency details. Departing from conventional dehazing methods that treat low-and-high frequency components equally, our feature refinement block strategically processes features with a frequency-aware approach. By employing a coarse-to-fine methodology, it not only refines the details at frequency levels but also significantly optimizes computational costs. The refinement is performed in a maximum 8x downsampled feature space, striking a favorable efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our method, WaveDH, outperforms many state-of-the-art methods on several image dehazing benchmarks with significantly reduced computational costs. Our code is available at https://github.com/AwesomeHwang/WaveDH.","sentences":["The surge in interest regarding image dehazing has led to notable advancements in deep learning-based single image dehazing approaches, exhibiting impressive performance in recent studies.","Despite these strides, many existing methods fall short in meeting the efficiency demands of practical applications.","In this paper, we introduce WaveDH, a novel and compact ConvNet designed to address this efficiency gap in image dehazing.","Our WaveDH leverages wavelet sub-bands for guided up-and-downsampling and frequency-aware feature refinement.","The key idea lies in utilizing wavelet decomposition to extract low-and-high frequency components from feature levels, allowing for faster processing while upholding high-quality reconstruction.","The downsampling block employs a novel squeeze-and-attention scheme to optimize the feature downsampling process in a structurally compact manner through wavelet domain learning, preserving discriminative features while discarding noise components.","In our upsampling block, we introduce a dual-upsample and fusion mechanism to enhance high-frequency component awareness, aiding in the reconstruction of high-frequency details.","Departing from conventional dehazing methods that treat low-and-high frequency components equally, our feature refinement block strategically processes features with a frequency-aware approach.","By employing a coarse-to-fine methodology, it not only refines the details at frequency levels but also significantly optimizes computational costs.","The refinement is performed in a maximum 8x downsampled feature space, striking a favorable efficiency-vs-accuracy trade-off.","Extensive experiments demonstrate that our method, WaveDH, outperforms many state-of-the-art methods on several image dehazing benchmarks with significantly reduced computational costs.","Our code is available at https://github.com/AwesomeHwang/WaveDH."],"url":"http://arxiv.org/abs/2404.01604v1","category":"cs.CV"}
{"created":"2024-04-02 02:20:47","title":"Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection","abstract":"In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects' poses given past observations, thus explicitly guiding to learn objects' temporal cues. To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current objects' poses given past observations and (ii) another branch that detects objects based on the current and past observations. The features predicting the current objects from branch (i) is fused into branch (ii) to transfer predictive knowledge. We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance. Our model can be used plug-and-play, showing consistent performance gain.","sentences":["In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams.","Recent work has focused on spatially aligning BEV-based features over timesteps.","However, this is often limited as its gain does not scale well with long-term past observations.","To address this, we advocate for supervising a model to predict objects' poses given past observations, thus explicitly guiding to learn objects' temporal cues.","To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current objects' poses given past observations and (ii) another branch that detects objects based on the current and past observations.","The features predicting the current objects from branch (i) is fused into branch (ii) to transfer predictive knowledge.","We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance.","Our model can be used plug-and-play, showing consistent performance gain."],"url":"http://arxiv.org/abs/2404.01580v1","category":"cs.CV"}
{"created":"2024-04-02 02:10:28","title":"Real-Time Hybrid Simulation for Infrastructure Degradation Assessment: Conceptual Framework and Illustrative Application","abstract":"To date, the prospect of using real-time hybrid simulation (RTHS) to study the effects of long-term or 'wear-and-tear' loads, such as exposure to harmful environmental conditions or fatigue, has remained underexplored. This study presents a conceptual framework to assess the impact of long-term degradation on infrastructure systems. The framework integrates the capabilities of RTHS with accelerated degradation techniques to evaluate the behavior of a degrading system over time. Experimental results obtained in this way not only capture the complex interactions but also provide a reliability-based method to determine the expected time-to-failure of the evaluated system. The developed framework is demonstrated using a virtual RTHS platform designed to test fiber-reinforced elastomeric isolators.","sentences":["To date, the prospect of using real-time hybrid simulation (RTHS) to study the effects of long-term or 'wear-and-tear' loads, such as exposure to harmful environmental conditions or fatigue, has remained underexplored.","This study presents a conceptual framework to assess the impact of long-term degradation on infrastructure systems.","The framework integrates the capabilities of RTHS with accelerated degradation techniques to evaluate the behavior of a degrading system over time.","Experimental results obtained in this way not only capture the complex interactions but also provide a reliability-based method to determine the expected time-to-failure of the evaluated system.","The developed framework is demonstrated using a virtual RTHS platform designed to test fiber-reinforced elastomeric isolators."],"url":"http://arxiv.org/abs/2404.01575v1","category":"eess.SY"}
{"created":"2024-04-02 02:07:00","title":"Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery","abstract":"In this article, we explore the potential of zero-shot Large Multimodal Models (LMMs) in the domain of drone perception. We focus on person detection and action recognition tasks and evaluate two prominent LMMs, namely YOLO-World and GPT-4V(ision) using a publicly available dataset captured from aerial views. Traditional deep learning approaches rely heavily on large and high-quality training datasets. However, in certain robotic settings, acquiring such datasets can be resource-intensive or impractical within a reasonable timeframe. The flexibility of prompt-based Large Multimodal Models (LMMs) and their exceptional generalization capabilities have the potential to revolutionize robotics applications in these scenarios. Our findings suggest that YOLO-World demonstrates good detection performance. GPT-4V struggles with accurately classifying action classes but delivers promising results in filtering out unwanted region proposals and in providing a general description of the scenery. This research represents an initial step in leveraging LMMs for drone perception and establishes a foundation for future investigations in this area.","sentences":["In this article, we explore the potential of zero-shot Large Multimodal Models (LMMs) in the domain of drone perception.","We focus on person detection and action recognition tasks and evaluate two prominent LMMs, namely YOLO-World and GPT-4V(ision) using a publicly available dataset captured from aerial views.","Traditional deep learning approaches rely heavily on large and high-quality training datasets.","However, in certain robotic settings, acquiring such datasets can be resource-intensive or impractical within a reasonable timeframe.","The flexibility of prompt-based Large Multimodal Models (LMMs) and their exceptional generalization capabilities have the potential to revolutionize robotics applications in these scenarios.","Our findings suggest that YOLO-World demonstrates good detection performance.","GPT-4V struggles with accurately classifying action classes but delivers promising results in filtering out unwanted region proposals and in providing a general description of the scenery.","This research represents an initial step in leveraging LMMs for drone perception and establishes a foundation for future investigations in this area."],"url":"http://arxiv.org/abs/2404.01571v1","category":"cs.CV"}
{"created":"2024-04-02 01:57:08","title":"Two-Phase Multi-Dose-Level PET Image Reconstruction with Dose Level Awareness","abstract":"To obtain high-quality positron emission tomography (PET) while minimizing radiation exposure, a range of methods have been designed to reconstruct standard-dose PET (SPET) from corresponding low-dose PET (LPET) images. However, most current methods merely learn the mapping between single-dose-level LPET and SPET images, but omit the dose disparity of LPET images in clinical scenarios. In this paper, to reconstruct high-quality SPET images from multi-dose-level LPET images, we design a novel two-phase multi-dose-level PET reconstruction algorithm with dose level awareness, containing a pre-training phase and a SPET prediction phase. Specifically, the pre-training phase is devised to explore both fine-grained discriminative features and effective semantic representation. The SPET prediction phase adopts a coarse prediction network utilizing pre-learned dose level prior to generate preliminary result, and a refinement network to precisely preserve the details. Experiments on MICCAI 2022 Ultra-low Dose PET Imaging Challenge Dataset have demonstrated the superiority of our method.","sentences":["To obtain high-quality positron emission tomography (PET) while minimizing radiation exposure, a range of methods have been designed to reconstruct standard-dose PET (SPET) from corresponding low-dose PET (LPET) images.","However, most current methods merely learn the mapping between single-dose-level LPET and SPET images, but omit the dose disparity of LPET images in clinical scenarios.","In this paper, to reconstruct high-quality SPET images from multi-dose-level LPET images, we design a novel two-phase multi-dose-level PET reconstruction algorithm with dose level awareness, containing a pre-training phase and a SPET prediction phase.","Specifically, the pre-training phase is devised to explore both fine-grained discriminative features and effective semantic representation.","The SPET prediction phase adopts a coarse prediction network utilizing pre-learned dose level prior to generate preliminary result, and a refinement network to precisely preserve the details.","Experiments on MICCAI 2022 Ultra-low Dose PET Imaging Challenge Dataset have demonstrated the superiority of our method."],"url":"http://arxiv.org/abs/2404.01563v1","category":"eess.IV"}
{"created":"2024-04-02 01:42:15","title":"FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion","abstract":"The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning. Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions.   To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning.In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning.","sentences":["The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot.","However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks.","As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning.","Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions.   ","To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions.","Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning.","While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning.","In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder.","In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis.","Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning."],"url":"http://arxiv.org/abs/2404.01554v1","category":"cs.SE"}
{"created":"2024-04-01 23:34:28","title":"ML KPI Prediction in 5G and B5G Networks","abstract":"Network operators are facing new challenges when meeting the needs of their customers. The challenges arise due to the rise of new services, such as HD video streaming, IoT, autonomous driving, etc., and the exponential growth of network traffic. In this context, 5G and B5G networks have been evolving to accommodate a wide range of applications and use cases. Additionally, this evolution brings new features, like the ability to create multiple end-to-end isolated virtual networks using network slicing. Nevertheless, to ensure the quality of service, operators must maintain and optimize their networks in accordance with the key performance indicators (KPIs) and the slice service-level agreements (SLAs).   In this paper, we introduce a machine learning (ML) model used to estimate throughput in 5G and B5G networks with end-to-end (E2E) network slices. Then, we combine the predicted throughput with the current network state to derive an estimate of other network KPIs, which can be used to further improve service assurance. To assess the efficiency of our solution, a performance metric was proposed. Numerical evaluations demonstrate that our KPI prediction model outperforms those derived from other methods with the same or nearly the same computational time.","sentences":["Network operators are facing new challenges when meeting the needs of their customers.","The challenges arise due to the rise of new services, such as HD video streaming, IoT, autonomous driving, etc., and the exponential growth of network traffic.","In this context, 5G and B5G networks have been evolving to accommodate a wide range of applications and use cases.","Additionally, this evolution brings new features, like the ability to create multiple end-to-end isolated virtual networks using network slicing.","Nevertheless, to ensure the quality of service, operators must maintain and optimize their networks in accordance with the key performance indicators (KPIs) and the slice service-level agreements (SLAs).   ","In this paper, we introduce a machine learning (ML) model used to estimate throughput in 5G and B5G networks with end-to-end (E2E) network slices.","Then, we combine the predicted throughput with the current network state to derive an estimate of other network KPIs, which can be used to further improve service assurance.","To assess the efficiency of our solution, a performance metric was proposed.","Numerical evaluations demonstrate that our KPI prediction model outperforms those derived from other methods with the same or nearly the same computational time."],"url":"http://arxiv.org/abs/2404.01530v1","category":"cs.NI"}
{"created":"2024-04-01 22:53:47","title":"Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation","abstract":"We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state-of-the-art results for the unsupervised video action segmentation task.","sentences":["We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem.","By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes.","Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency.","Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent.","We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training.","We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state-of-the-art results for the unsupervised video action segmentation task."],"url":"http://arxiv.org/abs/2404.01518v1","category":"cs.CV"}
{"created":"2024-04-01 21:49:05","title":"MosquitoFusion: A Multiclass Dataset for Real-Time Detection of Mosquitoes, Swarms, and Breeding Sites Using Deep Learning","abstract":"In this paper, we present an integrated approach to real-time mosquito detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse images and leverage cutting-edge technologies, specifically computer vision, to automate the identification of Mosquitoes, Swarms, and Breeding Sites. The pre-trained YOLOv8 model, trained on this dataset, achieved a mean Average Precision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%. The integration of Geographic Information Systems (GIS) further enriches the depth of our analysis, providing valuable insights into spatial patterns. The dataset and code are available at https://github.com/faiyazabdullah/MosquitoFusion.","sentences":["In this paper, we present an integrated approach to real-time mosquito detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse images and leverage cutting-edge technologies, specifically computer vision, to automate the identification of Mosquitoes, Swarms, and Breeding Sites.","The pre-trained YOLOv8 model, trained on this dataset, achieved a mean Average Precision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%.","The integration of Geographic Information Systems (GIS) further enriches the depth of our analysis, providing valuable insights into spatial patterns.","The dataset and code are available at https://github.com/faiyazabdullah/MosquitoFusion."],"url":"http://arxiv.org/abs/2404.01501v1","category":"cs.CV"}
{"created":"2024-04-01 21:23:03","title":"SUGAR: Pre-training 3D Visual Representations for Robotics","abstract":"Learning generalizable visual representations from Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D representation learning has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation. Experimental results show that SUGAR's 3D representation outperforms state-of-the-art 2D and 3D representations.","sentences":["Learning generalizable visual representations from Internet data has yielded promising results for robotics.","Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes.","Meanwhile, 3D representation learning has been limited to single-object understanding.","To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds.","We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation.","SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes.","We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation.","Experimental results show that SUGAR's 3D representation outperforms state-of-the-art 2D and 3D representations."],"url":"http://arxiv.org/abs/2404.01491v1","category":"cs.CV"}
{"created":"2024-04-02 17:13:32","title":"Counting rational points on the sphere with bounded denominator","abstract":"We give an optimal bound for the remainder when counting the number of rational points on the $n$-dimensional sphere with bounded denominator for any $n\\geq 3$.","sentences":["We give an optimal bound for the remainder when counting the number of rational points on the $n$-dimensional sphere with bounded denominator for any $n\\geq 3$."],"url":"http://arxiv.org/abs/2404.02114v1","category":"math.NT"}
{"created":"2024-04-02 16:31:24","title":"Regularity results for almost-minimizers of anisotropic free interface problem with H\u00f6lder dependence on the position","abstract":"We establish regularity results for almost-minimizers of a class of variational problems involving both bulk and interface energies. The bulk energy is of Dirichlet type. The surface energy exhibits anisotropic behaviour and is defined by means of an ellipsoidal density that is H\\\"older continuous with respect to the position variable.","sentences":["We establish regularity results for almost-minimizers of a class of variational problems involving both bulk and interface energies.","The bulk energy is of Dirichlet type.","The surface energy exhibits anisotropic behaviour and is defined by means of an ellipsoidal density that is H\\\"older continuous with respect to the position variable."],"url":"http://arxiv.org/abs/2404.02086v1","category":"math.OC"}
{"created":"2024-04-02 15:18:22","title":"Using the Empirical Attainment Function for Analyzing Single-objective Black-box Optimization Algorithms","abstract":"A widely accepted way to assess the performance of iterative black-box optimizers is to analyze their empirical cumulative distribution function (ECDF) of pre-defined quality targets achieved not later than a given runtime. In this work, we consider an alternative approach, based on the empirical attainment function (EAF) and we show that the target-based ECDF is an approximation of the EAF. We argue that the EAF has several advantages over the target-based ECDF. In particular, it does not require defining a priori quality targets per function, captures performance differences more precisely, and enables the use of additional summary statistics that enrich the analysis. We also show that the average area over the convergence curves is a simpler-to-calculate, but equivalent, measure of anytime performance. To facilitate the accessibility of the EAF, we integrate a module to compute it into the IOHanalyzer platform. Finally, we illustrate the use of the EAF via synthetic examples and via the data available for the BBOB suite.","sentences":["A widely accepted way to assess the performance of iterative black-box optimizers is to analyze their empirical cumulative distribution function (ECDF) of pre-defined quality targets achieved not later than a given runtime.","In this work, we consider an alternative approach, based on the empirical attainment function (EAF) and we show that the target-based ECDF is an approximation of the EAF.","We argue that the EAF has several advantages over the target-based ECDF.","In particular, it does not require defining a priori quality targets per function, captures performance differences more precisely, and enables the use of additional summary statistics that enrich the analysis.","We also show that the average area over the convergence curves is a simpler-to-calculate, but equivalent, measure of anytime performance.","To facilitate the accessibility of the EAF, we integrate a module to compute it into the IOHanalyzer platform.","Finally, we illustrate the use of the EAF via synthetic examples and via the data available for the BBOB suite."],"url":"http://arxiv.org/abs/2404.02031v1","category":"math.OC"}
{"created":"2024-04-02 10:13:18","title":"Surface Reconstruction from Gaussian Splatting via Novel Stereo Views","abstract":"The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements' locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: https://gs2mesh.github.io/.","sentences":["The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation.","It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions.","And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   ","We propose a novel approach for surface reconstruction from Gaussian splatting models.","Rather than relying on the Gaussian elements' locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS.","To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method.","We then combine the extracted RGB-D images into a geometrically consistent surface.","The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   ","We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities.","Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models.","Project page: https://gs2mesh.github.io/."],"url":"http://arxiv.org/abs/2404.01810v1","category":"cs.CV"}
{"created":"2024-04-02 09:38:18","title":"MHONGOOSE - A MeerKAT Nearby Galaxy HI Survey","abstract":"The MHONGOOSE (MeerKAT HI Observations of Nearby Galactic Objects: Observing Southern Emitters) survey maps the distribution and kinematics of the neutral atomic hydrogen (HI) gas in and around 30 nearby star-forming spiral and dwarf galaxies to extremely low HI column densities. The HI column density sensitivity (3 sigma over 16 km/s) ranges from ~ 5 x 10^{17} cm^{-2} at 90'' resolution to ~4 x 10^{19} cm^{-2} at the highest resolution of 7''. The HI mass sensitivity (3 sigma over 50 km/s) is ~5.5 X 10^5 M_sun at a distance of 10 Mpc (the median distance of the sample galaxies). The velocity resolution of the data is 1.4 km/s. One of the main science goals of the survey is the detection of cold, accreting gas in the outskirts of the sample galaxies. The sample was selected to cover a range in HI masses, from 10^7 M_sun to almost 10^{11} M_sun, to optimally sample possible accretion scenarios and environments. The distance to the sample galaxies ranges from 3 to 23 Mpc. In this paper, we present the sample selection, survey design, and observation and reduction procedures. We compare the integrated HI fluxes based on the MeerKAT data with those derived from single-dish measurement and find good agreement, indicating that our MeerKAT observations are recovering all flux. We present HI moment maps of the entire sample based on the first ten percent of the survey data, and find that a comparison of the zeroth- and second-moment values shows a clear separation between the physical properties of the HI in areas with star formation and areas without, related to the formation of a cold neutral medium. Finally, we give an overview of the HI-detected companion and satellite galaxies in the 30 fields, five of which have not previously been catalogued. We find a clear relation between the number of companion galaxies and the mass of the main target galaxy.","sentences":["The MHONGOOSE (MeerKAT HI Observations of Nearby Galactic Objects: Observing Southern Emitters) survey maps the distribution and kinematics of the neutral atomic hydrogen (HI) gas in and around 30 nearby star-forming spiral and dwarf galaxies to extremely low HI column densities.","The HI column density sensitivity (3 sigma over 16 km/s) ranges from ~ 5 x 10^{17} cm^{-2} at 90'' resolution to ~4 x 10^{19} cm^{-2} at the highest resolution of 7''.","The HI mass sensitivity (3 sigma over 50 km/s) is ~5.5 X 10^5 M_sun at a distance of 10 Mpc (the median distance of the sample galaxies).","The velocity resolution of the data is 1.4 km/s. One of the main science goals of the survey is the detection of cold, accreting gas in the outskirts of the sample galaxies.","The sample was selected to cover a range in HI masses, from 10^7 M_sun to almost 10^{11} M_sun, to optimally sample possible accretion scenarios and environments.","The distance to the sample galaxies ranges from 3 to 23 Mpc.","In this paper, we present the sample selection, survey design, and observation and reduction procedures.","We compare the integrated HI fluxes based on the MeerKAT data with those derived from single-dish measurement and find good agreement, indicating that our MeerKAT observations are recovering all flux.","We present HI moment maps of the entire sample based on the first ten percent of the survey data, and find that a comparison of the zeroth- and second-moment values shows a clear separation between the physical properties of the HI in areas with star formation and areas without, related to the formation of a cold neutral medium.","Finally, we give an overview of the HI-detected companion and satellite galaxies in the 30 fields, five of which have not previously been catalogued.","We find a clear relation between the number of companion galaxies and the mass of the main target galaxy."],"url":"http://arxiv.org/abs/2404.01774v1","category":"astro-ph.GA"}
{"created":"2024-04-02 09:04:09","title":"Improving the accuracy and consistency of the energy quadratization method with an energy-optimized technique","abstract":"We propose an energy-optimized invariant energy quadratization method to solve the gradient flow models in this paper, which requires only one linear energy-optimized step to correct the auxiliary variables on each time step. In addition to inheriting the benefits of the baseline and relaxed invariant energy quadratization method, our approach has several other advantages. Firstly, in the process of correcting auxiliary variables, we can directly solve linear programming problem by the energy-optimized technique, which greatly simplifies the nonlinear optimization problem in the previous relaxed invariant energy quadratization method. Secondly, we construct new linear unconditionally energy stable schemes by applying backward Euler formulas and Crank-Nicolson formula, so that the accuracy in time can reach the first- and second-order. Thirdly, comparing with relaxation technique, the modified energy obtained by energy-optimized technique is closer to the original energy, meanwhile the accuracy and consistency of the numerical solutions can be improved. Ample numerical examples have been presented to demonstrate the accuracy, efficiency and energy stability of the proposed schemes.","sentences":["We propose an energy-optimized invariant energy quadratization method to solve the gradient flow models in this paper, which requires only one linear energy-optimized step to correct the auxiliary variables on each time step.","In addition to inheriting the benefits of the baseline and relaxed invariant energy quadratization method, our approach has several other advantages.","Firstly, in the process of correcting auxiliary variables, we can directly solve linear programming problem by the energy-optimized technique, which greatly simplifies the nonlinear optimization problem in the previous relaxed invariant energy quadratization method.","Secondly, we construct new linear unconditionally energy stable schemes by applying backward Euler formulas and Crank-Nicolson formula, so that the accuracy in time can reach the first- and second-order.","Thirdly, comparing with relaxation technique, the modified energy obtained by energy-optimized technique is closer to the original energy, meanwhile the accuracy and consistency of the numerical solutions can be improved.","Ample numerical examples have been presented to demonstrate the accuracy, efficiency and energy stability of the proposed schemes."],"url":"http://arxiv.org/abs/2404.01747v1","category":"math.NA"}
{"created":"2024-04-02 02:27:54","title":"Efficient Computation of Mean field Control based Barycenters from Reaction-Diffusion Systems","abstract":"We develop a class of barycenter problems based on mean field control problems in three dimensions with associated reactive-diffusion systems of unnormalized multi-species densities. This problem is the generalization of the Wasserstein barycenter problem for single probability density functions. The primary objective is to present a comprehensive framework for efficiently computing the proposed variational problem: generalized Benamou-Brenier formulas with multiple input density vectors as boundary conditions. Our approach involves the utilization of high-order finite element discretizations of the spacetime domain to achieve improved accuracy. The discrete optimization problem is then solved using the primal-dual hybrid gradient (PDHG) algorithm, a first-order optimization method for effectively addressing a wide range of constrained optimization problems. The efficacy and robustness of our proposed framework are illustrated through several numerical examples in three dimensions, such as the computation of the barycenter of multi-density systems consisting of Gaussian distributions and reactive-diffusive multi-density systems involving 3D voxel densities. Additional examples highlighting computations on 2D embedded surfaces are also provided.","sentences":["We develop a class of barycenter problems based on mean field control problems in three dimensions with associated reactive-diffusion systems of unnormalized multi-species densities.","This problem is the generalization of the Wasserstein barycenter problem for single probability density functions.","The primary objective is to present a comprehensive framework for efficiently computing the proposed variational problem: generalized Benamou-Brenier formulas with multiple input density vectors as boundary conditions.","Our approach involves the utilization of high-order finite element discretizations of the spacetime domain to achieve improved accuracy.","The discrete optimization problem is then solved using the primal-dual hybrid gradient (PDHG) algorithm, a first-order optimization method for effectively addressing a wide range of constrained optimization problems.","The efficacy and robustness of our proposed framework are illustrated through several numerical examples in three dimensions, such as the computation of the barycenter of multi-density systems consisting of Gaussian distributions and reactive-diffusive multi-density systems involving 3D voxel densities.","Additional examples highlighting computations on 2D embedded surfaces are also provided."],"url":"http://arxiv.org/abs/2404.01586v1","category":"math.OC"}
{"created":"2024-04-02 02:26:07","title":"FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets","abstract":"Frequent Subgraph Mining (FSM) is the process of identifying common subgraph patterns that surpass a predefined frequency threshold. While FSM is widely applicable in fields like bioinformatics, chemical analysis, and social network anomaly detection, its execution remains time-consuming and complex. This complexity stems from the need to recognize high-frequency subgraphs and ascertain if they exceed the set threshold. Current approaches to identifying these patterns often rely on edge or vertex extension methods. However, these strategies can introduce redundancies and cause increased latency. To address these challenges, this paper introduces a novel approach for identifying potential k-vertex patterns by combining two frequently observed (k - 1)-vertex patterns. This method optimizes the breadth-]first search, which allows for quicker search termination based on vertices count and support value. Another challenge in FSM is the validation of the presumed pattern against a specific threshold. Existing metrics, such as Maximum Independent Set (MIS) and Minimum Node Image (MNI), either demand significant computational time or risk overestimating pattern counts. Our innovative approach aligns with the MIS and identifies independent subgraphs. Through the \"Maximal Independent Set\" metric, this paper offers an efficient solution that minimizes latency and provides users with control over pattern overlap. Through extensive experimentation, our proposed method achieves an average of 10.58x speedup when compared to GraMi and an average 3x speedup when compared to T-FSM","sentences":["Frequent Subgraph Mining (FSM) is the process of identifying common subgraph patterns that surpass a predefined frequency threshold.","While FSM is widely applicable in fields like bioinformatics, chemical analysis, and social network anomaly detection, its execution remains time-consuming and complex.","This complexity stems from the need to recognize high-frequency subgraphs and ascertain if they exceed the set threshold.","Current approaches to identifying these patterns often rely on edge or vertex extension methods.","However, these strategies can introduce redundancies and cause increased latency.","To address these challenges, this paper introduces a novel approach for identifying potential k-vertex patterns by combining two frequently observed (k - 1)-vertex patterns.","This method optimizes the breadth-]first search, which allows for quicker search termination based on vertices count and support value.","Another challenge in FSM is the validation of the presumed pattern against a specific threshold.","Existing metrics, such as Maximum Independent Set (MIS) and Minimum Node Image (MNI), either demand significant computational time or risk overestimating pattern counts.","Our innovative approach aligns with the MIS and identifies independent subgraphs.","Through the \"Maximal Independent Set\" metric, this paper offers an efficient solution that minimizes latency and provides users with control over pattern overlap.","Through extensive experimentation, our proposed method achieves an average of 10.58x speedup when compared to GraMi and an average 3x speedup when compared to T-FSM"],"url":"http://arxiv.org/abs/2404.01585v1","category":"cs.DB"}
{"created":"2024-04-01 23:01:03","title":"Role of the extended Hilbert space in the attainability of the Quantum Cram\u00e9r-Rao bound for multiparameter estimation","abstract":"The symmetric logarithmic derivative Cram\\'er-Rao bound (SLDCRB) provides a fundamental limit to the minimum variance with which a set of unknown parameters can be estimated in an unbiased manner. It is known that the SLDCRB can be saturated provided the optimal measurements for the individual parameters commute with one another. However, when this is not the case the SLDCRB cannot be attained in general. In the experimentally relevant setting, where quantum states are measured individually, necessary and sufficient conditions for when the SLDCRB can be saturated are not known. In this setting the SLDCRB is attainable provided the SLD operators can be chosen to commute on an extended Hilbert space. However, beyond this relatively little is known about when the SLD operators can be chosen in this manner. In this paper we present explicit examples which demonstrate novel aspects of this condition. Our examples demonstrate that the SLD operators commuting on any two of the following three spaces: support space, support-kernel space and kernel space, is neither a necessary nor sufficient condition for commutativity on the extended space. We present a simple analytic example showing that the Nagaoka-Hayashi Cram\\'er-Rao bound is not always attainable. Finally, we provide necessary and sufficient conditions for the attainability of the SLDCRB in the case when the kernel space is one-dimensional. These results provide new information on the necessary and sufficient conditions for the attainability of the SLDCRB.","sentences":["The symmetric logarithmic derivative Cram\\'er-Rao bound (SLDCRB) provides a fundamental limit to the minimum variance with which a set of unknown parameters can be estimated in an unbiased manner.","It is known that the SLDCRB can be saturated provided the optimal measurements for the individual parameters commute with one another.","However, when this is not the case the SLDCRB cannot be attained in general.","In the experimentally relevant setting, where quantum states are measured individually, necessary and sufficient conditions for when the SLDCRB can be saturated are not known.","In this setting the SLDCRB is attainable provided the SLD operators can be chosen to commute on an extended Hilbert space.","However, beyond this relatively little is known about when the SLD operators can be chosen in this manner.","In this paper we present explicit examples which demonstrate novel aspects of this condition.","Our examples demonstrate that the SLD operators commuting on any two of the following three spaces: support space, support-kernel space and kernel space, is neither a necessary nor sufficient condition for commutativity on the extended space.","We present a simple analytic example showing that the Nagaoka-Hayashi Cram\\'er-Rao bound is not always attainable.","Finally, we provide necessary and sufficient conditions for the attainability of the SLDCRB in the case when the kernel space is one-dimensional.","These results provide new information on the necessary and sufficient conditions for the attainability of the SLDCRB."],"url":"http://arxiv.org/abs/2404.01520v1","category":"quant-ph"}
{"created":"2024-04-01 21:31:53","title":"Probing Stochastic Ultralight Dark Matter with Space-based Gravitational-Wave Interferometers","abstract":"Ultralight particles are theoretically well-motivated dark matter candidates. In the vicinity of the solar system, these ultralight particles are described as a superposition of plane waves, resulting in a stochastic field with amplitude fluctuations on scales determined by the velocity dispersion of dark matter. In this work, we systematically investigate the sensitivity of space-based gravitational-wave interferometers to the stochastic ultralight dark matter field within the frequentist framework. We derive the projected sensitivity of a single detector using the time-delay interferometry and find that the stochastic effect will only diminish the performance of the detector marginally, unlike searches in other contexts where significant reductions are found. Furthermore, we explore the sensitivity of detector network in two typical configurations. We find that the well-separated configuration, where the distance between two detectors exceeds the coherence length of dark matter, is the optimal choice for ultralight dark matter detection due to a less chance of missing signal. This contrasts with the detection of stochastic gravitational-wave background, where the co-located and co-aligned configuration is preferred. Our results may provide useful insights for potential joint observations involving gravitational-wave detectors like LISA, Taiji and TianQin.","sentences":["Ultralight particles are theoretically well-motivated dark matter candidates.","In the vicinity of the solar system, these ultralight particles are described as a superposition of plane waves, resulting in a stochastic field with amplitude fluctuations on scales determined by the velocity dispersion of dark matter.","In this work, we systematically investigate the sensitivity of space-based gravitational-wave interferometers to the stochastic ultralight dark matter field within the frequentist framework.","We derive the projected sensitivity of a single detector using the time-delay interferometry and find that the stochastic effect will only diminish the performance of the detector marginally, unlike searches in other contexts where significant reductions are found.","Furthermore, we explore the sensitivity of detector network in two typical configurations.","We find that the well-separated configuration, where the distance between two detectors exceeds the coherence length of dark matter, is the optimal choice for ultralight dark matter detection due to a less chance of missing signal.","This contrasts with the detection of stochastic gravitational-wave background, where the co-located and co-aligned configuration is preferred.","Our results may provide useful insights for potential joint observations involving gravitational-wave detectors like LISA, Taiji and TianQin."],"url":"http://arxiv.org/abs/2404.01494v1","category":"hep-ph"}
{"created":"2024-04-01 21:01:31","title":"Information Processing in Hybrid Photonic Electrical Reservoir Computing","abstract":"Physical Reservoir Computing (PRC) is a recently developed variant of Neuromorphic Computing, where a pertinent physical system effectively projects information encoded in the input signal into a higher-dimensional space. While various physical hardware has demonstrated promising results for Reservoir Computing (RC), systems allowing tunability of their dynamical regimes have not received much attention regarding how to optimize relevant system parameters. In this work we employ hybrid photonic-electronic (HPE) system offering both parallelism inherent to light propagation, and electronic memory and programmable feedback allowing to induce nonlinear dynamics and tunable encoding of the photonic signal to realize HPE-RC. Specifically, we experimentally and theoretically analyze performance of integrated silicon photonic on-chip Mach-Zehnder interferometer and ring resonators with heaters acting as programmable phase modulators, controlled by detector and the feedback unit capable of realizing complex temporal dynamics of the photonic signal. Furthermore, we present an algorithm capable of predicting optimal parameters for RC by analyzing the corresponding Lyapunov exponent of the output signal and mutual information of reservoir nodes. By implementing the derived optimal parameters, we demonstrate that the corresponding resulting error of RC can be lowered by several orders of magnitude compared to a reservoir operating with randomly chosen set of parameters.","sentences":["Physical Reservoir Computing (PRC) is a recently developed variant of Neuromorphic Computing, where a pertinent physical system effectively projects information encoded in the input signal into a higher-dimensional space.","While various physical hardware has demonstrated promising results for Reservoir Computing (RC), systems allowing tunability of their dynamical regimes have not received much attention regarding how to optimize relevant system parameters.","In this work we employ hybrid photonic-electronic (HPE) system offering both parallelism inherent to light propagation, and electronic memory and programmable feedback allowing to induce nonlinear dynamics and tunable encoding of the photonic signal to realize HPE-RC.","Specifically, we experimentally and theoretically analyze performance of integrated silicon photonic on-chip Mach-Zehnder interferometer and ring resonators with heaters acting as programmable phase modulators, controlled by detector and the feedback unit capable of realizing complex temporal dynamics of the photonic signal.","Furthermore, we present an algorithm capable of predicting optimal parameters for RC by analyzing the corresponding Lyapunov exponent of the output signal and mutual information of reservoir nodes.","By implementing the derived optimal parameters, we demonstrate that the corresponding resulting error of RC can be lowered by several orders of magnitude compared to a reservoir operating with randomly chosen set of parameters."],"url":"http://arxiv.org/abs/2404.01479v1","category":"physics.optics"}
{"created":"2024-04-01 20:49:56","title":"gpu_tracker: Python package for tracking and profiling GPU utilization in both desktop and high-performance computing environments","abstract":"Determining the maximum usage of random-access memory (RAM) on both the motherboard and on a graphical processing unit (GPU) over the lifetime of a computing task can be extremely useful for troubleshooting points of failure as well as optimizing memory utilization, especially within a high-performance computing (HPC) setting. While there are tools for tracking compute time and RAM, including by job management tools themselves, tracking of GPU usage, to our knowledge, does not currently have sufficient solutions. We present gpu_tracker, a Python package that tracks the computational resource usage of a task while running in the background, including the real compute time that the task takes to complete, its maximum RAM usage, and the maximum GPU RAM usage, specifically for Nvidia GPUs. We demonstrate that gpu_tracker can seamlessly track computational resource usage with minimal overhead, both within desktop and HPC execution environments.","sentences":["Determining the maximum usage of random-access memory (RAM) on both the motherboard and on a graphical processing unit (GPU) over the lifetime of a computing task can be extremely useful for troubleshooting points of failure as well as optimizing memory utilization, especially within a high-performance computing (HPC) setting.","While there are tools for tracking compute time and RAM, including by job management tools themselves, tracking of GPU usage, to our knowledge, does not currently have sufficient solutions.","We present gpu_tracker, a Python package that tracks the computational resource usage of a task while running in the background, including the real compute time that the task takes to complete, its maximum RAM usage, and the maximum GPU RAM usage, specifically for Nvidia GPUs.","We demonstrate that gpu_tracker can seamlessly track computational resource usage with minimal overhead, both within desktop and HPC execution environments."],"url":"http://arxiv.org/abs/2404.01473v1","category":"cs.PF"}
{"created":"2024-04-01 18:00:57","title":"Convex relaxation for the generalized maximum-entropy sampling problem","abstract":"The generalized maximum-entropy sampling problem (GMESP) is to select an order-$s$ principal submatrix from an order-$n$ covariance matrix, to maximize the product of its $t$ greatest eigenvalues, $0<t\\leq s <n$. It is a problem that specializes to two fundamental problems in statistical design theory:(i) maximum-entropy sampling problem (MESP); (ii) binary D-optimality (D-Opt). In the general case, it is motivated by a selection problem in the context of PCA (principal component analysis).   We introduce the first convex-optimization based relaxation for GMESP, study its behavior, compare it to an earlier spectral bound, and demonstrate its use in a branch-and-bound scheme. We find that such an approach is practical when $s-t$ is very small.","sentences":["The generalized maximum-entropy sampling problem (GMESP) is to select an order-$s$ principal submatrix from an order-$n$ covariance matrix, to maximize the product of its $t$ greatest eigenvalues, $0<t\\leq s <n$. It is a problem that specializes to two fundamental problems in statistical design theory:(i) maximum-entropy sampling problem (MESP); (ii) binary D-optimality (D-Opt).","In the general case, it is motivated by a selection problem in the context of PCA (principal component analysis).   ","We introduce the first convex-optimization based relaxation for GMESP, study its behavior, compare it to an earlier spectral bound, and demonstrate its use in a branch-and-bound scheme.","We find that such an approach is practical when $s-t$ is very small."],"url":"http://arxiv.org/abs/2404.01390v1","category":"math.ST"}
{"created":"2024-04-01 17:56:28","title":"Beyond Linear Response: Equivalence between Thermodynamic Geometry and Optimal Transport","abstract":"A fundamental result of thermodynamic geometry is that the optimal, minimal-work protocol that drives a nonequilibrium system between two thermodynamic states in the slow-driving limit is given by a geodesic of the friction tensor, a Riemannian metric defined on control space. For overdamped dynamics in arbitrary dimensions, we demonstrate that thermodynamic geometry is equivalent to $L^2$ optimal transport geometry defined on the space of equilibrium distributions corresponding to the control parameters. We show that obtaining optimal protocols past the slow-driving or linear response regime is computationally tractable as the sum of a friction tensor geodesic and a counterdiabatic term related to the Fisher information metric. These geodesic-counterdiabatic optimal protocols are exact for parameteric harmonic potentials, reproduce the surprising non-monotonic behavior recently discovered in linearly-biased double well optimal protocols, and explain the ubiquitous discontinuous jumps observed at the beginning and end times.","sentences":["A fundamental result of thermodynamic geometry is that the optimal, minimal-work protocol that drives a nonequilibrium system between two thermodynamic states in the slow-driving limit is given by a geodesic of the friction tensor, a Riemannian metric defined on control space.","For overdamped dynamics in arbitrary dimensions, we demonstrate that thermodynamic geometry is equivalent to $L^2$ optimal transport geometry defined on the space of equilibrium distributions corresponding to the control parameters.","We show that obtaining optimal protocols past the slow-driving or linear response regime is computationally tractable as the sum of a friction tensor geodesic and a counterdiabatic term related to the Fisher information metric.","These geodesic-counterdiabatic optimal protocols are exact for parameteric harmonic potentials, reproduce the surprising non-monotonic behavior recently discovered in linearly-biased double well optimal protocols, and explain the ubiquitous discontinuous jumps observed at the beginning and end times."],"url":"http://arxiv.org/abs/2404.01286v2","category":"cond-mat.stat-mech"}
{"created":"2024-04-01 17:52:10","title":"Variable-Length Stop-Feedback Coding for Minimum Age of Incorrect Information","abstract":"We study the average Age of Incorrect Information (AoII) in the context of remote monitoring of a symmetric Markov source using variable-length stop-feedback (VLSF) coding. We consider sources with small cardinality, where feedback is non-instantaneous, as the transmitted information and feedback may have comparable lengths. We leverage recent results on the non-asymptotic achievable channel coding rate to derive optimal feedback sequences, i.e. the times of feedback transmissions, in terms of either AoII or delay. Our results showcase the impact of the feedback sequence and SNR on the AoII, revealing that a lower average delay does not necessarily correspond to a lower average AoII. We discuss implications and suggest directions for efficient coding scheme design.","sentences":["We study the average Age of Incorrect Information (AoII) in the context of remote monitoring of a symmetric Markov source using variable-length stop-feedback (VLSF) coding.","We consider sources with small cardinality, where feedback is non-instantaneous, as the transmitted information and feedback may have comparable lengths.","We leverage recent results on the non-asymptotic achievable channel coding rate to derive optimal feedback sequences, i.e. the times of feedback transmissions, in terms of either AoII or delay.","Our results showcase the impact of the feedback sequence and SNR on the AoII, revealing that a lower average delay does not necessarily correspond to a lower average AoII.","We discuss implications and suggest directions for efficient coding scheme design."],"url":"http://arxiv.org/abs/2404.01276v1","category":"cs.IT"}
{"created":"2024-04-01 17:44:07","title":"Non-asymptotic Global Convergence Rates of BFGS with Exact Line Search","abstract":"In this paper, we explore the non-asymptotic global convergence rates of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method implemented with exact line search. Notably, due to Dixon's equivalence result, our findings are also applicable to other quasi-Newton methods in the convex Broyden class employing exact line search, such as the Davidon-Fletcher-Powell (DFP) method. Specifically, we focus on problems where the objective function is strongly convex with Lipschitz continuous gradient and Hessian. Our results hold for any initial point and any symmetric positive definite initial Hessian approximation matrix. The analysis unveils a detailed three-phase convergence process, characterized by distinct linear and superlinear rates, contingent on the iteration progress. Additionally, our theoretical findings demonstrate the trade-offs between linear and superlinear convergence rates for BFGS when we modify the initial Hessian approximation matrix, a phenomenon further corroborated by our numerical experiments.","sentences":["In this paper, we explore the non-asymptotic global convergence rates of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method implemented with exact line search.","Notably, due to Dixon's equivalence result, our findings are also applicable to other quasi-Newton methods in the convex Broyden class employing exact line search, such as the Davidon-Fletcher-Powell (DFP) method.","Specifically, we focus on problems where the objective function is strongly convex with Lipschitz continuous gradient and Hessian.","Our results hold for any initial point and any symmetric positive definite initial Hessian approximation matrix.","The analysis unveils a detailed three-phase convergence process, characterized by distinct linear and superlinear rates, contingent on the iteration progress.","Additionally, our theoretical findings demonstrate the trade-offs between linear and superlinear convergence rates for BFGS when we modify the initial Hessian approximation matrix, a phenomenon further corroborated by our numerical experiments."],"url":"http://arxiv.org/abs/2404.01267v1","category":"math.OC"}
{"created":"2024-04-01 17:38:17","title":"JWST COMPASS: A NIRSpec/G395H Transmission Spectrum of the Sub-Neptune TOI-836c","abstract":"Planets between the sizes of Earth and Neptune are the most common in the Galaxy, bridging the gap between the terrestrial and giant planets in our Solar System. Now that we are firmly in the era of JWST, we can begin to measure, in more detail, the atmospheres of these ubiquitous planets to better understand their evolutionary trajectories. The two planets in the TOI-836 system are ideal candidates for such a study, as they fall on either side of the radius valley, allowing for direct comparisons of the present-day atmospheres of planets that formed in the same environment but had different ultimate end states. We present results from the JWST NIRSpec G395H transit observation of the larger and outer of the planets in this system, TOI-836c (2.587 R$_{\\oplus}$, 9.6 M$_{\\oplus}$, T$_{\\rm eq}$$\\sim$665 K). While we measure average 30-pixel binned precisions of $\\sim$24ppm for NRS1 and $\\sim$43ppm for NRS2 per spectral bin, we do find residual correlated noise in the data, which we attempt to correct using the JWST Engineering Database. We find a featureless transmission spectrum for this sub-Neptune planet, and are able to rule out atmospheric metallicities $<$175$\\times$ Solar in the absence of aerosols at $\\lesssim$1 millibar. We leverage microphysical models to determine that aerosols at such low pressures are physically plausible. The results presented herein represent the first observation from the COMPASS (Compositions of Mini-Planet Atmospheres for Statistical Study) JWST program, which also includes TOI-836b and will ultimately compare the presence and compositions of atmospheres for 12 super-Earths/sub-Neptunes.","sentences":["Planets between the sizes of Earth and Neptune are the most common in the Galaxy, bridging the gap between the terrestrial and giant planets in our Solar System.","Now that we are firmly in the era of JWST, we can begin to measure, in more detail, the atmospheres of these ubiquitous planets to better understand their evolutionary trajectories.","The two planets in the TOI-836 system are ideal candidates for such a study, as they fall on either side of the radius valley, allowing for direct comparisons of the present-day atmospheres of planets that formed in the same environment but had different ultimate end states.","We present results from the JWST NIRSpec G395H transit observation of the larger and outer of the planets in this system, TOI-836c (2.587 R$_{\\oplus}$, 9.6 M$_{\\oplus}$, T$_{\\rm eq}$$\\sim$665 K).","While we measure average 30-pixel binned precisions of $\\sim$24ppm for NRS1 and $\\sim$43ppm for NRS2 per spectral bin, we do find residual correlated noise in the data, which we attempt to correct using the JWST Engineering Database.","We find a featureless transmission spectrum for this sub-Neptune planet, and are able to rule out atmospheric metallicities $<$175$\\times$ Solar in the absence of aerosols at $\\lesssim$1 millibar.","We leverage microphysical models to determine that aerosols at such low pressures are physically plausible.","The results presented herein represent the first observation from the COMPASS (Compositions of Mini-Planet Atmospheres for Statistical Study) JWST program, which also includes TOI-836b and will ultimately compare the presence and compositions of atmospheres for 12 super-Earths/sub-Neptunes."],"url":"http://arxiv.org/abs/2404.01264v1","category":"astro-ph.EP"}
{"created":"2024-04-01 17:23:28","title":"Gradient Methods for Scalable Multi-value Electricity Network Expansion Planning","abstract":"We consider multi-value expansion planning (MEP), a general bilevel optimization model in which a planner optimizes arbitrary functions of the dispatch outcome in the presence of a partially controllable, competitive electricity market. The MEP problem can be used to jointly plan various grid assets, such as transmission, generation, and battery storage capacities; examples include identifying grid investments that minimize emissions in the absence of a carbon tax, maximizing the profit of a portfolio of renewable investments and long-term energy contracts, or reducing price inequities between different grid stakeholders. The MEP problem, however, is in general nonconvex, making it difficult to solve exactly for large real-world systems. Therefore, we propose a fast stochastic implicit gradient-based heuristic method that scales well to large networks with many scenarios. We use a strong duality reformulation and the McCormick envelope to provide a lower bound on the performance of our algorithm via convex relaxation. We test the performance of our method on a large model of the U.S. Western Interconnect and demonstrate that it scales linearly with network size and number of scenarios and can be efficiently parallelized on large machines. We find that for medium-sized 16 hour cases, gradient descent on average finds a 5.3x lower objective value in 16.5x less time compared to a traditional reformulation-based approach solved with an interior point method. We conclude with a large example in which we jointly plan transmission, generation, and storage for a 768 hour case on 100 node system, showing that emissions penalization leads to additional 40.0% reduction in carbon intensity at an additional cost of $17.1/MWh.","sentences":["We consider multi-value expansion planning (MEP), a general bilevel optimization model in which a planner optimizes arbitrary functions of the dispatch outcome in the presence of a partially controllable, competitive electricity market.","The MEP problem can be used to jointly plan various grid assets, such as transmission, generation, and battery storage capacities; examples include identifying grid investments that minimize emissions in the absence of a carbon tax, maximizing the profit of a portfolio of renewable investments and long-term energy contracts, or reducing price inequities between different grid stakeholders.","The MEP problem, however, is in general nonconvex, making it difficult to solve exactly for large real-world systems.","Therefore, we propose a fast stochastic implicit gradient-based heuristic method that scales well to large networks with many scenarios.","We use a strong duality reformulation and the McCormick envelope to provide a lower bound on the performance of our algorithm via convex relaxation.","We test the performance of our method on a large model of the U.S. Western Interconnect and demonstrate that it scales linearly with network size and number of scenarios and can be efficiently parallelized on large machines.","We find that for medium-sized 16 hour cases, gradient descent on average finds a 5.3x lower objective value in 16.5x less time compared to a traditional reformulation-based approach solved with an interior point method.","We conclude with a large example in which we jointly plan transmission, generation, and storage for a 768 hour case on 100 node system, showing that emissions penalization leads to additional 40.0% reduction in carbon intensity at an additional cost of $17.1/MWh."],"url":"http://arxiv.org/abs/2404.01255v1","category":"math.OC"}
{"created":"2024-04-01 17:14:54","title":"Duality based error control for the Signorini problem","abstract":"In this paper we study the a posteriori bounds for a conforming piecewise linear finite element approximation of the Signorini problem. We prove new rigorous a posteriori estimates of residual type in $L^{p}$, for $p \\in (4,\\infty)$ in two spatial dimensions. This new analysis treats the positive and negative parts of the discretisation error separately, requiring a novel sign- and bound-preserving interpolant, which is shown to have optimal approximation properties. The estimates rely on the sharp dual stability results on the problem in $W^{2,(4 - \\varepsilon)/3}$ for any $\\varepsilon \\ll 1$. We summarise extensive numerical experiments aimed at testing the robustness of the estimator to validate the theory.","sentences":["In this paper we study the a posteriori bounds for a conforming piecewise linear finite element approximation of the Signorini problem.","We prove new rigorous a posteriori estimates of residual type in $L^{p}$, for $p \\in (4,\\infty)$ in two spatial dimensions.","This new analysis treats the positive and negative parts of the discretisation error separately, requiring a novel sign- and bound-preserving interpolant, which is shown to have optimal approximation properties.","The estimates rely on the sharp dual stability results on the problem in $W^{2,(4 - \\varepsilon)/3}$ for any $\\varepsilon \\ll 1$.","We summarise extensive numerical experiments aimed at testing the robustness of the estimator to validate the theory."],"url":"http://arxiv.org/abs/2404.01251v1","category":"math.NA"}
{"created":"2024-04-01 17:03:41","title":"A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules","abstract":"Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.","sentences":["Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart.","In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules.","Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated).","Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written).","Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program.","We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks.","In particular, we derive optimal detection rules for these watermarks under our framework.","These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments."],"url":"http://arxiv.org/abs/2404.01245v1","category":"math.ST"}
{"created":"2024-04-01 16:51:19","title":"Optimal Ridge Regularization for Out-of-Distribution Prediction","abstract":"We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels.","sentences":["We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution.","We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts.","These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting.","For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized.","Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels.","In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels."],"url":"http://arxiv.org/abs/2404.01233v1","category":"math.ST"}
{"created":"2024-04-01 16:19:50","title":"LTL-D*: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications","abstract":"This paper presents an incremental replanning algorithm, dubbed LTL-D*, for temporal-logic-based task planning in a dynamically changing environment. Unexpected changes in the environment may lead to failures in satisfying a task specification in the form of a Linear Temporal Logic (LTL). In this study, the considered failures are categorized into two classes: (i) the desired LTL specification can be satisfied via replanning, and (ii) the desired LTL specification is infeasible to meet strictly and can only be satisfied in a \"relaxed\" fashion. To address these failures, the proposed algorithm finds an optimal replanning solution that minimally violates desired task specifications. In particular, our approach leverages the D* Lite algorithm and employs a distance metric within the synthesized automaton to quantify the degree of the task violation and then replan incrementally. This ensures plan optimality and reduces planning time, especially when frequent replanning is required. Our approach is implemented in a robot navigation simulation to demonstrate a significant improvement in the computational efficiency for replanning by two orders of magnitude.","sentences":["This paper presents an incremental replanning algorithm, dubbed LTL-D*, for temporal-logic-based task planning in a dynamically changing environment.","Unexpected changes in the environment may lead to failures in satisfying a task specification in the form of a Linear Temporal Logic (LTL).","In this study, the considered failures are categorized into two classes: (i) the desired LTL specification can be satisfied via replanning, and (ii) the desired LTL specification is infeasible to meet strictly and can only be satisfied in a \"relaxed\" fashion.","To address these failures, the proposed algorithm finds an optimal replanning solution that minimally violates desired task specifications.","In particular, our approach leverages the D* Lite algorithm and employs a distance metric within the synthesized automaton to quantify the degree of the task violation and then replan incrementally.","This ensures plan optimality and reduces planning time, especially when frequent replanning is required.","Our approach is implemented in a robot navigation simulation to demonstrate a significant improvement in the computational efficiency for replanning by two orders of magnitude."],"url":"http://arxiv.org/abs/2404.01219v1","category":"cs.RO"}
{"created":"2024-04-01 16:18:40","title":"Towards System Modelling to Support Diseases Data Extraction from the Electronic Health Records for Physicians Research Activities","abstract":"The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients. The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide. Therefore, the data can be utilized for secondary tasks such as research. This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population. As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group. One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms. Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities. There is a large amount of EHRs available, and solving the standardizing issues requires some optimized techniques. We used a first-hand EHR dataset extracted from EHR systems. Our application uploads the dataset from the EHRs and converts it to the ICD-10 coding system to solve the standardization problem. So, we first apply the steps of pre-processing, annotation, and transforming the data to convert it into the standard form. The data pre-processing is applied to normalize demographic formats. In the annotation step, a machine learning model is used to recognize the diseases from the text. Furthermore, the transforming step converts the disease name to the ICD-10 coding format. The model was evaluated manually by comparing its performance in terms of disease recognition with an available dictionary-based system (MetaMap). The accuracy of the proposed machine learning model is 81%, that outperformed MetaMap accuracy of 67%. This paper contributed to system modelling for EHR data extraction to support research activities.","sentences":["The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients.","The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide.","Therefore, the data can be utilized for secondary tasks such as research.","This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population.","As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group.","One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms.","Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities.","There is a large amount of EHRs available, and solving the standardizing issues requires some optimized techniques.","We used a first-hand EHR dataset extracted from EHR systems.","Our application uploads the dataset from the EHRs and converts it to the ICD-10 coding system to solve the standardization problem.","So, we first apply the steps of pre-processing, annotation, and transforming the data to convert it into the standard form.","The data pre-processing is applied to normalize demographic formats.","In the annotation step, a machine learning model is used to recognize the diseases from the text.","Furthermore, the transforming step converts the disease name to the ICD-10 coding format.","The model was evaluated manually by comparing its performance in terms of disease recognition with an available dictionary-based system (MetaMap).","The accuracy of the proposed machine learning model is 81%, that outperformed MetaMap accuracy of 67%.","This paper contributed to system modelling for EHR data extraction to support research activities."],"url":"http://arxiv.org/abs/2404.01218v1","category":"cs.LG"}
{"created":"2024-04-01 16:16:19","title":"Novel Node Category Detection Under Subpopulation Shift","abstract":"In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.","sentences":["In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories.","It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes.","We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts.","By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure.","Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods."],"url":"http://arxiv.org/abs/2404.01216v1","category":"cs.LG"}
{"created":"2024-04-01 16:10:03","title":"Hundreds of grocery outlets needed across the United States to achieve walkable cities","abstract":"The notion of the $x$-minute city is again popular in urban planning, but the practical implications of developing walkable neighborhoods have not been rigorously explored. What is the scale of the challenge that cities needing to retrofit face? Where should new stores or amenities be located? For 500 cities in the United States, we explored how many additional supermarkets would be required to achieve various levels of $x$-minute access and where new stores should be located so that this access is equally-distributed. Our method is unique because it combines a novel measure of equality with a new model that optimally locates amenities for inequality-minimizing community access. We found that 25% of the studied cities could reach 15-minute access by adding five or fewer stores, while only 10% of the cities could even achieve 5-minute average access when using neighborhood centroids as potential sites; the cities that could, on average, required more than 100 stores each. This work provides a tool for cities to use evidenced-based planning to efficiently retrofit in order to enable active transport, benefiting both the climate and their residents' health. It also highlights the major challenge facing our cities due to the existing and ongoing car-dependent urban design that renders these goals unfeasible.","sentences":["The notion of the $x$-minute city is again popular in urban planning, but the practical implications of developing walkable neighborhoods have not been rigorously explored.","What is the scale of the challenge that cities needing to retrofit face?","Where should new stores or amenities be located?","For 500 cities in the United States, we explored how many additional supermarkets would be required to achieve various levels of $x$-minute access and where new stores should be located so that this access is equally-distributed.","Our method is unique because it combines a novel measure of equality with a new model that optimally locates amenities for inequality-minimizing community access.","We found that 25% of the studied cities could reach 15-minute access by adding five or fewer stores, while only 10% of the cities could even achieve 5-minute average access when using neighborhood centroids as potential sites; the cities that could, on average, required more than 100 stores each.","This work provides a tool for cities to use evidenced-based planning to efficiently retrofit in order to enable active transport, benefiting both the climate and their residents' health.","It also highlights the major challenge facing our cities due to the existing and ongoing car-dependent urban design that renders these goals unfeasible."],"url":"http://arxiv.org/abs/2404.01209v1","category":"math.OC"}
{"created":"2024-04-01 16:00:01","title":"The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis","abstract":"Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.","sentences":["Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining.","The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models.","However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks.","Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining.","To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints.","Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters.","In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints.","This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers.","Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases.","Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process."],"url":"http://arxiv.org/abs/2404.01204v1","category":"cs.CL"}
{"created":"2024-04-01 15:59:09","title":"Device-independent quantum key distribution based on routed Bell tests","abstract":"Photon losses are the main obstacle to fully photonic implementations of device-independent quantum key distribution (DIQKD). Motivated by recent work showing that routed Bell scenarios offer increased robustness to detection inefficiencies for the certification of long-range quantum correlations, we investigate DIQKD protocols based on a routed setup. In these protocols, in some of the test rounds, photons from the source are routed by an actively controlled switch to a nearby test device instead of the distant one. We show how to analyze the security of these protocols and compute lower bounds on the key rates using non-commutative polynomial optimization and the Brown-Fawzi-Fazwi method. We determine lower bounds on the asymptotic key rates of several simple two-qubit routed DIQKD protocols based on CHSH or BB84 correlations and compare their performance to standard protocols. We find that in an ideal case routed DIQKD protocols can significantly improve detection efficiency requirements, by up to $\\sim 30\\%$, compared to their non-routed counterparts. Notably, the routed BB84 protocol achieves a positive key rate with a detection efficiency as low as $50\\%$ for the distant device, the minimal threshold for any QKD protocol featuring two untrusted measurements. However, the advantages we find are highly sensitive to noise and losses affecting the short-range correlations involving the additional test device.","sentences":["Photon losses are the main obstacle to fully photonic implementations of device-independent quantum key distribution (DIQKD).","Motivated by recent work showing that routed Bell scenarios offer increased robustness to detection inefficiencies for the certification of long-range quantum correlations, we investigate DIQKD protocols based on a routed setup.","In these protocols, in some of the test rounds, photons from the source are routed by an actively controlled switch to a nearby test device instead of the distant one.","We show how to analyze the security of these protocols and compute lower bounds on the key rates using non-commutative polynomial optimization and the Brown-Fawzi-Fazwi method.","We determine lower bounds on the asymptotic key rates of several simple two-qubit routed DIQKD protocols based on CHSH or BB84 correlations and compare their performance to standard protocols.","We find that in an ideal case routed DIQKD protocols can significantly improve detection efficiency requirements, by up to $\\sim 30\\%$, compared to their non-routed counterparts.","Notably, the routed BB84 protocol achieves a positive key rate with a detection efficiency as low as $50\\%$ for the distant device, the minimal threshold for any QKD protocol featuring two untrusted measurements.","However, the advantages we find are highly sensitive to noise and losses affecting the short-range correlations involving the additional test device."],"url":"http://arxiv.org/abs/2404.01202v1","category":"quant-ph"}
{"created":"2024-04-01 15:57:57","title":"Search for gravitationally lensed interstellar transmissions","abstract":"We consider interstellar light transmission aided by a gravitational lens. We find that optimal reception efficiency occurs in lensing geometries where the transmitter, lens, and receiver are nearly aligned. We explore various signal detection strategies, employing both existing and emerging technologies. With this study, our understanding of interstellar power transmission via gravitational lensing has significantly progressed. We observe that detection signals from nearby stars, may leverage established photonics and optical engineering technologies, and networks of collaborative astronomical facilities. Our findings support the feasibility of interstellar power transmission via gravitational lensing, directly contributing to the ongoing optical SETI efforts.","sentences":["We consider interstellar light transmission aided by a gravitational lens.","We find that optimal reception efficiency occurs in lensing geometries where the transmitter, lens, and receiver are nearly aligned.","We explore various signal detection strategies, employing both existing and emerging technologies.","With this study, our understanding of interstellar power transmission via gravitational lensing has significantly progressed.","We observe that detection signals from nearby stars, may leverage established photonics and optical engineering technologies, and networks of collaborative astronomical facilities.","Our findings support the feasibility of interstellar power transmission via gravitational lensing, directly contributing to the ongoing optical SETI efforts."],"url":"http://arxiv.org/abs/2404.01201v1","category":"gr-qc"}
{"created":"2024-04-01 15:55:45","title":"Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem","abstract":"We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\\Omega(\\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\\log k)$ approximation factor, achieving an overall $O(\\sqrt{k} \\log k)$ approximation relative to optimal.","sentences":["We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem.","An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far.","We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\\Omega(\\sqrt{k})$ approximation factor relative to the optimal reward.","We then provide a randomized online algorithm that guarantees an $O(\\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance.","We then show how to remove this assumption at the cost of an extra $O(\\log k)$ approximation factor, achieving an overall $O(\\sqrt{k} \\log k)$ approximation relative to optimal."],"url":"http://arxiv.org/abs/2404.01198v1","category":"cs.LG"}
{"created":"2024-04-01 15:53:59","title":"Robust Trajectory and Resource Optimization for Communication-assisted UAV SAR Sensing","abstract":"In this paper, we investigate joint 3-dimensional (3D) trajectory planning and resource allocation for rotary-wing unmanned aerial vehicle (UAV) synthetic aperture radar (SAR) sensing. To support emerging real-time SAR applications and enable live mission control, we incorporate real-time communication with a ground station (GS). The UAV's main mission is the mapping of large areas of interest (AoIs) using an onboard SAR system and transferring the unprocessed raw radar data to the ground in real time. We propose a robust trajectory and resource allocation design that takes into account random UAV trajectory deviations. To this end, we model the UAV trajectory deviations and study their effect on the radar coverage. Then, we formulate a robust non-convex mixed-integer non-linear program (MINLP) such that the UAV 3D trajectory and resources are jointly optimized for maximization of the radar ground coverage. A low-complexity sub-optimal solution for the formulated problem is presented. Furthermore, to assess the performance of the sub-optimal algorithm, we derive an upper bound on the optimal solution based on monotonic optimization theory. Simulation results show that the proposed sub-optimal algorithm achieves close-to-optimal performance and not only outperforms several benchmark schemes but is also robust with respect to UAV trajectory deviations.","sentences":["In this paper, we investigate joint 3-dimensional (3D) trajectory planning and resource allocation for rotary-wing unmanned aerial vehicle (UAV) synthetic aperture radar (SAR) sensing.","To support emerging real-time SAR applications and enable live mission control, we incorporate real-time communication with a ground station (GS).","The UAV's main mission is the mapping of large areas of interest (AoIs) using an onboard SAR system and transferring the unprocessed raw radar data to the ground in real time.","We propose a robust trajectory and resource allocation design that takes into account random UAV trajectory deviations.","To this end, we model the UAV trajectory deviations and study their effect on the radar coverage.","Then, we formulate a robust non-convex mixed-integer non-linear program (MINLP) such that the UAV 3D trajectory and resources are jointly optimized for maximization of the radar ground coverage.","A low-complexity sub-optimal solution for the formulated problem is presented.","Furthermore, to assess the performance of the sub-optimal algorithm, we derive an upper bound on the optimal solution based on monotonic optimization theory.","Simulation results show that the proposed sub-optimal algorithm achieves close-to-optimal performance and not only outperforms several benchmark schemes but is also robust with respect to UAV trajectory deviations."],"url":"http://arxiv.org/abs/2404.01195v1","category":"eess.SP"}
{"created":"2024-04-01 15:15:49","title":"Multiple Joint Chance Constraints Approximation for Uncertainty Modeling in Dispatch Problems","abstract":"Uncertainty modeling has become increasingly important in power system decision-making. The widely-used tractable uncertainty modeling method-chance constraints with Conditional Value at Risk (CVaR) approximation, can be overconservative and even turn an originally feasible problem into an infeasible one. This paper proposes a new approximation method for multiple joint chance constraints (JCCs) to model the uncertainty in dispatch problems, which solves the conservativeness and potential infeasibility concerns of CVaR. The proposed method is also convenient for controlling the risk levels of different JCCs, which is necessary for power system applications since different resources may be affected by varying degrees of uncertainty or have different importance to the system. We then formulate a data-driven distributionally robust chance-constrained programming model for the power system multiperiod dispatch problem and leverage the proposed approximation method to solve it. In the numerical simulations, two small general examples clearly demonstrate the superiority of the proposed method, and the results of the multiperiod dispatch problem on IEEE test cases verify its practicality.","sentences":["Uncertainty modeling has become increasingly important in power system decision-making.","The widely-used tractable uncertainty modeling method-chance constraints with Conditional Value at Risk (CVaR) approximation, can be overconservative and even turn an originally feasible problem into an infeasible one.","This paper proposes a new approximation method for multiple joint chance constraints (JCCs) to model the uncertainty in dispatch problems, which solves the conservativeness and potential infeasibility concerns of CVaR.","The proposed method is also convenient for controlling the risk levels of different JCCs, which is necessary for power system applications since different resources may be affected by varying degrees of uncertainty or have different importance to the system.","We then formulate a data-driven distributionally robust chance-constrained programming model for the power system multiperiod dispatch problem and leverage the proposed approximation method to solve it.","In the numerical simulations, two small general examples clearly demonstrate the superiority of the proposed method, and the results of the multiperiod dispatch problem on IEEE test cases verify its practicality."],"url":"http://arxiv.org/abs/2404.01167v1","category":"math.OC"}
{"created":"2024-04-01 14:50:03","title":"Visual-inertial state estimation based on Chebyshev polynomial optimization","abstract":"This paper proposes an innovative state estimation method for visual-inertial fusion based on Chebyshev polynomial optimization. Specifically, the pose is modeled as a Chebyshev polynomial of a certain order, and its time derivatives are used to calculate linear acceleration and angular velocity, which, along with inertial measurements, constitute dynamic constraints. This is coupled with a visual measurement model to construct a visual-inertial bundle adjustment formulation. Simulation and public dataset experiments show that the proposed method has better accuracy than the discrete-form preintegration method.","sentences":["This paper proposes an innovative state estimation method for visual-inertial fusion based on Chebyshev polynomial optimization.","Specifically, the pose is modeled as a Chebyshev polynomial of a certain order, and its time derivatives are used to calculate linear acceleration and angular velocity, which, along with inertial measurements, constitute dynamic constraints.","This is coupled with a visual measurement model to construct a visual-inertial bundle adjustment formulation.","Simulation and public dataset experiments show that the proposed method has better accuracy than the discrete-form preintegration method."],"url":"http://arxiv.org/abs/2404.01150v1","category":"cs.RO"}
{"created":"2024-04-01 14:48:46","title":"Joint Beam Scheduling and Beamforming Design for Cooperative Positioning in Multi-beam LEO Satellite Networks","abstract":"Cooperative positioning with multiple low earth orbit (LEO) satellites is promising in providing location-based services and enhancing satellite-terrestrial communication. However, positioning accuracy is greatly affected by inter-beam interference and satellite-terrestrial topology geometry. To select the best combination of satellites from visible ones and suppress inter-beam interference, this paper explores the utilization of flexible beam scheduling and beamforming of multi-beam LEO satellites that can adjust beam directions toward the same earth-fixed cell to send positioning signals simultaneously. By leveraging Cram\\'{e}r-Rao lower bound (CRLB) to characterize user Time Difference of Arrival (TDOA) positioning accuracy, the concerned problem is formulated, aiming at optimizing user positioning accuracy under beam scheduling and beam transmission power constraints. To deal with the mixed-integer-nonconvex problem, we decompose it into an inner beamforming design problem and an outer beam scheduling problem. For the former, we first prove the monotonic relationship between user positioning accuracy and its perceived signal-to-interference-plus-noise ratio (SINR) to reformulate the problem, and then semidefinite relaxation (SDR) is adopted for beamforming design. For the outer problem, a heuristic low-complexity beam scheduling scheme is proposed, whose core idea is to schedule users with lower channel correlation to mitigate inter-beam interference while seeking a proper satellite-terrestrial topology geometry. Simulation results verify the superior positioning performance of our proposed positioning-oriented beamforming and beam scheduling scheme, and it is shown that average user positioning accuracy is improved by $17.1\\%$ and $55.9\\%$ when the beam transmission power is 20 dBw, compared to conventional beamforming and beam scheduling schemes, respectively.","sentences":["Cooperative positioning with multiple low earth orbit (LEO) satellites is promising in providing location-based services and enhancing satellite-terrestrial communication.","However, positioning accuracy is greatly affected by inter-beam interference and satellite-terrestrial topology geometry.","To select the best combination of satellites from visible ones and suppress inter-beam interference, this paper explores the utilization of flexible beam scheduling and beamforming of multi-beam LEO satellites that can adjust beam directions toward the same earth-fixed cell to send positioning signals simultaneously.","By leveraging Cram\\'{e}r-Rao lower bound (CRLB) to characterize user Time Difference of Arrival (TDOA) positioning accuracy, the concerned problem is formulated, aiming at optimizing user positioning accuracy under beam scheduling and beam transmission power constraints.","To deal with the mixed-integer-nonconvex problem, we decompose it into an inner beamforming design problem and an outer beam scheduling problem.","For the former, we first prove the monotonic relationship between user positioning accuracy and its perceived signal-to-interference-plus-noise ratio (SINR) to reformulate the problem, and then semidefinite relaxation (SDR) is adopted for beamforming design.","For the outer problem, a heuristic low-complexity beam scheduling scheme is proposed, whose core idea is to schedule users with lower channel correlation to mitigate inter-beam interference while seeking a proper satellite-terrestrial topology geometry.","Simulation results verify the superior positioning performance of our proposed positioning-oriented beamforming and beam scheduling scheme, and it is shown that average user positioning accuracy is improved by $17.1\\%$ and $55.9\\%$ when the beam transmission power is 20 dBw, compared to conventional beamforming and beam scheduling schemes, respectively."],"url":"http://arxiv.org/abs/2404.01148v1","category":"cs.IT"}
{"created":"2024-04-01 14:34:45","title":"Protocols and Trade-Offs of Quantum State Purification","abstract":"Quantum state purification plays a pivotal role in quantum communication and quantum computation, aiming to recover the purified state from multiple copies of an unknown noisy state. This work introduces a general state purification framework designed to achieve the highest fidelity with a specified probability and characterize the associated trade-offs. In particular, for i.i.d. quantum states under depolarizing noise, we propose an explicit purification protocol capable of achieving maximal fidelity with a target probability. Furthermore, we present quantum circuits for implementing the optimal purification protocols via the block encoding technique and propose recursive protocols for stream purification. Finally, we demonstrate the advantages of our protocols in terms of efficiency and flexibility in purifying noisy quantum states under various quantum noise models of interest, showcasing the effectiveness and versatility of our approach.","sentences":["Quantum state purification plays a pivotal role in quantum communication and quantum computation, aiming to recover the purified state from multiple copies of an unknown noisy state.","This work introduces a general state purification framework designed to achieve the highest fidelity with a specified probability and characterize the associated trade-offs.","In particular, for i.i.d. quantum states under depolarizing noise, we propose an explicit purification protocol capable of achieving maximal fidelity with a target probability.","Furthermore, we present quantum circuits for implementing the optimal purification protocols via the block encoding technique and propose recursive protocols for stream purification.","Finally, we demonstrate the advantages of our protocols in terms of efficiency and flexibility in purifying noisy quantum states under various quantum noise models of interest, showcasing the effectiveness and versatility of our approach."],"url":"http://arxiv.org/abs/2404.01138v1","category":"quant-ph"}
{"created":"2024-04-01 14:02:06","title":"Probability-Based Optimal Control Design for Soft Landing of Short-Stroke Actuators","abstract":"The impact forces during switching operations of short-stroke actuators may cause bouncing, audible noise and mechanical wear. The application of soft-landing control strategies to these devices aims at minimizing the impact velocities of their moving components to ultimately improve their lifetime and performance. In this paper, a novel approach for soft-landing trajectory planning, including probability functions, is proposed for optimal control of the actuators. The main contribution of the proposal is that it considers uncertainty in the contact position and hence the obtained trajectories are more robust against system uncertainties. The problem is formulated as an optimal control problem and transformed into a two-point boundary value problem for its numerical resolution. Simulated and experimental tests have been performed using a dynamic model and a commercial short-stroke solenoid valve. The results show a significant improvement in the expected velocities and accelerations at contact with respect to past solutions in which the contact position is assumed to be perfectly known.","sentences":["The impact forces during switching operations of short-stroke actuators may cause bouncing, audible noise and mechanical wear.","The application of soft-landing control strategies to these devices aims at minimizing the impact velocities of their moving components to ultimately improve their lifetime and performance.","In this paper, a novel approach for soft-landing trajectory planning, including probability functions, is proposed for optimal control of the actuators.","The main contribution of the proposal is that it considers uncertainty in the contact position and hence the obtained trajectories are more robust against system uncertainties.","The problem is formulated as an optimal control problem and transformed into a two-point boundary value problem for its numerical resolution.","Simulated and experimental tests have been performed using a dynamic model and a commercial short-stroke solenoid valve.","The results show a significant improvement in the expected velocities and accelerations at contact with respect to past solutions in which the contact position is assumed to be perfectly known."],"url":"http://arxiv.org/abs/2404.01125v1","category":"eess.SY"}
{"created":"2024-04-01 13:55:44","title":"CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening","abstract":"Pansharpening aims to enhance remote sensing image (RSI) quality by merging high-resolution panchromatic (PAN) with multispectral (MS) images. However, prior techniques struggled to optimally fuse PAN and MS images for enhanced spatial and spectral information, due to a lack of a systematic framework capable of effectively coordinating their individual strengths. In response, we present the Cross Modulation Transformer (CMT), a pioneering method that modifies the attention mechanism. This approach utilizes a robust modulation technique from signal processing, integrating it into the attention mechanism's calculations. It dynamically tunes the weights of the carrier's value (V) matrix according to the modulator's features, thus resolving historical challenges and achieving a seamless integration of spatial and spectral attributes. Furthermore, considering that RSI exhibits large-scale features and edge details along with local textures, we crafted a hybrid loss function that combines Fourier and wavelet transforms to effectively capture these characteristics, thereby enhancing both spatial and spectral accuracy in pansharpening. Extensive experiments demonstrate our framework's superior performance over existing state-of-the-art methods. The code will be publicly available to encourage further research.","sentences":["Pansharpening aims to enhance remote sensing image (RSI) quality by merging high-resolution panchromatic (PAN) with multispectral (MS) images.","However, prior techniques struggled to optimally fuse PAN and MS images for enhanced spatial and spectral information, due to a lack of a systematic framework capable of effectively coordinating their individual strengths.","In response, we present the Cross Modulation Transformer (CMT), a pioneering method that modifies the attention mechanism.","This approach utilizes a robust modulation technique from signal processing, integrating it into the attention mechanism's calculations.","It dynamically tunes the weights of the carrier's value (V) matrix according to the modulator's features, thus resolving historical challenges and achieving a seamless integration of spatial and spectral attributes.","Furthermore, considering that RSI exhibits large-scale features and edge details along with local textures, we crafted a hybrid loss function that combines Fourier and wavelet transforms to effectively capture these characteristics, thereby enhancing both spatial and spectral accuracy in pansharpening.","Extensive experiments demonstrate our framework's superior performance over existing state-of-the-art methods.","The code will be publicly available to encourage further research."],"url":"http://arxiv.org/abs/2404.01121v1","category":"cs.CV"}
{"created":"2024-04-01 13:13:25","title":"Finite Sample Frequency Domain Identification","abstract":"We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\\mathcal{O}((d_{\\mathrm{u}}+\\sqrt{d_{\\mathrm{u}}d_{\\mathrm{y}}})\\sqrt{M/N_{\\mathrm{tot}}})$, where $N_{\\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\\mathrm{u}},\\,d_{\\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-space representation. By tuning $M$, we obtain a $N_{\\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency response over all frequencies in the $ \\mathcal{H}_{\\infty}$ norm. Our result draws upon an extension of the Hanson-Wright inequality to semi-infinite matrices. We study the finite-sample behavior of ETFE in simulations.","sentences":["We study non-parametric frequency-domain system identification from a finite-sample perspective.","We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples.","We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values.","The error rate is of the order of $\\mathcal{O}((d_{\\mathrm{u}}+\\sqrt{d_{\\mathrm{u}}d_{\\mathrm{y}}})\\sqrt{M/N_{\\mathrm{tot}}})$, where $N_{\\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\\mathrm{u}},\\,d_{\\mathrm{y}}$ are the dimensions of the input and output signals respectively.","This rate remains valid for general irrational transfer functions and does not require a finite order state-space representation.","By tuning $M$, we obtain a $N_{\\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency response over all frequencies in the $ \\mathcal{H}_{\\infty}$ norm.","Our result draws upon an extension of the Hanson-Wright inequality to semi-infinite matrices.","We study the finite-sample behavior of ETFE in simulations."],"url":"http://arxiv.org/abs/2404.01100v1","category":"eess.SY"}
{"created":"2024-04-01 12:44:23","title":"Mitigating Transient Bullwhip Effects Under Imperfect Demand Forecasts","abstract":"Motivated by how forecast errors exacerbate order fluctuations in supply chains, we use tools from robust control theory to characterize and compute the worst-case order fluctuation experienced by an individual supply chain vendor under bounded forecast errors and demand fluctuations. Building on existing discrete time, linear time-invariant (LTI) models of supply chains, we separately model forecast error and demand fluctuations as inputs to the inventory dynamics. We then define a transient Bullwhip measure to evaluate the vendor's worst-case order fluctuation and show that for bounded forecast errors and demand fluctuations, this measure is equivalent to the disturbance to control peak gain. To compute the controller that minimizes the worst-case peak gain, we formulate an optimization problem with bilinear matrix inequalities and show that solving this problem is equivalent to minimizing a quasi-convex function on a bounded domain. In contrast to the existing Bullwhip measure in literature, the transient Bullwhip measure has an explicit dependency on the forecast error and does not need the forecast to be a deterministic function of the demand history. This explicit dependency enables us to separately quantify the transient Bullwhip measure's sensitivity to forecast error and demand fluctuations. We empirically verify our model for vendors with non-zero perishable rates and order backlogging rates.","sentences":["Motivated by how forecast errors exacerbate order fluctuations in supply chains, we use tools from robust control theory to characterize and compute the worst-case order fluctuation experienced by an individual supply chain vendor under bounded forecast errors and demand fluctuations.","Building on existing discrete time, linear time-invariant (LTI) models of supply chains, we separately model forecast error and demand fluctuations as inputs to the inventory dynamics.","We then define a transient Bullwhip measure to evaluate the vendor's worst-case order fluctuation and show that for bounded forecast errors and demand fluctuations, this measure is equivalent to the disturbance to control peak gain.","To compute the controller that minimizes the worst-case peak gain, we formulate an optimization problem with bilinear matrix inequalities and show that solving this problem is equivalent to minimizing a quasi-convex function on a bounded domain.","In contrast to the existing Bullwhip measure in literature, the transient Bullwhip measure has an explicit dependency on the forecast error and does not need the forecast to be a deterministic function of the demand history.","This explicit dependency enables us to separately quantify the transient Bullwhip measure's sensitivity to forecast error and demand fluctuations.","We empirically verify our model for vendors with non-zero perishable rates and order backlogging rates."],"url":"http://arxiv.org/abs/2404.01090v1","category":"cs.ET"}
{"created":"2024-04-01 12:26:03","title":"Assessing the impact of instrument noise and astrophysical fluctuations on measurements of the first black hole photon ring","abstract":"Currently envisioned extensions of the Event Horizon Telescope to space will soon target the black hole photon ring: a narrow ring-shaped imprint of a black hole's strong gravity produced in its images by highly bent photon trajectories. In principle, the shape of the photon ring encodes information about the geometry of the underlying black hole spacetime. In practice, however, whether or not this information can be extracted from the ring shape depends on several factors, ranging from the astrophysical details of the emitting source (such as the magnitude of its plasma fluctuations) to the specific configuration of the interferometric array (such as the separation between its telescopes, or the level of noise in its instruments). Here, we employ a phenomenological model to assess the impact of astrophysical fluctuations and instrument noise on the inferred shape of the photon ring. Our systematic study of several astrophysical profiles suggests that this shape can be measured even in the presence of instrument noise across a wide range of baselines. The measurement accuracy and precision appear relatively insensitive to the noise level, up to a sharp threshold beyond which any measurement becomes incredibly challenging (at least without recourse to more sophisticated data analysis methods). Encouragingly, we find that only a few snapshot images are generally needed to overcome the impact of astrophysical fluctuations and correctly infer the ring diameter. Inference becomes more challenging when analyzing the visibility amplitude in a baseline window that is not entirely dominated by a single photon ring. Nevertheless, in most cases, it is still possible to fit a ring shape with the correct fractional asymmetry. These results provide excellent prospects for future precision measurements of black hole spin and fundamental astrophysics via black hole imaging.","sentences":["Currently envisioned extensions of the Event Horizon Telescope to space will soon target the black hole photon ring: a narrow ring-shaped imprint of a black hole's strong gravity produced in its images by highly bent photon trajectories.","In principle, the shape of the photon ring encodes information about the geometry of the underlying black hole spacetime.","In practice, however, whether or not this information can be extracted from the ring shape depends on several factors, ranging from the astrophysical details of the emitting source (such as the magnitude of its plasma fluctuations) to the specific configuration of the interferometric array (such as the separation between its telescopes, or the level of noise in its instruments).","Here, we employ a phenomenological model to assess the impact of astrophysical fluctuations and instrument noise on the inferred shape of the photon ring.","Our systematic study of several astrophysical profiles suggests that this shape can be measured even in the presence of instrument noise across a wide range of baselines.","The measurement accuracy and precision appear relatively insensitive to the noise level, up to a sharp threshold beyond which any measurement becomes incredibly challenging (at least without recourse to more sophisticated data analysis methods).","Encouragingly, we find that only a few snapshot images are generally needed to overcome the impact of astrophysical fluctuations and correctly infer the ring diameter.","Inference becomes more challenging when analyzing the visibility amplitude in a baseline window that is not entirely dominated by a single photon ring.","Nevertheless, in most cases, it is still possible to fit a ring shape with the correct fractional asymmetry.","These results provide excellent prospects for future precision measurements of black hole spin and fundamental astrophysics via black hole imaging."],"url":"http://arxiv.org/abs/2404.01083v1","category":"gr-qc"}
{"created":"2024-04-01 11:39:19","title":"Larger Nearly Orthogonal Sets over Finite Fields","abstract":"For a field $\\mathbb{F}$ and integers $d$ and $k$, a set ${\\cal A} \\subseteq \\mathbb{F}^d$ is called $k$-nearly orthogonal if its members are non-self-orthogonal and every $k+1$ vectors of ${\\cal A}$ include an orthogonal pair. We prove that for every prime $p$ there exists some $\\delta = \\delta(p)>0$, such that for every field $\\mathbb{F}$ of characteristic $p$ and for all integers $k \\geq 2$ and $d \\geq k$, there exists a $k$-nearly orthogonal set of at least $d^{\\delta \\cdot k/\\log k}$ vectors of $\\mathbb{F}^d$. The size of the set is optimal up to the $\\log k$ term in the exponent. We further prove two extensions of this result. In the first, we provide a large set ${\\cal A}$ of non-self-orthogonal vectors of $\\mathbb{F}^d$ such that for every two subsets of ${\\cal A}$ of size $k+1$ each, some vector of one of the subsets is orthogonal to some vector of the other. In the second extension, every $k+1$ vectors of the produced set ${\\cal A}$ include $\\ell+1$ pairwise orthogonal vectors for an arbitrary fixed integer $1 \\leq \\ell \\leq k$. The proofs involve probabilistic and spectral arguments and the hypergraph container method.","sentences":["For a field $\\mathbb{F}$ and integers $d$ and $k$, a set ${\\cal A} \\subseteq \\mathbb{F}^d$ is called $k$-nearly orthogonal if its members are non-self-orthogonal and every $k+1$ vectors of ${\\cal A}$ include an orthogonal pair.","We prove that for every prime $p$ there exists some $\\delta = \\delta(p)>0$, such that for every field $\\mathbb{F}$ of characteristic $p$ and for all integers $k \\geq 2$ and $d \\geq k$, there exists a $k$-nearly orthogonal set of at least $d^{\\delta \\cdot k/\\log k}$ vectors of $\\mathbb{F}^d$. The size of the set is optimal up to the $\\log k$ term in the exponent.","We further prove two extensions of this result.","In the first, we provide a large set ${\\cal A}$ of non-self-orthogonal vectors of $\\mathbb{F}^d$ such that for every two subsets of ${\\cal A}$ of size $k+1$ each, some vector of one of the subsets is orthogonal to some vector of the other.","In the second extension, every $k+1$ vectors of the produced set ${\\cal A}$ include $\\ell+1$ pairwise orthogonal vectors for an arbitrary fixed integer $1 \\leq \\ell \\leq k$.","The proofs involve probabilistic and spectral arguments and the hypergraph container method."],"url":"http://arxiv.org/abs/2404.01057v1","category":"math.CO"}
{"created":"2024-04-01 11:34:26","title":"Quantum circuit scheduler for QPUs usage optimization","abstract":"Progress in the realm of quantum technologies is paving the way for a multitude of potential applications across different sectors. However, the reduced number of available quantum computers, their technical limitations and the high demand for their use are posing some problems for developers and researchers. Mainly, users trying to execute quantum circuits on these devices are usually facing long waiting times in the tasks queues. In this context, this work propose a technique to reduce waiting times and optimize quantum computers usage by scheduling circuits from different users into combined circuits that are executed at the same time. To validate this proposal, different widely known quantum algorithms have been selected and executed in combined circuits. The obtained results are then compared with the results of executing the same algorithms in an isolated way. This allowed us to measure the impact of the use of the scheduler. Among the obtained results, it has been possible to verify that the noise suffered by executing a combination of circuits through the proposed scheduler does not critically affect the outcomes.","sentences":["Progress in the realm of quantum technologies is paving the way for a multitude of potential applications across different sectors.","However, the reduced number of available quantum computers, their technical limitations and the high demand for their use are posing some problems for developers and researchers.","Mainly, users trying to execute quantum circuits on these devices are usually facing long waiting times in the tasks queues.","In this context, this work propose a technique to reduce waiting times and optimize quantum computers usage by scheduling circuits from different users into combined circuits that are executed at the same time.","To validate this proposal, different widely known quantum algorithms have been selected and executed in combined circuits.","The obtained results are then compared with the results of executing the same algorithms in an isolated way.","This allowed us to measure the impact of the use of the scheduler.","Among the obtained results, it has been possible to verify that the noise suffered by executing a combination of circuits through the proposed scheduler does not critically affect the outcomes."],"url":"http://arxiv.org/abs/2404.01055v1","category":"cs.SE"}
{"created":"2024-04-01 11:26:50","title":"Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment","abstract":"Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective.","sentences":["Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding.","BoN sampling is susceptible to a problem known as reward hacking.","Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective.","A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model.","In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques.","We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective."],"url":"http://arxiv.org/abs/2404.01054v1","category":"cs.CL"}
{"created":"2024-04-01 11:09:40","title":"Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation","abstract":"Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise.","sentences":["Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models.","A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations.","This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing.","In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map.","The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor.","This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages.","Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing.","Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion.","Our codes are available at https://github.com/haofengl/DragNoise."],"url":"http://arxiv.org/abs/2404.01050v1","category":"cs.CV"}
{"created":"2024-04-01 11:08:53","title":"A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification","abstract":"This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network (CNN), we achieve state-of-the-art performance for star galaxy classification. Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings.","sentences":["This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18).","By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network (CNN), we achieve state-of-the-art performance for star galaxy classification.","Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings."],"url":"http://arxiv.org/abs/2404.01049v1","category":"astro-ph.IM"}
{"created":"2024-04-01 10:35:35","title":"Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification","abstract":"The recent emergence of the hybrid quantum-classical neural network (HQCNN) architecture has garnered considerable attention due to the potential advantages associated with integrating quantum principles to enhance various facets of machine learning algorithms and computations. However, the current investigated serial structure of HQCNN, wherein information sequentially passes from one network to another, often imposes limitations on the trainability and expressivity of the network. In this study, we introduce a novel architecture termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks (PPF-QSNN). The dataset information is simultaneously fed into both the spiking neural network and the variational quantum circuits, with the outputs amalgamated in proportion to their individual contributions. We systematically assess the impact of diverse PPF-QSNN parameters on network performance for image classification, aiming to identify the optimal configuration. Numerical results on the MNIST dataset unequivocally illustrate that our proposed PPF-QSNN outperforms both the existing spiking neural network and the serial quantum neural network across metrics such as accuracy, loss, and robustness. This study introduces a novel and effective amalgamation approach for HQCNN, thereby laying the groundwork for the advancement and application of quantum advantage in artificial intelligent computations.","sentences":["The recent emergence of the hybrid quantum-classical neural network (HQCNN) architecture has garnered considerable attention due to the potential advantages associated with integrating quantum principles to enhance various facets of machine learning algorithms and computations.","However, the current investigated serial structure of HQCNN, wherein information sequentially passes from one network to another, often imposes limitations on the trainability and expressivity of the network.","In this study, we introduce a novel architecture termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks (PPF-QSNN).","The dataset information is simultaneously fed into both the spiking neural network and the variational quantum circuits, with the outputs amalgamated in proportion to their individual contributions.","We systematically assess the impact of diverse PPF-QSNN parameters on network performance for image classification, aiming to identify the optimal configuration.","Numerical results on the MNIST dataset unequivocally illustrate that our proposed PPF-QSNN outperforms both the existing spiking neural network and the serial quantum neural network across metrics such as accuracy, loss, and robustness.","This study introduces a novel and effective amalgamation approach for HQCNN, thereby laying the groundwork for the advancement and application of quantum advantage in artificial intelligent computations."],"url":"http://arxiv.org/abs/2404.01359v1","category":"quant-ph"}
{"created":"2024-04-01 10:33:49","title":"Survival of Gas in Subhalos and Its Impact on the 21 cm Forest Signals: Insights from Hydrodynamic Simulations","abstract":"Understanding the survival of gas within subhalos under various astrophysical processes is crucial for elucidating cosmic structure formation and evolution. We study the resilience of gas in subhalos, focusing on the impact of tidal and ram pressure stripping through hydrodynamic simulations. Our results uncover significant gas stripping primarily driven by ram pressure effects, which also profoundly influence the gas distribution within these subhalos. Notably, despite their vulnerability to ram pressure effects, the low-mass subhalos can play a pivotal role in influencing the observable characteristics of cosmic structures due to their large abundance. Specifically, we explore the application of our findings to the 21 cm forest, showing how the survival dynamics of gas in subhalos can modulate the 21 cm optical depth, a key probe for detecting minihalos in the pre-reionization era. (abridged) In this work, we further investigate the contribution of subhalos to the 21 cm optical depth with hydrodynamics simulations, particularly highlighting the trajectories and fates of subhalos within mass ranges of \\(10^{4-6} M_{\\odot}h^{-1}\\) in a host halo of \\(10^7 M_{\\odot}h^{-1}\\). Despite their susceptibility to ram pressure stripping, the contribution of abundant low-mass subhalos to the 21-cm optical depth is more significant than that of their massive counterparts primarily due to their greater abundance. We find that the 21-cm optical depth can be increased by a factor of approximately two due to the abundant low-mass subhalos. (abridged) Our work provides critical insights into the gas dynamics within subhalos in the early Universe, highlighting their resilience against environmental stripping effects, and their impact on observable 21-cm signals.","sentences":["Understanding the survival of gas within subhalos under various astrophysical processes is crucial for elucidating cosmic structure formation and evolution.","We study the resilience of gas in subhalos, focusing on the impact of tidal and ram pressure stripping through hydrodynamic simulations.","Our results uncover significant gas stripping primarily driven by ram pressure effects, which also profoundly influence the gas distribution within these subhalos.","Notably, despite their vulnerability to ram pressure effects, the low-mass subhalos can play a pivotal role in influencing the observable characteristics of cosmic structures due to their large abundance.","Specifically, we explore the application of our findings to the 21 cm forest, showing how the survival dynamics of gas in subhalos can modulate the 21 cm optical depth, a key probe for detecting minihalos in the pre-reionization era.","(abridged)","In this work, we further investigate the contribution of subhalos to the 21 cm optical depth with hydrodynamics simulations, particularly highlighting the trajectories and fates of subhalos within mass ranges of \\(10^{4-6} M_{\\odot}h^{-1}\\) in a host halo of \\(10^7 M_{\\odot}h^{-1}\\).","Despite their susceptibility to ram pressure stripping, the contribution of abundant low-mass subhalos to the 21-cm optical depth is more significant than that of their massive counterparts primarily due to their greater abundance.","We find that the 21-cm optical depth can be increased by a factor of approximately two due to the abundant low-mass subhalos.","(abridged)","Our work provides critical insights into the gas dynamics within subhalos in the early Universe, highlighting their resilience against environmental stripping effects, and their impact on observable 21-cm signals."],"url":"http://arxiv.org/abs/2404.01034v1","category":"astro-ph.CO"}
{"created":"2024-04-01 08:46:35","title":"PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware Layout Generation","abstract":"Visual layout plays a critical role in graphic design fields such as advertising, posters, and web UI design. The recent trend towards content-aware layout generation through generative models has shown promise, yet it often overlooks the semantic intricacies of layout design by treating it as a simple numerical optimization. To bridge this gap, we introduce PosterLlama, a network designed for generating visually and textually coherent layouts by reformatting layout elements into HTML code and leveraging the rich design knowledge embedded within language models. Furthermore, we enhance the robustness of our model with a unique depth-based poster augmentation strategy. This ensures our generated layouts remain semantically rich but also visually appealing, even with limited data. Our extensive evaluations across several benchmarks demonstrate that PosterLlama outperforms existing methods in producing authentic and content-aware layouts. It supports an unparalleled range of conditions, including but not limited to unconditional layout generation, element conditional layout generation, layout completion, among others, serving as a highly versatile user manipulation tool.","sentences":["Visual layout plays a critical role in graphic design fields such as advertising, posters, and web UI design.","The recent trend towards content-aware layout generation through generative models has shown promise, yet it often overlooks the semantic intricacies of layout design by treating it as a simple numerical optimization.","To bridge this gap, we introduce PosterLlama, a network designed for generating visually and textually coherent layouts by reformatting layout elements into HTML code and leveraging the rich design knowledge embedded within language models.","Furthermore, we enhance the robustness of our model with a unique depth-based poster augmentation strategy.","This ensures our generated layouts remain semantically rich but also visually appealing, even with limited data.","Our extensive evaluations across several benchmarks demonstrate that PosterLlama outperforms existing methods in producing authentic and content-aware layouts.","It supports an unparalleled range of conditions, including but not limited to unconditional layout generation, element conditional layout generation, layout completion, among others, serving as a highly versatile user manipulation tool."],"url":"http://arxiv.org/abs/2404.00995v2","category":"cs.CV"}
{"created":"2024-04-01 08:20:18","title":"FlexiDreamer: Single Image-to-3D Generation with FlexiCubes","abstract":"3D content generation from text prompts or single images has made remarkable progress in quality and speed recently. One of its dominant paradigms involves generating consistent multi-view images followed by a sparse-view reconstruction. However, due to the challenge of directly deforming the mesh representation to approach the target topology, most methodologies learn an implicit representation (such as NeRF) during the sparse-view reconstruction and acquire the target mesh by a post-processing extraction. Although the implicit representation can effectively model rich 3D information, its training typically entails a long convergence time. In addition, the post-extraction operation from the implicit field also leads to undesirable visual artifacts. In this paper, we propose FlexiDreamer, a novel single image-to-3d generation framework that reconstructs the target mesh in an end-to-end manner. By leveraging a flexible gradient-based extraction known as FlexiCubes, our method circumvents the defects brought by the post-processing and facilitates a direct acquisition of the target mesh. Furthermore, we incorporate a multi-resolution hash grid encoding scheme that progressively activates the encoding levels into the implicit field in FlexiCubes to help capture geometric details for per-step optimization. Notably, FlexiDreamer recovers a dense 3D structure from a single-view image in approximately 1 minute on a single NVIDIA A100 GPU, outperforming previous methodologies by a large margin.","sentences":["3D content generation from text prompts or single images has made remarkable progress in quality and speed recently.","One of its dominant paradigms involves generating consistent multi-view images followed by a sparse-view reconstruction.","However, due to the challenge of directly deforming the mesh representation to approach the target topology, most methodologies learn an implicit representation (such as NeRF) during the sparse-view reconstruction and acquire the target mesh by a post-processing extraction.","Although the implicit representation can effectively model rich 3D information, its training typically entails a long convergence time.","In addition, the post-extraction operation from the implicit field also leads to undesirable visual artifacts.","In this paper, we propose FlexiDreamer, a novel single image-to-3d generation framework that reconstructs the target mesh in an end-to-end manner.","By leveraging a flexible gradient-based extraction known as FlexiCubes, our method circumvents the defects brought by the post-processing and facilitates a direct acquisition of the target mesh.","Furthermore, we incorporate a multi-resolution hash grid encoding scheme that progressively activates the encoding levels into the implicit field in FlexiCubes to help capture geometric details for per-step optimization.","Notably, FlexiDreamer recovers a dense 3D structure from a single-view image in approximately 1 minute on a single NVIDIA A100 GPU, outperforming previous methodologies by a large margin."],"url":"http://arxiv.org/abs/2404.00987v1","category":"cs.CV"}
{"created":"2024-04-01 08:18:38","title":"Make Continual Learning Stronger via C-Flat","abstract":"Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code will be publicly available upon publication.","sentences":["Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL).","Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD.","Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance.","In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL.","C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods.","A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases.","Code will be publicly available upon publication."],"url":"http://arxiv.org/abs/2404.00986v1","category":"cs.LG"}
{"created":"2024-04-01 07:56:10","title":"Wideband Channel Capacity Maximization With Beyond Diagonal RIS Reflection Matrices","abstract":"Following the promising beamforming gains offered by reconfigurable intelligent surfaces (RISs), a new hardware architecture, known as \\emph{beyond diagonal RIS (BD-RIS)}, has recently been proposed. This architecture enables controllable signal flows between the RIS elements, thereby providing greater design flexibility. However, the physics-imposed symmetry and orthogonality conditions on the non-diagonal reflection matrix make the design challenging. In this letter, we analyze how a BD-RIS can improve a wideband channel, starting from fundamental principles and deriving the capacity. Our analysis considers the effects of various channel taps and their frequency-domain characteristics. We introduce a new algorithm designed to optimize the configuration of the BD-RIS to maximize wideband capacity. The proposed algorithm has better performance than the benchmarks. A BD-RIS is beneficial compared to a conventional RIS in the absence of static path or when the Rician $\\kappa$-factor is smaller than $10$.","sentences":["Following the promising beamforming gains offered by reconfigurable intelligent surfaces (RISs), a new hardware architecture, known as \\emph{beyond diagonal RIS (BD-RIS)}, has recently been proposed.","This architecture enables controllable signal flows between the RIS elements, thereby providing greater design flexibility.","However, the physics-imposed symmetry and orthogonality conditions on the non-diagonal reflection matrix make the design challenging.","In this letter, we analyze how a BD-RIS can improve a wideband channel, starting from fundamental principles and deriving the capacity.","Our analysis considers the effects of various channel taps and their frequency-domain characteristics.","We introduce a new algorithm designed to optimize the configuration of the BD-RIS to maximize wideband capacity.","The proposed algorithm has better performance than the benchmarks.","A BD-RIS is beneficial compared to a conventional RIS in the absence of static path or when the Rician $\\kappa$-factor is smaller than $10$."],"url":"http://arxiv.org/abs/2404.00982v1","category":"eess.SP"}
{"created":"2024-04-01 07:52:05","title":"CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement Learning","abstract":"Optical proximity correction (OPC) is a vital step to ensure printability in modern VLSI manufacturing. Various OPC approaches based on machine learning have been proposed to pursue performance and efficiency, which are typically data-driven and hardly involve any particular considerations of the OPC problem, leading to potential performance or efficiency bottlenecks. In this paper, we propose CAMO, a reinforcement learning-based OPC system that specifically integrates important principles of the OPC problem. CAMO explicitly involves the spatial correlation among the movements of neighboring segments and an OPC-inspired modulation for movement action selection. Experiments are conducted on both via layer patterns and metal layer patterns. The results demonstrate that CAMO outperforms state-of-the-art OPC engines from both academia and industry.","sentences":["Optical proximity correction (OPC) is a vital step to ensure printability in modern VLSI manufacturing.","Various OPC approaches based on machine learning have been proposed to pursue performance and efficiency, which are typically data-driven and hardly involve any particular considerations of the OPC problem, leading to potential performance or efficiency bottlenecks.","In this paper, we propose CAMO, a reinforcement learning-based OPC system that specifically integrates important principles of the OPC problem.","CAMO explicitly involves the spatial correlation among the movements of neighboring segments and an OPC-inspired modulation for movement action selection.","Experiments are conducted on both via layer patterns and metal layer patterns.","The results demonstrate that CAMO outperforms state-of-the-art OPC engines from both academia and industry."],"url":"http://arxiv.org/abs/2404.00980v1","category":"cs.CV"}
{"created":"2024-04-01 07:49:11","title":"Prior Constraints-based Reward Model Training for Aligning Large Language Models","abstract":"Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling. As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as direct preference optimization, and can yield consistent improvement.","sentences":["Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.","However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.","This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem.","PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins.","We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL.","Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling.","As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as direct preference optimization, and can yield consistent improvement."],"url":"http://arxiv.org/abs/2404.00978v1","category":"cs.CL"}
{"created":"2024-04-01 07:24:13","title":"Optimal Bidding Strategies in Network-Constrained Demand Response: A Distributed Aggregative Game Theoretic Approach","abstract":"Demand response has been a promising solution for accommodating renewable energy in power systems. In this study, we consider a demand response scheme within a distribution network facing an energy supply deficit. The utility company incentivizes load aggregators to adjust their pre-scheduled energy consumption and generation to match the supply. Each aggregator, which represents a group of prosumers, aims to maximize its revenue by bidding strategically in the demand response scheme. Since aggregators act in their own self-interest and their revenues and feasible bids influence one another, we model their competition as a network-constrained aggregative game. This model incorporates power flow constraints to prevent potential line congestion. Given that there are no coordinators and aggregators can only communicate with their neighbours, we introduce a fully distributed generalized Nash equilibrium seeking algorithm to determine the optimal bidding strategies for aggregators in this game. Within this algorithm, only estimates of the aggregate and certain auxiliary variables are communicated among neighbouring aggregators. We demonstrate the convergence of this algorithm by constructing an equivalent iteration using the forward-backward splitting technique.","sentences":["Demand response has been a promising solution for accommodating renewable energy in power systems.","In this study, we consider a demand response scheme within a distribution network facing an energy supply deficit.","The utility company incentivizes load aggregators to adjust their pre-scheduled energy consumption and generation to match the supply.","Each aggregator, which represents a group of prosumers, aims to maximize its revenue by bidding strategically in the demand response scheme.","Since aggregators act in their own self-interest and their revenues and feasible bids influence one another, we model their competition as a network-constrained aggregative game.","This model incorporates power flow constraints to prevent potential line congestion.","Given that there are no coordinators and aggregators can only communicate with their neighbours, we introduce a fully distributed generalized Nash equilibrium seeking algorithm to determine the optimal bidding strategies for aggregators in this game.","Within this algorithm, only estimates of the aggregate and certain auxiliary variables are communicated among neighbouring aggregators.","We demonstrate the convergence of this algorithm by constructing an equivalent iteration using the forward-backward splitting technique."],"url":"http://arxiv.org/abs/2404.00968v1","category":"eess.SY"}
{"created":"2024-04-01 07:22:20","title":"Designing gradient coils with the shape derivative and the closed B-spline curves","abstract":"This study proposes a versatile and efficient optimisation method for discrete coils that induce a magnetic field by their steady currents. The prime target is gradient coils for MRI (Magnetic Resonance Imaging). The derivative (gradient) of the $z$-component the magnetic field, which is calculated by the Biot--Savart's law, with respect to the $z$-coordinate in the Cartesian $xyz$ coordinate system is considered as the objective function. Then, the derivative of the objective function with respect to a change of coils in shape is formulated according to the concept of shape optimisation. The resulting shape derivative (as well as the Biot--Savart's law) is smoothly discretised with the closed B-spline curves. In this case, the control points (CPs) of the curves are naturally selected as the design variables. As a consequence, the shape derivative is discretised to the sensitivities of the objective function with respect to the CPs. Those sensitivities are available to solve the present shape-optimisation problem with a certain gradient-based nonlinear-programming solver. The numerical examples exhibit the mathematical reliability, computational efficiency, and engineering applicability of the proposed methodology based on the shape derivative/sensitivities and the closed B-spline curves.","sentences":["This study proposes a versatile and efficient optimisation method for discrete coils that induce a magnetic field by their steady currents.","The prime target is gradient coils for MRI (Magnetic Resonance Imaging).","The derivative (gradient) of the $z$-component the magnetic field, which is calculated by the Biot--Savart's law, with respect to the $z$-coordinate in the Cartesian $xyz$ coordinate system is considered as the objective function.","Then, the derivative of the objective function with respect to a change of coils in shape is formulated according to the concept of shape optimisation.","The resulting shape derivative (as well as the Biot--Savart's law) is smoothly discretised with the closed B-spline curves.","In this case, the control points (CPs) of the curves are naturally selected as the design variables.","As a consequence, the shape derivative is discretised to the sensitivities of the objective function with respect to the CPs.","Those sensitivities are available to solve the present shape-optimisation problem with a certain gradient-based nonlinear-programming solver.","The numerical examples exhibit the mathematical reliability, computational efficiency, and engineering applicability of the proposed methodology based on the shape derivative/sensitivities and the closed B-spline curves."],"url":"http://arxiv.org/abs/2404.00967v1","category":"physics.med-ph"}
{"created":"2024-04-01 07:22:12","title":"GTS: GPU-based Tree Index for Fast Similarity Search","abstract":"Similarity search, the task of identifying objects most similar to a given query object under a specific metric, has gathered significant attention due to its practical applications. However, the absence of coordinate information to accelerate similarity search and the high computational cost of measuring object similarity hinder the efficiency of existing CPU-based methods. Additionally, these methods struggle to meet the demand for high throughput data management. To address these challenges, we propose GTS, a GPU-based tree index designed for the parallel processing of similarity search in general metric spaces, where only the distance metric for measuring object similarity is known. The GTS index utilizes a pivot-based tree structure to efficiently prune objects and employs list tables to facilitate GPU computing. To efficiently manage concurrent similarity queries with limited GPU memory, we have developed a two-stage search method that combines batch processing and sequential strategies to optimize memory usage. The paper also introduces an effective update strategy for the proposed GPU-based index, encompassing streaming data updates and batch data updates. Additionally, we present a cost model to evaluate search performance. Extensive experiments on five real-life datasets demonstrate that GTS achieves efficiency gains of up to two orders of magnitude over existing CPU baselines and up to 20x efficiency improvements compared to state-of-the-art GPU-based methods.","sentences":["Similarity search, the task of identifying objects most similar to a given query object under a specific metric, has gathered significant attention due to its practical applications.","However, the absence of coordinate information to accelerate similarity search and the high computational cost of measuring object similarity hinder the efficiency of existing CPU-based methods.","Additionally, these methods struggle to meet the demand for high throughput data management.","To address these challenges, we propose GTS, a GPU-based tree index designed for the parallel processing of similarity search in general metric spaces, where only the distance metric for measuring object similarity is known.","The GTS index utilizes a pivot-based tree structure to efficiently prune objects and employs list tables to facilitate GPU computing.","To efficiently manage concurrent similarity queries with limited GPU memory, we have developed a two-stage search method that combines batch processing and sequential strategies to optimize memory usage.","The paper also introduces an effective update strategy for the proposed GPU-based index, encompassing streaming data updates and batch data updates.","Additionally, we present a cost model to evaluate search performance.","Extensive experiments on five real-life datasets demonstrate that GTS achieves efficiency gains of up to two orders of magnitude over existing CPU baselines and up to 20x efficiency improvements compared to state-of-the-art GPU-based methods."],"url":"http://arxiv.org/abs/2404.00966v1","category":"cs.DB"}
{"created":"2024-04-01 06:59:56","title":"Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence","abstract":"Unsupervised non-rigid point cloud shape correspondence underpins a multitude of 3D vision tasks, yet itself is non-trivial given the exponential complexity stemming from inter-point degree-of-freedom, i.e., pose transformations. Based on the assumption of local rigidity, one solution for reducing complexity is to decompose the overall shape into independent local regions using Local Reference Frames (LRFs) that are invariant to SE(3) transformations. However, the focus solely on local structure neglects global geometric contexts, resulting in less distinctive LRFs that lack crucial semantic information necessary for effective matching. Furthermore, such complexity introduces out-of-distribution geometric contexts during inference, thus complicating generalization. To this end, we introduce 1) EquiShape, a novel structure tailored to learn pair-wise LRFs with global structural cues for both spatial and semantic consistency, and 2) LRF-Refine, an optimization strategy generally applicable to LRF-based methods, aimed at addressing the generalization challenges. Specifically, for EquiShape, we employ cross-talk within separate equivariant graph neural networks (Cross-GVP) to build long-range dependencies to compensate for the lack of semantic information in local structure modeling, deducing pair-wise independent SE(3)-equivariant LRF vectors for each point. For LRF-Refine, the optimization adjusts LRFs within specific contexts and knowledge, enhancing the geometric and semantic generalizability of point features. Our overall framework surpasses the state-of-the-art methods by a large margin on three benchmarks. Code and models will be publicly available.","sentences":["Unsupervised non-rigid point cloud shape correspondence underpins a multitude of 3D vision tasks, yet itself is non-trivial given the exponential complexity stemming from inter-point degree-of-freedom, i.e., pose transformations.","Based on the assumption of local rigidity, one solution for reducing complexity is to decompose the overall shape into independent local regions using Local Reference Frames (LRFs) that are invariant to SE(3) transformations.","However, the focus solely on local structure neglects global geometric contexts, resulting in less distinctive LRFs that lack crucial semantic information necessary for effective matching.","Furthermore, such complexity introduces out-of-distribution geometric contexts during inference, thus complicating generalization.","To this end, we introduce 1) EquiShape, a novel structure tailored to learn pair-wise LRFs with global structural cues for both spatial and semantic consistency, and 2) LRF-Refine, an optimization strategy generally applicable to LRF-based methods, aimed at addressing the generalization challenges.","Specifically, for EquiShape, we employ cross-talk within separate equivariant graph neural networks (Cross-GVP) to build long-range dependencies to compensate for the lack of semantic information in local structure modeling, deducing pair-wise independent SE(3)-equivariant LRF vectors for each point.","For LRF-Refine, the optimization adjusts LRFs within specific contexts and knowledge, enhancing the geometric and semantic generalizability of point features.","Our overall framework surpasses the state-of-the-art methods by a large margin on three benchmarks.","Code and models will be publicly available."],"url":"http://arxiv.org/abs/2404.00959v1","category":"cs.CV"}
{"created":"2024-04-01 06:38:58","title":"Digital Twins for Supporting AI Research with Autonomous Vehicle Networks","abstract":"Digital twins (DTs), which are virtual environments that simulate, predict, and optimize the performance of their physical counterparts, are envisioned to be essential technologies for advancing next-generation wireless networks. While DTs have been studied extensively for wireless networks, their use in conjunction with autonomous vehicles with programmable mobility remains relatively under-explored. In this paper, we study DTs used as a development environment to design, deploy, and test artificial intelligence (AI) techniques that use real-time observations, e.g. radio key performance indicators, for vehicle trajectory and network optimization decisions in an autonomous vehicle networks (AVN). We first compare and contrast the use of simulation, digital twin (software in the loop (SITL)), sandbox (hardware-in-the-loop (HITL)), and physical testbed environments for their suitability in developing and testing AI algorithms for AVNs. We then review various representative use cases of DTs for AVN scenarios. Finally, we provide an example from the NSF AERPAW platform where a DT is used to develop and test AI-aided solutions for autonomous unmanned aerial vehicles for localizing a signal source based solely on link quality measurements. Our results in the physical testbed show that SITL DTs, when supplemented with data from real-world (RW) measurements and simulations, can serve as an ideal environment for developing and testing innovative AI solutions for AVNs.","sentences":["Digital twins (DTs), which are virtual environments that simulate, predict, and optimize the performance of their physical counterparts, are envisioned to be essential technologies for advancing next-generation wireless networks.","While DTs have been studied extensively for wireless networks, their use in conjunction with autonomous vehicles with programmable mobility remains relatively under-explored.","In this paper, we study DTs used as a development environment to design, deploy, and test artificial intelligence (AI) techniques that use real-time observations, e.g. radio key performance indicators, for vehicle trajectory and network optimization decisions in an autonomous vehicle networks (AVN).","We first compare and contrast the use of simulation, digital twin (software in the loop (SITL)), sandbox (hardware-in-the-loop (HITL)), and physical testbed environments for their suitability in developing and testing AI algorithms for AVNs.","We then review various representative use cases of DTs for AVN scenarios.","Finally, we provide an example from the NSF AERPAW platform where a DT is used to develop and test AI-aided solutions for autonomous unmanned aerial vehicles for localizing a signal source based solely on link quality measurements.","Our results in the physical testbed show that SITL DTs, when supplemented with data from real-world (RW) measurements and simulations, can serve as an ideal environment for developing and testing innovative AI solutions for AVNs."],"url":"http://arxiv.org/abs/2404.00954v1","category":"eess.SP"}
{"created":"2024-04-01 06:37:29","title":"Movable Antenna-Aided Hybrid Beamforming for Multi-User Communications","abstract":"In this correspondence, we propose a movable antenna (MA)-aided multi-user hybrid beamforming scheme with a sub-connected structure, where multiple movable sub-arrays can independently change their positions within different local regions. To maximize the system sum rate, we jointly optimize the digital beamformer, analog beamformer, and positions of subarrays, under the constraints of unit modulus, finite movable regions, and power budget. Due to the non-concave/non-convex objective function/constraints, as well as the highly coupled variables, the formulated problem is challenging to solve. By employing fractional programming, we develop an alternating optimization framework to solve the problem via a combination of Lagrange multipliers, penalty method, and gradient descent. Numerical results reveal that the proposed MA-aided hybrid beamforming scheme significantly improves the sum rate compared to its fixed-position antenna (FPA) counterpart. Moreover, with sufficiently large movable regions, the proposed scheme with sub-connected MA arrays even outperforms the fully-connected FPA array.","sentences":["In this correspondence, we propose a movable antenna (MA)-aided multi-user hybrid beamforming scheme with a sub-connected structure, where multiple movable sub-arrays can independently change their positions within different local regions.","To maximize the system sum rate, we jointly optimize the digital beamformer, analog beamformer, and positions of subarrays, under the constraints of unit modulus, finite movable regions, and power budget.","Due to the non-concave/non-convex objective function/constraints, as well as the highly coupled variables, the formulated problem is challenging to solve.","By employing fractional programming, we develop an alternating optimization framework to solve the problem via a combination of Lagrange multipliers, penalty method, and gradient descent.","Numerical results reveal that the proposed MA-aided hybrid beamforming scheme significantly improves the sum rate compared to its fixed-position antenna (FPA) counterpart.","Moreover, with sufficiently large movable regions, the proposed scheme with sub-connected MA arrays even outperforms the fully-connected FPA array."],"url":"http://arxiv.org/abs/2404.00953v1","category":"cs.IT"}
{"created":"2024-04-01 05:39:36","title":"ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback","abstract":"ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.","sentences":["ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs).","In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences.","ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies.","Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges.","We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs.","Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM.","For instance, it achieves on average 15\\% more wins against ChatGLM-SFT in Chinese alignment tasks.","The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations."],"url":"http://arxiv.org/abs/2404.00934v1","category":"cs.CL"}
{"created":"2024-04-01 05:12:55","title":"VortexViz: Finding Vortex Boundaries by Learning from Particle Trajectories","abstract":"Vortices are studied in various scientific disciplines, offering insights into fluid flow behavior. Visualizing the boundary of vortices is crucial for understanding flow phenomena and detecting flow irregularities. This paper addresses the challenge of accurately extracting vortex boundaries using deep learning techniques. While existing methods primarily train on velocity components, we propose a novel approach incorporating particle trajectories (streamlines or pathlines) into the learning process. By leveraging the regional/local characteristics of the flow field captured by streamlines or pathlines, our methodology aims to enhance the accuracy of vortex boundary extraction.","sentences":["Vortices are studied in various scientific disciplines, offering insights into fluid flow behavior.","Visualizing the boundary of vortices is crucial for understanding flow phenomena and detecting flow irregularities.","This paper addresses the challenge of accurately extracting vortex boundaries using deep learning techniques.","While existing methods primarily train on velocity components, we propose a novel approach incorporating particle trajectories (streamlines or pathlines) into the learning process.","By leveraging the regional/local characteristics of the flow field captured by streamlines or pathlines, our methodology aims to enhance the accuracy of vortex boundary extraction."],"url":"http://arxiv.org/abs/2404.01352v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 17:30:46","title":"Search for flavour-changing neutral-current couplings between the top quark and the Higgs boson in multi-lepton final states in 13 TeV $pp$ collisions with the ATLAS detector","abstract":"A search is presented for flavour-changing neutral-current interactions involving the top quark, the Higgs boson and an up-type quark ($q=u,c$) with the ATLAS detector at the Large Hadron Collider. The analysis considers leptonic decays of the top quark along with Higgs boson decays into two $W$ bosons, two $Z$ bosons or a $\\tau^{+}\\tau^{-}$ pair. It focuses on final states containing either two leptons (electrons or muons) of the same charge or three leptons. The considered processes are $t\\bar{t}$ and $Ht$ production. For the $t\\bar{t}$ production, one top quark decays via $t\\to Hq$. The proton-proton collision data set analysed amounts to 140 fb$^{-1}$ at $\\sqrt{s}=13$ TeV. No significant excess beyond Standard Model expectations is observed and upper limits are set on the $t\\to Hq$ branching ratios at 95\\% confidence level, amounting to observed (expected) limits of $\\mathcal{B}(t\\to Hu)<2.8\\,(3.0) \\times 10^{-4}$ and $\\mathcal{B}(t\\to Hc)<3.3\\,(3.8) \\times 10^{-4}$. Combining this search with other searches for $tHq$ flavour-changing neutral-current interactions previously conducted by ATLAS, considering $H\\to b\\bar{b}$ and $H\\to\\gamma\\gamma$ decays, as well as $H\\to\\tau^{+}\\tau^{-}$ decays with one or two hadronically decaying $\\tau$-leptons, yields observed (expected) upper limits on the branching ratios of $\\mathcal{B}(t\\to Hu)<2.6\\,(1.8) \\times 10^{-4}$ and $\\mathcal{B}(t\\to Hc)<3.4\\,(2.3) \\times 10^{-4}$.","sentences":["A search is presented for flavour-changing neutral-current interactions involving the top quark, the Higgs boson and an up-type quark ($q=u,c$) with the ATLAS detector at the Large Hadron Collider.","The analysis considers leptonic decays of the top quark along with Higgs boson decays into two $W$ bosons, two $Z$ bosons or a $\\tau^{+}\\tau^{-}$ pair.","It focuses on final states containing either two leptons (electrons or muons) of the same charge or three leptons.","The considered processes are $t\\bar{t}$ and $Ht$ production.","For the $t\\bar{t}$ production, one top quark decays via $t\\to Hq$.","The proton-proton collision data set analysed amounts to 140 fb$^{-1}$","at $\\sqrt{s}=13$ TeV. No significant excess beyond Standard Model expectations is observed and upper limits are set on the $t\\to Hq$ branching ratios at 95\\% confidence level, amounting to observed (expected) limits of $\\mathcal{B}(t\\to Hu)<2.8\\,(3.0)","\\times 10^{-4}$ and $\\mathcal{B}(t\\to Hc)<3.3\\,(3.8)","\\times 10^{-4}$.","Combining this search with other searches for $tHq$ flavour-changing neutral-current interactions previously conducted by ATLAS, considering $H\\to b\\bar{b}$ and $H\\to\\gamma\\gamma$ decays, as well as $H\\to\\tau^{+}\\tau^{-}$ decays with one or two hadronically decaying $\\tau$-leptons, yields observed (expected) upper limits on the branching ratios of $\\mathcal{B}(t\\to Hu)<2.6\\,(1.8) \\times 10^{-4}$ and $\\mathcal{B}(t\\to Hc)<3.4\\,(2.3)","\\times 10^{-4}$."],"url":"http://arxiv.org/abs/2404.02123v1","category":"hep-ex"}
{"created":"2024-04-02 17:10:33","title":"The Effects of Group Sanctions on Participation and Toxicity: Quasi-experimental Evidence from the Fediverse","abstract":"Online communities often overlap and coexist, despite incongruent norms and approaches to content moderation. When communities diverge, decentralized and federated communities may pursue group-level sanctions, including defederation (disconnection) to block communication between members of specific communities. We investigate the effects of defederation in the context of the Fediverse, a set of decentralized, interconnected social networks with independent governance. Mastodon and Pleroma, the most popular software powering the Fediverse, allow administrators on one server to defederate from another. We use a difference-in-differences approach and matched controls to estimate the effects of defederation events on participation and message toxicity among affected members of the blocked and blocking servers. We find that defederation causes a drop in activity for accounts on the blocked servers, but not on the blocking servers. Also, we find no evidence of an effect of defederation on message toxicity.","sentences":["Online communities often overlap and coexist, despite incongruent norms and approaches to content moderation.","When communities diverge, decentralized and federated communities may pursue group-level sanctions, including defederation (disconnection) to block communication between members of specific communities.","We investigate the effects of defederation in the context of the Fediverse, a set of decentralized, interconnected social networks with independent governance.","Mastodon and Pleroma, the most popular software powering the Fediverse, allow administrators on one server to defederate from another.","We use a difference-in-differences approach and matched controls to estimate the effects of defederation events on participation and message toxicity among affected members of the blocked and blocking servers.","We find that defederation causes a drop in activity for accounts on the blocked servers, but not on the blocking servers.","Also, we find no evidence of an effect of defederation on message toxicity."],"url":"http://arxiv.org/abs/2404.02109v1","category":"cs.SI"}
{"created":"2024-04-02 16:21:23","title":"Results of the 2022 ECFA Early-Career Researchers Panel survey on career prospects and diversity","abstract":"This document presents the outcomes of a comprehensive survey conducted among early career researchers (ECRs) in academic particle physics. Running from September 24, 2022, to March 3, 2023, the survey gathered responses from 759 ECRs employed in 39 countries. The study aimed to gain insights into the career prospects and experiences of ECRs while also delving into diversity and sociological aspects within particle physics research. The survey results are presented in a manner consistent with the survey choices. The document offers insights for the particle physics community, and provides a set of recommendations for enhancing career prospects, fostering diversity, and addressing sociological dimensions within this field.","sentences":["This document presents the outcomes of a comprehensive survey conducted among early career researchers (ECRs) in academic particle physics.","Running from September 24, 2022, to March 3, 2023, the survey gathered responses from 759 ECRs employed in 39 countries.","The study aimed to gain insights into the career prospects and experiences of ECRs while also delving into diversity and sociological aspects within particle physics research.","The survey results are presented in a manner consistent with the survey choices.","The document offers insights for the particle physics community, and provides a set of recommendations for enhancing career prospects, fostering diversity, and addressing sociological dimensions within this field."],"url":"http://arxiv.org/abs/2404.02074v1","category":"physics.soc-ph"}
{"created":"2024-04-02 16:05:53","title":"QDarts: A Quantum Dot Array Transition Simulator for finding charge transitions in the presence of finite tunnel couplings, non-constant charging energies and sensor dots","abstract":"We present QDarts, an efficient simulator for realistic charge stability diagrams of quantum dot array (QDA) devices in equilibrium states. It allows for pinpointing the location of concrete charge states and their transitions in a high-dimensional voltage space (via arbitrary two-dimensional cuts through it), and includes effects of finite tunnel coupling, non-constant charging energy and a simulation of noisy sensor dots. These features enable close matching of various experimental results in the literature, and the package hence provides a flexible tool for testing QDA experiments, as well as opening the avenue for developing new methods of device tuning.","sentences":["We present QDarts, an efficient simulator for realistic charge stability diagrams of quantum dot array (QDA) devices in equilibrium states.","It allows for pinpointing the location of concrete charge states and their transitions in a high-dimensional voltage space (via arbitrary two-dimensional cuts through it), and includes effects of finite tunnel coupling, non-constant charging energy and a simulation of noisy sensor dots.","These features enable close matching of various experimental results in the literature, and the package hence provides a flexible tool for testing QDA experiments, as well as opening the avenue for developing new methods of device tuning."],"url":"http://arxiv.org/abs/2404.02064v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 15:46:54","title":"The Thousand-Pulsar-Array programme on MeerKAT XIII: Timing, flux density, rotation measure and dispersion measure timeseries of 597 pulsars","abstract":"We report here on the timing of 597 pulsars over the last four years with the MeerKAT telescope. We provide Times-of-Arrival, pulsar ephemeris files and per-epoch measurements of the flux density, dispersion measure (DM) and rotation measure (RM) for each pulsar. In addition we use a Gaussian process to model the timing residuals to measure the spin frequency derivative at each epoch. We also report the detection of 11 glitches in 9 individual pulsars. We find significant DM and RM variations in 87 and 76 pulsars respectively. We find that the DM variations scale approximately linearly with DM, which is broadly in agreement with models of the ionised interstellar medium. The observed RM variations seem largely independent of DM, which may suggest that the RM variations are dominated by variations in the interstellar magnetic field on the line of sight, rather than varying electron density. We also find that normal pulsars have around 5 times greater amplitude of DM variability compared to millisecond pulsars, and surmise that this is due to the known difference in their velocity distributions.","sentences":["We report here on the timing of 597 pulsars over the last four years with the MeerKAT telescope.","We provide Times-of-Arrival, pulsar ephemeris files and per-epoch measurements of the flux density, dispersion measure (DM) and rotation measure (RM) for each pulsar.","In addition we use a Gaussian process to model the timing residuals to measure the spin frequency derivative at each epoch.","We also report the detection of 11 glitches in 9 individual pulsars.","We find significant DM and RM variations in 87 and 76 pulsars respectively.","We find that the DM variations scale approximately linearly with DM, which is broadly in agreement with models of the ionised interstellar medium.","The observed RM variations seem largely independent of DM, which may suggest that the RM variations are dominated by variations in the interstellar magnetic field on the line of sight, rather than varying electron density.","We also find that normal pulsars have around 5 times greater amplitude of DM variability compared to millisecond pulsars, and surmise that this is due to the known difference in their velocity distributions."],"url":"http://arxiv.org/abs/2404.02051v1","category":"astro-ph.HE"}
{"created":"2024-04-02 14:40:31","title":"Search for pair-produced higgsinos decaying via Higgs or $Z$ bosons to final states containing a pair of photons and a pair of $b$-jets with the ATLAS detector","abstract":"A search is presented for the pair production of higgsinos $\\tilde{\\chi}$ in gauge-mediated supersymmetry models, where the lightest neutralinos $\\tilde{\\chi}_1^0$ decay into a light gravitino $\\tilde{G}$ either via a Higgs $h$ or $Z$ boson. The search is performed with the ATLAS detector at the Large Hadron Collider using 139 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}$ = 13 TeV. It targets final states in which a Higgs boson decays into a photon pair, while the other Higgs or $Z$ boson decays into a $b\\bar{b}$ pair, with missing transverse momentum associated with the two gravitinos. Search regions dependent on the amount of missing transverse momentum are defined by the requirements that the diphoton mass should be consistent with the mass of the Higgs boson, and the $b\\bar{b}$ mass with the mass of the Higgs or $Z$ boson. The main backgrounds are estimated with data-driven methods using the sidebands of the diphoton mass distribution. No excesses beyond Standard Model expectations are observed and higgsinos with masses up to 320 GeV are excluded, assuming a branching fraction of 100% for $\\tilde{\\chi}_1^0\\rightarrow h\\tilde{G}$. This analysis excludes higgsinos with masses of 130 GeV for branching fractions to $h\\tilde{G}$ as low as 36%, thus providing complementarity to previous ATLAS searches in final states with multiple leptons or multiple $b$-jets, targeting different decays of the electroweak bosons.","sentences":["A search is presented for the pair production of higgsinos $\\tilde{\\chi}$ in gauge-mediated supersymmetry models, where the lightest neutralinos $\\tilde{\\chi}_1^0$ decay into a light gravitino $\\tilde{G}$ either via a Higgs $h$ or $Z$ boson.","The search is performed with the ATLAS detector at the Large Hadron Collider using 139 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}$ = 13 TeV. It targets final states in which a Higgs boson decays into a photon pair, while the other Higgs or $Z$ boson decays into a $b\\bar{b}$ pair, with missing transverse momentum associated with the two gravitinos.","Search regions dependent on the amount of missing transverse momentum are defined by the requirements that the diphoton mass should be consistent with the mass of the Higgs boson, and the $b\\bar{b}$ mass with the mass of the Higgs or $Z$ boson.","The main backgrounds are estimated with data-driven methods using the sidebands of the diphoton mass distribution.","No excesses beyond Standard Model expectations are observed and higgsinos with masses up to 320 GeV are excluded, assuming a branching fraction of 100% for $\\tilde{\\chi}_1^0\\rightarrow","h\\tilde{G}$.","This analysis excludes higgsinos with masses of 130 GeV for branching fractions to $h\\tilde{G}$ as low as 36%, thus providing complementarity to previous ATLAS searches in final states with multiple leptons or multiple $b$-jets, targeting different decays of the electroweak bosons."],"url":"http://arxiv.org/abs/2404.01996v1","category":"hep-ex"}
{"created":"2024-04-02 14:35:09","title":"CME Deflection and East-West Asymmetry of ESP Intensity in Solar Cycles 23 and 24","abstract":"We investigate the East-West asymmetry in energetic storm particle (ESP) heavy ion intensities at interplanetary shocks driven by coronal mass ejections (CMEs) during solar cycles (SCs) 23 and 24. We use observations from NASA's ACE and STEREO missions of helium (He), oxygen (O), and iron (Fe) intensities from ~0.13 to 3 MeV/nucleon. We examine the longitudinal distribution of ESP intensities and the correlation of ESP intensities with the near-Sun CME speed and the average transit CME speed for eastern and western events. We observed an East-West asymmetry reversal of ESP heavy ion intensities from SC 23 to 24. We have determined that this change in asymmetry is caused by a shift in the heliolongitude distribution of the CME speed ratio (the ratio of CME near-Sun speed to CME average transit speed) from west to east.","sentences":["We investigate the East-West asymmetry in energetic storm particle (ESP) heavy ion intensities at interplanetary shocks driven by coronal mass ejections (CMEs) during solar cycles (SCs) 23 and 24.","We use observations from NASA's ACE and STEREO missions of helium (He), oxygen (O), and iron (Fe) intensities from ~0.13 to 3 MeV/nucleon.","We examine the longitudinal distribution of ESP intensities and the correlation of ESP intensities with the near-Sun CME speed and the average transit CME speed for eastern and western events.","We observed an East-West asymmetry reversal of ESP heavy ion intensities from SC 23 to 24.","We have determined that this change in asymmetry is caused by a shift in the heliolongitude distribution of the CME speed ratio (the ratio of CME near-Sun speed to CME average transit speed) from west to east."],"url":"http://arxiv.org/abs/2404.01993v1","category":"astro-ph.SR"}
{"created":"2024-04-02 14:24:51","title":"Duality transformations and the entanglement entropy of gauge theories","abstract":"The study of entanglement in gauge theories is expected to provide insights into many fundamental phenomena, including confinement. However, calculations of quantities related to entanglement in gauge theories are limited by ambiguities that stem from the non-factorizability of the Hilbert space. In this work we study lattice gauge theories that admit a dual description in terms of spin models, for which the replica trick and R\\'enyi entropies are well defined. In the first part of this work, we explicitly perform the duality transformation in a replica geometry, deriving the structure of a replica space for a gauge theory. Then, in the second part, we calculate, by means of Monte Carlo simulations, the entropic c-function of the $\\Z_2$ gauge theory in three spacetime dimension, exploiting its dual description in terms of the three-dimensional Ising model.","sentences":["The study of entanglement in gauge theories is expected to provide insights into many fundamental phenomena, including confinement.","However, calculations of quantities related to entanglement in gauge theories are limited by ambiguities that stem from the non-factorizability of the Hilbert space.","In this work we study lattice gauge theories that admit a dual description in terms of spin models, for which the replica trick and R\\'enyi entropies are well defined.","In the first part of this work, we explicitly perform the duality transformation in a replica geometry, deriving the structure of a replica space for a gauge theory.","Then, in the second part, we calculate, by means of Monte Carlo simulations, the entropic c-function of the $\\Z_2$ gauge theory in three spacetime dimension, exploiting its dual description in terms of the three-dimensional Ising model."],"url":"http://arxiv.org/abs/2404.01987v1","category":"quant-ph"}
{"created":"2024-04-02 14:22:07","title":"The open access coverage of OpenAlex, Scopus and Web of Science","abstract":"Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.","sentences":["Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals.","Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals.","Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language.","Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA.","The share of English-only journals is considerably higher among gold journals.","High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities.","Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models."],"url":"http://arxiv.org/abs/2404.01985v1","category":"cs.DL"}
{"created":"2024-04-02 14:09:35","title":"Hydrodynamics as sound-speed approaches light-speed","abstract":"I present the simplest 3+1 dimensional quantum field theory for which the speed of sound can be arbitrarily close to the speed of light. Examining the hydrodynamics, I find cases where the shear viscosity is finite, but the ``shear relaxation coefficient'' appears always to be divergently large.","sentences":["I present the simplest 3+1 dimensional quantum field theory for which the speed of sound can be arbitrarily close to the speed of light.","Examining the hydrodynamics, I find cases where the shear viscosity is finite, but the ``shear relaxation coefficient'' appears always to be divergently large."],"url":"http://arxiv.org/abs/2404.01968v1","category":"hep-ph"}
{"created":"2024-04-02 13:51:40","title":"Radiative $B$ decays at Belle and Belle II","abstract":"Rare decays of $B$ mesons to radiative final states serve as an ideal ground to search for New Physics effects from short range contributions. Observed by CLEO in the early 1990s as one of the first transitions of the electroweak penguin family, these decays continue to play a key role in testing predictions of the Standard Model. We present the latest results from Belle and Belle II for the radiative penguin transitions.","sentences":["Rare decays of $B$ mesons to radiative final states serve as an ideal ground to search for New Physics effects from short range contributions.","Observed by CLEO in the early 1990s as one of the first transitions of the electroweak penguin family, these decays continue to play a key role in testing predictions of the Standard Model.","We present the latest results from Belle and Belle II for the radiative penguin transitions."],"url":"http://arxiv.org/abs/2404.01957v1","category":"hep-ex"}
{"created":"2024-04-02 13:45:42","title":"An Exploratory Study of the Relationship between SATD and Other Software Development Activities","abstract":"Technical Debt is a common issue that arises when short-term gains are prioritized over long-term costs, leading to a degradation in the quality of the code. Self-Admitted Technical Debt (SATD) is a specific type of Technical Debt that involves documenting code to remind developers of its debt. Previous research has explored various aspects of SATD, including detection methods, distribution, and its impact on software quality. To better understand SATD, one comprehension technique is to examine its co-occurrence with other activities, such as refactoring and bug fixing. This study investigates the relationship between removing and adding SATD and activities such as refactoring, bug fixing, adding new features, and testing. To do so, we analyzed 77 open-source Java projects using TODO/FIXME/XXX removal or addition in inline comments as indicators of SATD. We examined the co-occurrence of SATD with each activity in each project through chi-square and odds ratio evaluations. Our results show that SATD removal occurs simultaneously with refactoring in 95% of projects, while its addition occurs in 89% of projects. Furthermore, we found that three types of refactoring - \"move class\", \"remove method\", and \"move attribute\" - occur more frequently in the presence of SATD. However, their distribution is similar in projects with and without SATD.","sentences":["Technical Debt is a common issue that arises when short-term gains are prioritized over long-term costs, leading to a degradation in the quality of the code.","Self-Admitted Technical Debt (SATD) is a specific type of Technical Debt that involves documenting code to remind developers of its debt.","Previous research has explored various aspects of SATD, including detection methods, distribution, and its impact on software quality.","To better understand SATD, one comprehension technique is to examine its co-occurrence with other activities, such as refactoring and bug fixing.","This study investigates the relationship between removing and adding SATD and activities such as refactoring, bug fixing, adding new features, and testing.","To do so, we analyzed 77 open-source Java projects using TODO/FIXME/XXX removal or addition in inline comments as indicators of SATD.","We examined the co-occurrence of SATD with each activity in each project through chi-square and odds ratio evaluations.","Our results show that SATD removal occurs simultaneously with refactoring in 95% of projects, while its addition occurs in 89% of projects.","Furthermore, we found that three types of refactoring - \"move class\", \"remove method\", and \"move attribute\" - occur more frequently in the presence of SATD.","However, their distribution is similar in projects with and without SATD."],"url":"http://arxiv.org/abs/2404.01950v1","category":"cs.SE"}
{"created":"2024-04-02 13:35:33","title":"Flow Of Information In a Mechanically Quenched Confined Flock","abstract":"Living entities in a group communicate and transfer information to one another for a variety of reasons. It might be for foraging food, migration, or escaping threats and obstacles, etc. They do so by interacting with each other and also with the environment. The tools from statistical mechanics and information theory can be useful to analyze the flow of information among the living entities modelled as active (i.e. self-propelling) particles. Here we consider the active particles confined in a circular trap. The self-organisation of the particles crucially depends on whether the trap boundary is soft or hard. We quench the trap boundary from soft to hard instantaneously. After the mechanical quench, the particles suddenly find themselves in a hard potential. The self-organised cluster of the active particles, which was stable when the boundary was soft, becomes unstable. The cluster undergoes extreme deformation after the quench to find another stable configuration suitable for the hard potential. Together with the structural relaxation, information regarding the quench also flows throughout the deforming cluster. Here, we quantify the flow of information by computing local transfer entropy. We find that the flow spans the whole cluster, propagating ballistically.","sentences":["Living entities in a group communicate and transfer information to one another for a variety of reasons.","It might be for foraging food, migration, or escaping threats and obstacles, etc.","They do so by interacting with each other and also with the environment.","The tools from statistical mechanics and information theory can be useful to analyze the flow of information among the living entities modelled as active (i.e. self-propelling) particles.","Here we consider the active particles confined in a circular trap.","The self-organisation of the particles crucially depends on whether the trap boundary is soft or hard.","We quench the trap boundary from soft to hard instantaneously.","After the mechanical quench, the particles suddenly find themselves in a hard potential.","The self-organised cluster of the active particles, which was stable when the boundary was soft, becomes unstable.","The cluster undergoes extreme deformation after the quench to find another stable configuration suitable for the hard potential.","Together with the structural relaxation, information regarding the quench also flows throughout the deforming cluster.","Here, we quantify the flow of information by computing local transfer entropy.","We find that the flow spans the whole cluster, propagating ballistically."],"url":"http://arxiv.org/abs/2404.01942v1","category":"physics.bio-ph"}
{"created":"2024-04-02 13:32:07","title":"Depolarization and distributive laws","abstract":"Given a vector space with two multiplications, one commutative the other anticommutative, possibly connected by a distributive law, the depolarization principle allows to look at this triplet through a single nonassociative multiplication. This is the case of Poisson algebras. We are interested here in the cases of transposed Poisson algebras and we show in this case that depolarization cannot be done with a single multiplication. We also examine the depolarization for Hom-Lie algebras.","sentences":["Given a vector space with two multiplications, one commutative the other anticommutative, possibly connected by a distributive law, the depolarization principle allows to look at this triplet through a single nonassociative multiplication.","This is the case of Poisson algebras.","We are interested here in the cases of transposed Poisson algebras and we show in this case that depolarization cannot be done with a single multiplication.","We also examine the depolarization for Hom-Lie algebras."],"url":"http://arxiv.org/abs/2404.01937v1","category":"math.RA"}
{"created":"2024-04-02 13:20:33","title":"Electric corrections to $\u03c0$-$\u03c0$ scattering lenghts in the linear sigma model","abstract":"In this article we analyze the role of an external electric field, in the weak field approximation, on $\\pi$-$\\pi$ scattering lengths. The discussion is presented in the frame of the linear sigma model. To achieve this, we take into account all one-loop corrections in the $s$, $t$, and $u$ channels associated with the insertion of a Schwinger propagator for charged pions, focusing on the region characterized by small values of the electric field. Furthermore, one of the novelties of our work is the explicit calculation of box diagrams, which were previously overlooked in discussions regarding magnetic corrections. It turns out that the electric field corrections have an opposite effect with respect to magnetic corrections calculated previously in the literature.","sentences":["In this article we analyze the role of an external electric field, in the weak field approximation, on $\\pi$-$\\pi$ scattering lengths.","The discussion is presented in the frame of the linear sigma model.","To achieve this, we take into account all one-loop corrections in the $s$, $t$, and $u$ channels associated with the insertion of a Schwinger propagator for charged pions, focusing on the region characterized by small values of the electric field.","Furthermore, one of the novelties of our work is the explicit calculation of box diagrams, which were previously overlooked in discussions regarding magnetic corrections.","It turns out that the electric field corrections have an opposite effect with respect to magnetic corrections calculated previously in the literature."],"url":"http://arxiv.org/abs/2404.01927v1","category":"hep-ph"}
{"created":"2024-04-02 13:14:48","title":"Massive celestial amplitudes and celestial amplitudes beyond four points","abstract":"We compute scalar three-point celestial amplitudes involving two and three massive scalars. The three-point coefficient of celestial amplitudes with two massive scalars contains a hypergeometric function, and the one with three massive scalars can be represented as a triple Mellin-Barnes integral. Using these three-point celestial amplitudes, we investigate the conformal block expansions of five- and six-point scalar celestial amplitudes in the comb channel. We observe the presence of two-particle operators in the conformal block expansion of five-point celestial amplitudes, which confirms the previous analysis by taking multi-collinear limit. Moreover, we find that there are new three-particle operators in the conformal block expansion of six-point celestial amplitudes. Based on these findings, we conjecture that exchanges of $n$-particle operators can be observed by considering the comb channel conformal block expansion of $(n+3)$-point massless celestial amplitudes. Finally, we show that a new series of operators appears when turning on the mass of the first incoming particle. The leading operator in this series can be interpreted as a two-particle exchange in the OPE of one massive and one massless scalars.","sentences":["We compute scalar three-point celestial amplitudes involving two and three massive scalars.","The three-point coefficient of celestial amplitudes with two massive scalars contains a hypergeometric function, and the one with three massive scalars can be represented as a triple Mellin-Barnes integral.","Using these three-point celestial amplitudes, we investigate the conformal block expansions of five- and six-point scalar celestial amplitudes in the comb channel.","We observe the presence of two-particle operators in the conformal block expansion of five-point celestial amplitudes, which confirms the previous analysis by taking multi-collinear limit.","Moreover, we find that there are new three-particle operators in the conformal block expansion of six-point celestial amplitudes.","Based on these findings, we conjecture that exchanges of $n$-particle operators can be observed by considering the comb channel conformal block expansion of $(n+3)$-point massless celestial amplitudes.","Finally, we show that a new series of operators appears when turning on the mass of the first incoming particle.","The leading operator in this series can be interpreted as a two-particle exchange in the OPE of one massive and one massless scalars."],"url":"http://arxiv.org/abs/2404.01920v1","category":"hep-th"}
{"created":"2024-04-02 13:14:22","title":"The onset of bar formation in a massive galaxy at $z \\sim 3.8$","abstract":"We examine the morphological and kinematical properties of SPT-2147, a strongly lensed, massive, dusty, star-forming galaxy at $z = 3.762$. Combining data from JWST, HST, and ALMA, we study the galaxy's stellar emission, dust continuum and gas properties. The imaging reveals a central bar structure in the stars and gas embedded within an extended disc with a spiral arm-like feature. The kinematics confirm the presence of the bar and of the regularly rotating disc. Dynamical modeling yields a dynamical mass, ${M}_{\\rm dyn} = (9.7 \\pm 2.0) \\times 10^{10}$ ${\\rm M}_{\\odot}$, and a maximum rotational velocity to velocity dispersion ratio, $V / \\sigma = 9.8 \\pm 1.2$. From multi-band imaging we infer, via SED fitting, a stellar mass, ${M}_{\\star} = (6.3 \\pm 0.9) \\times 10^{10}$ $\\rm{M}_{\\odot}$, and a star formation rate, ${\\rm SFR} = 781 \\pm 99$ ${\\rm M_{\\odot} yr^{-1}}$, after correcting for magnification. Combining these measurements with the molecular gas mass, we derive a baryonic-to-total mass ratio of ${M}_{\\rm bar} / {M}_{\\rm dyn} = 0.9 \\pm 0.2$ within 4.0 kpc. This finding suggests that the formation of bars in galaxies begins earlier in the history of the Universe than previously thought and can also occur in galaxies with elevated gas fractions.","sentences":["We examine the morphological and kinematical properties of SPT-2147, a strongly lensed, massive, dusty, star-forming galaxy at $z = 3.762$. Combining data from JWST, HST, and ALMA, we study the galaxy's stellar emission, dust continuum and gas properties.","The imaging reveals a central bar structure in the stars and gas embedded within an extended disc with a spiral arm-like feature.","The kinematics confirm the presence of the bar and of the regularly rotating disc.","Dynamical modeling yields a dynamical mass, ${M}_{\\rm dyn} = (9.7 \\pm 2.0)","\\times 10^{10}$ ${\\rm M}_{\\odot}$, and a maximum rotational velocity to velocity dispersion ratio, $V / \\sigma = 9.8 \\pm 1.2$. From multi-band imaging we infer, via SED fitting, a stellar mass, ${M}_{\\star} = (6.3 \\pm 0.9)","\\times 10^{10}$ $\\rm{M}_{\\odot}$, and a star formation rate, ${\\rm SFR} = 781 \\pm 99$ ${\\rm M_{\\odot} yr^{-1}}$, after correcting for magnification.","Combining these measurements with the molecular gas mass, we derive a baryonic-to-total mass ratio of ${M}_{\\rm bar} / {M}_{\\rm dyn} = 0.9 \\pm 0.2$ within 4.0 kpc.","This finding suggests that the formation of bars in galaxies begins earlier in the history of the Universe than previously thought and can also occur in galaxies with elevated gas fractions."],"url":"http://arxiv.org/abs/2404.01918v1","category":"astro-ph.GA"}
{"created":"2024-04-02 12:57:24","title":"Electrically tunable High-Chern-number quasiflat bands in twisted antiferromagnetic topological insulators","abstract":"Isolated flat bands with significantly quenched kinetic energy of electrons could give rise to exotic strongly correlated states from electron-electron interactions. More intriguingly, the interplay between topology and flat bands can further lead to richer physical phenomena, which have attracted much interest. Here, taking advantage of the recently proposed intertwined Dirac states induced from the anisotropic coupling between top and bottom surface states of an antiferromagnetic topological insulator thin film, we show the emergence of a high-Chern-number (quasi)flat-band state through Moir\\'e engineering of the surface states. Remarkably, the flat bands are isolated from other bands and located near the Fermi level. Furthermore, topological phase transitions between trivial and nontrivial flat-band states can be driven by tuning the out-of-plane electric field. Our work not only proposes a new scheme to realize high-Chern-number flat-band states, but also highlights the versatility of the intertwined Dirac-cone states.","sentences":["Isolated flat bands with significantly quenched kinetic energy of electrons could give rise to exotic strongly correlated states from electron-electron interactions.","More intriguingly, the interplay between topology and flat bands can further lead to richer physical phenomena, which have attracted much interest.","Here, taking advantage of the recently proposed intertwined Dirac states induced from the anisotropic coupling between top and bottom surface states of an antiferromagnetic topological insulator thin film, we show the emergence of a high-Chern-number (quasi)flat-band state through Moir\\'e engineering of the surface states.","Remarkably, the flat bands are isolated from other bands and located near the Fermi level.","Furthermore, topological phase transitions between trivial and nontrivial flat-band states can be driven by tuning the out-of-plane electric field.","Our work not only proposes a new scheme to realize high-Chern-number flat-band states, but also highlights the versatility of the intertwined Dirac-cone states."],"url":"http://arxiv.org/abs/2404.01912v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 12:35:37","title":"Multivariate post-processing of probabilistic sub-seasonal weather regime forecasts","abstract":"Reliable forecasts of quasi-stationary, recurrent, and persistent large-scale atmospheric circulation patterns (weather regimes) are crucial for various socio-economic sectors. Despite steady progress, probabilistic weather regime predictions still exhibit biases in the exact timing and amplitude of weather regimes. This study thus aims at advancing probabilistic weather regime predictions in the North Atlantic-European region through ensemble post-processing. Here, we focus on the representation of seven year-round weather regimes in the sub-seasonal to seasonal reforecasts of the European Centre for Medium-Range Weather Forecasts. The manifestation of each of the seven regimes can be expressed by a continuous weather regime index, representing the projection of the instantaneous 500-hPa geopotential height anomalies (Z500A) onto the respective mean regime pattern. We apply a two-step ensemble post-processing involving first univariate ensemble model output statistics and second ensemble copula coupling, which restores the multivariate dependency structure. Compared to current forecast calibration practices, which rely on correcting the Z500 field by the lead time dependent mean bias, our approach extends the forecast skill horizon for daily/instantaneous regime forecasts moderately by 1.2 days to 14.5 days. Additionally, to our knowledge our study is the first to systematically evaluate the multivariate aspects of forecast quality for weather regime forecasts. Our method outperforms current practices in the multivariate aspect, as measured by the energy and variogram score. Still our study shows, that even with advanced post-processing weather regime prediction becomes difficult beyond 14 days, which likely points towards intrinsic limits of predictability for daily/instantaneous regime forecasts. The proposed method can easily be applied to operational weather regime forecasts.","sentences":["Reliable forecasts of quasi-stationary, recurrent, and persistent large-scale atmospheric circulation patterns (weather regimes) are crucial for various socio-economic sectors.","Despite steady progress, probabilistic weather regime predictions still exhibit biases in the exact timing and amplitude of weather regimes.","This study thus aims at advancing probabilistic weather regime predictions in the North Atlantic-European region through ensemble post-processing.","Here, we focus on the representation of seven year-round weather regimes in the sub-seasonal to seasonal reforecasts of the European Centre for Medium-Range Weather Forecasts.","The manifestation of each of the seven regimes can be expressed by a continuous weather regime index, representing the projection of the instantaneous 500-hPa geopotential height anomalies (Z500A) onto the respective mean regime pattern.","We apply a two-step ensemble post-processing involving first univariate ensemble model output statistics and second ensemble copula coupling, which restores the multivariate dependency structure.","Compared to current forecast calibration practices, which rely on correcting the Z500 field by the lead time dependent mean bias, our approach extends the forecast skill horizon for daily/instantaneous regime forecasts moderately by 1.2 days to 14.5 days.","Additionally, to our knowledge our study is the first to systematically evaluate the multivariate aspects of forecast quality for weather regime forecasts.","Our method outperforms current practices in the multivariate aspect, as measured by the energy and variogram score.","Still our study shows, that even with advanced post-processing weather regime prediction becomes difficult beyond 14 days, which likely points towards intrinsic limits of predictability for daily/instantaneous regime forecasts.","The proposed method can easily be applied to operational weather regime forecasts."],"url":"http://arxiv.org/abs/2404.01895v1","category":"physics.ao-ph"}
{"created":"2024-04-02 12:35:29","title":"BOSS Constraints on Massive Particles during Inflation: The Cosmological Collider in Action","abstract":"Massive particles leave imprints on primordial non-Gaussianity via couplings to the inflaton, even despite their exponential dilution during inflation: practically, the Universe acts as a Cosmological Collider. We present the first dedicated search for spin-zero particles using BOSS redshift-space galaxy power spectrum and bispectrum multipoles, as well as Planck CMB non-Gaussianity data. We demonstrate that some Cosmological Collider models are well approximated by the standard equilateral and orthogonal parametrization; assuming negligible inflaton self-interactions, this facilitates us translating Planck non-Gaussianity constraints into bounds on Collider models. Many models have signatures that are not degenerate with equilateral and orthogonal non-Gaussianity and thus require dedicated searches. Here, we constrain such models using BOSS three-dimensional redshift-space galaxy clustering data, focusing on spin-zero particles in the principal series and constraining their couplings to the inflaton at varying speed and mass, marginalizing over the unknown inflaton self-interactions. This is made possible through an improvement in Cosmological Bootstrap techniques and the combination of perturbation theory and halo occupation distribution models for galaxy clustering. Our work sets the standard for inflationary spectroscopy with cosmological observations, providing the ultimate link between physics on the largest and smallest scales.","sentences":["Massive particles leave imprints on primordial non-Gaussianity via couplings to the inflaton, even despite their exponential dilution during inflation: practically, the Universe acts as a Cosmological Collider.","We present the first dedicated search for spin-zero particles using BOSS redshift-space galaxy power spectrum and bispectrum multipoles, as well as Planck CMB non-Gaussianity data.","We demonstrate that some Cosmological Collider models are well approximated by the standard equilateral and orthogonal parametrization; assuming negligible inflaton self-interactions, this facilitates us translating Planck non-Gaussianity constraints into bounds on Collider models.","Many models have signatures that are not degenerate with equilateral and orthogonal non-Gaussianity and thus require dedicated searches.","Here, we constrain such models using BOSS three-dimensional redshift-space galaxy clustering data, focusing on spin-zero particles in the principal series and constraining their couplings to the inflaton at varying speed and mass, marginalizing over the unknown inflaton self-interactions.","This is made possible through an improvement in Cosmological Bootstrap techniques and the combination of perturbation theory and halo occupation distribution models for galaxy clustering.","Our work sets the standard for inflationary spectroscopy with cosmological observations, providing the ultimate link between physics on the largest and smallest scales."],"url":"http://arxiv.org/abs/2404.01894v1","category":"astro-ph.CO"}
{"created":"2024-04-02 12:28:57","title":"A variational approach to the hot spots conjecture","abstract":"We review a recent new approach to the study of critical points of Laplacian eigenfunctions. Its core novelty is a non-standard variational principle for the eigenvalues of the Laplacians with Neumann and Dirichlet boundary conditions on bounded, simply connected planar domains. This principle can be used to provide simple proofs of some previously known results on the hot spots conjecture.","sentences":["We review a recent new approach to the study of critical points of Laplacian eigenfunctions.","Its core novelty is a non-standard variational principle for the eigenvalues of the Laplacians with Neumann and Dirichlet boundary conditions on bounded, simply connected planar domains.","This principle can be used to provide simple proofs of some previously known results on the hot spots conjecture."],"url":"http://arxiv.org/abs/2404.01890v1","category":"math.SP"}
{"created":"2024-04-02 11:58:20","title":"Laser mode-hopping assisted all-optical single beam pulsed atomic magnetometer","abstract":"We demonstrate an all-optical single beam pulsed atomic magnetometer assisted by laser mode-hopping in a distributed Bragg reflector (DBR) laser. We implement a temporal sequence of the laser current; sinusoidal current modulation including the laser mode-hop current for synchronous optical pumping and the following constant current for paramagnetic Faraday rotation measurements to probe the free induction decay (FID) of transverse $^{87}$Rb spin polarization. Repetitive sudden frequency shifts of 20 GHz around the pressure-broadened $^{87}$Rb spectra originating from laser mode-hopping enables discontinuous optical pumping modulation with a large depth which enhances transverse spin polarization. We achieve a sensitivity of 3.77 pT/Hz$^{1/2}$ in a magnetic field of 14 $\\mu$T, limited by the performance of the frequency counter. The Cramer-Rao lower bound (CRLB) of the sensitivity due to the non-magnetic noise such as photon shot-noise is 191 fT/Hz$^{1/2}$. Our approach based on laser mode-hopping can be applied to miniaturization of all-optical atomic magnetometers with sub-pT/Hz$^{1/2}$ sensitivities.","sentences":["We demonstrate an all-optical single beam pulsed atomic magnetometer assisted by laser mode-hopping in a distributed Bragg reflector (DBR) laser.","We implement a temporal sequence of the laser current; sinusoidal current modulation including the laser mode-hop current for synchronous optical pumping and the following constant current for paramagnetic Faraday rotation measurements to probe the free induction decay (FID) of transverse $^{87}$Rb spin polarization.","Repetitive sudden frequency shifts of 20 GHz around the pressure-broadened $^{87}$Rb spectra originating from laser mode-hopping enables discontinuous optical pumping modulation with a large depth which enhances transverse spin polarization.","We achieve a sensitivity of 3.77 pT/Hz$^{1/2}$ in a magnetic field of 14 $\\mu$T, limited by the performance of the frequency counter.","The Cramer-Rao lower bound (CRLB) of the sensitivity due to the non-magnetic noise such as photon shot-noise is 191 fT/Hz$^{1/2}$. Our approach based on laser mode-hopping can be applied to miniaturization of all-optical atomic magnetometers with sub-pT/Hz$^{1/2}$ sensitivities."],"url":"http://arxiv.org/abs/2404.01874v1","category":"physics.atom-ph"}
{"created":"2024-04-02 11:57:40","title":"Bjorken sum rule with analytic coupling","abstract":"We found good agreement between the experimental data obtained for the polarized Bjorken sum rule and the predictions of analytic QCD, as well as a strong difference between these data and the results obtained in the framework of perturbative QCD. To satisfy the limit of photoproduction and take into account Gerasimov-Drell-Hearn and Burkhardt-Cottingham sum rules, we develope new representation of the perturbative part of the polarized Bjorken sum rule.","sentences":["We found good agreement between the experimental data obtained for the polarized Bjorken sum rule and the predictions of analytic QCD, as well as a strong difference between these data and the results obtained in the framework of perturbative QCD.","To satisfy the limit of photoproduction and take into account Gerasimov-Drell-Hearn and Burkhardt-Cottingham sum rules, we develope new representation of the perturbative part of the polarized Bjorken sum rule."],"url":"http://arxiv.org/abs/2404.01873v1","category":"hep-ph"}
{"created":"2024-04-02 10:38:02","title":"The new $K_S \\rightarrow \u03c0e \u03bd$ branching fraction measurement at KLOE","abstract":"A new measurement of the branching fraction for the decay $K_S \\rightarrow \\pi e \\nu$ is presented, based on a sample of 300 million $K_S$ mesons recorded by the KLOE experiment at the DA$\\Phi$NE $e^+e^-$ collider. A two-step signal selection strategy is used, exploiting first kinematic variables and then time-of-flight measurements. Data control samples of $K_S \\rightarrow \\pi e \\nu$ decays are used to evaluate signal selection efficiencies. Normalizing the selected sample to the number of $K_S \\rightarrow \\pi^+ \\pi^-$ decay events the result for the branching fraction is $B(K_S \\rightarrow \\pi e \\nu)=(7.211 \\pm 0.046 \\pm 0.052)\\times 10^-4$. The combination with our previous measurement gives $B(K_S \\rightarrow \\pi e \\nu)=(7.153 \\pm 0.037 \\pm 0.043)\\times 10^-4 =(7.153 \\pm 0.057)\\times 10^-4$. From this value we derive $f^+(0)|Vus|=0.2170 \\pm 0.0009$.","sentences":["A new measurement of the branching fraction for the decay $K_S \\rightarrow \\pi e \\nu$ is presented, based on a sample of 300 million $K_S$ mesons recorded by the KLOE experiment at the DA$\\Phi$NE $e^+e^-$ collider.","A two-step signal selection strategy is used, exploiting first kinematic variables and then time-of-flight measurements.","Data control samples of $K_S \\rightarrow \\pi e \\nu$ decays are used to evaluate signal selection efficiencies.","Normalizing the selected sample to the number of $K_S \\rightarrow \\pi^+ \\pi^-$ decay events the result for the branching fraction is $B(K_S","\\rightarrow \\pi e \\nu)=(7.211 \\pm 0.046 \\pm 0.052)\\times 10^-4$.","The combination with our previous measurement gives $B(K_S \\rightarrow \\pi e \\nu)=(7.153 \\pm 0.037 \\pm 0.043)\\times 10^-4 =(7.153 \\pm 0.057)\\times 10^-4$.","From this value we derive $f^+(0)|Vus|=0.2170 \\pm 0.0009$."],"url":"http://arxiv.org/abs/2404.01826v1","category":"hep-ex"}
{"created":"2024-04-02 09:54:24","title":"Search of CP violation in baryons with the LHCb detector","abstract":"The latest results from the LHCb collaboration for the search for $CP$ violation in $b$-baryon decays are reported here. The first article presented is the search conducted in the $\\Lambda_b^0 \\rightarrow p \\pi^- \\pi^+ \\pi^- $ decay, and the subsequent observation of $P$ violation. The following paper describes the search and the first observation of the $\\Lambda_b^0 \\rightarrow D^0 p K^-$ decay, with $D^0 \\rightarrow K \\pi$, Cabibbo-favoured and doubly suppressed mode. This proceeding concludes with the first amplitude analysis performed in $\\Xi_b^- \\rightarrow p K^- K^-$, whose model allows for \\textit{CP} violation.","sentences":["The latest results from the LHCb collaboration for the search for $CP$ violation in $b$-baryon decays are reported here.","The first article presented is the search conducted in the $\\Lambda_b^0 \\rightarrow p \\pi^- \\pi^+ \\pi^- $ decay, and the subsequent observation of $P$ violation.","The following paper describes the search and the first observation of the $\\Lambda_b^0 \\rightarrow D^0 p K^-$ decay, with $D^0 \\rightarrow K \\pi$, Cabibbo-favoured and doubly suppressed mode.","This proceeding concludes with the first amplitude analysis performed in $\\Xi_b^- \\rightarrow p K^- K^-$, whose model allows for \\textit{CP} violation."],"url":"http://arxiv.org/abs/2404.01793v1","category":"hep-ex"}
{"created":"2024-04-02 09:41:18","title":"A thermionic electron gun to characterize silicon drift detectors with electrons","abstract":"The TRISTAN detector is a new detector for electron spectroscopy at the Karlsruhe Tritium Neutrino (KATRIN) experiment. The semiconductor detector utilizes the silicon drift detector technology and will enable the precise measurement of the entire tritium beta decay electron spectrum. Thus, a significant fraction of the parameter space of potential neutrino mass eigenstates in the keV-mass regime can be probed. We developed a custom electron gun based on the effect of thermionic emission to characterize the TRISTAN detector modules with mono-energetic electrons before installation into the KATRIN beamline. The electron gun provides an electron beam with up to 25 keV kinetic energy and an electron rate in the order of 1E5 electrons per second. This manuscript gives an overview of the design and commissioning of the electron gun. In addition, we will shortly discuss a first measurement with the electron gun to characterize the electron response of the TRISTAN detector.","sentences":["The TRISTAN detector is a new detector for electron spectroscopy at the Karlsruhe Tritium Neutrino (KATRIN) experiment.","The semiconductor detector utilizes the silicon drift detector technology and will enable the precise measurement of the entire tritium beta decay electron spectrum.","Thus, a significant fraction of the parameter space of potential neutrino mass eigenstates in the keV-mass regime can be probed.","We developed a custom electron gun based on the effect of thermionic emission to characterize the TRISTAN detector modules with mono-energetic electrons before installation into the KATRIN beamline.","The electron gun provides an electron beam with up to 25 keV kinetic energy and an electron rate in the order of 1E5 electrons per second.","This manuscript gives an overview of the design and commissioning of the electron gun.","In addition, we will shortly discuss a first measurement with the electron gun to characterize the electron response of the TRISTAN detector."],"url":"http://arxiv.org/abs/2404.01777v1","category":"physics.ins-det"}
{"created":"2024-04-02 09:27:30","title":"Diagnostics of 3D explosion asymmetries of stripped-envelope supernovae by nebular line profiles","abstract":"Understanding the explosion mechanism and hydrodynamic evolution of core-collapse supernovae is a long-standing quest in astronomy. The asymmetries caused by the explosion are encoded into the line profiles which appear in the nebular phase of the SN evolution -- with particularly clean imprints in He star explosions. Here, we carry out nine different supernova simulations of He-core progenitors, exploding them in 3D with parametrically varied neutrino luminosities using the $\\texttt{Prometheus-HotB}$ code, hydrodynamically evolving the models to the homologeous phase. We then compute nebular phase spectra with the 3D NLTE spectral synthesis code $\\texttt{ExTraSS}$ (EXplosive TRAnsient Spectral Simulator). We study how line widths and shifts depend on progenitor mass, explosion energy, and viewing angle. We compare the predicted line profile properties against a large set of Type Ib observations, and discuss the degree to which current neutrino-driven explosions can match observationally inferred asymmetries. With self-consistent 3D modelling -- circumventing the difficulties of representing $^{56}$Ni mixing and clumping accurately in 1D models -- we find that neither low-mass He cores exploding with high energies nor high-mass cores exploding with low energies contribute to the Type Ib SN population. Models which have line profile widths in agreement with this population give sufficiently large centroid shifts for calcium emission lines. Calcium is more strongly affected by explosion asymmetries connected to the neutron star kicks than oxygen and magnesium. Lastly, we turn to the NIR spectra from our models to investigate the potential of using this regime to look for the presence of He in the nebular phase.","sentences":["Understanding the explosion mechanism and hydrodynamic evolution of core-collapse supernovae is a long-standing quest in astronomy.","The asymmetries caused by the explosion are encoded into the line profiles which appear in the nebular phase of the SN evolution -- with particularly clean imprints in He star explosions.","Here, we carry out nine different supernova simulations of He-core progenitors, exploding them in 3D with parametrically varied neutrino luminosities using the $\\texttt{Prometheus-HotB}$ code, hydrodynamically evolving the models to the homologeous phase.","We then compute nebular phase spectra with the 3D NLTE spectral synthesis code $\\texttt{ExTraSS}$ (EXplosive TRAnsient Spectral Simulator).","We study how line widths and shifts depend on progenitor mass, explosion energy, and viewing angle.","We compare the predicted line profile properties against a large set of Type Ib observations, and discuss the degree to which current neutrino-driven explosions can match observationally inferred asymmetries.","With self-consistent 3D modelling -- circumventing the difficulties of representing $^{56}$Ni mixing and clumping accurately in 1D models -- we find that neither low-mass He cores exploding with high energies nor high-mass cores exploding with low energies contribute to the Type Ib SN population.","Models which have line profile widths in agreement with this population give sufficiently large centroid shifts for calcium emission lines.","Calcium is more strongly affected by explosion asymmetries connected to the neutron star kicks than oxygen and magnesium.","Lastly, we turn to the NIR spectra from our models to investigate the potential of using this regime to look for the presence of He in the nebular phase."],"url":"http://arxiv.org/abs/2404.01763v1","category":"astro-ph.HE"}
{"created":"2024-04-02 08:41:25","title":"Compact Binary Formation in Open Star Clusters II: Difficulty of Gaia NS formation in low-mass star clusters","abstract":"Gaia mission offers opportunities to search for compact binaries not involved in binary interactions (hereafter inert compact binaries), and results in the discoveries of binaries containing one black hole (BH) or one neutron star (NS), called ``Gaia BHs'' and ``Gaia NSs'', respectively. We have assessed if Gaia BHs and NSs can be formed in open clusters through dynamical interactions. In order to obtain a large number of inert compact binaries similar to Gaia BHs and NSs, we have performed gravitational $N$-body simulations for a large number of open clusters whose total mass is $1.2 \\times 10^8 M_\\odot$. These clusters have various masses, metallicities, densities, and binary fractions. We have found that open clusters form Gaia BHs ($10^{-6}$-$10^{-5} M_\\odot^{-1}$) much more efficiently than Gaia NSs ($\\lesssim 10^{-7} M_\\odot^{-1}$) for any cluster parameters. This is quite inconsistent with observational results, because the reported numbers of Gaia BHs and NSs are $2$ and $\\sim 20$, respectively. Additionally, we have switched off NS natal kicks for $10^4$ open clusters each weighing $10^3 M_\\odot$ in order to retain a large number of NSs in open clusters. Then, open clusters form inert NS binaries originating from primordial binaries rather than formed through dynamical interactions. This means that Gaia NSs are formed dominantly on isolated fields, not in open clusters, if there is no NS natal kick. We have concluded that Gaia BHs can be dominantly formed in open clusters, however Gaia NSs cannot.","sentences":["Gaia mission offers opportunities to search for compact binaries not involved in binary interactions (hereafter inert compact binaries), and results in the discoveries of binaries containing one black hole (BH) or one neutron star (NS), called ``Gaia BHs'' and ``Gaia NSs'', respectively.","We have assessed if Gaia BHs and NSs can be formed in open clusters through dynamical interactions.","In order to obtain a large number of inert compact binaries similar to Gaia BHs and NSs, we have performed gravitational $N$-body simulations for a large number of open clusters whose total mass is $1.2 \\times 10^8 M_\\odot$. These clusters have various masses, metallicities, densities, and binary fractions.","We have found that open clusters form Gaia BHs ($10^{-6}$-$10^{-5} M_\\odot^{-1}$) much more efficiently than Gaia NSs ($\\lesssim 10^{-7} M_\\odot^{-1}$) for any cluster parameters.","This is quite inconsistent with observational results, because the reported numbers of Gaia BHs and NSs are $2$ and $\\sim 20$, respectively.","Additionally, we have switched off NS natal kicks for $10^4$ open clusters each weighing $10^3 M_\\odot$ in order to retain a large number of NSs in open clusters.","Then, open clusters form inert NS binaries originating from primordial binaries rather than formed through dynamical interactions.","This means that Gaia NSs are formed dominantly on isolated fields, not in open clusters, if there is no NS natal kick.","We have concluded that Gaia BHs can be dominantly formed in open clusters, however Gaia NSs cannot."],"url":"http://arxiv.org/abs/2404.01731v1","category":"astro-ph.SR"}
{"created":"2024-04-02 06:57:18","title":"Semi-leptonic $W$ decay to $B$ meson with lepton pairs in HQET factorization upto $\\mathcal{O}(\u03b1_s)$","abstract":"Motivated by the study of heavy-light meson production within the framework of heavy quark effective theory (HQET) factorization, we extend the factorization formalism for rather a complicated process $W^+\\to B^+\\ell^+\\ell^-$ in the limit of non-zero invariant squared-mass of dilepton, $q^2$, at the lowest order in $1/m_b$ up to $\\mathcal{O}(\\alpha_s)$. The purpose of the current study is to extend the HQET factorization formula for the $W^+\\to B^+\\ell^+\\ell^-$ process and subsequently compute the form factors for this channel up to next-to-leading order (NLO) corrections in $\\alpha_s$. We explicitly show the amplitude of the $W^+\\to B^+\\ell^+\\ell^-$ process can also be factorized into a convolution between the perturbatively calculable hard-scattering kernel and the non-perturbative yet universal light-cone distribution amplitude (LCDA) defined in HQET. The validity of HQET factorization depends on the assumed scale hierarchy $m_W \\sim m_b \\gg \\Lambda_{\\mathrm{QCD}}$. Within the HQET framework, we evaluate the form factors associated with the $W^+ \\rightarrow B^+\\ell^+\\ell^-$ process, providing insights into its phenomenology. In addition, we also perform an exploratory phenomenological study on $W^+ \\rightarrow B^+\\ell^+\\ell^-$ by employing an exponential model for the LCDAs for $B^+$ meson. Our findings reveal that the branching ratio for $W^+ \\rightarrow B^+\\ell^+\\ell^-$ is below $10^{-10}$. Although the branching ratios are small, this channel in high luminosity LHC experiments may serve to further constraints the value of $\\lambda_B$.","sentences":["Motivated by the study of heavy-light meson production within the framework of heavy quark effective theory (HQET) factorization, we extend the factorization formalism for rather a complicated process $W^+\\to B^+\\ell^+\\ell^-$ in the limit of non-zero invariant squared-mass of dilepton, $q^2$, at the lowest order in $1/m_b$ up to $\\mathcal{O}(\\alpha_s)$. The purpose of the current study is to extend the HQET factorization formula for the $W^+\\to B^+\\ell^+\\ell^-$ process and subsequently compute the form factors for this channel up to next-to-leading order (NLO) corrections in $\\alpha_s$. We explicitly show the amplitude of the $W^+\\to B^+\\ell^+\\ell^-$ process can also be factorized into a convolution between the perturbatively calculable hard-scattering kernel and the non-perturbative yet universal light-cone distribution amplitude (LCDA) defined in HQET.","The validity of HQET factorization depends on the assumed scale hierarchy $m_W \\sim m_b","\\gg \\Lambda_{\\mathrm{QCD}}$. Within the HQET framework, we evaluate the form factors associated with the $W^+ \\rightarrow B^+\\ell^+\\ell^-$ process, providing insights into its phenomenology.","In addition, we also perform an exploratory phenomenological study on $W^+ \\rightarrow B^+\\ell^+\\ell^-$ by employing an exponential model for the LCDAs for $B^+$ meson.","Our findings reveal that the branching ratio for $W^+ \\rightarrow B^+\\ell^+\\ell^-$ is below $10^{-10}$. Although the branching ratios are small, this channel in high luminosity LHC experiments may serve to further constraints the value of $\\lambda_B$."],"url":"http://arxiv.org/abs/2404.01696v1","category":"hep-ph"}
{"created":"2024-04-02 06:10:34","title":"The resonance effect for the CP asymmetry associated with the process $\u03c9\\rightarrow \u03c0^{+}\u03c0^{-}\u03c0^{0}$","abstract":"The direct CP asymmetry in the weak decay process of hadrons is commonly attributed to the weak phase of the CKM matrix and the indeterminate strong phase. We propose a way of creating a strong phase difference between two decay paths involving vector mesons $V= \\omega,\\rho$ decaying to $\\pi^{+}\\pi^{-}\\pi^{0}$ considering the G-parity suppressed decay process $\\rho^{0} \\rightarrow \\pi^{+}\\pi^{-}\\pi^{0} $. It can lead to a significant CP asymmetry in the interference region. We consider the effects of $\\omega$ and $\\rho$ mixing on their decays to $\\pi^{+}\\pi^{-}\\pi^{0}$. Meanwhile, we calculate the CP asymmetry for the process $B\\rightarrow \\pi^{+}\\pi^{-}\\pi^{0}P$ (P refers to pseudoscalar meson) through $\\omega(\\rho) \\rightarrow \\pi^{+}\\pi^{-}\\pi^{0}$. Besides, we also calculate the integral results of different phase space regions. We hope that our work can help provide some theoretical guidance for future experimental studies of CP asymmetry in these decays.","sentences":["The direct CP asymmetry in the weak decay process of hadrons is commonly attributed to the weak phase of the CKM matrix and the indeterminate strong phase.","We propose a way of creating a strong phase difference between two decay paths involving vector mesons $V= \\omega,\\rho$ decaying to $\\pi^{+}\\pi^{-}\\pi^{0}$ considering the G-parity suppressed decay process $\\rho^{0} \\rightarrow \\pi^{+}\\pi^{-}\\pi^{0} $.","It can lead to a significant CP asymmetry in the interference region.","We consider the effects of $\\omega$ and $\\rho$ mixing on their decays to $\\pi^{+}\\pi^{-}\\pi^{0}$. Meanwhile, we calculate the CP asymmetry for the process $B\\rightarrow \\pi^{+}\\pi^{-}\\pi^{0}P$ (P refers to pseudoscalar meson) through $\\omega(\\rho) \\rightarrow \\pi^{+}\\pi^{-}\\pi^{0}$.","Besides, we also calculate the integral results of different phase space regions.","We hope that our work can help provide some theoretical guidance for future experimental studies of CP asymmetry in these decays."],"url":"http://arxiv.org/abs/2404.01665v1","category":"hep-ph"}
{"created":"2024-04-02 04:36:04","title":"Doubly heavy hadrons production in ultraperipheral collision","abstract":"We study the double heavy baryon $\\Xi_{QQ'}$ and tetraquark $T_{QQ}$ production through photon-photon and photon-gluon fusion via ultraperipheral collisions at the LHC and FCC within the framework of nonrelativistic QCD factorization formalism. Various ion-ion collisions are taken into account, two cc(bb)-diquark configurations ($[cc(bb),{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$ and $[cc(bb),{^1S_0}\\mbox{-}\\bm{6}]$) and four bc-diquark configurations ($[bc,{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$, $[bc,{^3S_1}\\mbox{-}\\bm{6}]$, $[bc,{^1S_0}\\mbox{-}\\bar{\\bm{3}}]$ and $[bc,{^1S_0}\\mbox{-}\\bm{6}]$) are considered in the calculation. Numerical results indicate that the $[cc,{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$ diquark provides dominant contribution for $\\Xi_{cc}$ ($T_{cc}$) production, and a considerable number of $\\Xi_{cc}$ ($T_{cc}$) can be produced. Due to the event topologies for ultraperipheral collision are very clear, the background from various QCD interactions can be suppressed, hence the experimental investigation for $\\Xi_{cc}$ and $T_{cc}$ are feasible. The productions for $\\Xi_{bc/bb}$ are also discussed, leaving only slightly possibility for $\\Xi_{bc}$ through photon-gluon fusion with ultraperipheral collisions at the FCC.","sentences":["We study the double heavy baryon $\\Xi_{QQ'}$ and tetraquark $T_{QQ}$ production through photon-photon and photon-gluon fusion via ultraperipheral collisions at the LHC and FCC within the framework of nonrelativistic QCD factorization formalism.","Various ion-ion collisions are taken into account, two cc(bb)-diquark configurations ($[cc(bb),{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$ and $[cc(bb),{^1S_0}\\mbox{-}\\bm{6}]$) and four bc-diquark configurations ($[bc,{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$, $[bc,{^3S_1}\\mbox{-}\\bm{6}]$, $[bc,{^1S_0}\\mbox{-}\\bar{\\bm{3}}]$ and $[bc,{^1S_0}\\mbox{-}\\bm{6}]$) are considered in the calculation.","Numerical results indicate that the $[cc,{^3S_1}\\mbox{-}\\bar{\\bm{3}}]$ diquark provides dominant contribution for $\\Xi_{cc}$ ($T_{cc}$) production, and a considerable number of $\\Xi_{cc}$ ($T_{cc}$) can be produced.","Due to the event topologies for ultraperipheral collision are very clear, the background from various QCD interactions can be suppressed, hence the experimental investigation for $\\Xi_{cc}$ and $T_{cc}$ are feasible.","The productions for $\\Xi_{bc/bb}$ are also discussed, leaving only slightly possibility for $\\Xi_{bc}$ through photon-gluon fusion with ultraperipheral collisions at the FCC."],"url":"http://arxiv.org/abs/2404.01633v1","category":"hep-ph"}
{"created":"2024-04-02 03:02:12","title":"Migration barriers for diffusion of As and P atoms in InP and InAs via vacancies and interstitial atoms","abstract":"Processes of diffusion of As and P atoms in InP and InAs, and atomic and energy structure of group-V vacancies and interstitial P and As atoms in InP and InAs have been investigated using density functional theory. Formation energies of group-V vacancies in InP and InAs and P and As interstitial atoms in InP and InAs have been calculated with hybrid functional. The main types of migration jumps have been determined, and the energy favorable migration paths and migration barriers of As and P atoms diffusion in InP and InAs via vacancies and interstitial atoms have been calculated using climbing image nudged elastic band method. In the case of diffusion of As and P atoms in InP and InAs via interstitial atoms the diffusion process occurs via indirect interstitial mechanism. The migration energy barriers for the vacancy diffusion mechanism are 1.5-2.0 eV, the migration energy barriers for the interstitialcy mechanism are 0.3-0.6 eV. The interstitial atoms have higher formation energies compared to the formation energies of the vacancies, and total activation energies of the diffusion are comparable for the vacancy and interstitialcy mechanisms. The obtained results will be useful for modeling of the diffusion processes in semiconductor structures based on InP and InAs.","sentences":["Processes of diffusion of As and P atoms in InP and InAs, and atomic and energy structure of group-V vacancies and interstitial P and As atoms in InP and InAs have been investigated using density functional theory.","Formation energies of group-V vacancies in InP and InAs and P and As interstitial atoms in InP and InAs have been calculated with hybrid functional.","The main types of migration jumps have been determined, and the energy favorable migration paths and migration barriers of As and P atoms diffusion in InP and InAs via vacancies and interstitial atoms have been calculated using climbing image nudged elastic band method.","In the case of diffusion of As and P atoms in InP and InAs via interstitial atoms the diffusion process occurs via indirect interstitial mechanism.","The migration energy barriers for the vacancy diffusion mechanism are 1.5-2.0 eV, the migration energy barriers for the interstitialcy mechanism are 0.3-0.6 eV. The interstitial atoms have higher formation energies compared to the formation energies of the vacancies, and total activation energies of the diffusion are comparable for the vacancy and interstitialcy mechanisms.","The obtained results will be useful for modeling of the diffusion processes in semiconductor structures based on InP and InAs."],"url":"http://arxiv.org/abs/2404.01607v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 02:46:21","title":"Understanding spin currents from magnon dispersion and polarization: Spin-Seebeck effect and neutron scattering study on Tb3Fe5O12","abstract":"Magnon spin currents in the ferrimagnetic garnet Tb3Fe5O12 with 4f electrons were examined through the spin-Seebeck effect and neutron scattering measurements. The compound shows a magnetic compensation, where the spin-Seebeck signal reverses above and below Tcomp = 249.5(4) K. Unpolarized neutron scattering unveils two major magnon branches with finite energy gaps, which are well-explained in the framework of spin-wave theory. Their temperature dependencies and the direction of the precession motion of magnetic moments, i.e. magnon polarization, defined using polarized neutrons, explain the reversal at Tcomp and decay of the spin-Seebeck signals at low temperatures. We illustrate an example that momentum- and energy-resolved microscopic information is a prerequisite to understanding the magnon spin current.","sentences":["Magnon spin currents in the ferrimagnetic garnet Tb3Fe5O12 with 4f electrons were examined through the spin-Seebeck effect and neutron scattering measurements.","The compound shows a magnetic compensation, where the spin-Seebeck signal reverses above and below Tcomp = 249.5(4) K. Unpolarized neutron scattering unveils two major magnon branches with finite energy gaps, which are well-explained in the framework of spin-wave theory.","Their temperature dependencies and the direction of the precession motion of magnetic moments, i.e. magnon polarization, defined using polarized neutrons, explain the reversal at Tcomp and decay of the spin-Seebeck signals at low temperatures.","We illustrate an example that momentum- and energy-resolved microscopic information is a prerequisite to understanding the magnon spin current."],"url":"http://arxiv.org/abs/2404.01603v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 02:01:21","title":"A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)","abstract":"We propose VecKM, a novel local point cloud geometry encoder that is descriptive, efficient and robust to noise. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point clouds. Such representation is descriptive and robust to noise, which is supported by two theorems that confirm its ability to reconstruct and preserve the similarity of the local shape. Moreover, VecKM is the first successful attempt to reduce the computation and memory costs from $O(n^2+nKd)$ to $O(nd)$ by sacrificing a marginal constant factor, where $n$ is the size of the point cloud and $K$ is neighborhood size. The efficiency is primarily due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighborhoods. In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also strongest descriptiveness and robustness compared with existing popular encoders. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10x.","sentences":["We propose VecKM, a novel local point cloud geometry encoder that is descriptive, efficient and robust to noise.","VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point clouds.","Such representation is descriptive and robust to noise, which is supported by two theorems that confirm its ability to reconstruct and preserve the similarity of the local shape.","Moreover, VecKM is the first successful attempt to reduce the computation and memory costs from $O(n^2+nKd)$ to $O(nd)$ by sacrificing a marginal constant factor, where $n$ is the size of the point cloud and $K$ is neighborhood size.","The efficiency is primarily due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighborhoods.","In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also strongest descriptiveness and robustness compared with existing popular encoders.","In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10x."],"url":"http://arxiv.org/abs/2404.01568v1","category":"cs.CV"}
{"created":"2024-04-02 02:00:42","title":"Heterogeneous Treatment Effects and Causal Mechanisms","abstract":"The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without additional, generally implicit, assumptions. Further, even when these assumptions are satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.","sentences":["The credibility revolution advances the use of research designs that permit identification and estimation of causal effects.","However, understanding which mechanisms produce measured causal effects remains a challenge.","A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates.","This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism.","We show first that this design cannot provide evidence of mechanism activation without additional, generally implicit, assumptions.","Further, even when these assumptions are satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation.","We provide novel guidance for interpretation and research design in light of these findings."],"url":"http://arxiv.org/abs/2404.01566v1","category":"econ.EM"}
{"created":"2024-04-02 01:57:20","title":"The radiative decay of scalar glueball from lattice QCD","abstract":"We perform the first lattice QCD study on the radiative decay of the scalar glueball to the vector meson $\\phi$ in the quenched approximation. The calculations are carried out on three gauge ensembles with different lattice spaicings, which enable us to do the continuum extrapolation. We first revisit the radiative $J/\\psi$ decay into the scalar glueball $G$ and obtain the partial decay width $\\Gamma(J/\\psi \\to \\gamma G)=0.449(44)~\\text{keV}$ and the branching fraction $\\text{Br}(J/\\psi \\to \\gamma G) = 4.8(5)\\times 10^{-3}$, which are in agreement with the previous lattice results. We then extend the similar calculation to the process $G\\to \\gamma \\phi$ and get the partial decay width $\\Gamma(G \\to \\gamma \\phi)= 0.074(47)~\\text{keV}$, which implies that the combined branching fraction of $J/\\psi\\to\\gamma G\\to \\gamma\\gamma\\phi$ is as small as $\\mathcal{O}(10^{-9})$ such that this process is hardly detected by the BESIII experiment even with the large $J/\\psi$ sample of $\\mathcal{O}(10^{10})$. With the vector meson dominance model, the two-photon decay width of the scalar glueball is estimated to be $\\Gamma(G\\to\\gamma\\gamma)=0.53(46)~\\text{eV}$, which results in a large stickiness $S(G)\\sim \\mathcal{O}(10^4)$ of the scalar glueball by assuming the stickiness of $f_2(1270)$ to be one.","sentences":["We perform the first lattice QCD study on the radiative decay of the scalar glueball to the vector meson $\\phi$ in the quenched approximation.","The calculations are carried out on three gauge ensembles with different lattice spaicings, which enable us to do the continuum extrapolation.","We first revisit the radiative $J/\\psi$ decay into the scalar glueball $G$ and obtain the partial decay width $\\Gamma(J/\\psi \\to \\gamma G)=0.449(44)~\\text{keV}$ and the branching fraction $\\text{Br}(J/\\psi \\to \\gamma G) = 4.8(5)\\times 10^{-3}$, which are in agreement with the previous lattice results.","We then extend the similar calculation to the process $G\\to \\gamma \\phi$ and get the partial decay width $\\Gamma(G \\to \\gamma \\phi)= 0.074(47)~\\text{keV}$, which implies that the combined branching fraction of $J/\\psi\\to\\gamma G\\to \\gamma\\gamma\\phi$ is as small as $\\mathcal{O}(10^{-9})$ such that this process is hardly detected by the BESIII experiment even with the large $J/\\psi$ sample of $\\mathcal{O}(10^{10})$. With the vector meson dominance model, the two-photon decay width of the scalar glueball is estimated to be $\\Gamma(G\\to\\gamma\\gamma)=0.53(46)~\\text{eV}$, which results in a large stickiness $S(G)\\sim \\mathcal{O}(10^4)$ of the scalar glueball by assuming the stickiness of $f_2(1270)$ to be one."],"url":"http://arxiv.org/abs/2404.01564v1","category":"hep-lat"}
{"created":"2024-04-02 00:33:17","title":"The Schwarzian from gauge theories","abstract":"The continuum of holographic dual gravitational charges is recovered out of the discrete spectrum of $U(N)$ $\\mathcal{N}=4$ SYM on $\\mathbb{R}\\times S^3\\,$. In such a limit, the free energy of the free gauge theory is computed up to logarithmic contributions and exponentially suppressed contributions. Assuming the supergravity dual prediction to correctly capture strong-coupling results in field theory, the answer is bound to encode a complete low-temperature expansion of the Gibbons-Hawking gravitational on-shell action, valid well beyond the vicinity of supersymmetric black hole solutions. The formula recovers the long awaited Schwarzian contribution at low enough temperatures. The computed mass-gap matches the conjectured strong-coupling result obtained by Boruch, Heydeman, Iliesiu and Turiaci in supergravity. The emergent reparameterizations, broken by the Schwarzian, correspond to redefinitions of the relevant cutoff scale. Observations are made regarding the existence of $\\frac{1}{8}$-BPS black holes and how this is in tension with BPS inequalities. The RG-flow procedure leading to these results opens the way to understanding the emergence of chaos in gauge theories and its relation to non-extremal and non-supersymmetric black hole physics.","sentences":["The continuum of holographic dual gravitational charges is recovered out of the discrete spectrum of $U(N)$ $\\mathcal{N}=4$ SYM on $\\mathbb{R}\\times S^3\\,$. In such a limit, the free energy of the free gauge theory is computed up to logarithmic contributions and exponentially suppressed contributions.","Assuming the supergravity dual prediction to correctly capture strong-coupling results in field theory, the answer is bound to encode a complete low-temperature expansion of the Gibbons-Hawking gravitational on-shell action, valid well beyond the vicinity of supersymmetric black hole solutions.","The formula recovers the long awaited Schwarzian contribution at low enough temperatures.","The computed mass-gap matches the conjectured strong-coupling result obtained by Boruch, Heydeman, Iliesiu and Turiaci in supergravity.","The emergent reparameterizations, broken by the Schwarzian, correspond to redefinitions of the relevant cutoff scale.","Observations are made regarding the existence of $\\frac{1}{8}$-BPS black holes and how this is in tension with BPS inequalities.","The RG-flow procedure leading to these results opens the way to understanding the emergence of chaos in gauge theories and its relation to non-extremal and non-supersymmetric black hole physics."],"url":"http://arxiv.org/abs/2404.01540v1","category":"hep-th"}
{"created":"2024-04-01 22:42:26","title":"Universal Bound on Effective Central Charge and Its Saturation","abstract":"The effective central charge (denoted by $c_{\\text{eff}}$) is a measure of entanglement through a conformal interface, while the transmission coefficient (encoded in the coefficient $c_{LR}$ of the two-point function of the energy-momentum tensor across the interface) is a measure of energy transmission through the interface. It has been pointed out that these two are generally different. In this article, we propose the inequalities, $0 \\leq c_{LR} \\leq c_{\\text{eff}} \\leq \\min (c_L,c_R)$. They have the simple but important implication that the amount of energy transmission can never exceed the amount of information transmission. We verify them using the AdS/CFT correspondence and in examples beyond holography. We also show that these inequalities are sharp by constructing a class of interfaces that saturate them.","sentences":["The effective central charge (denoted by $c_{\\text{eff}}$) is a measure of entanglement through a conformal interface, while the transmission coefficient (encoded in the coefficient $c_{LR}$ of the two-point function of the energy-momentum tensor across the interface) is a measure of energy transmission through the interface.","It has been pointed out that these two are generally different.","In this article, we propose the inequalities, $0 \\leq c_{LR} \\leq c_{\\text{eff}} \\leq \\min (c_L,c_R)$. They have the simple but important implication that the amount of energy transmission can never exceed the amount of information transmission.","We verify them using the AdS/CFT correspondence and in examples beyond holography.","We also show that these inequalities are sharp by constructing a class of interfaces that saturate them."],"url":"http://arxiv.org/abs/2404.01515v1","category":"hep-th"}
{"created":"2024-04-01 22:39:18","title":"A drug classification pipeline for Medicaid claims using RxNorm","abstract":"Objective: Freely preprocess drug codes recorded in electronic health records and insurance claims to drug classes that may then be used in biomedical research.   Materials and Methods: We developed a drug classification pipeline for linking National Drug Codes to the World Health Organization Anatomical Therapeutic Chemical classification. To implement our solution, we created an R package interface to the National Library of Medicine's RxNorm API.   Results: Using the classification pipeline, 59.4% of all unique NDC were linked to an ATC, resulting in 95.5% of all claims being successfully linked to a drug classification. We identified 12,004 unique NDC codes that were classified as being an opioid or non-opioid prescription for treating pain.   Discussion: Our proposed pipeline performed similarly well to other NDC classification routines using commercial databases. A check of a small, random sample of non-active NDC found the pipeline to be accurate for classifying these codes.   Conclusion: The RxNorm NDC classification pipeline is a practical and reliable tool for categorizing drugs in large-scale administrative claims data.","sentences":["Objective: Freely preprocess drug codes recorded in electronic health records and insurance claims to drug classes that may then be used in biomedical research.   ","Materials and Methods: We developed a drug classification pipeline for linking National Drug Codes to the World Health Organization Anatomical Therapeutic Chemical classification.","To implement our solution, we created an R package interface to the National Library of Medicine's RxNorm API.   ","Results: Using the classification pipeline, 59.4% of all unique NDC were linked to an ATC, resulting in 95.5% of all claims being successfully linked to a drug classification.","We identified 12,004 unique NDC codes that were classified as being an opioid or non-opioid prescription for treating pain.   ","Discussion: Our proposed pipeline performed similarly well to other NDC classification routines using commercial databases.","A check of a small, random sample of non-active NDC found the pipeline to be accurate for classifying these codes.   ","Conclusion: The RxNorm NDC classification pipeline is a practical and reliable tool for categorizing drugs in large-scale administrative claims data."],"url":"http://arxiv.org/abs/2404.01514v1","category":"q-bio.QM"}
{"created":"2024-04-01 22:36:09","title":"Breaking the mass-sheet degeneracy in strong lensing mass modeling with weak lensing observations","abstract":"The Hubble constant ($H_0$), a crucial parameter in cosmology, quantifies the expansion rate of the universe so its precise measurement is important to understand the fundamental dynamics of our evolving universe. One of the major limitations of measuring $H_0$ using time-delay cosmography is the presence of the mass-sheet degeneracy (MSD) in the lens mass modeling. We propose and quantitatively assess the use of galaxy-galaxy shear measurements to break the MSD in the strong lensing mass modeling. We use stacked galaxy-galaxy lensing profiles and corresponding covariance matrices from Huang et al. (2022) to constrain the MSD in lens mass modeling with a highly flexible mass profile. Our analyses show that if ideally all galaxy-galaxy lensing measurements from the Hyper Suprime-Cam (HSC) survey can be used to constrain the MSD, we can achieve $\\sim 10\\%$ precision on the MSD constraint. We forecast that galaxy-galaxy lensing measurements from LSST-like surveys can in general constrain the MSD with $\\sim 1-3\\%$ precision. Furthermore, if we push weak lensing measurements to a lower angular scale of $\\sim 0.04$ $\\rm Mpc$, a survey like LSST can provide $\\sim 1\\%$ precision on the MSD constraint, enabling a measurement of $H_0$ at the $1\\%$ level. We demonstrate that galaxy-galaxy weak lensing can robustly constrain the MSD independent of stellar kinematics of the deflector, with wide-field survey data alone.","sentences":["The Hubble constant ($H_0$), a crucial parameter in cosmology, quantifies the expansion rate of the universe so its precise measurement is important to understand the fundamental dynamics of our evolving universe.","One of the major limitations of measuring $H_0$ using time-delay cosmography is the presence of the mass-sheet degeneracy (MSD) in the lens mass modeling.","We propose and quantitatively assess the use of galaxy-galaxy shear measurements to break the MSD in the strong lensing mass modeling.","We use stacked galaxy-galaxy lensing profiles and corresponding covariance matrices from Huang et al. (2022) to constrain the MSD in lens mass modeling with a highly flexible mass profile.","Our analyses show that if ideally all galaxy-galaxy lensing measurements from the Hyper Suprime-Cam (HSC) survey can be used to constrain the MSD, we can achieve $\\sim 10\\%$ precision on the MSD constraint.","We forecast that galaxy-galaxy lensing measurements from LSST-like surveys can in general constrain the MSD with $\\sim 1-3\\%$ precision.","Furthermore, if we push weak lensing measurements to a lower angular scale of $\\sim 0.04$ $\\rm Mpc$, a survey like LSST can provide $\\sim 1\\%$ precision on the MSD constraint, enabling a measurement of $H_0$ at the $1\\%$ level.","We demonstrate that galaxy-galaxy weak lensing can robustly constrain the MSD independent of stellar kinematics of the deflector, with wide-field survey data alone."],"url":"http://arxiv.org/abs/2404.01513v1","category":"astro-ph.CO"}
{"created":"2024-04-01 22:35:09","title":"Approximating hyperbolic lattices by cubulations","abstract":"We show that an isometric action of a torsion-free uniform lattice $\\Gamma$ on hyperbolic space $\\mathbb{H}^n$ can be metrically approximated by geometric actions of $\\Gamma$ on $\\mathrm{CAT}(0)$ cube complexes, provided that either $n$ is at most three, or the lattice is arithmetic of simplest type. This solves a conjecture of Futer and Wise.   Our main tool is the study of a space of co-geodesic currents, consisting of invariant Radon measures supported on codimension-1 hyperspheres in the Gromov boundary of $\\mathbb{H}^n$. By pairing co-geodesic currents and geodesic currents via an intersection number, we show that asymptotic convergence of geometric actions can be deduced from the convergence of their dual co-geodesic currents.   For surface groups, our methods also imply approximation by cubulations for actions induced by non-positively curved Riemannian surfaces with singularities, Hitchin and maximal representations, and quasiFuchsian representations.","sentences":["We show that an isometric action of a torsion-free uniform lattice $\\Gamma$ on hyperbolic space $\\mathbb{H}^n$ can be metrically approximated by geometric actions of $\\Gamma$ on $\\mathrm{CAT}(0)$ cube complexes, provided that either $n$ is at most three, or the lattice is arithmetic of simplest type.","This solves a conjecture of Futer and Wise.   ","Our main tool is the study of a space of co-geodesic currents, consisting of invariant Radon measures supported on codimension-1 hyperspheres in the Gromov boundary of $\\mathbb{H}^n$. By pairing co-geodesic currents and geodesic currents via an intersection number, we show that asymptotic convergence of geometric actions can be deduced from the convergence of their dual co-geodesic currents.   ","For surface groups, our methods also imply approximation by cubulations for actions induced by non-positively curved Riemannian surfaces with singularities, Hitchin and maximal representations, and quasiFuchsian representations."],"url":"http://arxiv.org/abs/2404.01511v1","category":"math.GR"}
{"created":"2024-04-01 21:48:25","title":"Upcoming searches for decaying dark matter with ULTRASAT ultraviolet maps","abstract":"Decaying dark matter (DDM) can be tested via different astrophysical and cosmological probes. In particular, particles in the $\\sim$ 9.5 - 30 eV mass range that decay into monochromatic photons, would contribute to the extragalactic background light (EBL) in the ultraviolet (UV) bandwidth. In this work, we show that an intriguing improvement to the constraints on such DDM models can come from broadband UV surveys, such as GALEX or the upcoming ULTRASAT satellite. These provide diffuse light maps of the UV EBL, integrated over a wide redshift range. The cross correlation between intensity fluctuations in these maps with a reference spectroscopic galaxy survey, can be used to reconstruct the redshift evolution of the EBL intensity; in this way, it is also possible to detect signatures of contributions from DDM. We forecast the constraining power of (GALEX+ULTRASAT)$\\times$DESI, and we show they will be able to detect DDM with decay rate up to $\\mathcal{O}(10^{-26}\\,{\\rm s})$. In the context of axion-like particles (ALP), our forecasts can be converted to constraints on the ALP-photon coupling; our results show this technique will test ALP with coupling $\\lesssim\\mathcal{O}(10^{-12}\\,{\\rm GeV^{-1}})$, more than an order of magnitude better than current bounds in this mass range.","sentences":["Decaying dark matter (DDM) can be tested via different astrophysical and cosmological probes.","In particular, particles in the $\\sim$ 9.5 - 30 eV mass range that decay into monochromatic photons, would contribute to the extragalactic background light (EBL) in the ultraviolet (UV) bandwidth.","In this work, we show that an intriguing improvement to the constraints on such DDM models can come from broadband UV surveys, such as GALEX or the upcoming ULTRASAT satellite.","These provide diffuse light maps of the UV EBL, integrated over a wide redshift range.","The cross correlation between intensity fluctuations in these maps with a reference spectroscopic galaxy survey, can be used to reconstruct the redshift evolution of the EBL intensity; in this way, it is also possible to detect signatures of contributions from DDM.","We forecast the constraining power of (GALEX+ULTRASAT)$\\times$DESI, and we show they will be able to detect DDM with decay rate up to $\\mathcal{O}(10^{-26}\\,{\\rm s})$.","In the context of axion-like particles (ALP), our forecasts can be converted to constraints on the ALP-photon coupling; our results show this technique will test ALP with coupling $\\lesssim\\mathcal{O}(10^{-12}\\,{\\rm GeV^{-1}})$, more than an order of magnitude better than current bounds in this mass range."],"url":"http://arxiv.org/abs/2404.01500v1","category":"astro-ph.CO"}
{"created":"2024-04-01 21:34:00","title":"Estimating Heterogeneous Effects: Applications to Labor Economics","abstract":"A growing number of applications involve settings where, in order to infer heterogeneous effects, a researcher compares various units. Examples of research designs include children moving between different neighborhoods, workers moving between firms, patients migrating from one city to another, and banks offering loans to different firms. We present a unified framework for these settings, based on a linear model with normal random coefficients and normal errors. Using the model, we discuss how to recover the mean and dispersion of effects, other features of their distribution, and to construct predictors of the effects. We provide moment conditions on the model's parameters, and outline various estimation strategies. A main objective of the paper is to clarify some of the underlying assumptions by highlighting their economic content, and to discuss and inform some of the key practical choices.","sentences":["A growing number of applications involve settings where, in order to infer heterogeneous effects, a researcher compares various units.","Examples of research designs include children moving between different neighborhoods, workers moving between firms, patients migrating from one city to another, and banks offering loans to different firms.","We present a unified framework for these settings, based on a linear model with normal random coefficients and normal errors.","Using the model, we discuss how to recover the mean and dispersion of effects, other features of their distribution, and to construct predictors of the effects.","We provide moment conditions on the model's parameters, and outline various estimation strategies.","A main objective of the paper is to clarify some of the underlying assumptions by highlighting their economic content, and to discuss and inform some of the key practical choices."],"url":"http://arxiv.org/abs/2404.01495v1","category":"econ.EM"}
{"created":"2024-04-01 21:30:53","title":"Schur-Weyl dualities for the rook monoid: an approach via Schur algebras","abstract":"The rook monoid, also known as the symmetric inverse monoid, is the archetypal structure when it comes to extend the principle of symmetry. In this paper, we establish a Schur-Weyl duality between this monoid and an extension of the classical Schur algebra, which we name the extended Schur algebra. We also explain how this relates to Solomon's Schur-Weyl duality between the rook monoid and the general linear group and mention some advantages of our approach.","sentences":["The rook monoid, also known as the symmetric inverse monoid, is the archetypal structure when it comes to extend the principle of symmetry.","In this paper, we establish a Schur-Weyl duality between this monoid and an extension of the classical Schur algebra, which we name the extended Schur algebra.","We also explain how this relates to Solomon's Schur-Weyl duality between the rook monoid and the general linear group and mention some advantages of our approach."],"url":"http://arxiv.org/abs/2404.01493v1","category":"math.RT"}
{"created":"2024-04-01 21:00:29","title":"A Multidimensional Fractional Hawkes Process for Multiple Earthquake Mainshock Aftershock Sequences","abstract":"Most point process models for earthquakes currently in the literature assume the magnitude distribution is i.i.d. potentially hindering the ability of the model to describe the main features of data sets containing multiple earthquake mainshock aftershock sequences in succession. This study presents a novel multidimensional fractional Hawkes process model designed to capture magnitude dependent triggering behaviour by incorporating history dependence into the magnitude distribution. This is done by discretising the magnitude range into disjoint intervals and modelling events with magnitude in these ranges as the subprocesses of a mutually exciting Hawkes process using the Mittag-Leffler density as the kernel function. We demonstrate this model's use by applying it to two data sets, Japan and the Middle America Trench, both containing multiple mainshock aftershock sequences and compare it to the existing ETAS model by using information criteria, residual diagnostics and retrospective prediction performance. We find that for both data sets all metrics indicate that the multidimensional fractional Hawkes process performs favourably against the ETAS model. Furthermore, using the multidimensional fractional Hawkes process we are able to infer characteristics of the data sets that are consistent with results currently in the literature and that cannot be found by using the ETAS model.","sentences":["Most point process models for earthquakes currently in the literature assume the magnitude distribution is i.i.d.","potentially hindering the ability of the model to describe the main features of data sets containing multiple earthquake mainshock aftershock sequences in succession.","This study presents a novel multidimensional fractional Hawkes process model designed to capture magnitude dependent triggering behaviour by incorporating history dependence into the magnitude distribution.","This is done by discretising the magnitude range into disjoint intervals and modelling events with magnitude in these ranges as the subprocesses of a mutually exciting Hawkes process using the Mittag-Leffler density as the kernel function.","We demonstrate this model's use by applying it to two data sets, Japan and the Middle America Trench, both containing multiple mainshock aftershock sequences and compare it to the existing ETAS model by using information criteria, residual diagnostics and retrospective prediction performance.","We find that for both data sets all metrics indicate that the multidimensional fractional Hawkes process performs favourably against the ETAS model.","Furthermore, using the multidimensional fractional Hawkes process we are able to infer characteristics of the data sets that are consistent with results currently in the literature and that cannot be found by using the ETAS model."],"url":"http://arxiv.org/abs/2404.01478v1","category":"stat.AP"}
{"created":"2024-04-01 20:14:36","title":"Efficient configuration-interaction models for photoionization of molecular dimers","abstract":"We present R-matrix calculations of photoionization of molecular monomers and dimers, focusing on ammonia (NH3) and formic acid (HCOOH), utilizing configuration-interaction models including the Occupation-Restricted Multiple Active Space (ORMAS) approach. We show that ORMAS is a highly efficient choice for calculating photoionization observables, yielding results that are in excellent agreement with those obtained using the much more demanding configuration-interaction method, Complete Active Space (CAS). We demonstrate that models incorporating single and double (SD) excitations with respect to the Hartree-Fock configuration provide good agreement with experimental data. The approach developed here can be readily applied to study photoionization in complex molecular systems.","sentences":["We present R-matrix calculations of photoionization of molecular monomers and dimers, focusing on ammonia (NH3) and formic acid (HCOOH), utilizing configuration-interaction models including the Occupation-Restricted Multiple Active Space (ORMAS) approach.","We show that ORMAS is a highly efficient choice for calculating photoionization observables, yielding results that are in excellent agreement with those obtained using the much more demanding configuration-interaction method, Complete Active Space (CAS).","We demonstrate that models incorporating single and double (SD) excitations with respect to the Hartree-Fock configuration provide good agreement with experimental data.","The approach developed here can be readily applied to study photoionization in complex molecular systems."],"url":"http://arxiv.org/abs/2404.01460v1","category":"physics.chem-ph"}
{"created":"2024-04-01 20:11:05","title":"A Semi-blind Reconstruction of the History of Effective Number of Neutrinos Using CMB Data","abstract":"We explore the possibility of redshift-dependent deviations in the contribution of relativistic degrees of freedom to the radiation budget of the cosmos, conventionally parameterized by the effective number of neutrinos $N_{\\rm eff}$, from the predictions of the standard model. We expand the deviations $\\Delta N_{\\rm eff}(z)$ in terms of top-hat functions and treat their amplitudes as the free parameters of the theory to be measured alongside the standard cosmological parameters by the Planck measurements of the cosmic microwave background (CMB) anisotropies and Baryonic Acoustic Oscillations, as well as performing forecasts for futuristic CMB surveys such as PICO and CMB-S4. We reconstruct the history of $\\Delta N_{\\rm eff}$ and find that with the current data the history is consistent with the standard scenario. Inclusion of the new degrees of freedom in the analysis increases $H_0$ to $68.71\\pm 0.44$, slightly reducing the Hubble tension. With the smaller forecasted errors on the $\\Delta N_{\\rm eff}(z)$ parametrization modes from future CMB surveys, very accurate bounds are expected within the possible range of dark radiation models.","sentences":["We explore the possibility of redshift-dependent deviations in the contribution of relativistic degrees of freedom to the radiation budget of the cosmos, conventionally parameterized by the effective number of neutrinos $N_{\\rm eff}$, from the predictions of the standard model.","We expand the deviations $\\Delta N_{\\rm eff}(z)$ in terms of top-hat functions and treat their amplitudes as the free parameters of the theory to be measured alongside the standard cosmological parameters by the Planck measurements of the cosmic microwave background (CMB) anisotropies and Baryonic Acoustic Oscillations, as well as performing forecasts for futuristic CMB surveys such as PICO and CMB-S4.","We reconstruct the history of $\\Delta N_{\\rm eff}$ and find that with the current data the history is consistent with the standard scenario.","Inclusion of the new degrees of freedom in the analysis increases $H_0$ to $68.71\\pm 0.44$, slightly reducing the Hubble tension.","With the smaller forecasted errors on the $\\Delta N_{\\rm eff}(z)$ parametrization modes from future CMB surveys, very accurate bounds are expected within the possible range of dark radiation models."],"url":"http://arxiv.org/abs/2404.01457v1","category":"astro-ph.CO"}
{"created":"2024-04-01 20:09:21","title":"Monster shocks, gamma-ray bursts and black hole quasi-normal modes from neutron-star collapse","abstract":"We perform the first magnetohydrodynamic simulation tracking the magnetosphere of a collapsing magnetar. The collapse is expected for massive rotating magnetars formed in merger events, and it may occur many hours after the merger. The results suggest a novel gamma-ray burst (GRB) scenario, which creates a delayed high- energy counterpart of the merger gravitational waves. The collapse launches an outgoing magnetospheric shock, and a hot magnetized outflow forms behind the shock. The outflow is modulated by the ring-down of the nascent black hole, imprinting its kilohertz quasi-normal modes on the GRB tail.","sentences":["We perform the first magnetohydrodynamic simulation tracking the magnetosphere of a collapsing magnetar.","The collapse is expected for massive rotating magnetars formed in merger events, and it may occur many hours after the merger.","The results suggest a novel gamma-ray burst (GRB) scenario, which creates a delayed high- energy counterpart of the merger gravitational waves.","The collapse launches an outgoing magnetospheric shock, and a hot magnetized outflow forms behind the shock.","The outflow is modulated by the ring-down of the nascent black hole, imprinting its kilohertz quasi-normal modes on the GRB tail."],"url":"http://arxiv.org/abs/2404.01456v1","category":"astro-ph.HE"}
