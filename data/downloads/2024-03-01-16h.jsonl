{"created":"2024-02-28 18:58:25","title":"Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards","abstract":"Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).","sentences":["Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs.","While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications.","To address this limitation, we introduce the Directional Preference Alignment (DPA) framework.","Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles.","Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control.","Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2.","This method enjoys a better performance trade-off across various reward objectives.","In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity).","We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B.","Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO)."],"url":"http://arxiv.org/abs/2402.18571v2","category":"cs.LG"}
{"created":"2024-02-28 18:54:18","title":"Approaching Human-Level Forecasting with Language Models","abstract":"Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.","sentences":["Forecasting future events is important for policy and decision making.","In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters.","Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions.","To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms.","Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts.","On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it.","Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making."],"url":"http://arxiv.org/abs/2402.18563v1","category":"cs.LG"}
{"created":"2024-02-28 18:29:25","title":"Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces","abstract":"A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization","sentences":["A major goal in neuroscience is to discover neural data representations that generalize.","This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others.","Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments.","In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024).","EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model.","We find that TOTEM outperforms or matches EEGNet across all generalizability cases.","Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization"],"url":"http://arxiv.org/abs/2402.18546v2","category":"cs.LG"}
{"created":"2024-02-28 18:29:07","title":"Crowdsourcing Dermatology Images with Google Search Ads: Creating a Real-World Skin Condition Dataset","abstract":"Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development. Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets.   Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic and symptom information. With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023. The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images.   Results: We received a median of 22 submissions/day (IQR 14-30). Female (66.72%) and younger (52% < age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6% of contributors reported a non-White racial or ethnic identity. Over 97.5% of contributions were genuine images of skin conditions. Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weaker correlation with image sharpness (Spearman's P values <0.001 and 0.01 respectively). Most contributions were short-duration (54% with onset < 7 days ago ) and 89% were allergic, infectious, or inflammatory conditions. eFST and eMST distributions reflected the geographical origin of the dataset. The dataset is available at github.com/google-research-datasets/scin .   Conclusion: Search ads are effective at crowdsourcing images of health conditions. The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions.","sentences":["Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development.","Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets.   ","Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic and symptom information.","With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023.","The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images.   ","Results:","We received a median of 22 submissions/day (IQR 14-30).","Female (66.72%) and younger (52% < age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6% of contributors reported a non-White racial or ethnic identity.","Over 97.5% of contributions were genuine images of skin conditions.","Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weaker correlation with image sharpness (Spearman's P values <0.001 and 0.01 respectively).","Most contributions were short-duration (54% with onset < 7 days ago ) and 89% were allergic, infectious, or inflammatory conditions.","eFST and eMST distributions reflected the geographical origin of the dataset.","The dataset is available at github.com/google-research-datasets/scin .   ","Conclusion: Search ads are effective at crowdsourcing images of health conditions.","The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions."],"url":"http://arxiv.org/abs/2402.18545v1","category":"cs.CY"}
{"created":"2024-02-28 18:23:49","title":"Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates","abstract":"Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the \"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases.","sentences":["Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research.","These models underwent alignment training and were considered safe.","Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.","The current paper is about methods and best practices to mitigate such loss of alignment.","Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the \"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time.","Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases."],"url":"http://arxiv.org/abs/2402.18540v1","category":"cs.LG"}
{"created":"2024-02-28 17:40:01","title":"Leveraging Compliant Tactile Perception for Haptic Blind Surface Reconstruction","abstract":"Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces.","sentences":["Non-flat surfaces pose difficulties for robots operating in unstructured environments.","Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions.","This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors.","The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects.","While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points.","These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface.","Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy.","Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces."],"url":"http://arxiv.org/abs/2402.18511v1","category":"cs.RO"}
{"created":"2024-02-28 17:27:55","title":"On the exact solution for the Schr\u00f6dinger equation","abstract":"For almost 75 years, the general solution for the Schr\\\"odinger equation was assumed to be generated by a time-ordered exponential known as the Dyson series. We discuss under which conditions the unitarity of this solution is broken, and additional singular dynamics emerges. Then, we provide an alternative construction that is manifestly unitary, regardless of the choice of the Hamiltonian, and study various aspects of the implications. The new construction involves an additional self-adjoint operator that might evolve in a non-gradual way. Its corresponding dynamics for gauge theories exhibit the behavior of a collective object governed by a singular Liouville's equation that performs transitions at a measure $0$ set. Our considerations show that Schr\\\"odinger's and Liouville's equations are, in fact, two sides of the same coin, and together they become the unified description of quantum systems.","sentences":["For almost 75 years, the general solution for the Schr\\\"odinger equation was assumed to be generated by a time-ordered exponential known as the Dyson series.","We discuss under which conditions the unitarity of this solution is broken, and additional singular dynamics emerges.","Then, we provide an alternative construction that is manifestly unitary, regardless of the choice of the Hamiltonian, and study various aspects of the implications.","The new construction involves an additional self-adjoint operator that might evolve in a non-gradual way.","Its corresponding dynamics for gauge theories exhibit the behavior of a collective object governed by a singular Liouville's equation that performs transitions at a measure $0$ set.","Our considerations show that Schr\\\"odinger's and Liouville's equations are, in fact, two sides of the same coin, and together they become the unified description of quantum systems."],"url":"http://arxiv.org/abs/2402.18499v2","category":"quant-ph"}
{"created":"2024-02-28 17:25:59","title":"Language Models Represent Beliefs of Self and Others","abstract":"Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.","sentences":["Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning.","While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive.","In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs.","By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process.","Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations."],"url":"http://arxiv.org/abs/2402.18496v2","category":"cs.AI"}
{"created":"2024-02-28 17:10:22","title":"Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay","abstract":"The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our contributions include (1) a reinforcement learning framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios. The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values.","sentences":["The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness.","However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground.","This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions.","We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations.","Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR.","Our contributions include (1) a reinforcement learning framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios.","The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values."],"url":"http://arxiv.org/abs/2402.18487v1","category":"cs.RO"}
{"created":"2024-02-28 16:58:31","title":"Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes","abstract":"Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\". In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.","sentences":["Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance.","Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\".","In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent advances in signature kernels.","We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space.","Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph.","Assuming faithfulness and a CI oracle, our algorithm is sound and complete.","We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings."],"url":"http://arxiv.org/abs/2402.18477v1","category":"cs.LG"}
{"created":"2024-02-28 16:39:34","title":"Understanding the Impact of AI Generated Content on Social Media: The Pixiv Case","abstract":"In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms. The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design. Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem. To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations. Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023). Importantly, it allows both human and AI generated content to be uploaded. Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv. Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns. Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv.","sentences":["In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms.","The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design.","Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem.","To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations.","Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023).","Importantly, it allows both human and AI generated content to be uploaded.","Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv.","Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns.","Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv."],"url":"http://arxiv.org/abs/2402.18463v1","category":"cs.CY"}
{"created":"2024-02-28 16:37:23","title":"Search for baryon number violation in top quark production and decay using proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is presented for baryon number violating interactions in top quark production and decay. The analysis uses data from proton-proton collisions at a center-of-mass energy of 13 TeV, collected with the CMS detector at the LHC with an integrated luminosity of 138 fb$^{-1}$. Candidate events are selected by requiring two oppositely-charged leptons (electrons or muons) and exactly one jet identified as originating from a bottom quark. Multivariate discriminants are used to separate the signal from the background. No significant deviation from the standard model prediction is observed. Upper limits are placed on the strength of baryon number violating couplings. For the first time the production of single top quarks via baryon number violating interactions is studied. This allows the search to set the most stringent constraints to date on the branching fraction of the top quark decay to a lepton, an up-type quark (u or c), and a down-type quark (d, s, or b). The results improve the previous bounds by three to six orders of magnitude based on the fermion flavor combination of the baryon number violating interactions.","sentences":["A search is presented for baryon number violating interactions in top quark production and decay.","The analysis uses data from proton-proton collisions at a center-of-mass energy of 13 TeV, collected with the CMS detector at the LHC with an integrated luminosity of 138 fb$^{-1}$. Candidate events are selected by requiring two oppositely-charged leptons (electrons or muons) and exactly one jet identified as originating from a bottom quark.","Multivariate discriminants are used to separate the signal from the background.","No significant deviation from the standard model prediction is observed.","Upper limits are placed on the strength of baryon number violating couplings.","For the first time the production of single top quarks via baryon number violating interactions is studied.","This allows the search to set the most stringent constraints to date on the branching fraction of the top quark decay to a lepton, an up-type quark (u or c), and a down-type quark (d, s, or b).","The results improve the previous bounds by three to six orders of magnitude based on the fermion flavor combination of the baryon number violating interactions."],"url":"http://arxiv.org/abs/2402.18461v1","category":"hep-ex"}
{"created":"2024-02-28 16:21:02","title":"HOP to the Next Tasks and Domains for Continual Learning in NLP","abstract":"Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP.","sentences":["Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones.","Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework.","Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem.","Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP."],"url":"http://arxiv.org/abs/2402.18449v1","category":"cs.CL"}
{"created":"2024-02-28 16:13:44","title":"LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs","abstract":"Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user.","sentences":["Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge.","This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions.","In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge.","The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters.","We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component.","We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user."],"url":"http://arxiv.org/abs/2402.18443v1","category":"cs.LG"}
{"created":"2024-02-28 16:07:54","title":"Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication","abstract":"Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \\url{https://github.com/thunlp/AutoForm}.","sentences":["Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs).","Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression.","NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined.","In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts.","We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness.","Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs.","Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication.","Our code is released at \\url{https://github.com/thunlp/AutoForm}."],"url":"http://arxiv.org/abs/2402.18439v1","category":"cs.CL"}
{"created":"2024-02-28 15:51:05","title":"A Relational Inductive Bias for Dimensional Abstraction in Neural Networks","abstract":"The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility. Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols.","sentences":["The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment.","In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training.","This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing.","We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases.","Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility.","Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols."],"url":"http://arxiv.org/abs/2402.18426v1","category":"cs.AI"}
{"created":"2024-02-28 15:46:09","title":"Emotion Classification in Low and Moderate Resource Languages","abstract":"It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \\textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani. Our results indicate that our approaches outperform random baselines and transfer emotions across languages successfully. For all languages, the direct cross-lingual transfer of emotion yields better results. We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia.","sentences":["It is important to be able to analyze the emotional state of people around the globe.","There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive.","Particularly for low-resource and endangered languages, building emotion classification can be quite challenging.","We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \\textit{English} in our work) and transfer the learning to low and moderate resource languages.","We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language.","One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages.","We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani.","Our results indicate that our approaches outperform random baselines and transfer emotions across languages successfully.","For all languages, the direct cross-lingual transfer of emotion yields better results.","We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia."],"url":"http://arxiv.org/abs/2402.18424v1","category":"cs.CL"}
{"created":"2024-02-28 15:39:53","title":"Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?","abstract":"Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach. Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts.","sentences":["Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage.","For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task.","One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc.","In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster.","We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record.","We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique.","Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach.","Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts."],"url":"http://arxiv.org/abs/2402.18419v1","category":"cs.CL"}
{"created":"2024-02-28 15:28:36","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models","abstract":"Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.","sentences":["Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities.","Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics.","It defines eight reasoning capabilities and consists of an image description task and a visual question answering task.","Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans."],"url":"http://arxiv.org/abs/2402.18409v2","category":"cs.AI"}
{"created":"2024-02-28 15:13:33","title":"Evaluating Decision Optimality of Autonomous Driving via Metamorphic Testing","abstract":"Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions. In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs. Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions. The MR identifies the property that the ADS should retain optimal decisions when the optimal path remains unaffected by non-invasive changes. Subsequently, we develop a new framework, Decictor, designed to generate NoDSs efficiently. Decictor comprises three main components: Non-invasive Mutation, MR Check, and Feedback. The Non-invasive Mutation ensures that the original optimal path in the mutated scenarios is not affected, while the MR Check is responsible for determining whether non-optimal decisions are made. To enhance the effectiveness of identifying NoDSs, we design a feedback metric that combines both spatial and temporal aspects of the AV's movement. We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS. The experimental results validate the effectiveness of Decictor in detecting non-optimal decisions of ADSs. Our work provides valuable and original insights into evaluating the non-safety-critical performance of ADSs.","sentences":["Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety.","However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs.","Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions.","In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs.","Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions.","The MR identifies the property that the ADS should retain optimal decisions when the optimal path remains unaffected by non-invasive changes.","Subsequently, we develop a new framework, Decictor, designed to generate NoDSs efficiently.","Decictor comprises three main components: Non-invasive Mutation, MR Check, and Feedback.","The Non-invasive Mutation ensures that the original optimal path in the mutated scenarios is not affected, while the MR Check is responsible for determining whether non-optimal decisions are made.","To enhance the effectiveness of identifying NoDSs, we design a feedback metric that combines both spatial and temporal aspects of the AV's movement.","We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS.","The experimental results validate the effectiveness of Decictor in detecting non-optimal decisions of ADSs.","Our work provides valuable and original insights into evaluating the non-safety-critical performance of ADSs."],"url":"http://arxiv.org/abs/2402.18393v1","category":"cs.AI"}
{"created":"2024-02-28 15:12:24","title":"Unveiling the Potential of Robustness in Evaluating Causal Inference Models","abstract":"The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection. The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator. Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones.","sentences":["The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE).","The intersection of machine learning and causal inference has yielded various effective CATE estimators.","However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation.","Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges.","Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners.","Secondly, they lack a specific focus on selecting a robust estimator.","To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection.","The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator.","Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones."],"url":"http://arxiv.org/abs/2402.18392v1","category":"cs.LG"}
{"created":"2024-02-28 15:11:39","title":"Sound Concurrent Traces for Online Monitoring Technical Report","abstract":"Monitoring concurrent programs typically rely on collecting traces to abstract program executions. However, existing approaches targeting general behavioral properties are either not tailored for online monitoring, are no longer maintained, or implement naive instrumentation that often leads to unsound verdicts. We first define the notion of when a trace is representative of a concurrent execution. We then present a non-blocking vector clock algorithm to collect sound concurrent traces on the fly reflecting the partial order between events. Moreover, concurrent events in the representative trace pose a soundness problem for monitors synthesized from total order formalisms. For this, we extract a causal dependence relation from the monitor to check if the trace has the needed orderings and define the conditions to decide at runtime when a collected trace is monitorable. We implement our contributions in a tool, FACTS, which instruments programs compiling to Java bytecode, constructs sound representative traces, and warns the monitor about non-monitorable traces. We evaluate our work and compare it with existing approaches.","sentences":["Monitoring concurrent programs typically rely on collecting traces to abstract program executions.","However, existing approaches targeting general behavioral properties are either not tailored for online monitoring, are no longer maintained, or implement naive instrumentation that often leads to unsound verdicts.","We first define the notion of when a trace is representative of a concurrent execution.","We then present a non-blocking vector clock algorithm to collect sound concurrent traces on the fly reflecting the partial order between events.","Moreover, concurrent events in the representative trace pose a soundness problem for monitors synthesized from total order formalisms.","For this, we extract a causal dependence relation from the monitor to check if the trace has the needed orderings and define the conditions to decide at runtime when a collected trace is monitorable.","We implement our contributions in a tool, FACTS, which instruments programs compiling to Java bytecode, constructs sound representative traces, and warns the monitor about non-monitorable traces.","We evaluate our work and compare it with existing approaches."],"url":"http://arxiv.org/abs/2402.18391v1","category":"cs.SE"}
{"created":"2024-02-28 15:11:02","title":"Neuromorphic Event-Driven Semantic Communication in Microgrids","abstract":"Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes. As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs. Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids.","sentences":["Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids.","On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer.","This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance.","Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes.","As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs.","Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids."],"url":"http://arxiv.org/abs/2402.18390v1","category":"cs.ET"}
{"created":"2024-02-28 15:05:43","title":"The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA","abstract":"Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the \"Conversational Multi-Doc QA\" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.","sentences":["Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations.","In this paper, we introduce our winning approach for the \"Conversational Multi-Doc QA\" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs).","We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data.","Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble.","Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent.","The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024."],"url":"http://arxiv.org/abs/2402.18385v1","category":"cs.CL"}
{"created":"2024-02-28 15:02:17","title":"Large Language Models As Evolution Strategies","abstract":"Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks. Hence, LLMs can act as `plug-in' in-context recombination operators. We provide several comparative studies of the LLM's model size, prompt strategy, and context construction. Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories.","sentences":["Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms.","These include gradient descent, classification, sequence completion, transformation, and improvement.","In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms.","While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization.","We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation.","Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks.","Hence, LLMs can act as `plug-in' in-context recombination operators.","We provide several comparative studies of the LLM's model size, prompt strategy, and context construction.","Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories."],"url":"http://arxiv.org/abs/2402.18381v1","category":"cs.AI"}
{"created":"2024-02-28 14:52:58","title":"Out-of-Domain Generalization in Dynamical Systems Reconstruction","abstract":"In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.","sentences":["In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena.","While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data.","State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge.","Yet, this is a crucial property we would expect from any viable scientific theory.","In this work, we provide a formal framework that addresses generalization in DSR.","We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning.","We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model.","We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model.","We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space.","Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice."],"url":"http://arxiv.org/abs/2402.18377v1","category":"cs.LG"}
{"created":"2024-02-28 14:52:15","title":"Tokenization Is More Than Compression","abstract":"Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.","sentences":["Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models.","Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens.","We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary.","Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization.","To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers.","Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction.","We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available."],"url":"http://arxiv.org/abs/2402.18376v1","category":"cs.CL"}
{"created":"2024-02-28 14:50:27","title":"Low-Modeling of Software Systems","abstract":"There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.","sentences":["There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems.","New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle.","In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage.","In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems."],"url":"http://arxiv.org/abs/2402.18375v1","category":"cs.SE"}
{"created":"2024-02-28 14:33:14","title":"Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model","abstract":"As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.","sentences":["As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life.","However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling.","In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models.","Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions.","By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis.","Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation.","Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation.","Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy.","Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations."],"url":"http://arxiv.org/abs/2402.18362v1","category":"cs.CV"}
{"created":"2024-02-28 14:31:34","title":"Similarity-based analogical proportions","abstract":"The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra. The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former. The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions.","sentences":["The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra.","The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former.","The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions."],"url":"http://arxiv.org/abs/2402.18360v1","category":"cs.LO"}
{"created":"2024-02-28 14:01:09","title":"Generating candidates in global optimization algorithms using complementary energy landscapes","abstract":"Global optimization of atomistic structure rely on the generation of new candidate structures in order to drive the exploration of the potential energy surface (PES) in search for the global minimum energy (GM) structure. In this work, we discuss a type of structure generation, which locally optimizes structures in complementary energy (CE) landscapes. These landscapes are formulated temporarily during the searches as machine learned potentials (MLPs) using local atomistic environments sampled from collected data. The CE landscapes are deliberately incomplete MLPs that rather than mimicking every aspect of the true PES are sought to become much smoother, having only few local minima. This means that local optimization in the CE landscapes may facilitate identification of new funnels in the true PES. We discuss how to construct the CE landscapes and we test their influence on global optimization of a reduced rutile SnO2(110)-(4x1) surface, and an olivine (Mg2SiO4)4 cluster for which we report a new global minimum energy structure.","sentences":["Global optimization of atomistic structure rely on the generation of new candidate structures in order to drive the exploration of the potential energy surface (PES) in search for the global minimum energy (GM) structure.","In this work, we discuss a type of structure generation, which locally optimizes structures in complementary energy (CE) landscapes.","These landscapes are formulated temporarily during the searches as machine learned potentials (MLPs) using local atomistic environments sampled from collected data.","The CE landscapes are deliberately incomplete MLPs that rather than mimicking every aspect of the true PES are sought to become much smoother, having only few local minima.","This means that local optimization in the CE landscapes may facilitate identification of new funnels in the true PES.","We discuss how to construct the CE landscapes and we test their influence on global optimization of a reduced rutile SnO2(110)-(4x1) surface, and an olivine (Mg2SiO4)4 cluster for which we report a new global minimum energy structure."],"url":"http://arxiv.org/abs/2402.18338v1","category":"physics.chem-ph"}
{"created":"2024-02-28 13:53:02","title":"On the simulation of quantum multimeters","abstract":"In the quest for robust and universal quantum devices, the notion of simulation plays a crucial role, both from a theoretical and from an applied perspective. In this work, we go beyond the simulation of quantum channels and quantum measurements, studying what it means to simulate a collection of measurements, which we call a multimeter. To this end, we first explicitly characterize the completely positive transformations between multimeters. However, not all of these transformations correspond to valid simulations, as evidenced by the existence of maps that always prepare the same multimeter regardless of the input, which we call trash-and-prepare. We give a new definition of multimeter simulations as transformations that are triviality-preserving, i.e., when given a multimeter consisting of trivial measurements they can only produce another trivial multimeter. In the absence of a quantum ancilla, we then characterize the transformations that are triviality-preserving and the transformations that are trash-and-prepare. Finally, we use these characterizations to compare our new definition of multimeter simulation to three existing ones: classical simulations, compression of multimeters, and compatibility-preserving simulations.","sentences":["In the quest for robust and universal quantum devices, the notion of simulation plays a crucial role, both from a theoretical and from an applied perspective.","In this work, we go beyond the simulation of quantum channels and quantum measurements, studying what it means to simulate a collection of measurements, which we call a multimeter.","To this end, we first explicitly characterize the completely positive transformations between multimeters.","However, not all of these transformations correspond to valid simulations, as evidenced by the existence of maps that always prepare the same multimeter regardless of the input, which we call trash-and-prepare.","We give a new definition of multimeter simulations as transformations that are triviality-preserving, i.e., when given a multimeter consisting of trivial measurements they can only produce another trivial multimeter.","In the absence of a quantum ancilla, we then characterize the transformations that are triviality-preserving and the transformations that are trash-and-prepare.","Finally, we use these characterizations to compare our new definition of multimeter simulation to three existing ones: classical simulations, compression of multimeters, and compatibility-preserving simulations."],"url":"http://arxiv.org/abs/2402.18333v1","category":"quant-ph"}
{"created":"2024-02-28 13:49:23","title":"Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation","abstract":"The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks. Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like adversarial training, and it is possible to deploy near-real-time models with almost-zero false alarms.","sentences":["The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs.","LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning.","To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs.","Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors.","We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks.","Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like adversarial training, and it is possible to deploy near-real-time models with almost-zero false alarms."],"url":"http://arxiv.org/abs/2402.18329v1","category":"cs.CR"}
{"created":"2024-02-28 13:48:44","title":"When Should Algorithms Resign?","abstract":"This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations. Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems. Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided. We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance. Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement. Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated with AI but also leverages its benefits, ensuring the responsible and effective use of AI systems.","sentences":["This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations.","Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems.","Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided.","We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance.","Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement.","Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated with AI but also leverages its benefits, ensuring the responsible and effective use of AI systems."],"url":"http://arxiv.org/abs/2402.18326v1","category":"cs.CY"}
{"created":"2024-02-28 13:36:27","title":"Privacy Policies and Consent Management Platforms: Growth and Users' Interactions over Time","abstract":"In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising. The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices. Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features.   This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs. Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide. We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection. For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option. Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent. The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision. Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices.","sentences":["In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising.","The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices.","Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features.   ","This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years.","We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs.","Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe.","Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide.","We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection.","For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option.","Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent.","The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision.","Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices."],"url":"http://arxiv.org/abs/2402.18321v2","category":"cs.CY"}
{"created":"2024-02-28 13:33:43","title":"Location-guided Head Pose Estimation for Fisheye Image","abstract":"Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \\textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \\textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \\textcolor{blue}{a} fisheye-\\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.","sentences":["Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection.","Serious fisheye \\textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \\textcolor{blue}{existing} head pose estimation models trained on undistorted images.","This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion.","We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location.","Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration.","We also created \\textcolor{blue}{a} fisheye-\\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments.","Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods."],"url":"http://arxiv.org/abs/2402.18320v1","category":"cs.CV"}
{"created":"2024-02-28 13:08:46","title":"Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis","abstract":"In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road. Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape. The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences. By automating this process, municipalities can address potential road space constraints, enhancing safety for all. They may also save valuable time by carrying out the inspections more systematically. Our open-source code gives communities inspiration on how to automate the process themselves.","sentences":["In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance.","Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants.","Accurately estimating this space from simple images proves challenging due to a lack of depth information.","This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective.","Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving.","These scans, however, also open up possibilities in urban management.","In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed.","Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road.","Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape.","The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences.","By automating this process, municipalities can address potential road space constraints, enhancing safety for all.","They may also save valuable time by carrying out the inspections more systematically.","Our open-source code gives communities inspiration on how to automate the process themselves."],"url":"http://arxiv.org/abs/2402.18309v1","category":"cs.CV"}
{"created":"2024-02-28 12:41:06","title":"Comparative Analysis of XGBoost and Minirocket Algortihms for Human Activity Recognition","abstract":"Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification. This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors. The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone. The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers. Monte Carlo cross-validation is employed to evaluate the models' robustness. The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classification. XGBoost exhibits a slightly superior performance compared to MiniRocket. Notably, both algorithms surpass the performance of other ML and DL algorithms reported in the literature for HAR tasks. Additionally, the study compares the computational efficiency of the two algorithms, revealing XGBoost's advantage in terms of training time. Furthermore, the performance of MiniRocket, which achieves accuracy and F1 values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one channel from the sensors, highlights the potential of directly leveraging unprocessed signals. It also suggests potential advantages that could be gained by utilizing sensor fusion or channel fusion techniques. Overall, this research sheds light on the effectiveness and computational characteristics of XGBoost and MiniRocket in HAR tasks, providing insights for future studies in activity recognition using smartphone sensor data.","sentences":["Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification.","This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors.","The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone.","The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers.","Monte Carlo cross-validation is employed to evaluate the models' robustness.","The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classification.","XGBoost exhibits a slightly superior performance compared to MiniRocket.","Notably, both algorithms surpass the performance of other ML and DL algorithms reported in the literature for HAR tasks.","Additionally, the study compares the computational efficiency of the two algorithms, revealing XGBoost's advantage in terms of training time.","Furthermore, the performance of MiniRocket, which achieves accuracy and F1 values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one channel from the sensors, highlights the potential of directly leveraging unprocessed signals.","It also suggests potential advantages that could be gained by utilizing sensor fusion or channel fusion techniques.","Overall, this research sheds light on the effectiveness and computational characteristics of XGBoost and MiniRocket in HAR tasks, providing insights for future studies in activity recognition using smartphone sensor data."],"url":"http://arxiv.org/abs/2402.18296v1","category":"cs.LG"}
{"created":"2024-02-28 12:37:30","title":"FSL Model can Score Higher as It Is","abstract":"In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.","sentences":["In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised.","Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training.","Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly.","In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation.","An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples.","Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample.","It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample.","Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset.","According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs.","By augmenting both the support set and the queries, we can achieve even more performance improvement.","Our Github Repository is publicly available."],"url":"http://arxiv.org/abs/2402.18292v1","category":"cs.CV"}
{"created":"2024-02-28 12:25:01","title":"Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis","abstract":"In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.","sentences":["In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field.","We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution.","Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization.","We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance.","We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important."],"url":"http://arxiv.org/abs/2402.18286v1","category":"cs.CV"}
{"created":"2024-02-28 12:24:27","title":"PiShield: A NeSy Framework for Learning with Requirements","abstract":"Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.","sentences":["Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs.","In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology.","PiShield guarantees compliance with these requirements, regardless of input.","Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs.","Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains.","Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation."],"url":"http://arxiv.org/abs/2402.18285v1","category":"cs.LG"}
{"created":"2024-02-28 12:24:07","title":"Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization","abstract":"Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.","sentences":["Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback.","However, its training pipeline relies on manual ranking, a resource-intensive process.","To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators.","Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.","We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics.","Subsequently, we construct a reward model to learn the rank and optimize our generative policy.","Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores.","Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans.","This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models."],"url":"http://arxiv.org/abs/2402.18284v1","category":"cs.CL"}
{"created":"2024-02-28 12:04:05","title":"Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?","abstract":"Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.","sentences":["Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs.","In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms.","Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs.","We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt.","Further study reveals the common interaction mechanisms of LLMs during the discussion."],"url":"http://arxiv.org/abs/2402.18272v1","category":"cs.CL"}
{"created":"2024-02-28 12:00:48","title":"Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach","abstract":"This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures. Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information. This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective multi-modal sensing performance. Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation. Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world.","sentences":["This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures.","Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information.","This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective multi-modal sensing performance.","Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation.","Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world."],"url":"http://arxiv.org/abs/2402.18271v1","category":"eess.SP"}
{"created":"2024-02-28 11:57:12","title":"A Survey on Neural Question Generation: Methods, Applications, and Prospects","abstract":"In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG.","sentences":["In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images.","The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications.","It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities.","This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations.","The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths.","Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG."],"url":"http://arxiv.org/abs/2402.18267v1","category":"cs.CL"}
{"created":"2024-02-28 11:39:26","title":"A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames","abstract":"Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.","sentences":["Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent.","This configuration significantly limits the surface form of user utterances and the capacity of output semantics.","In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS.","The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases.","Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network.","Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin."],"url":"http://arxiv.org/abs/2402.18258v1","category":"cs.CL"}
{"created":"2024-02-28 11:32:31","title":"Photon quantum kinetic equations and collective modes in an axion background","abstract":"We develop a quantum kinetic theory for photons in the presence of an axion background and in the collisioness limit. In deriving the classical regime of our quantum kinetic equations, we observe that they capture well known features of axion electrodynamics. By projecting the Wigner function onto a polarization basis, relating the Wigner matrix function with the Stokes parameters, we establish the dispersion relations and transport equations for each polarization space component. Additionally, we investigate how the axion background affects the dispersion relations of photon collective modes within an electron-positron plasma at equilibrium temperature $T$. While the plasmon remains unaffected, we find that the axion background breaks the degeneracy of transverse collective modes at order $e g_{a\\gamma}T(\\partial a)$, where $e$ represents the electron charge, $ g_{a\\gamma}$ denotes the photon-axion coupling, and $\\partial a$ represents the scale associated with variations in the axion field.","sentences":["We develop a quantum kinetic theory for photons in the presence of an axion background and in the collisioness limit.","In deriving the classical regime of our quantum kinetic equations, we observe that they capture well known features of axion electrodynamics.","By projecting the Wigner function onto a polarization basis, relating the Wigner matrix function with the Stokes parameters, we establish the dispersion relations and transport equations for each polarization space component.","Additionally, we investigate how the axion background affects the dispersion relations of photon collective modes within an electron-positron plasma at equilibrium temperature $T$. While the plasmon remains unaffected, we find that the axion background breaks the degeneracy of transverse collective modes at order $e g_{a\\gamma}T(\\partial a)$, where $e$ represents the electron charge, $ g_{a\\gamma}$ denotes the photon-axion coupling, and $\\partial a$ represents the scale associated with variations in the axion field."],"url":"http://arxiv.org/abs/2402.18254v1","category":"hep-ph"}
{"created":"2024-02-28 11:29:09","title":"Towards Generalist Prompting for Large Language Models by Mental Models","abstract":"Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs.","sentences":["Large language models (LLMs) have demonstrated impressive performance on many tasks.","However, to achieve optimal performance, specially designed prompting methods are still needed.","These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks.","In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems.","Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting.","MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings.","We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs."],"url":"http://arxiv.org/abs/2402.18252v1","category":"cs.CL"}
{"created":"2024-02-28 11:28:56","title":"On the Accuracy of Edge Detectors in Number Plate Extraction","abstract":"Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system. This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle. As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection. This paper presents a method of number plate extraction using edge detection technique. Edges in number plates are identified with changes in the intensity of pixel values. Therefore, these edges are identified using a single based pixel or collection of pixel-based approach. The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented. Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric","sentences":["Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system.","This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle.","As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection.","This paper presents a method of number plate extraction using edge detection technique.","Edges in number plates are identified with changes in the intensity of pixel values.","Therefore, these edges are identified using a single based pixel or collection of pixel-based approach.","The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented.","Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric"],"url":"http://arxiv.org/abs/2402.18251v1","category":"cs.CV"}
{"created":"2024-02-28 11:23:08","title":"String Dimension: VC Dimension for Infinite Shattering","abstract":"In computer science, combinatorics, and model theory, the VC dimension is a central notion underlying far-reaching topics such as error rate for decision rules, combinatorial measurements of classes of finite structures, and neo-stability theory. In all cases, it measures the capacity for a collection of sets $\\mathcal{F}\\subseteq\\mathscr{P}(X)$ to shatter subsets of $X$. The VC dimension of this class then takes values in $\\mathbb{N}\\cup\\{\\infty\\}$. We extend this notion to an infinitary framework and use this to generate ideals on $2^\\kappa$ of families of bounded shattering. We explore the cardinals characteristics of ideals generated by this generalised VC dimension, dubbed string dimension, and present various consistency results.   We also introduce the finality of forcing iteration. A $\\kappa$-final iteration is one for which any sequences of ground model elements of length less than $\\kappa$ in the final model must have been introduced at an intermediate stage. This technique is often used for, say, controlling sets of real numbers when manipulating values of cardinal characteristics, and is often exhibited as a consequence of a chain condition. We demonstrate a precise characterisation of such notions of forcing as a generalisation of distributivity.","sentences":["In computer science, combinatorics, and model theory, the VC dimension is a central notion underlying far-reaching topics such as error rate for decision rules, combinatorial measurements of classes of finite structures, and neo-stability theory.","In all cases, it measures the capacity for a collection of sets $\\mathcal{F}\\subseteq\\mathscr{P}(X)$ to shatter subsets of $X$. The VC dimension of this class then takes values in $\\mathbb{N}\\cup\\{\\infty\\}$. We extend this notion to an infinitary framework and use this to generate ideals on $2^\\kappa$ of families of bounded shattering.","We explore the cardinals characteristics of ideals generated by this generalised VC dimension, dubbed string dimension, and present various consistency results.   ","We also introduce the finality of forcing iteration.","A $\\kappa$-final iteration is one for which any sequences of ground model elements of length less than $\\kappa$ in the final model must have been introduced at an intermediate stage.","This technique is often used for, say, controlling sets of real numbers when manipulating values of cardinal characteristics, and is often exhibited as a consequence of a chain condition.","We demonstrate a precise characterisation of such notions of forcing as a generalisation of distributivity."],"url":"http://arxiv.org/abs/2402.18250v1","category":"math.LO"}
{"created":"2024-02-28 11:12:47","title":"Affective State Detection using fNIRs and Machine Learning","abstract":"Affective states regulate our day to day to function and has a tremendous effect on mental and physical health. Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management. In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy. We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning. A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave one out cross validation. It was found that prediction accuracy for cognitive load was higher (evoked using a pen and paper task) than the other two classes (evoked using computer bases tasks). To verify that this discrepancy was not due to motor skills involved in the pen and paper task, a second experiment was conducted using four participants and the results of that experiment has also been presented in the paper.","sentences":["Affective states regulate our day to day to function and has a tremendous effect on mental and physical health.","Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management.","In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy.","We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning.","A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave one out cross validation.","It was found that prediction accuracy for cognitive load was higher (evoked using a pen and paper task) than the other two classes (evoked using computer bases tasks).","To verify that this discrepancy was not due to motor skills involved in the pen and paper task, a second experiment was conducted using four participants and the results of that experiment has also been presented in the paper."],"url":"http://arxiv.org/abs/2402.18241v1","category":"cs.HC"}
{"created":"2024-02-28 11:00:38","title":"On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks","abstract":"Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.","sentences":["Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM).","These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups.","Until recently, the main focus of research was the structural characteristics of this model.","The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion.","A total of 26 datasets were collected for this work.","It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion.","However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures.","These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses.","Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships."],"url":"http://arxiv.org/abs/2402.18235v1","category":"cs.SI"}
{"created":"2024-02-28 10:43:54","title":"CogBench: a large language model walks into a psychology lab","abstract":"Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.","sentences":["Large language models (LLMs) have significantly advanced the field of artificial intelligence.","Yet, evaluating them comprehensively remains challenging.","We argue that this is partly due to the predominant focus on performance metrics in most benchmarks.","This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments.","This novel approach offers a toolkit for phenotyping LLMs' behavior.","We apply CogBench to 35 LLMs, yielding a rich and diverse dataset.","We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs.","Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior.","Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.","Finally, we explore the effects of prompt-engineering techniques.","We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors."],"url":"http://arxiv.org/abs/2402.18225v1","category":"cs.CL"}
{"created":"2024-02-28 18:51:10","title":"Lossy anharmonic polaritons under periodic driving","abstract":"We report on the anharmonic signatures in dissipative polaritons' stationary energy distribution and thermodynamics under external periodic driving. First, we introduce a dynamic model for the dissipative anharmonic Jaynes-Cummings polariton with a generic time-periodic interaction representing modulations of the polariton's energy due to an external force or field. We characterize the stationary state in terms of the exciton, phonon, and interaction energy dependence on the phonon anharmonicity, exciton-phonon coupling strength, and intensity and form of the external field-polariton coupling. Our model also captures the quantum thermodynamics of the driven polariton, which we analyze in connection with the irreversible heat, maximum power, and efficiency of the process. We find considerable differences in energy distribution and thermodynamics between harmonic, moderate, and strongly anharmonic polaritons. Moreover, comparing the external modulations to the phonon and exciton energy, we conclude that the former enhances the polariton's energy storage capacity and is occasionally limited by interference effects and energy saturation at the exciton.","sentences":["We report on the anharmonic signatures in dissipative polaritons' stationary energy distribution and thermodynamics under external periodic driving.","First, we introduce a dynamic model for the dissipative anharmonic Jaynes-Cummings polariton with a generic time-periodic interaction representing modulations of the polariton's energy due to an external force or field.","We characterize the stationary state in terms of the exciton, phonon, and interaction energy dependence on the phonon anharmonicity, exciton-phonon coupling strength, and intensity and form of the external field-polariton coupling.","Our model also captures the quantum thermodynamics of the driven polariton, which we analyze in connection with the irreversible heat, maximum power, and efficiency of the process.","We find considerable differences in energy distribution and thermodynamics between harmonic, moderate, and strongly anharmonic polaritons.","Moreover, comparing the external modulations to the phonon and exciton energy, we conclude that the former enhances the polariton's energy storage capacity and is occasionally limited by interference effects and energy saturation at the exciton."],"url":"http://arxiv.org/abs/2402.18560v1","category":"quant-ph"}
{"created":"2024-02-28 18:06:45","title":"Mental Models of Meeting Goals: Supporting Intentionality in Meeting Technologies","abstract":"Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies. To design for intentionality, we need to understand workers' attitudes and practices around goals. We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves. We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees. We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings. Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings.","sentences":["Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies.","To design for intentionality, we need to understand workers' attitudes and practices around goals.","We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves.","We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees.","We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings.","Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings."],"url":"http://arxiv.org/abs/2402.18526v1","category":"cs.HC"}
{"created":"2024-02-28 17:47:27","title":"Model Predictive Control with adaptive resilience for Denial-of-Service Attacks mitigation on a Regulated Dam","abstract":"In recent years, SCADA (Supervisory Control and Data Acquisition) systems have increasingly become the target of cyber attacks. SCADAs are no longer isolated, as web-based applications expose strategic infrastructures to the outside world connection. In a cyber-warfare context, we propose a Model Predictive Control (MPC) architecture with adaptive resilience, capable of guaranteeing control performance in normal operating conditions and driving towards resilience against DoS (controller-actuator) attacks when needed. Since the attackers' goal is typically to maximize the system damage, we assume they solve an adversarial optimal control problem. An adaptive resilience factor is then designed as a function of the intensity function of a Hawkes process, a point process model estimating the occurrence of random events in time, trained on a moving window to estimate the return time of the next attack. We demonstrate the resulting MPC strategy's effectiveness in 2 attack scenarios on a real system with actual data, the regulated Olginate dam of Lake Como.","sentences":["In recent years, SCADA (Supervisory Control and Data Acquisition) systems have increasingly become the target of cyber attacks.","SCADAs are no longer isolated, as web-based applications expose strategic infrastructures to the outside world connection.","In a cyber-warfare context, we propose a Model Predictive Control (MPC) architecture with adaptive resilience, capable of guaranteeing control performance in normal operating conditions and driving towards resilience against DoS (controller-actuator) attacks when needed.","Since the attackers' goal is typically to maximize the system damage, we assume they solve an adversarial optimal control problem.","An adaptive resilience factor is then designed as a function of the intensity function of a Hawkes process, a point process model estimating the occurrence of random events in time, trained on a moving window to estimate the return time of the next attack.","We demonstrate the resulting MPC strategy's effectiveness in 2 attack scenarios on a real system with actual data, the regulated Olginate dam of Lake Como."],"url":"http://arxiv.org/abs/2402.18516v1","category":"eess.SY"}
{"created":"2024-02-28 17:36:45","title":"Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling","abstract":"In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences. We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality. Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.","sentences":["In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical.","This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism.","Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning.","At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network.","We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation.","The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences.","We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality.","Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers.","This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling."],"url":"http://arxiv.org/abs/2402.18508v1","category":"cs.LG"}
{"created":"2024-02-28 17:34:41","title":"Optimality conditions for sparse optimal control of viscous Cahn-Hilliard systems with logarithmic potential","abstract":"In this paper we study the optimal control of a parabolic initial-boundary value problem of viscous Cahn-Hilliard type with zero Neumann boundary conditions. Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter. It is assumed that the nonlinear functions driving the physical processes within the spatial domain are double-well potentials of logarithmic type whose derivatives become singular at the boundary of their respective domains of definition. For such systems, optimal control problems have been studied in the past. We focus here on the situation when the cost functional of the optimal control problem contains a nondifferentiable term like the L1-norm, which leads to sparsity of optimal controls. For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls. In the approach to second-order sufficient conditions, the main novelty of this paper, we adapt a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper [SIAM J. Control Optim. 53 (2015), 2168-2202]. In this paper, we show that this method can also be successfully applied to systems of viscous Cahn-Hilliard type with logarithmic nonlinearity. Since the Cahn-Hilliard system corresponds to a fourth-order partial differential equation in contrast to the second-order systems investigated before, additional technical difficulties have to be overcome.","sentences":["In this paper we study the optimal control of a parabolic initial-boundary value problem of viscous Cahn-Hilliard type with zero Neumann boundary conditions.","Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter.","It is assumed that the nonlinear functions driving the physical processes within the spatial domain are double-well potentials of logarithmic type whose derivatives become singular at the boundary of their respective domains of definition.","For such systems, optimal control problems have been studied in the past.","We focus here on the situation when the cost functional of the optimal control problem contains a nondifferentiable term like the L1-norm, which leads to sparsity of optimal controls.","For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls.","In the approach to second-order sufficient conditions, the main novelty of this paper, we adapt a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper","[SIAM J. Control Optim.","53 (2015), 2168-2202].","In this paper, we show that this method can also be successfully applied to systems of viscous Cahn-Hilliard type with logarithmic nonlinearity.","Since the Cahn-Hilliard system corresponds to a fourth-order partial differential equation in contrast to the second-order systems investigated before, additional technical difficulties have to be overcome."],"url":"http://arxiv.org/abs/2402.18506v1","category":"math.OC"}
{"created":"2024-02-28 17:31:39","title":"Detection of Micromobility Vehicles in Urban Traffic Videos","abstract":"Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects. Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.","sentences":["Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes.","To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks.","This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture.","This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability.","Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects.","Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur."],"url":"http://arxiv.org/abs/2402.18503v1","category":"cs.CV"}
{"created":"2024-02-28 17:18:38","title":"TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding","abstract":"The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \\url{https://alanzhangcs.github.io/tamm-page}.","sentences":["The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes.","However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods.","This is attributed to the domain shift in the 2D images and the distinct focus of each modality.","To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters.","First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs.","Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training.","Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks.","Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0.","Project page: \\url{https://alanzhangcs.github.io/tamm-page}."],"url":"http://arxiv.org/abs/2402.18490v1","category":"cs.CV"}
{"created":"2024-02-28 16:57:22","title":"IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding","abstract":"Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.","sentences":["Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations.","An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations.","In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique.","Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text.","We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions.","Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response."],"url":"http://arxiv.org/abs/2402.18476v1","category":"cs.CV"}
{"created":"2024-02-28 16:41:52","title":"Semantic Information in MC: Chemotaxis Beyond Shannon","abstract":"The recently emerging molecular communication (MC) paradigm intents to leverage communication engineering tools for the design of synthetic chemical communication systems. These systems are envisioned to operate on nanoscale and in biological environments, such as the human body, and catalyze the emergence of revolutionary applications in the context of early disease monitoring and drug targeting. However, while a plethora of theoretical (and more recently also more and more practical) MC system designs have been proposed over the past years, some fundamental questions remain open, hindering the breakthrough of MC in real-world applications. One of these questions is: What is a useful measure of information in the context of MC-based applications? While most existing works in MC build upon the concept of syntactic information as introduced by Shannon, in this paper, we explore the framework of semantic information as introduced by Kolchinsky and Wolpert for the information theoretical analysis of a natural MC system, namely bacterial chemotaxis. Exploiting the computational modeling tool of agent-based modeling (ABM), we are able to demonstrate that the semantic information framework can provide a useful information theoretical framework for quantifying the information exchange of chemotactic bacteria with their environment. In particular, we show that the measured semantic information provides a useful measure of the ability of the bacteria to adapt to and survive in a changing environment. Encouraged by our results, we envision that the semantic information framework can open new avenues for developing theoretical and practical MC system designs and in this way help to unleash the full potential of MC for complex adaptive systems-based nanoscale applications.","sentences":["The recently emerging molecular communication (MC) paradigm intents to leverage communication engineering tools for the design of synthetic chemical communication systems.","These systems are envisioned to operate on nanoscale and in biological environments, such as the human body, and catalyze the emergence of revolutionary applications in the context of early disease monitoring and drug targeting.","However, while a plethora of theoretical (and more recently also more and more practical) MC system designs have been proposed over the past years, some fundamental questions remain open, hindering the breakthrough of MC in real-world applications.","One of these questions is: What is a useful measure of information in the context of MC-based applications?","While most existing works in MC build upon the concept of syntactic information as introduced by Shannon, in this paper, we explore the framework of semantic information as introduced by Kolchinsky and Wolpert for the information theoretical analysis of a natural MC system, namely bacterial chemotaxis.","Exploiting the computational modeling tool of agent-based modeling (ABM), we are able to demonstrate that the semantic information framework can provide a useful information theoretical framework for quantifying the information exchange of chemotactic bacteria with their environment.","In particular, we show that the measured semantic information provides a useful measure of the ability of the bacteria to adapt to and survive in a changing environment.","Encouraged by our results, we envision that the semantic information framework can open new avenues for developing theoretical and practical MC system designs and in this way help to unleash the full potential of MC for complex adaptive systems-based nanoscale applications."],"url":"http://arxiv.org/abs/2402.18465v1","category":"cs.IT"}
{"created":"2024-02-28 16:39:40","title":"Transition from weak turbulence to collapse turbulence regimes in MMT model","abstract":"It is well known that wave collapses can emerge from the focusing one-dimensional (1-D) Majda-McLaughlin-Tabak (MMT) model as a result of modulational instability. However, how these wave collapses affect the spectral properties and statistics of the wave field has not been adequately studied. We undertake this task by simulating the forced-dissipated 1-D MMT model over a range of forcing amplitudes. Our results show that when the forcing is weak, the spectrum agrees well with the prediction by wave turbulence theory with few collapses in the field. As the forcing strength increases, we see an increase in the occurrence of collapses, together with a transition from a power-law spectrum to an exponentially decaying spectrum. Through a spectral decomposition, we find that the exponential spectrum is dominated by the wave collapse component in the non-integrable MMT model, which is in analogy to a soliton gas in integrable turbulence.","sentences":["It is well known that wave collapses can emerge from the focusing one-dimensional (1-D) Majda-McLaughlin-Tabak (MMT) model as a result of modulational instability.","However, how these wave collapses affect the spectral properties and statistics of the wave field has not been adequately studied.","We undertake this task by simulating the forced-dissipated 1-D MMT model over a range of forcing amplitudes.","Our results show that when the forcing is weak, the spectrum agrees well with the prediction by wave turbulence theory with few collapses in the field.","As the forcing strength increases, we see an increase in the occurrence of collapses, together with a transition from a power-law spectrum to an exponentially decaying spectrum.","Through a spectral decomposition, we find that the exponential spectrum is dominated by the wave collapse component in the non-integrable MMT model, which is in analogy to a soliton gas in integrable turbulence."],"url":"http://arxiv.org/abs/2402.18464v1","category":"nlin.PS"}
{"created":"2024-02-28 16:24:08","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation","abstract":"The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at https://github.com/ayanglab/MambaMIR.","sentences":["The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks.","This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN.","Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model.","The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation.","Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods.","Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality.","The code is publicly available at https://github.com/ayanglab/MambaMIR."],"url":"http://arxiv.org/abs/2402.18451v1","category":"eess.IV"}
{"created":"2024-02-28 16:16:51","title":"Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization","abstract":"Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.","sentences":["Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains.","Existing works primarily focus on improving the generalization ability of static networks.","However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability.","Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios.","In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity.","Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts.","Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability.","Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection.","The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method."],"url":"http://arxiv.org/abs/2402.18447v1","category":"cs.CV"}
{"created":"2024-02-28 16:02:10","title":"Magnetization fluctuations and magnetic aftereffect probed via the anomalous Hall effect","abstract":"Taking advantage of the anomalous Hall effect, we electrically probe low-frequency magnetization fluctuations at room temperature in a thin ferromagnetic Pt/Co/AlO$_x$ layer stack with perpendicular magnetic anisotropy. We observe a strong enhancement of the Hall voltage fluctuations within the hysteretic region of the magnetization loop. Analyzing both the temporal evolution of the anomalous Hall voltage and its frequency-dependent noise power density, we identify two types of magnetic noise: abrupt changes in the magnetic domain configuration, evident as Barkhausen-like steps in the Hall voltage time trace, yield a noise power density spectrum scaling with frequency as $1/f^{\\beta}$ with $\\beta\\approx 1.9$. In contrast, quasi-stationary magnetization configurations are connected with a magnetic noise power density with an exponent $\\beta\\approx 0.9$. The observation of Barkausen steps and relaxation effects shows that the magnetic system is in a non-stationary state in the hysteresis region, such that the fluctuation-dissipation theorem cannot be expected to hold. However, the time-dependent change in the Hall voltage for constant magnetic field strength resembles the integrated noise power.","sentences":["Taking advantage of the anomalous Hall effect, we electrically probe low-frequency magnetization fluctuations at room temperature in a thin ferromagnetic Pt/Co/AlO$_x$ layer stack with perpendicular magnetic anisotropy.","We observe a strong enhancement of the Hall voltage fluctuations within the hysteretic region of the magnetization loop.","Analyzing both the temporal evolution of the anomalous Hall voltage and its frequency-dependent noise power density, we identify two types of magnetic noise: abrupt changes in the magnetic domain configuration, evident as Barkhausen-like steps in the Hall voltage time trace, yield a noise power density spectrum scaling with frequency as $1/f^{\\beta}$ with $\\beta\\approx 1.9$.","In contrast, quasi-stationary magnetization configurations are connected with a magnetic noise power density with an exponent $\\beta\\approx 0.9$. The observation of Barkausen steps and relaxation effects shows that the magnetic system is in a non-stationary state in the hysteresis region, such that the fluctuation-dissipation theorem cannot be expected to hold.","However, the time-dependent change in the Hall voltage for constant magnetic field strength resembles the integrated noise power."],"url":"http://arxiv.org/abs/2402.18436v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 15:24:58","title":"A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation","abstract":"In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.","sentences":["In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation.","In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies.","All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same).","In this paper, we propose SyMPIE to solve these shortcomings.","To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost.","Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples.","Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels.","We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board.","The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication."],"url":"http://arxiv.org/abs/2402.18402v2","category":"cs.CV"}
{"created":"2024-02-28 15:04:44","title":"Robust Quantification of Percent Emphysema on CT via Domain Attention: the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study","abstract":"Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans. Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density. Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study. To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework. We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors. We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results.","sentences":["Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans.","Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density.","Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study.","To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework.","We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors.","We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results."],"url":"http://arxiv.org/abs/2402.18383v1","category":"cs.CV"}
{"created":"2024-02-28 14:10:35","title":"Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks","abstract":"Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals. Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities. Additionally, they often rely on engineered heuristics for assigning entities among agents. We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in simulation. Leveraging permutation invariant neural network architectures and model-free reinforcement learning, our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity. We validate our approach through both simulations and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities. We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems. Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems. The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a zero-shot manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors.","sentences":["Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals.","Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities.","Additionally, they often rely on engineered heuristics for assigning entities among agents.","We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in simulation.","Leveraging permutation invariant neural network architectures and model-free reinforcement learning, our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity.","We validate our approach through both simulations and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities.","We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems.","Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems.","The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a zero-shot manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors."],"url":"http://arxiv.org/abs/2402.18345v1","category":"cs.RO"}
{"created":"2024-02-28 13:54:57","title":"Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation","abstract":"We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.","sentences":["We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning.","Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data.","We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates.","The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response.","We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models.","We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline.","For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points.","We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators.","Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains.","The model, dataset, and code are available at https://github.com/BatsResearch/bonito."],"url":"http://arxiv.org/abs/2402.18334v1","category":"cs.CL"}
{"created":"2024-02-28 13:42:12","title":"Robotising Psychometrics: Validating Wellbeing Assessment Tools in Child-Robot Interactions","abstract":"The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains. However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation. In this work, we conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing. Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI. Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings. Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI. Findings from this work highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations.","sentences":["The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains.","However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation.","In this work, we conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing.","Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI.","Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings.","Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI.","Findings from this work highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations."],"url":"http://arxiv.org/abs/2402.18325v1","category":"cs.HC"}
{"created":"2024-02-28 13:22:42","title":"Rare events in a stochastic vegetation-water dynamical system based on machine learning","abstract":"Stochastic vegetation-water dynamical systems play a pivotal role in ecological stability, biodiversity, water resource management, and adaptation to climate change. This research proposes a machine learning-based method for analyzing rare events in stochastic vegetation-water dynamical systems with multiplicative Gaussian noise. Utilizing the Freidlin-Wentzell large deviation theory, we derive the asymptotic expressions for the quasipotential and the mean first exit time. Based on the decomposition of vector field, we design a neural network architecture to compute the most probable transition paths and the mean first exit time for both non-characteristic and characteristic boundary scenarios. The results indicate that this method can effectively predict early warnings of vegetation degradation, providing new theoretical foundations and mathematical tools for ecological management and conservation. Moreover, the method offers new possibilities for exploring more complex and higher-dimensional stochastic dynamical systems.","sentences":["Stochastic vegetation-water dynamical systems play a pivotal role in ecological stability, biodiversity, water resource management, and adaptation to climate change.","This research proposes a machine learning-based method for analyzing rare events in stochastic vegetation-water dynamical systems with multiplicative Gaussian noise.","Utilizing the Freidlin-Wentzell large deviation theory, we derive the asymptotic expressions for the quasipotential and the mean first exit time.","Based on the decomposition of vector field, we design a neural network architecture to compute the most probable transition paths and the mean first exit time for both non-characteristic and characteristic boundary scenarios.","The results indicate that this method can effectively predict early warnings of vegetation degradation, providing new theoretical foundations and mathematical tools for ecological management and conservation.","Moreover, the method offers new possibilities for exploring more complex and higher-dimensional stochastic dynamical systems."],"url":"http://arxiv.org/abs/2402.18315v1","category":"math.DS"}
{"created":"2024-02-28 13:07:16","title":"Feature Denoising For Low-Light Instance Segmentation Using Weighted Non-Local Blocks","abstract":"Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0.","sentences":["Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast.","In this paper, we propose an end-to-end solution to address this challenging task.","Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor.","This integration enables an inherent denoising process at the feature level.","As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets.","We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   ","Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0."],"url":"http://arxiv.org/abs/2402.18307v1","category":"cs.CV"}
{"created":"2024-02-28 12:17:40","title":"Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient","abstract":"Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL.","sentences":["Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach.","However, the reasons behind its remarkable effectiveness remain unclear.","Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg).","However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly.","Therefore, two questions arise:","First, what commonalities enable various contrastive losses to achieve superior performance in SRL?","Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective?","To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio.","Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance.","Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL."],"url":"http://arxiv.org/abs/2402.18281v1","category":"cs.CL"}
{"created":"2024-02-28 11:47:15","title":"Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning","abstract":"Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example.","sentences":["Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space.","Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose.","In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed.","This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles.","We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP.","Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed.","The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example."],"url":"http://arxiv.org/abs/2402.18260v1","category":"cs.LG"}
{"created":"2024-02-28 11:45:35","title":"Phononic Crystals in Superfluid Thin-Film Helium","abstract":"In recent years, nanomechanical oscillators in thin films of superfluid helium have attracted attention in the field of optomechanics due to their exceptionally low mechanical dissipation and optical scattering. Mechanical excitations in superfluid thin films - so-called third sound waves - can interact with the optical mode of an optical microresonator by modulation of its effective refractive index enabling optomechanical coupling. Strong confinement of third sound modes enhances their intrinsic mechanical non-linearity paving the way for strong phonon-phonon interactions with applications in quantum optomechanics. Here, we realize a phononic crystal cavity confining third sound modes in a superfluid helium film to length scales close to the third sound wavelength. A few nanometer thick superfluid film is self-assembled on top of a silicon nanobeam optical resonator. The periodic patterning of the silicon material creates a periodic modulation of the superfluid film leading to the formation of a phononic band gap. By engineering the geometry of the silicon nanobeam, the phononic band gap allows the confinement of a localized phononic mode.","sentences":["In recent years, nanomechanical oscillators in thin films of superfluid helium have attracted attention in the field of optomechanics due to their exceptionally low mechanical dissipation and optical scattering.","Mechanical excitations in superfluid thin films - so-called third sound waves - can interact with the optical mode of an optical microresonator by modulation of its effective refractive index enabling optomechanical coupling.","Strong confinement of third sound modes enhances their intrinsic mechanical non-linearity paving the way for strong phonon-phonon interactions with applications in quantum optomechanics.","Here, we realize a phononic crystal cavity confining third sound modes in a superfluid helium film to length scales close to the third sound wavelength.","A few nanometer thick superfluid film is self-assembled on top of a silicon nanobeam optical resonator.","The periodic patterning of the silicon material creates a periodic modulation of the superfluid film leading to the formation of a phononic band gap.","By engineering the geometry of the silicon nanobeam, the phononic band gap allows the confinement of a localized phononic mode."],"url":"http://arxiv.org/abs/2402.18259v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 11:12:17","title":"Prospect Personalized Recommendation on Large Language Model-based Agent Platform","abstract":"The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.","sentences":["The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity.","In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender.","Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop.","Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user.","A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application.","Lastly, we discuss potential issues and promising directions for future research."],"url":"http://arxiv.org/abs/2402.18240v1","category":"cs.IR"}
{"created":"2024-02-28 10:38:21","title":"Improving Open-Ended Text Generation via Adaptive Decoding","abstract":"Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.com/zwhong714/adaptive_decoding.","sentences":["Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality.","This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically.","Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process.","The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively.","The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms.","The code is available at https://github.com/zwhong714/adaptive_decoding."],"url":"http://arxiv.org/abs/2402.18223v1","category":"cs.CL"}
{"created":"2024-02-28 10:24:36","title":"Region-Aware Exposure Consistency Network for Mixed Exposure Correction","abstract":"Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet.","sentences":["Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects.","Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions.","The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process.","In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations.","Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space.","Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information.","To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity.","Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method.","The code is released at: https://github.com/kravrolens/RECNet."],"url":"http://arxiv.org/abs/2402.18217v1","category":"cs.CV"}
{"created":"2024-02-28 09:46:56","title":"Learning Invariant Inter-pixel Correlations for Superpixel Generation","abstract":"Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel.","sentences":["Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones.","Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset.","Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios.","To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise.","Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations.","Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations.","Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles.","The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency.","Code and pre-trained model are available at https://github.com/rookiie/CDSpixel."],"url":"http://arxiv.org/abs/2402.18201v1","category":"cs.CV"}
{"created":"2024-02-28 09:20:55","title":"Quantile Outcome Adaptive Lasso: Covariate Selection for Inverse Probability Weighting Estimator of Quantile Treatment Effects","abstract":"When using the propensity score method to estimate the treatment effects, it is important to select the covariates to be included in the propensity score model. The inclusion of covariates unrelated to the outcome in the propensity score model led to bias and large variance in the estimator of treatment effects. Many data-driven covariate selection methods have been proposed for selecting covariates related to outcomes. However, most of them assume an average treatment effect estimation and may not be designed to estimate quantile treatment effects (QTE), which is the effect of treatment on the quantiles of outcome distribution. In QTE estimation, we consider two relation types with the outcome as the expected value and quantile point. To achieve this, we propose a data-driven covariate selection method for propensity score models that allows for the selection of covariates related to the expected value and quantile of the outcome for QTE estimation. Assuming the quantile regression model as an outcome regression model, covariate selection was performed using a regularization method with the partial regression coefficients of the quantile regression model as weights. The proposed method was applied to artificial data and a dataset of mothers and children born in King County, Washington, to compare the performance of existing methods and QTE estimators. As a result, the proposed method performs well in the presence of covariates related to both the expected value and quantile of the outcome.","sentences":["When using the propensity score method to estimate the treatment effects, it is important to select the covariates to be included in the propensity score model.","The inclusion of covariates unrelated to the outcome in the propensity score model led to bias and large variance in the estimator of treatment effects.","Many data-driven covariate selection methods have been proposed for selecting covariates related to outcomes.","However, most of them assume an average treatment effect estimation and may not be designed to estimate quantile treatment effects (QTE), which is the effect of treatment on the quantiles of outcome distribution.","In QTE estimation, we consider two relation types with the outcome as the expected value and quantile point.","To achieve this, we propose a data-driven covariate selection method for propensity score models that allows for the selection of covariates related to the expected value and quantile of the outcome for QTE estimation.","Assuming the quantile regression model as an outcome regression model, covariate selection was performed using a regularization method with the partial regression coefficients of the quantile regression model as weights.","The proposed method was applied to artificial data and a dataset of mothers and children born in King County, Washington, to compare the performance of existing methods and QTE estimators.","As a result, the proposed method performs well in the presence of covariates related to both the expected value and quantile of the outcome."],"url":"http://arxiv.org/abs/2402.18185v1","category":"stat.ME"}
{"created":"2024-02-28 09:12:01","title":"CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation","abstract":"Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.","sentences":["Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching.","While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues.","In this work, we introduce a framework based on contrastive feature distillation (CFD).","This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features.","This framework helps to enhance model generalization across both clean and foggy environments.","Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method."],"url":"http://arxiv.org/abs/2402.18181v2","category":"cs.CV"}
{"created":"2024-02-28 08:55:20","title":"Sequence-level Semantic Representation Fusion for Recommender Systems","abstract":"With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance. Specially, we focus on the utilization of the associated \\emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation. However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective. To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul \\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation, namely \\textbf{\\our}. The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts. The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency domain}. In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features. Our fusion approach can be proved to have the same effects of contextual convolution, so as to achieving sequence-level semantic fusion. In order to further improve the fusion performance, we propose to enhance the discriminability of the text embeddings from the text encoder, by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method. Our implementation is available at this repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}.","sentences":["With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance.","Specially, we focus on the utilization of the associated \\emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation.","However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective.","To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul \\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation, namely \\textbf{\\our}.","The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts.","The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency domain}.","In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features.","Our fusion approach can be proved to have the same effects of contextual convolution, so as to achieving sequence-level semantic fusion.","In order to further improve the fusion performance, we propose to enhance the discriminability of the text embeddings from the text encoder, by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method.","Our implementation is available at this repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}."],"url":"http://arxiv.org/abs/2402.18166v1","category":"cs.IR"}
{"created":"2024-02-28 08:34:23","title":"Diffusion-based Neural Network Weights Generation","abstract":"Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance.","sentences":["Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks.","While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets.","Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task.","To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling.","Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets.","By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance."],"url":"http://arxiv.org/abs/2402.18153v1","category":"cs.LG"}
{"created":"2024-02-28 07:37:26","title":"Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization","abstract":"Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: https://github.com/Alexiland/MLOMAE","sentences":["Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning.","It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones.","A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask.","To overcome this, some approaches propose masking based on patch informativeness.","However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks.","In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining.","Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning.","Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency.","Our code is available at: https://github.com/Alexiland/MLOMAE"],"url":"http://arxiv.org/abs/2402.18128v1","category":"cs.CV"}
{"created":"2024-02-28 07:22:13","title":"Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian","abstract":"This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.","sentences":["This study assesses four cutting-edge language models in the underexplored Aminoacian language.","Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding.","Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps.","By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology."],"url":"http://arxiv.org/abs/2402.18121v1","category":"cs.CL"}
{"created":"2024-02-28 06:40:57","title":"Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models","abstract":"Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited.","sentences":["Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged.","It has been proven effective in resolving hallucination and out-of-date issues in LLMs.","As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable.","In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts.","Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge.","Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing.","It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs.","These adapters are assigned scaling values based on the corresponding specific knowledge.","To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics.","Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited."],"url":"http://arxiv.org/abs/2402.18099v1","category":"cs.CL"}
{"created":"2024-02-28 06:20:30","title":"Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera Force Control: Cooperative vs. Teleoperation Strategies","abstract":"Performing intricate eye microsurgery, such as retinal vein cannulation (RVC), as a potential treatment for retinal vein occlusion (RVO), without the assistance of a surgical robotic system is very challenging to do safely. The main limitation has to do with the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not address crucial safety considerations, such as minimizing contact force between the surgical instrument and the sclera surface to prevent tissue damage. An adaptive sclera force control algorithm was proposed to address this limitation using an FBG-based force-sensing tool to measure and minimize the tool-sclera interaction force. Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. We implemented a bimanual adaptive teleoperation (BMAT) control mode using SHER 2.0 and SHER 2.1 and compared its performance with a bimanual adaptive cooperative (BMAC) mode. Both BMAT and BMAC modes were tested in sitting and standing postures during a vessel-following experiment under a surgical microscope. It is shown, for the first time to the best of our knowledge in robot-assisted retinal surgery, that integrating the adaptive sclera force control algorithm with the bimanual teleoperation framework enables surgeons to safely perform bimanual telemanipulation of the eye without over-stretching it, even in the absence of registration between the two robots.","sentences":["Performing intricate eye microsurgery, such as retinal vein cannulation (RVC), as a potential treatment for retinal vein occlusion (RVO), without the assistance of a surgical robotic system is very challenging to do safely.","The main limitation has to do with the physiological hand tremor of surgeons.","Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC.","The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively.","However, the admittance-based cooperative control mode does not address crucial safety considerations, such as minimizing contact force between the surgical instrument and the sclera surface to prevent tissue damage.","An adaptive sclera force control algorithm was proposed to address this limitation using an FBG-based force-sensing tool to measure and minimize the tool-sclera interaction force.","Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework.","We implemented a bimanual adaptive teleoperation (BMAT) control mode using SHER 2.0 and SHER 2.1 and compared its performance with a bimanual adaptive cooperative (BMAC) mode.","Both BMAT and BMAC modes were tested in sitting and standing postures during a vessel-following experiment under a surgical microscope.","It is shown, for the first time to the best of our knowledge in robot-assisted retinal surgery, that integrating the adaptive sclera force control algorithm with the bimanual teleoperation framework enables surgeons to safely perform bimanual telemanipulation of the eye without over-stretching it, even in the absence of registration between the two robots."],"url":"http://arxiv.org/abs/2402.18088v1","category":"cs.RO"}
{"created":"2024-02-28 04:43:46","title":"Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore","abstract":"Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.","sentences":["Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge.","While extensive research has addressed this in English, little is known about multilingual LLMs.","This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions.","We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages.","Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts.","Furthermore, multilingual models demonstrate a bias towards factual information from Western continents.","These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation."],"url":"http://arxiv.org/abs/2402.18045v1","category":"cs.CL"}
{"created":"2024-02-28 04:33:20","title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","abstract":"As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .","sentences":["As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs).","However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model.","To address this, we propose ResLoRA, an improved framework of LoRA.","By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA.","The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method.","To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA.","The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora ."],"url":"http://arxiv.org/abs/2402.18039v1","category":"cs.CL"}
{"created":"2024-02-28 03:51:02","title":"OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine","abstract":"The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.","sentences":["The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas.","However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages.","It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data.","The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner.","In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models.","It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data.","Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks.","We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab."],"url":"http://arxiv.org/abs/2402.18028v1","category":"cs.CV"}
{"created":"2024-02-28 03:38:54","title":"Synchronization of Complex Dynamical Networks via Event-Triggered Pinning Impulses","abstract":"This article studies the synchronization problem of complex dynamical networks. The impulsive control method is considered with a novel event-triggered pinning algorithm. Sufficient conditions on the network topology are obtained to ensure network synchronization. It is shown that synchronization can be realized with a careful selection of the pinning nodes. Furthermore, an adaptive coupling strength is incorporated into the network to allow network synchronization with an arbitrary selection of the pinning nodes. An example of a network with node dynamics described by the Chen system is studied to demonstrate the theoretical results.","sentences":["This article studies the synchronization problem of complex dynamical networks.","The impulsive control method is considered with a novel event-triggered pinning algorithm.","Sufficient conditions on the network topology are obtained to ensure network synchronization.","It is shown that synchronization can be realized with a careful selection of the pinning nodes.","Furthermore, an adaptive coupling strength is incorporated into the network to allow network synchronization with an arbitrary selection of the pinning nodes.","An example of a network with node dynamics described by the Chen system is studied to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2402.18024v1","category":"math.OC"}
{"created":"2024-02-28 03:16:44","title":"A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems","abstract":"This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.","sentences":["This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs).","This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems."],"url":"http://arxiv.org/abs/2402.18013v1","category":"cs.CL"}
{"created":"2024-02-28 01:50:31","title":"Emergence of Large-Scale Structures in Holographic Superfluid Turbulence","abstract":"In two-dimensional turbulence systems, the emergence of large-scale structures holds profound physical implications, particularly as it indicates the occurrence of inverse energy cascades, thereby garnering significant attention. In this paper, we report a novel vortex clusters formation in the background of near-extreme Reissner-Nordstr$\\ddot{o}$m black hole holographic model. At temperatures nearing absolute zero, we observe not only the formation of vortex clusters but also the emergence of an inverse energy cascade. Distinct from typical quantum systems, the genesis of holographic vortex clusters is rooted in unique quantum dissipation properties, characterized by the near immobilization of vortex dipoles at low temperatures. Through a comparative analysis with the dynamics of the Gross-Pitaevskii equation, our investigation enhances the understanding of inverse energy cascades under these extreme conditions, thereby broadening our comprehension of quantum turbulence.","sentences":["In two-dimensional turbulence systems, the emergence of large-scale structures holds profound physical implications, particularly as it indicates the occurrence of inverse energy cascades, thereby garnering significant attention.","In this paper, we report a novel vortex clusters formation in the background of near-extreme Reissner-Nordstr$\\ddot{o}$m black hole holographic model.","At temperatures nearing absolute zero, we observe not only the formation of vortex clusters but also the emergence of an inverse energy cascade.","Distinct from typical quantum systems, the genesis of holographic vortex clusters is rooted in unique quantum dissipation properties, characterized by the near immobilization of vortex dipoles at low temperatures.","Through a comparative analysis with the dynamics of the Gross-Pitaevskii equation, our investigation enhances the understanding of inverse energy cascades under these extreme conditions, thereby broadening our comprehension of quantum turbulence."],"url":"http://arxiv.org/abs/2402.17980v1","category":"hep-th"}
{"created":"2024-02-28 01:42:31","title":"Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks","abstract":"Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. These attack methods have garnered considerable attention from researchers in recent years. However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking. To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process. DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments. We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability. Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead. We will make our code publicly available soon.","sentences":["Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images.","These attack methods have garnered considerable attention from researchers in recent years.","However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking.","To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process.","DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images.","Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments.","We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker.","Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios.","Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability.","Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead.","We will make our code publicly available soon."],"url":"http://arxiv.org/abs/2402.17976v1","category":"cs.CV"}
{"created":"2024-02-28 00:09:07","title":"Gradient-Free Adaptive Global Pruning for Pre-trained Language Models","abstract":"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.","sentences":["The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.","Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency.","Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions.","Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality.","AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17946v1","category":"cs.CL"}
{"created":"2024-02-27 23:12:45","title":"Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures","abstract":"Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures.","sentences":["Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive.","Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters.","However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets.","In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning.","FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters.","By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets.","Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures."],"url":"http://arxiv.org/abs/2402.17934v1","category":"cs.CL"}
{"created":"2024-02-27 23:09:55","title":"A Heterogeneous Agent Model of Mortgage Servicing: An Income-based Relief Analysis","abstract":"Mortgages account for the largest portion of household debt in the United States, totaling around \\$12 trillion nationwide. In times of financial hardship, alleviating mortgage burdens is essential for supporting affected households. The mortgage servicing industry plays a vital role in offering this assistance, yet there has been limited research modelling the complex relationship between households and servicers. To bridge this gap, we developed an agent-based model that explores household behavior and the effectiveness of relief measures during financial distress. Our model represents households as adaptive learning agents with realistic financial attributes. These households experience exogenous income shocks, which may influence their ability to make mortgage payments. Mortgage servicers provide relief options to these households, who then choose the most suitable relief based on their unique financial circumstances and individual preferences. We analyze the impact of various external shocks and the success of different mortgage relief strategies on specific borrower subgroups. Through this analysis, we show that our model can not only replicate real-world mortgage studies but also act as a tool for conducting a broad range of what-if scenario analyses. Our approach offers fine-grained insights that can inform the development of more effective and inclusive mortgage relief solutions.","sentences":["Mortgages account for the largest portion of household debt in the United States, totaling around \\$12 trillion nationwide.","In times of financial hardship, alleviating mortgage burdens is essential for supporting affected households.","The mortgage servicing industry plays a vital role in offering this assistance, yet there has been limited research modelling the complex relationship between households and servicers.","To bridge this gap, we developed an agent-based model that explores household behavior and the effectiveness of relief measures during financial distress.","Our model represents households as adaptive learning agents with realistic financial attributes.","These households experience exogenous income shocks, which may influence their ability to make mortgage payments.","Mortgage servicers provide relief options to these households, who then choose the most suitable relief based on their unique financial circumstances and individual preferences.","We analyze the impact of various external shocks and the success of different mortgage relief strategies on specific borrower subgroups.","Through this analysis, we show that our model can not only replicate real-world mortgage studies but also act as a tool for conducting a broad range of what-if scenario analyses.","Our approach offers fine-grained insights that can inform the development of more effective and inclusive mortgage relief solutions."],"url":"http://arxiv.org/abs/2402.17932v2","category":"cs.MA"}
{"created":"2024-02-27 23:07:59","title":"Universal energy-speed-accuracy trade-offs in driven nonequilibrium systems","abstract":"Physical systems driven away from equilibrium by an external controller dissipate heat to the environment; the excess entropy production in the thermal reservoir can be interpreted as a \"cost\" to transform the system in a finite time. The connection between measure theoretic optimal transport and dissipative nonequilibrium dynamics provides a language for quantifying this cost and has resulted in a collection of \"thermodynamic speed limits\", which argue that the minimum dissipation of a transformation between two probability distributions is directly proportional to the rate of driving. Thermodynamic speed limits rely on the assumption that the target probability distribution is perfectly realized, which is almost never the case in experiments or numerical simulations. Here, we address the ubiquitous situation in which the external controller is imperfect. As a consequence, we obtain a lower bound for the dissipated work in generic nonequilibrium control problems that 1) is asymptotically tight and 2) matches the thermodynamic speed limit in the case of optimal driving. We illustrate these bounds on analytically solvable examples and also develop a strategy for optimizing minimally dissipative protocols based on optimal transport flow matching, a generative machine learning technique. This latter approach ensures the scalability of both the theoretical and computational framework we put forth. Crucially, we demonstrate that we can compute the terms in our bound numerically using efficient algorithms from the computational optimal transport literature and that the protocols that we learn saturate the bound.","sentences":["Physical systems driven away from equilibrium by an external controller dissipate heat to the environment; the excess entropy production in the thermal reservoir can be interpreted as a \"cost\" to transform the system in a finite time.","The connection between measure theoretic optimal transport and dissipative nonequilibrium dynamics provides a language for quantifying this cost and has resulted in a collection of \"thermodynamic speed limits\", which argue that the minimum dissipation of a transformation between two probability distributions is directly proportional to the rate of driving.","Thermodynamic speed limits rely on the assumption that the target probability distribution is perfectly realized, which is almost never the case in experiments or numerical simulations.","Here, we address the ubiquitous situation in which the external controller is imperfect.","As a consequence, we obtain a lower bound for the dissipated work in generic nonequilibrium control problems that 1) is asymptotically tight and 2) matches the thermodynamic speed limit in the case of optimal driving.","We illustrate these bounds on analytically solvable examples and also develop a strategy for optimizing minimally dissipative protocols based on optimal transport flow matching, a generative machine learning technique.","This latter approach ensures the scalability of both the theoretical and computational framework we put forth.","Crucially, we demonstrate that we can compute the terms in our bound numerically using efficient algorithms from the computational optimal transport literature and that the protocols that we learn saturate the bound."],"url":"http://arxiv.org/abs/2402.17931v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 23:04:28","title":"Decremental $(1+\u03b5)$-Approximate Maximum Eigenvector: Dynamic Power Method","abstract":"We present a dynamic algorithm for maintaining $(1+\\epsilon)$-approximate maximum eigenvector and eigenvalue of a positive semi-definite matrix $A$ undergoing \\emph{decreasing} updates, i.e., updates which may only decrease eigenvalues. Given a vector $v$ updating $A\\gets A-vv^{\\top}$, our algorithm takes $\\tilde{O}(\\mathrm{nnz}(v))$ amortized update time, i.e., polylogarithmic per non-zeros in the update vector.   Our technique is based on a novel analysis of the influential power method in the dynamic setting. The two previous sets of techniques have the following drawbacks (1) algebraic techniques can maintain exact solutions but their update time is at least polynomial per non-zeros, and (2) sketching techniques admit polylogarithmic update time but suffer from a crude additive approximation.   Our algorithm exploits an oblivious adversary. Interestingly, we show that any algorithm with polylogarithmic update time per non-zeros that works against an adaptive adversary and satisfies an additional natural property would imply a breakthrough for checking psd-ness of matrices in $\\tilde{O}(n^{2})$ time, instead of $O(n^{\\omega})$ time.","sentences":["We present a dynamic algorithm for maintaining $(1+\\epsilon)$-approximate maximum eigenvector and eigenvalue of a positive semi-definite matrix $A$ undergoing \\emph{decreasing} updates, i.e., updates which may only decrease eigenvalues.","Given a vector $v$ updating $A\\gets A-vv^{\\top}$, our algorithm takes $\\tilde{O}(\\mathrm{nnz}(v))$ amortized update time, i.e., polylogarithmic per non-zeros in the update vector.   ","Our technique is based on a novel analysis of the influential power method in the dynamic setting.","The two previous sets of techniques have the following drawbacks (1) algebraic techniques can maintain exact solutions but their update time is at least polynomial per non-zeros, and (2) sketching techniques admit polylogarithmic update time but suffer from a crude additive approximation.   ","Our algorithm exploits an oblivious adversary.","Interestingly, we show that any algorithm with polylogarithmic update time per non-zeros that works against an adaptive adversary and satisfies an additional natural property would imply a breakthrough for checking psd-ness of matrices in $\\tilde{O}(n^{2})$ time, instead of $O(n^{\\omega})$ time."],"url":"http://arxiv.org/abs/2402.17929v1","category":"cs.DS"}
{"created":"2024-02-27 22:35:55","title":"Band structure and excitonic properties of WSe$_2$ in the isolated monolayer limit in an all-electron approach","abstract":"A study is presented of the electronic band structure and optical absorption spectrum of monolayer WSe$_2$ using an all-electron quasiparticle self-consistent $GW$ approach, QS$G\\hat W$, in which the screened Coulomb interaction $\\hat W$ is calculated including ladder diagrams representing electron-hole interaction. The Bethe-Salpeter Equation is used to calculate both the screened Coulomb interaction $\\hat W$ in the quasiparticle band structure and the imaginary part of the macroscopic dielectric function. The convergence of the quasiparticle band gap and lowest exciton peak position is studied as function of the separation of the monolayers when using periodic boundary conditions. The quasiparticle gap is found to vary as $1/d$ with $d$ the size of the vacuum separation, while the excitonic lowest peak reaches convergence much faster. The nature of the exciton spectrum is analyzed and shows several excitonic peaks below the quasiparticle gap when a sufficient number of $\\textbf{k}$ points is used. They are found to be in good agreement with prior work and experiment after adding spin-orbit coupling corrections and can be explained in the context of the Wannier-Mott theory adapted to 2D.","sentences":["A study is presented of the electronic band structure and optical absorption spectrum of monolayer WSe$_2$ using an all-electron quasiparticle self-consistent $GW$ approach, QS$G\\hat W$, in which the screened Coulomb interaction $\\hat W$ is calculated including ladder diagrams representing electron-hole interaction.","The Bethe-Salpeter Equation is used to calculate both the screened Coulomb interaction $\\hat W$ in the quasiparticle band structure and the imaginary part of the macroscopic dielectric function.","The convergence of the quasiparticle band gap and lowest exciton peak position is studied as function of the separation of the monolayers when using periodic boundary conditions.","The quasiparticle gap is found to vary as $1/d$ with $d$ the size of the vacuum separation, while the excitonic lowest peak reaches convergence much faster.","The nature of the exciton spectrum is analyzed and shows several excitonic peaks below the quasiparticle gap when a sufficient number of $\\textbf{k}$ points is used.","They are found to be in good agreement with prior work and experiment after adding spin-orbit coupling corrections and can be explained in the context of the Wannier-Mott theory adapted to 2D."],"url":"http://arxiv.org/abs/2402.17924v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 21:48:41","title":"NIIRF: Neural IIR Filter Field for HRTF Upsampling and Personalization","abstract":"Head-related transfer functions (HRTFs) are important for immersive audio, and their spatial interpolation has been studied to upsample finite measurements. Recently, neural fields (NFs) which map from sound source direction to HRTF have gained attention. Existing NF-based methods focused on estimating the magnitude of the HRTF from a given sound source direction, and the magnitude is converted to a finite impulse response (FIR) filter. We propose the neural infinite impulse response filter field (NIIRF) method that instead estimates the coefficients of cascaded IIR filters. IIR filters mimic the modal nature of HRTFs, thus needing fewer coefficients to approximate them well compared to FIR filters. We find that our method can match the performance of existing NF-based methods on multiple datasets, even outperforming them when measurements are sparse. We also explore approaches to personalize the NF to a subject and experimentally find low-rank adaptation to be effective.","sentences":["Head-related transfer functions (HRTFs) are important for immersive audio, and their spatial interpolation has been studied to upsample finite measurements.","Recently, neural fields (NFs) which map from sound source direction to HRTF have gained attention.","Existing NF-based methods focused on estimating the magnitude of the HRTF from a given sound source direction, and the magnitude is converted to a finite impulse response (FIR) filter.","We propose the neural infinite impulse response filter field (NIIRF) method that instead estimates the coefficients of cascaded IIR filters.","IIR filters mimic the modal nature of HRTFs, thus needing fewer coefficients to approximate them well compared to FIR filters.","We find that our method can match the performance of existing NF-based methods on multiple datasets, even outperforming them when measurements are sparse.","We also explore approaches to personalize the NF to a subject and experimentally find low-rank adaptation to be effective."],"url":"http://arxiv.org/abs/2402.17907v1","category":"eess.AS"}
{"created":"2024-02-27 21:27:35","title":"A Language Model based Framework for New Concept Placement in Ontologies","abstract":"We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we proposed explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.","sentences":["We investigate the task of inserting new concepts extracted from texts into an ontology using language models.","We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into.","In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection.","We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark.","The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection.","Zero-shot prompting of LLMs is still not adequate for the task, and we proposed explainable instruction tuning of LLMs for improved performance.","Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies."],"url":"http://arxiv.org/abs/2402.17897v1","category":"cs.CL"}
{"created":"2024-02-27 21:18:04","title":"Exact Controllability and Stabilization of the Wave Equation","abstract":"These Notes originated from a course I delivered at the Institute of Mathematics of the Universidade Federal do Rio de Janeiro, Brazil (UFRJ) in July-September 1989, were initially published in 1989 in Spanish under the title \"Controlabilidad Exacta y Estabilizaci\\'on de la Ecuaci\\'on de Ondas\" in the Lecture Notes Series of the Institute.   Despite the significant evolution of the topic over the last three decades, I believe that the text, with its synthetic presentation of fundamental tools in the field, remains valuable for researchers in the area, especially for younger generations. It is written from the perspective of the young mathematician I was when I authored the Notes, needing to learn many things in the process and, therefore, taking care to develop details often left to the reader or not readily available elsewhere.   These Notes were written one year after completing my PhD at the Universit\\'e Pierre et Marie Curie in Paris and drafting the lectures of Professor Jacques-Louis Lions at Coll\\`ege de France in the academic year 1986-1987, later published as a book in 1988. Parts of these Notes offer a concise presentation of content developed in more detail in that book, supplemented by work on the decay of dissipative wave equations during my PhD under the supervision of Professor Alain Haraux in Paris.","sentences":["These Notes originated from a course I delivered at the Institute of Mathematics of the Universidade Federal do Rio de Janeiro, Brazil (UFRJ) in July-September 1989, were initially published in 1989 in Spanish under the title \"Controlabilidad Exacta y Estabilizaci\\'on de la Ecuaci\\'on de Ondas\" in the Lecture Notes Series of the Institute.   ","Despite the significant evolution of the topic over the last three decades, I believe that the text, with its synthetic presentation of fundamental tools in the field, remains valuable for researchers in the area, especially for younger generations.","It is written from the perspective of the young mathematician I was when I authored the Notes, needing to learn many things in the process and, therefore, taking care to develop details often left to the reader or not readily available elsewhere.   ","These Notes were written one year after completing my PhD at the Universit\\'e Pierre et Marie Curie in Paris and drafting the lectures of Professor Jacques-Louis Lions at Coll\\`ege de France in the academic year 1986-1987, later published as a book in 1988.","Parts of these Notes offer a concise presentation of content developed in more detail in that book, supplemented by work on the decay of dissipative wave equations during my PhD under the supervision of Professor Alain Haraux in Paris."],"url":"http://arxiv.org/abs/2402.17894v1","category":"math.OC"}
{"created":"2024-02-27 19:28:18","title":"Looking for Complexity at Phase Boundaries in Continuous Cellular Automata","abstract":"One key challenge in Artificial Life is designing systems that display an emergence of complex behaviors. Many such systems depend on a high-dimensional parameter space, only a small subset of which displays interesting dynamics. Focusing on the case of continuous systems, we introduce the 'Phase Transition Finder'(PTF) algorithm, which can be used to efficiently generate parameters lying at the border between two phases. We argue that such points are more likely to display complex behaviors, and confirm this by applying PTF to Lenia showing it can increase the frequency of interesting behaviors more than two-fold, while remaining efficient enough for large-scale searches.","sentences":["One key challenge in Artificial Life is designing systems that display an emergence of complex behaviors.","Many such systems depend on a high-dimensional parameter space, only a small subset of which displays interesting dynamics.","Focusing on the case of continuous systems, we introduce the 'Phase Transition Finder'(PTF) algorithm, which can be used to efficiently generate parameters lying at the border between two phases.","We argue that such points are more likely to display complex behaviors, and confirm this by applying PTF to Lenia showing it can increase the frequency of interesting behaviors more than two-fold, while remaining efficient enough for large-scale searches."],"url":"http://arxiv.org/abs/2402.17848v1","category":"nlin.AO"}
{"created":"2024-02-27 19:08:05","title":"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems","abstract":"Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.","sentences":["Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation.","We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs).","We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection.","The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up.","Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves."],"url":"http://arxiv.org/abs/2402.17840v1","category":"cs.CL"}
{"created":"2024-02-28 18:59:07","title":"Logarithmic Sobolev Inequalities for Bounded Domains and Applications to Drift-Diffusion Equations","abstract":"We prove logarithmic Sobolev inequalities on higher-dimensional bounded smooth domains based on novel Gagliardo-Nirenberg type interpolation inequalities. Moreover, we use them to address the long-time dynamics of some nonlinear nonlocal drift-diffusion models and prove the exponential decay of their solutions to constant steady states.","sentences":["We prove logarithmic Sobolev inequalities on higher-dimensional bounded smooth domains based on novel Gagliardo-Nirenberg type interpolation inequalities.","Moreover, we use them to address the long-time dynamics of some nonlinear nonlocal drift-diffusion models and prove the exponential decay of their solutions to constant steady states."],"url":"http://arxiv.org/abs/2402.18572v1","category":"math.AP"}
{"created":"2024-02-28 18:58:11","title":"Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators","abstract":"In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability. In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training. However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness. Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model. However, it has been observed that these techniques are not effective w.r.t. accuracy. Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements. In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices. We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets. Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy.","sentences":["In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability.","In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training.","However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness.","Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model.","However, it has been observed that these techniques are not effective w.r.t. accuracy.","Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements.","In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices.","We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets.","Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy."],"url":"http://arxiv.org/abs/2402.18569v1","category":"cs.AR"}
{"created":"2024-02-28 18:34:53","title":"Implicit Bias of Next-Token Prediction","abstract":"Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \\NTP-separability conditions. Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP.","sentences":["Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence.","Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context.","This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary.","It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)?","Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound.","We also demonstrate that these conditions hold under overparameterization.","Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities.","Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \\NTP-separability conditions.","Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP."],"url":"http://arxiv.org/abs/2402.18551v1","category":"cs.LG"}
{"created":"2024-02-28 18:32:43","title":"Deformed neutron stars","abstract":"We present solutions for non-spherically symmetric neutron stars. We begin by deriving the Tolman-Oppenheimer-Volkoff equations from a parameterized metric that takes into account the deformation of the star due to differences in equatorial and polar pressures, expressed in terms of a parameter D, which is the ratio between polar and equatorial radius. The stellar structure is solved using the GM1 equation of state and the Tolman-Oppenheimer-Volkoff equations for deformed objects are numerically integrated using the fourth-order Runge-Kutta method for different values of the parameter D. We show that larger values of D > 1, that describe prolate neutron stars, yield smaller values of mass and radius, while for smaller values of D < 1, describing oblate neutron stars, larger values for mass and radius are attained. From the confrontation of our model theoretical predictions with recent observational data on pulsars, it is possible to constrain the values of the parameter D.","sentences":["We present solutions for non-spherically symmetric neutron stars.","We begin by deriving the Tolman-Oppenheimer-Volkoff equations from a parameterized metric that takes into account the deformation of the star due to differences in equatorial and polar pressures, expressed in terms of a parameter D, which is the ratio between polar and equatorial radius.","The stellar structure is solved using the GM1 equation of state and the Tolman-Oppenheimer-Volkoff equations for deformed objects are numerically integrated using the fourth-order Runge-Kutta method for different values of the parameter D. We show that larger values of D > 1, that describe prolate neutron stars, yield smaller values of mass and radius, while for smaller values of D < 1, describing oblate neutron stars, larger values for mass and radius are attained.","From the confrontation of our model theoretical predictions with recent observational data on pulsars, it is possible to constrain the values of the parameter D."],"url":"http://arxiv.org/abs/2402.18550v2","category":"gr-qc"}
{"created":"2024-02-28 18:28:21","title":"Optimizing Beer Glass Shapes to Minimize Heat Transfer During Consumption","abstract":"This paper addresses the problem of determining the optimum shape for a beer glass that minimizes the heat transfer while the liquid is consumed, thereby keeping it cold for as long as possible. The proposed solution avoids the use of insulating materials. The glass is modelled as a body of revolution generated by a smooth curve S, constructed from a material with negligible thermal resistance at the revolution surface but insulated at the bottom. The ordinary differential equation describing the problem is derived from the first law of Thermodynamics applied to a control volume encompassing the liquid. This is an inverse optimization problem, aiming to find the shape of the glass (represented by curve S) that minimizes the heat transfer rate. In contrast, the direct problem aims to determine the heat transfer rate for a given geometry. The solution obtained is analytic, and the resulting expression for S is in closed form, providing a family of optimal glass shapes that can be manufactured using conventional methods.","sentences":["This paper addresses the problem of determining the optimum shape for a beer glass that minimizes the heat transfer while the liquid is consumed, thereby keeping it cold for as long as possible.","The proposed solution avoids the use of insulating materials.","The glass is modelled as a body of revolution generated by a smooth curve S, constructed from a material with negligible thermal resistance at the revolution surface but insulated at the bottom.","The ordinary differential equation describing the problem is derived from the first law of Thermodynamics applied to a control volume encompassing the liquid.","This is an inverse optimization problem, aiming to find the shape of the glass (represented by curve S) that minimizes the heat transfer rate.","In contrast, the direct problem aims to determine the heat transfer rate for a given geometry.","The solution obtained is analytic, and the resulting expression for S is in closed form, providing a family of optimal glass shapes that can be manufactured using conventional methods."],"url":"http://arxiv.org/abs/2402.18544v1","category":"physics.pop-ph"}
{"created":"2024-02-28 18:17:19","title":"Dark matter bound-state formation in the Sun","abstract":"The Sun may capture asymmetric dark matter (DM), which can subsequently form bound-states through the radiative emission of a sub-GeV scalar. This process enables generation of scalars without requiring DM annihilation. In addition to DM capture on nucleons, the DM-scalar coupling responsible for bound-state formation also induces capture from self-scatterings of ambient DM particles with DM particles already captured, as well as with DM bound-states formed in-situ within the Sun. This scenario is studied in detail by solving Boltzmann equations numerically and analytically. In particular, we take into consideration that the DM self-capture rates require a treatment beyond the conventional Born approximation. We show that, thanks to DM scatterings on bound-states, the number of DM particles captured increases exponentially, leading to enhanced emission of relativistic scalars through bound-state formation, whose final decay products could be observable. We explore phenomenological signatures with the example that the scalar mediator decays to neutrinos. We find that the neutrino flux emitted can be comparable to atmospheric neutrino fluxes within the range of energies below one hundred MeV. Future facilities like Hyper-K, and direct DM detection experiments can further test such scenario.","sentences":["The Sun may capture asymmetric dark matter (DM), which can subsequently form bound-states through the radiative emission of a sub-GeV scalar.","This process enables generation of scalars without requiring DM annihilation.","In addition to DM capture on nucleons, the DM-scalar coupling responsible for bound-state formation also induces capture from self-scatterings of ambient DM particles with DM particles already captured, as well as with DM bound-states formed in-situ within the Sun.","This scenario is studied in detail by solving Boltzmann equations numerically and analytically.","In particular, we take into consideration that the DM self-capture rates require a treatment beyond the conventional Born approximation.","We show that, thanks to DM scatterings on bound-states, the number of DM particles captured increases exponentially, leading to enhanced emission of relativistic scalars through bound-state formation, whose final decay products could be observable.","We explore phenomenological signatures with the example that the scalar mediator decays to neutrinos.","We find that the neutrino flux emitted can be comparable to atmospheric neutrino fluxes within the range of energies below one hundred MeV. Future facilities like Hyper-K, and direct DM detection experiments can further test such scenario."],"url":"http://arxiv.org/abs/2402.18535v1","category":"hep-ph"}
{"created":"2024-02-28 18:08:35","title":"High angular momentum hot differentially rotating equilibrium star evolutions in conformally flat spacetime","abstract":"The conformal flatness approximation to the Einstein equations has been successfully used in many astrophysical applications such as initial data constructions and dynamical simulations. Although it has been shown that full general relativistic strongly differentially rotating equilibrium models deviate by at most a few percents from their conformally flat counterparts, whether those solutions share the same dynamical stabilities has not been fully addressed. To further understand the limitations of the conformal flatness approximation, in this work, we construct spatially-conformally-flat hot hypermassive neutron stars with postmerger-like rotation laws, and perform conformally flat evolutions and analysis over dynamical timescales. We found that the stellar profiles of quasi-toroidal models with high angular momentum for $J \\gtrsim 9 \\;G M_{\\odot}^2 / c$ can change significantly over dynamical timescales. In contrast, all the quasi-spherical models considered in this work remain stable even with high angular momentum $J=9\\;G M_{\\odot}^2 / c$. Our investigation suggest that the quasi-spherical models are suitable initial data for long-lived hypermassive neutron star modelings in conformally flat spacetime.","sentences":["The conformal flatness approximation to the Einstein equations has been successfully used in many astrophysical applications such as initial data constructions and dynamical simulations.","Although it has been shown that full general relativistic strongly differentially rotating equilibrium models deviate by at most a few percents from their conformally flat counterparts, whether those solutions share the same dynamical stabilities has not been fully addressed.","To further understand the limitations of the conformal flatness approximation, in this work, we construct spatially-conformally-flat hot hypermassive neutron stars with postmerger-like rotation laws, and perform conformally flat evolutions and analysis over dynamical timescales.","We found that the stellar profiles of quasi-toroidal models with high angular momentum for $J \\gtrsim 9 \\;G M_{\\odot}^2 / c$ can change significantly over dynamical timescales.","In contrast, all the quasi-spherical models considered in this work remain stable even with high angular momentum $J=9\\;G M_{\\odot}^2 / c$. Our investigation suggest that the quasi-spherical models are suitable initial data for long-lived hypermassive neutron star modelings in conformally flat spacetime."],"url":"http://arxiv.org/abs/2402.18529v1","category":"astro-ph.HE"}
{"created":"2024-02-28 17:56:10","title":"Rigorously proven chaos in chemical kinetics","abstract":"This study addresses a longstanding question regarding the mathematical proof of chaotic behavior in kinetic differential equations. Following the numerous numerical and experimental results in the past 50 years, we introduce two formal chemical reactions that rigorously demonstrate this behavior. Our approach involves transforming chaotic equations into kinetic differential equations and subsequently realizing these equations through formal chemical reactions. The findings present a novel perspective on chaotic dynamics within chemical kinetics, thereby resolving a longstanding open problem.","sentences":["This study addresses a longstanding question regarding the mathematical proof of chaotic behavior in kinetic differential equations.","Following the numerous numerical and experimental results in the past 50 years, we introduce two formal chemical reactions that rigorously demonstrate this behavior.","Our approach involves transforming chaotic equations into kinetic differential equations and subsequently realizing these equations through formal chemical reactions.","The findings present a novel perspective on chaotic dynamics within chemical kinetics, thereby resolving a longstanding open problem."],"url":"http://arxiv.org/abs/2402.18523v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 17:47:40","title":"BPS chiral vortices in a Maxwell-Higgs electrodynamics","abstract":"We investigate the existence of BPS structures in a Maxwell-Higgs electrodynamics immersed within a chiral medium, whose electromagnetic properties are described by both the Chern-Simons term and a neutral scalar field. The implementation of the Bogomol'nyi-Prasad-Sommerfield's technique provides the BPS potential and the self-dual equations whose solutions saturate the Bogomol'nyi bound. In such a context, we look for vortices in two chiral media: the first one engenders localized vortices with an exponential decay similar to that of the Abrikosov-Nielsen-Olesen solutions, whereas the second medium generates delocalized profiles whose tail follows a power-law decay. Once we have solved the BPS systems, we comment on the effects induced by the presence of the chiral medium on the Maxwell-Higgs vortices.","sentences":["We investigate the existence of BPS structures in a Maxwell-Higgs electrodynamics immersed within a chiral medium, whose electromagnetic properties are described by both the Chern-Simons term and a neutral scalar field.","The implementation of the Bogomol'nyi-Prasad-Sommerfield's technique provides the BPS potential and the self-dual equations whose solutions saturate the Bogomol'nyi bound.","In such a context, we look for vortices in two chiral media: the first one engenders localized vortices with an exponential decay similar to that of the Abrikosov-Nielsen-Olesen solutions, whereas the second medium generates delocalized profiles whose tail follows a power-law decay.","Once we have solved the BPS systems, we comment on the effects induced by the presence of the chiral medium on the Maxwell-Higgs vortices."],"url":"http://arxiv.org/abs/2402.18517v1","category":"hep-th"}
{"created":"2024-02-28 17:40:05","title":"Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference","abstract":"The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit.","sentences":["The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path.","Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state.","As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data.","Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs.","The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution.","On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit."],"url":"http://arxiv.org/abs/2402.18512v1","category":"cs.LG"}
{"created":"2024-02-28 17:38:06","title":"RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval","abstract":"This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.","sentences":["This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems.","We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.","Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers.","A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease.","Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers."],"url":"http://arxiv.org/abs/2402.18510v2","category":"cs.LG"}
{"created":"2024-02-28 17:19:47","title":"The Electrochemical Transistor: a device based on the Electrochemical control of a polymer Polaronic state. The PCPDT-BT as a case study","abstract":"This work presents an original concept directed to implement an unconventional methodology where a device is produced by integrating a solid-state circuitry concept and an electrochemistry cell. In our experimental system an organic semiconductor, (PCPDT-BT), serves both as the gate and working electrode. Gating is obtained via electrochemical polarization exploiting a conventional three electrodes electrochemical cell placed on top of a traditional source/drain/gate solid-state device configuration. Source/drain conduction is probed via impedance measurement (electrochemical impedance spectroscopy, EIS) performed as a function of time at constant frequency, under constant potential control. The conductivity of the PCPDT-BT is due to the polaronic state induced via application of a suitable electrochemical potential (in the oxidation regime), as it is proved by infrared (IR) spectra recorded in-situ/in-operando (in attenuated under total reflection, ATR, mode) upon both electrochemical and chemical ionization/doping of the PCPDT-BT.","sentences":["This work presents an original concept directed to implement an unconventional methodology where a device is produced by integrating a solid-state circuitry concept and an electrochemistry cell.","In our experimental system an organic semiconductor, (PCPDT-BT), serves both as the gate and working electrode.","Gating is obtained via electrochemical polarization exploiting a conventional three electrodes electrochemical cell placed on top of a traditional source/drain/gate solid-state device configuration.","Source/drain conduction is probed via impedance measurement (electrochemical impedance spectroscopy, EIS) performed as a function of time at constant frequency, under constant potential control.","The conductivity of the PCPDT-BT is due to the polaronic state induced via application of a suitable electrochemical potential (in the oxidation regime), as it is proved by infrared (IR) spectra recorded in-situ/in-operando (in attenuated under total reflection, ATR, mode) upon both electrochemical and chemical ionization/doping of the PCPDT-BT."],"url":"http://arxiv.org/abs/2402.18492v1","category":"physics.app-ph"}
{"created":"2024-02-28 17:07:10","title":"Finite skew braces of square-free order and supersolubility","abstract":"The aim of this paper is to study supersoluble skew braces, a class of skew braces that encompasses all finite skew braces of square-free order. It turns out that finite supersoluble skew braces have Sylow towers, and that in an arbitrary supersoluble skew brace $B$ many relevant skew brace-theoretical properties are easier to identify: for example, a centrally nilpotent ideal of $B$ is $B$-centrally nilpotent, a fact that simplifies the computational search for the Fitting ideal; also, $B$ has finite multipermutational level if and only if $(B,+)$ is nilpotent.   Given a finite presentation of the structure skew brace $G(X,r)$ associated with a finite non-degenerate solution of the Yang--Baxter Equation (YBE), there is an algorithm that decides if $G(X,r)$ is supersoluble or not. Moreover, supersoluble skew braces are examples of almost polycyclic skew braces, so they give rise to solutions of the YBE on which one can algorithmically work on.","sentences":["The aim of this paper is to study supersoluble skew braces, a class of skew braces that encompasses all finite skew braces of square-free order.","It turns out that finite supersoluble skew braces have Sylow towers, and that in an arbitrary supersoluble skew brace $B$ many relevant skew brace-theoretical properties are easier to identify: for example, a centrally nilpotent ideal of $B$ is $B$-centrally nilpotent, a fact that simplifies the computational search for the Fitting ideal; also, $B$ has finite multipermutational level if and only if $(B,+)$ is nilpotent.   ","Given a finite presentation of the structure skew brace $G(X,r)$ associated with a finite non-degenerate solution of the Yang--Baxter Equation (YBE), there is an algorithm that decides if $G(X,r)$ is supersoluble or not.","Moreover, supersoluble skew braces are examples of almost polycyclic skew braces, so they give rise to solutions of the YBE on which one can algorithmically work on."],"url":"http://arxiv.org/abs/2402.18486v1","category":"math.GR"}
{"created":"2024-02-28 17:06:19","title":"A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics","abstract":"Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades. As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes. However, coarse models suffer from inherent bias due to the ignored \"sub-grid\" scales. We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators. Previous efforts have attempted to train such operators using loss functions that match statistics. However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged. Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data. The key obstacle is the chaotic nature of the underlying dynamics. To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model simulation nudged towards that reference. The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM). For the former, our method enables the quantification of events that have return period two orders longer than the training data. For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases.","sentences":["Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades.","As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes.","However, coarse models suffer from inherent bias due to the ignored \"sub-grid\" scales.","We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators.","Previous efforts have attempted to train such operators using loss functions that match statistics.","However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged.","Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data.","The key obstacle is the chaotic nature of the underlying dynamics.","To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model simulation nudged towards that reference.","The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM).","For the former, our method enables the quantification of events that have return period two orders longer than the training data.","For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases."],"url":"http://arxiv.org/abs/2402.18484v1","category":"physics.ao-ph"}
{"created":"2024-02-28 17:06:15","title":"A generalised Nehari manifold method for a class of non linear Schr\u00f6dinger systems in $\\mathbb{R}^3$","abstract":"We study the existence of positive solutions of a particular elliptic system in $\\mathbb{R}^3$ composed of two coupled non linear stationary Schr\\\"odinger equations (NLSEs), that is $-\\epsilon^2 \\Delta u + V(x) u= h_v(u,v), - \\epsilon^2 \\Delta v + V(x) v=h_u (u,v)$. Under certain hypotheses on the potential $V$ and the non linearity $h$, we manage to prove that there exists a solution $(u_\\epsilon,v_\\epsilon)$ that decays exponentially with respect to local minima points of the potential and whose energy tends to concentrate around these points, as $\\epsilon \\to 0$. We also estimate this energy in terms of particular ground state energies. This work follows closely what is done in https://doi.org/10.1007/s00526-007-0103-z , although here we consider a more general non linearity and we restrict ourselves to the case where the domain is $\\mathbb{R}^3$.","sentences":["We study the existence of positive solutions of a particular elliptic system in $\\mathbb{R}^3$ composed of two coupled non linear stationary Schr\\\"odinger equations (NLSEs), that is $-\\epsilon^2 \\Delta u + V(x) u= h_v(u,v), - \\epsilon^2 \\Delta v + V(x) v=h_u (u,v)$. Under certain hypotheses on the potential $V$ and the non linearity $h$, we manage to prove that there exists a solution $(u_\\epsilon,v_\\epsilon)$ that decays exponentially with respect to local minima points of the potential and whose energy tends to concentrate around these points, as $\\epsilon \\to 0$.","We also estimate this energy in terms of particular ground state energies.","This work follows closely what is done in https://doi.org/10.1007/s00526-007-0103-z , although here we consider a more general non linearity and we restrict ourselves to the case where the domain is $\\mathbb{R}^3$."],"url":"http://arxiv.org/abs/2402.18483v1","category":"math.AP"}
{"created":"2024-02-28 16:54:46","title":"Three-leg form factor on Coulomb branch","abstract":"We study the form factor of the lowest component of the stress-tensor multiplet away from the origin of the moduli space in the spontaneously broken, aka Coulomb, phase of the maximally supersymmetric Yang-Mills theory for decay into three massive W-bosons. The calculations are done at two-loop order by deriving and solving canonical differential equations in the asymptotical limit of nearly vanishing W-masses. We confirm our previous findings that infrared physics of `off-shell observables' is governed by the octagon anomalous dimension rather than the cusp. In addition, the form factor in question possesses a nontrivial remainder function, which was found to be identical to the massless case, upon a proper subtraction of infrared logarithms (and finite terms). However, the iterative structure of the object is more intricate and is not simply related to the previous orders in coupling as opposed to amplitudes/form factors at the origin of the moduli space.","sentences":["We study the form factor of the lowest component of the stress-tensor multiplet away from the origin of the moduli space in the spontaneously broken, aka Coulomb, phase of the maximally supersymmetric Yang-Mills theory for decay into three massive W-bosons.","The calculations are done at two-loop order by deriving and solving canonical differential equations in the asymptotical limit of nearly vanishing W-masses.","We confirm our previous findings that infrared physics of `off-shell observables' is governed by the octagon anomalous dimension rather than the cusp.","In addition, the form factor in question possesses a nontrivial remainder function, which was found to be identical to the massless case, upon a proper subtraction of infrared logarithms (and finite terms).","However, the iterative structure of the object is more intricate and is not simply related to the previous orders in coupling as opposed to amplitudes/form factors at the origin of the moduli space."],"url":"http://arxiv.org/abs/2402.18475v1","category":"hep-th"}
{"created":"2024-02-28 16:50:38","title":"Flux Quantization","abstract":"Flux- and charge-quantization laws for higher gauge fields of Maxwell type -- e.g. the common electromagnetic field (the \"A-field\") but also the B-, RR-, and C-fields considered in string/M-theory -- specify non-perturbative completions of these fields by encoding their solitonic behaviour and hence by specifying the discrete charges carried by the individual branes (higher-dimensional monopoles or solitons) that source the field fluxes.   This article surveys the general (rational-)homotopy theoretic understanding of flux- and charge-quantization via the Chern-Dold character map generalized to the non-linear (self-sourcing) Bianchi identities that appear in higher-dimensional supergravity theories, notably for B&RR-fields in D=10, for the C-field in D=11 supergravity, and for the B-field on fivebrane worldvolumes.","sentences":["Flux- and charge-quantization laws for higher gauge fields of Maxwell type -- e.g. the common electromagnetic field (the \"A-field\") but also the B-, RR-, and C-fields considered in string/M-theory -- specify non-perturbative completions of these fields by encoding their solitonic behaviour and hence by specifying the discrete charges carried by the individual branes (higher-dimensional monopoles or solitons) that source the field fluxes.   ","This article surveys the general (rational-)homotopy theoretic understanding of flux- and charge-quantization via the Chern-Dold character map generalized to the non-linear (self-sourcing) Bianchi identities that appear in higher-dimensional supergravity theories, notably for B&RR-fields in D=10, for the C-field in D=11 supergravity, and for the B-field on fivebrane worldvolumes."],"url":"http://arxiv.org/abs/2402.18473v1","category":"hep-th"}
{"created":"2024-02-28 16:50:05","title":"Implementing Online Reinforcement Learning with Clustering Neural Networks","abstract":"An agent employing reinforcement learning takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective. Rewards (positive or negative) guide the agent toward improved future actions. This paper builds on prior clustering neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes). The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition. Simulation results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method.","sentences":["An agent employing reinforcement learning takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective.","Rewards (positive or negative) guide the agent toward improved future actions.","This paper builds on prior clustering neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes).","The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition.","Simulation results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method."],"url":"http://arxiv.org/abs/2402.18472v1","category":"cs.NE"}
{"created":"2024-02-28 16:46:00","title":"Introducing cuDisc: a 2D code for protoplanetary disc structure and evolution calculations","abstract":"We present a new 2D axisymmetric code, cuDisc, for studying protoplanetary discs, focusing on the self-consistent calculation of dust dynamics, grain size distribution and disc temperature. Self-consistently studying these physical processes is essential for many disc problems, such as structure formation and dust removal, given that the processes heavily depend on one another. To follow the evolution over substantial fractions of the disc lifetime, cuDisc uses the CUDA language and libraries to speed up the code through GPU acceleration. cuDisc employs a second-order finite-volume Godonuv solver for dust dynamics, solves the Smoluchowski equation for dust growth and calculates radiative transfer using a multi-frequency hybrid ray-tracing/flux-limited-diffusion method. We benchmark our code against current state-of-the-art codes. Through studying steady-state problems, we find that including 2D structure reveals that when collisions are important, the dust vertical structure appears to reach a diffusion-settling-coagulation equilibrium that can differ substantially from standard models that ignore coagulation. For low fragmentation velocities, we find an enhancement of intermediate-sized dust grains at heights of ~ 1 gas scale height due to the variation in collision rates with height, and for large fragmentation velocities, we find an enhancement of small grains around the disc mid-plane due to collisional ''sweeping'' of small grains by large grains. These results could be important for the analysis of disc SEDs or scattered light images, given these observables are sensitive to the vertical grain distribution.","sentences":["We present a new 2D axisymmetric code, cuDisc, for studying protoplanetary discs, focusing on the self-consistent calculation of dust dynamics, grain size distribution and disc temperature.","Self-consistently studying these physical processes is essential for many disc problems, such as structure formation and dust removal, given that the processes heavily depend on one another.","To follow the evolution over substantial fractions of the disc lifetime, cuDisc uses the CUDA language and libraries to speed up the code through GPU acceleration.","cuDisc employs a second-order finite-volume Godonuv solver for dust dynamics, solves the Smoluchowski equation for dust growth and calculates radiative transfer using a multi-frequency hybrid ray-tracing/flux-limited-diffusion method.","We benchmark our code against current state-of-the-art codes.","Through studying steady-state problems, we find that including 2D structure reveals that when collisions are important, the dust vertical structure appears to reach a diffusion-settling-coagulation equilibrium that can differ substantially from standard models that ignore coagulation.","For low fragmentation velocities, we find an enhancement of intermediate-sized dust grains at heights of ~","1 gas scale height due to the variation in collision rates with height, and for large fragmentation velocities, we find an enhancement of small grains around the disc mid-plane due to collisional ''sweeping'' of small grains by large grains.","These results could be important for the analysis of disc SEDs or scattered light images, given these observables are sensitive to the vertical grain distribution."],"url":"http://arxiv.org/abs/2402.18471v1","category":"astro-ph.EP"}
{"created":"2024-02-28 16:44:01","title":"Controllability for a non-local formulation of surface gravity waves","abstract":"In this paper, we study the approximate controllability of a system governed by an evolution problem known as the sloshing problem. This problem involves a spatial, nonlocal differential operator inherent in the dynamics of a two-dimensional, incompressible, non-viscous fluid within a confined domain. Our work establishes unique continuation results that enable the application of source control localized in an interior domain, allowing the aforementioned controllability.","sentences":["In this paper, we study the approximate controllability of a system governed by an evolution problem known as the sloshing problem.","This problem involves a spatial, nonlocal differential operator inherent in the dynamics of a two-dimensional, incompressible, non-viscous fluid within a confined domain.","Our work establishes unique continuation results that enable the application of source control localized in an interior domain, allowing the aforementioned controllability."],"url":"http://arxiv.org/abs/2402.18468v1","category":"math.AP"}
{"created":"2024-02-28 16:27:22","title":"A gallery of maximum-entropy distributions: 14 and 21 moments","abstract":"This work explores the different shapes that can be realized by the one-particle velocity distribution functions (VDFs) associated with the fourth-order maximum-entropy moment method. These distributions take the form of an exponential of a polynomial of the particle velocity, with terms up to the fourth-order. The 14- and 21-moment approximations are investigated. Various non-equilibrium gas states are probed throughout moment space. The resulting maximum-entropy distributions deviate strongly from the equilibrium VDF, and show a number of lobes and branches. The Maxwellian and the anisotropic Gaussian distributions are recovered as special cases. The eigenvalues associated with the maximum-entropy system of transport equations are also illustrated for some selected gas states. Anisotropic and/or asymmetric non-equilibrium states are seen to be associated with a non-uniform spacial propagation of perturbations.","sentences":["This work explores the different shapes that can be realized by the one-particle velocity distribution functions (VDFs) associated with the fourth-order maximum-entropy moment method.","These distributions take the form of an exponential of a polynomial of the particle velocity, with terms up to the fourth-order.","The 14- and 21-moment approximations are investigated.","Various non-equilibrium gas states are probed throughout moment space.","The resulting maximum-entropy distributions deviate strongly from the equilibrium VDF, and show a number of lobes and branches.","The Maxwellian and the anisotropic Gaussian distributions are recovered as special cases.","The eigenvalues associated with the maximum-entropy system of transport equations are also illustrated for some selected gas states.","Anisotropic and/or asymmetric non-equilibrium states are seen to be associated with a non-uniform spacial propagation of perturbations."],"url":"http://arxiv.org/abs/2402.18453v1","category":"math-ph"}
{"created":"2024-02-28 16:02:56","title":"On Affinely Homogeneous Submanifolds: The Power Series Method of Equivalence","abstract":"We determine all affinely homogeneous models for surfaces $S^2 \\subset \\mathbb{R}^4$, including the simply transitive models. We employ an improved power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations. We find several inequivalent terminal branches yielding each to some nonempty moduli space of homogeneous models, sometimes parametrized by a certain invariant algebraic variety. Three main features may be emphasized: 1) Iterated single-pointed jet bundles; 2) Cartan-enhanced power series method of equivalence; 3) Constant ping-pong between normal forms (nf) and vector fields (vf).","sentences":["We determine all affinely homogeneous models for surfaces $S^2 \\subset \\mathbb{R}^4$, including the simply transitive models.","We employ an improved power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations.","We find several inequivalent terminal branches yielding each to some nonempty moduli space of homogeneous models, sometimes parametrized by a certain invariant algebraic variety.","Three main features may be emphasized: 1) Iterated single-pointed jet bundles; 2) Cartan-enhanced power series method of equivalence; 3) Constant ping-pong between normal forms (nf) and vector fields (vf)."],"url":"http://arxiv.org/abs/2402.18437v1","category":"math.DG"}
{"created":"2024-02-28 15:57:22","title":"Universal neural network potentials as descriptors: Towards scalable chemical property prediction using quantum and classical computers","abstract":"Accurate prediction of diverse chemical properties is crucial for advancing molecular design and materials discovery. Here we present a versatile approach that uses the intermediate information of a universal neural network potential as a general-purpose descriptor for chemical property prediction. Our method is based on the insight that by training a sophisticated neural network architecture for universal force fields, it learns transferable representations of atomic environments. We show that transfer learning with a graph neural network potential M3GNet achieves accuracy comparable to state-of-the-art methods for predicting the NMR chemical shifts of$^1$H, $^{13}$C, $^{15}$N, $^{17}$O, and $^{19}$F using quantum machine learning as well as a standard classical regression model, despite the compactness of its descriptors. This work provides an efficient way to accurately predict properties, potentially accelerating the discovery of new molecules and materials.","sentences":["Accurate prediction of diverse chemical properties is crucial for advancing molecular design and materials discovery.","Here we present a versatile approach that uses the intermediate information of a universal neural network potential as a general-purpose descriptor for chemical property prediction.","Our method is based on the insight that by training a sophisticated neural network architecture for universal force fields, it learns transferable representations of atomic environments.","We show that transfer learning with a graph neural network potential M3GNet achieves accuracy comparable to state-of-the-art methods for predicting the NMR chemical shifts of$^1$H, $^{13}$C, $^{15}$N, $^{17}$O, and $^{19}$F using quantum machine learning as well as a standard classical regression model, despite the compactness of its descriptors.","This work provides an efficient way to accurately predict properties, potentially accelerating the discovery of new molecules and materials."],"url":"http://arxiv.org/abs/2402.18433v1","category":"quant-ph"}
{"created":"2024-02-28 15:56:38","title":"NEC violation in $f(\\bar{R},\\bar{T})$ gravity in the context of a non-canonical theory via modified Raychaudhuri equation","abstract":"In this work, we have developed the Raychaudhuri equation in $f(\\bar{R},\\bar{T})$ gravity in the setting of a non-canonical theory, namely K-essence theory. We solve the modified Raychaudhuri equation for the additive form of $f(\\bar{R},\\bar{T})$, which is $f_{1}(\\bar{R})+f_{2}(\\bar{T})$. For this solution, we employ two different scale factors to give two types of $f(\\bar{R},\\bar{T})$ solutions. By conducting a viability test and analyzing energy conditions, we have determined that in the first scenario, the null energy condition (NEC) is violated between two regions where the NEC is satisfied. Additionally, we have observed that this violation of the NEC exhibits a symmetric property during the phase transition. These observations indicate that bouncing events may occur as a result of the symmetrical violation of the NEC during the expansion of the universe. Once again, this model indicates that resonant-type quantum tunneling may take place during the period where the NEC is violated. In the second scenario, our model indicates that the strong energy condition is violated, but the NEC and weak energy conditions are satisfied. The effective energy density decreases and is positive, while the effective pressure and equation of state parameters are negative. This suggests that the universe is expanding with acceleration and is dominated by dark energy.","sentences":["In this work, we have developed the Raychaudhuri equation in $f(\\bar{R},\\bar{T})$ gravity in the setting of a non-canonical theory, namely K-essence theory.","We solve the modified Raychaudhuri equation for the additive form of $f(\\bar{R},\\bar{T})$, which is $f_{1}(\\bar{R})+f_{2}(\\bar{T})$.","For this solution, we employ two different scale factors to give two types of $f(\\bar{R},\\bar{T})$ solutions.","By conducting a viability test and analyzing energy conditions, we have determined that in the first scenario, the null energy condition (NEC) is violated between two regions where the NEC is satisfied.","Additionally, we have observed that this violation of the NEC exhibits a symmetric property during the phase transition.","These observations indicate that bouncing events may occur as a result of the symmetrical violation of the NEC during the expansion of the universe.","Once again, this model indicates that resonant-type quantum tunneling may take place during the period where the NEC is violated.","In the second scenario, our model indicates that the strong energy condition is violated, but the NEC and weak energy conditions are satisfied.","The effective energy density decreases and is positive, while the effective pressure and equation of state parameters are negative.","This suggests that the universe is expanding with acceleration and is dominated by dark energy."],"url":"http://arxiv.org/abs/2402.18431v1","category":"gr-qc"}
{"created":"2024-02-28 15:55:02","title":"Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation","abstract":"Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students. To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models. Extensive experiments on four widely used benchmarks show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 BLEU scores respectively, and can also outperform the current best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding.","sentences":["Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT).","AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations.","NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation.","Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models.","However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models.","In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students.","To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models.","Extensive experiments on four widely used benchmarks show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 BLEU scores respectively, and can also outperform the current best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding."],"url":"http://arxiv.org/abs/2402.18428v1","category":"cs.CL"}
{"created":"2024-02-28 15:41:12","title":"CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots","abstract":"When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the challenges is kinematic modeling. Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue. Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications. By observing the topology within CDPRs, in this letter, we propose a graph-based representation to model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging Graph Neural Network (GNN). Extensive experiments are conducted on 3D and 2D CDPRs across various configurations, including under-constrained, fully-constrained, and over-constrained cases, in both simulation environments and real-world scenarios. The experimental results showcase that CafkNet can learn the internal topological information of CDPRs and accurately solve the FK problem as an FK solver. Furthermore, training the CafkNet model on partial configurations enables zero-shot generalization to other configurations. Lastly, CafkNet effectively bridges the sim2real gap by using both simulation data and part of real-world data. To the best of our knowledge, it is the first study that employs the GNN to solve the FK problem for CDPRs.","sentences":["When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the challenges is kinematic modeling.","Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue.","Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications.","By observing the topology within CDPRs, in this letter, we propose a graph-based representation to model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging Graph Neural Network (GNN).","Extensive experiments are conducted on 3D and 2D CDPRs across various configurations, including under-constrained, fully-constrained, and over-constrained cases, in both simulation environments and real-world scenarios.","The experimental results showcase that CafkNet can learn the internal topological information of CDPRs and accurately solve the FK problem as an FK solver.","Furthermore, training the CafkNet model on partial configurations enables zero-shot generalization to other configurations.","Lastly, CafkNet effectively bridges the sim2real gap by using both simulation data and part of real-world data.","To the best of our knowledge, it is the first study that employs the GNN to solve the FK problem for CDPRs."],"url":"http://arxiv.org/abs/2402.18420v1","category":"cs.RO"}
{"created":"2024-02-28 15:27:41","title":"WKB-based third order method for the highly oscillatory 1D stationary Schr\u00f6dinger equation","abstract":"This paper introduces an efficient high-order numerical method for solving the 1D stationary Schr\\\"odinger equation in the highly oscillatory regime. Building upon the ideas from [2], we first analytically transform the given equation into a smoother (i.e. less oscillatory) equation. By developing sufficiently accurate quadratures for several (iterated) oscillatory integrals occurring in the Picard approximation of the solution, we obtain a one-step method that is third order w.r.t. the step size. The accuracy and efficiency of the method are illustrated through several numerical examples.","sentences":["This paper introduces an efficient high-order numerical method for solving the 1D stationary Schr\\\"odinger equation in the highly oscillatory regime.","Building upon the ideas from [2], we first analytically transform the given equation into a smoother (i.e. less oscillatory) equation.","By developing sufficiently accurate quadratures for several (iterated) oscillatory integrals occurring in the Picard approximation of the solution, we obtain a one-step method that is third order w.r.t.","the step size.","The accuracy and efficiency of the method are illustrated through several numerical examples."],"url":"http://arxiv.org/abs/2402.18406v1","category":"math.NA"}
{"created":"2024-02-28 15:17:41","title":"Hamiltonian simulation for time-evolving partial differential equation by scalable quantum circuits","abstract":"Solving partial differential equations for extremely large-scale systems within a feasible computation time serves in accelerating engineering developments. Quantum computing algorithm, particularly the Hamiltonian simulation, is a potential and promising approach to achieve this purpose. Actually there are several proposals of Hamiltonian simulation with potential quantum speedup, but their detailed implementation and accordingly the detailed computational complexity are all somewhat unclear. This paper presents a method that enables us to explicitly implement the quantum circuit for Hamiltonian simulation; the key technique is the explicit gate construction of differential operators contained in the target partial differential equation. Moreover, we show that the space and time complexity of the constructed circuit is exponentially smaller than that of all classical algorithms. We also provide numerical experiments and an experiment on a real device for the wave equation to demonstrate the validity of our proposed method.","sentences":["Solving partial differential equations for extremely large-scale systems within a feasible computation time serves in accelerating engineering developments.","Quantum computing algorithm, particularly the Hamiltonian simulation, is a potential and promising approach to achieve this purpose.","Actually there are several proposals of Hamiltonian simulation with potential quantum speedup, but their detailed implementation and accordingly the detailed computational complexity are all somewhat unclear.","This paper presents a method that enables us to explicitly implement the quantum circuit for Hamiltonian simulation; the key technique is the explicit gate construction of differential operators contained in the target partial differential equation.","Moreover, we show that the space and time complexity of the constructed circuit is exponentially smaller than that of all classical algorithms.","We also provide numerical experiments and an experiment on a real device for the wave equation to demonstrate the validity of our proposed method."],"url":"http://arxiv.org/abs/2402.18398v1","category":"quant-ph"}
{"created":"2024-02-28 15:13:42","title":"Dual-IMU State Estimation for Relative Localization of Two Mobile Agents","abstract":"In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world.","sentences":["In this paper, we address the problem of relative localization of two mobile agents.","Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them.","Previous works, however, typically assumed known ego motion and ignored biases of the IMUs.","Instead, we study the most general case of unknown biases for both IMUs.","Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions.","Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency.","Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world."],"url":"http://arxiv.org/abs/2402.18394v2","category":"cs.RO"}
{"created":"2024-02-28 18:42:46","title":"Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks","abstract":"The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has evolved into a leading research platform. The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms. Consequently, the field is wide and disjointed, hindering direct comparison of methods and making it difficult to assess the state-of-the-art. Therefore, we aim to unify the field by surveying current approaches, describing common methods and providing benchmark results to facilitate clear comparison and establish a baseline for future work. We survey current work in F1TENTH racing in the classical and learning categories, explaining the different solution approaches. We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control (MPCC), follow-the-gap and end-to-end reinforcement learning. We provide an open-source evaluation of benchmark methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods and reward signal and training map for learning methods. The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the MPCC planner. Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain.","sentences":["The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has evolved into a leading research platform.","The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms.","Consequently, the field is wide and disjointed, hindering direct comparison of methods and making it difficult to assess the state-of-the-art.","Therefore, we aim to unify the field by surveying current approaches, describing common methods and providing benchmark results to facilitate clear comparison and establish a baseline for future work.","We survey current work in F1TENTH racing in the classical and learning categories, explaining the different solution approaches.","We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control (MPCC), follow-the-gap and end-to-end reinforcement learning.","We provide an open-source evaluation of benchmark methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods and reward signal and training map for learning methods.","The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the MPCC planner.","Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain."],"url":"http://arxiv.org/abs/2402.18558v1","category":"cs.RO"}
{"created":"2024-02-28 18:38:34","title":"Probabilistic work extraction on a classical oscillator beyond the second law","abstract":"We demonstrate experimentally that, applying optimal protocols which drive the system between two equilibrium states characterized by a free energy difference $\\Delta F$, we can maximize the probability of performing the transition between the two states with a work $W$ smaller than $\\Delta F$. The second law holds only on average, resulting in the inequality $\\langle W \\rangle \\geq \\Delta F$. The experiment is performed using an underdamped oscillator evolving in a double-well potential. We show that with a suitable choice of parameters the probability of obtaining trajectories with $W \\le \\Delta F$ can be larger than 90 %. Very fast protocols are a key feature to obtain these results which are explained in terms of the Jarzynski equality.","sentences":["We demonstrate experimentally that, applying optimal protocols which drive the system between two equilibrium states characterized by a free energy difference $\\Delta F$, we can maximize the probability of performing the transition between the two states with a work $W$ smaller than $\\Delta F$.","The second law holds only on average, resulting in the inequality","$\\langle W \\rangle \\geq \\Delta F$.","The experiment is performed using an underdamped oscillator evolving in a double-well potential.","We show that with a suitable choice of parameters the probability of obtaining trajectories with $W \\le \\Delta F$ can be larger than 90 %.","Very fast protocols are a key feature to obtain these results which are explained in terms of the Jarzynski equality."],"url":"http://arxiv.org/abs/2402.18556v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 18:36:07","title":"Extended Kalman filter -- Koopman operator for tractable stochastic optimal control","abstract":"It has been more than seven decades since the introduction of the theory of dual control \\cite{feldbaum1960dual}. Although it has provided rich insights to the fields of control, estimation, and system identification, dual control is generally computationally prohibitive. In recent years, however, the use of Koopman operator theory for control applications has been emerging. The paper presents a new reformulation of the stochastic optimal control problem that, employing the Koopman operator, yields a standard LQR problem with the dual control as its solution. We conclude the paper with a numerical example that demonstrates the effectiveness of the proposed approach, compared to certainty equivalence control, when applied to systems with varying observability.","sentences":["It has been more than seven decades since the introduction of the theory of dual control \\cite{feldbaum1960dual}.","Although it has provided rich insights to the fields of control, estimation, and system identification, dual control is generally computationally prohibitive.","In recent years, however, the use of Koopman operator theory for control applications has been emerging.","The paper presents a new reformulation of the stochastic optimal control problem that, employing the Koopman operator, yields a standard LQR problem with the dual control as its solution.","We conclude the paper with a numerical example that demonstrates the effectiveness of the proposed approach, compared to certainty equivalence control, when applied to systems with varying observability."],"url":"http://arxiv.org/abs/2402.18554v1","category":"eess.SY"}
{"created":"2024-02-28 18:31:16","title":"Stabilizing topological superconductivity in disordered spin-orbit coupled semiconductor-superconductor heterostructures","abstract":"We investigate theoretically a one-dimensional semiconductor-superconductor (SM-SC) heterostructure with Rashba spin-orbit coupling and parallel Zeeman field in the presence of disorder generated by random charged impurities and identify the optimal regimes for realizing topological superconductivity and Majorana zero modes. Using a Green's function approach, we show that upon increasing the disorder strength the stable topological superconducting phase characterized by robust end-to-end Majorana correlations \"migrates\" toward larger values of the Zeeman field and can be stabilized by increasing the effective SM-SC coupling. Based on these findings, we propose a strategy for accessing a regime characterized by well-separated Majorana zero modes that is based on (a) enhancing the strength of the effective SM-SC coupling (e.g., through interface engineering) and (b) expanding the range of accessible Zeeman fields (e.g., by enhancing the gyromagnetic ratio or optimizing the parent superconductor, to enable the application of larger magnetic fields). While this strategy may still require some reduction of the disorder strength, this requirement is significantly less strict than the corresponding requirement in a strategy that focuses exclusively on disorder reduction.","sentences":["We investigate theoretically a one-dimensional semiconductor-superconductor (SM-SC) heterostructure with Rashba spin-orbit coupling and parallel Zeeman field in the presence of disorder generated by random charged impurities and identify the optimal regimes for realizing topological superconductivity and Majorana zero modes.","Using a Green's function approach, we show that upon increasing the disorder strength the stable topological superconducting phase characterized by robust end-to-end Majorana correlations \"migrates\" toward larger values of the Zeeman field and can be stabilized by increasing the effective SM-SC coupling.","Based on these findings, we propose a strategy for accessing a regime characterized by well-separated Majorana zero modes that is based on (a) enhancing the strength of the effective SM-SC coupling (e.g., through interface engineering) and (b) expanding the range of accessible Zeeman fields (e.g., by enhancing the gyromagnetic ratio or optimizing the parent superconductor, to enable the application of larger magnetic fields).","While this strategy may still require some reduction of the disorder strength, this requirement is significantly less strict than the corresponding requirement in a strategy that focuses exclusively on disorder reduction."],"url":"http://arxiv.org/abs/2402.18549v2","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 18:17:25","title":"Quantum Gravity: are we there yet?","abstract":"The turn of the millennium was a time of optimism about an approach to noncommutative geometry inspired by rich mathematical objects called `quantum groups' and its applications to quantum spacetime. This would model quantum gravity effects as noncommutativity of spacetime coordinates and was arguably going to solve quantum gravity itself. It took a further 20 years from that point to develop a particularly suitable formalism of `quantum Riemannian geometry', but this was largely done and has begun to be used to construct baby quantum gravity models. In this article, we obtain new results for state of the art fuzzy sphere and n-gon models in this approach. We also review what are some elements of quantum gravity that we can already see and what are the critical conceptual and mathematical elements that are still missing to more fully achieve this goal.","sentences":["The turn of the millennium was a time of optimism about an approach to noncommutative geometry inspired by rich mathematical objects called `quantum groups' and its applications to quantum spacetime.","This would model quantum gravity effects as noncommutativity of spacetime coordinates and was arguably going to solve quantum gravity itself.","It took a further 20 years from that point to develop a particularly suitable formalism of `quantum Riemannian geometry', but this was largely done and has begun to be used to construct baby quantum gravity models.","In this article, we obtain new results for state of the art fuzzy sphere and n-gon models in this approach.","We also review what are some elements of quantum gravity that we can already see and what are the critical conceptual and mathematical elements that are still missing to more fully achieve this goal."],"url":"http://arxiv.org/abs/2402.18536v1","category":"gr-qc"}
{"created":"2024-02-28 18:14:06","title":"Constructing Bayesian Optimal Designs for Discrete Choice Experiments by Simulated Annealing","abstract":"Discrete Choice Experiments (DCEs) investigate the attributes that influence individuals' choices when selecting among various options. To enhance the quality of the estimated choice models, researchers opt for Bayesian optimal designs that utilize existing information about the attributes' preferences. Given the nonlinear nature of choice models, the construction of an appropriate design requires efficient algorithms. Among these, the Coordinate-Exchange (CE) algorithm is most commonly employed for constructing designs based on the multinomial logit model. Since this is a hill-climbing algorithm, obtaining better designs necessitates multiple random starting designs. This approach increases the algorithm's run-time, but may not lead to a significant improvement in results. We propose the use of a Simulated Annealing (SA) algorithm to construct Bayesian D-optimal designs. This algorithm accepts both superior and inferior solutions, avoiding premature convergence and allowing a more thorough exploration of potential designs. Consequently, it ultimately obtains higher-quality choice designs within the same time-frame. Our work represents the first application of an SA algorithm in constructing Bayesian optimal designs for DCEs. Through computational experiments and a real-life case study, we demonstrate that the SA designs consistently outperform the CE designs in terms of Bayesian D-efficiency, especially when the prior preference information is highly uncertain.","sentences":["Discrete Choice Experiments (DCEs) investigate the attributes that influence individuals' choices when selecting among various options.","To enhance the quality of the estimated choice models, researchers opt for Bayesian optimal designs that utilize existing information about the attributes' preferences.","Given the nonlinear nature of choice models, the construction of an appropriate design requires efficient algorithms.","Among these, the Coordinate-Exchange (CE) algorithm is most commonly employed for constructing designs based on the multinomial logit model.","Since this is a hill-climbing algorithm, obtaining better designs necessitates multiple random starting designs.","This approach increases the algorithm's run-time, but may not lead to a significant improvement in results.","We propose the use of a Simulated Annealing (SA) algorithm to construct Bayesian D-optimal designs.","This algorithm accepts both superior and inferior solutions, avoiding premature convergence and allowing a more thorough exploration of potential designs.","Consequently, it ultimately obtains higher-quality choice designs within the same time-frame.","Our work represents the first application of an SA algorithm in constructing Bayesian optimal designs for DCEs.","Through computational experiments and a real-life case study, we demonstrate that the SA designs consistently outperform the CE designs in terms of Bayesian D-efficiency, especially when the prior preference information is highly uncertain."],"url":"http://arxiv.org/abs/2402.18533v1","category":"stat.ME"}
{"created":"2024-02-28 18:12:33","title":"All electrical cooling of an optically levitated nanoparticle","abstract":"We implement an all electrical controller for 3D feedback cooling of an optically levitated nanoparticle capable of reaching sub-Kelvin temperatures for the center of mass motion. The controller is based on an optimal policy where state estimation is made by delayed position measurements. The method offers a simplified path for pre-cooling and decoupling the transverse degrees of freedom of the nanoparticle. Numerical simulations show that in an improved setup with quantum limited detection, 3D ground state cooling and all electrical quantum control can be achieved.","sentences":["We implement an all electrical controller for 3D feedback cooling of an optically levitated nanoparticle capable of reaching sub-Kelvin temperatures for the center of mass motion.","The controller is based on an optimal policy where state estimation is made by delayed position measurements.","The method offers a simplified path for pre-cooling and decoupling the transverse degrees of freedom of the nanoparticle.","Numerical simulations show that in an improved setup with quantum limited detection, 3D ground state cooling and all electrical quantum control can be achieved."],"url":"http://arxiv.org/abs/2402.18532v1","category":"quant-ph"}
{"created":"2024-02-28 18:08:52","title":"Reducing the Number of Qubits from $n^2$ to $n\\log_{2} (n)$ to Solve the Traveling Salesman Problem with Quantum Computers: A Proposal for Demonstrating Quantum Supremacy in the NISQ Era","abstract":"In our pursuit of quantum supremacy during the NISQ era, this research introduces a novel approach rooted in the Quantum Approximate Optimization Algorithm (QAOA) framework to address the Traveling Salesman Problem (TSP). By strategically reducing the requisite qubit count from $n^2$ to $n\\log_{2} (n)$, our QAOA-based algorithm not only contributes to the ongoing discourse on qubit efficiency but also demonstrates improved performance based on established metrics, underscoring its potential for achieving NISQ-era supremacy in solving real-world optimization challenges.","sentences":["In our pursuit of quantum supremacy during the NISQ era, this research introduces a novel approach rooted in the Quantum Approximate Optimization Algorithm (QAOA) framework to address the Traveling Salesman Problem (TSP).","By strategically reducing the requisite qubit count from $n^2$ to $n\\log_{2} (n)$, our QAOA-based algorithm not only contributes to the ongoing discourse on qubit efficiency but also demonstrates improved performance based on established metrics, underscoring its potential for achieving NISQ-era supremacy in solving real-world optimization challenges."],"url":"http://arxiv.org/abs/2402.18530v1","category":"quant-ph"}
{"created":"2024-02-28 18:08:03","title":"Gradient Reweighting: Towards Imbalanced Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.","sentences":["Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge.","A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance).","We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL.","Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning.","Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases.","To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data.","We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness."],"url":"http://arxiv.org/abs/2402.18528v1","category":"cs.CV"}
{"created":"2024-02-28 17:43:02","title":"A Primal-Dual Frank-Wolfe Algorithm for Linear Programming","abstract":"We present two first-order primal-dual algorithms for solving saddle point formulations of linear programs, namely FWLP (Frank-Wolfe Linear Programming) and FWLP-P. The former iteratively applies the Frank-Wolfe algorithm to both the primal and dual of the saddle point formulation of a standard-form LP. The latter is a modification of FWLP in which regularizing perturbations are used in computing the iterates. We show that FWLP-P converges to a primal-dual solution with error $O(1/\\sqrt{k})$ after $k$ iterations, while no convergence guarantees are provided for FWLP. We also discuss the advantages of using FWLP and FWLP-P for solving very large LPs. In particular, we argue that only part of the matrix $A$ is needed at each iteration, in contrast to other first-order methods.","sentences":["We present two first-order primal-dual algorithms for solving saddle point formulations of linear programs, namely FWLP (Frank-Wolfe Linear Programming) and FWLP-P.","The former iteratively applies the Frank-Wolfe algorithm to both the primal and dual of the saddle point formulation of a standard-form LP.","The latter is a modification of FWLP in which regularizing perturbations are used in computing the iterates.","We show that FWLP-P converges to a primal-dual solution with error $O(1/\\sqrt{k})$ after $k$ iterations, while no convergence guarantees are provided for FWLP.","We also discuss the advantages of using FWLP and FWLP-P for solving very large LPs.","In particular, we argue that only part of the matrix $A$ is needed at each iteration, in contrast to other first-order methods."],"url":"http://arxiv.org/abs/2402.18514v1","category":"math.OC"}
{"created":"2024-02-28 17:34:21","title":"Evolving machine learning workflows through interactive AutoML","abstract":"Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters. This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P). Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included. In this paper we present \\ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest. Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML. To evaluate our approach, we present an experimental study in which 20 participants interact with \\ourmethod to evolve workflows according to their preferences. Our results confirm that the collaboration between \\ourmethod and humans allows us to find high-performance workflows in terms of accuracy that require less tuning time than those found without human intervention.","sentences":["Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters.","This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P).","Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included.","In this paper we present \\ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest.","Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML.","To evaluate our approach, we present an experimental study in which 20 participants interact with \\ourmethod to evolve workflows according to their preferences.","Our results confirm that the collaboration between \\ourmethod and humans allows us to find high-performance workflows in terms of accuracy that require less tuning time than those found without human intervention."],"url":"http://arxiv.org/abs/2402.18505v1","category":"cs.LG"}
{"created":"2024-02-28 17:19:26","title":"Dynamical Regimes of Diffusion Models","abstract":"Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.","sentences":["Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally.","Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process.","The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions.","It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase.","For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data.","The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models.","Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions."],"url":"http://arxiv.org/abs/2402.18491v1","category":"cs.LG"}
{"created":"2024-02-28 17:00:33","title":"Libfork: portable continuation-stealing with stackless coroutines","abstract":"Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.","sentences":["Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling.","The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques.","We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks.","We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks.","Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory.","Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory.","Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers."],"url":"http://arxiv.org/abs/2402.18480v1","category":"cs.DC"}
{"created":"2024-02-28 16:16:50","title":"Entanglement cost of discriminating quantum states under locality constraints","abstract":"The unique features of entanglement and non-locality in quantum systems, where there are pairs of bipartite states perfectly distinguishable by general entangled measurements yet indistinguishable by local operations and classical communication, hold significant importance in quantum entanglement theory, distributed quantum information processing, and quantum data hiding. This paper delves into the entanglement cost for discriminating two bipartite quantum states, employing positive operator-valued measures (POVMs) with positive partial transpose (PPT) to achieve optimal success probability through general entangled measurements. First, we introduce an efficiently computable quantity called the spectral PPT-distance of a POVM to quantify the localness of a general measurement. We show that it can be a lower bound for the entanglement cost of optimal discrimination by PPT POVMs. Second, we establish an upper bound on the entanglement cost of optimal discrimination by PPT POVMs for any pair of states. Leveraging this result, we show that a pure state can be optimally discriminated against any other state with the assistance of a single Bell state. This study advances our understanding of the pivotal role played by entanglement in quantum state discrimination, serving as a crucial element in unlocking quantum data hiding against locally constrained measurements.","sentences":["The unique features of entanglement and non-locality in quantum systems, where there are pairs of bipartite states perfectly distinguishable by general entangled measurements yet indistinguishable by local operations and classical communication, hold significant importance in quantum entanglement theory, distributed quantum information processing, and quantum data hiding.","This paper delves into the entanglement cost for discriminating two bipartite quantum states, employing positive operator-valued measures (POVMs) with positive partial transpose (PPT) to achieve optimal success probability through general entangled measurements.","First, we introduce an efficiently computable quantity called the spectral PPT-distance of a POVM to quantify the localness of a general measurement.","We show that it can be a lower bound for the entanglement cost of optimal discrimination by PPT POVMs.","Second, we establish an upper bound on the entanglement cost of optimal discrimination by PPT POVMs for any pair of states.","Leveraging this result, we show that a pure state can be optimally discriminated against any other state with the assistance of a single Bell state.","This study advances our understanding of the pivotal role played by entanglement in quantum state discrimination, serving as a crucial element in unlocking quantum data hiding against locally constrained measurements."],"url":"http://arxiv.org/abs/2402.18446v1","category":"quant-ph"}
{"created":"2024-02-28 15:44:33","title":"An interior-point trust-region method for nonsmooth regularized bound-constrained optimization","abstract":"We develop an interior-point method for nonsmooth regularized bound-constrained optimization problems. Our method consists of iteratively solving a sequence of unconstrained nonsmooth barrier subproblems. We use a variant of the proximal quasi-Newton trust-region algorithm TR of arXiv:2103.15993v3 to solve the barrier subproblems, with additional assumptions inspired from well-known smooth interior-point trust-region methods. We show global convergence of our algorithm with respect to the criticality measure of arXiv:2103.15993v3. Under an additional assumption linked to the convexity of the nonsmooth term in the objective, we present an alternative interior-point algorithm with a slightly modified criticality measure, which performs better in practice. Numerical experiments show that our algorithm performs better than the trust-region method TR, the trust-region method with diagonal hessian approximations TRDH of arXiv:2309.08433, and the quadratic regularization method R2 of arXiv:2103.15993v3 for two out of four tested bound-constrained problems. On those two problems, our algorithm obtains smaller objective values than the other solvers using fewer objective and gradient evaluations. On the two other problems, it performs similarly to TR, R2 and TRDH.","sentences":["We develop an interior-point method for nonsmooth regularized bound-constrained optimization problems.","Our method consists of iteratively solving a sequence of unconstrained nonsmooth barrier subproblems.","We use a variant of the proximal quasi-Newton trust-region algorithm TR of arXiv:2103.15993v3 to solve the barrier subproblems, with additional assumptions inspired from well-known smooth interior-point trust-region methods.","We show global convergence of our algorithm with respect to the criticality measure of arXiv:2103.15993v3.","Under an additional assumption linked to the convexity of the nonsmooth term in the objective, we present an alternative interior-point algorithm with a slightly modified criticality measure, which performs better in practice.","Numerical experiments show that our algorithm performs better than the trust-region method TR, the trust-region method with diagonal hessian approximations TRDH of arXiv:2309.08433, and the quadratic regularization method R2 of arXiv:2103.15993v3 for two out of four tested bound-constrained problems.","On those two problems, our algorithm obtains smaller objective values than the other solvers using fewer objective and gradient evaluations.","On the two other problems, it performs similarly to TR, R2 and TRDH."],"url":"http://arxiv.org/abs/2402.18423v1","category":"math.OC"}
{"created":"2024-02-28 15:32:59","title":"ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization","abstract":"This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control. The main contribution lies in demonstrating how this approach simplifies the black-box optimization problem while ensuring \"always-feasible\" setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints. Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to \"some\" setpoint using simple feedback controllers. Bayesian optimization then computes the optimum setpoint for the constraint controllers. By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme. In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the Gaussian process model used in Bayesian optimization. The effectiveness of the proposed approach is demonstrated on a benchmark Williams-Otto reactor example.","sentences":["This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control.","The main contribution lies in demonstrating how this approach simplifies the black-box optimization problem while ensuring \"always-feasible\" setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints.","Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to \"some\" setpoint using simple feedback controllers.","Bayesian optimization then computes the optimum setpoint for the constraint controllers.","By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme.","In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the Gaussian process model used in Bayesian optimization.","The effectiveness of the proposed approach is demonstrated on a benchmark Williams-Otto reactor example."],"url":"http://arxiv.org/abs/2402.18415v1","category":"math.OC"}
{"created":"2024-02-28 15:32:35","title":"QAOA with random and subgraph driver Hamiltonians","abstract":"The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems. The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians. In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom driver Hamiltonians, can affect the performance of QAOA. We derive an expected value formula for QAOA with custom driver Hamiltonians at p = 1 and show numerically that some of these custom drivers can achieve higher approximation ratio than the original algorithm implementation. Out of all the graphs tested, 0.036% of the random custom drivers, 75.9% of the subgraph custom drivers, 95.1% of the triangle-removed custom drivers, and 93.9% of the maximal degree edge-removed custom drivers have a higher approximation ratio than the original QAOA implementation. This finding opens up the question of whether better driver Hamiltonians can be designed to further improve the performance of QAOA.","sentences":["The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems.","The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians.","In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom driver Hamiltonians, can affect the performance of QAOA.","We derive an expected value formula for QAOA with custom driver Hamiltonians at p = 1 and show numerically that some of these custom drivers can achieve higher approximation ratio than the original algorithm implementation.","Out of all the graphs tested, 0.036% of the random custom drivers, 75.9% of the subgraph custom drivers, 95.1% of the triangle-removed custom drivers, and 93.9% of the maximal degree edge-removed custom drivers have a higher approximation ratio than the original QAOA implementation.","This finding opens up the question of whether better driver Hamiltonians can be designed to further improve the performance of QAOA."],"url":"http://arxiv.org/abs/2402.18412v1","category":"quant-ph"}
{"created":"2024-02-28 15:31:45","title":"Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport","abstract":"Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT.","sentences":["Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data.","Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment.","However, these segregated strategies overlook the potential synergies between these tasks.","This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework.","ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR.","By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios.","Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning.","This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness.","ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets.","Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%.","Code is available at https://github.com/HCVLAB/ProtoOT."],"url":"http://arxiv.org/abs/2402.18411v1","category":"cs.CV"}
{"created":"2024-02-28 15:27:20","title":"Multi-cell Coordinated Joint Sensing and Communications","abstract":"This paper proposes block-level precoder (BLP) designs for a multi-input single-output (MISO) system that performs joint sensing and communication across multiple cells and users. The Cramer-Rao-Bound for estimating a target's azimuth angle is determined for coordinated beamforming (CBF) and coordinated multi-point (CoMP) scenarios while considering inter-cell communication and sensing links. The formulated optimization problems to minimize the CRB and maximize the minimum-signal-to-interference-plus-noise-ratio (SINR) are non-convex and are represented in the semidefinite relaxed (SDR) form to solve using an alternate optimization algorithm. The proposed solutions show improved performance compared to the baseline scenario that neglects the signal component from neighboring cells.","sentences":["This paper proposes block-level precoder (BLP) designs for a multi-input single-output (MISO) system that performs joint sensing and communication across multiple cells and users.","The Cramer-Rao-Bound for estimating a target's azimuth angle is determined for coordinated beamforming (CBF) and coordinated multi-point (CoMP) scenarios while considering inter-cell communication and sensing links.","The formulated optimization problems to minimize the CRB and maximize the minimum-signal-to-interference-plus-noise-ratio (SINR) are non-convex and are represented in the semidefinite relaxed (SDR) form to solve using an alternate optimization algorithm.","The proposed solutions show improved performance compared to the baseline scenario that neglects the signal component from neighboring cells."],"url":"http://arxiv.org/abs/2402.18405v1","category":"cs.IT"}
{"created":"2024-02-28 15:25:23","title":"Preconditioned iterative solvers for constrained high-order implicit shock tracking methods","abstract":"High-order implicit shock tracking (fitting) is a class of high-order numerical methods that use numerical optimization to simultaneously compute a high-order approximation to a conservation law solution and align elements of the computational mesh with non-smooth features. This alignment ensures that non-smooth features are perfectly represented by inter-element jumps and high-order basis functions approximate smooth regions of the solution without nonlinear stabilization, which leads to accurate approximations on traditionally coarse meshes. In this work, we devise a family of preconditioners for the saddle point linear system that defines the step toward optimality at each iteration of the optimization solver so Krylov solvers can be effectively used. Our preconditioners integrate standard preconditioners from constrained optimization with popular preconditioners for discontinuous Galerkin discretizations such as block Jacobi, block incomplete LU factorizations with minimum discarded fill reordering, and p-multigrid. Thorough studies are performed using two inviscid compressible flow problems to evaluate the effectivity of each preconditioner in this family and their sensitivity to critical shock tracking parameters such as the mesh and Hessian regularization, linearization state, and resolution of the solution space.","sentences":["High-order implicit shock tracking (fitting) is a class of high-order numerical methods that use numerical optimization to simultaneously compute a high-order approximation to a conservation law solution and align elements of the computational mesh with non-smooth features.","This alignment ensures that non-smooth features are perfectly represented by inter-element jumps and high-order basis functions approximate smooth regions of the solution without nonlinear stabilization, which leads to accurate approximations on traditionally coarse meshes.","In this work, we devise a family of preconditioners for the saddle point linear system that defines the step toward optimality at each iteration of the optimization solver so Krylov solvers can be effectively used.","Our preconditioners integrate standard preconditioners from constrained optimization with popular preconditioners for discontinuous Galerkin discretizations such as block Jacobi, block incomplete LU factorizations with minimum discarded fill reordering, and p-multigrid.","Thorough studies are performed using two inviscid compressible flow problems to evaluate the effectivity of each preconditioner in this family and their sensitivity to critical shock tracking parameters such as the mesh and Hessian regularization, linearization state, and resolution of the solution space."],"url":"http://arxiv.org/abs/2402.18403v1","category":"math.NA"}
{"created":"2024-02-28 15:08:58","title":"Discovery of an extended Horizontal Branch in the Large Magellanic Cloud globular cluster NGC1835","abstract":"We present a high angular resolution multi-wavelength study of the massive globular cluster NGC 1835 in the Large Magellanic Cloud. Thanks to a combination of optical and near ultraviolet images acquired with the WFC3 on board the HST, we performed a detailed inspection of the stellar population in this stellar system adopting a ``UV-guided search'' to optimize the detection of relatively hot stars. This allowed us to discover a remarkably extended horizontal branch (HB), spanning more than 4.5 magnitudes in both magnitude and colour from the region redder than the instability strip, up to effective temperatures of 30,000 K, and including a large population of RR Lyrae (67 confirmed variables, and 52 new candidates). This is the first time that such a feature has been detected in an extra-Galactic cluster, demonstrating that the physical conditions responsible for the formation of extended HBs are ubiquitous. The acquired dataset has been also used to redetermine the cluster distance modulus, reddening, and absolute age, yielding $(m-M)_0=18.58$, $E(B-V)=0.08$, and $t=12.5$ Gyr, respectively.","sentences":["We present a high angular resolution multi-wavelength study of the massive globular cluster NGC 1835 in the Large Magellanic Cloud.","Thanks to a combination of optical and near ultraviolet images acquired with the WFC3 on board the HST, we performed a detailed inspection of the stellar population in this stellar system adopting a ``UV-guided search'' to optimize the detection of relatively hot stars.","This allowed us to discover a remarkably extended horizontal branch (HB), spanning more than 4.5 magnitudes in both magnitude and colour from the region redder than the instability strip, up to effective temperatures of 30,000 K, and including a large population of RR Lyrae (67 confirmed variables, and 52 new candidates).","This is the first time that such a feature has been detected in an extra-Galactic cluster, demonstrating that the physical conditions responsible for the formation of extended HBs are ubiquitous.","The acquired dataset has been also used to redetermine the cluster distance modulus, reddening, and absolute age, yielding $(m-M)_0=18.58$, $E(B-V)=0.08$, and $t=12.5$ Gyr, respectively."],"url":"http://arxiv.org/abs/2402.18389v1","category":"astro-ph.GA"}
{"created":"2024-02-28 15:07:36","title":"Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing","abstract":"This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users. We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP). In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing. In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection. We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors. The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex. We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques. Simulations suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system. The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value.","sentences":["This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users.","We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP).","In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing.","In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection.","We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors.","The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex.","We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques.","Simulations suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system.","The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value."],"url":"http://arxiv.org/abs/2402.18387v1","category":"cs.IT"}
{"created":"2024-02-28 14:27:20","title":"Port-Based State Preparation and Applications","abstract":"We introduce Port-Based State Preparation (PBSP), a teleportation task where Alice holds a complete classical description of the target state and Bob's correction operations are restricted to only tracing out registers. We show a protocol that implements PBSP with error decreasing exponentially in the number of ports, in contrast to the polynomial trade-off for the related task of Port-Based Teleportation, and we prove that this is optimal when a maximally entangled resource state is used.   As an application, we introduce approximate Universal Programmable Hybrid Processors (UPHP). Here the goal is to encode a unitary as a quantum state, and the UPHP can apply this unitary to a quantum state when knowing its classical description. We give a construction that needs strictly less memory in terms of dimension than the optimal approximate Universal Programmable Quantum Processor achieving the same error. Additionally, we provide lower bounds for the optimal trade-off between memory and error of UPHPs.","sentences":["We introduce Port-Based State Preparation (PBSP), a teleportation task where Alice holds a complete classical description of the target state and Bob's correction operations are restricted to only tracing out registers.","We show a protocol that implements PBSP with error decreasing exponentially in the number of ports, in contrast to the polynomial trade-off for the related task of Port-Based Teleportation, and we prove that this is optimal when a maximally entangled resource state is used.   ","As an application, we introduce approximate Universal Programmable Hybrid Processors (UPHP).","Here the goal is to encode a unitary as a quantum state, and the UPHP can apply this unitary to a quantum state when knowing its classical description.","We give a construction that needs strictly less memory in terms of dimension than the optimal approximate Universal Programmable Quantum Processor achieving the same error.","Additionally, we provide lower bounds for the optimal trade-off between memory and error of UPHPs."],"url":"http://arxiv.org/abs/2402.18356v1","category":"quant-ph"}
{"created":"2024-02-28 13:59:20","title":"Probabilistic Bayesian optimal experimental design using conditional normalizing flows","abstract":"Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations.","sentences":["Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework.","Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs.","To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach.","This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution.","We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations."],"url":"http://arxiv.org/abs/2402.18337v1","category":"cs.LG"}
{"created":"2024-02-28 13:51:25","title":"Recursive GNNs for Learning Precoding Policies with Size-Generalizability","abstract":"Graph neural networks (GNNs) have been shown promising in optimizing power allocation and link scheduling with good size generalizability and low training complexity. These merits are important for learning wireless policies under dynamic environments, which partially come from the matched permutation equivariance (PE) properties of the GNNs to the policies to be learned. Nonetheless, it has been noticed in literature that only satisfying the PE property of a precoding policy in multi-antenna systems cannot ensure a GNN for learning precoding to be generalizable to the unseen number of users. Incorporating models with GNNs helps improve size generalizability, which however is only applicable to specific problems, settings, and algorithms. In this paper, we propose a framework of size generalizable GNNs for learning precoding policies that are purely data-driven and can learn wireless policies including but not limited to baseband and hybrid precoding in multi-user multi-antenna systems. To this end, we first find a special structure of each iteration of two numerical algorithms for optimizing precoding, from which we identify the key characteristics of a GNN that affect its size generalizability. Then, we design size-generalizable GNNs that are with these key characteristics and satisfy the PE properties of precoding policies in a recursive manner. Simulation results show that the proposed GNNs can be well-generalized to the number of users for learning baseband and hybrid precoding policies and require much fewer samples than existing counterparts to achieve the same performance.","sentences":["Graph neural networks (GNNs) have been shown promising in optimizing power allocation and link scheduling with good size generalizability and low training complexity.","These merits are important for learning wireless policies under dynamic environments, which partially come from the matched permutation equivariance (PE) properties of the GNNs to the policies to be learned.","Nonetheless, it has been noticed in literature that only satisfying the PE property of a precoding policy in multi-antenna systems cannot ensure a GNN for learning precoding to be generalizable to the unseen number of users.","Incorporating models with GNNs helps improve size generalizability, which however is only applicable to specific problems, settings, and algorithms.","In this paper, we propose a framework of size generalizable GNNs for learning precoding policies that are purely data-driven and can learn wireless policies including but not limited to baseband and hybrid precoding in multi-user multi-antenna systems.","To this end, we first find a special structure of each iteration of two numerical algorithms for optimizing precoding, from which we identify the key characteristics of a GNN that affect its size generalizability.","Then, we design size-generalizable GNNs that are with these key characteristics and satisfy the PE properties of precoding policies in a recursive manner.","Simulation results show that the proposed GNNs can be well-generalized to the number of users for learning baseband and hybrid precoding policies and require much fewer samples than existing counterparts to achieve the same performance."],"url":"http://arxiv.org/abs/2402.18332v1","category":"eess.SP"}
{"created":"2024-02-28 13:11:06","title":"Escaping Local Optima in Global Placement","abstract":"Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics. Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement. However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results. This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively. The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks.","sentences":["Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics.","Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement.","However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results.","This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively.","The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks."],"url":"http://arxiv.org/abs/2402.18311v1","category":"cs.LG"}
{"created":"2024-02-28 13:07:51","title":"A restricted memory quasi-Newton bundle method for nonsmooth optimization on Riemannian manifolds","abstract":"In this paper, a restricted memory quasi-Newton bundle method for minimizing a locally Lipschitz function over a Riemannian manifold is proposed, which extends the classical one in Euclidean spaces to the manifold setting. The curvature information of the objective function is approximated by applying the Riemannian version of the quasi-Newton updating formulas. The subgradient aggregation technique is used to avoid solving the time-consuming quadratic programming subproblem when calculating the candidate descent direction. Moreover, a new Riemannian line search procedure is proposed to generate the stepsizes, and the process is finitely terminated under a new version of the Riemannian semismooth assumption. Global convergence of the proposed method is established: if the serious iteration steps are finite, then the last serious iterate is stationary; otherwise, every accumulation point of the serious iteration sequence is stationary. Finally, some preliminary numerical results show that the proposed method is efficient.","sentences":["In this paper, a restricted memory quasi-Newton bundle method for minimizing a locally Lipschitz function over a Riemannian manifold is proposed, which extends the classical one in Euclidean spaces to the manifold setting.","The curvature information of the objective function is approximated by applying the Riemannian version of the quasi-Newton updating formulas.","The subgradient aggregation technique is used to avoid solving the time-consuming quadratic programming subproblem when calculating the candidate descent direction.","Moreover, a new Riemannian line search procedure is proposed to generate the stepsizes, and the process is finitely terminated under a new version of the Riemannian semismooth assumption.","Global convergence of the proposed method is established: if the serious iteration steps are finite, then the last serious iterate is stationary; otherwise, every accumulation point of the serious iteration sequence is stationary.","Finally, some preliminary numerical results show that the proposed method is efficient."],"url":"http://arxiv.org/abs/2402.18308v1","category":"math.OC"}
{"created":"2024-02-28 12:57:22","title":"Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning","abstract":"In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge. The primary goal is to minimize communication overhead and runtime cost. However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead. Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning. This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable. Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types. In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning. S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets. Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game. Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints. For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines. Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph.","sentences":["In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge.","The primary goal is to minimize communication overhead and runtime cost.","However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead.","Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning.","This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable.","Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types.","In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning.","S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets.","Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game.","Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints.","For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines.","Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph."],"url":"http://arxiv.org/abs/2402.18304v1","category":"cs.DC"}
{"created":"2024-02-28 12:42:34","title":"Mapping between measurement scales in meta-analysis, with application to measures of body mass index in children","abstract":"Quantitative evidence synthesis methods aim to combine data from multiple medical trials to infer relative effects of different interventions. A challenge arises when trials report continuous outcomes on different measurement scales. To include all evidence in one coherent analysis, we require methods to `map' the outcomes onto a single scale. This is particularly challenging when trials report aggregate rather than individual data. We are motivated by a meta-analysis of interventions to prevent obesity in children. Trials report aggregate measurements of body mass index (BMI) either expressed as raw values or standardised for age and sex. We develop three methods for mapping between aggregate BMI data using known relationships between individual measurements on different scales. The first is an analytical method based on the mathematical definitions of z-scores and percentiles. The other two approaches involve sampling individual participant data on which to perform the conversions. One method is a straightforward sampling routine, while the other involves optimization with respect to the reported outcomes. In contrast to the analytical approach, these methods also have wider applicability for mapping between any pair of measurement scales with known or estimable individual-level relationships. We verify and contrast our methods using trials from our data set which report outcomes on multiple scales. We find that all methods recreate mean values with reasonable accuracy, but for standard deviations, optimization outperforms the other methods. However, the optimization method is more likely to underestimate standard deviations and is vulnerable to non-convergence.","sentences":["Quantitative evidence synthesis methods aim to combine data from multiple medical trials to infer relative effects of different interventions.","A challenge arises when trials report continuous outcomes on different measurement scales.","To include all evidence in one coherent analysis, we require methods to `map' the outcomes onto a single scale.","This is particularly challenging when trials report aggregate rather than individual data.","We are motivated by a meta-analysis of interventions to prevent obesity in children.","Trials report aggregate measurements of body mass index (BMI) either expressed as raw values or standardised for age and sex.","We develop three methods for mapping between aggregate BMI data using known relationships between individual measurements on different scales.","The first is an analytical method based on the mathematical definitions of z-scores and percentiles.","The other two approaches involve sampling individual participant data on which to perform the conversions.","One method is a straightforward sampling routine, while the other involves optimization with respect to the reported outcomes.","In contrast to the analytical approach, these methods also have wider applicability for mapping between any pair of measurement scales with known or estimable individual-level relationships.","We verify and contrast our methods using trials from our data set which report outcomes on multiple scales.","We find that all methods recreate mean values with reasonable accuracy, but for standard deviations, optimization outperforms the other methods.","However, the optimization method is more likely to underestimate standard deviations and is vulnerable to non-convergence."],"url":"http://arxiv.org/abs/2402.18298v1","category":"stat.ME"}
{"created":"2024-02-28 12:17:21","title":"Indirect Job-Shop coding using rank: application to QAOA (IQAOA)","abstract":"The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling. It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive graph, with earliest starting times computed using a longest path algorithm. The complexity of solving this problem arises in part from the requirement that disjunctive graphs representing solutions must be acyclic. Consequently, enumerating these graphs is feasible for small-scale instances only. A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the 'vector by repetition' (commonly known as Bierwith's vector). Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive graph, thereby enabling the mapping of a vector to a solution. This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive graphs. Our objective is to demonstrate how Bierwith's vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach.","sentences":["The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling.","It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive graph, with earliest starting times computed using a longest path algorithm.","The complexity of solving this problem arises in part from the requirement that disjunctive graphs representing solutions must be acyclic.","Consequently, enumerating these graphs is feasible for small-scale instances only.","A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the 'vector by repetition' (commonly known as Bierwith's vector).","Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive graph, thereby enabling the mapping of a vector to a solution.","This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive graphs.","Our objective is to demonstrate how Bierwith's vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach."],"url":"http://arxiv.org/abs/2402.18280v1","category":"quant-ph"}
{"created":"2024-02-28 12:04:10","title":"Necessary and sufficient conditions of extremum for polynomials and power series in the case of two variables","abstract":"The present paper is a continuation of the author's previous works, in which necessary and sufficient local extrema at a stationary point of a polynomial or a power series (and thus of an analytic function) are given. It is known that for the case of one variable, the necessary and sufficient conditions of the extremum are closing, i.e., they can be formulated as a single condition. The next most complicated case is the case with two variables, which is the one considered in this paper. In this case, many procedures, to which the verification of necessary and sufficient conditions is reduced, are based on the computation of real roots of a polynomial from one variable, as well as on the solution of some other rather simple practically realizable problems. An algorithm based on these procedures is described. Nevertheless, there are still cases where this algorithm \"doesn't work\". For such cases we propose the method of \"substitution of polynomials with uncertain coefficients\", using which, in particular, we have described an algorithm that allows us to unambiguously answer the question about the presence of a local minimum at a stationary point for a polynomial that is the sum of two A-quasi-homogeneous forms, where A - is a two-dimensional vector, whose components are natural numbers.","sentences":["The present paper is a continuation of the author's previous works, in which necessary and sufficient local extrema at a stationary point of a polynomial or a power series (and thus of an analytic function) are given.","It is known that for the case of one variable, the necessary and sufficient conditions of the extremum are closing, i.e., they can be formulated as a single condition.","The next most complicated case is the case with two variables, which is the one considered in this paper.","In this case, many procedures, to which the verification of necessary and sufficient conditions is reduced, are based on the computation of real roots of a polynomial from one variable, as well as on the solution of some other rather simple practically realizable problems.","An algorithm based on these procedures is described.","Nevertheless, there are still cases where this algorithm \"doesn't work\".","For such cases we propose the method of \"substitution of polynomials with uncertain coefficients\", using which, in particular, we have described an algorithm that allows us to unambiguously answer the question about the presence of a local minimum at a stationary point for a polynomial that is the sum of two A-quasi-homogeneous forms, where A - is a two-dimensional vector, whose components are natural numbers."],"url":"http://arxiv.org/abs/2402.18273v1","category":"math.OC"}
{"created":"2024-02-28 11:59:12","title":"FPM-WSI: Fourier ptychographic whole slide imaging via feature-domain backdiffraction","abstract":"Fourier ptychographic microscopy (FPM), characterized by high-throughput computational imaging, theoretically provides a cunning solution to the trade-off between spatial resolution and field of view (FOV), which has a promising prospect in the application of digital pathology. However, block reconstruction and then stitching has currently become an unavoidable procedure due to vignetting effects. The stitched image tends to present color inconsistency in different image segments, or even stitching artifacts. In response, we reported a computational framework based on feature-domain backdiffraction to realize full-FOV, stitching-free FPM reconstruction. Different from conventional algorithms that establish the loss function in the image domain, our method formulates it in the feature domain, where effective information of images is extracted by a feature extractor to bypass the vignetting effect. The feature-domain error between predicted images based on estimation of model parameters and practically captured images is then digitally diffracted back through the optical system for complex amplitude reconstruction and aberration compensation. Through massive simulations and experiments, the method presents effective elimination of vignetting artifacts, and reduces the requirement of precise knowledge of illumination positions. We also found its great potential to recover the data with a lower overlapping rate of spectrum and to realize automatic blind-digital refocusing without a prior defocus distance.","sentences":["Fourier ptychographic microscopy (FPM), characterized by high-throughput computational imaging, theoretically provides a cunning solution to the trade-off between spatial resolution and field of view (FOV), which has a promising prospect in the application of digital pathology.","However, block reconstruction and then stitching has currently become an unavoidable procedure due to vignetting effects.","The stitched image tends to present color inconsistency in different image segments, or even stitching artifacts.","In response, we reported a computational framework based on feature-domain backdiffraction to realize full-FOV, stitching-free FPM reconstruction.","Different from conventional algorithms that establish the loss function in the image domain, our method formulates it in the feature domain, where effective information of images is extracted by a feature extractor to bypass the vignetting effect.","The feature-domain error between predicted images based on estimation of model parameters and practically captured images is then digitally diffracted back through the optical system for complex amplitude reconstruction and aberration compensation.","Through massive simulations and experiments, the method presents effective elimination of vignetting artifacts, and reduces the requirement of precise knowledge of illumination positions.","We also found its great potential to recover the data with a lower overlapping rate of spectrum and to realize automatic blind-digital refocusing without a prior defocus distance."],"url":"http://arxiv.org/abs/2402.18270v1","category":"physics.optics"}
{"created":"2024-02-28 11:58:23","title":"Control sets of linear control systems on $\\R^2$. The real case","abstract":"In this paper, we study the dynamical behavior of a linear control system on $\\R^2$ when the associated matrix has real eigenvalues. Different from the complex case, we show that the position of the control zero relative to the control range can have a strong interference in such dynamics if the matrix is not invertible. In the invertible case, we explicitly construct the unique control set with a nonempty interior.","sentences":["In this paper, we study the dynamical behavior of a linear control system on $\\R^2$ when the associated matrix has real eigenvalues.","Different from the complex case, we show that the position of the control zero relative to the control range can have a strong interference in such dynamics if the matrix is not invertible.","In the invertible case, we explicitly construct the unique control set with a nonempty interior."],"url":"http://arxiv.org/abs/2402.18269v1","category":"math.OC"}
{"created":"2024-02-28 11:37:09","title":"The Cost of Permissionless Liquidity Provision in Automated Market Makers","abstract":"Automated market makers (AMMs) allocate fee revenue proportional to the amount of liquidity investors deposit. In this paper, we study the economic consequences of the competition between passive liquidity providers (LPs) caused by this allocation rule. We employ a game-theoretic model in which $N$ strategic agents optimally provide liquidity. In this setting, we find that competition drives LPs to provide excess liquidity. In the limit, the excess liquidity converges to a constant that linearly increases with the amount of base demand, demand that is insensitive to trading costs. Providing excess liquidity is costly as more capital is exposed to adverse selection costs, leading to a loss in welfare. Our main result is that the price of anarchy, defined over the liquidity provider performance, is $O(N)$, implying that the welfare loss scales linearly with the number of liquidity providers. We show that this result is still observable when using richer aggregate demand models.","sentences":["Automated market makers (AMMs) allocate fee revenue proportional to the amount of liquidity investors deposit.","In this paper, we study the economic consequences of the competition between passive liquidity providers (LPs) caused by this allocation rule.","We employ a game-theoretic model in which $N$ strategic agents optimally provide liquidity.","In this setting, we find that competition drives LPs to provide excess liquidity.","In the limit, the excess liquidity converges to a constant that linearly increases with the amount of base demand, demand that is insensitive to trading costs.","Providing excess liquidity is costly as more capital is exposed to adverse selection costs, leading to a loss in welfare.","Our main result is that the price of anarchy, defined over the liquidity provider performance, is $O(N)$, implying that the welfare loss scales linearly with the number of liquidity providers.","We show that this result is still observable when using richer aggregate demand models."],"url":"http://arxiv.org/abs/2402.18256v1","category":"cs.GT"}
{"created":"2024-02-28 11:21:32","title":"Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment","abstract":"This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of Reinforcement Learning (RL) and Graph Neural Networks (GNNs). The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees. This paper's original idea is to apply RL algorithms to solve a PRA model represented with a graph model. Given enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods. Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems.","sentences":["This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of Reinforcement Learning (RL) and Graph Neural Networks (GNNs).","The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees.","This paper's original idea is to apply RL algorithms to solve a PRA model represented with a graph model.","Given enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods.","Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems."],"url":"http://arxiv.org/abs/2402.18246v1","category":"eess.SY"}
{"created":"2024-02-28 10:53:50","title":"Joint Beamforming Design and Stream Allocation for Non-Coherent Joint Transmission in Cell-Free MIMO Networks","abstract":"We consider joint beamforming and stream allocation to maximize the weighted sum rate (WSR) for non-coherent joint transmission (NCJT) in user-centric cell-free MIMO networks, where distributed access points (APs) are organized in clusters to transmit different signals to serve each user equipment (UE). We for the first time consider the common limits of maximum number of receive streams at UEs in practical networks, and formulate a joint beamforming and transmit stream allocation problem for WSR maximization under per-AP transmit power constraints. Since the integer number of transmit streams determines the dimension of the beamformer, the joint optimization problem is mixed-integer and nonconvex with coupled decision variables that is inherently NP-hard. In this paper, we first propose a distributed low-interaction reduced weighted minimum mean square error (RWMMSE) beamforming algorithm for WSR maximization with fixed streams. Our proposed RWMMSE algorithm requires significantly less interaction across the network and has the current lowest computational complexity that scales linearly with the number of transmit antennas, without any compromise on WSR. We draw insights on the joint beamforming and stream allocation problem to decouple the decision variables and relax the mixed-integer constraints. We then propose a joint beamforming and linear stream allocation algorithm, termed as RWMMSE-LSA, which yields closed-form updates with linear stream allocation complexity and is guaranteed to converge to the stationary points of the original joint optimization problem. Simulation results demonstrate substantial performance gain of our proposed algorithms over the current best alternatives in both WSR performance and convergence time.","sentences":["We consider joint beamforming and stream allocation to maximize the weighted sum rate (WSR) for non-coherent joint transmission (NCJT) in user-centric cell-free MIMO networks, where distributed access points (APs) are organized in clusters to transmit different signals to serve each user equipment (UE).","We for the first time consider the common limits of maximum number of receive streams at UEs in practical networks, and formulate a joint beamforming and transmit stream allocation problem for WSR maximization under per-AP transmit power constraints.","Since the integer number of transmit streams determines the dimension of the beamformer, the joint optimization problem is mixed-integer and nonconvex with coupled decision variables that is inherently NP-hard.","In this paper, we first propose a distributed low-interaction reduced weighted minimum mean square error (RWMMSE) beamforming algorithm for WSR maximization with fixed streams.","Our proposed RWMMSE algorithm requires significantly less interaction across the network and has the current lowest computational complexity that scales linearly with the number of transmit antennas, without any compromise on WSR.","We draw insights on the joint beamforming and stream allocation problem to decouple the decision variables and relax the mixed-integer constraints.","We then propose a joint beamforming and linear stream allocation algorithm, termed as RWMMSE-LSA, which yields closed-form updates with linear stream allocation complexity and is guaranteed to converge to the stationary points of the original joint optimization problem.","Simulation results demonstrate substantial performance gain of our proposed algorithms over the current best alternatives in both WSR performance and convergence time."],"url":"http://arxiv.org/abs/2402.18231v1","category":"eess.SP"}
{"created":"2024-02-28 10:09:04","title":"Multi-objective Differentiable Neural Architecture Search","abstract":"Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.","sentences":["Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training.","Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices.","Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint.","In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run.","To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices.","Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method.","Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation."],"url":"http://arxiv.org/abs/2402.18213v1","category":"cs.LG"}
{"created":"2024-02-28 10:01:44","title":"Catastrophic Overfitting: A Potential Blessing in Disguise","abstract":"Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness. Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field. Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples. To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO. Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways. By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation. Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work. On this basis, we harness CO to achieve `attack obfuscation', aiming to bolster model performance. Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and adversarial data when adding random noise to inputs during evaluation. We also validate their robustness against transferred adversarial examples and the necessity of inducing CO to improve robustness. Hence, CO may not be a problem that has to be solved.","sentences":["Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness.","Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field.","Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples.","To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO.","Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways.","By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation.","Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work.","On this basis, we harness CO to achieve `attack obfuscation', aiming to bolster model performance.","Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and adversarial data when adding random noise to inputs during evaluation.","We also validate their robustness against transferred adversarial examples and the necessity of inducing CO to improve robustness.","Hence, CO may not be a problem that has to be solved."],"url":"http://arxiv.org/abs/2402.18211v1","category":"cs.LG"}
{"created":"2024-02-28 09:40:36","title":"Automated Machine Learning for Multi-Label Classification","abstract":"Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand. For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results. However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far. In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies. While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders.   In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most. This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods. Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification.   In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems.","sentences":["Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand.","For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results.","However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far.","In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies.","While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders.   ","In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most.","This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods.","Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification.   ","In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems."],"url":"http://arxiv.org/abs/2402.18198v1","category":"cs.LG"}
{"created":"2024-02-28 09:18:17","title":"Computational Offloading in Semantic-Aware Cloud-Edge-End Collaborative Networks","abstract":"The trend of massive connectivity pushes forward the explosive growth of end devices. The emergence of various applications has prompted a demand for pervasive connectivity and more efficient computing paradigms. On the other hand, the lack of computational capacity of the end devices restricts the implementation of the intelligent applications, and becomes a bottleneck of the multiple access for supporting massive connectivity. Mobile cloud computing (MCC) and mobile edge computing (MEC) techniques enable end devices to offload local computation-intensive tasks to servers by networks. In this paper, we consider the cloud-edge-end collaborative networks to utilize distributed computing resources. Furthermore, we apply task-oriented semantic communications to tackle the fast-varying channel between the end devices and MEC servers and reduce the communication cost. To minimize long-term energy consumption on constraints queue stability and computational delay, a Lyapunov-guided deep reinforcement learning hybrid (DRLH) framework is proposed to solve the mixed integer non-linear programming (MINLP) problem. The long-term energy consumption minimization problem is transformed into the deterministic problem in each time frame. The DRLH framework integrates a model-free deep reinforcement learning algorithm with a model-based mathematical optimization algorithm to mitigate computational complexity and leverage the scenario information, so that improving the convergence performance. Numerical results demonstrate that the proposed DRLH framework achieves near-optimal performance on energy consumption while stabilizing all queues.","sentences":["The trend of massive connectivity pushes forward the explosive growth of end devices.","The emergence of various applications has prompted a demand for pervasive connectivity and more efficient computing paradigms.","On the other hand, the lack of computational capacity of the end devices restricts the implementation of the intelligent applications, and becomes a bottleneck of the multiple access for supporting massive connectivity.","Mobile cloud computing (MCC) and mobile edge computing (MEC) techniques enable end devices to offload local computation-intensive tasks to servers by networks.","In this paper, we consider the cloud-edge-end collaborative networks to utilize distributed computing resources.","Furthermore, we apply task-oriented semantic communications to tackle the fast-varying channel between the end devices and MEC servers and reduce the communication cost.","To minimize long-term energy consumption on constraints queue stability and computational delay, a Lyapunov-guided deep reinforcement learning hybrid (DRLH) framework is proposed to solve the mixed integer non-linear programming (MINLP) problem.","The long-term energy consumption minimization problem is transformed into the deterministic problem in each time frame.","The DRLH framework integrates a model-free deep reinforcement learning algorithm with a model-based mathematical optimization algorithm to mitigate computational complexity and leverage the scenario information, so that improving the convergence performance.","Numerical results demonstrate that the proposed DRLH framework achieves near-optimal performance on energy consumption while stabilizing all queues."],"url":"http://arxiv.org/abs/2402.18183v1","category":"eess.SP"}
{"created":"2024-02-28 08:53:37","title":"Strategies for the alignment of electronic states in quantum-dot tunnel-injection lasers and their influence on the emission dynamics","abstract":"In quantum-dot tunnel-injection lasers, the excited charge carriers are efficiently captured from the bulk states via an injector quantum well and then transferred into the quantum dots via a tunnel barrier. The alignment of the electronic levels is crucial for the high efficiency of these processes and especially for the fast modulation dynamics of these lasers. In particular, the quantum mechanical nature of the tunneling process must be taken into account in the transition from two-dimensional quantum well states to zero-dimensional quantum dot states. This results in hybrid states, from which the scattering into the quantum-dot ground states takes place. We combine electronic state calculations of the tunnel-injection structures with many-body calculations of the scattering processes and insert this into a complete laser simulator. This allows us to study the influence of the level alignment and limitations due to inhomogeneous quantum-dot distributions. We find that the optimal alignment deviates from a simple picture in which the of the quantum-dot ground state energies are one LO-phonon energy below the injector quantum well ground state. \\author{Frank Jahnke","sentences":["In quantum-dot tunnel-injection lasers, the excited charge carriers are efficiently captured from the bulk states via an injector quantum well and then transferred into the quantum dots via a tunnel barrier.","The alignment of the electronic levels is crucial for the high efficiency of these processes and especially for the fast modulation dynamics of these lasers.","In particular, the quantum mechanical nature of the tunneling process must be taken into account in the transition from two-dimensional quantum well states to zero-dimensional quantum dot states.","This results in hybrid states, from which the scattering into the quantum-dot ground states takes place.","We combine electronic state calculations of the tunnel-injection structures with many-body calculations of the scattering processes and insert this into a complete laser simulator.","This allows us to study the influence of the level alignment and limitations due to inhomogeneous quantum-dot distributions.","We find that the optimal alignment deviates from a simple picture in which the of the quantum-dot ground state energies are one LO-phonon energy below the injector quantum well ground state.","\\author{Frank Jahnke"],"url":"http://arxiv.org/abs/2402.18165v1","category":"cond-mat.mes-hall"}
