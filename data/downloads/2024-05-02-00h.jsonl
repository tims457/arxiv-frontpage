{"created":"2024-04-30 17:59:58","title":"Generalized Symmetries in 2D from String Theory: SymTFTs, Intrinsic Relativeness, and Anomalies of Non-invertible Symmetries","abstract":"Generalized global symmetries, in particular non-invertible and categorical symmetries, have become a focal point in the recent study of quantum field theory (QFT). In this paper, we investigate aspects of symmetry topological field theories (SymTFTs) and anomalies of non-invertible symmetries for 2D QFTs from a string theory perspective. Our primary focus is on an infinite class of 2D QFTs engineered on D1-branes probing toric Calabi-Yau 4-fold singularities. We derive 3D SymTFTs from the topological sector of IIB supergravity and discuss the resulting 2D QFTs, which can be intrinsically relative or absolute. For intrinsically relative QFTs, we propose a sufficient condition for them to exist. For absolute QFTs, we show that they exhibit non-invertible symmetries with an elegant brane origin. Furthermore, we find that these non-invertible symmetries can suffer from anomalies, which we discuss from a top-down perspective. Explicit examples are provided, including theories for $Y^{(p,k)}(\\mathbb{P}^2)$, $Y^{(2,0)}(\\mathbb{P}^1\\times \\mathbb{P}^1)$, and $\\mathbb{C}^4/\\mathbb{Z}_4$ geometries.","sentences":["Generalized global symmetries, in particular non-invertible and categorical symmetries, have become a focal point in the recent study of quantum field theory (QFT).","In this paper, we investigate aspects of symmetry topological field theories (SymTFTs) and anomalies of non-invertible symmetries for 2D QFTs from a string theory perspective.","Our primary focus is on an infinite class of 2D QFTs engineered on D1-branes probing toric Calabi-Yau 4-fold singularities.","We derive 3D SymTFTs from the topological sector of IIB supergravity and discuss the resulting 2D QFTs, which can be intrinsically relative or absolute.","For intrinsically relative QFTs, we propose a sufficient condition for them to exist.","For absolute QFTs, we show that they exhibit non-invertible symmetries with an elegant brane origin.","Furthermore, we find that these non-invertible symmetries can suffer from anomalies, which we discuss from a top-down perspective.","Explicit examples are provided, including theories for $Y^{(p,k)}(\\mathbb{P}^2)$, $Y^{(2,0)}(\\mathbb{P}^1\\times \\mathbb{P}^1)$, and $\\mathbb{C}^4/\\mathbb{Z}_4$ geometries."],"url":"http://arxiv.org/abs/2404.19761v1","category":"hep-th"}
{"created":"2024-04-30 17:59:51","title":"Lightplane: Highly-Scalable Components for Neural 3D Fields","abstract":"Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision. However, current designs for these 2D-3D mapping are memory-intensive, posing a significant bottleneck for existing methods and hindering new applications. In response, we propose a pair of highly scalable components for 3D neural fields: Lightplane Render and Splatter, which significantly reduce memory usage in 2D-3D mapping. These innovations enable the processing of vastly more and higher resolution images with small memory and computational costs. We demonstrate their utility in various applications, from benefiting single-scene optimization with image-level losses to realizing a versatile pipeline for dramatically scaling 3D reconstruction and generation. Code: \\url{https://github.com/facebookresearch/lightplane}.","sentences":["Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision.","However, current designs for these 2D-3D mapping are memory-intensive, posing a significant bottleneck for existing methods and hindering new applications.","In response, we propose a pair of highly scalable components for 3D neural fields:","Lightplane Render and Splatter, which significantly reduce memory usage in 2D-3D mapping.","These innovations enable the processing of vastly more and higher resolution images with small memory and computational costs.","We demonstrate their utility in various applications, from benefiting single-scene optimization with image-level losses to realizing a versatile pipeline for dramatically scaling 3D reconstruction and generation.","Code: \\url{https://github.com/facebookresearch/lightplane}."],"url":"http://arxiv.org/abs/2404.19760v1","category":"cs.CV"}
{"created":"2024-04-30 17:59:47","title":"MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model","abstract":"This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD). By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.","sentences":["This work introduces MotionLCM, extending controllable motion generation to a real-time level.","Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency.","To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD).","By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation.","To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation.","By employing these techniques, our approach can generate human motions with text and control signals in real-time.","Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency."],"url":"http://arxiv.org/abs/2404.19759v1","category":"cs.CV"}
{"created":"2024-04-30 17:59:40","title":"Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting","abstract":"3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models. Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry. These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation. These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt. In this work, we make two fundamental contributions to the field of 3D scene generation. First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene. We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene. Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene.","sentences":["3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models.","Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry.","These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation.","These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt.","In this work, we make two fundamental contributions to the field of 3D scene generation.","First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene.","We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene.","Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene."],"url":"http://arxiv.org/abs/2404.19758v1","category":"cs.CV"}
{"created":"2024-04-30 17:58:29","title":"KAN: Kolmogorov-Arnold Networks","abstract":"Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.","sentences":["Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).","While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\").","KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline.","We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability.","For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving.","Theoretically and empirically, KANs possess faster neural scaling laws than MLPs.","For interpretability, KANs can be intuitively visualized and can easily interact with human users.","Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws.","In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs."],"url":"http://arxiv.org/abs/2404.19756v1","category":"cs.LG"}
{"created":"2024-04-30 17:58:06","title":"Succinct arguments for QMA from standard assumptions via compiled nonlocal games","abstract":"We construct a succinct classical argument system for QMA, the quantum analogue of NP, from generic and standard cryptographic assumptions. Previously, building on the prior work of Mahadev (FOCS '18), Bartusek et al. (CRYPTO '22) also constructed a succinct classical argument system for QMA. However, their construction relied on post-quantumly secure indistinguishability obfuscation, a very strong primitive which is not known from standard cryptographic assumptions. In contrast, the primitives we use (namely, collapsing hash functions and a mild version of quantum homomorphic encryption) are much weaker and are implied by standard assumptions such as LWE. Our protocol is constructed using a general transformation which was designed by Kalai et al. (STOC '23) as a candidate method to compile any quantum nonlocal game into an argument system. Our main technical contribution is to analyze the soundness of this transformation when it is applied to a succinct self-test for Pauli measurements on maximally entangled states, the latter of which is a key component in the proof of MIP*=RE in quantum complexity.","sentences":["We construct a succinct classical argument system for QMA, the quantum analogue of NP, from generic and standard cryptographic assumptions.","Previously, building on the prior work of Mahadev (FOCS '18), Bartusek et al.","(CRYPTO '22) also constructed a succinct classical argument system for QMA.","However, their construction relied on post-quantumly secure indistinguishability obfuscation, a very strong primitive which is not known from standard cryptographic assumptions.","In contrast, the primitives we use (namely, collapsing hash functions and a mild version of quantum homomorphic encryption) are much weaker and are implied by standard assumptions such as LWE.","Our protocol is constructed using a general transformation which was designed by Kalai et al.","(STOC '23) as a candidate method to compile any quantum nonlocal game into an argument system.","Our main technical contribution is to analyze the soundness of this transformation when it is applied to a succinct self-test for Pauli measurements on maximally entangled states, the latter of which is a key component in the proof of MIP*=RE in quantum complexity."],"url":"http://arxiv.org/abs/2404.19754v1","category":"quant-ph"}
{"created":"2024-04-30 17:56:24","title":"DOCCI: Descriptions of Connected and Contrasting Images","abstract":"Vision-language datasets are vital for both text-to-image (T2I) and image-to-text (I2T) research. However, current datasets lack descriptions with fine-grained detail that would allow for richer associations to be learned by models. To fill the gap, we introduce Descriptions of Connected and Contrasting Images (DOCCI), a dataset with long, human-annotated English descriptions for 15k images that were taken, curated and donated by a single researcher intent on capturing key challenges such as spatial relations, counting, text rendering, world knowledge, and more. We instruct human annotators to create comprehensive descriptions for each image; these average 136 words in length and are crafted to clearly distinguish each image from those that are related or similar. Each description is highly compositional and typically encompasses multiple challenges. Through both quantitative and qualitative analyses, we demonstrate that DOCCI serves as an effective training resource for image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for text-to-image generation, highlighting the limitations of current text-to-image models in capturing long descriptions and fine details.","sentences":["Vision-language datasets are vital for both text-to-image (T2I) and image-to-text (I2T) research.","However, current datasets lack descriptions with fine-grained detail that would allow for richer associations to be learned by models.","To fill the gap, we introduce Descriptions of Connected and Contrasting Images (DOCCI), a dataset with long, human-annotated English descriptions for 15k images that were taken, curated and donated by a single researcher intent on capturing key challenges such as spatial relations, counting, text rendering, world knowledge, and more.","We instruct human annotators to create comprehensive descriptions for each image; these average 136 words in length and are crafted to clearly distinguish each image from those that are related or similar.","Each description is highly compositional and typically encompasses multiple challenges.","Through both quantitative and qualitative analyses, we demonstrate that DOCCI serves as an effective training resource for image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and InstructBLIP 7B.","Furthermore, we show that DOCCI is a useful testbed for text-to-image generation, highlighting the limitations of current text-to-image models in capturing long descriptions and fine details."],"url":"http://arxiv.org/abs/2404.19753v1","category":"cs.CV"}
{"created":"2024-04-30 17:55:27","title":"Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation","abstract":"Existing automatic captioning methods for visual content face challenges such as lack of detail, content hallucination, and poor instruction following. In this work, we propose VisualFactChecker (VFC), a flexible training-free pipeline that generates high-fidelity and detailed captions for both 2D images and 3D objects. VFC consists of three steps: 1) proposal, where image-to-text captioning models propose multiple initial captions; 2) verification, where a large language model (LLM) utilizes tools such as object detection and VQA models to fact-check proposed captions; 3) captioning, where an LLM generates the final caption by summarizing caption proposals and the fact check verification results. In this step, VFC can flexibly generate captions in various styles following complex instructions. We conduct comprehensive captioning evaluations using four metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity between the original and the reconstructed image generated by a text-to-image model using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V for fine-grained evaluation. Evaluation results show that VFC outperforms state-of-the-art open-sourced captioning methods for 2D images on the COCO dataset and 3D assets on the Objaverse dataset. Our study demonstrates that by combining open-source models into a pipeline, we can attain captioning capability comparable to proprietary models such as GPT-4V, despite being over 10x smaller in model size.","sentences":["Existing automatic captioning methods for visual content face challenges such as lack of detail, content hallucination, and poor instruction following.","In this work, we propose VisualFactChecker (VFC), a flexible training-free pipeline that generates high-fidelity and detailed captions for both 2D images and 3D objects.","VFC consists of three steps: 1) proposal, where image-to-text captioning models propose multiple initial captions; 2) verification, where a large language model (LLM) utilizes tools such as object detection and VQA models to fact-check proposed captions; 3) captioning, where an LLM generates the final caption by summarizing caption proposals and the fact check verification results.","In this step, VFC can flexibly generate captions in various styles following complex instructions.","We conduct comprehensive captioning evaluations using four metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity between the original and the reconstructed image generated by a text-to-image model using the caption.","3) human study on Amazon Mechanical Turk; 4) GPT-4V for fine-grained evaluation.","Evaluation results show that VFC outperforms state-of-the-art open-sourced captioning methods for 2D images on the COCO dataset and 3D assets on the Objaverse dataset.","Our study demonstrates that by combining open-source models into a pipeline, we can attain captioning capability comparable to proprietary models such as GPT-4V, despite being over 10x smaller in model size."],"url":"http://arxiv.org/abs/2404.19752v1","category":"cs.CV"}
{"created":"2024-04-30 17:55:02","title":"A Joint Communication and Computation Design for Distributed RISs Assisted Probabilistic Semantic Communication in IIoT","abstract":"In this paper, the problem of spectral-efficient communication and computation resource allocation for distributed reconfigurable intelligent surfaces (RISs) assisted probabilistic semantic communication (PSC) in industrial Internet-of-Things (IIoT) is investigated. In the considered model, multiple RISs are deployed to serve multiple users, while PSC adopts compute-then-transmit protocol to reduce the transmission data size. To support high-rate transmission, the semantic compression ratio, transmit power allocation, and distributed RISs deployment must be jointly considered. This joint communication and computation problem is formulated as an optimization problem whose goal is to maximize the sum semantic-aware transmission rate of the system under total transmit power, phase shift, RIS-user association, and semantic compression ratio constraints. To solve this problem, a many-to-many matching scheme is proposed to solve the RIS-user association subproblem, the semantic compression ratio subproblem is addressed following greedy policy, while the phase shift of RIS can be optimized using the tensor based beamforming. Numerical results verify the superiority of the proposed algorithm.","sentences":["In this paper, the problem of spectral-efficient communication and computation resource allocation for distributed reconfigurable intelligent surfaces (RISs) assisted probabilistic semantic communication (PSC) in industrial Internet-of-Things (IIoT) is investigated.","In the considered model, multiple RISs are deployed to serve multiple users, while PSC adopts compute-then-transmit protocol to reduce the transmission data size.","To support high-rate transmission, the semantic compression ratio, transmit power allocation, and distributed RISs deployment must be jointly considered.","This joint communication and computation problem is formulated as an optimization problem whose goal is to maximize the sum semantic-aware transmission rate of the system under total transmit power, phase shift, RIS-user association, and semantic compression ratio constraints.","To solve this problem, a many-to-many matching scheme is proposed to solve the RIS-user association subproblem, the semantic compression ratio subproblem is addressed following greedy policy, while the phase shift of RIS can be optimized using the tensor based beamforming.","Numerical results verify the superiority of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.19750v1","category":"cs.IT"}
{"created":"2024-04-30 17:52:31","title":"Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning","abstract":"Every year, plant parasitic nematodes, one of the major groups of plant pathogens, cause a significant loss of crops worldwide. To mitigate crop yield losses caused by nematodes, an efficient nematode monitoring method is essential for plant and crop disease management. In other respects, efficient nematode detection contributes to medical research and drug discovery, as nematodes are model organisms. With the rapid development of computer technology, computer vision techniques provide a feasible solution for quantifying nematodes or nematode infections. In this paper, we survey and categorise the studies and available datasets on nematode detection through deep-learning models. To stimulate progress in related research, this survey presents the potential state-of-the-art object detection models, training techniques, optimisation techniques, and evaluation metrics for deep learning beginners. Moreover, seven state-of-the-art object detection models are validated on three public datasets and the AgriNema dataset for plant parasitic nematodes to construct a baseline for nematode detection.","sentences":["Every year, plant parasitic nematodes, one of the major groups of plant pathogens, cause a significant loss of crops worldwide.","To mitigate crop yield losses caused by nematodes, an efficient nematode monitoring method is essential for plant and crop disease management.","In other respects, efficient nematode detection contributes to medical research and drug discovery, as nematodes are model organisms.","With the rapid development of computer technology, computer vision techniques provide a feasible solution for quantifying nematodes or nematode infections.","In this paper, we survey and categorise the studies and available datasets on nematode detection through deep-learning models.","To stimulate progress in related research, this survey presents the potential state-of-the-art object detection models, training techniques, optimisation techniques, and evaluation metrics for deep learning beginners.","Moreover, seven state-of-the-art object detection models are validated on three public datasets and the AgriNema dataset for plant parasitic nematodes to construct a baseline for nematode detection."],"url":"http://arxiv.org/abs/2404.19748v1","category":"cs.CV"}
{"created":"2024-04-30 17:44:44","title":"PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification","abstract":"Data protection and privacy is becoming increasingly crucial in the digital era. Numerous companies depend on third-party vendors and service providers to carry out critical functions within their operations, encompassing tasks such as data handling and storage. However, this reliance introduces potential vulnerabilities, as these vendors' security measures and practices may not always align with the standards expected by regulatory bodies. Businesses are required, often under the penalty of law, to ensure compliance with the evolving regulatory rules. Interpreting and implementing these regulations pose challenges due to their complexity. Regulatory documents are extensive, demanding significant effort for interpretation, while vendor-drafted privacy policies often lack the detail required for full legal compliance, leading to ambiguity. To ensure a concise interpretation of the regulatory requirements and compliance of organizational privacy policy with said regulations, we propose a Large Language Model (LLM) and Semantic Web based approach for privacy compliance. In this paper, we develop the novel Privacy Policy Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to efficiently store and retrieve comprehensive information concerning privacy policies, regulatory frameworks, and domain-specific knowledge pertaining to the legal landscape of privacy. Using Retrieval Augmented Generation, we identify the relevant sections in a privacy policy with corresponding regulatory rules. This information about individual privacy policies is populated into the PrivComp-KG. Combining this with the domain context and rules, the PrivComp-KG can be queried to check for compliance with privacy policies by each vendor against relevant policy regulations. We demonstrate the relevance of the PrivComp-KG, by verifying compliance of privacy policy documents for various organizations.","sentences":["Data protection and privacy is becoming increasingly crucial in the digital era.","Numerous companies depend on third-party vendors and service providers to carry out critical functions within their operations, encompassing tasks such as data handling and storage.","However, this reliance introduces potential vulnerabilities, as these vendors' security measures and practices may not always align with the standards expected by regulatory bodies.","Businesses are required, often under the penalty of law, to ensure compliance with the evolving regulatory rules.","Interpreting and implementing these regulations pose challenges due to their complexity.","Regulatory documents are extensive, demanding significant effort for interpretation, while vendor-drafted privacy policies often lack the detail required for full legal compliance, leading to ambiguity.","To ensure a concise interpretation of the regulatory requirements and compliance of organizational privacy policy with said regulations, we propose a Large Language Model (LLM) and Semantic Web based approach for privacy compliance.","In this paper, we develop the novel Privacy Policy Compliance Verification Knowledge Graph, PrivComp-KG.","It is designed to efficiently store and retrieve comprehensive information concerning privacy policies, regulatory frameworks, and domain-specific knowledge pertaining to the legal landscape of privacy.","Using Retrieval Augmented Generation, we identify the relevant sections in a privacy policy with corresponding regulatory rules.","This information about individual privacy policies is populated into the PrivComp-KG.","Combining this with the domain context and rules, the PrivComp-KG can be queried to check for compliance with privacy policies by each vendor against relevant policy regulations.","We demonstrate the relevance of the PrivComp-KG, by verifying compliance of privacy policy documents for various organizations."],"url":"http://arxiv.org/abs/2404.19744v1","category":"cs.CR"}
{"created":"2024-04-30 17:37:21","title":"Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule Generation","abstract":"Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery. Diffusion models currently achieve state of the art performance for 3D molecule generation. In this work, we explore the use of flow matching, a recently proposed generative modeling framework that generalizes diffusion models, for the task of de novo molecule generation. Flow matching provides flexibility in model design; however, the framework is predicated on the assumption of continuously-valued data. 3D de novo molecule generation requires jointly sampling continuous and categorical variables such as atom position and atom type. We extend the flow matching framework to categorical data by constructing flows that are constrained to exist on a continuous representation of categorical data known as the probability simplex. We call this extension SimplexFlow. We explore the use of SimplexFlow for de novo molecule generation. However, we find that, in practice, a simpler approach that makes no accommodations for the categorical nature of the data yields equivalent or superior performance. As a result of these experiments, we present FlowMol, a flow matching model for 3D de novo generative model that achieves improved performance over prior flow matching methods, and we raise important questions about the design of prior distributions for achieving strong performance in flow matching models. Code and trained models for reproducing this work are available at https://github.com/dunni3/FlowMol","sentences":["Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery.","Diffusion models currently achieve state of the art performance for 3D molecule generation.","In this work, we explore the use of flow matching, a recently proposed generative modeling framework that generalizes diffusion models, for the task of de novo molecule generation.","Flow matching provides flexibility in model design; however, the framework is predicated on the assumption of continuously-valued data.","3D de novo molecule generation requires jointly sampling continuous and categorical variables such as atom position and atom type.","We extend the flow matching framework to categorical data by constructing flows that are constrained to exist on a continuous representation of categorical data known as the probability simplex.","We call this extension SimplexFlow.","We explore the use of SimplexFlow for de novo molecule generation.","However, we find that, in practice, a simpler approach that makes no accommodations for the categorical nature of the data yields equivalent or superior performance.","As a result of these experiments, we present FlowMol, a flow matching model for 3D de novo generative model that achieves improved performance over prior flow matching methods, and we raise important questions about the design of prior distributions for achieving strong performance in flow matching models.","Code and trained models for reproducing this work are available at https://github.com/dunni3/FlowMol"],"url":"http://arxiv.org/abs/2404.19739v1","category":"q-bio.BM"}
{"created":"2024-04-30 17:33:57","title":"Better & Faster Large Language Models via Multi-token Prediction","abstract":"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.","sentences":["Large language models such as GPT and Llama are trained with a next-token prediction loss.","In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency.","More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk.","Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models.","The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs.","Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points.","Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models.","Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities.","As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes."],"url":"http://arxiv.org/abs/2404.19737v1","category":"cs.CL"}
{"created":"2024-04-30 17:30:01","title":"Replica-assisted super-resolution fluorescence imaging in scattering media","abstract":"Far-field super-resolution fluorescence microscopy has been rapidly developed for applications ranging from cell biology to nanomaterials. However, it remains a significant challenge to achieve super-resolution imaging at depth in opaque materials. In this study, we present a super-resolution microscopy technique for imaging hidden fluorescent objects through scattering media, started by exploiting the inherent object replica generation arising from the memory effect, i.e. the seemingly informationless emission speckle can be regarded as a random superposition of multiple object copies. Inspired by the concept of super-resolution optical fluctuation imaging, we use temporally-fluctuating speckles to excite fluorescent signals and perform high-order cumulant analysis on the fluctuation, which can not only improve the image resolution, but also increase the speckle contrast to isolate only the bright object replicas. A super-resolved image can be finally retrieved by simply unmixing the sparsely distributed replicas with their location map. This methodology allows to overcome scattering and achieve robust super-resolution fluorescence imaging, circumventing the need of heavy computational steps.","sentences":["Far-field super-resolution fluorescence microscopy has been rapidly developed for applications ranging from cell biology to nanomaterials.","However, it remains a significant challenge to achieve super-resolution imaging at depth in opaque materials.","In this study, we present a super-resolution microscopy technique for imaging hidden fluorescent objects through scattering media, started by exploiting the inherent object replica generation arising from the memory effect, i.e. the seemingly informationless emission speckle can be regarded as a random superposition of multiple object copies.","Inspired by the concept of super-resolution optical fluctuation imaging, we use temporally-fluctuating speckles to excite fluorescent signals and perform high-order cumulant analysis on the fluctuation, which can not only improve the image resolution, but also increase the speckle contrast to isolate only the bright object replicas.","A super-resolved image can be finally retrieved by simply unmixing the sparsely distributed replicas with their location map.","This methodology allows to overcome scattering and achieve robust super-resolution fluorescence imaging, circumventing the need of heavy computational steps."],"url":"http://arxiv.org/abs/2404.19734v1","category":"physics.optics"}
{"created":"2024-04-30 17:28:05","title":"Iterative Reasoning Preference Optimization","abstract":"Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy for Llama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting out of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on ARC-Challenge, which outperforms other Llama-2-based models not relying on additionally sourced datasets.","sentences":["Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024).","In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer.","We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial.","We show reasoning improves across repeated iterations of this scheme.","While only relying on examples in the training set, our approach results in increasing accuracy for Llama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting out of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on ARC-Challenge, which outperforms other Llama-2-based models not relying on additionally sourced datasets."],"url":"http://arxiv.org/abs/2404.19733v1","category":"cs.CL"}
{"created":"2024-04-30 17:25:44","title":"Investigating the correlations between IceCube high-energy neutrinos and Fermi-LAT $\u03b3$-ray observations. II","abstract":"Given that gamma rays with energies larger than TeV are severely absorbed by background radiation fields, for many extragalactic sources, the GeV-TeV gamma-ray observations are the messengers that are closest in energy to the TeV-PeV neutrinos observed by IceCube. Investigating whether there is a correlation between the gamma-ray and neutrino observations can help us identify high-energy neutrino sources and determine which sources are the main contributors to the all-sky diffuse neutrino flux of IceCube. In previous work, we have already studied the possible gamma-neutrino correlations by analyzing 10 years of IceCube muon-track data. In this work, we further investigate such correlations by employing the IceCube p-value sky map of the scan for point sources. We examine the spatial associations of hotspots in the neutrino sky map with various gamma-ray source samples: the third Fermi-LAT catalog of high-energy sources (3FHL), LAT 14-year source catalog (4FGL), the fourth catalog of active galactic nuclei (4LAC) and subsets of these samples. Among all the samples, the 3FHL sample shows a possible correlation with the neutrino hotspots, with a pre-trial p-value of $1.1\\times10^{-4}$ ($\\sim 3.9\\,\\sigma$). However, this is found to be caused by three already known neutrino sources/source candidates: NGC 1068, TXS 0506+056, and PKS 1424+240. In order to validate our analysis procedure and to test the robustness of the previously claimed correlation between the 5BZCAT blazars and neutrino hotspots, we also consider 5BZCAT blazars in our correlation study. We find that the way in which mock sources are generated in the simulation used to derive the chance coincidence probability may have a large impact on the claimed correlation.","sentences":["Given that gamma rays with energies larger than TeV are severely absorbed by background radiation fields, for many extragalactic sources, the GeV-TeV gamma-ray observations are the messengers that are closest in energy to the TeV-PeV neutrinos observed by IceCube.","Investigating whether there is a correlation between the gamma-ray and neutrino observations can help us identify high-energy neutrino sources and determine which sources are the main contributors to the all-sky diffuse neutrino flux of IceCube.","In previous work, we have already studied the possible gamma-neutrino correlations by analyzing 10 years of IceCube muon-track data.","In this work, we further investigate such correlations by employing the IceCube p-value sky map of the scan for point sources.","We examine the spatial associations of hotspots in the neutrino sky map with various gamma-ray source samples: the third Fermi-LAT catalog of high-energy sources (3FHL), LAT 14-year source catalog (4FGL), the fourth catalog of active galactic nuclei (4LAC) and subsets of these samples.","Among all the samples, the 3FHL sample shows a possible correlation with the neutrino hotspots, with a pre-trial p-value of $1.1\\times10^{-4}$ ($\\sim 3.9\\,\\sigma$).","However, this is found to be caused by three already known neutrino sources/source candidates: NGC 1068, TXS 0506+056, and PKS 1424+240.","In order to validate our analysis procedure and to test the robustness of the previously claimed correlation between the 5BZCAT blazars and neutrino hotspots, we also consider 5BZCAT blazars in our correlation study.","We find that the way in which mock sources are generated in the simulation used to derive the chance coincidence probability may have a large impact on the claimed correlation."],"url":"http://arxiv.org/abs/2404.19730v1","category":"astro-ph.HE"}
{"created":"2024-04-30 17:24:55","title":"A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications","abstract":"External knowledge graphs (KGs) can be used to augment large language models (LLMs), while simultaneously providing an explainable knowledge base of facts that can be inspected by a human. This approach may be particularly valuable in domains where explainability is critical, like human trafficking data analysis. However, creating KGs can pose challenges. KGs parsed from documents may comprise explicit connections (those directly stated by a document) but miss implicit connections (those obvious to a human although not directly stated). To address these challenges, this preliminary research introduces the GAME-KG framework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge Graphs.\" GAME-KG is a federated approach to modifying explicit as well as implicit connections in KGs by using crowdsourced feedback collected through video games. GAME-KG is shown through two demonstrations: a Unity test scenario from Dark Shadows, a video game that collects feedback on KGs parsed from US Department of Justice (DOJ) Press Releases on human trafficking, and a following experiment where OpenAI's GPT-4 is prompted to answer questions based on a modified and unmodified KG. Initial results suggest that GAME-KG can be an effective framework for enhancing KGs, while simultaneously providing an explainable set of structured facts verified by humans.","sentences":["External knowledge graphs (KGs) can be used to augment large language models (LLMs), while simultaneously providing an explainable knowledge base of facts that can be inspected by a human.","This approach may be particularly valuable in domains where explainability is critical, like human trafficking data analysis.","However, creating KGs can pose challenges.","KGs parsed from documents may comprise explicit connections (those directly stated by a document) but miss implicit connections (those obvious to a human although not directly stated).","To address these challenges, this preliminary research introduces the GAME-KG framework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge Graphs.\"","GAME-KG is a federated approach to modifying explicit as well as implicit connections in KGs by using crowdsourced feedback collected through video games.","GAME-KG is shown through two demonstrations: a Unity test scenario from Dark Shadows, a video game that collects feedback on KGs parsed from US Department of Justice (DOJ) Press Releases on human trafficking, and a following experiment where OpenAI's GPT-4 is prompted to answer questions based on a modified and unmodified KG.","Initial results suggest that GAME-KG can be an effective framework for enhancing KGs, while simultaneously providing an explainable set of structured facts verified by humans."],"url":"http://arxiv.org/abs/2404.19729v1","category":"cs.HC"}
{"created":"2024-04-30 17:22:28","title":"On the inefficiency of fermion level-crossing under the parity-violating spin-2 gravitational field","abstract":"Gravitational chiral anomaly connects the topological charge of spacetime and the chirality of fermions. It has been known that the chirality is carried by the particles (or the excited states) and also by vacuum. While the gravitational anomaly equation has been applied to cosmology, distinction between these two contributions has been rarely discussed. In the study of gravitational leptogenesis, for example, lepton asymmetry associated with the chiral gravitational waves sourced during inflation is evaluated only by integrating the anomaly equation. How these two contributions are distributed has not been seriously investigated. Meanwhile, a dominance of vacuum contribution is observed in some specific types of Bianchi spacetime with parity-violating gravitational fields, whose application to cosmology is not straightforward. One may wonder whether such a vacuum dominance takes place also in the system with chiral gravitational waves around the flat background, which is more suitable for application to realistic cosmology. In this work, we apply an analogy between U(1) electromagnetism and the weak gravity to the spacetime that resembles the one considered in the gravitational leptogenesis scenario. This approach allows us to obtain intuitive understanding of the fermion chirality generation under the parity-violating spin-2 gravitational field. By assuming the emergence of Landau level-like dispersion relation in our setup, we conjecture that level-crossing does not seem to be efficient while the charge accumulation in the vacuum likely takes place. Phenomenological implication is also discussed in the context of gravitational leptogenesis.","sentences":["Gravitational chiral anomaly connects the topological charge of spacetime and the chirality of fermions.","It has been known that the chirality is carried by the particles (or the excited states) and also by vacuum.","While the gravitational anomaly equation has been applied to cosmology, distinction between these two contributions has been rarely discussed.","In the study of gravitational leptogenesis, for example, lepton asymmetry associated with the chiral gravitational waves sourced during inflation is evaluated only by integrating the anomaly equation.","How these two contributions are distributed has not been seriously investigated.","Meanwhile, a dominance of vacuum contribution is observed in some specific types of Bianchi spacetime with parity-violating gravitational fields, whose application to cosmology is not straightforward.","One may wonder whether such a vacuum dominance takes place also in the system with chiral gravitational waves around the flat background, which is more suitable for application to realistic cosmology.","In this work, we apply an analogy between U(1) electromagnetism and the weak gravity to the spacetime that resembles the one considered in the gravitational leptogenesis scenario.","This approach allows us to obtain intuitive understanding of the fermion chirality generation under the parity-violating spin-2 gravitational field.","By assuming the emergence of Landau level-like dispersion relation in our setup, we conjecture that level-crossing does not seem to be efficient while the charge accumulation in the vacuum likely takes place.","Phenomenological implication is also discussed in the context of gravitational leptogenesis."],"url":"http://arxiv.org/abs/2404.19726v1","category":"hep-ph"}
{"created":"2024-04-30 17:19:52","title":"Fairness Without Demographics in Human-Centered Federated Learning","abstract":"Federated learning (FL) enables collaborative model training while preserving data privacy, making it suitable for decentralized human-centered AI applications. However, a significant research gap remains in ensuring fairness in these systems. Current fairness strategies in FL require knowledge of bias-creating/sensitive attributes, clashing with FL's privacy principles. Moreover, in human-centered datasets, sensitive attributes may remain latent. To tackle these challenges, we present a novel bias mitigation approach inspired by \"Fairness without Demographics\" in machine learning. The presented approach achieves fairness without needing knowledge of sensitive attributes by minimizing the top eigenvalue of the Hessian matrix during training, ensuring equitable loss landscapes across FL participants. Notably, we introduce a novel FL aggregation scheme that promotes participating models based on error rates and loss landscape curvature attributes, fostering fairness across the FL system. This work represents the first approach to attaining \"Fairness without Demographics\" in human-centered FL. Through comprehensive evaluation, our approach demonstrates effectiveness in balancing fairness and efficacy across various real-world applications, FL setups, and scenarios involving single and multiple bias-inducing factors, representing a significant advancement in human-centered FL.","sentences":["Federated learning (FL) enables collaborative model training while preserving data privacy, making it suitable for decentralized human-centered AI applications.","However, a significant research gap remains in ensuring fairness in these systems.","Current fairness strategies in FL require knowledge of bias-creating/sensitive attributes, clashing with FL's privacy principles.","Moreover, in human-centered datasets, sensitive attributes may remain latent.","To tackle these challenges, we present a novel bias mitigation approach inspired by \"Fairness without Demographics\" in machine learning.","The presented approach achieves fairness without needing knowledge of sensitive attributes by minimizing the top eigenvalue of the Hessian matrix during training, ensuring equitable loss landscapes across FL participants.","Notably, we introduce a novel FL aggregation scheme that promotes participating models based on error rates and loss landscape curvature attributes, fostering fairness across the FL system.","This work represents the first approach to attaining \"Fairness without Demographics\" in human-centered FL.","Through comprehensive evaluation, our approach demonstrates effectiveness in balancing fairness and efficacy across various real-world applications, FL setups, and scenarios involving single and multiple bias-inducing factors, representing a significant advancement in human-centered FL."],"url":"http://arxiv.org/abs/2404.19725v1","category":"cs.LG"}
{"created":"2024-04-30 17:17:07","title":"Attention-Constrained Inference for Robust Decoder-Only Text-to-Speech","abstract":"Recent popular decoder-only text-to-speech models are known for their ability of generating natural-sounding speech. However, such models sometimes suffer from word skipping and repeating due to the lack of explicit monotonic alignment constraints. In this paper, we notice from the attention maps that some particular attention heads of the decoder-only model indicate the alignments between speech and text. We call the attention maps of those heads Alignment-Emerged Attention Maps (AEAMs). Based on this discovery, we propose a novel inference method without altering the training process, named Attention-Constrained Inference (ACI), to facilitate monotonic synthesis. It first identifies AEAMs using the Attention Sweeping algorithm and then applies constraining masks on AEAMs. Our experimental results on decoder-only TTS model VALL-E show that the WER of synthesized speech is reduced by up to 20.5% relatively with ACI while the naturalness and speaker similarity are comparable.","sentences":["Recent popular decoder-only text-to-speech models are known for their ability of generating natural-sounding speech.","However, such models sometimes suffer from word skipping and repeating due to the lack of explicit monotonic alignment constraints.","In this paper, we notice from the attention maps that some particular attention heads of the decoder-only model indicate the alignments between speech and text.","We call the attention maps of those heads Alignment-Emerged Attention Maps (AEAMs).","Based on this discovery, we propose a novel inference method without altering the training process, named Attention-Constrained Inference (ACI), to facilitate monotonic synthesis.","It first identifies AEAMs using the Attention Sweeping algorithm and then applies constraining masks on AEAMs.","Our experimental results on decoder-only TTS model VALL-E show that the WER of synthesized speech is reduced by up to 20.5% relatively with ACI while the naturalness and speaker similarity are comparable."],"url":"http://arxiv.org/abs/2404.19723v1","category":"eess.AS"}
{"created":"2024-04-30 17:15:42","title":"PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios","abstract":"We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios. Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios. This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond. In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory. The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy. This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control. Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios. More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .","sentences":["We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios.","Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios.","This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond.","In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory.","The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy.","This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control.","Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios.","More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html ."],"url":"http://arxiv.org/abs/2404.19722v1","category":"cs.CV"}
{"created":"2024-04-30 17:11:54","title":"PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games","abstract":"This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer's high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs). Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative. The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses. PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative. A novel validation system that uses the LLM's intelligence evaluates text input and aligns generated responses with the unfolding narrative. Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative. For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs. PANGeA's ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game. These are, a custom, browser-based GPT and a Unity demo. As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input.","sentences":["This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer's high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs).","Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative.","The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses.","PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative.","A novel validation system that uses the LLM's intelligence evaluates text input and aligns generated responses with the unfolding narrative.","Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative.","For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs.","PANGeA's ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game.","These are, a custom, browser-based GPT and a Unity demo.","As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input."],"url":"http://arxiv.org/abs/2404.19721v1","category":"cs.AI"}
{"created":"2024-04-30 17:11:39","title":"Efficient Multiparty Quantum Key Distribution over Quantum Networks","abstract":"Multiparty quantum key distribution (QKD) is useful for many applications that involve secure communication or collaboration among multiple parties. While it can be achieved using pairwise QKD, a more efficient approach is to achieve it using multipartite entanglement distributed over quantum networks that connect the multiple parties. Existing studies on multipartite entanglement distribution, however, are not designed for multiparty QKD, and hence do not aim to maximize secret key generation rate. In this paper, we design efficient strategies for multiparty QKD over quantum networks. For 3-party QKD, we derive closed-form expressions for analyzing key distribution over quantum networks. We then use it to develop an efficient strategy for 3-party QKD by packing multiple stars that connect the 3 parties. For the general form of N-party QKD, we develop an approach that packs multiple trees to connect the N parties, while directly incorporating the estimated key rates on network paths. Extensive evaluation of our strategies, in both grid and random graphs, under a wide range of settings, demonstrates that our schemes achieve high key rate, which degrades gracefully when increasing the number of parties.","sentences":["Multiparty quantum key distribution (QKD) is useful for many applications that involve secure communication or collaboration among multiple parties.","While it can be achieved using pairwise QKD, a more efficient approach is to achieve it using multipartite entanglement distributed over quantum networks that connect the multiple parties.","Existing studies on multipartite entanglement distribution, however, are not designed for multiparty QKD, and hence do not aim to maximize secret key generation rate.","In this paper, we design efficient strategies for multiparty QKD over quantum networks.","For 3-party QKD, we derive closed-form expressions for analyzing key distribution over quantum networks.","We then use it to develop an efficient strategy for 3-party QKD by packing multiple stars that connect the 3 parties.","For the general form of N-party QKD, we develop an approach that packs multiple trees to connect the N parties, while directly incorporating the estimated key rates on network paths.","Extensive evaluation of our strategies, in both grid and random graphs, under a wide range of settings, demonstrates that our schemes achieve high key rate, which degrades gracefully when increasing the number of parties."],"url":"http://arxiv.org/abs/2404.19720v1","category":"quant-ph"}
{"created":"2024-04-30 17:06:27","title":"Assessing LLMs in Malicious Code Deobfuscation of Real-world Malware Campaigns","abstract":"The integration of large language models (LLMs) into various pipelines is increasingly widespread, effectively automating many manual tasks and often surpassing human capabilities. Cybersecurity researchers and practitioners have recognised this potential. Thus, they are actively exploring its applications, given the vast volume of heterogeneous data that requires processing to identify anomalies, potential bypasses, attacks, and fraudulent incidents. On top of this, LLMs' advanced capabilities in generating functional code, comprehending code context, and summarising its operations can also be leveraged for reverse engineering and malware deobfuscation. To this end, we delve into the deobfuscation capabilities of state-of-the-art LLMs. Beyond merely discussing a hypothetical scenario, we evaluate four LLMs with real-world malicious scripts used in the notorious Emotet malware campaign. Our results indicate that while not absolutely accurate yet, some LLMs can efficiently deobfuscate such payloads. Thus, fine-tuning LLMs for this task can be a viable potential for future AI-powered threat intelligence pipelines in the fight against obfuscated malware.","sentences":["The integration of large language models (LLMs) into various pipelines is increasingly widespread, effectively automating many manual tasks and often surpassing human capabilities.","Cybersecurity researchers and practitioners have recognised this potential.","Thus, they are actively exploring its applications, given the vast volume of heterogeneous data that requires processing to identify anomalies, potential bypasses, attacks, and fraudulent incidents.","On top of this, LLMs' advanced capabilities in generating functional code, comprehending code context, and summarising its operations can also be leveraged for reverse engineering and malware deobfuscation.","To this end, we delve into the deobfuscation capabilities of state-of-the-art LLMs.","Beyond merely discussing a hypothetical scenario, we evaluate four LLMs with real-world malicious scripts used in the notorious Emotet malware campaign.","Our results indicate that while not absolutely accurate yet, some LLMs can efficiently deobfuscate such payloads.","Thus, fine-tuning LLMs for this task can be a viable potential for future AI-powered threat intelligence pipelines in the fight against obfuscated malware."],"url":"http://arxiv.org/abs/2404.19715v1","category":"cs.CR"}
{"created":"2024-04-30 17:06:11","title":"Automated Generation of High-Quality Medical Simulation Scenarios Through Integration of Semi-Structured Data and Large Language Models","abstract":"This study introduces a transformative framework for medical education by integrating semi-structured data with Large Language Models (LLMs), primarily OpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios. Traditionally, developing these scenarios was a time-intensive process with limited flexibility to meet diverse educational needs. The proposed approach utilizes AI to efficiently generate detailed, clinically relevant scenarios that are tailored to specific educational objectives. This innovation has significantly reduced the time and resources required for scenario development, allowing for a broader variety of simulations. Preliminary feedback from educators and learners has shown enhanced engagement and improved knowledge acquisition, confirming the effectiveness of this AI-enhanced methodology in simulation-based learning. The integration of structured data with LLMs not only streamlines the creation process but also offers a scalable, dynamic solution that could revolutionize medical training, highlighting the critical role of AI in advancing educational outcomes and patient care standards.","sentences":["This study introduces a transformative framework for medical education by integrating semi-structured data with Large Language Models (LLMs), primarily OpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios.","Traditionally, developing these scenarios was a time-intensive process with limited flexibility to meet diverse educational needs.","The proposed approach utilizes AI to efficiently generate detailed, clinically relevant scenarios that are tailored to specific educational objectives.","This innovation has significantly reduced the time and resources required for scenario development, allowing for a broader variety of simulations.","Preliminary feedback from educators and learners has shown enhanced engagement and improved knowledge acquisition, confirming the effectiveness of this AI-enhanced methodology in simulation-based learning.","The integration of structured data with LLMs not only streamlines the creation process but also offers a scalable, dynamic solution that could revolutionize medical training, highlighting the critical role of AI in advancing educational outcomes and patient care standards."],"url":"http://arxiv.org/abs/2404.19713v1","category":"cs.CL"}
{"created":"2024-04-30 17:02:51","title":"Elevating electron energy gain and betatron X-ray emission in proton-driven wakefield acceleration","abstract":"The long proton beams present at CERN have the potential to evolve into a train of microbunches through the self-modulation instability process. The resonant wakefield generated by a periodic train of proton microbunches can establish a high acceleration field within the plasma, facilitating electron acceleration. This paper investigates the impact of plasma density on resonant wakefield excitation, thus influencing acceleration of a witness electron bunch and its corresponding betatron radiation within the wakefield. Various scenarios involving different plasma densities are explored through particle-in-cell simulations. The peak wakefield in each scenario is calculated by considering a long pre-modulated proton driver with a fixed peak current. Subsequently, the study delves into the witness beam acceleration in the wakefield and its radiation emission. Elevated plasma density increases both the number of microbunches and the accelerating gradient of each microbunch, consequently resulting in heightened resonant wakefield. Nevertheless, the scaling is disrupted by the saturation of the resonant wakefield due to the nonlinearities. The simulation results reveal that at high plasma densities an intense and broadband radiation spectrum extending into the domain of the hard X-rays and gamma rays is generated. Furthermore, in such instances, the energy gain of the witness beam is significantly enhanced. The impact of wakefield on the witness energy gain and the corresponding radiation spectrum is clearly evident at extremely elevated densities.","sentences":["The long proton beams present at CERN have the potential to evolve into a train of microbunches through the self-modulation instability process.","The resonant wakefield generated by a periodic train of proton microbunches can establish a high acceleration field within the plasma, facilitating electron acceleration.","This paper investigates the impact of plasma density on resonant wakefield excitation, thus influencing acceleration of a witness electron bunch and its corresponding betatron radiation within the wakefield.","Various scenarios involving different plasma densities are explored through particle-in-cell simulations.","The peak wakefield in each scenario is calculated by considering a long pre-modulated proton driver with a fixed peak current.","Subsequently, the study delves into the witness beam acceleration in the wakefield and its radiation emission.","Elevated plasma density increases both the number of microbunches and the accelerating gradient of each microbunch, consequently resulting in heightened resonant wakefield.","Nevertheless, the scaling is disrupted by the saturation of the resonant wakefield due to the nonlinearities.","The simulation results reveal that at high plasma densities an intense and broadband radiation spectrum extending into the domain of the hard X-rays and gamma rays is generated.","Furthermore, in such instances, the energy gain of the witness beam is significantly enhanced.","The impact of wakefield on the witness energy gain and the corresponding radiation spectrum is clearly evident at extremely elevated densities."],"url":"http://arxiv.org/abs/2404.19711v1","category":"physics.acc-ph"}
{"created":"2024-04-30 17:00:32","title":"Harmonic LLMs are Trustworthy","abstract":"We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM in real-time, based upon the local deviation from harmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the first completely model-agnostic and unsupervised method of measuring the robustness of any given response from an LLM, based upon the model itself conforming to a purely mathematical standard. We conduct human annotation experiments to show the positive correlation of $\\gamma$ with false or misleading answers, and demonstrate that following the gradient of $\\gamma$ in stochastic gradient ascent efficiently exposes adversarial prompts. Measuring $\\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT, Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory answers automatically and quantitatively rank the reliability of these models in various objective domains (Web QA, TruthfulQA, and Programming QA). Across all models and domains tested, human ratings confirm that $\\gamma \\to 0$ indicates trustworthiness, and the low-$\\gamma$ leaders among these models are GPT-4, ChatGPT, and Smaug-72B.","sentences":["We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM in real-time, based upon the local deviation from harmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the first completely model-agnostic and unsupervised method of measuring the robustness of any given response from an LLM, based upon the model itself conforming to a purely mathematical standard.","We conduct human annotation experiments to show the positive correlation of $\\gamma$ with false or misleading answers, and demonstrate that following the gradient of $\\gamma$ in stochastic gradient ascent efficiently exposes adversarial prompts.","Measuring $\\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT, Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory answers automatically and quantitatively rank the reliability of these models in various objective domains (Web QA, TruthfulQA, and Programming QA).","Across all models and domains tested, human ratings confirm that $\\gamma \\to 0$ indicates trustworthiness, and the low-$\\gamma$ leaders among these models are GPT-4, ChatGPT, and Smaug-72B."],"url":"http://arxiv.org/abs/2404.19708v1","category":"cs.LG"}
{"created":"2024-04-30 16:52:55","title":"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively","abstract":"In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question. Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, <RET>, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that Adapt-LLM is able to generate the <RET> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.","sentences":["In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question.","Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself.","Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage.","Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets.","Here, LLMs are trained to generate a special token, <RET>, when they do not know the answer to a question.","Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever.","Through our analysis, we demonstrate that Adapt-LLM is able to generate the <RET> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory."],"url":"http://arxiv.org/abs/2404.19705v1","category":"cs.CL"}
{"created":"2024-04-30 16:49:36","title":"$C$-embedding, Lindel\u00f6fness, \u010cech-completeness","abstract":"We show that in the class of Lindel\\\"of \\v{C}ech-complete spaces the property of being $C$-embedded is quite well-behaved. It admits a useful characterization that can be used to show that products and perfect preimages of $C$-embedded spaces are again $C$-embedded. We also show that both properties, Lindel\\\"of and \\v{C}ech-complete, are needed in the product result.","sentences":["We show that in the class of Lindel\\\"of \\v{C}ech-complete spaces the property of being $C$-embedded is quite well-behaved.","It admits a useful characterization that can be used to show that products and perfect preimages of $C$-embedded spaces are again $C$-embedded.","We also show that both properties, Lindel\\\"of and \\v{C}ech-complete, are needed in the product result."],"url":"http://arxiv.org/abs/2404.19703v1","category":"math.GN"}
{"created":"2024-04-30 16:47:46","title":"GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting","abstract":"We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ .","sentences":["We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU.","Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering.","In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity.","We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively.","In both scenarios, the models outperform state-of-the-art baselines by a wide margin.","We also demonstrate applications of our model in downstream 3D generation tasks.","Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ ."],"url":"http://arxiv.org/abs/2404.19702v1","category":"cs.CV"}
{"created":"2024-04-30 16:45:54","title":"Generative AI Usage and Academic Performance","abstract":"This study evaluates the impact of students' usage of generative artificial intelligence (GenAI) tools such as ChatGPT on their academic performance. We analyze student essays using GenAI detection systems to identify GenAI users among the cohort. Employing multivariate regression analysis, we find that students using GenAI tools score on average 6.71 (out of 100) points lower than non-users. While GenAI tools may offer benefits for learning and engagement, the way students actually use it correlates with diminished academic outcomes. Exploring the underlying mechanism, additional analyses show that the effect is particularly detrimental to students with high learning potential, suggesting an effect whereby GenAI tool usage hinders learning. Our findings provide important empirical evidence for the ongoing debate on the integration of GenAI in higher education and underscores the necessity for educators, institutions, and policymakers to carefully consider its implications for student performance.","sentences":["This study evaluates the impact of students' usage of generative artificial intelligence (GenAI) tools such as ChatGPT on their academic performance.","We analyze student essays using GenAI detection systems to identify GenAI users among the cohort.","Employing multivariate regression analysis, we find that students using GenAI tools score on average 6.71 (out of 100) points lower than non-users.","While GenAI tools may offer benefits for learning and engagement, the way students actually use it correlates with diminished academic outcomes.","Exploring the underlying mechanism, additional analyses show that the effect is particularly detrimental to students with high learning potential, suggesting an effect whereby GenAI tool usage hinders learning.","Our findings provide important empirical evidence for the ongoing debate on the integration of GenAI in higher education and underscores the necessity for educators, institutions, and policymakers to carefully consider its implications for student performance."],"url":"http://arxiv.org/abs/2404.19699v1","category":"econ.GN"}
{"created":"2024-04-30 16:44:42","title":"Fibering polarizations and Mabuchi rays on symmetric spaces of compact type","abstract":"In this paper, we describe holomorphic quantizations of the cotangent bundle of a symmetric space of compact type $T^*(U/K)\\cong U_\\mathbb{C}/K_\\mathbb{C}$, along Mabuchi rays of $U$-invariant K\\\"ahler structures. At infinite geodesic time, the K\\\"ahler polarizations converge to a mixed polarization $\\mathcal{P}_\\infty$. We show how a generalized coherent state transform relates the quantizations along the Mabuchi geodesics such that holomorphic sections converge, as geodesic time goes to infinity, to distributional $\\mathcal{P}_\\infty$-polarized sections. Unlike in the case of $T^*U$, the gCST mapping from the Hilbert space of vertically polarized sections are not asymptotically unitary due to the appearance of representation dependent factors associated to the isotypical decomposition for the $U$-action. In agreement with the general program outlined in [Bai+23], we also describe how the quantization in the limit polarization $\\mathcal{P}_\\infty$ is given by the direct sum of the quantizations for all the symplectic reductions relative to the invariant torus action associated to the Hamiltonian action of $U$.","sentences":["In this paper, we describe holomorphic quantizations of the cotangent bundle of a symmetric space of compact type $T^*(U/K)\\cong U_\\mathbb{C}/K_\\mathbb{C}$, along Mabuchi rays of $U$-invariant K\\\"ahler structures.","At infinite geodesic time, the K\\\"ahler polarizations converge to a mixed polarization $\\mathcal{P}_\\infty$. We show how a generalized coherent state transform relates the quantizations along the Mabuchi geodesics such that holomorphic sections converge, as geodesic time goes to infinity, to distributional $\\mathcal{P}_\\infty$-polarized sections.","Unlike in the case of $T^*U$, the gCST mapping from the Hilbert space of vertically polarized sections are not asymptotically unitary due to the appearance of representation dependent factors associated to the isotypical decomposition for the $U$-action.","In agreement with the general program outlined in [Bai+23], we also describe how the quantization in the limit polarization $\\mathcal{P}_\\infty$ is given by the direct sum of the quantizations for all the symplectic reductions relative to the invariant torus action associated to the Hamiltonian action of $U$."],"url":"http://arxiv.org/abs/2404.19697v1","category":"math.SG"}
{"created":"2024-04-30 16:44:18","title":"Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners","abstract":"3D visual grounding is a challenging task that often requires direct and dense supervision, notably the semantic label for each object in the scene. In this paper, we instead study the naturally supervised setting that learns from only 3D scene and QA pairs, where prior works underperform. We propose the Language-Regularized Concept Learner (LARC), which uses constraints from language as regularization to significantly improve the accuracy of neuro-symbolic concept learners in the naturally supervised setting. Our approach is based on two core insights: the first is that language constraints (e.g., a word's relation to another) can serve as effective regularization for structured representations in neuro-symbolic models; the second is that we can query large language models to distill such constraints from language properties. We show that LARC improves performance of prior works in naturally supervised 3D visual grounding, and demonstrates a wide range of 3D visual reasoning capabilities-from zero-shot composition, to data efficiency and transferability. Our method represents a promising step towards regularizing structured visual reasoning frameworks with language-based priors, for learning in settings without dense supervision.","sentences":["3D visual grounding is a challenging task that often requires direct and dense supervision, notably the semantic label for each object in the scene.","In this paper, we instead study the naturally supervised setting that learns from only 3D scene and QA pairs, where prior works underperform.","We propose the Language-Regularized Concept Learner (LARC), which uses constraints from language as regularization to significantly improve the accuracy of neuro-symbolic concept learners in the naturally supervised setting.","Our approach is based on two core insights: the first is that language constraints (e.g., a word's relation to another) can serve as effective regularization for structured representations in neuro-symbolic models; the second is that we can query large language models to distill such constraints from language properties.","We show that LARC improves performance of prior works in naturally supervised 3D visual grounding, and demonstrates a wide range of 3D visual reasoning capabilities-from zero-shot composition, to data efficiency and transferability.","Our method represents a promising step towards regularizing structured visual reasoning frameworks with language-based priors, for learning in settings without dense supervision."],"url":"http://arxiv.org/abs/2404.19696v1","category":"cs.CV"}
{"created":"2024-04-30 16:43:22","title":"Early dark energy and scalarization in a scalar-tensor model","abstract":"We present a model in which the Gauss-Bonnet invariant holds the quintessence at a fixed point, respecting an initial $Z_2$ symmetry in the radiation-dominated era. This results in an early dark energy, which becomes significant around the matter-radiation equality era. However, due to $Z_2$ symmetry breaking, scalarization occurs, leading to a rapid reduction in the early dark energy density. The model then quickly behaves like the $\\Lambda$CDM model. This scenario alleviates the Hubble tension and aligns with the assumption that the gravitational wave speed is infinitesimally close to the speed of light.","sentences":["We present a model in which the Gauss-Bonnet invariant holds the quintessence at a fixed point, respecting an initial $Z_2$ symmetry in the radiation-dominated era.","This results in an early dark energy, which becomes significant around the matter-radiation equality era.","However, due to $Z_2$ symmetry breaking, scalarization occurs, leading to a rapid reduction in the early dark energy density.","The model then quickly behaves like the $\\Lambda$CDM model.","This scenario alleviates the Hubble tension and aligns with the assumption that the gravitational wave speed is infinitesimally close to the speed of light."],"url":"http://arxiv.org/abs/2404.19695v1","category":"gr-qc"}
{"created":"2024-04-30 16:42:52","title":"Scalar perturbations from inflation in the presence of gauge fields","abstract":"We study how Abelian-gauge-field production during inflation affects scalar perturbations in the case when the gauge field interacts with the inflaton directly (by means of generic kinetic and axial couplings) and via gravity. The homogeneous background solution is defined by self-consistently taking into account the backreaction of the gauge field on the evolution of the inflaton and the scale factor. For the perturbations on top of this background, all possible scalar contributions coming from the inflaton, the metric, and the gauge field are considered. We derive a second-order differential equation for the curvature perturbation, $\\zeta$, capturing the impact of the gauge field, both on the background dynamics and on the evolution of scalar perturbations. The latter is described by a source term in the $\\zeta$-equation, which is quadratic in the gauge-field operators and leads to non-Gaussianities in the curvature perturbations. We derive general expressions for the induced scalar power spectrum and bispectrum. Finally, we apply our formalism to the well-known case of axion inflation without backreaction. Numerical results show that, in this example, the effect of including metric perturbations is small for values of the gauge-field production parameter $\\xi> 3$. This is in agreement with the results of previous studies in the literature. However, in the region of smaller values, $\\xi\\lesssim 2$, our new results exhibit order-of-unity deviations when compared to previous results.","sentences":["We study how Abelian-gauge-field production during inflation affects scalar perturbations in the case when the gauge field interacts with the inflaton directly (by means of generic kinetic and axial couplings) and via gravity.","The homogeneous background solution is defined by self-consistently taking into account the backreaction of the gauge field on the evolution of the inflaton and the scale factor.","For the perturbations on top of this background, all possible scalar contributions coming from the inflaton, the metric, and the gauge field are considered.","We derive a second-order differential equation for the curvature perturbation, $\\zeta$, capturing the impact of the gauge field, both on the background dynamics and on the evolution of scalar perturbations.","The latter is described by a source term in the $\\zeta$-equation, which is quadratic in the gauge-field operators and leads to non-Gaussianities in the curvature perturbations.","We derive general expressions for the induced scalar power spectrum and bispectrum.","Finally, we apply our formalism to the well-known case of axion inflation without backreaction.","Numerical results show that, in this example, the effect of including metric perturbations is small for values of the gauge-field production parameter $\\xi> 3$.","This is in agreement with the results of previous studies in the literature.","However, in the region of smaller values, $\\xi\\lesssim 2$, our new results exhibit order-of-unity deviations when compared to previous results."],"url":"http://arxiv.org/abs/2404.19694v1","category":"astro-ph.CO"}
{"created":"2024-04-30 16:37:27","title":"SwipeGANSpace: Swipe-to-Compare Image Generation via Efficient Latent Space Exploration","abstract":"Generating preferred images using generative adversarial networks (GANs) is challenging owing to the high-dimensional nature of latent space. In this study, we propose a novel approach that uses simple user-swipe interactions to generate preferred images for users. To effectively explore the latent space with only swipe interactions, we apply principal component analysis to the latent space of the StyleGAN, creating meaningful subspaces. We use a multi-armed bandit algorithm to decide the dimensions to explore, focusing on the preferences of the user. Experiments show that our method is more efficient in generating preferred images than the baseline methods. Furthermore, changes in preferred images during image generation or the display of entirely different image styles were observed to provide new inspirations, subsequently altering user preferences. This highlights the dynamic nature of user preferences, which our proposed approach recognizes and enhances.","sentences":["Generating preferred images using generative adversarial networks (GANs) is challenging owing to the high-dimensional nature of latent space.","In this study, we propose a novel approach that uses simple user-swipe interactions to generate preferred images for users.","To effectively explore the latent space with only swipe interactions, we apply principal component analysis to the latent space of the StyleGAN, creating meaningful subspaces.","We use a multi-armed bandit algorithm to decide the dimensions to explore, focusing on the preferences of the user.","Experiments show that our method is more efficient in generating preferred images than the baseline methods.","Furthermore, changes in preferred images during image generation or the display of entirely different image styles were observed to provide new inspirations, subsequently altering user preferences.","This highlights the dynamic nature of user preferences, which our proposed approach recognizes and enhances."],"url":"http://arxiv.org/abs/2404.19693v1","category":"cs.HC"}
{"created":"2024-04-30 16:35:09","title":"The finite-$T$ Lorentz number and the thermal conductivity. Aluminum and carbon conductivities from ambient to millions of degrees Kelvin","abstract":"Theoretical prediction of the thermal conductivity $\\kappa$ of metal-like electron-ion systems would be greatly simplified if a convenient generalization of the Lorentz number $L_N$ for arbitrary temperatures ($T$) and densities were available. Such calculations are needed in astrophysics, high-energy-density physics, semiconductor physics as well as in materials science. We present a finite-$T$ form of $L_N(T)$, expressed in terms of elementary Fermi integrals. It is a universal function of $t=T/E_F$, where $E_F$ is the Fermi energy of the electrons. A convenient four-parameter fit to $L_N(t)$ for $t=0-\\infty$ further simplifies the applications. The effect of electron-electron interactions is also briefly discussed. Calculations for $L_N(t)$ and thermal conductivities $\\kappa$ for Al and C are presented at several compressions and into the million-Kelvin range. Experimental isobaric conductivities for Al just above the meting point, and isochoric conductivities for Al and C from available density-functional theory simulations and average-atom calculations are used as comparisons.","sentences":["Theoretical prediction of the thermal conductivity $\\kappa$ of metal-like electron-ion systems would be greatly simplified if a convenient generalization of the Lorentz number $L_N$ for arbitrary temperatures ($T$) and densities were available.","Such calculations are needed in astrophysics, high-energy-density physics, semiconductor physics as well as in materials science.","We present a finite-$T$ form of $L_N(T)$, expressed in terms of elementary Fermi integrals.","It is a universal function of $t=T/E_F$, where $E_F$ is the Fermi energy of the electrons.","A convenient four-parameter fit to $L_N(t)$ for $t=0-\\infty$ further simplifies the applications.","The effect of electron-electron interactions is also briefly discussed.","Calculations for $L_N(t)$ and thermal conductivities $\\kappa$ for Al and C are presented at several compressions and into the million-Kelvin range.","Experimental isobaric conductivities for Al just above the meting point, and isochoric conductivities for Al and C from available density-functional theory simulations and average-atom calculations are used as comparisons."],"url":"http://arxiv.org/abs/2404.19692v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 16:25:59","title":"ColosSUMO: Evaluating Cooperative Driving Applications with Colosseum","abstract":"The quest for safer and more efficient transportation through cooperative, connected and automated mobility (CCAM) calls for realistic performance analysis tools, especially with respect to wireless communications. While the simulation of existing and emerging communication technologies is an option, the most realistic results can be obtained by employing real hardware, as done for example in field operational tests (FOTs). For CCAM, however, performing FOTs requires vehicles, which are generally expensive. and performing such tests can be very demanding in terms of manpower, let alone considering safety issues. Mobility simulation with hardware-in-the-loop (HIL) serves as a middle ground, but current solutions lack flexibility and reconfigurability. This work thus proposes ColosSUMO as a way to couple Colosseum, the world's largest wireless network emulator, with the SUMO mobility simulator, showing its design concept, how it can be exploited to simulate realistic vehicular environments, and its flexibility in terms of communication technologies.","sentences":["The quest for safer and more efficient transportation through cooperative, connected and automated mobility (CCAM) calls for realistic performance analysis tools, especially with respect to wireless communications.","While the simulation of existing and emerging communication technologies is an option, the most realistic results can be obtained by employing real hardware, as done for example in field operational tests (FOTs).","For CCAM, however, performing FOTs requires vehicles, which are generally expensive.","and performing such tests can be very demanding in terms of manpower, let alone considering safety issues.","Mobility simulation with hardware-in-the-loop (HIL) serves as a middle ground, but current solutions lack flexibility and reconfigurability.","This work thus proposes ColosSUMO as a way to couple Colosseum, the world's largest wireless network emulator, with the SUMO mobility simulator, showing its design concept, how it can be exploited to simulate realistic vehicular environments, and its flexibility in terms of communication technologies."],"url":"http://arxiv.org/abs/2404.19686v1","category":"cs.NI"}
{"created":"2024-04-30 16:18:01","title":"Collaborative Control Method of Transit Signal Priority Based on Cooperative Game and Reinforcement Learning","abstract":"To address the low efficiency in priority signal control within intelligent transportation systems, this study introduces a novel eight-phase priority signal control method, CBQL-TSP, leveraging a hybrid decision-making framework that integrates cooperative game theory and reinforcement learning. This approach conceptualizes the allocation of bus signal priorities as a multi-objective decision-making problem across an eight-phase signal sequence, differentiating between priority and non-priority phases. It employs a cooperative game model to facilitate this differentiation. The developed hybrid decision-making algorithm, CBQL, effectively tackles the multi-objective decision-making challenges inherent in the eight-phase signal sequence. By computing the Shapley value function, it quantifies the marginal contributions of each participant, which in turn inform the construction of a state transition probability equation based on Shapley value ratios. Compared to conventional control methods, the CBQL-TSP method not only upholds the fairness principles of cooperative game theory but also harnesses the adaptive learning capabilities of Q-Learning. This enables dynamic adjustments to signal timing in response to real-time traffic conditions, significantly enhancing the flexibility and efficiency of priority signal control.","sentences":["To address the low efficiency in priority signal control within intelligent transportation systems, this study introduces a novel eight-phase priority signal control method, CBQL-TSP, leveraging a hybrid decision-making framework that integrates cooperative game theory and reinforcement learning.","This approach conceptualizes the allocation of bus signal priorities as a multi-objective decision-making problem across an eight-phase signal sequence, differentiating between priority and non-priority phases.","It employs a cooperative game model to facilitate this differentiation.","The developed hybrid decision-making algorithm, CBQL, effectively tackles the multi-objective decision-making challenges inherent in the eight-phase signal sequence.","By computing the Shapley value function, it quantifies the marginal contributions of each participant, which in turn inform the construction of a state transition probability equation based on Shapley value ratios.","Compared to conventional control methods, the CBQL-TSP method not only upholds the fairness principles of cooperative game theory but also harnesses the adaptive learning capabilities of Q-Learning.","This enables dynamic adjustments to signal timing in response to real-time traffic conditions, significantly enhancing the flexibility and efficiency of priority signal control."],"url":"http://arxiv.org/abs/2404.19683v1","category":"cs.GT"}
{"created":"2024-04-30 16:16:35","title":"General Relativity: New insights from a Geometric Algebra approach","abstract":"Geometric algebra (GA) is a formalism capable of describing all fields of physics with elegance, simplifying mathematics and providing new physical insights. Due to the coordinate dependence of tensor formalism, General Relativity(GR) is a challenging subject known for its complicated calculations and interpretation, which has not been thoroughly translated into GA until now.   In this paper, we introduce GR with GA, emphasizing the physical interpretation of quantities and providing a step-by-step guide on performing calculations. In doing so, we show how GA provides insightful information on the physical meaning of the connection coefficients, the Riemann tensor and other physical quantities.","sentences":["Geometric algebra (GA) is a formalism capable of describing all fields of physics with elegance, simplifying mathematics and providing new physical insights.","Due to the coordinate dependence of tensor formalism, General Relativity(GR) is a challenging subject known for its complicated calculations and interpretation, which has not been thoroughly translated into GA until now.   ","In this paper, we introduce GR with GA, emphasizing the physical interpretation of quantities and providing a step-by-step guide on performing calculations.","In doing so, we show how GA provides insightful information on the physical meaning of the connection coefficients, the Riemann tensor and other physical quantities."],"url":"http://arxiv.org/abs/2404.19682v1","category":"gr-qc"}
{"created":"2024-04-30 16:16:30","title":"Metrization of Gromov-Hausdorff-type topologies on boundedly-compact metric spaces","abstract":"We present a new general framework for metrization of Gromov-Hausdorff-type topologies on non-compact metric spaces. We also give easy-to-check conditions for separability and completeness and hence the measure theoretic requirements are provided to study convergence of random spaces with additional random objects. In particular, our framework enables us to define a metric inducing a suitable Gromov-Hausdorff-type topology on the space of rooted boundedly-compact metric spaces with laws of stochastic processes and/or random fields, which was not clear how to do in previous frameworks. In addition to general theory, this paper includes several examples of Gromov-Hausdorff-type topologies, verifying that classical examples such as the Gromov-Hausdorff topology and the Gromov-Hausdorff-Prohorov topology are contained within our framework.","sentences":["We present a new general framework for metrization of Gromov-Hausdorff-type topologies on non-compact metric spaces.","We also give easy-to-check conditions for separability and completeness and hence the measure theoretic requirements are provided to study convergence of random spaces with additional random objects.","In particular, our framework enables us to define a metric inducing a suitable Gromov-Hausdorff-type topology on the space of rooted boundedly-compact metric spaces with laws of stochastic processes and/or random fields, which was not clear how to do in previous frameworks.","In addition to general theory, this paper includes several examples of Gromov-Hausdorff-type topologies, verifying that classical examples such as the Gromov-Hausdorff topology and the Gromov-Hausdorff-Prohorov topology are contained within our framework."],"url":"http://arxiv.org/abs/2404.19681v1","category":"math.MG"}
{"created":"2024-04-30 16:10:21","title":"A Comprehensive Analysis of Pegasus Spyware and Its Implications for Digital Privacy and Security","abstract":"This paper comprehensively analyzes the Pegasus spyware and its implications for digital privacy and security. The Israeli cyber intelligence company NSO Group's Pegasus has gained recognition as a potent surveillance tool capable of hacking into smartphones and extracting data without the user's knowledge [49], [50]. The research emphasizes the technical aspects of this spyware, its deployment methods, and the controversies surrounding its use. The research also emphasizes the growing worries surrounding digital privacy and security as a result of the prevalent use of advanced spyware. By delving into legal, ethical, and policy issues, the objective of this study is to deliver a holistic understanding of the challenges posed by Pegasus and similar spyware tools. Through a comprehensive examination of the subject, the paper presents potential solutions to mitigate the threats and protect users from invasive surveillance techniques.","sentences":["This paper comprehensively analyzes the Pegasus spyware and its implications for digital privacy and security.","The Israeli cyber intelligence company NSO Group's Pegasus has gained recognition as a potent surveillance tool capable of hacking into smartphones and extracting data without the user's knowledge [49], [50].","The research emphasizes the technical aspects of this spyware, its deployment methods, and the controversies surrounding its use.","The research also emphasizes the growing worries surrounding digital privacy and security as a result of the prevalent use of advanced spyware.","By delving into legal, ethical, and policy issues, the objective of this study is to deliver a holistic understanding of the challenges posed by Pegasus and similar spyware tools.","Through a comprehensive examination of the subject, the paper presents potential solutions to mitigate the threats and protect users from invasive surveillance techniques."],"url":"http://arxiv.org/abs/2404.19677v1","category":"cs.CR"}
{"created":"2024-04-30 16:05:59","title":"Specific Wasserstein divergence between continuous martingales","abstract":"Defining a divergence between the laws of continuous martingales is a delicate task, owing to the fact that these laws tend to be singular to each other. An important idea, put forward by N. Gantert, is to instead consider a scaling limit of the relative entropy between such continuous martingales sampled over a finite time grid. This gives rise to the concept of specific relative entropy. In order to develop a general theory of divergences between continuous martingales, it is only natural to replace the role of the relative entropy in this construction by a different notion of discrepancy between finite dimensional probability distributions. In the present work we take a first step in this direction, taking a power $p$ of the Wasserstein distance instead of the relative entropy. We call the newly obtained scaling limit the specific $p$-Wasserstein divergence.   In our first main result we prove that the specific $p$-Wasserstein divergence is well-defined, and exhibit an explicit expression for it in terms of the quadratic variations of the martingales involved. This is obtained under vastly weaker assumptions than the corresponding results for the specific relative entropy. Next we illustrate the usefulness of the concept, by considering the problem of optimizing the specific $p$-Wasserstein divergence over the set of win-martingales. In our second main result we characterize the solution of this optimization problem for all $p>0$ and, somewhat surprisingly, we single out the case $p=1/2$ as the one with the best probabilistic properties. For instance, the optimal martingale in this case is very explicit and can be connected, through a space transformation, to the solution of a variant of the Schr\\\"odinger problem.","sentences":["Defining a divergence between the laws of continuous martingales is a delicate task, owing to the fact that these laws tend to be singular to each other.","An important idea, put forward by N. Gantert, is to instead consider a scaling limit of the relative entropy between such continuous martingales sampled over a finite time grid.","This gives rise to the concept of specific relative entropy.","In order to develop a general theory of divergences between continuous martingales, it is only natural to replace the role of the relative entropy in this construction by a different notion of discrepancy between finite dimensional probability distributions.","In the present work we take a first step in this direction, taking a power $p$ of the Wasserstein distance instead of the relative entropy.","We call the newly obtained scaling limit the specific $p$-Wasserstein divergence.   ","In our first main result we prove that the specific $p$-Wasserstein divergence is well-defined, and exhibit an explicit expression for it in terms of the quadratic variations of the martingales involved.","This is obtained under vastly weaker assumptions than the corresponding results for the specific relative entropy.","Next we illustrate the usefulness of the concept, by considering the problem of optimizing the specific $p$-Wasserstein divergence over the set of win-martingales.","In our second main result we characterize the solution of this optimization problem for all $p>0$ and, somewhat surprisingly, we single out the case $p=1/2$ as the one with the best probabilistic properties.","For instance, the optimal martingale in this case is very explicit and can be connected, through a space transformation, to the solution of a variant of the Schr\\\"odinger problem."],"url":"http://arxiv.org/abs/2404.19672v1","category":"math.PR"}
{"created":"2024-04-30 16:04:16","title":"Non-semisimple Crane-Yetter theory varying over the character stack","abstract":"We construct a relative version of the Crane-Yetter topological quantum field theory in four dimensions, from non-semisimple data. Our theory is defined relative to the classical $G$-gauge theory in five dimensions -- this latter theory assigns to each manifold $M$ the appropriate linearization of the moduli stack of $G$-local systems, called the character stack. Our main result is to establish a relative invertibility property for our construction. This invertibility echoes -- recovers and greatly generalizes -- the key invertibility property of the original Crane-Yetter theory which allowed it to capture the framing anomaly of the celebrated Witten-Reshetikhin-Turaev theory. In particular our invertibilty statement at the level of surfaces implies a categorical, stacky version of the unicity theorem for skein algebras; at the level of 3-manifolds it equips the character stack with a canonical line bundle. Regarded as a topological symmetry defect of classical gauge theory, our work establishes invertibility of this defect by a gauging procedure.","sentences":["We construct a relative version of the Crane-Yetter topological quantum field theory in four dimensions, from non-semisimple data.","Our theory is defined relative to the classical $G$-gauge theory in five dimensions -- this latter theory assigns to each manifold $M$ the appropriate linearization of the moduli stack of $G$-local systems, called the character stack.","Our main result is to establish a relative invertibility property for our construction.","This invertibility echoes -- recovers and greatly generalizes -- the key invertibility property of the original Crane-Yetter theory which allowed it to capture the framing anomaly of the celebrated Witten-Reshetikhin-Turaev theory.","In particular our invertibilty statement at the level of surfaces implies a categorical, stacky version of the unicity theorem for skein algebras; at the level of 3-manifolds it equips the character stack with a canonical line bundle.","Regarded as a topological symmetry defect of classical gauge theory, our work establishes invertibility of this defect by a gauging procedure."],"url":"http://arxiv.org/abs/2404.19667v1","category":"math.QA"}
{"created":"2024-04-30 16:01:14","title":"Beyond MOS: Subjective Image Quality Score Preprocessing Method Based on Perceptual Similarity","abstract":"Image quality assessment often relies on raw opinion scores provided by subjects in subjective experiments, which can be noisy and unreliable. To address this issue, postprocessing procedures such as ITU-R BT.500, ITU-T P.910, and ITU-T P.913 have been standardized to clean up the original opinion scores. These methods use annotator-based statistical priors, but they do not take into account extensive information about the image itself, which limits their performance in less annotated scenarios. Generally speaking, image quality datasets usually contain similar scenes or distortions, and it is inevitable for subjects to compare images to score a reasonable score when scoring. Therefore, In this paper, we proposed Subjective Image Quality Score Preprocessing Method perceptual similarity Subjective Preprocessing (PSP), which exploit the perceptual similarity between images to alleviate subjective bias in less annotated scenarios. Specifically, we model subjective scoring as a conditional probability model based on perceptual similarity with previously scored images, called subconscious reference scoring. The reference images are stored by a neighbor dictionary, which is obtained by a normalized vector dot-product based nearest neighbor search of the images' perceptual depth features. Then the preprocessed score is updated by the exponential moving average (EMA) of the subconscious reference scoring, called similarity regularized EMA. Our experiments on multiple datasets (LIVE, TID2013, CID2013) show that this method can effectively remove the bias of the subjective scores. Additionally, Experiments prove that the Preprocesed dataset can improve the performance of downstream IQA tasks very well.","sentences":["Image quality assessment often relies on raw opinion scores provided by subjects in subjective experiments, which can be noisy and unreliable.","To address this issue, postprocessing procedures such as ITU-R BT.500, ITU-T P.910, and ITU-T P.913 have been standardized to clean up the original opinion scores.","These methods use annotator-based statistical priors, but they do not take into account extensive information about the image itself, which limits their performance in less annotated scenarios.","Generally speaking, image quality datasets usually contain similar scenes or distortions, and it is inevitable for subjects to compare images to score a reasonable score when scoring.","Therefore, In this paper, we proposed Subjective Image Quality Score Preprocessing Method perceptual similarity Subjective Preprocessing (PSP), which exploit the perceptual similarity between images to alleviate subjective bias in less annotated scenarios.","Specifically, we model subjective scoring as a conditional probability model based on perceptual similarity with previously scored images, called subconscious reference scoring.","The reference images are stored by a neighbor dictionary, which is obtained by a normalized vector dot-product based nearest neighbor search of the images' perceptual depth features.","Then the preprocessed score is updated by the exponential moving average (EMA) of the subconscious reference scoring, called similarity regularized EMA.","Our experiments on multiple datasets (LIVE, TID2013, CID2013) show that this method can effectively remove the bias of the subjective scores.","Additionally, Experiments prove that the Preprocesed dataset can improve the performance of downstream IQA tasks very well."],"url":"http://arxiv.org/abs/2404.19666v1","category":"cs.CV"}
{"created":"2024-04-30 16:00:21","title":"ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging","abstract":"AI is revolutionizing MRI along the acquisition and processing chain. Advanced AI frameworks have been developed to apply AI in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation. Existing frameworks are often designed to perform tasks independently or are focused on specific models or datasets, limiting generalization. We introduce ATOMMIC, an open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis. ATOMMIC implements several tasks using DL networks and enables MultiTask Learning (MTL) to perform related tasks integrated, targeting generalization in the MRI domain. We first review the current state of AI frameworks for MRI through a comprehensive literature search and by parsing 12,479 GitHub repositories. We benchmark 25 DL models on eight publicly available datasets to present distinct applications of ATOMMIC on accelerated MRI reconstruction, image segmentation, quantitative parameter map estimation, and joint accelerated MRI reconstruction and image segmentation utilizing MTL. Our findings demonstrate that ATOMMIC is the only MTL framework with harmonized complex-valued and real-valued data support. Evaluations on single tasks show that physics-based models, which enforce data consistency by leveraging the physical properties of MRI, outperform other models in reconstructing highly accelerated acquisitions. Physics-based models that produce high reconstruction quality can accurately estimate quantitative parameter maps. When high-performing reconstruction models are combined with robust segmentation networks utilizing MTL, performance is improved in both tasks. ATOMMIC facilitates MRI reconstruction and analysis by standardizing workflows, enhancing data interoperability, integrating unique features like MTL, and effectively benchmarking DL models.","sentences":["AI is revolutionizing MRI along the acquisition and processing chain.","Advanced AI frameworks have been developed to apply AI in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation.","Existing frameworks are often designed to perform tasks independently or are focused on specific models or datasets, limiting generalization.","We introduce ATOMMIC, an open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis.","ATOMMIC implements several tasks using DL networks and enables MultiTask Learning (MTL) to perform related tasks integrated, targeting generalization in the MRI domain.","We first review the current state of AI frameworks for MRI through a comprehensive literature search and by parsing 12,479 GitHub repositories.","We benchmark 25 DL models on eight publicly available datasets to present distinct applications of ATOMMIC on accelerated MRI reconstruction, image segmentation, quantitative parameter map estimation, and joint accelerated MRI reconstruction and image segmentation utilizing MTL.","Our findings demonstrate that ATOMMIC is the only MTL framework with harmonized complex-valued and real-valued data support.","Evaluations on single tasks show that physics-based models, which enforce data consistency by leveraging the physical properties of MRI, outperform other models in reconstructing highly accelerated acquisitions.","Physics-based models that produce high reconstruction quality can accurately estimate quantitative parameter maps.","When high-performing reconstruction models are combined with robust segmentation networks utilizing MTL, performance is improved in both tasks.","ATOMMIC facilitates MRI reconstruction and analysis by standardizing workflows, enhancing data interoperability, integrating unique features like MTL, and effectively benchmarking DL models."],"url":"http://arxiv.org/abs/2404.19665v1","category":"physics.med-ph"}
{"created":"2024-04-30 15:57:41","title":"Towards Generalist Robot Learning from Internet Video: A Survey","abstract":"This survey presents an overview of methods for learning from video (LfV) in the context of reinforcement learning (RL) and robotics. We focus on methods capable of scaling to large internet video datasets and, in the process, extracting foundational knowledge about the world's dynamics and physical human behaviour. Such methods hold great promise for developing general-purpose robots.   We open with an overview of fundamental concepts relevant to the LfV-for-robotics setting. This includes a discussion of the exciting benefits LfV methods can offer (e.g., improved generalization beyond the available robot data) and commentary on key LfV challenges (e.g., challenges related to missing information in video and LfV distribution shifts). Our literature review begins with an analysis of video foundation model techniques that can extract knowledge from large, heterogeneous video datasets. Next, we review methods that specifically leverage video data for robot learning. Here, we categorise work according to which RL knowledge modality benefits from the use of video data. We additionally highlight techniques for mitigating LfV challenges, including reviewing action representations that address the issue of missing action labels in video.   Finally, we examine LfV datasets and benchmarks, before concluding the survey by discussing challenges and opportunities in LfV. Here, we advocate for scalable approaches that can leverage the full range of available data and that target the key benefits of LfV. Overall, we hope this survey will serve as a comprehensive reference for the emerging field of LfV, catalysing further research in the area, and ultimately facilitating progress towards obtaining general-purpose robots.","sentences":["This survey presents an overview of methods for learning from video (LfV) in the context of reinforcement learning (RL) and robotics.","We focus on methods capable of scaling to large internet video datasets and, in the process, extracting foundational knowledge about the world's dynamics and physical human behaviour.","Such methods hold great promise for developing general-purpose robots.   ","We open with an overview of fundamental concepts relevant to the LfV-for-robotics setting.","This includes a discussion of the exciting benefits LfV methods can offer (e.g., improved generalization beyond the available robot data) and commentary on key LfV challenges (e.g., challenges related to missing information in video and LfV distribution shifts).","Our literature review begins with an analysis of video foundation model techniques that can extract knowledge from large, heterogeneous video datasets.","Next, we review methods that specifically leverage video data for robot learning.","Here, we categorise work according to which RL knowledge modality benefits from the use of video data.","We additionally highlight techniques for mitigating LfV challenges, including reviewing action representations that address the issue of missing action labels in video.   ","Finally, we examine LfV datasets and benchmarks, before concluding the survey by discussing challenges and opportunities in LfV. Here, we advocate for scalable approaches that can leverage the full range of available data and that target the key benefits of LfV. Overall, we hope this survey will serve as a comprehensive reference for the emerging field of LfV, catalysing further research in the area, and ultimately facilitating progress towards obtaining general-purpose robots."],"url":"http://arxiv.org/abs/2404.19664v1","category":"cs.RO"}
{"created":"2024-04-30 15:55:23","title":"Gravitational Lensing Using Werner's Method in Cartesian-like Coordinates","abstract":"The Gibbons-Werner method for calculating deflection angles using the Gauss-Bonnet theorem and optical/Jacobi metric has become widely popular in recent years. Werner extended this method to stationary spacetimes, where the optical/Jacobi metric takes the form of a Finsler metric of Randers type, by adopting an osculating Riemannian metric. Werner's method is significant as it provides a concise expression for the deflection angle, retains applicability for gravitational lensing in Finsler geometry beyond the Randers type, and has the potential to stimulate widespread application of Finsler geometry across diverse fields. However, because of the cumbersome calculations required in Werner's method using conventional coordinates $(r,\\phi)$, it has not been widely adopted. The aim of this paper is to alleviate the computational burden associated with Werner's method. To this end, we introduce Cartesian-like coordinates $(X,Y)$ to construct the osculating Riemannian metric and calculate the deflection angle using the Gauss-Bonnet theorem. We illustrate the current method with examples of the deflection of massive particles in Kerr spacetime, rotating Bardeen (Hayward) regular spacetime, and Teo rotating wormhole spacetime, respectively.","sentences":["The Gibbons-Werner method for calculating deflection angles using the Gauss-Bonnet theorem and optical/Jacobi metric has become widely popular in recent years.","Werner extended this method to stationary spacetimes, where the optical/Jacobi metric takes the form of a Finsler metric of Randers type, by adopting an osculating Riemannian metric.","Werner's method is significant as it provides a concise expression for the deflection angle, retains applicability for gravitational lensing in Finsler geometry beyond the Randers type, and has the potential to stimulate widespread application of Finsler geometry across diverse fields.","However, because of the cumbersome calculations required in Werner's method using conventional coordinates $(r,\\phi)$, it has not been widely adopted.","The aim of this paper is to alleviate the computational burden associated with Werner's method.","To this end, we introduce Cartesian-like coordinates $(X,Y)$ to construct the osculating Riemannian metric and calculate the deflection angle using the Gauss-Bonnet theorem.","We illustrate the current method with examples of the deflection of massive particles in Kerr spacetime, rotating Bardeen (Hayward) regular spacetime, and Teo rotating wormhole spacetime, respectively."],"url":"http://arxiv.org/abs/2404.19658v1","category":"gr-qc"}
{"created":"2024-04-30 15:54:12","title":"The Unreasonable Effectiveness of the Tunneling Potential","abstract":"The Tunneling Potential Formalism was introduced to calculate the tunneling actions that control vacuum decay as an alternative to the standard Euclidean Formalism. The new approach sets the problem as a simple variational problem in field space with decay described by a tunneling potential function $V_t$ that extremizes a simple action functional $S[V_t]$ and has a number of appealing properties that have been presented elsewhere. In this note I discuss several instances in which this $V_t$ approach seems to give more than one would have expected a priori, as the following: the $V_t$ describing the decay is a minimum of the new action $S[V_t]$ rather than a saddle point; the decay of AdS, dS or Minkowski vacua are governed by a unique universal $S[V_t]$ which also gives the Hawking-Moss instanton in the appropriate limit; physically relevant solutions beyond the Coleman-De Luccia (CdL) bounce, like pseudo-bounces or bubbles of nothing (BoNs), show up in a straightforward way as generalizations of the CdL bounce, with the correct boundary conditions; in cases for which the Euclidean action calculation requires the inclusion of particular boundary terms (like for BoNs or for the decay of AdS maxima above the Breitenlohner-Freedman bound) $S[V_t]$ gives the correct result without the need of including any boundary term.","sentences":["The Tunneling Potential Formalism was introduced to calculate the tunneling actions that control vacuum decay as an alternative to the standard Euclidean Formalism.","The new approach sets the problem as a simple variational problem in field space with decay described by a tunneling potential function $V_t$ that extremizes a simple action functional $S[V_t]$ and has a number of appealing properties that have been presented elsewhere.","In this note I discuss several instances in which this $V_t$ approach seems to give more than one would have expected a priori, as the following: the $V_t$ describing the decay is a minimum of the new action $S[V_t]$ rather than a saddle point; the decay of AdS, dS or Minkowski vacua are governed by a unique universal $S[V_t]$ which also gives the Hawking-Moss instanton in the appropriate limit; physically relevant solutions beyond the Coleman-De Luccia (CdL) bounce, like pseudo-bounces or bubbles of nothing (BoNs), show up in a straightforward way as generalizations of the CdL bounce, with the correct boundary conditions; in cases for which the Euclidean action calculation requires the inclusion of particular boundary terms (like for BoNs or for the decay of AdS maxima above the Breitenlohner-Freedman bound)","$S[V_t]$ gives the correct result without the need of including any boundary term."],"url":"http://arxiv.org/abs/2404.19657v1","category":"hep-th"}
{"created":"2024-04-30 15:52:49","title":"Towards Scenario- and Capability-Driven Dataset Development and Evaluation: An Approach in the Context of Mapless Automated Driving","abstract":"The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation. At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems. Sensor-based, mapless automated driving is one of the contexts where this limitation is evident. While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.   To address these challenges, we propose a scenario- and capability-based approach to dataset development. Grounded in the principles of ISO 21448 (safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our approach facilitates the structured derivation of dataset requirements. This not only aids in the development of meaningful new datasets but also enables the effective comparison of existing ones. Applying this methodology to a broad range of existing lane detection datasets, we identify significant limitations in current datasets, particularly in terms of real-world applicability, a lack of labeling of critical features, and an absence of comprehensive information for complex driving maneuvers.","sentences":["The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation.","At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems.","Sensor-based, mapless automated driving is one of the contexts where this limitation is evident.","While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.   ","To address these challenges, we propose a scenario- and capability-based approach to dataset development.","Grounded in the principles of ISO 21448 (safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our approach facilitates the structured derivation of dataset requirements.","This not only aids in the development of meaningful new datasets but also enables the effective comparison of existing ones.","Applying this methodology to a broad range of existing lane detection datasets, we identify significant limitations in current datasets, particularly in terms of real-world applicability, a lack of labeling of critical features, and an absence of comprehensive information for complex driving maneuvers."],"url":"http://arxiv.org/abs/2404.19656v1","category":"cs.CV"}
{"created":"2024-04-30 15:49:09","title":"BAD-NEUS: Rapidly converging trajectory stratification","abstract":"An issue for molecular dynamics simulations is that events of interest often involve timescales that are much longer than the simulation time step, which is set by the fastest timescales of the model. Because of this timescale separation, direct simulation of many events is prohibitively computationally costly. This issue can be overcome by aggregating information from many relatively short simulations that sample segments of trajectories involving events of interest. This is the strategy of Markov state models (MSMs) and related approaches, but such methods suffer from approximation error because the variables defining the states generally do not capture the dynamics fully. By contrast, once converged, the weighted ensemble (WE) method aggregates information from trajectory segments so as to yield unbiased estimates of both thermodynamic and kinetic statistics. Unfortunately, errors decay no faster than unbiased simulation in WE. Here we introduce a theoretical framework for describing WE that shows that introduction of an element of stratification, as in nonequilibrium umbrella sampling (NEUS), accelerates convergence. Then, building on ideas from MSMs and related methods, we propose an improved stratification that allows approximation error to be reduced systematically. We show that the improved stratification can decrease simulation times required to achieve a desired precision by orders of magnitude.","sentences":["An issue for molecular dynamics simulations is that events of interest often involve timescales that are much longer than the simulation time step, which is set by the fastest timescales of the model.","Because of this timescale separation, direct simulation of many events is prohibitively computationally costly.","This issue can be overcome by aggregating information from many relatively short simulations that sample segments of trajectories involving events of interest.","This is the strategy of Markov state models (MSMs) and related approaches, but such methods suffer from approximation error because the variables defining the states generally do not capture the dynamics fully.","By contrast, once converged, the weighted ensemble (WE) method aggregates information from trajectory segments so as to yield unbiased estimates of both thermodynamic and kinetic statistics.","Unfortunately, errors decay no faster than unbiased simulation in WE.","Here we introduce a theoretical framework for describing WE that shows that introduction of an element of stratification, as in nonequilibrium umbrella sampling (NEUS), accelerates convergence.","Then, building on ideas from MSMs and related methods, we propose an improved stratification that allows approximation error to be reduced systematically.","We show that the improved stratification can decrease simulation times required to achieve a desired precision by orders of magnitude."],"url":"http://arxiv.org/abs/2404.19653v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 15:49:03","title":"VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization","abstract":"Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the https://VimTextSpotter.github.io.","sentences":["Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization.","In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks.","Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters.","The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task.","Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.","Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500.","For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data.","We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data.","The code and datasets will be made available at the https://VimTextSpotter.github.io."],"url":"http://arxiv.org/abs/2404.19652v1","category":"cs.CV"}
{"created":"2024-04-30 15:49:01","title":"Provably Robust Conformal Prediction with Improved Efficiency","abstract":"Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d.. Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated. To address this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was first proposed to certify the robustness of conformal prediction methods to adversarial noise. However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets. To address these limitations, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method. Next, we propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the baseline method only yields trivial predictions including full label set, while our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and $16.9\\times$ respectively and provide practical robustness guarantee. Our codes are available at https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.","sentences":["Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d..","Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated.","To address this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was first proposed to certify the robustness of conformal prediction methods to adversarial noise.","However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets.","To address these limitations, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method.","Next, we propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead.","Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the baseline method only yields trivial predictions including full label set, while our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and $16.9\\times$ respectively and provide practical robustness guarantee.","Our codes are available at https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction."],"url":"http://arxiv.org/abs/2404.19651v1","category":"cs.LG"}
{"created":"2024-04-30 15:46:00","title":"A Fully Screen-Printed Vanadium-Dioxide Switches Based Wideband Reconfigurable Intelligent Surface for 5G Bands","abstract":"Reconfigurable Intelligent Surface (RIS) is attracting more and more research interest because of its ability to reprogram the radio environment. Designing and implementing the RIS, however, is challenging because of limitations of printed circuit board (PCB) technology related to manufacturing of large sizes as well as the cost of switches. Thus, a low-cost manufacturing process suitable for large size and volume of devices, such as screen-printing is necessary. In this paper, for the first time, a fully screen-printed reconfigurable intelligent surface (RIS) with vanadium dioxide (VO2) switches for 5G and beyond communications is proposed. A VO2 ink has been prepared and batches of switches have been printed and integrated with the resonator elements. These switches are a fraction of the cost of commercial switches. Furthermore, the printing of these switches directly on metal patterns negates the need of any minute soldering of the switches. To avoid the complications of multilayer printing and realizing the RIS without vias, the resonators and the biasing lines are realized on a single layer. However, this introduces the challenge of interference between the biasing lines and the resonators, which is tackled in this work by designing the bias lines as part of the resonator. By adjusting the unit cell periodicity and the dimension of the H-shaped resonator, we achieve a 220 to 170{\\deg} phase shift from 23.5 GHz to 29.5 GHz covering both n257 and n258 bands. Inside the wide bandwidth, the maximum ON reflection magnitude is 74%, and the maximum OFF magnitude is 94%. The RIS array comprises 20x20 unit cells (4.54x4.54{\\lambda}^2 at 29.5 GHz). Each column of unit cells is serially connected to a current biasing circuit. To validate the array's performance, we conduct full-wave simulations as well as near-field and far-field measurements.","sentences":["Reconfigurable Intelligent Surface (RIS) is attracting more and more research interest because of its ability to reprogram the radio environment.","Designing and implementing the RIS, however, is challenging because of limitations of printed circuit board (PCB) technology related to manufacturing of large sizes as well as the cost of switches.","Thus, a low-cost manufacturing process suitable for large size and volume of devices, such as screen-printing is necessary.","In this paper, for the first time, a fully screen-printed reconfigurable intelligent surface (RIS) with vanadium dioxide (VO2) switches for 5G and beyond communications is proposed.","A VO2 ink has been prepared and batches of switches have been printed and integrated with the resonator elements.","These switches are a fraction of the cost of commercial switches.","Furthermore, the printing of these switches directly on metal patterns negates the need of any minute soldering of the switches.","To avoid the complications of multilayer printing and realizing the RIS without vias, the resonators and the biasing lines are realized on a single layer.","However, this introduces the challenge of interference between the biasing lines and the resonators, which is tackled in this work by designing the bias lines as part of the resonator.","By adjusting the unit cell periodicity and the dimension of the H-shaped resonator, we achieve a 220 to 170{\\deg} phase shift from 23.5 GHz to 29.5 GHz covering both n257 and n258 bands.","Inside the wide bandwidth, the maximum ON reflection magnitude is 74%, and the maximum OFF magnitude is 94%.","The RIS array comprises 20x20 unit cells (4.54x4.54{\\lambda}^2 at 29.5 GHz).","Each column of unit cells is serially connected to a current biasing circuit.","To validate the array's performance, we conduct full-wave simulations as well as near-field and far-field measurements."],"url":"http://arxiv.org/abs/2404.19646v1","category":"eess.SP"}
{"created":"2024-04-30 15:44:55","title":"Monadic aspects of the ideal lattice functor on the category of distributive lattices","abstract":"It is known that the construction of the frame of ideals from a distributive lattice induces a monad whose algebras are precisely the frames and frame homomorphisms. Using the Fakir construction of an idempotent approximation of a monad, we extend B. Jacobs' results on lax idempotent monads and show that the sequence of monads and comonads generated by successive iterations of this ideal functor on its algebras and coalgebras do not strictly lead to a new category. We further extend this result and provide a new proof of the equivalence between distributive lattices and coherent frames by showing that when the first inductive step in the Fakir construction is the identity monad, then the ambient category is equivalent to the free algebras.","sentences":["It is known that the construction of the frame of ideals from a distributive lattice induces a monad whose algebras are precisely the frames and frame homomorphisms.","Using the Fakir construction of an idempotent approximation of a monad, we extend B. Jacobs' results on lax idempotent monads and show that the sequence of monads and comonads generated by successive iterations of this ideal functor on its algebras and coalgebras do not strictly lead to a new category.","We further extend this result and provide a new proof of the equivalence between distributive lattices and coherent frames by showing that when the first inductive step in the Fakir construction is the identity monad, then the ambient category is equivalent to the free algebras."],"url":"http://arxiv.org/abs/2404.19642v1","category":"math.CT"}
{"created":"2024-04-30 15:42:45","title":"ESP-Zero: Unsupervised enhancement of zero-shot classification for Extremely Sparse Point cloud","abstract":"In recent years, zero-shot learning has attracted the focus of many researchers, due to its flexibility and generality. Many approaches have been proposed to achieve the zero-shot classification of the point clouds for 3D object understanding, following the schema of CLIP. However, in the real world, the point clouds could be extremely sparse, dramatically limiting the effectiveness of the 3D point cloud encoders, and resulting in the misalignment of point cloud features and text embeddings. To the point cloud encoders to fit the extremely sparse point clouds without re-running the pre-training procedure which could be time-consuming and expensive, in this work, we propose an unsupervised model adaptation approach to enhance the point cloud encoder for the extremely sparse point clouds. We propose a novel fused-cross attention layer that expands the pre-trained self-attention layer with additional learnable tokens and attention blocks, which effectively modifies the point cloud features while maintaining the alignment between point cloud features and text embeddings. We also propose a complementary learning-based self-distillation schema that encourages the modified features to be pulled apart from the irrelevant text embeddings without overfitting the feature space to the observed text embeddings. Extensive experiments demonstrate that the proposed approach effectively increases the zero-shot capability on extremely sparse point clouds, and overwhelms other state-of-the-art model adaptation approaches.","sentences":["In recent years, zero-shot learning has attracted the focus of many researchers, due to its flexibility and generality.","Many approaches have been proposed to achieve the zero-shot classification of the point clouds for 3D object understanding, following the schema of CLIP.","However, in the real world, the point clouds could be extremely sparse, dramatically limiting the effectiveness of the 3D point cloud encoders, and resulting in the misalignment of point cloud features and text embeddings.","To the point cloud encoders to fit the extremely sparse point clouds without re-running the pre-training procedure which could be time-consuming and expensive, in this work, we propose an unsupervised model adaptation approach to enhance the point cloud encoder for the extremely sparse point clouds.","We propose a novel fused-cross attention layer that expands the pre-trained self-attention layer with additional learnable tokens and attention blocks, which effectively modifies the point cloud features while maintaining the alignment between point cloud features and text embeddings.","We also propose a complementary learning-based self-distillation schema that encourages the modified features to be pulled apart from the irrelevant text embeddings without overfitting the feature space to the observed text embeddings.","Extensive experiments demonstrate that the proposed approach effectively increases the zero-shot capability on extremely sparse point clouds, and overwhelms other state-of-the-art model adaptation approaches."],"url":"http://arxiv.org/abs/2404.19639v1","category":"cs.CV"}
{"created":"2024-04-30 15:34:51","title":"On Training a Neural Network to Explain Binaries","abstract":"In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding. Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign. Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction. However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models. Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries. A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs. Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space. We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated. We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value.","sentences":["In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding.","Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign.","Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction.","However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models.","Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries.","A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs.","Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space.","We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated.","We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value."],"url":"http://arxiv.org/abs/2404.19631v1","category":"cs.LG"}
{"created":"2024-04-30 15:34:51","title":"Behavioural Metrics: Compositionality of the Kantorovich Lifting and an Application to Up-To Techniques","abstract":"Behavioural distances of transition systems modelled as coalgebras for endofunctors generalize the traditional notions of behavioural equivalence to a quantitative setting, in which states are equipped with a measure of how (dis)similar they are. Endowing transition systems with such distances essentially relies on the ability to lift functors describing the one-step behavior of the transition systems to the category of pseudometric spaces. We consider the Kantorovich lifting of a functor on quantale-valued relations, which subsumes equivalences, preorders and (directed) metrics. We use tools from fibred category theory, which allow one to see the Kantorovich lifting as arising from an appropriate fibred adjunction. Our main contributions are compositionality results for the Kantorovich lifting, where we show that that the lifting of a composed functor coincides with the composition of the liftings. In addition we describe how to lift distributive laws in the case where one of the two functors is polynomial. These results are essential ingredients for adopting up-to-techniques to the case of quantale-valued behavioural distances. Up-to techniques are a well-known coinductive technique for efficiently showing lower bounds for behavioural distances. We conclude by illustrating the results of our paper in two case studies.","sentences":["Behavioural distances of transition systems modelled as coalgebras for endofunctors generalize the traditional notions of behavioural equivalence to a quantitative setting, in which states are equipped with a measure of how (dis)similar they are.","Endowing transition systems with such distances essentially relies on the ability to lift functors describing the one-step behavior of the transition systems to the category of pseudometric spaces.","We consider the Kantorovich lifting of a functor on quantale-valued relations, which subsumes equivalences, preorders and (directed) metrics.","We use tools from fibred category theory, which allow one to see the Kantorovich lifting as arising from an appropriate fibred adjunction.","Our main contributions are compositionality results for the Kantorovich lifting, where we show that that the lifting of a composed functor coincides with the composition of the liftings.","In addition we describe how to lift distributive laws in the case where one of the two functors is polynomial.","These results are essential ingredients for adopting up-to-techniques to the case of quantale-valued behavioural distances.","Up-to techniques are a well-known coinductive technique for efficiently showing lower bounds for behavioural distances.","We conclude by illustrating the results of our paper in two case studies."],"url":"http://arxiv.org/abs/2404.19632v1","category":"cs.LO"}
{"created":"2024-04-30 15:30:14","title":"Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction","abstract":"The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP. However, among these leading DL models, there is a wide variance in both the training settings and architecture used. Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success. In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS. We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect. We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size.","sentences":["The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP.","However, among these leading DL models, there is a wide variance in both the training settings and architecture used.","Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success.","In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets.","Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS.","We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect.","We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size."],"url":"http://arxiv.org/abs/2404.19630v1","category":"cs.LG"}
{"created":"2024-04-30 15:29:01","title":"The Drawback of Insight: Detailed Explanations Can Reduce Agreement with XAI","abstract":"With the emergence of Artificial Intelligence (AI)-based decision-making, explanations help increase new technology adoption through enhanced trust and reliability. However, our experimental study challenges the notion that every user universally values explanations. We argue that the agreement with AI suggestions, whether accompanied by explanations or not, is influenced by individual differences in personality traits and the users' comfort with technology. We found that people with higher neuroticism and lower technological comfort showed more agreement with the recommendations without explanations. As more users become exposed to eXplainable AI (XAI) and AI-based systems, we argue that the XAI design should not provide explanations for users with high neuroticism and low technology comfort. Prioritizing user personalities in XAI systems will help users become better collaborators of AI systems.","sentences":["With the emergence of Artificial Intelligence (AI)-based decision-making, explanations help increase new technology adoption through enhanced trust and reliability.","However, our experimental study challenges the notion that every user universally values explanations.","We argue that the agreement with AI suggestions, whether accompanied by explanations or not, is influenced by individual differences in personality traits and the users' comfort with technology.","We found that people with higher neuroticism and lower technological comfort showed more agreement with the recommendations without explanations.","As more users become exposed to eXplainable AI (XAI) and AI-based systems, we argue that the XAI design should not provide explanations for users with high neuroticism and low technology comfort.","Prioritizing user personalities in XAI systems will help users become better collaborators of AI systems."],"url":"http://arxiv.org/abs/2404.19629v1","category":"cs.HC"}
{"created":"2024-04-30 15:23:16","title":"Level-$k$ Reasoning, Cognitive Hierarchy, and Rationalizability","abstract":"We use a uniform framework to cognitive hierarchy (CH) solution concepts a decision-theoretical foundation by the epistemic game theoretical solution concept $\\Delta$-rationalizability (Battigalli and Siniscalchi, 2003). We formulate level-$k$ strategic sophistication as an information type, and, by putting intuitive conditions on the the belief of players with a strategic sophistication, we define a restriction $\\Delta^\\kappa$, from which the levels of reasoning is endogenously determined. We show that in static games, Camerer, Ho, and Chong's (2004) CH solution generically coincides with the behavioral consequence of rationality, common belief in rationality, and transparency of $\\Delta^\\kappa$; based on this, we connect CH with Bayesian equilibrium. By adapting $\\Delta^\\kappa$ into dynamic games, we show that Lin and Palfrey's (2024) DCH solution generically coincides with the behavioral consequence of rationality, common strong belief in rationality, and transparency of (dynamic) $\\Delta^\\kappa$. The same structure could analyze many variations of CH in the literature.","sentences":["We use a uniform framework to cognitive hierarchy (CH) solution concepts a decision-theoretical foundation by the epistemic game theoretical solution concept $\\Delta$-rationalizability (Battigalli and Siniscalchi, 2003).","We formulate level-$k$ strategic sophistication as an information type, and, by putting intuitive conditions on the the belief of players with a strategic sophistication, we define a restriction $\\Delta^\\kappa$, from which the levels of reasoning is endogenously determined.","We show that in static games, Camerer, Ho, and Chong's (2004) CH solution generically coincides with the behavioral consequence of rationality, common belief in rationality, and transparency of $\\Delta^\\kappa$; based on this, we connect CH with Bayesian equilibrium.","By adapting $\\Delta^\\kappa$ into dynamic games, we show that Lin and Palfrey's (2024) DCH solution generically coincides with the behavioral consequence of rationality, common strong belief in rationality, and transparency of (dynamic) $\\Delta^\\kappa$. The same structure could analyze many variations of CH in the literature."],"url":"http://arxiv.org/abs/2404.19623v1","category":"econ.TH"}
{"created":"2024-04-30 15:22:19","title":"Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis","abstract":"Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a new and emerging field. These technologies hold great promise for more human-like, efficient, expressive, and robust synthetic communication, but are currently held back by the lack of suitably large datasets, as existing methods are trained on parallel data from all constituent modalities. Inspired by student-teacher methods, we propose a straightforward solution to the data shortage, by simply synthesising additional training material. Specifically, we use unimodal synthesis models trained on large datasets to create multimodal (but synthetic) parallel training data, and then pre-train a joint synthesis model on that material. In addition, we propose a new synthesis architecture that adds better and more controllable prosody modelling to the state-of-the-art method in the field. Our results confirm that pre-training on large amounts of synthetic data improves the quality of both the speech and the motion synthesised by the multimodal model, with the proposed architecture yielding further benefits when pre-trained on the synthetic data. See https://shivammehta25.github.io/MAGI/ for example output.","sentences":["Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a new and emerging field.","These technologies hold great promise for more human-like, efficient, expressive, and robust synthetic communication, but are currently held back by the lack of suitably large datasets, as existing methods are trained on parallel data from all constituent modalities.","Inspired by student-teacher methods, we propose a straightforward solution to the data shortage, by simply synthesising additional training material.","Specifically, we use unimodal synthesis models trained on large datasets to create multimodal (but synthetic) parallel training data, and then pre-train a joint synthesis model on that material.","In addition, we propose a new synthesis architecture that adds better and more controllable prosody modelling to the state-of-the-art method in the field.","Our results confirm that pre-training on large amounts of synthetic data improves the quality of both the speech and the motion synthesised by the multimodal model, with the proposed architecture yielding further benefits when pre-trained on the synthetic data.","See https://shivammehta25.github.io/MAGI/ for example output."],"url":"http://arxiv.org/abs/2404.19622v1","category":"cs.HC"}
{"created":"2024-04-30 15:21:25","title":"Fibonacci and Lucas Sequences in Aperiodic Monotile Supertiles","abstract":"This paper first discusses the size and orientation of hat supertiles. Fibonacci and Lucas sequences, as well as a third integer sequence linearly related to the Lucas sequence are involved. The result is then generalized to any aperiodic tile in the hat family.","sentences":["This paper first discusses the size and orientation of hat supertiles.","Fibonacci and Lucas sequences, as well as a third integer sequence linearly related to the Lucas sequence are involved.","The result is then generalized to any aperiodic tile in the hat family."],"url":"http://arxiv.org/abs/2404.19621v1","category":"math.CO"}
{"created":"2024-04-30 15:18:46","title":"Novel Round Trip Time Estimation in 5G NR","abstract":"The fifth generation new radio (5G NR) technology is expected to fulfill reliable and accurate positioning requirements of industry use cases, such as autonomous robots, connected vehicles, and future factories. Starting from Third Generation Partnership Project (3GPP) Release-16, several enhanced positioning solutions are featured in the 5G standards, including the multi-cell round trip time (multi-RTT) method. This work presents a novel framework to estimate the round-trip time (RTT) between a user equipment (UE) and a base station (gNB) in 5G NR. Unlike the existing scheme in the standards, RTT can be estimated without the need to send timing measurements from both the gNB and UE to a central node. The proposed method relies on obtaining multiple coherent uplink wide-band channel measurements at the gNB by circumventing the timing advance control loops and the clock drift. The performance is evaluated through experiments leveraging a real world 5G testbed based on OpenAirInterface (OAI). Under a moderate system bandwidth of 40MHz, the experimental results show meter level range accuracy even in low signal-to-noise ratio (SNR) conditions.","sentences":["The fifth generation new radio (5G NR) technology is expected to fulfill reliable and accurate positioning requirements of industry use cases, such as autonomous robots, connected vehicles, and future factories.","Starting from Third Generation Partnership Project (3GPP) Release-16, several enhanced positioning solutions are featured in the 5G standards, including the multi-cell round trip time (multi-RTT) method.","This work presents a novel framework to estimate the round-trip time (RTT) between a user equipment (UE) and a base station (gNB) in 5G NR.","Unlike the existing scheme in the standards, RTT can be estimated without the need to send timing measurements from both the gNB and UE to a central node.","The proposed method relies on obtaining multiple coherent uplink wide-band channel measurements at the gNB by circumventing the timing advance control loops and the clock drift.","The performance is evaluated through experiments leveraging a real world 5G testbed based on OpenAirInterface (OAI).","Under a moderate system bandwidth of 40MHz, the experimental results show meter level range accuracy even in low signal-to-noise ratio (SNR) conditions."],"url":"http://arxiv.org/abs/2404.19618v1","category":"cs.IT"}
{"created":"2024-04-30 15:14:03","title":"Unified Framework of Forced Magnetic Reconnection and Alfv\u00e9n Resonance","abstract":"A unified linear theory that includes forced reconnection as a particular case of Alfv\\'en resonance is presented. We consider a generalized Taylor problem in which a sheared magnetic field is subject to a time-dependent boundary perturbation oscillating at frequency $\\omega_0$. By analyzing the asymptotic time response of the system, the theory demonstrates that the Alfv\\'en resonance is due to the residues at the resonant poles, in the complex frequency plane, introduced by the boundary perturbation. Alfv\\'en resonance transitions towards forced reconnection, described by the constant-psi regime for (normalized) times $t\\gg S^{1/3}$, when the forcing frequency of the boundary perturbation is $\\omega_0\\ll S^{-1/3}$, allowing the coupling of the Alfv\\'en resonances across the neutral line with the reconnecting mode, as originally suggested in [1]. Additionally, it is shown that even if forced reconnection develops for finite, albeit small, frequencies, the reconnection rate and reconnected flux are strongly reduced for frequencies $\\omega_0\\gg S^{-3/5}$.","sentences":["A unified linear theory that includes forced reconnection as a particular case of Alfv\\'en resonance is presented.","We consider a generalized Taylor problem in which a sheared magnetic field is subject to a time-dependent boundary perturbation oscillating at frequency $\\omega_0$. By analyzing the asymptotic time response of the system, the theory demonstrates that the Alfv\\'en resonance is due to the residues at the resonant poles, in the complex frequency plane, introduced by the boundary perturbation.","Alfv\\'en","resonance transitions towards forced reconnection, described by the constant-psi regime for (normalized) times $t\\gg S^{1/3}$, when the forcing frequency of the boundary perturbation is $\\omega_0\\ll S^{-1/3}$, allowing the coupling of the Alfv\\'en resonances across the neutral line with the reconnecting mode, as originally suggested in [1].","Additionally, it is shown that even if forced reconnection develops for finite, albeit small, frequencies, the reconnection rate and reconnected flux are strongly reduced for frequencies $\\omega_0\\gg S^{-3/5}$."],"url":"http://arxiv.org/abs/2404.19616v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 15:12:31","title":"COTS: Connected OpenAPI Test Synthesis for RESTful Applications","abstract":"We present a novel model-driven approach for testing RESTful applications. We introduce a (i) domain-specific language for OpenAPI specifications and (ii) a tool to support our methodology. Our DSL is inspired by session types and enables the modelling of communication protocols between a REST client and server. Our tool, dubbed COTS, generates (randomised) model-based test executions and reports software defects. We evaluate the effectiveness of our approach by applying it to test several open source applications. Our findings indicate that our methodology can identify nuanced defects in REST APIs and achieve comparable or superior code coverage when compared to much larger handcrafted test suites.","sentences":["We present a novel model-driven approach for testing RESTful applications.","We introduce a (i) domain-specific language for OpenAPI specifications and (ii) a tool to support our methodology.","Our DSL is inspired by session types and enables the modelling of communication protocols between a REST client and server.","Our tool, dubbed COTS, generates (randomised) model-based test executions and reports software defects.","We evaluate the effectiveness of our approach by applying it to test several open source applications.","Our findings indicate that our methodology can identify nuanced defects in REST APIs and achieve comparable or superior code coverage when compared to much larger handcrafted test suites."],"url":"http://arxiv.org/abs/2404.19614v1","category":"cs.SE"}
{"created":"2024-04-30 15:12:26","title":"High-throughput discovery of metal oxides with high thermoelectric performance via interpretable feature engineering on small data","abstract":"In this work, we have proposed a data-driven screening framework combining the interpretable machine learning with high-throughput calculations to identify a series of metal oxides that exhibit both high-temperature tolerance and high power factors. Aiming at the problem of weak generalization ability of small data with power factors at high temperatures, we employ symbolic regression for feature creation which enhances the robustness of the model while preserving the physical meaning of features. 33 candidate metal oxides are finally targeted for high-temperature thermoelectric applications from a pool of 48,694 compounds in the Materials Project database. The Boltzmann transport theory is utilized to perform electrical transport properties calculations at 1,000 K. The relaxation time is approximated by employing constant electron-phonon coupling based on the deformation potential theory. Considering band degeneracy, the electron group velocity is obtained using the momentum matrix element method, yielding 28 materials with power factors greater than 50 ${\\mu}W cm^{-1} K^{-2} $. The high-throughput framework we proposed is instrumental in the selection of metal oxides for high-temperature thermoelectric applications. Furthermore, our data-driven analysis and transport calculation suggest that metal oxides rich in elements such as cerium (Ce), tin (Sn), and lead (Pb) tend to exhibit high power factors at high temperatures.","sentences":["In this work, we have proposed a data-driven screening framework combining the interpretable machine learning with high-throughput calculations to identify a series of metal oxides that exhibit both high-temperature tolerance and high power factors.","Aiming at the problem of weak generalization ability of small data with power factors at high temperatures, we employ symbolic regression for feature creation which enhances the robustness of the model while preserving the physical meaning of features.","33 candidate metal oxides are finally targeted for high-temperature thermoelectric applications from a pool of 48,694 compounds in the Materials Project database.","The Boltzmann transport theory is utilized to perform electrical transport properties calculations at 1,000 K. The relaxation time is approximated by employing constant electron-phonon coupling based on the deformation potential theory.","Considering band degeneracy, the electron group velocity is obtained using the momentum matrix element method, yielding 28 materials with power factors greater than 50 ${\\mu}W cm^{-1} K^{-2} $.","The high-throughput framework we proposed is instrumental in the selection of metal oxides for high-temperature thermoelectric applications.","Furthermore, our data-driven analysis and transport calculation suggest that metal oxides rich in elements such as cerium (Ce), tin (Sn), and lead (Pb) tend to exhibit high power factors at high temperatures."],"url":"http://arxiv.org/abs/2404.19613v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 15:06:20","title":"Radio Resource Management Design for RSMA: Optimization of Beamforming, User Admission, and Discrete/Continuous Rates with Imperfect SIC","abstract":"This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC). Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources. Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve. Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions. We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains. Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate. Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics. Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable.","sentences":["This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC).","Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources.","Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve.","Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions.","We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains.","Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate.","Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics.","Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable."],"url":"http://arxiv.org/abs/2404.19611v1","category":"eess.SP"}
{"created":"2024-04-30 15:05:34","title":"Hilbert series of representations of categories of $G$-sets","abstract":"Let $G$ be a finite group. A contravariant functor from the category of finite free $G$-sets to vector spaces has an associated Hilbert series, which records the underlying sequence of $G^n$ representations, $n \\in \\mathbb N$. We prove that this Hilbert series is rational with denominator given by linear polynomials with coefficients in the field generated by the character table of $G$.","sentences":["Let $G$ be a finite group.","A contravariant functor from the category of finite free $G$-sets to vector spaces has an associated Hilbert series, which records the underlying sequence of $G^n$ representations, $n \\in \\mathbb N$.","We prove that this Hilbert series is rational with denominator given by linear polynomials with coefficients in the field generated by the character table of $G$."],"url":"http://arxiv.org/abs/2404.19610v1","category":"math.RT"}
{"created":"2024-04-30 15:03:27","title":"Seeing Through the Clouds: Cloud Gap Imputation with Prithvi Foundation Model","abstract":"Filling cloudy pixels in multispectral satellite imagery is essential for accurate data analysis and downstream applications, especially for tasks which require time series data. To address this issue, we compare the performance of a foundational Vision Transformer (ViT) model with a baseline Conditional Generative Adversarial Network (CGAN) model for missing value imputation in time series of multispectral satellite imagery. We randomly mask time series of satellite images using real-world cloud masks and train each model to reconstruct the missing pixels. The ViT model is fine-tuned from a pretrained model, while the CGAN is trained from scratch. Using quantitative evaluation metrics such as structural similarity index and mean absolute error as well as qualitative visual analysis, we assess imputation accuracy and contextual preservation.","sentences":["Filling cloudy pixels in multispectral satellite imagery is essential for accurate data analysis and downstream applications, especially for tasks which require time series data.","To address this issue, we compare the performance of a foundational Vision Transformer (ViT) model with a baseline Conditional Generative Adversarial Network (CGAN) model for missing value imputation in time series of multispectral satellite imagery.","We randomly mask time series of satellite images using real-world cloud masks and train each model to reconstruct the missing pixels.","The ViT model is fine-tuned from a pretrained model, while the CGAN is trained from scratch.","Using quantitative evaluation metrics such as structural similarity index and mean absolute error as well as qualitative visual analysis, we assess imputation accuracy and contextual preservation."],"url":"http://arxiv.org/abs/2404.19609v1","category":"cs.CV"}
{"created":"2024-04-30 15:00:58","title":"Three-dimensional Moir\u00e9 Crystal","abstract":"The work intends to extend the moir\\'e physics to three dimensions. Three-dimensional moir\\'e patterns can be realized in ultracold atomic gases by coupling two spin states in spin-dependent optical lattices with a relative twist, a structure currently unachievable in solid-state materials. We give the commensurate conditions under which the three-dimensional moir\\'e pattern features a periodic structure, termed a three-dimensional moir\\'e crystal. We emphasize a key distinction of three-dimensional moir\\'e physics: in three dimensions, the twist operation generically does not commute with the rotational symmetry of the original lattice, unlike in two dimensions, where these two always commute. Consequently, the moir\\'e crystal can exhibit a crystalline structure that differs from the original underlying lattice. We demonstrate that twisting a simple cubic lattice can generate various crystal structures. This capability of altering crystal structures by twisting offers a broad range of tunability for three-dimensional band structures.","sentences":["The work intends to extend the moir\\'e physics to three dimensions.","Three-dimensional moir\\'e patterns can be realized in ultracold atomic gases by coupling two spin states in spin-dependent optical lattices with a relative twist, a structure currently unachievable in solid-state materials.","We give the commensurate conditions under which the three-dimensional moir\\'e pattern features a periodic structure, termed a three-dimensional moir\\'e crystal.","We emphasize a key distinction of three-dimensional moir\\'e physics: in three dimensions, the twist operation generically does not commute with the rotational symmetry of the original lattice, unlike in two dimensions, where these two always commute.","Consequently, the moir\\'e crystal can exhibit a crystalline structure that differs from the original underlying lattice.","We demonstrate that twisting a simple cubic lattice can generate various crystal structures.","This capability of altering crystal structures by twisting offers a broad range of tunability for three-dimensional band structures."],"url":"http://arxiv.org/abs/2404.19608v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 14:53:07","title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models","abstract":"In this work, we present X-Diffusion, a cross-sectional diffusion model tailored for Magnetic Resonance Imaging (MRI) data. X-Diffusion is capable of generating the entire MRI volume from just a single MRI slice or optionally from few multiple slices, setting new benchmarks in the precision of synthesized MRIs from extremely sparse observations. The uniqueness lies in the novel view-conditional training and inference of X-Diffusion on MRI volumes, allowing for generalized MRI learning. Our evaluations span both brain tumour MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) and MRI modalities in the UK Biobank dataset, X-Diffusion is able to generate detailed 3D MRI volume from a single full-body DXA. Remarkably, the resultant MRIs not only stand out in precision on unseen examples (surpassing state-of-the-art results by large margins) but also flawlessly retain essential features of the original MRI, including tumour profiles, spine curvature, brain volume, and beyond. Furthermore, the trained X-Diffusion model on the MRI datasets attains a generalization capacity out-of-domain (e.g. generating knee MRIs even though it is trained on brains). The code is available on the project website https://emmanuelleb985.github.io/XDiffusion/ .","sentences":["In this work, we present X-Diffusion, a cross-sectional diffusion model tailored for Magnetic Resonance Imaging (MRI) data.","X-Diffusion is capable of generating the entire MRI volume from just a single MRI slice or optionally from few multiple slices, setting new benchmarks in the precision of synthesized MRIs from extremely sparse observations.","The uniqueness lies in the novel view-conditional training and inference of X-Diffusion on MRI volumes, allowing for generalized MRI learning.","Our evaluations span both brain tumour MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset.","Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) and MRI modalities in the UK Biobank dataset, X-Diffusion is able to generate detailed 3D MRI volume from a single full-body DXA.","Remarkably, the resultant MRIs not only stand out in precision on unseen examples (surpassing state-of-the-art results by large margins) but also flawlessly retain essential features of the original MRI, including tumour profiles, spine curvature, brain volume, and beyond.","Furthermore, the trained X-Diffusion model on the MRI datasets attains a generalization capacity out-of-domain (e.g. generating knee MRIs even though it is trained on brains).","The code is available on the project website https://emmanuelleb985.github.io/XDiffusion/ ."],"url":"http://arxiv.org/abs/2404.19604v1","category":"eess.IV"}
{"created":"2024-04-30 14:52:15","title":"Uncertainty quantification for charge transport in GNRs through particle Galerkin methods for the semiclassical Boltzmann equation","abstract":"In this article, we investigate some issues related to the quantification of uncertainties associated with the electrical properties of graphene nanoribbons. The approach is suited to understand the effects of missing information linked to the difficulty of fixing some material parameters, such as the band gap, and the strength of the applied electric field. In particular, we focus on the extension of particle Galerkin methods for kinetic equations in the case of the semiclassical Boltzmann equation for charge transport in graphene nanoribbons with uncertainties. To this end, we develop an efficient particle scheme which allows us to parallelize the computation and then, after a suitable generalization of the scheme to the case of random inputs, we present a Galerkin reformulation of the particle dynamics, obtained by means of a generalized polynomial chaos approach, which allows the reconstruction of the kinetic distribution. As a consequence, the proposed particle-based scheme preserves the physical properties and the positivity of the distribution function also in the presence of a complex scattering in the transport equation of electrons. The impact of the uncertainty of the band gap and applied field on the electrical current is analyzed.","sentences":["In this article, we investigate some issues related to the quantification of uncertainties associated with the electrical properties of graphene nanoribbons.","The approach is suited to understand the effects of missing information linked to the difficulty of fixing some material parameters, such as the band gap, and the strength of the applied electric field.","In particular, we focus on the extension of particle Galerkin methods for kinetic equations in the case of the semiclassical Boltzmann equation for charge transport in graphene nanoribbons with uncertainties.","To this end, we develop an efficient particle scheme which allows us to parallelize the computation and then, after a suitable generalization of the scheme to the case of random inputs, we present a Galerkin reformulation of the particle dynamics, obtained by means of a generalized polynomial chaos approach, which allows the reconstruction of the kinetic distribution.","As a consequence, the proposed particle-based scheme preserves the physical properties and the positivity of the distribution function also in the presence of a complex scattering in the transport equation of electrons.","The impact of the uncertainty of the band gap and applied field on the electrical current is analyzed."],"url":"http://arxiv.org/abs/2404.19602v1","category":"physics.comp-ph"}
{"created":"2024-04-30 14:51:39","title":"Stabilized POD Reduced Order Models for convection-dominated incompressible flows","abstract":"We present a comparative computational study of two stabilized Reduced Order Models (ROMs) for the simulation of convection-dominated incompressible flow (Reynolds number of the order of a few thousands). Representative solutions in the parameter space, which includes either time only or time and Reynolds number, are computed with a Finite Volume method and used to generate a reduced basis via Proper Orthogonal Decomposition (POD). Galerkin projection of the Navier-Stokes equations onto the reduced space is used to compute the ROM solution. To ensure computational efficiency, the number of POD modes is truncated and ROM solution accuracy is recovered through two stabilization methods: i) adding a global constant artificial viscosity to the reduced dimensional model, and ii) adding a different value of artificial viscosity for the different POD modes. We test the stabilized ROMs for fluid flow in an idealized medical device consisting of a conical convergent, a narrow throat, and a sudden expansion. Both stabilization methods significantly improve the ROM solution accuracy over a standard (non-stabilized) POD-Galerkin model.","sentences":["We present a comparative computational study of two stabilized Reduced Order Models (ROMs) for the simulation of convection-dominated incompressible flow (Reynolds number of the order of a few thousands).","Representative solutions in the parameter space, which includes either time only or time and Reynolds number, are computed with a Finite Volume method and used to generate a reduced basis via Proper Orthogonal Decomposition (POD).","Galerkin projection of the Navier-Stokes equations onto the reduced space is used to compute the ROM solution.","To ensure computational efficiency, the number of POD modes is truncated and ROM solution accuracy is recovered through two stabilization methods: i) adding a global constant artificial viscosity to the reduced dimensional model, and ii) adding a different value of artificial viscosity for the different POD modes.","We test the stabilized ROMs for fluid flow in an idealized medical device consisting of a conical convergent, a narrow throat, and a sudden expansion.","Both stabilization methods significantly improve the ROM solution accuracy over a standard (non-stabilized) POD-Galerkin model."],"url":"http://arxiv.org/abs/2404.19600v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 14:49:03","title":"Artificial Intelligence in Bone Metastasis Analysis: Current Advancements, Opportunities and Challenges","abstract":"In recent years, Artificial Intelligence (AI) has been widely used in medicine, particularly in the analysis of medical imaging, which has been driven by advances in computer vision and deep learning methods. This is particularly important in overcoming the challenges posed by diseases such as Bone Metastases (BM), a common and complex malignancy of the bones. Indeed, there have been an increasing interest in developing Machine Learning (ML) techniques into oncologic imaging for BM analysis. In order to provide a comprehensive overview of the current state-of-the-art and advancements for BM analysis using artificial intelligence, this review is conducted with the accordance with PRISMA guidelines. Firstly, this review highlights the clinical and oncologic perspectives of BM and the used medical imaging modalities, with discussing their advantages and limitations. Then the review focuses on modern approaches with considering the main BM analysis tasks, which includes: classification, detection and segmentation. The results analysis show that ML technologies can achieve promising performance for BM analysis and have significant potential to improve clinician efficiency and cope with time and cost limitations. Furthermore, there are requirements for further research to validate the clinical performance of ML tools and facilitate their integration into routine clinical practice.","sentences":["In recent years, Artificial Intelligence (AI) has been widely used in medicine, particularly in the analysis of medical imaging, which has been driven by advances in computer vision and deep learning methods.","This is particularly important in overcoming the challenges posed by diseases such as Bone Metastases (BM), a common and complex malignancy of the bones.","Indeed, there have been an increasing interest in developing Machine Learning (ML) techniques into oncologic imaging for BM analysis.","In order to provide a comprehensive overview of the current state-of-the-art and advancements for BM analysis using artificial intelligence, this review is conducted with the accordance with PRISMA guidelines.","Firstly, this review highlights the clinical and oncologic perspectives of BM and the used medical imaging modalities, with discussing their advantages and limitations.","Then the review focuses on modern approaches with considering the main BM analysis tasks, which includes: classification, detection and segmentation.","The results analysis show that ML technologies can achieve promising performance for BM analysis and have significant potential to improve clinician efficiency and cope with time and cost limitations.","Furthermore, there are requirements for further research to validate the clinical performance of ML tools and facilitate their integration into routine clinical practice."],"url":"http://arxiv.org/abs/2404.19598v1","category":"eess.IV"}
{"created":"2024-04-30 14:43:51","title":"Debiased Collaborative Filtering with Kernel-Based Causal Balancing","abstract":"Debiased collaborative filtering aims to learn an unbiased prediction model by removing different biases in observational datasets. To solve this problem, one of the simple and effective methods is based on the propensity score, which adjusts the observational sample distribution to the target one by reweighting observed instances. Ideally, propensity scores should be learned with causal balancing constraints. However, existing methods usually ignore such constraints or implement them with unreasonable approximations, which may affect the accuracy of the learned propensity scores. To bridge this gap, in this paper, we first analyze the gaps between the causal balancing requirements and existing methods such as learning the propensity with cross-entropy loss or manually selecting functions to balance. Inspired by these gaps, we propose to approximate the balancing functions in reproducing kernel Hilbert space and demonstrate that, based on the universal property and representer theorem of kernel functions, the causal balancing constraints can be better satisfied. Meanwhile, we propose an algorithm that adaptively balances the kernel function and theoretically analyze the generalization error bound of our methods. We conduct extensive experiments to demonstrate the effectiveness of our methods, and to promote this research direction, we have released our project at https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.","sentences":["Debiased collaborative filtering aims to learn an unbiased prediction model by removing different biases in observational datasets.","To solve this problem, one of the simple and effective methods is based on the propensity score, which adjusts the observational sample distribution to the target one by reweighting observed instances.","Ideally, propensity scores should be learned with causal balancing constraints.","However, existing methods usually ignore such constraints or implement them with unreasonable approximations, which may affect the accuracy of the learned propensity scores.","To bridge this gap, in this paper, we first analyze the gaps between the causal balancing requirements and existing methods such as learning the propensity with cross-entropy loss or manually selecting functions to balance.","Inspired by these gaps, we propose to approximate the balancing functions in reproducing kernel Hilbert space and demonstrate that, based on the universal property and representer theorem of kernel functions, the causal balancing constraints can be better satisfied.","Meanwhile, we propose an algorithm that adaptively balances the kernel function and theoretically analyze the generalization error bound of our methods.","We conduct extensive experiments to demonstrate the effectiveness of our methods, and to promote this research direction, we have released our project at https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing."],"url":"http://arxiv.org/abs/2404.19596v1","category":"cs.IR"}
{"created":"2024-04-30 14:41:06","title":"Reactive Temporal Logic-based Planning and Control for Interactive Robotic Tasks","abstract":"Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes. Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt. To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans. We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes. At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL). Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes.","sentences":["Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes.","Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt.","To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans.","We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes.","At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL).","Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes."],"url":"http://arxiv.org/abs/2404.19594v1","category":"cs.RO"}
{"created":"2024-04-30 14:39:45","title":"Magnetophononics and the Chiral Phonon Misnomer","abstract":"The direct, ultrafast excitation of polar phonons with electromagnetic radiation is a potent strategy for controlling the properties of a wide range of materials, particularly in the context of influencing their magnetic behavior. Here, we show that, contrary to common perception, the origin of phonon-induced magnetic activity does not stem from the motion of ions themselves; instead, it arises from the effect their motion exerts on the electron subsystem. Through the mechanism of electron-phonon coupling, a coherent state of circularly polarized phonons generates substantial non-Maxwellian fields that disrupt time reversal symmetry, effectively emulating the behavior of authentic magnetic fields. Notably, the effective field can reach magnitudes as high as 100 T, surpassing by several orders of magnitude the Maxwellian field resulting from the circular motion of the ions. Because the light-induced non-reciprocal fields depend on the square of the phonon displacements, the chirality the photons transferred to the ions plays no role in magnetophononics.","sentences":["The direct, ultrafast excitation of polar phonons with electromagnetic radiation is a potent strategy for controlling the properties of a wide range of materials, particularly in the context of influencing their magnetic behavior.","Here, we show that, contrary to common perception, the origin of phonon-induced magnetic activity does not stem from the motion of ions themselves; instead, it arises from the effect their motion exerts on the electron subsystem.","Through the mechanism of electron-phonon coupling, a coherent state of circularly polarized phonons generates substantial non-Maxwellian fields that disrupt time reversal symmetry, effectively emulating the behavior of authentic magnetic fields.","Notably, the effective field can reach magnitudes as high as 100 T, surpassing by several orders of magnitude the Maxwellian field resulting from the circular motion of the ions.","Because the light-induced non-reciprocal fields depend on the square of the phonon displacements, the chirality the photons transferred to the ions plays no role in magnetophononics."],"url":"http://arxiv.org/abs/2404.19593v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 14:36:04","title":"Towards Interactively Improving ML Data Preparation Code via \"Shadow Pipelines\"","abstract":"Data scientists develop ML pipelines in an iterative manner: they repeatedly screen a pipeline for potential issues, debug it, and then revise and improve its code according to their findings. However, this manual process is tedious and error-prone. Therefore, we propose to support data scientists during this development cycle with automatically derived interactive suggestions for pipeline improvements. We discuss our vision to generate these suggestions with so-called shadow pipelines, hidden variants of the original pipeline that modify it to auto-detect potential issues, try out modifications for improvements, and suggest and explain these modifications to the user. We envision to apply incremental view maintenance-based optimisations to ensure low-latency computation and maintenance of the shadow pipelines. We conduct preliminary experiments to showcase the feasibility of our envisioned approach and the potential benefits of our proposed optimisations.","sentences":["Data scientists develop ML pipelines in an iterative manner: they repeatedly screen a pipeline for potential issues, debug it, and then revise and improve its code according to their findings.","However, this manual process is tedious and error-prone.","Therefore, we propose to support data scientists during this development cycle with automatically derived interactive suggestions for pipeline improvements.","We discuss our vision to generate these suggestions with so-called shadow pipelines, hidden variants of the original pipeline that modify it to auto-detect potential issues, try out modifications for improvements, and suggest and explain these modifications to the user.","We envision to apply incremental view maintenance-based optimisations to ensure low-latency computation and maintenance of the shadow pipelines.","We conduct preliminary experiments to showcase the feasibility of our envisioned approach and the potential benefits of our proposed optimisations."],"url":"http://arxiv.org/abs/2404.19591v1","category":"cs.DB"}
{"created":"2024-04-30 14:31:23","title":"Internal migration after a uniform minimum wage introduction","abstract":"Internal migration is an essential aspect to study labor mobility. I exploit the German statutory minimum wage introduction in 2015 to estimate its push and pull effects on internal migration using a 2% sample of administrative data. In a conditional fixed effects Poisson difference-in-differences framework with a continuous treatment, I find that the minimum wage introduction leads to an increase in the out-migration of low-skilled workers with migrant background by 25% with an increasing tendency over time from districts where a high share of workers are subject to the minimum wage (high-bite districts). In contrast the migration decision of native-born low-skilled workers is not affected by the policy. However, both native-born low-skilled workers and those with a migrant background do relocate across establishments, leaving high-bite districts as their workplace. In addition, I find an increase for unemployed individuals with a migrant background in out-migrating from high-bite districts. These results emphasize the importance of considering the effects on geographical labor mobility when implementing and analyzing policies that affect the determinants of internal migration.","sentences":["Internal migration is an essential aspect to study labor mobility.","I exploit the German statutory minimum wage introduction in 2015 to estimate its push and pull effects on internal migration using a 2% sample of administrative data.","In a conditional fixed effects Poisson difference-in-differences framework with a continuous treatment, I find that the minimum wage introduction leads to an increase in the out-migration of low-skilled workers with migrant background by 25% with an increasing tendency over time from districts where a high share of workers are subject to the minimum wage (high-bite districts).","In contrast the migration decision of native-born low-skilled workers is not affected by the policy.","However, both native-born low-skilled workers and those with a migrant background do relocate across establishments, leaving high-bite districts as their workplace.","In addition, I find an increase for unemployed individuals with a migrant background in out-migrating from high-bite districts.","These results emphasize the importance of considering the effects on geographical labor mobility when implementing and analyzing policies that affect the determinants of internal migration."],"url":"http://arxiv.org/abs/2404.19590v1","category":"econ.GN"}
{"created":"2024-04-30 14:25:32","title":"AI techniques for near real-time monitoring of contaminants in coastal waters on board future Phisat-2 mission","abstract":"Differently from conventional procedures, the proposed solution advocates for a groundbreaking paradigm in water quality monitoring through the integration of satellite Remote Sensing (RS) data, Artificial Intelligence (AI) techniques, and onboard processing. The objective is to offer nearly real-time detection of contaminants in coastal waters addressing a significant gap in the existing literature. Moreover, the expected outcomes include substantial advancements in environmental monitoring, public health protection, and resource conservation. The specific focus of our study is on the estimation of Turbidity and pH parameters, for their implications on human and aquatic health. Nevertheless, the designed framework can be extended to include other parameters of interest in the water environment and beyond. Originating from our participation in the European Space Agency (ESA) OrbitalAI Challenge, this article describes the distinctive opportunities and issues for the contaminants monitoring on the Phisat-2 mission. The specific characteristics of this mission, with the tools made available, will be presented, with the methodology proposed by the authors for the onboard monitoring of water contaminants in near real-time. Preliminary promising results are discussed and in progress and future work introduced.","sentences":["Differently from conventional procedures, the proposed solution advocates for a groundbreaking paradigm in water quality monitoring through the integration of satellite Remote Sensing (RS) data, Artificial Intelligence (AI) techniques, and onboard processing.","The objective is to offer nearly real-time detection of contaminants in coastal waters addressing a significant gap in the existing literature.","Moreover, the expected outcomes include substantial advancements in environmental monitoring, public health protection, and resource conservation.","The specific focus of our study is on the estimation of Turbidity and pH parameters, for their implications on human and aquatic health.","Nevertheless, the designed framework can be extended to include other parameters of interest in the water environment and beyond.","Originating from our participation in the European Space Agency (ESA) OrbitalAI","Challenge, this article describes the distinctive opportunities and issues for the contaminants monitoring on the Phisat-2 mission.","The specific characteristics of this mission, with the tools made available, will be presented, with the methodology proposed by the authors for the onboard monitoring of water contaminants in near real-time.","Preliminary promising results are discussed and in progress and future work introduced."],"url":"http://arxiv.org/abs/2404.19586v1","category":"cs.CV"}
{"created":"2024-04-30 14:22:12","title":"Broadband microwave-rate dark pulse microcombs in dissipation-engineered LiNbO$_3$ microresonators","abstract":"Kerr microcombs generated in optical microresonators provide broadband light sources bridging optical and microwave signals. Their translation to thin-film lithium niobate unlocks second-order nonlinear optical interfaces such as electro-optic modulation and frequency doubling for completing comb functionalities. However, the strong Raman response of LiNbO$_3$ has complicated the formation of Kerr microcombs. Until now, dark pulse microcombs, requiring a double balance between Kerr nonlinearity and normal group velocity dispersion as well as gain and loss, have remained elusive in LiNbO$_3$ microresonators. Here, by incorporating dissipation engineering, we demonstrate dark pulse microcombs with 25 GHz repetition frequency and 200 nm span in a high-$Q$ LiNbO$_3$ microresonator. Resonances near the Raman-active wavelengths are strongly damped by controlling phase-matching conditions of a specially designed pulley coupler. The coherence and tunability of the dark pulse microcombs are also investigated. Our work provides a solution to realize high-power microcombs operating at microwave rates on LiNbO$_3$ chips, promising new opportunities for the monolithic integration of applications spanning communication to microwave photonics.","sentences":["Kerr microcombs generated in optical microresonators provide broadband light sources bridging optical and microwave signals.","Their translation to thin-film lithium niobate unlocks second-order nonlinear optical interfaces such as electro-optic modulation and frequency doubling for completing comb functionalities.","However, the strong Raman response of LiNbO$_3$ has complicated the formation of Kerr microcombs.","Until now, dark pulse microcombs, requiring a double balance between Kerr nonlinearity and normal group velocity dispersion as well as gain and loss, have remained elusive in LiNbO$_3$ microresonators.","Here, by incorporating dissipation engineering, we demonstrate dark pulse microcombs with 25 GHz repetition frequency and 200 nm span in a high-$Q$ LiNbO$_3$ microresonator.","Resonances near the Raman-active wavelengths are strongly damped by controlling phase-matching conditions of a specially designed pulley coupler.","The coherence and tunability of the dark pulse microcombs are also investigated.","Our work provides a solution to realize high-power microcombs operating at microwave rates on LiNbO$_3$ chips, promising new opportunities for the monolithic integration of applications spanning communication to microwave photonics."],"url":"http://arxiv.org/abs/2404.19584v1","category":"physics.optics"}
{"created":"2024-04-30 14:18:11","title":"Soft pattern of gravitational Rutherford scattering from heavy target mass expansion","abstract":"We investigate the soft behavior of the tree-level Rutherford scattering processes mediated via $t$-channel one-graviton exchange. We consider two types of Rutherford scattering processes, {\\it e.g.}, a low-energy massless structureless projectile (up to spin-$1$) hits a static massive composite particle carrying various spins (up to spin-$2$), and a slowly-moving light projectile hits a heavy static composite target. The unpolarized cross sections in the first type are found to exhibit universal forms at the first two orders in $1/M$ expansion, yet differ at the next-to-next-to-leading order, though some terms at this order still remain universal or depend on the target spin in a definite manner. The unpolarized cross sections in the second type are universal at the lowest order in projectile velocity expansion and through all orders in $1/M$, independent of the spins of both projectile and target. The universality partially breaks down at relative order-$v^2/M^2$, albeit some terms at this order still depend on the target spin in a specific manner.","sentences":["We investigate the soft behavior of the tree-level Rutherford scattering processes mediated via $t$-channel one-graviton exchange.","We consider two types of Rutherford scattering processes, {\\it e.g.}, a low-energy massless structureless projectile (up to spin-$1$) hits a static massive composite particle carrying various spins (up to spin-$2$), and a slowly-moving light projectile hits a heavy static composite target.","The unpolarized cross sections in the first type are found to exhibit universal forms at the first two orders in $1/M$ expansion, yet differ at the next-to-next-to-leading order, though some terms at this order still remain universal or depend on the target spin in a definite manner.","The unpolarized cross sections in the second type are universal at the lowest order in projectile velocity expansion and through all orders in $1/M$, independent of the spins of both projectile and target.","The universality partially breaks down at relative order-$v^2/M^2$, albeit some terms at this order still depend on the target spin in a specific manner."],"url":"http://arxiv.org/abs/2404.19581v1","category":"hep-ph"}
{"created":"2024-04-30 14:11:37","title":"Test function approach to fully nonlinear equations in thin domains","abstract":"In this note we extend to fully nonlinear operators the well known result on thin domains of Hale and Raugel. The result is more general even in the case of the Laplacian.","sentences":["In this note we extend to fully nonlinear operators the well known result on thin domains of Hale and Raugel.","The result is more general even in the case of the Laplacian."],"url":"http://arxiv.org/abs/2404.19577v1","category":"math.AP"}
{"created":"2024-04-30 14:09:48","title":"On the non-existence of oscillation numbers in Sturm-Liouville theory","abstract":"We prove an old conjecture that relates the existence of non-real eigenvalues of Sturm-Liouville Dirichlet problems on a finite interval to the non-existence of oscillation numbers of its real eigenfunctions, [[6], p.104, Problems 3 and 5]. This extends to the general case, a previous result in [1], [2] where it was shown that the presence of even one pair of non-real eigenvalues implies the non-existence of a positive eigenfunction (or ground state). We also provide estimates on the Haupt and Richardson indices and Haupt and Richardson numbers thereby complementing the original Sturm oscillation theorem with the Haupt-Richardson oscillation theorem discovered over 100 years ago with estimates on the missing oscillation numbers of the real eigenfunctions observed.","sentences":["We prove an old conjecture that relates the existence of non-real eigenvalues of Sturm-Liouville Dirichlet problems on a finite interval to the non-existence of oscillation numbers of its real eigenfunctions, [[6], p.104, Problems 3 and 5].","This extends to the general case, a previous result in [1], [2] where it was shown that the presence of even one pair of non-real eigenvalues implies the non-existence of a positive eigenfunction (or ground state).","We also provide estimates on the Haupt and Richardson indices and Haupt and Richardson numbers thereby complementing the original Sturm oscillation theorem with the Haupt-Richardson oscillation theorem discovered over 100 years ago with estimates on the missing oscillation numbers of the real eigenfunctions observed."],"url":"http://arxiv.org/abs/2404.19575v1","category":"math.CA"}
{"created":"2024-04-30 14:07:57","title":"War Elephants: Rethinking Combat AI and Human Oversight","abstract":"This paper explores the changes that pervasive AI is having on the nature of combat. We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended. Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\" By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems. We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems. This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support.","sentences":["This paper explores the changes that pervasive AI is having on the nature of combat.","We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended.","Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\"","By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems.","We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems.","This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support."],"url":"http://arxiv.org/abs/2404.19573v1","category":"cs.CY"}
{"created":"2024-04-30 13:59:28","title":"Consensus + Innovations Approach for Online Distributed Multi-Area Inertia Estimation","abstract":"The reduction of overall system inertia in modern power systems due to the increasing deployment of distributed energy resources is generally recognized as a major issue for system stability. Consequently, real-time monitoring of system inertia is critical to ensure a reliable and cost-effective system operation. Large-scale power systems are typically managed by multiple transmission system operators, making it difficult to have a central entity with access to global measurement data, which is usually required for estimating the overall system inertia. We address this problem by proposing a fully distributed inertia estimation algorithm with rigorous analytical convergence guarantees. This method requires only peer-to-peer sharing of local parameter estimates between neighboring control areas, eliminating the need for a centralized collection of real-time measurements. We robustify the algorithm in the presence of typical power system disturbances and demonstrate its performance in simulations based on the well-known New England IEEE-39 bus system.","sentences":["The reduction of overall system inertia in modern power systems due to the increasing deployment of distributed energy resources is generally recognized as a major issue for system stability.","Consequently, real-time monitoring of system inertia is critical to ensure a reliable and cost-effective system operation.","Large-scale power systems are typically managed by multiple transmission system operators, making it difficult to have a central entity with access to global measurement data, which is usually required for estimating the overall system inertia.","We address this problem by proposing a fully distributed inertia estimation algorithm with rigorous analytical convergence guarantees.","This method requires only peer-to-peer sharing of local parameter estimates between neighboring control areas, eliminating the need for a centralized collection of real-time measurements.","We robustify the algorithm in the presence of typical power system disturbances and demonstrate its performance in simulations based on the well-known New England IEEE-39 bus system."],"url":"http://arxiv.org/abs/2404.19569v1","category":"eess.SY"}
{"created":"2024-04-30 13:59:13","title":"Enhancing Deep Learning Model Explainability in Brain Tumor Datasets using Post-Heuristic Approaches","abstract":"The application of deep learning models in medical diagnosis has showcased considerable efficacy in recent years. Nevertheless, a notable limitation involves the inherent lack of explainability during decision-making processes. This study addresses such a constraint, by enhancing the interpretability robustness. The primary focus is directed towards refining the explanations generated by the LIME Library and LIME image explainer. This is achieved throuhg post-processing mechanisms, based on scenario-specific rules. Multiple experiments have been conducted using publicly accessible datasets related to brain tumor detection. Our proposed post-heuristic approach demonstrates significant advancements, yielding more robust and concrete results, in the context of medical diagnosis.","sentences":["The application of deep learning models in medical diagnosis has showcased considerable efficacy in recent years.","Nevertheless, a notable limitation involves the inherent lack of explainability during decision-making processes.","This study addresses such a constraint, by enhancing the interpretability robustness.","The primary focus is directed towards refining the explanations generated by the LIME Library and LIME image explainer.","This is achieved throuhg post-processing mechanisms, based on scenario-specific rules.","Multiple experiments have been conducted using publicly accessible datasets related to brain tumor detection.","Our proposed post-heuristic approach demonstrates significant advancements, yielding more robust and concrete results, in the context of medical diagnosis."],"url":"http://arxiv.org/abs/2404.19568v1","category":"eess.IV"}
{"created":"2024-04-30 13:50:55","title":"RepEval: Effective Text Evaluation with LLM Representation","abstract":"Automatic evaluation metrics for generated texts play an important role in the NLG field, especially with the rapid growth of LLMs. However, existing metrics are often limited to specific scenarios, making it challenging to meet the evaluation requirements of expanding LLM applications. Therefore, there is a demand for new, flexible, and effective metrics. In this study, we introduce RepEval, the first metric leveraging the projection of LLM representations for evaluation. RepEval requires minimal sample pairs for training, and through simple prompt modifications, it can easily transition to various tasks. Results on ten datasets from three tasks demonstrate the high effectiveness of our method, which exhibits stronger correlations with human judgments compared to previous metrics, even outperforming GPT-4. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.","sentences":["Automatic evaluation metrics for generated texts play an important role in the NLG field, especially with the rapid growth of LLMs.","However, existing metrics are often limited to specific scenarios, making it challenging to meet the evaluation requirements of expanding LLM applications.","Therefore, there is a demand for new, flexible, and effective metrics.","In this study, we introduce RepEval, the first metric leveraging the projection of LLM representations for evaluation.","RepEval requires minimal sample pairs for training, and through simple prompt modifications, it can easily transition to various tasks.","Results on ten datasets from three tasks demonstrate the high effectiveness of our method, which exhibits stronger correlations with human judgments compared to previous metrics, even outperforming GPT-4.","Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics."],"url":"http://arxiv.org/abs/2404.19563v1","category":"cs.CL"}
{"created":"2024-04-30 13:50:45","title":"Starshaped compact hypersurfaces in warped product manifolds I: prescribed curvature equations","abstract":"We derive global curvature estimates for closed strictly star-shaped (n-2)-convex hypersurfaces in warped product spaces, satisfying the prescribed (n-2)-curvature equation with a general right-hand side. The proof is inspired by the recent breakthrough of [Guan-Ren-Wang CPAM 2015] and it can be readily adapted to establish curvature estimates for semi-convex and (k+1)-convex solutions to the general k-curvature equation. Furthermore, it can also be used to prove the same estimates for prescribed curvature measure type equations.","sentences":["We derive global curvature estimates for closed strictly star-shaped (n-2)-convex hypersurfaces in warped product spaces, satisfying the prescribed (n-2)-curvature equation with a general right-hand side.","The proof is inspired by the recent breakthrough of [Guan-Ren-Wang CPAM 2015] and it can be readily adapted to establish curvature estimates for semi-convex and (k+1)-convex solutions to the general k-curvature equation.","Furthermore, it can also be used to prove the same estimates for prescribed curvature measure type equations."],"url":"http://arxiv.org/abs/2404.19562v1","category":"math.AP"}
{"created":"2024-04-30 13:42:31","title":"Contactifications: a Lagrangian description of compact Hamiltonian systems","abstract":"If $\\eta$ is a contact form on a manifold $M$ such that the orbits of the Reeb vector field form a simple foliation $\\mathcal{F}$ on $M$, then the presymplectic 2-form $d\\eta$ on $M$ induces a symplectic structure $\\omega$ on the quotient manifold $N=M/\\mathcal{F}$. We call $(M,\\eta)$ a $\\textit contactification$ of the symplectic manifold $(N,\\omega)$. First, we present an explicit geometric construction of contactifications of some coadjoint orbits of connected Lie groups. Our construction is a far going generalization of the well-known contactification of the complex projective space $\\mathbb{C}P^{n-1}$, being the unit sphere $S^{2n-1}$ in $\\mathbb{C}^{n}$, and equipped with the restriction of the Liouville 1-form on $\\mathbb{C}^n$. Second, we describe a constructive procedure for obtaining contactification in the process of the Marsden-Weinstein-Meyer symplectic reduction and indicate geometric obstructions for the existence of compact contactifications. Third, we show that contactifications provide a nice geometrical tool for a Lagrangian description of Hamiltonian systems on compact symplectic manifolds $(N,\\omega)$, on which symplectic forms never admit a `vector potential'.","sentences":["If $\\eta$ is a contact form on a manifold $M$ such that the orbits of the Reeb vector field form a simple foliation $\\mathcal{F}$ on $M$, then the presymplectic 2-form $d\\eta$ on $M$ induces a symplectic structure $\\omega$ on the quotient manifold $N=M/\\mathcal{F}$.","We call $(M,\\eta)$ a $\\textit contactification$ of the symplectic manifold $(N,\\omega)$. First, we present an explicit geometric construction of contactifications of some coadjoint orbits of connected Lie groups.","Our construction is a far going generalization of the well-known contactification of the complex projective space $\\mathbb{C}P^{n-1}$, being the unit sphere $S^{2n-1}$ in $\\mathbb{C}^{n}$, and equipped with the restriction of the Liouville 1-form on $\\mathbb{C}^n$. Second, we describe a constructive procedure for obtaining contactification in the process of the Marsden-Weinstein-Meyer symplectic reduction and indicate geometric obstructions for the existence of compact contactifications.","Third, we show that contactifications provide a nice geometrical tool for a Lagrangian description of Hamiltonian systems on compact symplectic manifolds $(N,\\omega)$, on which symplectic forms never admit a `vector potential'."],"url":"http://arxiv.org/abs/2404.19560v1","category":"math.SG"}
{"created":"2024-04-30 13:38:05","title":"Transforming Credit Guarantee Schemes with Distributed Ledger Technology","abstract":"Credit Guarantee Schemes (CGSs) are crucial in mitigating SMEs' financial constraints. However, they are renownedly affected by critical shortcomings, such as a lack of financial sustainability and operational efficiency. Distributed Ledger Technologies (DLTs) have shown significant revolutionary influence in several sectors, including finance and banking, thanks to the full operational traceability they bring alongside verifiable computation. Nevertheless, the potential synergy between DLTs and CGSs has not been thoroughly investigated yet. This paper proposes a comprehensive framework to utilise DLTs, particularly blockchain technologies, in CGS processes to improve operational efficiency and effectiveness. To this end, we compare key architectural characteristics considering access level, governance structure, and consensus method, to examine their fit with CGS processes. We believe this study can guide policymakers and stakeholders, thereby stimulating further innovation in this promising field.","sentences":["Credit Guarantee Schemes (CGSs) are crucial in mitigating SMEs' financial constraints.","However, they are renownedly affected by critical shortcomings, such as a lack of financial sustainability and operational efficiency.","Distributed Ledger Technologies (DLTs) have shown significant revolutionary influence in several sectors, including finance and banking, thanks to the full operational traceability they bring alongside verifiable computation.","Nevertheless, the potential synergy between DLTs and CGSs has not been thoroughly investigated yet.","This paper proposes a comprehensive framework to utilise DLTs, particularly blockchain technologies, in CGS processes to improve operational efficiency and effectiveness.","To this end, we compare key architectural characteristics considering access level, governance structure, and consensus method, to examine their fit with CGS processes.","We believe this study can guide policymakers and stakeholders, thereby stimulating further innovation in this promising field."],"url":"http://arxiv.org/abs/2404.19555v1","category":"cs.CE"}
{"created":"2024-04-30 13:25:20","title":"Extending Llama-3's Context Ten-Fold Overnight","abstract":"We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning. The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine. The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-context language understanding; meanwhile, it also well preserves the original capability over short contexts. The dramatic context extension is mainly attributed to merely 3.5K synthetic training samples generated by GPT-4 , which indicates the LLMs' inherent (yet largely underestimated) potential to extend its original context length. In fact, the context length could be extended far beyond 80K with more computation resources. Therefore, the team will publicly release the entire resources (including data, model, data generation pipeline, training code) so as to facilitate the future research from the community: \\url{https://github.com/FlagOpen/FlagEmbedding}.","sentences":["We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning.","The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine.","The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-context language understanding; meanwhile, it also well preserves the original capability over short contexts.","The dramatic context extension is mainly attributed to merely 3.5K synthetic training samples generated by GPT-4 , which indicates the LLMs' inherent (yet largely underestimated) potential to extend its original context length.","In fact, the context length could be extended far beyond 80K with more computation resources.","Therefore, the team will publicly release the entire resources (including data, model, data generation pipeline, training code) so as to facilitate the future research from the community: \\url{https://github.com/FlagOpen/FlagEmbedding}."],"url":"http://arxiv.org/abs/2404.19553v1","category":"cs.CL"}
{"created":"2024-04-30 13:24:41","title":"Type-Based Unsourced Multiple Access","abstract":"We generalize the type-based multiple access framework proposed by Mergen and Tong (2006) to the case of unsourced multiple access. In the proposed framework, each device tracks the state of a physical/digital process, quantizes this state, and communicates it to a common receiver through a shared channel in an uncoordinated manner. The receiver aims to estimate the type of the states, i.e., the set of states and their multiplicity in the sequence of states reported by all devices. We measure the type estimation error using the Wasserstein distance. Considering an example of multi-target position tracking, we show that type estimation can be performed effectively via approximate message passing. Furthermore, we determine the quantization resolution that minimizes the type estimation error by balancing quantization distortion and communication error.","sentences":["We generalize the type-based multiple access framework proposed by Mergen and Tong (2006) to the case of unsourced multiple access.","In the proposed framework, each device tracks the state of a physical/digital process, quantizes this state, and communicates it to a common receiver through a shared channel in an uncoordinated manner.","The receiver aims to estimate the type of the states, i.e., the set of states and their multiplicity in the sequence of states reported by all devices.","We measure the type estimation error using the Wasserstein distance.","Considering an example of multi-target position tracking, we show that type estimation can be performed effectively via approximate message passing.","Furthermore, we determine the quantization resolution that minimizes the type estimation error by balancing quantization distortion and communication error."],"url":"http://arxiv.org/abs/2404.19552v1","category":"cs.IT"}
{"created":"2024-04-30 13:14:51","title":"RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing","abstract":"Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.","sentences":["Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge.","To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks.","This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications.","The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications.","RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications.","The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment.","It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research.","In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP.","The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey."],"url":"http://arxiv.org/abs/2404.19543v1","category":"cs.CL"}
{"created":"2024-04-30 13:14:28","title":"One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features","abstract":"Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced video analysis approach that expands Closed-vocabulary Temporal Action Detection (Closed-vocab TAD) capabilities. Closed-vocab TAD is typically confined to localizing and classifying actions based on a predefined set of categories. In contrast, Open-vocab TAD goes further and is not limited to these predefined categories. This is particularly useful in real-world scenarios where the variety of actions in videos can be vast and not always predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage approach, which involves generating action proposals and then identifying those actions. However, errors made during the first stage can adversely affect the subsequent action identification accuracy. Additionally, existing studies face challenges in handling actions of different durations owing to the use of fixed temporal processing methods. Therefore, we propose a 1-stage approach consisting of two primary modules: Multi-scale Video Analysis (MVA) and Video-Text Alignment (VTA). The MVA module captures actions at varying temporal resolutions, overcoming the challenge of detecting actions with diverse durations. The VTA module leverages the synergy between visual and textual modalities to precisely align video segments with corresponding action labels, a critical step for accurate action identification in Open-vocab scenarios. Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed that the proposed method achieved superior results compared to the other methods in both Open-vocab and Closed-vocab settings. This serves as a strong demonstration of the effectiveness of the proposed method in the TAD task.","sentences":["Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced video analysis approach that expands Closed-vocabulary Temporal Action Detection (Closed-vocab TAD) capabilities.","Closed-vocab TAD is typically confined to localizing and classifying actions based on a predefined set of categories.","In contrast, Open-vocab TAD goes further and is not limited to these predefined categories.","This is particularly useful in real-world scenarios where the variety of actions in videos can be vast and not always predictable.","The prevalent methods in Open-vocab TAD typically employ a 2-stage approach, which involves generating action proposals and then identifying those actions.","However, errors made during the first stage can adversely affect the subsequent action identification accuracy.","Additionally, existing studies face challenges in handling actions of different durations owing to the use of fixed temporal processing methods.","Therefore, we propose a 1-stage approach consisting of two primary modules: Multi-scale Video Analysis (MVA) and Video-Text Alignment (VTA).","The MVA module captures actions at varying temporal resolutions, overcoming the challenge of detecting actions with diverse durations.","The VTA module leverages the synergy between visual and textual modalities to precisely align video segments with corresponding action labels, a critical step for accurate action identification in Open-vocab scenarios.","Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed that the proposed method achieved superior results compared to the other methods in both Open-vocab and Closed-vocab settings.","This serves as a strong demonstration of the effectiveness of the proposed method in the TAD task."],"url":"http://arxiv.org/abs/2404.19542v1","category":"cs.CV"}
{"created":"2024-04-30 13:14:11","title":"Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging","abstract":"While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\\%$).","sentences":["While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity.","Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy.","In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances.","We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors.","Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation.","To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS.","For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB).","Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\\%$)."],"url":"http://arxiv.org/abs/2404.19541v1","category":"cs.CV"}
{"created":"2024-04-30 13:11:32","title":"Ferroelectrically-enhanced Schottky barrier transistors for Logic-in-Memory applications","abstract":"Artificial neural networks (ANNs) have had an enormous impact on a multitude of sectors, from research to industry, generating an unprecedented demand for tailor-suited hardware platforms. Their training and execution is highly memory-intensive, clearly evidencing the limitations affecting the currently available hardware based on the von Neumann architecture, which requires frequent data shuttling due to the physical separation of logic and memory units. This does not only limit the achievable performances but also greatly increases the energy consumption, hindering the integration of ANNs into low-power platforms. New Logic in Memory (LiM) architectures, able to unify memory and logic functionalities into a single component, are highly promising for overcoming these limitations, by drastically reducing the need of data transfers. Recently, it has been shown that a very flexible platform for logic applications can be realized recurring to a multi-gated Schottky-Barrier Field Effect Transistor (SBFET). If equipped with memory capabilities, this architecture could represent an ideal building block for versatile LiM hardware. To reach this goal, here we investigate the integration of a ferroelectric Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) layer onto Dual Top Gated SBFETs. We demonstrate that HZO polarization charges can be successfully employed to tune the height of the two Schottky barriers, influencing the injection behavior, thus defining the transistor mode, switching it between n and p-type transport. The modulation strength is strongly dependent on the polarization pulse height, allowing for the selection of multiple current levels. All these achievable states can be well retained over time, thanks to the HZO stability. The presented result show how ferroelectric-enhanced SBFETs are promising for the realization of novel LiM hardware, enabling low-power circuits for ANNs execution.","sentences":["Artificial neural networks (ANNs) have had an enormous impact on a multitude of sectors, from research to industry, generating an unprecedented demand for tailor-suited hardware platforms.","Their training and execution is highly memory-intensive, clearly evidencing the limitations affecting the currently available hardware based on the von Neumann architecture, which requires frequent data shuttling due to the physical separation of logic and memory units.","This does not only limit the achievable performances but also greatly increases the energy consumption, hindering the integration of ANNs into low-power platforms.","New Logic in Memory (LiM) architectures, able to unify memory and logic functionalities into a single component, are highly promising for overcoming these limitations, by drastically reducing the need of data transfers.","Recently, it has been shown that a very flexible platform for logic applications can be realized recurring to a multi-gated Schottky-Barrier Field Effect Transistor (SBFET).","If equipped with memory capabilities, this architecture could represent an ideal building block for versatile LiM hardware.","To reach this goal, here we investigate the integration of a ferroelectric Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) layer onto Dual Top Gated SBFETs.","We demonstrate that HZO polarization charges can be successfully employed to tune the height of the two Schottky barriers, influencing the injection behavior, thus defining the transistor mode, switching it between n and p-type transport.","The modulation strength is strongly dependent on the polarization pulse height, allowing for the selection of multiple current levels.","All these achievable states can be well retained over time, thanks to the HZO stability.","The presented result show how ferroelectric-enhanced SBFETs are promising for the realization of novel LiM hardware, enabling low-power circuits for ANNs execution."],"url":"http://arxiv.org/abs/2404.19535v1","category":"physics.app-ph"}
{"created":"2024-04-30 13:11:12","title":"MIPI 2024 Challenge on Nighttime Flare Removal: Methods and Results","abstract":"The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems. However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms. In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024. In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase. The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/.","sentences":["The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems.","However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI).","Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms.","In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024.","In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase.","The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal.","More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/."],"url":"http://arxiv.org/abs/2404.19534v1","category":"cs.CV"}
{"created":"2024-04-30 13:11:11","title":"Shocks in the Warm Neutral Medium I -- Theoretical model","abstract":"Context. Atomic and molecular line emissions from shocks may provide valuable information on the injection of mechanical energy in the interstellar medium (ISM), the generation of turbulence, and the processes of phase transition between the Warm Neutral Medium (WNM) and the Cold Neutral Medium (CNM).Aims. In this series of papers, we investigate the properties of shocks propagating in the WNM. Our objective is to identify the tracers of these shocks, use them to interpret ancillary observations of the local diffuse matter, and provide predictions for future observations.   Methods. Shocks propagating in the WNM are studied using the Paris-Durham shock code, a multi-fluid model built to follow the thermodynamical and chemical structures of shock waves, at steady-state, in a plane-parallel geometry. The code, already designed to take into account the impact of an external radiation field, is updated to treat self-irradiated shocks at intermediate ($30<V_S <100$ km s$^{-1}$) and high velocity ($V_S \\ge 100$ km s$^{-1}$) which emit ultraviolet (UV), extreme-ultraviolet (EUV), and X-ray photons. The couplings between the photons generated by the shock, the radiative precursor, and the shock structure are computed self-consistently using an exact radiative transfer algorithm for line emission. The resulting code is explored over a wide range of parameters ($0.1 \\le n_H \\le 2$ cm$^{-3}$, $10 \\le V_S \\le 500$ km s$^{-1}$, and $0.1 \\le B \\le 10$ $\\mu$G) that covers the typical conditions of the WNM in the solar neighborhood.   Results. The explored physical conditions are prompt to the existence of a diversity of stationary magnetohydrodynamic solutions, including J-type, CJ-type, and C-type shocks. These shocks are found to naturally induce phase transition between the WNM and the CNM, provided that the postshock thermal pressure is larger than the maximum pressure of the WNM and that the maximum density allowed by magnetic compression is larger than the minimum density of the CNM. The input flux of mechanical energy is primarily reprocessed into line emissions from the X-ray to the submillimeter domain. Intermediate and high velocity shocks are found to generate a UV radiation field that scales as $V_S^3$ for $V_S < 100$ km s$^{-1}$ and as $V_S^2$ at higher velocities, and an X-ray radiation field that scales as $V_S^3$ for $V_S \\ge 100$ km s$^{-1}$. Both radiation fields may extend over large distances in the preshock depending of the density of the surrounding medium and the hardness of the X-ray field which is solely driven by the shock velocity.   Conclusions. This first paper presents the thermochemical trajectories of shocks in the WNM and their associated spectra. It corresponds to a new milestone in the development of the Paris-Durham shock code and a stepping stone for the analysis of observations that will be carried out in forthcoming works.","sentences":["Context.","Atomic and molecular line emissions from shocks may provide valuable information on the injection of mechanical energy in the interstellar medium (ISM), the generation of turbulence, and the processes of phase transition between the Warm Neutral Medium (WNM) and the Cold Neutral Medium (CNM).Aims.","In this series of papers, we investigate the properties of shocks propagating in the WNM.","Our objective is to identify the tracers of these shocks, use them to interpret ancillary observations of the local diffuse matter, and provide predictions for future observations.   Methods.","Shocks propagating in the WNM are studied using the Paris-Durham shock code, a multi-fluid model built to follow the thermodynamical and chemical structures of shock waves, at steady-state, in a plane-parallel geometry.","The code, already designed to take into account the impact of an external radiation field, is updated to treat self-irradiated shocks at intermediate ($30<V_S <100$ km s$^{-1}$) and high velocity ($V_S \\ge 100$ km s$^{-1}$) which emit ultraviolet (UV), extreme-ultraviolet (EUV), and X-ray photons.","The couplings between the photons generated by the shock, the radiative precursor, and the shock structure are computed self-consistently using an exact radiative transfer algorithm for line emission.","The resulting code is explored over a wide range of parameters ($0.1 \\le n_H \\le 2$ cm$^{-3}$, $10 \\le V_S \\le 500$ km s$^{-1}$, and $0.1 \\le B \\le 10$ $\\mu$G) that covers the typical conditions of the WNM in the solar neighborhood.   ","Results.","The explored physical conditions are prompt to the existence of a diversity of stationary magnetohydrodynamic solutions, including J-type, CJ-type, and C-type shocks.","These shocks are found to naturally induce phase transition between the WNM and the CNM, provided that the postshock thermal pressure is larger than the maximum pressure of the WNM and that the maximum density allowed by magnetic compression is larger than the minimum density of the CNM.","The input flux of mechanical energy is primarily reprocessed into line emissions from the X-ray to the submillimeter domain.","Intermediate and high velocity shocks are found to generate a UV radiation field that scales as $V_S^3$ for $V_S < 100$ km s$^{-1}$ and as $V_S^2$ at higher velocities, and an X-ray radiation field that scales as $V_S^3$ for $V_S \\ge 100$ km s$^{-1}$. Both radiation fields may extend over large distances in the preshock depending of the density of the surrounding medium and the hardness of the X-ray field which is solely driven by the shock velocity.   ","Conclusions.","This first paper presents the thermochemical trajectories of shocks in the WNM and their associated spectra.","It corresponds to a new milestone in the development of the Paris-Durham shock code and a stepping stone for the analysis of observations that will be carried out in forthcoming works."],"url":"http://arxiv.org/abs/2404.19533v1","category":"astro-ph.GA"}
{"created":"2024-04-30 13:10:19","title":"Optimized Soft-Aided Decoding of OFEC and Staircase Codes","abstract":"We propose a novel soft-aided hard-decision decoding algorithm for general product-like codes. It achieves error correcting performance similar to that of a soft-decision turbo decoder for staircase and OFEC codes, while maintaining a low complexity.","sentences":["We propose a novel soft-aided hard-decision decoding algorithm for general product-like codes.","It achieves error correcting performance similar to that of a soft-decision turbo decoder for staircase and OFEC codes, while maintaining a low complexity."],"url":"http://arxiv.org/abs/2404.19532v1","category":"cs.IT"}
{"created":"2024-04-30 13:09:41","title":"MoST: Multi-modality Scene Tokenization for Motion Prediction","abstract":"Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.","sentences":["Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights.","This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions).","An alternative paradigm is end-to-end learning from raw sensors.","However, this approach suffers from the lack of interpretability and requires significantly more training resources.","In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner.","The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information.","Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures.","To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings.","Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art."],"url":"http://arxiv.org/abs/2404.19531v1","category":"cs.CV"}
{"created":"2024-04-30 13:07:20","title":"Prospects for weighing neutrinos in interacting dark energy models using joint observations of gravitational waves and $\u03b3$-ray bursts","abstract":"In this work, we explore the capability of future gravitational wave (GW) standard siren observations to constrain the total neutrino mass in some typical interacting dark energy (IDE) models. We examine the combined potential of the third-generation ground-based GW detector network and a short $\\gamma$-ray burst (GRB) detector similar to the THESEUS telescope for cosmological analysis. Our findings suggest that future GW standard siren observations could modestly refine the upper limit on the total neutrino mass, enhancing it by $5\\%$ to $10\\%$ over the existing limit given by the CMB+BAO+SN data. Additionally, the future observations are expected to significantly improve the constraints on $\\Omega_{\\rm m}$ and $H_0$. Moreover, the measurements could enhance the precision of the coupling strength $\\beta$ by $23\\%$ to $42\\%$.","sentences":["In this work, we explore the capability of future gravitational wave (GW) standard siren observations to constrain the total neutrino mass in some typical interacting dark energy (IDE) models.","We examine the combined potential of the third-generation ground-based GW detector network and a short $\\gamma$-ray burst (GRB) detector similar to the THESEUS telescope for cosmological analysis.","Our findings suggest that future GW standard siren observations could modestly refine the upper limit on the total neutrino mass, enhancing it by $5\\%$ to $10\\%$ over the existing limit given by the CMB+BAO+SN data.","Additionally, the future observations are expected to significantly improve the constraints on $\\Omega_{\\rm m}$ and $H_0$. Moreover, the measurements could enhance the precision of the coupling strength $\\beta$ by $23\\%$ to $42\\%$."],"url":"http://arxiv.org/abs/2404.19530v1","category":"astro-ph.CO"}
{"created":"2024-04-30 12:58:38","title":"Scale and Conformal Invariance in 2d Sigma Models, with an Application to N=4 Supersymmetry","abstract":"By adapting previously known arguments concerning Ricci flow and the c-theorem, we give a direct proof that in a two-dimensional sigma-model with compact target space, scale invariance implies conformal invariance in perturbation theory. This argument, which applies to a general sigma-model constructed with a target space metric and B-field, is in accord with a more general proof in the literature that applies to arbitrary two-dimensional quantum field theories. Models with extended supersymmetry and a B-field are known to provide interesting test cases for the relation between scale invariance and conformal invariance in sigma-model perturbation theory. We give examples showing that in such models, the obstructions to conformal invariance suggested by general arguments can actually occur in models with target spaces that are not compact or complete. Thus compactness of the target space, or at least a suitable condition of completeness, is necessary as well as sufficient to ensure that scale invariance implies conformal invariance in models of this type.","sentences":["By adapting previously known arguments concerning Ricci flow and the c-theorem, we give a direct proof that in a two-dimensional sigma-model with compact target space, scale invariance implies conformal invariance in perturbation theory.","This argument, which applies to a general sigma-model constructed with a target space metric and B-field, is in accord with a more general proof in the literature that applies to arbitrary two-dimensional quantum field theories.","Models with extended supersymmetry and a B-field are known to provide interesting test cases for the relation between scale invariance and conformal invariance in sigma-model perturbation theory.","We give examples showing that in such models, the obstructions to conformal invariance suggested by general arguments can actually occur in models with target spaces that are not compact or complete.","Thus compactness of the target space, or at least a suitable condition of completeness, is necessary as well as sufficient to ensure that scale invariance implies conformal invariance in models of this type."],"url":"http://arxiv.org/abs/2404.19526v1","category":"hep-th"}
{"created":"2024-04-30 12:56:14","title":"MicroDreamer: Zero-shot 3D Generation in $\\sim$20 Seconds by Score-based Iterative Reconstruction","abstract":"Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample. In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model. Given the images produced by the diffusion model, SIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single optimization in SDS, mimicking the 3D reconstruction process. With other improvements including optimization in the pixel space, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks. In particular, retaining a comparable performance, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field and takes about 20 seconds to generate meshes from 3D Gaussian splitting on a single A100 GPU, halving the time of the fastest zero-shot baseline, DreamGaussian. Our code is available at https://github.com/ML-GSAI/MicroDreamer.","sentences":["Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample.","In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model.","Given the images produced by the diffusion model, SIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single optimization in SDS, mimicking the 3D reconstruction process.","With other improvements including optimization in the pixel space, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks.","In particular, retaining a comparable performance, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field and takes about 20 seconds to generate meshes from 3D Gaussian splitting on a single A100 GPU, halving the time of the fastest zero-shot baseline, DreamGaussian.","Our code is available at https://github.com/ML-GSAI/MicroDreamer."],"url":"http://arxiv.org/abs/2404.19525v1","category":"cs.CV"}
{"created":"2024-04-30 12:50:29","title":"DARWIN/XLZD: a future xenon observatory for dark matter and other rare interactions","abstract":"The DARWIN/XLZD experiment is a next-generation dark matter detector with a multi-ten-ton liquid xenon time projection chamber at its core. Its principal goal will be to explore the experimentally accessible parameter space for Weakly Interacting Massive Particles (WIMPs) in a wide mass-range, until interactions of astrophysical neutrinos will become an irreducible background. The prompt scintillation light and the charge signals induced by particle interactions in the liquid xenon target will be observed by VUV-sensitive, ultra-low background photosensors. Besides its excellent sensitivity to WIMPs with masses above $\\sim$5\\,GeV, such a detector with its large mass, low-energy threshold and ultra-low background level will also be sensitive to other rare interactions, and in particular also to bosonic dark matter candidates with masses at the keV-scale. We present the detector concept, discuss the main sources of backgrounds, the technological challenges and some of the ongoing detector design and R&D efforts, as well as the large-scale demonstrators. We end by discussing the sensitivity to particle dark matter interactions.","sentences":["The DARWIN/XLZD experiment is a next-generation dark matter detector with a multi-ten-ton liquid xenon time projection chamber at its core.","Its principal goal will be to explore the experimentally accessible parameter space for Weakly Interacting Massive Particles (WIMPs) in a wide mass-range, until interactions of astrophysical neutrinos will become an irreducible background.","The prompt scintillation light and the charge signals induced by particle interactions in the liquid xenon target will be observed by VUV-sensitive, ultra-low background photosensors.","Besides its excellent sensitivity to WIMPs with masses above $\\sim$5\\,GeV, such a detector with its large mass, low-energy threshold and ultra-low background level will also be sensitive to other rare interactions, and in particular also to bosonic dark matter candidates with masses at the keV-scale.","We present the detector concept, discuss the main sources of backgrounds, the technological challenges and some of the ongoing detector design and R&D efforts, as well as the large-scale demonstrators.","We end by discussing the sensitivity to particle dark matter interactions."],"url":"http://arxiv.org/abs/2404.19524v1","category":"astro-ph.IM"}
{"created":"2024-04-30 12:50:10","title":"Nonlinear scalarization of Schwarzschild black holes in Einstein-scalar-Gauss-Bonnet gravity","abstract":"In this paper, we propose a fully nonlinear mechanism for obtaining scalarized black holes in Einstein-scalar-Gauss-Bonnet (EsGB) gravity which is beyond the spontaneous scalarization. Introducing three coupling functions $f(\\varphi)$ satisfying $f''(0) = 0$, we find that Schwarzschild black hole is linearly stable against scalar perturbation, whereas it is unstable against nonlinear scalar perturbation if the coupling function includes term higher than $\\varphi^6$. For a specific choice of coupling function $f(\\varphi)=\\alpha(\\varphi^4-\\beta\\varphi^6)$, we obtain new black holes with scalar hair in the EsGB gravity. In this case, the coupling parameter $\\alpha$ plays a major role in making different nonlinear scalarized black holes, while the other parameter $\\beta$ plays a supplementary role. Furthermore, we study thermodynamic aspects of these scalarized black holes and prove the first-law of thermodynamics.","sentences":["In this paper, we propose a fully nonlinear mechanism for obtaining scalarized black holes in Einstein-scalar-Gauss-Bonnet (EsGB) gravity which is beyond the spontaneous scalarization.","Introducing three coupling functions $f(\\varphi)$ satisfying $f''(0) = 0$, we find that Schwarzschild black hole is linearly stable against scalar perturbation, whereas it is unstable against nonlinear scalar perturbation if the coupling function includes term higher than $\\varphi^6$. For a specific choice of coupling function $f(\\varphi)=\\alpha(\\varphi^4-\\beta\\varphi^6)$, we obtain new black holes with scalar hair in the EsGB gravity.","In this case, the coupling parameter $\\alpha$ plays a major role in making different nonlinear scalarized black holes, while the other parameter $\\beta$ plays a supplementary role.","Furthermore, we study thermodynamic aspects of these scalarized black holes and prove the first-law of thermodynamics."],"url":"http://arxiv.org/abs/2404.19521v1","category":"gr-qc"}
{"created":"2024-04-30 12:49:54","title":"MGCBS: An Optimal and Efficient Algorithm for Solving Multi-Goal Multi-Agent Path Finding Problem","abstract":"With the expansion of the scale of robotics applications, the multi-goal multi-agent pathfinding (MG-MAPF) problem began to gain widespread attention. This problem requires each agent to visit pre-assigned multiple goal points at least once without conflict. Some previous methods have been proposed to solve the MG-MAPF problem based on Decoupling the goal Vertex visiting order search and the Single-agent pathfinding (DVS). However, this paper demonstrates that the methods based on DVS cannot always obtain the optimal solution. To obtain the optimal result, we propose the Multi-Goal Conflict-Based Search (MGCBS), which is based on Decoupling the goal Safe interval visiting order search and the Single-agent pathfinding (DSS). Additionally, we present the Time-Interval-Space Forest (TIS Forest) to enhance the efficiency of MGCBS by maintaining the shortest paths from any start point at any start time step to each safe interval at the goal points. The experiment demonstrates that our method can consistently obtain optimal results and execute up to 7 times faster than the state-of-the-art method in our evaluation.","sentences":["With the expansion of the scale of robotics applications, the multi-goal multi-agent pathfinding (MG-MAPF) problem began to gain widespread attention.","This problem requires each agent to visit pre-assigned multiple goal points at least once without conflict.","Some previous methods have been proposed to solve the MG-MAPF problem based on Decoupling the goal Vertex visiting order search and the Single-agent pathfinding (DVS).","However, this paper demonstrates that the methods based on DVS cannot always obtain the optimal solution.","To obtain the optimal result, we propose the Multi-Goal Conflict-Based Search (MGCBS), which is based on Decoupling the goal Safe interval visiting order search and the Single-agent pathfinding (DSS).","Additionally, we present the Time-Interval-Space Forest (TIS Forest) to enhance the efficiency of MGCBS by maintaining the shortest paths from any start point at any start time step to each safe interval at the goal points.","The experiment demonstrates that our method can consistently obtain optimal results and execute up to 7 times faster than the state-of-the-art method in our evaluation."],"url":"http://arxiv.org/abs/2404.19518v1","category":"cs.MA"}
{"created":"2024-04-30 12:49:54","title":"Generating Robust Counterfactual Witnesses for Graph Neural Networks","abstract":"This paper introduces a new class of explanation structures, called robust counterfactual witnesses (RCWs), to provide robust, both counterfactual and factual explanations for graph neural networks. Given a graph neural network M, a robust counterfactual witness refers to the fraction of a graph G that are counterfactual and factual explanation of the results of M over G, but also remains so for any \"disturbed\" G by flipping up to k of its node pairs. We establish the hardness results, from tractable results to co-NP-hardness, for verifying and generating robust counterfactual witnesses. We study such structures for GNN-based node classification, and present efficient algorithms to verify and generate RCWs. We also provide a parallel algorithm to verify and generate RCWs for large graphs with scalability guarantees. We experimentally verify our explanation generation process for benchmark datasets, and showcase their applications.","sentences":["This paper introduces a new class of explanation structures, called robust counterfactual witnesses (RCWs), to provide robust, both counterfactual and factual explanations for graph neural networks.","Given a graph neural network M, a robust counterfactual witness refers to the fraction of a graph G that are counterfactual and factual explanation of the results of M over G, but also remains so for any \"disturbed\" G by flipping up to k of its node pairs.","We establish the hardness results, from tractable results to co-NP-hardness, for verifying and generating robust counterfactual witnesses.","We study such structures for GNN-based node classification, and present efficient algorithms to verify and generate RCWs.","We also provide a parallel algorithm to verify and generate RCWs for large graphs with scalability guarantees.","We experimentally verify our explanation generation process for benchmark datasets, and showcase their applications."],"url":"http://arxiv.org/abs/2404.19519v1","category":"cs.LG"}
{"created":"2024-04-30 12:45:47","title":"Chirality and odd mechanics in active columnar phases","abstract":"Chiral active materials display odd dynamical effects in both their elastic and viscous responses. We show that the most symmetric mesophase with two-dimensional odd elasticity in three dimensions is chiral, polar and columnar, with two-dimensional translational order in the plane perpendicular to the columns and no elastic restoring force for their relative sliding. We derive its hydrodynamic equations from those of a chiral active variant of model H. The most striking prediction of the odd dynamics is two distinct types of column oscillation whose frequencies do not vanish at zero wavenumber. In addition, activity leads to a buckling instability coming from the generic force-dipole active stress analogous to the mechanical Helfrich-Hurault instability in passive materials, while the chiral torque-dipole active stress fundamentally modifies the instability by the selection of helical column undulations.","sentences":["Chiral active materials display odd dynamical effects in both their elastic and viscous responses.","We show that the most symmetric mesophase with two-dimensional odd elasticity in three dimensions is chiral, polar and columnar, with two-dimensional translational order in the plane perpendicular to the columns and no elastic restoring force for their relative sliding.","We derive its hydrodynamic equations from those of a chiral active variant of model H. The most striking prediction of the odd dynamics is two distinct types of column oscillation whose frequencies do not vanish at zero wavenumber.","In addition, activity leads to a buckling instability coming from the generic force-dipole active stress analogous to the mechanical Helfrich-Hurault instability in passive materials, while the chiral torque-dipole active stress fundamentally modifies the instability by the selection of helical column undulations."],"url":"http://arxiv.org/abs/2404.19514v1","category":"cond-mat.soft"}
{"created":"2024-04-30 12:45:14","title":"Thermalization via three-wave mixing","abstract":"We discuss thermalization in a multimode quantum cavity under unitary evolution. According to general principles, an isolated system with quadratic couplings does not exhibit thermalization. However, we find that three-wave perturbation, typical for instance in superconducting Josephson systems, may lead to thermalization into a Bose-Einstein distribution of occupations of the modes. The temperature of this state is dictated by energy conservation in this closed system, and the thermal distribution is robust against weak disturbances. We discuss how our findings open up new avenues to experimentally probe fundamental assumptions of statistical physics in solid-state systems.","sentences":["We discuss thermalization in a multimode quantum cavity under unitary evolution.","According to general principles, an isolated system with quadratic couplings does not exhibit thermalization.","However, we find that three-wave perturbation, typical for instance in superconducting Josephson systems, may lead to thermalization into a Bose-Einstein distribution of occupations of the modes.","The temperature of this state is dictated by energy conservation in this closed system, and the thermal distribution is robust against weak disturbances.","We discuss how our findings open up new avenues to experimentally probe fundamental assumptions of statistical physics in solid-state systems."],"url":"http://arxiv.org/abs/2404.19511v1","category":"quant-ph"}
{"created":"2024-04-30 12:43:53","title":"Do Large Language Models Understand Conversational Implicature -- A case study with a chinese sitcom","abstract":"Understanding the non-literal meaning of an utterance is critical for large language models (LLMs) to become human-like social communicators. In this work, we introduce SwordsmanImp, the first Chinese multi-turn-dialogue-based dataset aimed at conversational implicature, sourced from dialogues in the Chinese sitcom $\\textit{My Own Swordsman}$. It includes 200 carefully handcrafted questions, all annotated on which Gricean maxims have been violated. We test eight close-source and open-source LLMs under two tasks: a multiple-choice question task and an implicature explanation task. Our results show that GPT-4 attains human-level accuracy (94%) on multiple-choice questions. CausalLM demonstrates a 78.5% accuracy following GPT-4. Other models, including GPT-3.5 and several open-source models, demonstrate a lower accuracy ranging from 20% to 60% on multiple-choice questions. Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency. While all models generate largely fluent and self-consistent text, their explanations score low on reasonability except for GPT-4, suggesting that most LLMs cannot produce satisfactory explanations of the implicatures in the conversation. Moreover, we find LLMs' performance does not vary significantly by Gricean maxims, suggesting that LLMs do not seem to process implicatures derived from different maxims differently. Our data and code are available at https://github.com/sjtu-compling/llm-pragmatics.","sentences":["Understanding the non-literal meaning of an utterance is critical for large language models (LLMs) to become human-like social communicators.","In this work, we introduce SwordsmanImp, the first Chinese multi-turn-dialogue-based dataset aimed at conversational implicature, sourced from dialogues in the Chinese sitcom $\\textit{My Own Swordsman}$.","It includes 200 carefully handcrafted questions, all annotated on which Gricean maxims have been violated.","We test eight close-source and open-source LLMs under two tasks: a multiple-choice question task and an implicature explanation task.","Our results show that GPT-4 attains human-level accuracy (94%) on multiple-choice questions.","CausalLM demonstrates a 78.5% accuracy following GPT-4.","Other models, including GPT-3.5 and several open-source models, demonstrate a lower accuracy ranging from 20% to 60% on multiple-choice questions.","Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.","While all models generate largely fluent and self-consistent text, their explanations score low on reasonability except for GPT-4, suggesting that most LLMs cannot produce satisfactory explanations of the implicatures in the conversation.","Moreover, we find LLMs' performance does not vary significantly by Gricean maxims, suggesting that LLMs do not seem to process implicatures derived from different maxims differently.","Our data and code are available at https://github.com/sjtu-compling/llm-pragmatics."],"url":"http://arxiv.org/abs/2404.19509v1","category":"cs.CL"}
{"created":"2024-04-30 12:41:02","title":"High power single crystal KTA optical parametric amplifier for efficient 1.4-3.5 $\u03bc$m mid-IR radiation generation","abstract":"In this paper, we present a single crystal, KTA (potassium titanyl-arsenate, KTiOAsO$_4$) based picosecond optical parametric amplifier pumped by an in-house built 1030 nm Yb:YAG thin-disk laser, capable of tunability from 1.46 to 3.5 $\\mu$m, operating at 90 kHz, with high average power in the signal and idler beams. The highest output power of 8.9 W was reached for the 1750 nm signal beam with 19% conversion efficiency and the respective 2500 nm idler beam power was 6.2 W with 13% efficiency. The highest combined signal and idler mid-IR power was obtained at 17 W at the 2060 nm wavelength degeneracy point.","sentences":["In this paper, we present a single crystal, KTA (potassium titanyl-arsenate, KTiOAsO$_4$) based picosecond optical parametric amplifier pumped by an in-house built 1030 nm","Yb:YAG thin-disk laser, capable of tunability from 1.46 to 3.5 $\\mu$m, operating at 90 kHz, with high average power in the signal and idler beams.","The highest output power of 8.9 W was reached for the 1750 nm signal beam with 19% conversion efficiency and the respective 2500 nm idler beam power was 6.2 W with 13% efficiency.","The highest combined signal and idler mid-IR power was obtained at 17 W at the 2060 nm wavelength degeneracy point."],"url":"http://arxiv.org/abs/2404.19506v1","category":"physics.optics"}
{"created":"2024-04-30 12:37:01","title":"Towards Real-world Video Face Restoration: A New Benchmark","abstract":"Blind face restoration (BFR) on images has significantly progressed over the last several years, while real-world video face restoration (VFR), which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved, remains unsolved. Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images, which are limited in their coverage of real-world video frames. In this work, we introduced new real-world datasets named FOS with a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to study the applicability of current methods on videos. Compared with existing test datasets, FOS datasets cover more diverse degradations and involve face samples from more complex scenarios, which helps to revisit current face restoration approaches more comprehensively. Given the established datasets, we benchmarked both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches, identifying their potential and limitations in VFR tasks. In addition, we studied the effectiveness of the commonly used image quality assessment (IQA) metrics and face IQA (FIQA) metrics by leveraging a subjective user study. With extensive experimental results and detailed analysis provided, we gained insights from the successes and failures of both current BFR and VSR methods. These results also pose challenges to current face restoration approaches, which we hope stimulate future advances in VFR research.","sentences":["Blind face restoration (BFR) on images has significantly progressed over the last several years, while real-world video face restoration (VFR), which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved, remains unsolved.","Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images, which are limited in their coverage of real-world video frames.","In this work, we introduced new real-world datasets named FOS with a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to study the applicability of current methods on videos.","Compared with existing test datasets, FOS datasets cover more diverse degradations and involve face samples from more complex scenarios, which helps to revisit current face restoration approaches more comprehensively.","Given the established datasets, we benchmarked both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches, identifying their potential and limitations in VFR tasks.","In addition, we studied the effectiveness of the commonly used image quality assessment (IQA) metrics and face IQA (FIQA) metrics by leveraging a subjective user study.","With extensive experimental results and detailed analysis provided, we gained insights from the successes and failures of both current BFR and VSR methods.","These results also pose challenges to current face restoration approaches, which we hope stimulate future advances in VFR research."],"url":"http://arxiv.org/abs/2404.19500v1","category":"cs.CV"}
{"created":"2024-04-30 12:37:01","title":"A Unified Theory of Exact Inference and Learning in Exponential Family Latent Variable Models","abstract":"Bayes' rule describes how to infer posterior beliefs about latent variables given observations, and inference is a critical step in learning algorithms for latent variable models (LVMs). Although there are exact algorithms for inference and learning for certain LVMs such as linear Gaussian models and mixture models, researchers must typically develop approximate inference and learning algorithms when applying novel LVMs. In this paper we study the line that separates LVMs that rely on approximation schemes from those that do not, and develop a general theory of exponential family, latent variable models for which inference and learning may be implemented exactly. Firstly, under mild assumptions about the exponential family form of a given LVM, we derive necessary and sufficient conditions under which the LVM prior is in the same exponential family as its posterior, such that the prior is conjugate to the posterior. We show that all models that satisfy these conditions are constrained forms of a particular class of exponential family graphical model. We then derive general inference and learning algorithms, and demonstrate them on a variety of example models. Finally, we show how to compose our models into graphical models that retain tractable inference and learning. In addition to our theoretical work, we have implemented our algorithms in a collection of libraries with which we provide numerous demonstrations of our theory, and with which researchers may apply our theory in novel statistical settings.","sentences":["Bayes' rule describes how to infer posterior beliefs about latent variables given observations, and inference is a critical step in learning algorithms for latent variable models (LVMs).","Although there are exact algorithms for inference and learning for certain LVMs such as linear Gaussian models and mixture models, researchers must typically develop approximate inference and learning algorithms when applying novel LVMs.","In this paper we study the line that separates LVMs that rely on approximation schemes from those that do not, and develop a general theory of exponential family, latent variable models for which inference and learning may be implemented exactly.","Firstly, under mild assumptions about the exponential family form of a given LVM, we derive necessary and sufficient conditions under which the LVM prior is in the same exponential family as its posterior, such that the prior is conjugate to the posterior.","We show that all models that satisfy these conditions are constrained forms of a particular class of exponential family graphical model.","We then derive general inference and learning algorithms, and demonstrate them on a variety of example models.","Finally, we show how to compose our models into graphical models that retain tractable inference and learning.","In addition to our theoretical work, we have implemented our algorithms in a collection of libraries with which we provide numerous demonstrations of our theory, and with which researchers may apply our theory in novel statistical settings."],"url":"http://arxiv.org/abs/2404.19501v1","category":"cs.LG"}
{"created":"2024-04-30 12:33:41","title":"Into the MAG-verse or: Cosmology of the Complete Quadratic Metric-Affine Gravity","abstract":"We study the cosmology of the complete quadratic (in torsion and nonmetricity) metric-affine gravity. Namely, we add to the scalar-curvature gravitational Lagrangian, the 17 independent quadratic (parity-even and parity-odd) torsion and nonmetricity invariants. Sticking to a homogeneous and isotropic Friedmann-Robertson-Walker spacetime and assuming a perfect hyperfluid source, we explore the new effects that torsion and nonmetricity bring into play. It is shown that the inclusion of these invariants offers rich phenomenology. In particular, some well-known examples of exotic matter like cosmic strings, domain walls, stiff matter, etc., emerge quite naturally as manifestations of the fluid's intrinsic structure (hypermomentum). By studying the extended Friedmann equations in the complete quadratic theory and isolating the various parts of the hypermomentum, we find a plethora of solutions with interesting features.","sentences":["We study the cosmology of the complete quadratic (in torsion and nonmetricity) metric-affine gravity.","Namely, we add to the scalar-curvature gravitational Lagrangian, the 17 independent quadratic (parity-even and parity-odd) torsion and nonmetricity invariants.","Sticking to a homogeneous and isotropic Friedmann-Robertson-Walker spacetime and assuming a perfect hyperfluid source, we explore the new effects that torsion and nonmetricity bring into play.","It is shown that the inclusion of these invariants offers rich phenomenology.","In particular, some well-known examples of exotic matter like cosmic strings, domain walls, stiff matter, etc., emerge quite naturally as manifestations of the fluid's intrinsic structure (hypermomentum).","By studying the extended Friedmann equations in the complete quadratic theory and isolating the various parts of the hypermomentum, we find a plethora of solutions with interesting features."],"url":"http://arxiv.org/abs/2404.19498v1","category":"gr-qc"}
{"created":"2024-04-30 12:26:05","title":"The harms of class imbalance corrections for machine learning based prediction models: a simulation study","abstract":"Risk prediction models are increasingly used in healthcare to aid in clinical decision making. In most clinical contexts, model calibration (i.e., assessing the reliability of risk estimates) is critical. Data available for model development are often not perfectly balanced with respect to the modeled outcome (i.e., individuals with vs. without the event of interest are not equally represented in the data). It is common for researchers to correct this class imbalance, yet, the effect of such imbalance corrections on the calibration of machine learning models is largely unknown. We studied the effect of imbalance corrections on model calibration for a variety of machine learning algorithms. Using extensive Monte Carlo simulations we compared the out-of-sample predictive performance of models developed with an imbalance correction to those developed without a correction for class imbalance across different data-generating scenarios (varying sample size, the number of predictors and event fraction). Our findings were illustrated in a case study using MIMIC-III data. In all simulation scenarios, prediction models developed without a correction for class imbalance consistently had equal or better calibration performance than prediction models developed with a correction for class imbalance. The miscalibration introduced by correcting for class imbalance was characterized by an over-estimation of risk and was not always able to be corrected with re-calibration. Correcting for class imbalance is not always necessary and may even be harmful for clinical prediction models which aim to produce reliable risk estimates on an individual basis.","sentences":["Risk prediction models are increasingly used in healthcare to aid in clinical decision making.","In most clinical contexts, model calibration (i.e., assessing the reliability of risk estimates) is critical.","Data available for model development are often not perfectly balanced with respect to the modeled outcome (i.e., individuals with vs. without the event of interest are not equally represented in the data).","It is common for researchers to correct this class imbalance, yet, the effect of such imbalance corrections on the calibration of machine learning models is largely unknown.","We studied the effect of imbalance corrections on model calibration for a variety of machine learning algorithms.","Using extensive Monte Carlo simulations we compared the out-of-sample predictive performance of models developed with an imbalance correction to those developed without a correction for class imbalance across different data-generating scenarios (varying sample size, the number of predictors and event fraction).","Our findings were illustrated in a case study using MIMIC-III data.","In all simulation scenarios, prediction models developed without a correction for class imbalance consistently had equal or better calibration performance than prediction models developed with a correction for class imbalance.","The miscalibration introduced by correcting for class imbalance was characterized by an over-estimation of risk and was not always able to be corrected with re-calibration.","Correcting for class imbalance is not always necessary and may even be harmful for clinical prediction models which aim to produce reliable risk estimates on an individual basis."],"url":"http://arxiv.org/abs/2404.19494v1","category":"stat.ME"}
{"created":"2024-04-30 12:24:07","title":"Spherically symmetric Einstein-scalar-field equations for slowly particle-like decaying null infinity","abstract":"We show that the spherically symmetric Einstein-scalar-field equations for small slowly particle-like decaying initial data at null infinity have unique global solutions.","sentences":["We show that the spherically symmetric Einstein-scalar-field equations for small slowly particle-like decaying initial data at null infinity have unique global solutions."],"url":"http://arxiv.org/abs/2404.19493v1","category":"gr-qc"}
{"created":"2024-04-30 12:17:52","title":"Theoretical investigation of the relations between quantum decoherence and weak-to-strong measurement transition","abstract":"This paper delves into the crucial aspects of pointer-induced quantum decoherence and the transition between von Neumann's projective strong measurement and Aharonov's weak measurement. Both phenomena significantly impact the dynamical understanding of quantum measurement processes. Specifically, we focus on the interplay between quantum decoherence and the transition from weak to strong measurement by deducing and comparing the quantum decoherence and weak-to-strong measurement transition factors within a general model and using the well-known Stern-Gerlach experiment as an illustrative example. Our findings reveal that both phenomena can be effectively characterized by a universal transition factor intricately linked to the coupling between the system and the measurement apparatus. The analysis presented can clarify the mechanism behind the relations of quantum decoherence to the weak measurement and weak-to-strong measurement transition.","sentences":["This paper delves into the crucial aspects of pointer-induced quantum decoherence and the transition between von Neumann's projective strong measurement and Aharonov's weak measurement.","Both phenomena significantly impact the dynamical understanding of quantum measurement processes.","Specifically, we focus on the interplay between quantum decoherence and the transition from weak to strong measurement by deducing and comparing the quantum decoherence and weak-to-strong measurement transition factors within a general model and using the well-known Stern-Gerlach experiment as an illustrative example.","Our findings reveal that both phenomena can be effectively characterized by a universal transition factor intricately linked to the coupling between the system and the measurement apparatus.","The analysis presented can clarify the mechanism behind the relations of quantum decoherence to the weak measurement and weak-to-strong measurement transition."],"url":"http://arxiv.org/abs/2404.19488v1","category":"quant-ph"}
{"created":"2024-04-30 12:09:55","title":"Safe Training with Sensitive In-domain Data: Leveraging Data Fragmentation To Mitigate Linkage Attacks","abstract":"Current text generation models are trained using real data which can potentially contain sensitive information, such as confidential patient information and the like. Under certain conditions output of the training data which they have memorised can be triggered, exposing sensitive data. To mitigate against this risk we propose a safer alternative which sees fragmented data in the form of domain-specific short phrases randomly grouped together shared instead of full texts. Thus, text fragments that could re-identify an individual cannot be reproduced by the model in one sequence, giving significant protection against linkage attacks. We fine-tune several state-of-the-art LLMs using meaningful syntactic chunks to explore their utility. In particular, we fine-tune BERT-based models to predict two cardiovascular diagnoses. Our results demonstrate the capacity of LLMs to benefit from the pre-trained knowledge and deliver classification results when fine-tuned with fragmented data comparable to fine-tuning with full training data.","sentences":["Current text generation models are trained using real data which can potentially contain sensitive information, such as confidential patient information and the like.","Under certain conditions output of the training data which they have memorised can be triggered, exposing sensitive data.","To mitigate against this risk we propose a safer alternative which sees fragmented data in the form of domain-specific short phrases randomly grouped together shared instead of full texts.","Thus, text fragments that could re-identify an individual cannot be reproduced by the model in one sequence, giving significant protection against linkage attacks.","We fine-tune several state-of-the-art LLMs using meaningful syntactic chunks to explore their utility.","In particular, we fine-tune BERT-based models to predict two cardiovascular diagnoses.","Our results demonstrate the capacity of LLMs to benefit from the pre-trained knowledge and deliver classification results when fine-tuned with fragmented data comparable to fine-tuning with full training data."],"url":"http://arxiv.org/abs/2404.19486v1","category":"cs.CL"}
{"created":"2024-04-30 12:09:53","title":"IID Relaxation by Logical Expressivity: A Research Agenda for Fitting Logics to Neurosymbolic Requirements","abstract":"Neurosymbolic background knowledge and the expressivity required of its logic can break Machine Learning assumptions about data Independence and Identical Distribution. In this position paper we propose to analyze IID relaxation in a hierarchy of logics that fit different use case requirements. We discuss the benefits of exploiting known data dependencies and distribution constraints for Neurosymbolic use cases and argue that the expressivity required for this knowledge has implications for the design of underlying ML routines. This opens a new research agenda with general questions about Neurosymbolic background knowledge and the expressivity required of its logic.","sentences":["Neurosymbolic background knowledge and the expressivity required of its logic can break Machine Learning assumptions about data Independence and Identical Distribution.","In this position paper we propose to analyze IID relaxation in a hierarchy of logics that fit different use case requirements.","We discuss the benefits of exploiting known data dependencies and distribution constraints for Neurosymbolic use cases and argue that the expressivity required for this knowledge has implications for the design of underlying ML routines.","This opens a new research agenda with general questions about Neurosymbolic background knowledge and the expressivity required of its logic."],"url":"http://arxiv.org/abs/2404.19485v1","category":"cs.AI"}
{"created":"2024-04-30 12:05:48","title":"More Compute Is What You Need","abstract":"Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal. In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size. Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance.","sentences":["Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal.","In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size.","Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance."],"url":"http://arxiv.org/abs/2404.19484v1","category":"cs.LG"}
{"created":"2024-04-30 12:02:10","title":"Commuting matrices via commuting endomorphisms","abstract":"Evidences have suggested that counting representations are sometimes tractable even when the corresponding classification problem is almost impossible, or \"wild\" in a precise sense. Such counting problems are directly related to matrix counting problems, many of which are under active research. Using a general framework we formulate for such counting problems, we reduce some counting problems about commuting matries to problems about endomorphisms on all finite abelian $p$-groups. As an application, we count finite modules on some first examples of nonreduced curves over $\\mathbb{F}_q$. We also relate some classical and hard problems regarding commuting triples of matrices to a conjecture of Onn on counting conjugacy classes of the automorphism group of an arbitrary finite abelian $p$-group.","sentences":["Evidences have suggested that counting representations are sometimes tractable even when the corresponding classification problem is almost impossible, or \"wild\" in a precise sense.","Such counting problems are directly related to matrix counting problems, many of which are under active research.","Using a general framework we formulate for such counting problems, we reduce some counting problems about commuting matries to problems about endomorphisms on all finite abelian $p$-groups.","As an application, we count finite modules on some first examples of nonreduced curves over $\\mathbb{F}_q$. We also relate some classical and hard problems regarding commuting triples of matrices to a conjecture of Onn on counting conjugacy classes of the automorphism group of an arbitrary finite abelian $p$-group."],"url":"http://arxiv.org/abs/2404.19483v1","category":"math.RT"}
{"created":"2024-04-30 11:55:20","title":"FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking","abstract":"We introduce 'FactCheck Editor', an advanced text editor designed to automate fact-checking and correct factual inaccuracies. Given the widespread issue of misinformation, often a result of unintentional mistakes by content creators, our tool aims to address this challenge. It supports over 90 languages and utilizes transformer models to assist humans in the labor-intensive process of fact verification. This demonstration showcases a complete workflow that detects text claims in need of verification, generates relevant search engine queries, and retrieves appropriate documents from the web. It employs Natural Language Inference (NLI) to predict the veracity of claims and uses LLMs to summarize the evidence and suggest textual revisions to correct any errors in the text. Additionally, the effectiveness of models used in claim detection and veracity assessment is evaluated across multiple languages.","sentences":["We introduce 'FactCheck Editor', an advanced text editor designed to automate fact-checking and correct factual inaccuracies.","Given the widespread issue of misinformation, often a result of unintentional mistakes by content creators, our tool aims to address this challenge.","It supports over 90 languages and utilizes transformer models to assist humans in the labor-intensive process of fact verification.","This demonstration showcases a complete workflow that detects text claims in need of verification, generates relevant search engine queries, and retrieves appropriate documents from the web.","It employs Natural Language Inference (NLI) to predict the veracity of claims and uses LLMs to summarize the evidence and suggest textual revisions to correct any errors in the text.","Additionally, the effectiveness of models used in claim detection and veracity assessment is evaluated across multiple languages."],"url":"http://arxiv.org/abs/2404.19482v1","category":"cs.CL"}
{"created":"2024-04-30 11:48:13","title":"Mitigating and Analysis of Memory Usage Attack in IoE System","abstract":"Internet of Everything (IoE) is a newly emerging trend, especially in homes. Marketing forces toward smart homes are also accelerating the spread of IoE devices in households. An obvious risk of the rapid adoption of these smart devices is that many lack controls for protecting the privacy and security of end users from attacks designed to disrupt lives and incur financial losses. Today the smart home is a system for managing the basic life support processes of both small systems, e.g., commercial, office premises, apartments, cottages, and largely automated complexes, e.g., commercial and industrial complexes. One of the critical tasks to be solved by the concept of a modern smart home is the problem of preventing the usage of IoE resources. Recently, there has been a rapid increase in attacks on consumer IoE devices.   Memory corruption vulnerabilities constitute a significant class of vulnerabilities in software security through which attackers can gain control of an entire system. Numerous memory corruption vulnerabilities have been found in IoE firmware already deployed in the consumer market. This paper aims to analyze and explain the resource usage attack and create a low-cost simulation environment to aid in the dynamic analysis of the attack. Further, we perform controlled resource usage attacks while measuring resource consumption on resource-constrained victims' IoE devices, such as CPU and memory utilization. We also build a lightweight algorithm to detect memory usage attacks in the IoE environment. The result shows high efficiency in detecting and mitigating memory usage attacks by detecting when the intruder starts and stops the attack.","sentences":["Internet of Everything (IoE) is a newly emerging trend, especially in homes.","Marketing forces toward smart homes are also accelerating the spread of IoE devices in households.","An obvious risk of the rapid adoption of these smart devices is that many lack controls for protecting the privacy and security of end users from attacks designed to disrupt lives and incur financial losses.","Today the smart home is a system for managing the basic life support processes of both small systems, e.g., commercial, office premises, apartments, cottages, and largely automated complexes, e.g., commercial and industrial complexes.","One of the critical tasks to be solved by the concept of a modern smart home is the problem of preventing the usage of IoE resources.","Recently, there has been a rapid increase in attacks on consumer IoE devices.   ","Memory corruption vulnerabilities constitute a significant class of vulnerabilities in software security through which attackers can gain control of an entire system.","Numerous memory corruption vulnerabilities have been found in IoE firmware already deployed in the consumer market.","This paper aims to analyze and explain the resource usage attack and create a low-cost simulation environment to aid in the dynamic analysis of the attack.","Further, we perform controlled resource usage attacks while measuring resource consumption on resource-constrained victims' IoE devices, such as CPU and memory utilization.","We also build a lightweight algorithm to detect memory usage attacks in the IoE environment.","The result shows high efficiency in detecting and mitigating memory usage attacks by detecting when the intruder starts and stops the attack."],"url":"http://arxiv.org/abs/2404.19480v1","category":"cs.CR"}
{"created":"2024-04-30 11:47:35","title":"Reachability in temporal graphs under perturbation","abstract":"Reachability and other path-based measures on temporal graphs can be used to understand spread of infection, information, and people in modelled systems. Due to delays and errors in reporting, temporal graphs derived from data are unlikely to perfectly reflect reality, especially with respect to the precise times at which edges appear. To reflect this uncertainty, we consider a model in which some number $\\zeta$ of edge appearances may have their timestamps perturbed by $\\pm\\delta$ for some $\\delta$. Within this model, we investigate temporal reachability and consider the problem of determining the maximum number of vertices any vertex can reach under these perturbations. We show that this problem is intractable in general but is efficiently solvable when $\\zeta$ is sufficiently large. We also give algorithms which solve this problem in several restricted settings. We complement this with some contrasting results concerning the complexity of related temporal eccentricity problems under perturbation.","sentences":["Reachability and other path-based measures on temporal graphs can be used to understand spread of infection, information, and people in modelled systems.","Due to delays and errors in reporting, temporal graphs derived from data are unlikely to perfectly reflect reality, especially with respect to the precise times at which edges appear.","To reflect this uncertainty, we consider a model in which some number $\\zeta$ of edge appearances may have their timestamps perturbed by $\\pm\\delta$ for some $\\delta$. Within this model, we investigate temporal reachability and consider the problem of determining the maximum number of vertices any vertex can reach under these perturbations.","We show that this problem is intractable in general but is efficiently solvable when $\\zeta$ is sufficiently large.","We also give algorithms which solve this problem in several restricted settings.","We complement this with some contrasting results concerning the complexity of related temporal eccentricity problems under perturbation."],"url":"http://arxiv.org/abs/2404.19479v1","category":"cs.DM"}
{"created":"2024-04-30 11:46:43","title":"Non-Gaussian statistics in static and dynamic Galton boards","abstract":"Perturbing the arrangements of pegs on a static Galton board can result in non-trivial stationary distributions, which in the continuum limit correspond to departure from regular gaussian behavior. Two such distributions are obtained. Further, the distributions generated for a dynamic galton board under external forcing in a general direction are obtained by solution of the corresponding stochastic differential equations. Exact cumulant generating functions for the distribution are presented for forcing in one dimension. An approximate expression, correct to first order in the forcing amplitude, is presented for the case of two dimensions. Both cases show nontrivial departures from the static gaussian solution.","sentences":["Perturbing the arrangements of pegs on a static Galton board can result in non-trivial stationary distributions, which in the continuum limit correspond to departure from regular gaussian behavior.","Two such distributions are obtained.","Further, the distributions generated for a dynamic galton board under external forcing in a general direction are obtained by solution of the corresponding stochastic differential equations.","Exact cumulant generating functions for the distribution are presented for forcing in one dimension.","An approximate expression, correct to first order in the forcing amplitude, is presented for the case of two dimensions.","Both cases show nontrivial departures from the static gaussian solution."],"url":"http://arxiv.org/abs/2404.19478v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 11:43:37","title":"TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models","abstract":"Diffusion models have emerged as effective tools for generating diverse and high-quality content. However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions. In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: Crop Fusion for quality enhancement and Cross Sampling for efficiency optimization. We introduce a training-free optimizing stage to refine the similarity of the adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process. A comprehensive evaluation is conducted to compare TwinDiffusion with the existing methods, considering factors including coherence, fidelity, compatibility, and efficiency. The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation.","sentences":["Diffusion models have emerged as effective tools for generating diverse and high-quality content.","However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions.","In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: Crop Fusion for quality enhancement and Cross Sampling for efficiency optimization.","We introduce a training-free optimizing stage to refine the similarity of the adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process.","A comprehensive evaluation is conducted to compare TwinDiffusion with the existing methods, considering factors including coherence, fidelity, compatibility, and efficiency.","The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation."],"url":"http://arxiv.org/abs/2404.19475v1","category":"cs.CV"}
{"created":"2024-04-30 11:37:10","title":"New phenomenology in the first-order thermodynamics of scalar-tensor gravity for Bianchi universes","abstract":"The phase space of Bianchi I universes in vacuum Brans-Dicke gravity is analyzed in terms of physical variables. The behaviour of the solutions of the field equations near the fixed points (which are solutions of Einstein gravity) is compared with basic ideas of the recent first-order thermodynamics of scalar-tensor gravity, elucidating new phenomenology.","sentences":["The phase space of Bianchi I universes in vacuum Brans-Dicke gravity is analyzed in terms of physical variables.","The behaviour of the solutions of the field equations near the fixed points (which are solutions of Einstein gravity) is compared with basic ideas of the recent first-order thermodynamics of scalar-tensor gravity, elucidating new phenomenology."],"url":"http://arxiv.org/abs/2404.19470v1","category":"gr-qc"}
{"created":"2024-04-30 11:32:49","title":"Novel Topological Insulators with Hybrid-order Boundary States","abstract":"We report the discovery of several classes of novel topological insulators (TIs) with hybrid-order boundary states generated from the first-order TIs with additional crystalline symmetries. Unlike the current studies on hybrid-order TIs where different-order topology arises from merging different-order TIs in various energy, these novel TIs exhibit a remarkable coexsitence of first-order gapless modes and higher-order Fermi-arc states, behaving as a hybrid between the first-order TIs and higher-order topological semimetals within a single bulk gap. Our findings establish a profound connection between these novel $d$-dimensional ($d$D) TIs and ($d-1$)D higher-order TIs (HOTIs), which can be understood as a result of stacking $(d-1)$D HOTIs to $d$D with $d=3,4$, revealing unconventional topological phase transitions by closing the gap in certain first-order boundaries rather than the bulk. The bulk-boundary correspondence between these higher-order Fermi-arcs and bulk topological invariants acossiated with additional crystallline symmetries is also demonstrated. We then discuss the conventional topological phase transitions from these novel TIs to nodal-line/nodal-surface semimetal phases, where the gapless phases host new kinds of topological responses. Meawhile, we present the corresponding topological semimetal phases by stacking these unique TIs. Finally, we discuss potential ways to realize these novel phases in synthetic and real materials, with a particular focus on the feasible implementation in optical lattices using ultracold atoms.","sentences":["We report the discovery of several classes of novel topological insulators (TIs) with hybrid-order boundary states generated from the first-order TIs with additional crystalline symmetries.","Unlike the current studies on hybrid-order TIs where different-order topology arises from merging different-order TIs in various energy, these novel TIs exhibit a remarkable coexsitence of first-order gapless modes and higher-order Fermi-arc states, behaving as a hybrid between the first-order TIs and higher-order topological semimetals within a single bulk gap.","Our findings establish a profound connection between these novel $d$-dimensional ($d$D) TIs and ($d-1$)D higher-order TIs (HOTIs), which can be understood as a result of stacking $(d-1)$D HOTIs to $d$D with $d=3,4$, revealing unconventional topological phase transitions by closing the gap in certain first-order boundaries rather than the bulk.","The bulk-boundary correspondence between these higher-order Fermi-arcs and bulk topological invariants acossiated with additional crystallline symmetries is also demonstrated.","We then discuss the conventional topological phase transitions from these novel TIs to nodal-line/nodal-surface semimetal phases, where the gapless phases host new kinds of topological responses.","Meawhile, we present the corresponding topological semimetal phases by stacking these unique TIs.","Finally, we discuss potential ways to realize these novel phases in synthetic and real materials, with a particular focus on the feasible implementation in optical lattices using ultracold atoms."],"url":"http://arxiv.org/abs/2404.19469v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 11:32:28","title":"Compute-Forward Multiple Access for Gaussian Fast Fading Channels","abstract":"Compute-forward multiple access (CFMA) is a transmission strategy which allows the receiver in a multiple access channel (MAC) to first decode linear combinations of the transmitted signals and then solve for individual messages. Compared to existing MAC strategies such as joint decoding or successive interference cancellation (SIC), CFMA was shown to achieve the MAC capacity region for fixed channels under certain signal-to-noise (SNR) conditions without time-sharing using only single-user decoders. This paper studies the CFMA scheme for a two-user Gaussian fast fading MAC with channel state information only available at the receiver (CSIR). We develop appropriate lattice decoding schemes for the fading MAC and derive the achievable rate pairs for decoding linear combinations of codewords with any integer coefficients. We give a sufficient and necessary condition under which the proposed scheme can achieve the ergodic sum capacity. Furthermore, we investigate the impact of channel statistics on the capacity achievability of the CFMA scheme. In general, the sum capacity is achievable if the channel variance is small compared to the mean value of the channel strengths. Various numerical results are presented to illustrate the theoretical findings.","sentences":["Compute-forward multiple access (CFMA) is a transmission strategy which allows the receiver in a multiple access channel (MAC) to first decode linear combinations of the transmitted signals and then solve for individual messages.","Compared to existing MAC strategies such as joint decoding or successive interference cancellation (SIC), CFMA was shown to achieve the MAC capacity region for fixed channels under certain signal-to-noise (SNR) conditions without time-sharing using only single-user decoders.","This paper studies the CFMA scheme for a two-user Gaussian fast fading MAC with channel state information only available at the receiver (CSIR).","We develop appropriate lattice decoding schemes for the fading MAC and derive the achievable rate pairs for decoding linear combinations of codewords with any integer coefficients.","We give a sufficient and necessary condition under which the proposed scheme can achieve the ergodic sum capacity.","Furthermore, we investigate the impact of channel statistics on the capacity achievability of the CFMA scheme.","In general, the sum capacity is achievable if the channel variance is small compared to the mean value of the channel strengths.","Various numerical results are presented to illustrate the theoretical findings."],"url":"http://arxiv.org/abs/2404.19468v1","category":"cs.IT"}
{"created":"2024-04-30 11:28:28","title":"Optimal E-Values for Exponential Families: the Simple Case","abstract":"We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\\cal Q}$ and the null are in a certain relation. Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.","sentences":["We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family.","Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time.","Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families.","We start with a simple alternative $Q$ and a regular exponential family null.","Together these induce a second exponential family ${\\cal Q}$ containing $Q$, with the same sufficient statistic as the null.","Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\\cal Q}$ and the null are in a certain relation.","Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families."],"url":"http://arxiv.org/abs/2404.19465v1","category":"stat.ME"}
{"created":"2024-04-30 11:13:23","title":"Imitation Learning: A Survey of Learning Methods, Environments and Metrics","abstract":"Imitation learning is an approach in which an agent learns how to execute a task by trying to mimic how one or more teachers perform it. This learning approach offers a compromise between the time it takes to learn a new task and the effort needed to collect teacher samples for the agent. It achieves this by balancing learning from the teacher, who has some information on how to perform the task, and deviating from their examples when necessary, such as states not present in the teacher samples. Consequently, the field of imitation learning has received much attention from researchers in recent years, resulting in many new methods and applications. However, with this increase in published work and past surveys focusing mainly on methodology, a lack of standardisation became more prominent in the field. This non-standardisation is evident in the use of environments, which appear in no more than two works, and evaluation processes, such as qualitative analysis, that have become rare in current literature. In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers.","sentences":["Imitation learning is an approach in which an agent learns how to execute a task by trying to mimic how one or more teachers perform it.","This learning approach offers a compromise between the time it takes to learn a new task and the effort needed to collect teacher samples for the agent.","It achieves this by balancing learning from the teacher, who has some information on how to perform the task, and deviating from their examples when necessary, such as states not present in the teacher samples.","Consequently, the field of imitation learning has received much attention from researchers in recent years, resulting in many new methods and applications.","However, with this increase in published work and past surveys focusing mainly on methodology, a lack of standardisation became more prominent in the field.","This non-standardisation is evident in the use of environments, which appear in no more than two works, and evaluation processes, such as qualitative analysis, that have become rare in current literature.","In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers."],"url":"http://arxiv.org/abs/2404.19456v1","category":"cs.LG"}
{"created":"2024-04-30 11:11:50","title":"Bifurcations and explicit unfoldings of grazing loops connecting one high multiplicity tangent point","abstract":"For piecewise-smooth differential systems, in this paper we focus on crossing limit cycles and sliding loops bifurcating from a grazing loop connecting one high multiplicity tangent point. For the low multiplicity cases considered in previous publications, the method is to define and analyze return maps following the classic idea of Poincar\\'e. However, high multiplicity leads to that either domains or properties of return maps are unclear under perturbations. To overcome these difficulties, we unfold grazing loops by functional parameters and functional functions, and analyze this unfolding along some specific parameter curve. Relationships between multiplicity and the numbers of crossing limit cycles and sliding loops are given, and our results not only generalize the results obtained in [J. Differential Equations 255(2013), 4403-4436; 269(2020), 11396-11434], but also are new for some specific grazing loops.","sentences":["For piecewise-smooth differential systems, in this paper we focus on crossing limit cycles and sliding loops bifurcating from a grazing loop connecting one high multiplicity tangent point.","For the low multiplicity cases considered in previous publications, the method is to define and analyze return maps following the classic idea of Poincar\\'e.","However, high multiplicity leads to that either domains or properties of return maps are unclear under perturbations.","To overcome these difficulties, we unfold grazing loops by functional parameters and functional functions, and analyze this unfolding along some specific parameter curve.","Relationships between multiplicity and the numbers of crossing limit cycles and sliding loops are given, and our results not only generalize the results obtained in [J. Differential Equations 255(2013), 4403-4436; 269(2020), 11396-11434], but also are new for some specific grazing loops."],"url":"http://arxiv.org/abs/2404.19455v1","category":"math.DS"}
{"created":"2024-04-30 11:10:34","title":"Optimized neural forms for solving ordinary differential equations","abstract":"A critical issue in approximating solutions of ordinary differential equations using neural networks is the exact satisfaction of the boundary or initial conditions. For this purpose, neural forms have been introduced, i.e., functional expressions that depend on neural networks which, by design, satisfy the prescribed conditions exactly. Expanding upon prior progress, the present work contributes in three distinct aspects. First, it presents a novel formalism for crafting optimized neural forms. Second, it outlines a method for establishing an upper bound on the absolute deviation from the exact solution. Third, it introduces a technique for converting problems with Neumann or Robin conditions into equivalent problems with parametric Dirichlet conditions. The proposed optimized neural forms were numerically tested on a set of diverse problems, encompassing first-order and second-order ordinary differential equations, as well as first-order systems. Stiff and delay differential equations were also considered. The obtained solutions were compared against solutions obtained via Runge-Kutta methods and exact solutions wherever available. The reported results and analysis verify that in addition to the exact satisfaction of the boundary or initial conditions, optimized neural forms provide closed-form solutions of superior interpolation capability and controllable overall accuracy.","sentences":["A critical issue in approximating solutions of ordinary differential equations using neural networks is the exact satisfaction of the boundary or initial conditions.","For this purpose, neural forms have been introduced, i.e., functional expressions that depend on neural networks which, by design, satisfy the prescribed conditions exactly.","Expanding upon prior progress, the present work contributes in three distinct aspects.","First, it presents a novel formalism for crafting optimized neural forms.","Second, it outlines a method for establishing an upper bound on the absolute deviation from the exact solution.","Third, it introduces a technique for converting problems with Neumann or Robin conditions into equivalent problems with parametric Dirichlet conditions.","The proposed optimized neural forms were numerically tested on a set of diverse problems, encompassing first-order and second-order ordinary differential equations, as well as first-order systems.","Stiff and delay differential equations were also considered.","The obtained solutions were compared against solutions obtained via Runge-Kutta methods and exact solutions wherever available.","The reported results and analysis verify that in addition to the exact satisfaction of the boundary or initial conditions, optimized neural forms provide closed-form solutions of superior interpolation capability and controllable overall accuracy."],"url":"http://arxiv.org/abs/2404.19454v1","category":"cs.AI"}
{"created":"2024-04-30 11:06:42","title":"Lower General Position in Cartesian Products","abstract":"A subset $S$ of vertices of a graph $G$ is in \\emph{general position} if no shortest path in $G$ contains three vertices of $S$. The \\emph{general position problem} consists of finding the number of vertices in a largest general position set of $G$, whilst the \\emph{lower general position problem} asks for a smallest maximal general position set. In this paper we determine the lower general position numbers of several families of Cartesian products. We also show that the existence of small maximal general position sets in a Cartesian product is connected to a special type of general position set in the factors, which we call a \\emph{terminal set}, for which adding any vertex $u$ from outside the set creates three vertices in a line with $u$ as an endpoint. We give a constructive proof of the existence of terminal sets for graphs with diameter at most three. We also present conjectures on the existence of terminal sets for all graphs and a lower bound on the lower general position number of a Cartesian product in terms of the lower general position numbers of its factors.","sentences":["A subset $S$ of vertices of a graph $G$ is in \\emph{general position} if no shortest path in $G$ contains three vertices of $S$. The \\emph{general position problem} consists of finding the number of vertices in a largest general position set of $G$, whilst the \\emph{lower general position problem} asks for a smallest maximal general position set.","In this paper we determine the lower general position numbers of several families of Cartesian products.","We also show that the existence of small maximal general position sets in a Cartesian product is connected to a special type of general position set in the factors, which we call a \\emph{terminal set}, for which adding any vertex $u$ from outside the set creates three vertices in a line with $u$ as an endpoint.","We give a constructive proof of the existence of terminal sets for graphs with diameter at most three.","We also present conjectures on the existence of terminal sets for all graphs and a lower bound on the lower general position number of a Cartesian product in terms of the lower general position numbers of its factors."],"url":"http://arxiv.org/abs/2404.19451v1","category":"math.CO"}
{"created":"2024-04-30 11:04:37","title":"Classifications and bifurcations of tangent points and their loops of planar piecewise-smooth systems","abstract":"Tangent points, especial dynamics existing only in piecewise-smooth systems, usually have dynamical properties like equilibria of smooth systems. Loops connecting tangent points own partly properties of limit cycles and homoclinic loops of smooth systems. In this paper we give classifications for tangent points by tangency degree and for loops connecting them by configuration, and investigate their bifurcations. The classic method is to construct functional parameters for the case of low tangency degree but, is no longer valid for the case of general tangency degree, which leads to complicated interlacement of sliding and crossing motions on the switching manifold. We provide an explicit unfolding for tangent points of general tangency degree and their loops, in which explicit functional functions are constructed to replace functional parameters. We mainly obtain relations between original tangency degree and numbers of bifurcating tangent points, bifurcating tangent orbits and bifurcating loops for this unfolding. Some of these relations are generalizations to general tangency degree and others are new for previous publications.","sentences":["Tangent points, especial dynamics existing only in piecewise-smooth systems, usually have dynamical properties like equilibria of smooth systems.","Loops connecting tangent points own partly properties of limit cycles and homoclinic loops of smooth systems.","In this paper we give classifications for tangent points by tangency degree and for loops connecting them by configuration, and investigate their bifurcations.","The classic method is to construct functional parameters for the case of low tangency degree but, is no longer valid for the case of general tangency degree, which leads to complicated interlacement of sliding and crossing motions on the switching manifold.","We provide an explicit unfolding for tangent points of general tangency degree and their loops, in which explicit functional functions are constructed to replace functional parameters.","We mainly obtain relations between original tangency degree and numbers of bifurcating tangent points, bifurcating tangent orbits and bifurcating loops for this unfolding.","Some of these relations are generalizations to general tangency degree and others are new for previous publications."],"url":"http://arxiv.org/abs/2404.19450v1","category":"math.DS"}
{"created":"2024-04-30 10:48:43","title":"AnomalyXFusion: Multi-modal Anomaly Synthesis with Diffusion","abstract":"Anomaly synthesis is one of the effective methods to augment abnormal samples for training. However, current anomaly synthesis methods predominantly rely on texture information as input, which limits the fidelity of synthesized abnormal samples. Because texture information is insufficient to correctly depict the pattern of anomalies, especially for logical anomalies. To surmount this obstacle, we present the AnomalyXFusion framework, designed to harness multi-modality information to enhance the quality of synthesized abnormal samples. The AnomalyXFusion framework comprises two distinct yet synergistic modules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion (DDF) module. The MIF module refines modality alignment by aggregating and integrating various modality features into a unified embedding space, termed X-embedding, which includes image, text, and mask features. Concurrently, the DDF module facilitates controlled generation through an adaptive adjustment of X-embedding conditioned on the diffusion steps. In addition, to reveal the multi-modality representational power of AnomalyXFusion, we propose a new dataset, called MVTec Caption. More precisely, MVTec Caption extends 2.2k accurate image-mask-text annotations for the MVTec AD and LOCO datasets. Comprehensive evaluations demonstrate the effectiveness of AnomalyXFusion, especially regarding the fidelity and diversity for logical anomalies. Project page: http:github.com/hujiecpp/MVTec-Caption","sentences":["Anomaly synthesis is one of the effective methods to augment abnormal samples for training.","However, current anomaly synthesis methods predominantly rely on texture information as input, which limits the fidelity of synthesized abnormal samples.","Because texture information is insufficient to correctly depict the pattern of anomalies, especially for logical anomalies.","To surmount this obstacle, we present the AnomalyXFusion framework, designed to harness multi-modality information to enhance the quality of synthesized abnormal samples.","The AnomalyXFusion framework comprises two distinct yet synergistic modules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion (DDF) module.","The MIF module refines modality alignment by aggregating and integrating various modality features into a unified embedding space, termed X-embedding, which includes image, text, and mask features.","Concurrently, the DDF module facilitates controlled generation through an adaptive adjustment of X-embedding conditioned on the diffusion steps.","In addition, to reveal the multi-modality representational power of AnomalyXFusion, we propose a new dataset, called MVTec Caption.","More precisely, MVTec Caption extends 2.2k accurate image-mask-text annotations for the MVTec AD and LOCO datasets.","Comprehensive evaluations demonstrate the effectiveness of AnomalyXFusion, especially regarding the fidelity and diversity for logical anomalies.","Project page: http:github.com/hujiecpp/MVTec-Caption"],"url":"http://arxiv.org/abs/2404.19444v1","category":"cs.CV"}
{"created":"2024-04-30 10:45:40","title":"Which Nigerian-Pidgin does Generative AI speak?: Issues about Representativeness and Bias for Multilingual and Low Resource Languages","abstract":"Naija is the Nigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed language (e.g., English, Portuguese and Indigenous languages). Although it has mainly been a spoken language until recently, there are currently two written genres (BBC and Wikipedia) in Naija. Through statistical analyses and Machine Translation experiments, we prove that these two genres do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on Naija written in the BBC genre. In other words, Naija written in Wikipedia genre is not represented in Generative AI.","sentences":["Naija is the Nigerian-Pidgin spoken by approx.","120M speakers in Nigeria and it is a mixed language (e.g., English, Portuguese and Indigenous languages).","Although it has mainly been a spoken language until recently, there are currently two written genres (BBC and Wikipedia) in Naija.","Through statistical analyses and Machine Translation experiments, we prove that these two genres do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on Naija written in the BBC genre.","In other words, Naija written in Wikipedia genre is not represented in Generative AI."],"url":"http://arxiv.org/abs/2404.19442v1","category":"cs.CL"}
{"created":"2024-04-30 10:41:27","title":"Invariant divisors and equivariant line bundles","abstract":"Scalar relative invariants play an important role in the theory of group actions on a manifold as their zero sets are invariant hypersurfaces. Relative invariants are central in many applications, where they often are treated locally since an invariant hypersurface may not be a locus of a single function. Our aim is to establish a global theory of relative invariants.   For a Lie algebra $\\mathfrak{g}$ of holomorphic vector fields on a complex manifold $M$, any holomorphic $\\mathfrak{g}$-invariant hypersurface is given in terms of a $\\mathfrak{g}$-invariant divisor. This generalizes the classical notion of scalar relative $\\mathfrak{g}$-invariant. Any $\\mathfrak{g}$-invariant divisor gives rise to a $\\mathfrak{g}$-equivariant line bundle, and a large part of this paper is therefore devoted to the investigation of the group $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ of $\\mathfrak{g}$-equivariant line bundles. We give a cohomological description of $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ in terms of a double complex interpolating the Chevalley-Eilenberg complex for $\\mathfrak{g}$ with the \\v{C}ech complex of the sheaf of holomorphic functions on $M$.   We also obtain results about polynomial divisors on affine bundles and jet bundles. This has applications to the theory of differential invariants. Those were actively studied in relation to invariant differential equations, but the description of multipliers (or weights) of relative differential invariants was an open problem. We derive a characterization of them with our general theory. Examples, including projective geometry of curves and second-order ODEs, not only illustrate the developed machinery, but also give another approach and rigorously justify some classical computations. At the end, we briefly discuss generalizations of this theory.","sentences":["Scalar relative invariants play an important role in the theory of group actions on a manifold as their zero sets are invariant hypersurfaces.","Relative invariants are central in many applications, where they often are treated locally since an invariant hypersurface may not be a locus of a single function.","Our aim is to establish a global theory of relative invariants.   ","For a Lie algebra $\\mathfrak{g}$ of holomorphic vector fields on a complex manifold $M$, any holomorphic $\\mathfrak{g}$-invariant hypersurface is given in terms of a $\\mathfrak{g}$-invariant divisor.","This generalizes the classical notion of scalar relative $\\mathfrak{g}$-invariant.","Any $\\mathfrak{g}$-invariant divisor gives rise to a $\\mathfrak{g}$-equivariant line bundle, and a large part of this paper is therefore devoted to the investigation of the group $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ of $\\mathfrak{g}$-equivariant line bundles.","We give a cohomological description of $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ in terms of a double complex interpolating the Chevalley-Eilenberg complex for $\\mathfrak{g}$ with the \\v{C}ech complex of the sheaf of holomorphic functions on $M$.   We also obtain results about polynomial divisors on affine bundles and jet bundles.","This has applications to the theory of differential invariants.","Those were actively studied in relation to invariant differential equations, but the description of multipliers (or weights) of relative differential invariants was an open problem.","We derive a characterization of them with our general theory.","Examples, including projective geometry of curves and second-order ODEs, not only illustrate the developed machinery, but also give another approach and rigorously justify some classical computations.","At the end, we briefly discuss generalizations of this theory."],"url":"http://arxiv.org/abs/2404.19439v1","category":"math.DG"}
{"created":"2024-04-30 10:35:04","title":"Quintom cosmology and modified gravity after DESI 2024","abstract":"We reconstruct the cosmological background evolution under the scenario of dynamical dark energy through the Gaussian process approach, using the latest Dark Energy Spectroscopic Instrument (DESI) baryon acoustic oscillations (BAO) \\cite{DESI:2024mwx} combined with other observations. Our results reveal that the reconstructed dark-energy equation-of-state (EoS) parameter $w(z)$ exhibits the so-called quintom-B behavior, crossing $-1$ from phantom to quintessence regime as the universe expands. We investigate under what situation this type of evolution could be achieved from the perspectives of field theories and modified gravity. In particular, we reconstruct the corresponding actions for $f(R)$, $f(T)$, and $f(Q)$ gravity, respectively. We explicitly show that, certain modified gravity can exhibit the quintom dynamics and fit the recent DESI data efficiently, and for all cases the quadratic deviation from the $\\Lambda$CDM scenario is mildly favored.","sentences":["We reconstruct the cosmological background evolution under the scenario of dynamical dark energy through the Gaussian process approach, using the latest Dark Energy Spectroscopic Instrument (DESI) baryon acoustic oscillations (BAO) \\cite{DESI:2024mwx} combined with other observations.","Our results reveal that the reconstructed dark-energy equation-of-state (EoS) parameter $w(z)$ exhibits the so-called quintom-B behavior, crossing $-1$ from phantom to quintessence regime as the universe expands.","We investigate under what situation this type of evolution could be achieved from the perspectives of field theories and modified gravity.","In particular, we reconstruct the corresponding actions for $f(R)$, $f(T)$, and $f(Q)$ gravity, respectively.","We explicitly show that, certain modified gravity can exhibit the quintom dynamics and fit the recent DESI data efficiently, and for all cases the quadratic deviation from the $\\Lambda$CDM scenario is mildly favored."],"url":"http://arxiv.org/abs/2404.19437v1","category":"astro-ph.CO"}
{"created":"2024-04-30 10:34:43","title":"Electro-phononic and magneto-phononic frequency conversion","abstract":"Nonlinear frequency conversion by optical rectification, as well as difference- and sum-frequency generation are fundamental processes for producing electromagnetic radiation at different frequencies. Here, we demonstrate that coherently excited infrared-active phonons can be used as transducers for generating nonlinear electric polarizations and magnetizations via phonon-phonon and phonon-magnon interactions, in a way similar to nonlinear optical frequency conversion. We derive analytical solutions for the time-dependent polarizations and magnetizations for the second-order response to the electric field component of an ultrashort laser pulse. These allow us to define second-order nonlinear electric and magneto-electric susceptibilities that capture the rectification, as well as the impulsive and sum-frequency excitation of coherent phonons and magnons. Our theoretical framework naturally incorporates existing mechanisms and further leads to the prediction of a hybrid magneto-opto-phononic inverse Faraday effect involving photon-phonon-magnon scattering. Our work demonstrates nonlinear phononics as a pathway to controlling the electric polarization and magnetization in solids.","sentences":["Nonlinear frequency conversion by optical rectification, as well as difference- and sum-frequency generation are fundamental processes for producing electromagnetic radiation at different frequencies.","Here, we demonstrate that coherently excited infrared-active phonons can be used as transducers for generating nonlinear electric polarizations and magnetizations via phonon-phonon and phonon-magnon interactions, in a way similar to nonlinear optical frequency conversion.","We derive analytical solutions for the time-dependent polarizations and magnetizations for the second-order response to the electric field component of an ultrashort laser pulse.","These allow us to define second-order nonlinear electric and magneto-electric susceptibilities that capture the rectification, as well as the impulsive and sum-frequency excitation of coherent phonons and magnons.","Our theoretical framework naturally incorporates existing mechanisms and further leads to the prediction of a hybrid magneto-opto-phononic inverse Faraday effect involving photon-phonon-magnon scattering.","Our work demonstrates nonlinear phononics as a pathway to controlling the electric polarization and magnetization in solids."],"url":"http://arxiv.org/abs/2404.19436v1","category":"physics.optics"}
{"created":"2024-04-30 10:29:25","title":"Detection of Energy Consumption Cyber Attacks on Smart Devices","abstract":"With the rapid development of Internet of Things (IoT) technology, intelligent systems are increasingly integrating into everyday life and people's homes. However, the proliferation of these technologies raises concerns about the security of smart home devices. These devices often face resource constraints and may connect to unreliable networks, posing risks to the data they handle. Securing IoT technology is crucial due to the sensitive data involved.   Preventing energy attacks and ensuring the security of IoT infrastructure are key challenges in modern smart homes. Monitoring energy consumption can be an effective approach to detecting abnormal behavior and IoT cyberattacks. Lightweight algorithms are necessary to accommodate the resource limitations of IoT devices.   This paper presents a lightweight technique for detecting energy consumption attacks on smart home devices by analyzing received packets. The proposed algorithm considers TCP, UDP, and MQTT protocols, as well as device statuses (Idle, active, under attack). It accounts for resource constraints and promptly alerts administrators upon detecting an attack. The proposed approach effectively identifies energy consumption attacks by measuring packet reception rates for different protocols.","sentences":["With the rapid development of Internet of Things (IoT) technology, intelligent systems are increasingly integrating into everyday life and people's homes.","However, the proliferation of these technologies raises concerns about the security of smart home devices.","These devices often face resource constraints and may connect to unreliable networks, posing risks to the data they handle.","Securing IoT technology is crucial due to the sensitive data involved.   ","Preventing energy attacks and ensuring the security of IoT infrastructure are key challenges in modern smart homes.","Monitoring energy consumption can be an effective approach to detecting abnormal behavior and IoT cyberattacks.","Lightweight algorithms are necessary to accommodate the resource limitations of IoT devices.   ","This paper presents a lightweight technique for detecting energy consumption attacks on smart home devices by analyzing received packets.","The proposed algorithm considers TCP, UDP, and MQTT protocols, as well as device statuses (Idle, active, under attack).","It accounts for resource constraints and promptly alerts administrators upon detecting an attack.","The proposed approach effectively identifies energy consumption attacks by measuring packet reception rates for different protocols."],"url":"http://arxiv.org/abs/2404.19434v1","category":"cs.CR"}
{"created":"2024-04-30 10:28:04","title":"Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships","abstract":"Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason (or rather, approximately reason). Since to date these lines of work progressed largely in parallel (with notable exceptions), we are interested in investigating the intersection: probing for reasoning about the implicitly-held knowledge. Suspecting the performance to be lacking in this area, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs a bird has versus the number of wheels on a tricycle). We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only. It is difficult to argue that pure statistical learning can cope with the combinatorial explosion inherent in many commonsense reasoning tasks, especially once arithmetical notions are involved. Further, we argue that bigger is not always better and chasing purely statistical improvements is flawed at the core, since it only exacerbates the dangerous conflation of the production of correct answers with genuine reasoning ability.","sentences":["Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason (or rather, approximately reason).","Since to date these lines of work progressed largely in parallel (with notable exceptions), we are interested in investigating the intersection: probing for reasoning about the implicitly-held knowledge.","Suspecting the performance to be lacking in this area, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs a bird has versus the number of wheels on a tricycle).","We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only.","It is difficult to argue that pure statistical learning can cope with the combinatorial explosion inherent in many commonsense reasoning tasks, especially once arithmetical notions are involved.","Further, we argue that bigger is not always better and chasing purely statistical improvements is flawed at the core, since it only exacerbates the dangerous conflation of the production of correct answers with genuine reasoning ability."],"url":"http://arxiv.org/abs/2404.19432v1","category":"cs.CL"}
{"created":"2024-04-30 10:16:21","title":"InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation","abstract":"In the field of personalized image generation, the ability to create images preserving concepts has significantly improved. Creating an image that naturally integrates multiple concepts in a cohesive and visually appealing composition can indeed be challenging. This paper introduces \"InstantFamily,\" an approach that employs a novel masked cross-attention mechanism and a multimodal embedding stack to achieve zero-shot multi-ID image generation. Our method effectively preserves ID as it utilizes global and local features from a pre-trained face recognition model integrated with text conditions. Additionally, our masked cross-attention mechanism enables the precise control of multi-ID and composition in the generated images. We demonstrate the effectiveness of InstantFamily through experiments showing its dominance in generating images with multi-ID, while resolving well-known multi-ID generation problems. Additionally, our model achieves state-of-the-art performance in both single-ID and multi-ID preservation. Furthermore, our model exhibits remarkable scalability with a greater number of ID preservation than it was originally trained with.","sentences":["In the field of personalized image generation, the ability to create images preserving concepts has significantly improved.","Creating an image that naturally integrates multiple concepts in a cohesive and visually appealing composition can indeed be challenging.","This paper introduces \"InstantFamily,\" an approach that employs a novel masked cross-attention mechanism and a multimodal embedding stack to achieve zero-shot multi-ID image generation.","Our method effectively preserves ID as it utilizes global and local features from a pre-trained face recognition model integrated with text conditions.","Additionally, our masked cross-attention mechanism enables the precise control of multi-ID and composition in the generated images.","We demonstrate the effectiveness of InstantFamily through experiments showing its dominance in generating images with multi-ID, while resolving well-known multi-ID generation problems.","Additionally, our model achieves state-of-the-art performance in both single-ID and multi-ID preservation.","Furthermore, our model exhibits remarkable scalability with a greater number of ID preservation than it was originally trained with."],"url":"http://arxiv.org/abs/2404.19427v1","category":"cs.CV"}
{"created":"2024-04-30 10:15:57","title":"Heat capacity of periodically driven two-level systems","abstract":"We define the heat capacity for steady periodically driven systems and as an example we compute it for dissipative two-level systems where the energy gap is time-modulated. There, as a function of ambient temperature, the Schottky peak remains the dominant feature. Yet, in contrast with equilibrium, the quasistatic thermal response of a nonequilibrium system also reveals kinetic information present in the transition rates; e.g., the heat capacity depends on the time-symmetric reactivities and changes by the presence of a kinetic barrier. It still vanishes though at absolute zero, in accord with an extended Nernst heat postulate, but at a different rate from the equilibrium case. More generally, we discuss the dependence on driving frequency and amplitude.","sentences":["We define the heat capacity for steady periodically driven systems and as an example we compute it for dissipative two-level systems where the energy gap is time-modulated.","There, as a function of ambient temperature, the Schottky peak remains the dominant feature.","Yet, in contrast with equilibrium, the quasistatic thermal response of a nonequilibrium system also reveals kinetic information present in the transition rates; e.g., the heat capacity depends on the time-symmetric reactivities and changes by the presence of a kinetic barrier.","It still vanishes though at absolute zero, in accord with an extended Nernst heat postulate, but at a different rate from the equilibrium case.","More generally, we discuss the dependence on driving frequency and amplitude."],"url":"http://arxiv.org/abs/2404.19426v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 10:13:18","title":"Intrinsic negative magnetoresistance from the chiral anomaly of multifold fermions","abstract":"The chiral anomaly, a hallmark of chiral spin-1/2 Weyl fermions, is an imbalance between left- and right-moving particles that underpins both high and low energy phenomena, including particle decay and negative longitudinal magnetoresistance in Weyl semimetals. The discovery that chiral crystals can host higher-spin generalizations of Weyl quasiparticles without high-energy counterparts, known as multifold fermions, raises the fundamental question of whether the chiral anomaly is a more general phenomenon. Answering this question requires materials with chiral quasiparticles within a sizable energy window around the Fermi level, that are unaffected by trivial extrinsic effects such as current jetting. Here we report the chiral anomaly of multifold fermions in CoSi, which features multifold bands within about 0.85 eV around the Fermi level. By excluding current jetting through the squeezing test, we measure an intrinsic, longitudinal negative magnetoresistance. We develop the semiclassical theory of magnetotransport of multifold fermions that shows that the negative magnetoresistance originates in their chiral anomaly, despite a sizable and detrimental orbital magnetic moment contribution, previously unaccounted for. A concomitant nonlinear Hall effect supports the multifold-fermion origin of magnetotransport. Our work confirms the chiral anomaly of higher-spin generalizations of Weyl fermions, currently inaccessible outside the solid-state.","sentences":["The chiral anomaly, a hallmark of chiral spin-1/2 Weyl fermions, is an imbalance between left- and right-moving particles that underpins both high and low energy phenomena, including particle decay and negative longitudinal magnetoresistance in Weyl semimetals.","The discovery that chiral crystals can host higher-spin generalizations of Weyl quasiparticles without high-energy counterparts, known as multifold fermions, raises the fundamental question of whether the chiral anomaly is a more general phenomenon.","Answering this question requires materials with chiral quasiparticles within a sizable energy window around the Fermi level, that are unaffected by trivial extrinsic effects such as current jetting.","Here we report the chiral anomaly of multifold fermions in CoSi, which features multifold bands within about 0.85 eV around the Fermi level.","By excluding current jetting through the squeezing test, we measure an intrinsic, longitudinal negative magnetoresistance.","We develop the semiclassical theory of magnetotransport of multifold fermions that shows that the negative magnetoresistance originates in their chiral anomaly, despite a sizable and detrimental orbital magnetic moment contribution, previously unaccounted for.","A concomitant nonlinear Hall effect supports the multifold-fermion origin of magnetotransport.","Our work confirms the chiral anomaly of higher-spin generalizations of Weyl fermions, currently inaccessible outside the solid-state."],"url":"http://arxiv.org/abs/2404.19424v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 10:13:01","title":"Thermodynamics of charged Lifshitz black holes with scalar hair","abstract":"In this work, we discuss the generalized Einstein-Maxwell-Dilaton gravity theory with a nonminimal coupling between the Maxwell field and scalar field. Considering different geometric properties of black hole horizon structure, the charged dilaton Lifshitz black hole solutions are presented in 4-dimensional spacetimes. Later, utilizing the Wald Formalism, we derive the thermodynamic first law of black hole and conserved quantities. According to the relationship between the heat capacity and the local stability of black hole, we study the stability of charged Lifshitz black holes and identify the thermodynamic stable region of black holes that meet the criteria.","sentences":["In this work, we discuss the generalized Einstein-Maxwell-Dilaton gravity theory with a nonminimal coupling between the Maxwell field and scalar field.","Considering different geometric properties of black hole horizon structure, the charged dilaton Lifshitz black hole solutions are presented in 4-dimensional spacetimes.","Later, utilizing the Wald Formalism, we derive the thermodynamic first law of black hole and conserved quantities.","According to the relationship between the heat capacity and the local stability of black hole, we study the stability of charged Lifshitz black holes and identify the thermodynamic stable region of black holes that meet the criteria."],"url":"http://arxiv.org/abs/2404.19423v2","category":"gr-qc"}
{"created":"2024-04-30 10:11:44","title":"Let's Focus: Focused Backdoor Attack against Federated Transfer Learning","abstract":"Federated Transfer Learning (FTL) is the most general variation of Federated Learning. According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data. After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor. Each involved client contributes by locally training only the classification layers on a private training set. The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor. State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients. Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step. Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation. In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class. Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario. With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning.","sentences":["Federated Transfer Learning (FTL) is the most general variation of Federated Learning.","According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data.","After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor.","Each involved client contributes by locally training only the classification layers on a private training set.","The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor.","State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients.","Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step.","Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation.","In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class.","Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario.","With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning."],"url":"http://arxiv.org/abs/2404.19420v1","category":"cs.LG"}
{"created":"2024-04-30 10:02:58","title":"Fat equator effect and Minimality in immersions and submersions of the Sphere","abstract":"Inspired by the equatorial concentration of measure phenomenon in the sphere, a result which is deduced from the general, (and intrinsic), concentration of measure in $\\mathbb{S}^n(1)$, we describe in this paper an equatorial concentration of measure satisfied by the closed, (compact without boundary), isometric and minimal immersions $x:\\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\leq n$), and by the minimal Riemannian submersions $\\pi: \\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\geq n$).","sentences":["Inspired by the equatorial concentration of measure phenomenon in the sphere, a result which is deduced from the general, (and intrinsic), concentration of measure in $\\mathbb{S}^n(1)$, we describe in this paper an equatorial concentration of measure satisfied by the closed, (compact without boundary), isometric and minimal immersions $x:\\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\leq n$), and by the minimal Riemannian submersions $\\pi: \\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\geq n$)."],"url":"http://arxiv.org/abs/2404.19416v1","category":"math.DG"}
{"created":"2024-04-30 10:01:45","title":"Two-Stage Robust Planning Model for Park-Level Integrated Energy System Considering Uncertain Equipment Contingency","abstract":"In this paper, we propose a two-stage robust planning model for an Integrated Energy System (IES) that serves an industrial park. The term 'Park-level IES' is used to refers to IES of a smaller scale but have high demands for various forms of energy. The proposed planning model considers uncertainties like load demand fluctuations and equipment contingencies, and provides a reliable scheme of equipment selection and sizing for IES investors. Inspired by the unit commitment problem, we formulate an equipment contingency uncertainty set to accurately describe the potential equipment contingencies which happen and can be repaired within a day. Then, a novel and modified nested column-and-constraint generation algorithm is applied to solve this two-stage robust planning model with integer recourse efficiently. In the case study, the role of energy storage system for IES reliability enhancement is analyzed in detail. Computational results demonstrate the advantage of the proposed models over the deterministic planning model in terms of improving reliability.","sentences":["In this paper, we propose a two-stage robust planning model for an Integrated Energy System (IES) that serves an industrial park.","The term 'Park-level IES' is used to refers to IES of a smaller scale but have high demands for various forms of energy.","The proposed planning model considers uncertainties like load demand fluctuations and equipment contingencies, and provides a reliable scheme of equipment selection and sizing for IES investors.","Inspired by the unit commitment problem, we formulate an equipment contingency uncertainty set to accurately describe the potential equipment contingencies which happen and can be repaired within a day.","Then, a novel and modified nested column-and-constraint generation algorithm is applied to solve this two-stage robust planning model with integer recourse efficiently.","In the case study, the role of energy storage system for IES reliability enhancement is analyzed in detail.","Computational results demonstrate the advantage of the proposed models over the deterministic planning model in terms of improving reliability."],"url":"http://arxiv.org/abs/2404.19415v1","category":"eess.SY"}
{"created":"2024-04-30 10:01:36","title":"Toward the application of large-$N$ deconfinement to SU(3) QCD","abstract":"It is generally known for $\\mathrm{U}(N)$ gauge theory at finite temperature that phase transitions are manifested by taking the large-$N$ limit. Since the large-$N$ theory undergoes two thermodynamic phase transitions, a nontrivial intermediate phase can be realized in addition to the phases classified as the conventional confined and deconfined phases. In this talk, we discuss that a similar picture can be applied to QCD with $N=3$. In particular, we analyze the gauge configurations of lattice QCD calculations involving dynamical quarks and show the results of an analysis of the deviation due to finite-temperature effects from the Haar randomness expected at zero temperature in $\\mathrm{SU}(N)$ gauge theory, using physical pictures suggested by the large-$N$ theory.","sentences":["It is generally known for $\\mathrm{U}(N)$ gauge theory at finite temperature that phase transitions are manifested by taking the large-$N$ limit.","Since the large-$N$ theory undergoes two thermodynamic phase transitions, a nontrivial intermediate phase can be realized in addition to the phases classified as the conventional confined and deconfined phases.","In this talk, we discuss that a similar picture can be applied to QCD with $N=3$. In particular, we analyze the gauge configurations of lattice QCD calculations involving dynamical quarks and show the results of an analysis of the deviation due to finite-temperature effects from the Haar randomness expected at zero temperature in $\\mathrm{SU}(N)$ gauge theory, using physical pictures suggested by the large-$N$ theory."],"url":"http://arxiv.org/abs/2404.19414v1","category":"hep-th"}
{"created":"2024-04-30 09:57:21","title":"Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning","abstract":"While Reinforcement Learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO). Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning. Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself. Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective. Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function. This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation. We show the effectiveness of RCfD on three language tasks, which achieves comparable performance to carefully tuned baselines while mitigating ROO.","sentences":["While Reinforcement Learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO).","Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning.","Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself.","Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective.","Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function.","This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation.","We show the effectiveness of RCfD on three language tasks, which achieves comparable performance to carefully tuned baselines while mitigating ROO."],"url":"http://arxiv.org/abs/2404.19409v1","category":"cs.CL"}
{"created":"2024-04-30 09:56:07","title":"A simple method for compiling quantum stabilizer circuits","abstract":"Stabilizer circuits play an important role in quantum error correction protocols, and will be vital for ensuring fault tolerance in future quantum hardware. While stabilizer circuits are defined on the Clifford generating set, {H, S, CX}, not all of these gates are native to quantum hardware. As such they must be compiled into the native gateset, with the key difference across hardware archetypes being the native two qubit gate. Here we introduce an intuitive and accessible method for Clifford gate compilation. While multiple open source solutions exist for quantum circuit compilation, these operate on arbitrary quantum gates. By restricting ourselves to Clifford gates, the compilation process becomes almost trivial and even large circuits can be compiled manually. The core idea is well known: if two Clifford circuits conjugate Paulis identically, they are equivalent. Compilation is then reduced to ensuring that the instantaneous Pauli conjugation is correct for each qubit at every timestep. This is Tableaux Manipulation, so called as we directly interrogate stabilizer tableaux to ensure correct Pauli conjugation. We provide a brief explanation of the process along with some worked examples to build intuition; we finally show some comparisons for compiling large circuits to open source software, and highlight that this method ensures a minimal number of quantum gates are employed.","sentences":["Stabilizer circuits play an important role in quantum error correction protocols, and will be vital for ensuring fault tolerance in future quantum hardware.","While stabilizer circuits are defined on the Clifford generating set, {H, S, CX}, not all of these gates are native to quantum hardware.","As such they must be compiled into the native gateset, with the key difference across hardware archetypes being the native two qubit gate.","Here we introduce an intuitive and accessible method for Clifford gate compilation.","While multiple open source solutions exist for quantum circuit compilation, these operate on arbitrary quantum gates.","By restricting ourselves to Clifford gates, the compilation process becomes almost trivial and even large circuits can be compiled manually.","The core idea is well known: if two Clifford circuits conjugate Paulis identically, they are equivalent.","Compilation is then reduced to ensuring that the instantaneous Pauli conjugation is correct for each qubit at every timestep.","This is Tableaux Manipulation, so called as we directly interrogate stabilizer tableaux to ensure correct Pauli conjugation.","We provide a brief explanation of the process along with some worked examples to build intuition; we finally show some comparisons for compiling large circuits to open source software, and highlight that this method ensures a minimal number of quantum gates are employed."],"url":"http://arxiv.org/abs/2404.19408v1","category":"quant-ph"}
{"created":"2024-04-30 09:48:11","title":"Transformer-Enhanced Motion Planner: Attention-Guided Sampling for State-Specific Decision Making","abstract":"Sampling-based motion planning (SBMP) algorithms are renowned for their robust global search capabilities. However, the inherent randomness in their sampling mechanisms often result in inconsistent path quality and limited search efficiency. In response to these challenges, this work proposes a novel deep learning-based motion planning framework, named Transformer-Enhanced Motion Planner (TEMP), which synergizes an Environmental Information Semantic Encoder (EISE) with a Motion Planning Transformer (MPT). EISE converts environmental data into semantic environmental information (SEI), providing MPT with an enriched environmental comprehension. MPT leverages an attention mechanism to dynamically recalibrate its focus on SEI, task objectives, and historical planning data, refining the sampling node generation. To demonstrate the capabilities of TEMP, we train our model using a dataset comprised of planning results produced by the RRT*. EISE and MPT are collaboratively trained, enabling EISE to autonomously learn and extract patterns from environmental data, thereby forming semantic representations that MPT could more effectively interpret and utilize for motion planning. Subsequently, we conducted a systematic evaluation of TEMP's efficacy across diverse task dimensions, which demonstrates that TEMP achieves exceptional performance metrics and a heightened degree of generalizability compared to state-of-the-art SBMPs.","sentences":["Sampling-based motion planning (SBMP) algorithms are renowned for their robust global search capabilities.","However, the inherent randomness in their sampling mechanisms often result in inconsistent path quality and limited search efficiency.","In response to these challenges, this work proposes a novel deep learning-based motion planning framework, named Transformer-Enhanced Motion Planner (TEMP), which synergizes an Environmental Information Semantic Encoder (EISE) with a Motion Planning Transformer (MPT).","EISE converts environmental data into semantic environmental information (SEI), providing MPT with an enriched environmental comprehension.","MPT leverages an attention mechanism to dynamically recalibrate its focus on SEI, task objectives, and historical planning data, refining the sampling node generation.","To demonstrate the capabilities of TEMP, we train our model using a dataset comprised of planning results produced by the RRT*.","EISE and MPT are collaboratively trained, enabling EISE to autonomously learn and extract patterns from environmental data, thereby forming semantic representations that MPT could more effectively interpret and utilize for motion planning.","Subsequently, we conducted a systematic evaluation of TEMP's efficacy across diverse task dimensions, which demonstrates that TEMP achieves exceptional performance metrics and a heightened degree of generalizability compared to state-of-the-art SBMPs."],"url":"http://arxiv.org/abs/2404.19403v1","category":"cs.RO"}
{"created":"2024-04-30 09:47:44","title":"UniFS: Universal Few-shot Instance Perception with Point Representations","abstract":"Instance perception tasks (object detection, instance segmentation, pose estimation, counting) play a key role in industrial applications of visual models. As supervised learning methods suffer from high labeling cost, few-shot learning methods which effectively learn from a limited number of labeled examples are desired. Existing few-shot learning methods primarily focus on a restricted set of tasks, presumably due to the challenges involved in designing a generic model capable of representing diverse tasks in a unified manner. In this paper, we propose UniFS, a universal few-shot instance perception model that unifies a wide range of instance perception tasks by reformulating them into a dynamic point representation learning framework. Additionally, we propose Structure-Aware Point Learning (SAPL) to exploit the higher-order structural relationship among points to further enhance representation learning. Our approach makes minimal assumptions about the tasks, yet it achieves competitive results compared to highly specialized and well optimized specialist models. Codes will be released soon.","sentences":["Instance perception tasks (object detection, instance segmentation, pose estimation, counting) play a key role in industrial applications of visual models.","As supervised learning methods suffer from high labeling cost, few-shot learning methods which effectively learn from a limited number of labeled examples are desired.","Existing few-shot learning methods primarily focus on a restricted set of tasks, presumably due to the challenges involved in designing a generic model capable of representing diverse tasks in a unified manner.","In this paper, we propose UniFS, a universal few-shot instance perception model that unifies a wide range of instance perception tasks by reformulating them into a dynamic point representation learning framework.","Additionally, we propose Structure-Aware Point Learning (SAPL) to exploit the higher-order structural relationship among points to further enhance representation learning.","Our approach makes minimal assumptions about the tasks, yet it achieves competitive results compared to highly specialized and well optimized specialist models.","Codes will be released soon."],"url":"http://arxiv.org/abs/2404.19401v1","category":"cs.CV"}
{"created":"2024-04-30 09:45:41","title":"3D Gaussian Blendshapes for Head Avatar Animation","abstract":"We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.","sentences":["We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars.","Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models.","Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance.","The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients.","High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting.","Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance."],"url":"http://arxiv.org/abs/2404.19398v1","category":"cs.GR"}
{"created":"2024-04-30 09:42:40","title":"Can humans teach machines to code?","abstract":"The goal of inductive program synthesis is for a machine to automatically generate a program from user-supplied examples of the desired behaviour of the program. A key underlying assumption is that humans can provide examples of sufficient quality to teach a concept to a machine. However, as far as we are aware, this assumption lacks both empirical and theoretical support. To address this limitation, we explore the question `Can humans teach machines to code?'. To answer this question, we conduct a study where we ask humans to generate examples for six programming tasks, such as finding the maximum element of a list. We compare the performance of a program synthesis system trained on (i) human-provided examples, (ii) randomly sampled examples, and (iii) expert-provided examples. Our results show that, on most of the tasks, non-expert participants did not provide sufficient examples for a program synthesis system to learn an accurate program. Our results also show that non-experts need to provide more examples than both randomly sampled and expert-provided examples.","sentences":["The goal of inductive program synthesis is for a machine to automatically generate a program from user-supplied examples of the desired behaviour of the program.","A key underlying assumption is that humans can provide examples of sufficient quality to teach a concept to a machine.","However, as far as we are aware, this assumption lacks both empirical and theoretical support.","To address this limitation, we explore the question `Can humans teach machines to code?'.","To answer this question, we conduct a study where we ask humans to generate examples for six programming tasks, such as finding the maximum element of a list.","We compare the performance of a program synthesis system trained on (i) human-provided examples, (ii) randomly sampled examples, and (iii) expert-provided examples.","Our results show that, on most of the tasks, non-expert participants did not provide sufficient examples for a program synthesis system to learn an accurate program.","Our results also show that non-experts need to provide more examples than both randomly sampled and expert-provided examples."],"url":"http://arxiv.org/abs/2404.19397v1","category":"cs.HC"}
{"created":"2024-04-30 09:40:07","title":"CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation","abstract":"State space models and Mamba-based models have been increasingly applied across various domains, achieving state-of-the-art performance. This technical report introduces the first attempt to train a transferable Mamba model utilizing contrastive language-image pretraining (CLIP). We have trained Mamba models of varying sizes and undertaken comprehensive evaluations of these models on 26 zero-shot classification datasets and 16 out-of-distribution (OOD) datasets. Our findings reveal that a Mamba model with 67 million parameters is on par with a 307 million-parameter Vision Transformer (ViT) model in zero-shot classification tasks, highlighting the parameter efficiency of Mamba models. In tests of OOD generalization, Mamba-based models exhibit exceptional performance in conditions of OOD image contrast or when subjected to high-pass filtering. However, a Hessian analysis indicates that Mamba models feature a sharper and more non-convex landscape compared to ViT-based models, making them more challenging to train. The source code is available at https://github.com/raytrun/mamba-clip.","sentences":["State space models and Mamba-based models have been increasingly applied across various domains, achieving state-of-the-art performance.","This technical report introduces the first attempt to train a transferable Mamba model utilizing contrastive language-image pretraining (CLIP).","We have trained Mamba models of varying sizes and undertaken comprehensive evaluations of these models on 26 zero-shot classification datasets and 16 out-of-distribution (OOD) datasets.","Our findings reveal that a Mamba model with 67 million parameters is on par with a 307 million-parameter Vision Transformer (ViT) model in zero-shot classification tasks, highlighting the parameter efficiency of Mamba models.","In tests of OOD generalization, Mamba-based models exhibit exceptional performance in conditions of OOD image contrast or when subjected to high-pass filtering.","However, a Hessian analysis indicates that Mamba models feature a sharper and more non-convex landscape compared to ViT-based models, making them more challenging to train.","The source code is available at https://github.com/raytrun/mamba-clip."],"url":"http://arxiv.org/abs/2404.19394v1","category":"cs.CV"}
{"created":"2024-04-30 09:38:40","title":"Sharp embedding results and geometric inequalities for H\u00f6rmander vector fields","abstract":"Let $U$ be a connected open subset of $\\mathbb{R}^n$, and let $X=(X_1,X_{2},\\ldots,X_m)$ be a system of H\\\"{o}rmander vector fields defined on $U$. This paper addresses sharp embedding results and geometric inequalities in the generalized Sobolev space $\\mathcal{W}_{X,0}^{k,p}(\\Omega)$, where $\\Omega\\subset\\subset U$ is a general open bounded subset of $U$. By employing Rothschild-Stein's lifting technique and saturation method, we prove the representation formula for smooth functions with compact support in $\\Omega$. Combining this representation formula with weighted weak-$L^p$ estimates, we derive sharp Sobolev inequalities on $\\mathcal{W}_{X,0}^{k,p}(\\Omega)$, where the critical Sobolev exponent depends on the generalized M\\'{e}tivier index. As applications of these sharp Sobolev inequalities, we establish the isoperimetric inequality, logarithmic Sobolev inequalities, Rellich-Kondrachov compact embedding theorem, Gagliardo-Nirenberg inequality, Nash inequality, and Moser-Trudinger inequality in the context of general H\\\"{o}rmander vector fields.","sentences":["Let $U$ be a connected open subset of $\\mathbb{R}^n$, and let $X=(X_1,X_{2},\\ldots,X_m)$ be a system of H\\\"{o}rmander vector fields defined on $U$. This paper addresses sharp embedding results and geometric inequalities in the generalized Sobolev space $\\mathcal{W}_{X,0}^{k,p}(\\Omega)$, where $\\Omega\\subset\\subset U$ is a general open bounded subset of $U$. By employing Rothschild-Stein's lifting technique and saturation method, we prove the representation formula for smooth functions with compact support in $\\Omega$. Combining this representation formula with weighted weak-$L^p$ estimates, we derive sharp Sobolev inequalities on $\\mathcal{W}_{X,0}^{k,p}(\\Omega)$, where the critical Sobolev exponent depends on the generalized M\\'{e}tivier index.","As applications of these sharp Sobolev inequalities, we establish the isoperimetric inequality, logarithmic Sobolev inequalities, Rellich-Kondrachov compact embedding theorem, Gagliardo-Nirenberg inequality, Nash inequality, and Moser-Trudinger inequality in the context of general H\\\"{o}rmander vector fields."],"url":"http://arxiv.org/abs/2404.19393v1","category":"math.AP"}
{"created":"2024-04-30 09:23:42","title":"Online Electricity Purchase for Data Center with Dynamic Virtual Battery from Flexibility Aggregation","abstract":"As a critical component of modern infrastructure, data centers account for a huge amount of power consumption and greenhouse gas emission. This paper studies the electricity purchase strategy for a data center to lower its energy cost while integrating local renewable generation under uncertainty. To facilitate efficient and scalable decision-making, we propose a two-layer hierarchy where the lower layer consists of the operation of all electrical equipment in the data center and the upper layer determines the procurement and dispatch of electricity. At the lower layer, instead of device-level scheduling in real time, we propose to exploit the inherent flexibility in demand, such as thermostatically controlled loads and flexible computing tasks, and aggregate them into virtual batteries. By this means, the upper-layer decision only needs to take into account these virtual batteries, the size of which is generally small and independent of the data center scale. We further propose an online algorithm based on Lyapunov optimization to purchase electricity from the grid with a manageable energy cost, even though the prices, renewable availability, and battery specifications are uncertain and dynamic. In particular, we show that, under mild conditions, our algorithm can achieve bounded loss compared with the offline optimal cost, while strictly respecting battery operational constraints. Extensive simulation studies validate the theoretical analysis and illustrate the tradeoff between optimality and conservativeness.","sentences":["As a critical component of modern infrastructure, data centers account for a huge amount of power consumption and greenhouse gas emission.","This paper studies the electricity purchase strategy for a data center to lower its energy cost while integrating local renewable generation under uncertainty.","To facilitate efficient and scalable decision-making, we propose a two-layer hierarchy where the lower layer consists of the operation of all electrical equipment in the data center and the upper layer determines the procurement and dispatch of electricity.","At the lower layer, instead of device-level scheduling in real time, we propose to exploit the inherent flexibility in demand, such as thermostatically controlled loads and flexible computing tasks, and aggregate them into virtual batteries.","By this means, the upper-layer decision only needs to take into account these virtual batteries, the size of which is generally small and independent of the data center scale.","We further propose an online algorithm based on Lyapunov optimization to purchase electricity from the grid with a manageable energy cost, even though the prices, renewable availability, and battery specifications are uncertain and dynamic.","In particular, we show that, under mild conditions, our algorithm can achieve bounded loss compared with the offline optimal cost, while strictly respecting battery operational constraints.","Extensive simulation studies validate the theoretical analysis and illustrate the tradeoff between optimality and conservativeness."],"url":"http://arxiv.org/abs/2404.19387v1","category":"eess.SY"}
{"created":"2024-04-30 09:20:35","title":"Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection","abstract":"Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.","sentences":["Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA).","These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain.","However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background.","Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process.","To resolve this problem, in this paper, we propose a novel pseudo label refinery framework.","Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy.","This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box.","Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process.","We alleviate this issue by generating additional proposals and aligning RoI features across different domains.","Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks.","Code will be available at https://github.com/Zhanwei-Z/PERE."],"url":"http://arxiv.org/abs/2404.19384v1","category":"cs.CV"}
{"created":"2024-04-30 09:16:30","title":"Cross-Block Fine-Grained Semantic Cascade for Skeleton-Based Sports Action Recognition","abstract":"Human action video recognition has recently attracted more attention in applications such as video security and sports posture correction. Popular solutions, including graph convolutional networks (GCNs) that model the human skeleton as a spatiotemporal graph, have proven very effective. GCNs-based methods with stacked blocks usually utilize top-layer semantics for classification/annotation purposes. Although the global features learned through the procedure are suitable for the general classification, they have difficulty capturing fine-grained action change across adjacent frames -- decisive factors in sports actions. In this paper, we propose a novel ``Cross-block Fine-grained Semantic Cascade (CFSC)'' module to overcome this challenge. In summary, the proposed CFSC progressively integrates shallow visual knowledge into high-level blocks to allow networks to focus on action details. In particular, the CFSC module utilizes the GCN feature maps produced at different levels, as well as aggregated features from proceeding levels to consolidate fine-grained features. In addition, a dedicated temporal convolution is applied at each level to learn short-term temporal features, which will be carried over from shallow to deep layers to maximize the leverage of low-level details. This cross-block feature aggregation methodology, capable of mitigating the loss of fine-grained information, has resulted in improved performance. Last, FD-7, a new action recognition dataset for fencing sports, was collected and will be made publicly available. Experimental results and empirical analysis on public benchmarks (FSD-10) and self-collected (FD-7) demonstrate the advantage of our CFSC module on learning discriminative patterns for action classification over others.","sentences":["Human action video recognition has recently attracted more attention in applications such as video security and sports posture correction.","Popular solutions, including graph convolutional networks (GCNs) that model the human skeleton as a spatiotemporal graph, have proven very effective.","GCNs-based methods with stacked blocks usually utilize top-layer semantics for classification/annotation purposes.","Although the global features learned through the procedure are suitable for the general classification, they have difficulty capturing fine-grained action change across adjacent frames -- decisive factors in sports actions.","In this paper, we propose a novel ``Cross-block Fine-grained Semantic Cascade (CFSC)''","module to overcome this challenge.","In summary, the proposed CFSC progressively integrates shallow visual knowledge into high-level blocks to allow networks to focus on action details.","In particular, the CFSC module utilizes the GCN feature maps produced at different levels, as well as aggregated features from proceeding levels to consolidate fine-grained features.","In addition, a dedicated temporal convolution is applied at each level to learn short-term temporal features, which will be carried over from shallow to deep layers to maximize the leverage of low-level details.","This cross-block feature aggregation methodology, capable of mitigating the loss of fine-grained information, has resulted in improved performance.","Last, FD-7, a new action recognition dataset for fencing sports, was collected and will be made publicly available.","Experimental results and empirical analysis on public benchmarks (FSD-10) and self-collected (FD-7) demonstrate the advantage of our CFSC module on learning discriminative patterns for action classification over others."],"url":"http://arxiv.org/abs/2404.19383v1","category":"cs.CV"}
{"created":"2024-04-30 09:14:54","title":"Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective","abstract":"Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.","sentences":["Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation.","Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models.","However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts.","This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model.","And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity.","Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting.","This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models.","Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models.","This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods.","Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts."],"url":"http://arxiv.org/abs/2404.19382v1","category":"cs.CV"}
{"created":"2024-04-30 09:14:12","title":"Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders","abstract":"To overcome the memory capacity wall of large-scale AI and big data applications, Compute Express Link (CXL) enables cost-efficient memory expansion beyond the local DRAM of processors. While its CXL.mem protocol stack minimizes interconnect latency, CXL memory accesses can still result in significant slowdowns for memory-bound applications. While near-data processing (NDP) in CXL memory can overcome such limitations, prior works propose application-specific HW units that are not suitable for practical CXL memory-based systems that should support various applications. On the other hand, existing CPU or GPU cores are not cost-effective for NDP because they are not optimized for memory-bound applications. In addition, the communication between the host processor and CXL controller for NDP offloading should achieve low latency, but the CXL$.$io (or PCIe) protocol incurs $\\mu$s-scale latency and is not suitable for fine-grain NDP.   To achieve high-performance NDP end-to-end, we propose a low-overhead general-purpose NDP architecture for CXL memory referred to as Memory-Mapped NDP (M$^2$NDP), which comprises memory-mapped functions (M$^2$func) and memory-mapped $\\mu$threading (M$^2\\mu$thr). The M$^2$func is a CXL.mem-compatible low-overhead communication mechanism between the host processor and NDP controller in the CXL memory. The M$^2\\mu$thr enables low-cost, general-purpose NDP unit design by introducing lightweight $\\mu$threads that support highly concurrent execution of NDP kernels with minimal resource wastage. By combining them, our M$^2$NDP achieves significant speedups for various applications, including in-memory OLAP, key-value store, large language model, recommendation model, and graph analytics by up to 128$\\times$ (11.5$\\times$ overall) and reduces energy by up to 87.9\\% (80.1\\% overall) compared to a baseline CPU or GPU host with passive CXL memory.","sentences":["To overcome the memory capacity wall of large-scale AI and big data applications, Compute Express Link (CXL) enables cost-efficient memory expansion beyond the local DRAM of processors.","While its CXL.mem protocol stack minimizes interconnect latency, CXL memory accesses can still result in significant slowdowns for memory-bound applications.","While near-data processing (NDP) in CXL memory can overcome such limitations, prior works propose application-specific HW units that are not suitable for practical CXL memory-based systems that should support various applications.","On the other hand, existing CPU or GPU cores are not cost-effective for NDP because they are not optimized for memory-bound applications.","In addition, the communication between the host processor and CXL controller for NDP offloading should achieve low latency, but the CXL$.$io (or PCIe) protocol incurs $\\mu$s-scale latency and is not suitable for fine-grain NDP.   ","To achieve high-performance NDP end-to-end, we propose a low-overhead general-purpose NDP architecture for CXL memory referred to as Memory-Mapped NDP (M$^2$NDP), which comprises memory-mapped functions (M$^2$func) and memory-mapped $\\mu$threading (M$^2\\mu$thr).","The M$^2$func is a CXL.mem-compatible low-overhead communication mechanism between the host processor and NDP controller in the CXL memory.","The M$^2\\mu$thr enables low-cost, general-purpose NDP unit design by introducing lightweight $\\mu$threads that support highly concurrent execution of NDP kernels with minimal resource wastage.","By combining them, our M$^2$NDP achieves significant speedups for various applications, including in-memory OLAP, key-value store, large language model, recommendation model, and graph analytics by up to 128$\\times$ (11.5$\\times$ overall) and reduces energy by up to 87.9\\% (80.1\\% overall) compared to a baseline CPU or GPU host with passive CXL memory."],"url":"http://arxiv.org/abs/2404.19381v1","category":"cs.AR"}
{"created":"2024-04-30 09:12:55","title":"Strength in numbers: A multiphase wind model with multiple cloud populations","abstract":"Galactic outflows have a multiphase nature making them challenging to model analytically. Many previous studies have tried to produce models that come closer to reality. In this work, we continue these efforts and describe the interaction of the hot wind fluid with multiple cold cloud populations, with their number density determined by different probability density functions. To do so, we introduced realistic cloud-wind interaction source terms and a time-varying cooling area. We find that the model reproduces well results from small-scale hydrodynamic simulations, but exhibits a general destructive behavior both for a single cloud population as well as multiple ones. We show that including multiple cloud populations can alter the evolution of the wind drastically. We also compare our model to observations and show that the differential acceleration of multiple clouds can lead to a non-negligible velocity `dispersion' relevant for down-the-barrel studies. Furthermore, we compute the emitted cooling surface brightness and find it generally too faint to explain observed Lyman-$\\alpha$ halos.","sentences":["Galactic outflows have a multiphase nature making them challenging to model analytically.","Many previous studies have tried to produce models that come closer to reality.","In this work, we continue these efforts and describe the interaction of the hot wind fluid with multiple cold cloud populations, with their number density determined by different probability density functions.","To do so, we introduced realistic cloud-wind interaction source terms and a time-varying cooling area.","We find that the model reproduces well results from small-scale hydrodynamic simulations, but exhibits a general destructive behavior both for a single cloud population as well as multiple ones.","We show that including multiple cloud populations can alter the evolution of the wind drastically.","We also compare our model to observations and show that the differential acceleration of multiple clouds can lead to a non-negligible velocity `dispersion' relevant for down-the-barrel studies.","Furthermore, we compute the emitted cooling surface brightness and find it generally too faint to explain observed Lyman-$\\alpha$ halos."],"url":"http://arxiv.org/abs/2404.19380v1","category":"astro-ph.GA"}
{"created":"2024-04-30 09:09:14","title":"Gaussian mixtures closest to a given measure via optimal transport","abstract":"Given a determinate (multivariate) probability measure $\\mu$, we characterize Gaussian mixtures $\\nu\\_\\phi$ which minimize the Wasserstein distance $W\\_2(\\mu,\\nu\\_\\phi)$ to $\\mu$ when the mixing probability measure $\\phi$ on the parameters $(m,\\Sigma)$ of the Gaussians is supported on a compact set $S$.(i) We first show that such mixtures are optimal solutions of a particular optimal transport (OT) problem where the marginal $\\nu\\_{\\phi}$ of the OT problem is also unknown via the mixing measure variable $\\phi$. Next (ii) by using a well-known specific property of Gaussian measures, this optimal transport is then viewed as a Generalized Moment Problem (GMP) and if the set $S$ of mixture parameters $(m,\\Sigma)$ is a basic compact semi-algebraic set, we provide a \"mesh-free\" numerical scheme to approximate as closely as desired the optimal distance by solving a hierarchy of semidefinite relaxations of increasing size. In particular, we neither assume that the mixing measure is finitely supported nor that the variance is the same for all components. If the original measure $\\mu$ is not a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then a strictly positive distance is detected at a finite step of the hierarchy. If the original measure $\\mu$ is a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then all semidefinite relaxations of the hierarchy have same zero optimal value. Moreover if the mixing measure is atomic with finite support, its components can sometimes be extracted from an optimal solution at some semidefinite relaxation of the hierarchy when Curto & Fialkow's flatness condition holds for some moment matrix.","sentences":["Given a determinate (multivariate) probability measure $\\mu$, we characterize Gaussian mixtures $\\nu\\_\\phi$ which minimize the Wasserstein distance $W\\_2(\\mu,\\nu\\_\\phi)$ to $\\mu$ when the mixing probability measure $\\phi$ on the parameters $(m,\\Sigma)$ of the Gaussians is supported on a compact set $S$.(i)","We first show that such mixtures are optimal solutions of a particular optimal transport (OT) problem where the marginal $\\nu\\_{\\phi}$ of the OT problem is also unknown via the mixing measure variable $\\phi$. Next (ii) by using a well-known specific property of Gaussian measures, this optimal transport is then viewed as a Generalized Moment Problem (GMP) and if the set $S$ of mixture parameters $(m,\\Sigma)$ is a basic compact semi-algebraic set, we provide a \"mesh-free\" numerical scheme to approximate as closely as desired the optimal distance by solving a hierarchy of semidefinite relaxations of increasing size.","In particular, we neither assume that the mixing measure is finitely supported nor that the variance is the same for all components.","If the original measure $\\mu$ is not a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then a strictly positive distance is detected at a finite step of the hierarchy.","If the original measure $\\mu$ is a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then all semidefinite relaxations of the hierarchy have same zero optimal value.","Moreover if the mixing measure is atomic with finite support, its components can sometimes be extracted from an optimal solution at some semidefinite relaxation of the hierarchy when Curto & Fialkow's flatness condition holds for some moment matrix."],"url":"http://arxiv.org/abs/2404.19378v1","category":"math.OC"}
{"created":"2024-04-30 09:09:03","title":"Toroidic phase transitions in a direct-kagome artificial spin ice","abstract":"Ferrotoroidicity, the fourth form of primary ferroic order, breaks both space and time inversion symmetry. So far, direct observation of ferrotoroidicity in natural materials remains elusive, which impedes the exploration of ferrotoroidic phase transitions. Here, we overcome the limitations of natural materials using an artificial nanomagnet system that can be characterized at the constituent level and at different effective temperatures. We design a nanomagnet array as to realize a direct-kagome spin ice. This artificial spin ice exhibits robust toroidal moments and a quasi-degenerate ground state with two distinct low-temperature toroidal phases: ferrotoroidicity and paratoroidicity. Using magnetic force microscopy and Monte Carlo simulation, we demonstrate a phase transition between ferrotoroidicity and paratoroidicity, along with a crossover to a non-toroidal paramagnetic phase. Our quasi-degenerate artificial spin ice in a direct-kagome structure provides a model system for the investigation of magnetic states and phase transitions that are inaccessible in natural materials.","sentences":["Ferrotoroidicity, the fourth form of primary ferroic order, breaks both space and time inversion symmetry.","So far, direct observation of ferrotoroidicity in natural materials remains elusive, which impedes the exploration of ferrotoroidic phase transitions.","Here, we overcome the limitations of natural materials using an artificial nanomagnet system that can be characterized at the constituent level and at different effective temperatures.","We design a nanomagnet array as to realize a direct-kagome spin ice.","This artificial spin ice exhibits robust toroidal moments and a quasi-degenerate ground state with two distinct low-temperature toroidal phases: ferrotoroidicity and paratoroidicity.","Using magnetic force microscopy and Monte Carlo simulation, we demonstrate a phase transition between ferrotoroidicity and paratoroidicity, along with a crossover to a non-toroidal paramagnetic phase.","Our quasi-degenerate artificial spin ice in a direct-kagome structure provides a model system for the investigation of magnetic states and phase transitions that are inaccessible in natural materials."],"url":"http://arxiv.org/abs/2404.19377v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 09:06:32","title":"Deep low-latency joint speech transmission and enhancement over a gaussian channel","abstract":"Ensuring intelligible speech communication for hearing assistive devices in low-latency scenarios presents significant challenges in terms of speech enhancement, coding and transmission. In this paper, we propose novel solutions for low-latency joint speech transmission and enhancement, leveraging deep neural networks (DNNs). Our approach integrates two state-of-the-art DNN architectures for low-latency speech enhancement and low-latency analog joint source-channel-based transmission, creating a combined low-latency system and jointly training both systems in an end-to-end approach. Due to the computational demands of the enhancement system, this order is suitable when high computational power is unavailable in the decoder, like hearing assistive devices. The proposed system enables the configuration of total latency, achieving high performance even at latencies as low as 3 ms, which is typically challenging to attain. The simulation results provide compelling evidence that a joint enhancement and transmission system is superior to a simple concatenation system in diverse settings, encompassing various wireless channel conditions, latencies, and background noise scenarios.","sentences":["Ensuring intelligible speech communication for hearing assistive devices in low-latency scenarios presents significant challenges in terms of speech enhancement, coding and transmission.","In this paper, we propose novel solutions for low-latency joint speech transmission and enhancement, leveraging deep neural networks (DNNs).","Our approach integrates two state-of-the-art DNN architectures for low-latency speech enhancement and low-latency analog joint source-channel-based transmission, creating a combined low-latency system and jointly training both systems in an end-to-end approach.","Due to the computational demands of the enhancement system, this order is suitable when high computational power is unavailable in the decoder, like hearing assistive devices.","The proposed system enables the configuration of total latency, achieving high performance even at latencies as low as 3 ms, which is typically challenging to attain.","The simulation results provide compelling evidence that a joint enhancement and transmission system is superior to a simple concatenation system in diverse settings, encompassing various wireless channel conditions, latencies, and background noise scenarios."],"url":"http://arxiv.org/abs/2404.19375v1","category":"eess.AS"}
{"created":"2024-04-30 08:59:53","title":"AutoNet: Automatic Reachability Policy Management in Public Cloud Networks","abstract":"Virtual Private Cloud (VPC) is the main network abstraction technology used in public cloud systems. VPCs are composed of a set of network services that permit the definition of complex network reachability properties among internal and external cloud entities such as tenants' VMs or some generic internet nodes. Although hiding the underlying complexity through a comprehensible abstraction layer, manually enforcing particular reachability intents in VPC networks is still notably error-prone and complex. In this paper, we propose AutoNet, a new model for assisting cloud tenants in managing reachability-based policies in VPC networks. AutoNet is capable of safely generating incremental VPC configurations while satisfying some metric-based high-level intent defined by the tenants. To achieve this goal, we leverage a MaxSAT-based encoding of the network configuration combined with several optimizations to scale to topologies with thousands of nodes. Our results show that the developed system is capable of achieving a sub-second response time for production VPC deployments while still providing fine-grained control over the generated configurations.","sentences":["Virtual Private Cloud (VPC) is the main network abstraction technology used in public cloud systems.","VPCs are composed of a set of network services that permit the definition of complex network reachability properties among internal and external cloud entities such as tenants' VMs or some generic internet nodes.","Although hiding the underlying complexity through a comprehensible abstraction layer, manually enforcing particular reachability intents in VPC networks is still notably error-prone and complex.","In this paper, we propose AutoNet, a new model for assisting cloud tenants in managing reachability-based policies in VPC networks.","AutoNet is capable of safely generating incremental VPC configurations while satisfying some metric-based high-level intent defined by the tenants.","To achieve this goal, we leverage a MaxSAT-based encoding of the network configuration combined with several optimizations to scale to topologies with thousands of nodes.","Our results show that the developed system is capable of achieving a sub-second response time for production VPC deployments while still providing fine-grained control over the generated configurations."],"url":"http://arxiv.org/abs/2404.19372v1","category":"cs.NI"}
{"created":"2024-04-30 08:58:47","title":"Numeric Reward Machines","abstract":"Reward machines inform reinforcement learning agents about the reward structure of the environment and often drastically speed up the learning process. However, reward machines only accept Boolean features such as robot-reached-gold. Consequently, many inherently numeric tasks cannot profit from the guidance offered by reward machines. To address this gap, we aim to extend reward machines with numeric features such as distance-to-gold. For this, we present two types of reward machines: numeric-Boolean and numeric. In a numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold. In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold. We compare our new approaches to a baseline reward machine in the Craft domain, where the numeric feature is the agent-to-target distance. We use cross-product Q-learning, Q-learning with counter-factual experiences, and the options framework for learning. Our experimental results show that our new approaches significantly outperform the baseline approach. Extending reward machines with numeric features opens up new possibilities of using reward machines in inherently numeric tasks.","sentences":["Reward machines inform reinforcement learning agents about the reward structure of the environment and often drastically speed up the learning process.","However, reward machines only accept Boolean features such as robot-reached-gold.","Consequently, many inherently numeric tasks cannot profit from the guidance offered by reward machines.","To address this gap, we aim to extend reward machines with numeric features such as distance-to-gold.","For this, we present two types of reward machines: numeric-Boolean and numeric.","In a numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold.","In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold.","We compare our new approaches to a baseline reward machine in the Craft domain, where the numeric feature is the agent-to-target distance.","We use cross-product Q-learning, Q-learning with counter-factual experiences, and the options framework for learning.","Our experimental results show that our new approaches significantly outperform the baseline approach.","Extending reward machines with numeric features opens up new possibilities of using reward machines in inherently numeric tasks."],"url":"http://arxiv.org/abs/2404.19370v1","category":"cs.AI"}
{"created":"2024-04-30 08:51:49","title":"Exploring Multi-Lingual Bias of Large Code Models in Code Generation","abstract":"Code generation aims to synthesize code and fulfill functional requirements based on natural language (NL) specifications, which can greatly improve development efficiency. In the era of large language models (LLMs), large code models (LCMs) have been recently proposed to generate source code. LCMs can generate highly feasible solutions for programming problems described in natural language. Despite the effectiveness, we observe a noticeable multilingual bias in the generation performance of LCMs. Specifically, LCMs demonstrate proficiency in generating solutions when provided with instructions in English, yet may falter when faced with semantically equivalent instructions in other NLs such as Chinese. Moreover, the ability of LCMs to generate code exhibits variety across different programming languages (PLs), such as Python and C++. The observed phenomenon indicates the presence of multi-lingual bias within the generative capabilities of LCMs, which has remained unexplored.   In this paper, we aim to investigate the multi-lingual bias that exists in current LCMs. First, we initiate our investigation by constructing the first multi-lingual evaluation benchmark X-HumanEval-X, enabling us to systematically evaluate the extent of multi-lingual bias that exists in current LCMs. In our large-scale experiments on nine popular LCMs, we observe a pronounced multi-lingual bias of LCMs in code generation, including multi-NL and multi-PL bias. Specifically, when using Chinese instructions, the code generation capabilities of LCMs decrease by at least 13% in terms of the Pass@1 metric. Furthermore, LCMs perform variously across different programming languages, e.g., the performance gap between Python and C++ reaches as high as 20.9%. ...","sentences":["Code generation aims to synthesize code and fulfill functional requirements based on natural language (NL) specifications, which can greatly improve development efficiency.","In the era of large language models (LLMs), large code models (LCMs) have been recently proposed to generate source code.","LCMs can generate highly feasible solutions for programming problems described in natural language.","Despite the effectiveness, we observe a noticeable multilingual bias in the generation performance of LCMs.","Specifically, LCMs demonstrate proficiency in generating solutions when provided with instructions in English, yet may falter when faced with semantically equivalent instructions in other NLs such as Chinese.","Moreover, the ability of LCMs to generate code exhibits variety across different programming languages (PLs), such as Python and C++.","The observed phenomenon indicates the presence of multi-lingual bias within the generative capabilities of LCMs, which has remained unexplored.   ","In this paper, we aim to investigate the multi-lingual bias that exists in current LCMs.","First, we initiate our investigation by constructing the first multi-lingual evaluation benchmark X-HumanEval-X, enabling us to systematically evaluate the extent of multi-lingual bias that exists in current LCMs.","In our large-scale experiments on nine popular LCMs, we observe a pronounced multi-lingual bias of LCMs in code generation, including multi-NL and multi-PL bias.","Specifically, when using Chinese instructions, the code generation capabilities of LCMs decrease by at least 13% in terms of the Pass@1 metric.","Furthermore, LCMs perform variously across different programming languages, e.g., the performance gap between Python and C++ reaches as high as 20.9%. ..."],"url":"http://arxiv.org/abs/2404.19368v1","category":"cs.SE"}
{"created":"2024-04-30 08:51:13","title":"Toward thermoelectric characterization of (nano)materials by in situ transmission electron microscopy","abstract":"We explore the possibility to perform an in situ transmission electron microscopy (TEM) thermoelectric characterization of materials. A differential heating element on a custom in situ TEM microchip allows to generate a temperature gradient across the studied materials, which are simultaneously measured electrically. A thermovoltage was induced in all studied devices, whose sign corresponds to the sign of the Seebeck coefficient of the tested materials. The results indicate that in situ thermoelectric TEM studies can help to profoundly understand the interplay between the thermoelectric properties on one side and the structure/composition of the materials down to the atomic level on the other side, including grain boundaries, dopants or crystal defects. We propose an improved in situ TEM microchip design, which should facilitate a full quantitative measurement of the induced temperature gradient, the electrical and thermal conductivities, as well as the Seebeck coefficient.","sentences":["We explore the possibility to perform an in situ transmission electron microscopy (TEM) thermoelectric characterization of materials.","A differential heating element on a custom in situ TEM microchip allows to generate a temperature gradient across the studied materials, which are simultaneously measured electrically.","A thermovoltage was induced in all studied devices, whose sign corresponds to the sign of the Seebeck coefficient of the tested materials.","The results indicate that in situ thermoelectric TEM studies can help to profoundly understand the interplay between the thermoelectric properties on one side and the structure/composition of the materials down to the atomic level on the other side, including grain boundaries, dopants or crystal defects.","We propose an improved in situ TEM microchip design, which should facilitate a full quantitative measurement of the induced temperature gradient, the electrical and thermal conductivities, as well as the Seebeck coefficient."],"url":"http://arxiv.org/abs/2404.19366v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 08:48:39","title":"Nonlocal electrodynamics and the penetration depth of superconducting Sr$_2$RuO$_4$","abstract":"The thermal quasiparticles in a clean type-II superconductor with line nodes give rise to a quadratic low-temperature change of the penetration depth, $\\Delta \\lambda \\sim T^2$, as first shown by Kosztin and Leggett [I. Kosztin and A. J. Leggett, Phys. Rev. Lett. 79, 135 (1997)]. Here, we generalize this result to multiple nodes and compare it to numerically exact evaluations of the temperature-dependent penetration depth in Sr$_2$RuO$_4$ using a high-precision tight-binding model. We compare the calculations to recent penetration depth measurements in high purity single crystals of Sr$_2$RuO$_4$ [J. F. Landaeta et al., arXiv:2312.05129]. When assuming the order parameter to have $\\mathrm{B}_{1\\mathrm{g}}$ symmetry, we find that both a simple $d_{x^2-y^2}$-wave and complicated gap structures with contributions from higher harmonics and accidental nodes can accommodate the experimental data.","sentences":["The thermal quasiparticles in a clean type-II superconductor with line nodes give rise to a quadratic low-temperature change of the penetration depth, $\\Delta \\lambda \\sim T^2$, as first shown by Kosztin and Leggett [I. Kosztin and A. J. Leggett, Phys.","Rev. Lett.","79, 135 (1997)].","Here, we generalize this result to multiple nodes and compare it to numerically exact evaluations of the temperature-dependent penetration depth in Sr$_2$RuO$_4$ using a high-precision tight-binding model.","We compare the calculations to recent penetration depth measurements in high purity single crystals of Sr$_2$RuO$_4$","[J. F. Landaeta et al., arXiv:2312.05129].","When assuming the order parameter to have $\\mathrm{B}_{1\\mathrm{g}}$ symmetry, we find that both a simple $d_{x^2-y^2}$-wave and complicated gap structures with contributions from higher harmonics and accidental nodes can accommodate the experimental data."],"url":"http://arxiv.org/abs/2404.19365v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 08:47:24","title":"Expressivity and Speech Synthesis","abstract":"Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research. From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of. After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances. This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours. In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity. We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms.","sentences":["Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research.","From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of.","After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances.","This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours.","In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity.","We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms."],"url":"http://arxiv.org/abs/2404.19363v1","category":"cs.CL"}
{"created":"2024-04-30 08:45:18","title":"A Negotiator's Backup Plan: Optimal Concessions with a Reservation Value","abstract":"Automated negotiation is a well-known mechanism for autonomous agents to reach agreements. To realize beneficial agreements quickly, it is key to employ a good bidding strategy. When a negotiating agent has a good back-up plan, i.e., a high reservation value, failing to reach an agreement is not necessarily disadvantageous. Thus, the agent can adopt a risk-seeking strategy, aiming for outcomes with a higher utilities.   Accordingly, this paper develops an optimal bidding strategy called MIA-RVelous for bilateral negotiations with private reservation values. The proposed greedy algorithm finds the optimal bid sequence given the agent's beliefs about the opponent in $O(n^2D)$ time, with $D$ the maximum number of rounds and $n$ the number of outcomes. The results obtained here can pave the way to realizing effective concurrent negotiations, given that concurrent negotiations can serve as a (probabilistic) backup plan.","sentences":["Automated negotiation is a well-known mechanism for autonomous agents to reach agreements.","To realize beneficial agreements quickly, it is key to employ a good bidding strategy.","When a negotiating agent has a good back-up plan, i.e., a high reservation value, failing to reach an agreement is not necessarily disadvantageous.","Thus, the agent can adopt a risk-seeking strategy, aiming for outcomes with a higher utilities.   ","Accordingly, this paper develops an optimal bidding strategy called MIA-RVelous for bilateral negotiations with private reservation values.","The proposed greedy algorithm finds the optimal bid sequence given the agent's beliefs about the opponent in $O(n^2D)$ time, with $D$ the maximum number of rounds and $n$ the number of outcomes.","The results obtained here can pave the way to realizing effective concurrent negotiations, given that concurrent negotiations can serve as a (probabilistic) backup plan."],"url":"http://arxiv.org/abs/2404.19361v1","category":"cs.GT"}
{"created":"2024-04-30 08:41:06","title":"Evaluating Lexicon Incorporation for Depression Symptom Estimation","abstract":"This paper explores the impact of incorporating sentiment, emotion, and domain-specific lexicons into a transformer-based model for depression symptom estimation. Lexicon information is added by marking the words in the input transcripts of patient-therapist conversations as well as in social media posts. Overall results show that the introduction of external knowledge within pre-trained language models can be beneficial for prediction performance, while different lexicons show distinct behaviours depending on the targeted task. Additionally, new state-of-the-art results are obtained for the estimation of depression level over patient-therapist interviews.","sentences":["This paper explores the impact of incorporating sentiment, emotion, and domain-specific lexicons into a transformer-based model for depression symptom estimation.","Lexicon information is added by marking the words in the input transcripts of patient-therapist conversations as well as in social media posts.","Overall results show that the introduction of external knowledge within pre-trained language models can be beneficial for prediction performance, while different lexicons show distinct behaviours depending on the targeted task.","Additionally, new state-of-the-art results are obtained for the estimation of depression level over patient-therapist interviews."],"url":"http://arxiv.org/abs/2404.19359v1","category":"cs.CL"}
{"created":"2024-04-30 08:39:43","title":"QML-IB: Quantized Collaborative Intelligence between Multiple Devices and the Mobile Network","abstract":"The integration of artificial intelligence (AI) and mobile networks is regarded as one of the most important scenarios for 6G. In 6G, a major objective is to realize the efficient transmission of task-relevant data. Then a key problem arises, how to design collaborative AI models for the device side and the network side, so that the transmitted data between the device and the network is efficient enough, which means the transmission overhead is low but the AI task result is accurate. In this paper, we propose the multi-link information bottleneck (ML-IB) scheme for such collaborative models design. We formulate our problem based on a novel performance metric, which can evaluate both task accuracy and transmission overhead. Then we introduce a quantizer that is adjustable in the quantization bit depth, amplitudes, and breakpoints. Given the infeasibility of calculating our proposed metric on high-dimensional data, we establish a variational upper bound for this metric. However, due to the incorporation of quantization, the closed form of the variational upper bound remains uncomputable. Hence, we employ the Log-Sum Inequality to derive an approximation and provide a theoretical guarantee. Based on this, we devise the quantized multi-link information bottleneck (QML-IB) algorithm for collaborative AI models generation. Finally, numerical experiments demonstrate the superior performance of our QML-IB algorithm compared to the state-of-the-art algorithm.","sentences":["The integration of artificial intelligence (AI) and mobile networks is regarded as one of the most important scenarios for 6G.","In 6G, a major objective is to realize the efficient transmission of task-relevant data.","Then a key problem arises, how to design collaborative AI models for the device side and the network side, so that the transmitted data between the device and the network is efficient enough, which means the transmission overhead is low but the AI task result is accurate.","In this paper, we propose the multi-link information bottleneck (ML-IB) scheme for such collaborative models design.","We formulate our problem based on a novel performance metric, which can evaluate both task accuracy and transmission overhead.","Then we introduce a quantizer that is adjustable in the quantization bit depth, amplitudes, and breakpoints.","Given the infeasibility of calculating our proposed metric on high-dimensional data, we establish a variational upper bound for this metric.","However, due to the incorporation of quantization, the closed form of the variational upper bound remains uncomputable.","Hence, we employ the Log-Sum Inequality to derive an approximation and provide a theoretical guarantee.","Based on this, we devise the quantized multi-link information bottleneck (QML-IB) algorithm for collaborative AI models generation.","Finally, numerical experiments demonstrate the superior performance of our QML-IB algorithm compared to the state-of-the-art algorithm."],"url":"http://arxiv.org/abs/2404.19358v1","category":"cs.IT"}
{"created":"2024-04-30 08:37:25","title":"Statistical Theory of Neutron-Induced Nuclear Fission and of Heavy-Ion Fusion","abstract":"For both reactions we use an approach similar to that of compound-nucleus reaction theory. For neutron-induced fission, we describe the compound system generated by absorption of the neutron and the nuclear system near the scission point as two statistically independent systems governed by random-matrix theory. The systems are connected either by a barrier penetration factor or by a set of transition states above the barrier. Each system is coupled to a different set of channels. An analogous model is used for heavy-ion fusion. Assuming that (seen from the entrance channel) the system on the other side of the barrier is in the regime of strongly overlapping resonances, we obtain for fixed spin and parity closed-form analytical expressions for the total probability for fission and for fusion. Parts of these expressions can be calculated reliably within existing compound-nucleus reaction theory. The remaining parts are the probabilities for passage through or over the barrier. These may be determined theoretically from the liquid-drop model or experimentally from total fission or fusion cross sections.","sentences":["For both reactions we use an approach similar to that of compound-nucleus reaction theory.","For neutron-induced fission, we describe the compound system generated by absorption of the neutron and the nuclear system near the scission point as two statistically independent systems governed by random-matrix theory.","The systems are connected either by a barrier penetration factor or by a set of transition states above the barrier.","Each system is coupled to a different set of channels.","An analogous model is used for heavy-ion fusion.","Assuming that (seen from the entrance channel) the system on the other side of the barrier is in the regime of strongly overlapping resonances, we obtain for fixed spin and parity closed-form analytical expressions for the total probability for fission and for fusion.","Parts of these expressions can be calculated reliably within existing compound-nucleus reaction theory.","The remaining parts are the probabilities for passage through or over the barrier.","These may be determined theoretically from the liquid-drop model or experimentally from total fission or fusion cross sections."],"url":"http://arxiv.org/abs/2404.19355v1","category":"nucl-th"}
{"created":"2024-04-30 08:28:03","title":"Deep Learning Forecasts Caldera Collapse Events at K\u012blauea Volcano","abstract":"During the three month long eruption of K\\=ilauea volcano, Hawaii in 2018, the pre-existing summit caldera collapsed in over 60 quasi-periodic failure events. The last 40 of these events, which generated Mw >5 very long period (VLP) earthquakes, had inter-event times between 0.8 - 2.2 days. These failure events offer a unique dataset for testing methods for predicting earthquake recurrence based on locally recorded GPS, tilt, and seismicity data. In this work, we train a deep learning graph neural network (GNN) to predict the time-to-failure of the caldera collapse events using only a fraction of the data recorded at the start of each cycle. We find that the GNN generalizes to unseen data and can predict the time-to-failure to within a few hours using only 0.5 days of data, substantially improving upon a null model based only on inter-event statistics. Predictions improve with increasing input data length, and are most accurate when using high-SNR tilt-meter data. Applying the trained GNN to synthetic data with different magma pressure decay times predicts failure at a nearly constant stress threshold, revealing that the GNN is sensing the underling physics of caldera collapse. These findings demonstrate the predictability of caldera collapse sequences under well monitored conditions, and highlight the potential of machine learning methods for forecasting real world catastrophic events with limited training data.","sentences":["During the three month long eruption of K\\=ilauea volcano, Hawaii in 2018, the pre-existing summit caldera collapsed in over 60 quasi-periodic failure events.","The last 40 of these events, which generated Mw >5 very long period (VLP) earthquakes, had inter-event times between 0.8 - 2.2 days.","These failure events offer a unique dataset for testing methods for predicting earthquake recurrence based on locally recorded GPS, tilt, and seismicity data.","In this work, we train a deep learning graph neural network (GNN) to predict the time-to-failure of the caldera collapse events using only a fraction of the data recorded at the start of each cycle.","We find that the GNN generalizes to unseen data and can predict the time-to-failure to within a few hours using only 0.5 days of data, substantially improving upon a null model based only on inter-event statistics.","Predictions improve with increasing input data length, and are most accurate when using high-SNR tilt-meter data.","Applying the trained GNN to synthetic data with different magma pressure decay times predicts failure at a nearly constant stress threshold, revealing that the GNN is sensing the underling physics of caldera collapse.","These findings demonstrate the predictability of caldera collapse sequences under well monitored conditions, and highlight the potential of machine learning methods for forecasting real world catastrophic events with limited training data."],"url":"http://arxiv.org/abs/2404.19351v1","category":"physics.geo-ph"}
{"created":"2024-04-30 08:20:31","title":"Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization","abstract":"While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study.","sentences":["While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited.","We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications.","To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study."],"url":"http://arxiv.org/abs/2404.19349v1","category":"cs.RO"}
{"created":"2024-04-30 08:16:52","title":"Pessimistic Value Iteration for Multi-Task Data Sharing in Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) has shown promising results in learning a task-specific policy from a fixed dataset. However, successful offline RL often relies heavily on the coverage and quality of the given dataset. In scenarios where the dataset for a specific task is limited, a natural approach is to improve offline RL with datasets from other tasks, namely, to conduct Multi-Task Data Sharing (MTDS). Nevertheless, directly sharing datasets from other tasks exacerbates the distribution shift in offline RL. In this paper, we propose an uncertainty-based MTDS approach that shares the entire dataset without data selection. Given ensemble-based uncertainty quantification, we perform pessimistic value iteration on the shared offline dataset, which provides a unified framework for single- and multi-task offline RL. We further provide theoretical analysis, which shows that the optimality gap of our method is only related to the expected data coverage of the shared dataset, thus resolving the distribution shift issue in data sharing. Empirically, we release an MTDS benchmark and collect datasets from three challenging domains. The experimental results show our algorithm outperforms the previous state-of-the-art methods in challenging MTDS problems. See https://github.com/Baichenjia/UTDS for the datasets and code.","sentences":["Offline Reinforcement Learning (RL) has shown promising results in learning a task-specific policy from a fixed dataset.","However, successful offline RL often relies heavily on the coverage and quality of the given dataset.","In scenarios where the dataset for a specific task is limited, a natural approach is to improve offline RL with datasets from other tasks, namely, to conduct Multi-Task Data Sharing (MTDS).","Nevertheless, directly sharing datasets from other tasks exacerbates the distribution shift in offline RL.","In this paper, we propose an uncertainty-based MTDS approach that shares the entire dataset without data selection.","Given ensemble-based uncertainty quantification, we perform pessimistic value iteration on the shared offline dataset, which provides a unified framework for single- and multi-task offline RL.","We further provide theoretical analysis, which shows that the optimality gap of our method is only related to the expected data coverage of the shared dataset, thus resolving the distribution shift issue in data sharing.","Empirically, we release an MTDS benchmark and collect datasets from three challenging domains.","The experimental results show our algorithm outperforms the previous state-of-the-art methods in challenging MTDS problems.","See https://github.com/Baichenjia/UTDS for the datasets and code."],"url":"http://arxiv.org/abs/2404.19346v1","category":"cs.LG"}
{"created":"2024-04-30 08:11:14","title":"Connecting physics to systems with modular spin-circuits","abstract":"An emerging paradigm in modern electronics is that of CMOS + $\\sf X$ requiring the integration of standard CMOS technology with novel materials and technologies denoted by $\\sf X$. In this context, a crucial challenge is to develop accurate circuit models for $\\sf X$ that are compatible with standard models for CMOS-based circuits and systems. In this perspective we present physics-based, experimentally benchmarked modular circuit models that can be used to evaluate a class of CMOS + $\\sf X$ systems, where $\\sf X$ denotes magnetic and spintronic materials and phenomena. This class of materials is particularly challenging because they go beyond conventional charge-based phenomena and involve the spin degree of freedom which involves non-trivial quantum effects. Starting from density matrices $-$ the central quantity in quantum transport $-$ using well-defined approximations, it is possible to obtain spin-circuits that generalize ordinary circuit theory to 4-component currents and voltages (1 for charge and 3 for spin). With step-by-step examples that progressively go higher in the computing stack, we illustrate how the spin-circuit approach can be used to start from the physics of magnetism and spintronics to enable accurate system-level evaluations. We believe the core approach can be extended to include other quantum degrees of freedom like valley and pseudospins starting from corresponding density matrices.","sentences":["An emerging paradigm in modern electronics is that of CMOS + $\\sf X$ requiring the integration of standard CMOS technology with novel materials and technologies denoted by $\\sf X$. In this context, a crucial challenge is to develop accurate circuit models for $\\sf X$ that are compatible with standard models for CMOS-based circuits and systems.","In this perspective we present physics-based, experimentally benchmarked modular circuit models that can be used to evaluate a class of CMOS + $\\sf X$ systems, where $\\sf X$ denotes magnetic and spintronic materials and phenomena.","This class of materials is particularly challenging because they go beyond conventional charge-based phenomena and involve the spin degree of freedom which involves non-trivial quantum effects.","Starting from density matrices $-$ the central quantity in quantum transport $-$ using well-defined approximations, it is possible to obtain spin-circuits that generalize ordinary circuit theory to 4-component currents and voltages (1 for charge and 3 for spin).","With step-by-step examples that progressively go higher in the computing stack, we illustrate how the spin-circuit approach can be used to start from the physics of magnetism and spintronics to enable accurate system-level evaluations.","We believe the core approach can be extended to include other quantum degrees of freedom like valley and pseudospins starting from corresponding density matrices."],"url":"http://arxiv.org/abs/2404.19345v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 08:06:04","title":"Reliable or Deceptive? Investigating Gated Features for Smooth Visual Explanations in CNNs","abstract":"Deep learning models have achieved remarkable success across diverse domains. However, the intricate nature of these models often impedes a clear understanding of their decision-making processes. This is where Explainable AI (XAI) becomes indispensable, offering intuitive explanations for model decisions. In this work, we propose a simple yet highly effective approach, ScoreCAM++, which introduces modifications to enhance the promising ScoreCAM method for visual explainability. Our proposed approach involves altering the normalization function within the activation layer utilized in ScoreCAM, resulting in significantly improved results compared to previous efforts. Additionally, we apply an activation function to the upsampled activation layers to enhance interpretability. This improvement is achieved by selectively gating lower-priority values within the activation layer. Through extensive experiments and qualitative comparisons, we demonstrate that ScoreCAM++ consistently achieves notably superior performance and fairness in interpreting the decision-making process compared to both ScoreCAM and previous methods.","sentences":["Deep learning models have achieved remarkable success across diverse domains.","However, the intricate nature of these models often impedes a clear understanding of their decision-making processes.","This is where Explainable AI (XAI) becomes indispensable, offering intuitive explanations for model decisions.","In this work, we propose a simple yet highly effective approach, ScoreCAM++, which introduces modifications to enhance the promising ScoreCAM method for visual explainability.","Our proposed approach involves altering the normalization function within the activation layer utilized in ScoreCAM, resulting in significantly improved results compared to previous efforts.","Additionally, we apply an activation function to the upsampled activation layers to enhance interpretability.","This improvement is achieved by selectively gating lower-priority values within the activation layer.","Through extensive experiments and qualitative comparisons, we demonstrate that ScoreCAM++ consistently achieves notably superior performance and fairness in interpreting the decision-making process compared to both ScoreCAM and previous methods."],"url":"http://arxiv.org/abs/2404.19341v1","category":"cs.CV"}
{"created":"2024-04-30 08:04:28","title":"NMSSM Explanation for Excesses in the Search for Neutralinos and Charginos and a 95 GeV Higgs Boson","abstract":"The observed excesses in the search for neutralinos and charginos by ATLAS and CMS can be fitted simultaneously in the minimal supersymmetric standard model (MSSM) assuming a light higgsino mass, of magnitude less than about 250 GeV, and a compressed higgsino dominated neutralino and chargino spectrum, with $5-10\\%$ mass splittings. However, light higgsinos as dark matter would have far too large direct detection cross sections. We consider the next-to-MSSM (NMSSM) with an additional singlino-like lightest supersymmetric particle (LSP) a few GeV below the next-to-lightest supersymmetric particle (NLSP). Sparticles prefer to decay first into the NLSP and remnants from the final decay into the LSP are too soft to contribute to the observed signals. Co-annihilation in the higgsino-sector can generate a relic density in the WMAP/Planck window. The singlino-like LSP has automatically a direct detection cross section below present and future sensitivities: a direct detection signal in the near future would exclude this scenario. The singlet-like Higgs scalar of the NMSSM can have a mass around 95 GeV and signal cross sections in the $b\\bar{b}$ channel at LEP and in the $\\gamma\\gamma$ channel at the LHC compatible with the respective observations.","sentences":["The observed excesses in the search for neutralinos and charginos by ATLAS and CMS can be fitted simultaneously in the minimal supersymmetric standard model (MSSM) assuming a light higgsino mass, of magnitude less than about 250 GeV, and a compressed higgsino dominated neutralino and chargino spectrum, with $5-10\\%$ mass splittings.","However, light higgsinos as dark matter would have far too large direct detection cross sections.","We consider the next-to-MSSM (NMSSM) with an additional singlino-like lightest supersymmetric particle (LSP) a few GeV below the next-to-lightest supersymmetric particle (NLSP).","Sparticles prefer to decay first into the NLSP and remnants from the final decay into the LSP are too soft to contribute to the observed signals.","Co-annihilation in the higgsino-sector can generate a relic density in the WMAP/Planck window.","The singlino-like LSP has automatically a direct detection cross section below present and future sensitivities: a direct detection signal in the near future would exclude this scenario.","The singlet-like Higgs scalar of the NMSSM can have a mass around 95 GeV and signal cross sections in the $b\\bar{b}$ channel at LEP and in the $\\gamma\\gamma$ channel at the LHC compatible with the respective observations."],"url":"http://arxiv.org/abs/2404.19338v1","category":"hep-ph"}
{"created":"2024-04-30 08:03:22","title":"Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts","abstract":"LLMs trained in the understanding of programming syntax are now providing effective assistance to developers and are being used in programming education such as in generation of coding problem examples or providing code explanations. A key aspect of programming education is understanding and dealing with error message. However, 'logical errors' in which the program operates against the programmer's intentions do not receive error messages from the compiler. In this study, building on existing research on programming errors, we first define the types of logical errors that can occur in programming in general. Based on the definition, we propose an effective approach for detecting logical errors with LLMs that makes use of relations among error types in the Chain-of-Thought and Tree-of-Thought prompts. The experimental results indicate that when such logical error descriptions in the prompt are used, the average classifition performance is about 21% higher than the ones without them. We also conducted an experiment for exploiting the relations among errors in generating a new logical error dataset using LLMs. As there is very limited dataset for logical errors such benchmark dataset can be very useful for various programming related applications. We expect that our work can assist novice programmers in identifying the causes of code errors and correct them more effectively.","sentences":["LLMs trained in the understanding of programming syntax are now providing effective assistance to developers and are being used in programming education such as in generation of coding problem examples or providing code explanations.","A key aspect of programming education is understanding and dealing with error message.","However, 'logical errors' in which the program operates against the programmer's intentions do not receive error messages from the compiler.","In this study, building on existing research on programming errors, we first define the types of logical errors that can occur in programming in general.","Based on the definition, we propose an effective approach for detecting logical errors with LLMs that makes use of relations among error types in the Chain-of-Thought and Tree-of-Thought prompts.","The experimental results indicate that when such logical error descriptions in the prompt are used, the average classifition performance is about 21% higher than the ones without them.","We also conducted an experiment for exploiting the relations among errors in generating a new logical error dataset using LLMs.","As there is very limited dataset for logical errors such benchmark dataset can be very useful for various programming related applications.","We expect that our work can assist novice programmers in identifying the causes of code errors and correct them more effectively."],"url":"http://arxiv.org/abs/2404.19336v2","category":"cs.AI"}
{"created":"2024-04-30 07:53:34","title":"G2LTraj: A Global-to-Local Generation Approach for Trajectory Prediction","abstract":"Predicting future trajectories of traffic agents accurately holds substantial importance in various applications such as autonomous driving. Previous methods commonly infer all future steps of an agent either recursively or simultaneously. However, the recursive strategy suffers from the accumulated error, while the simultaneous strategy overlooks the constraints among future steps, resulting in kinematically infeasible predictions. To address these issues, in this paper, we propose G2LTraj, a plug-and-play global-to-local generation approach for trajectory prediction. Specifically, we generate a series of global key steps that uniformly cover the entire future time range. Subsequently, the local intermediate steps between the adjacent key steps are recursively filled in. In this way, we prevent the accumulated error from propagating beyond the adjacent key steps. Moreover, to boost the kinematical feasibility, we not only introduce the spatial constraints among key steps but also strengthen the temporal constraints among the intermediate steps. Finally, to ensure the optimal granularity of key steps, we design a selectable granularity strategy that caters to each predicted trajectory. Our G2LTraj significantly improves the performance of seven existing trajectory predictors across the ETH, UCY and nuScenes datasets. Experimental results demonstrate its effectiveness. Code will be available at https://github.com/Zhanwei-Z/G2LTraj.","sentences":["Predicting future trajectories of traffic agents accurately holds substantial importance in various applications such as autonomous driving.","Previous methods commonly infer all future steps of an agent either recursively or simultaneously.","However, the recursive strategy suffers from the accumulated error, while the simultaneous strategy overlooks the constraints among future steps, resulting in kinematically infeasible predictions.","To address these issues, in this paper, we propose G2LTraj, a plug-and-play global-to-local generation approach for trajectory prediction.","Specifically, we generate a series of global key steps that uniformly cover the entire future time range.","Subsequently, the local intermediate steps between the adjacent key steps are recursively filled in.","In this way, we prevent the accumulated error from propagating beyond the adjacent key steps.","Moreover, to boost the kinematical feasibility, we not only introduce the spatial constraints among key steps but also strengthen the temporal constraints among the intermediate steps.","Finally, to ensure the optimal granularity of key steps, we design a selectable granularity strategy that caters to each predicted trajectory.","Our G2LTraj significantly improves the performance of seven existing trajectory predictors across the ETH, UCY and nuScenes datasets.","Experimental results demonstrate its effectiveness.","Code will be available at https://github.com/Zhanwei-Z/G2LTraj."],"url":"http://arxiv.org/abs/2404.19330v1","category":"cs.CV"}
{"created":"2024-04-30 07:52:26","title":"Computational Approaches for Integrating out Subjectivity in Cognate Synonym Selection","abstract":"Working with cognate data involves handling synonyms, that is, multiple words that describe the same concept in a language. In the early days of language phylogenetics it was recommended to select one synonym only. However, as we show here, binary character matrices, which are used as input for computational methods, do allow for representing the entire dataset including all synonyms. Here we address the question how one can and if one should include all synonyms or whether it is preferable to select synonyms a priori. To this end, we perform maximum likelihood tree inferences with the widely used RAxML-NG tool and show that it yields plausible trees when all synonyms are used as input. Furthermore, we show that a priori synonym selection can yield topologically substantially different trees and we therefore advise against doing so. To represent cognate data including all synonyms, we introduce two types of character matrices beyond the standard binary ones: probabilistic binary and probabilistic multi-valued character matrices. We further show that it is dataset-dependent for which character matrix type the inferred RAxML-NG tree is topologically closest to the gold standard. We also make available a Python interface for generating all of the above character matrix types for cognate data provided in CLDF format.","sentences":["Working with cognate data involves handling synonyms, that is, multiple words that describe the same concept in a language.","In the early days of language phylogenetics it was recommended to select one synonym only.","However, as we show here, binary character matrices, which are used as input for computational methods, do allow for representing the entire dataset including all synonyms.","Here we address the question how one can and if one should include all synonyms or whether it is preferable to select synonyms a priori.","To this end, we perform maximum likelihood tree inferences with the widely used RAxML-NG tool and show that it yields plausible trees when all synonyms are used as input.","Furthermore, we show that a priori synonym selection can yield topologically substantially different trees and we therefore advise against doing so.","To represent cognate data including all synonyms, we introduce two types of character matrices beyond the standard binary ones: probabilistic binary and probabilistic multi-valued character matrices.","We further show that it is dataset-dependent for which character matrix type the inferred RAxML-NG tree is topologically closest to the gold standard.","We also make available a Python interface for generating all of the above character matrix types for cognate data provided in CLDF format."],"url":"http://arxiv.org/abs/2404.19328v1","category":"cs.CL"}
{"created":"2024-04-30 07:51:43","title":"Assessment of physical schemes for WRF model in convection-permitting mode over southern Iberian Peninsula","abstract":"Convection-permitting models (CPMs) enable the representation of meteorological variables at horizontal high resolution spatial scales (higher than 4 km), where convection plays a significant role. Physical schemes need to be evaluated considering factors in the studied region such as orography and climate variability. This study investigates the sensitivity of the WRF model as CPM to the use of different physics schemes on Andalusia, a complex orography region in southern Iberian Peninsula (IP). A set of 1-year WRF simulations was completed based on two one-way nested domains: the parent domain (d01) spanning the entire IP with 5 km spatial resolution and the nested domain (d02) for the region of Andalusia at 1 km of spatial resolution. 12 physic schemes were examined from combinations of microphysics (MP) schemes including THOMPSON, WRF single moment 6-class (WSM6), and WRF single moment 7-class (WSM7), and different options for the convection in d01, the Grell 3D (G3), Grell-Freitas (GF), Kain-Fritsch (KF), and deactivated cumulus parameterization (OFF). The simulated precipitation and 2-m temperature for the year 2018, a very wet year, were compared with observational datasets to determine the optimal WRF configuration, including point-to-point and station-point comparisons at different time aggregations. In general, greater differences were shown when comparing the results of convection schemes in d01. Simulations completed with GF or OFF presented better performance compared to the reference datasets. Concerning the MP, although THOMPSON showed a better fit in high mountain areas, it generally presents a worse agreement with the reference datasets. In terms of temperature, the results were very similar and, therefore, the selection of the best configuration was based mainly on the precipitation results with the WSM7-GF scheme being suitable for Andalusia region.","sentences":["Convection-permitting models (CPMs) enable the representation of meteorological variables at horizontal high resolution spatial scales (higher than 4 km), where convection plays a significant role.","Physical schemes need to be evaluated considering factors in the studied region such as orography and climate variability.","This study investigates the sensitivity of the WRF model as CPM to the use of different physics schemes on Andalusia, a complex orography region in southern Iberian Peninsula (IP).","A set of 1-year WRF simulations was completed based on two one-way nested domains: the parent domain (d01) spanning the entire IP with 5 km spatial resolution and the nested domain (d02) for the region of Andalusia at 1 km of spatial resolution.","12 physic schemes were examined from combinations of microphysics (MP) schemes including THOMPSON, WRF single moment 6-class (WSM6), and WRF single moment 7-class (WSM7), and different options for the convection in d01, the Grell 3D (G3), Grell-Freitas (GF), Kain-Fritsch (KF), and deactivated cumulus parameterization (OFF).","The simulated precipitation and 2-m temperature for the year 2018, a very wet year, were compared with observational datasets to determine the optimal WRF configuration, including point-to-point and station-point comparisons at different time aggregations.","In general, greater differences were shown when comparing the results of convection schemes in d01.","Simulations completed with GF or OFF presented better performance compared to the reference datasets.","Concerning the MP, although THOMPSON showed a better fit in high mountain areas, it generally presents a worse agreement with the reference datasets.","In terms of temperature, the results were very similar and, therefore, the selection of the best configuration was based mainly on the precipitation results with the WSM7-GF scheme being suitable for Andalusia region."],"url":"http://arxiv.org/abs/2404.19327v1","category":"physics.ao-ph"}
{"created":"2024-04-30 07:47:26","title":"The Effect of Data Types' on the Performance of Machine Learning Algorithms for Financial Prediction","abstract":"Forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits. A small improvement in forecasting performance can lead to increased profitability; therefore, obtaining a realistic forecast is very important for investors. Successful forecasting provides traders with effective buy-or-hold strategies, allowing them to make more profits. The most important thing in this process is to produce accurate forecasts suitable for real-life applications. Bitcoin, frequently mentioned recently due to its volatility and chaotic behavior, has begun to pay great attention and has become an investment tool, especially during and after the COVID-19 pandemic. This study provided a comprehensive methodology, including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning (ML) algorithms to forecast Bitcoin price movement. A binarization procedure was applied using continuous data to construct the trend data representing each input feature trend. Following the related literature, the input features are determined as technical indicators, google trends, and the number of tweets. Random forest (RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB), Support vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks (ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected features for prediction purposes. This work investigates two main research questions: i. How does the sample size affect the prediction performance of ML algorithms? ii. How does the data type affect the prediction performance of ML algorithms? Accuracy and area under the ROC curve (AUC) values were used to compare the model performance. A t-test was performed to test the statistical significance of the prediction results.","sentences":["Forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits.","A small improvement in forecasting performance can lead to increased profitability; therefore, obtaining a realistic forecast is very important for investors.","Successful forecasting provides traders with effective buy-or-hold strategies, allowing them to make more profits.","The most important thing in this process is to produce accurate forecasts suitable for real-life applications.","Bitcoin, frequently mentioned recently due to its volatility and chaotic behavior, has begun to pay great attention and has become an investment tool, especially during and after the COVID-19 pandemic.","This study provided a comprehensive methodology, including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning (ML) algorithms to forecast Bitcoin price movement.","A binarization procedure was applied using continuous data to construct the trend data representing each input feature trend.","Following the related literature, the input features are determined as technical indicators, google trends, and the number of tweets.","Random forest (RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB), Support vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks (ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected features for prediction purposes.","This work investigates two main research questions:","i. How does the sample size affect the prediction performance of ML algorithms?","ii.","How does the data type affect the prediction performance of ML algorithms?","Accuracy and area under the ROC curve (AUC) values were used to compare the model performance.","A t-test was performed to test the statistical significance of the prediction results."],"url":"http://arxiv.org/abs/2404.19324v1","category":"q-fin.CP"}
{"created":"2024-04-30 07:45:59","title":"Neutrinos in Cosmology","abstract":"Neutrinos are the least known particle in the Standard Model of elementary particle physics. They play a crucial role in cosmology, governing the universe's evolution and shaping the large-scale structures we observe today. In this chapter, we review crucial topics in neutrino cosmology, such as the neutrino decoupling process in the very early universe. We shall also revisit the current constraints on the number of effective relativistic degrees of freedom and the departures from its standard expectation of 3. Neutrino masses represent the very first departure from the Standard Model of elementary particle physics and may imply the existence of new unexplored mass generation mechanisms. Cosmology provides the tightest bound on the sum of neutrino masses, and we shall carefully present the nature of these constraints, both on the total mass of the neutrinos and on their precise spectrum. The ordering of the neutrino masses plays a major role in the design of future neutrino mass searches from laboratory experiments, such as neutrinoless double beta decay probes. Finally, we shall also present the futuristic perspectives for an eventual direct detection of cosmic, relic neutrinos.","sentences":["Neutrinos are the least known particle in the Standard Model of elementary particle physics.","They play a crucial role in cosmology, governing the universe's evolution and shaping the large-scale structures we observe today.","In this chapter, we review crucial topics in neutrino cosmology, such as the neutrino decoupling process in the very early universe.","We shall also revisit the current constraints on the number of effective relativistic degrees of freedom and the departures from its standard expectation of 3.","Neutrino masses represent the very first departure from the Standard Model of elementary particle physics and may imply the existence of new unexplored mass generation mechanisms.","Cosmology provides the tightest bound on the sum of neutrino masses, and we shall carefully present the nature of these constraints, both on the total mass of the neutrinos and on their precise spectrum.","The ordering of the neutrino masses plays a major role in the design of future neutrino mass searches from laboratory experiments, such as neutrinoless double beta decay probes.","Finally, we shall also present the futuristic perspectives for an eventual direct detection of cosmic, relic neutrinos."],"url":"http://arxiv.org/abs/2404.19322v1","category":"astro-ph.CO"}
{"created":"2024-04-30 07:38:08","title":"Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores","abstract":"A good summary can often be very useful during program comprehension. While a brief, fluent, and relevant summary can be helpful, it does require significant human effort to produce. Often, good summaries are unavailable in software projects, thus making maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies.   However, LLMs often err and generate something quite unlike what a human might say. Given an LLM-produced code summary, is there a way to gauge whether it's likely to be sufficiently similar to a human produced summary, or not? In this paper, we study this question, as a calibration problem: given a summary from an LLM, can we compute a confidence measure, which is a good indication of whether the summary is sufficiently similar to what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. We suggest an approach which provides well-calibrated predictions of likelihood of similarity to human summaries.","sentences":["A good summary can often be very useful during program comprehension.","While a brief, fluent, and relevant summary can be helpful, it does require significant human effort to produce.","Often, good summaries are unavailable in software projects, thus making maintenance more difficult.","There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced.","Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies.   ","However, LLMs often err and generate something quite unlike what a human might say.","Given an LLM-produced code summary, is there a way to gauge whether it's likely to be sufficiently similar to a human produced summary, or not?","In this paper, we study this question, as a calibration problem: given a summary from an LLM, can we compute a confidence measure, which is a good indication of whether the summary is sufficiently similar to what a human would have produced in this situation?","We examine this question using several LLMs, for several languages, and in several different settings.","We suggest an approach which provides well-calibrated predictions of likelihood of similarity to human summaries."],"url":"http://arxiv.org/abs/2404.19318v1","category":"cs.SE"}
{"created":"2024-04-30 07:32:17","title":"Revealing the working mechanism of quantum neural networks by mutual information","abstract":"Quantum neural networks (QNNs) is a parameterized quantum circuit model, which can be trained by gradient-based optimizer, can be used for supervised learning, regression tasks, combinatorial optimization, etc. Although many works have demonstrated that QNNs have better learnability, generalizability, etc. compared to classical neural networks. However, as with classical neural networks, we still can't explain their working mechanism well. In this paper, we reveal the training mechanism of QNNs by mutual information. Unlike traditional mutual information in neural networks, due to quantum computing remains information conserved, the mutual information is trivial of the input and output of U operator. In our work, in order to observe the change of mutual information during training, we divide the quantum circuit (U operator) into two subsystems, discard subsystem (D) and measurement subsystem (M) respectively. We calculate two mutual information, I(Di : Mo) and I(Mi : Mo) (i and o means input or output of the corresponding subsystem), and observe their behavior during training. As the epochs increases, I(Di : Mo) gradually increases, this may means some information of discard subsystem is continuously pushed into the measurement subsystem, the information should be label-related. What's more, I(Mi : Mo) exist two-phase behavior in training process, this consistent with the information bottleneck anticipation. The first phase, I(Mi : Mo) is increasing, this means the measurement subsystem perform feature fitting. The second phase, I(Mi : Mo) is decreasing, this may means the system is generalizing, the measurement subsystem discard label-irrelevant information into the discard subsystem as many as possible. Our work discussed the working mechanism of QNNs by mutual information, further, it can be used to analyze the accuracy and generalization of QNNs.","sentences":["Quantum neural networks (QNNs) is a parameterized quantum circuit model, which can be trained by gradient-based optimizer, can be used for supervised learning, regression tasks, combinatorial optimization, etc.","Although many works have demonstrated that QNNs have better learnability, generalizability, etc. compared to classical neural networks.","However, as with classical neural networks, we still can't explain their working mechanism well.","In this paper, we reveal the training mechanism of QNNs by mutual information.","Unlike traditional mutual information in neural networks, due to quantum computing remains information conserved, the mutual information is trivial of the input and output of U operator.","In our work, in order to observe the change of mutual information during training, we divide the quantum circuit (U operator) into two subsystems, discard subsystem (D) and measurement subsystem (M) respectively.","We calculate two mutual information, I(Di : Mo) and I(Mi : Mo) (i and o means input or output of the corresponding subsystem), and observe their behavior during training.","As the epochs increases, I(Di : Mo) gradually increases, this may means some information of discard subsystem is continuously pushed into the measurement subsystem, the information should be label-related.","What's more, I(Mi : Mo) exist two-phase behavior in training process, this consistent with the information bottleneck anticipation.","The first phase, I(Mi : Mo) is increasing, this means the measurement subsystem perform feature fitting.","The second phase, I(Mi : Mo) is decreasing, this may means the system is generalizing, the measurement subsystem discard label-irrelevant information into the discard subsystem as many as possible.","Our work discussed the working mechanism of QNNs by mutual information, further, it can be used to analyze the accuracy and generalization of QNNs."],"url":"http://arxiv.org/abs/2404.19312v1","category":"quant-ph"}
{"created":"2024-04-30 07:30:33","title":"A Light-weight Transformer-based Self-supervised Matching Network for Heterogeneous Images","abstract":"Matching visible and near-infrared (NIR) images remains a significant challenge in remote sensing image fusion. The nonlinear radiometric differences between heterogeneous remote sensing images make the image matching task even more difficult. Deep learning has gained substantial attention in computer vision tasks in recent years. However, many methods rely on supervised learning and necessitate large amounts of annotated data. Nevertheless, annotated data is frequently limited in the field of remote sensing image matching. To address this challenge, this paper proposes a novel keypoint descriptor approach that obtains robust feature descriptors via a self-supervised matching network. A light-weight transformer network, termed as LTFormer, is designed to generate deep-level feature descriptors. Furthermore, we implement an innovative triplet loss function, LT Loss, to enhance the matching performance further. Our approach outperforms conventional hand-crafted local feature descriptors and proves equally competitive compared to state-of-the-art deep learning-based methods, even amidst the shortage of annotated data.","sentences":["Matching visible and near-infrared (NIR) images remains a significant challenge in remote sensing image fusion.","The nonlinear radiometric differences between heterogeneous remote sensing images make the image matching task even more difficult.","Deep learning has gained substantial attention in computer vision tasks in recent years.","However, many methods rely on supervised learning and necessitate large amounts of annotated data.","Nevertheless, annotated data is frequently limited in the field of remote sensing image matching.","To address this challenge, this paper proposes a novel keypoint descriptor approach that obtains robust feature descriptors via a self-supervised matching network.","A light-weight transformer network, termed as LTFormer, is designed to generate deep-level feature descriptors.","Furthermore, we implement an innovative triplet loss function, LT Loss, to enhance the matching performance further.","Our approach outperforms conventional hand-crafted local feature descriptors and proves equally competitive compared to state-of-the-art deep learning-based methods, even amidst the shortage of annotated data."],"url":"http://arxiv.org/abs/2404.19311v1","category":"cs.CV"}
{"created":"2024-04-30 07:18:10","title":"Comprehensive Forecasting-Based Analysis of Hybrid and Stacked Stateful/ Stateless Models","abstract":"Wind speed is a powerful source of renewable energy, which can be used as an alternative to the non-renewable resources for production of electricity. Renewable sources are clean, infinite and do not impact the environment negatively during production of electrical energy. However, while eliciting electrical energy from renewable resources viz. solar irradiance, wind speed, hydro should require special planning failing which may result in huge loss of labour and money for setting up the system. In this paper, we discuss four deep recurrent neural networks viz. Stacked Stateless LSTM, Stacked Stateless GRU, Stacked Stateful LSTM and Statcked Stateful GRU which will be used to predict wind speed on a short-term basis for the airport sites beside two campuses of Mississippi State University. The paper does a comprehensive analysis of the performance of the models used describing their architectures and how efficiently they elicit the results with the help of RMSE values. A detailed description of the time and space complexities of the above models has also been discussed.","sentences":["Wind speed is a powerful source of renewable energy, which can be used as an alternative to the non-renewable resources for production of electricity.","Renewable sources are clean, infinite and do not impact the environment negatively during production of electrical energy.","However, while eliciting electrical energy from renewable resources viz.","solar irradiance, wind speed, hydro should require special planning failing which may result in huge loss of labour and money for setting up the system.","In this paper, we discuss four deep recurrent neural networks viz.","Stacked Stateless LSTM, Stacked Stateless GRU, Stacked Stateful LSTM and Statcked Stateful GRU which will be used to predict wind speed on a short-term basis for the airport sites beside two campuses of Mississippi State University.","The paper does a comprehensive analysis of the performance of the models used describing their architectures and how efficiently they elicit the results with the help of RMSE values.","A detailed description of the time and space complexities of the above models has also been discussed."],"url":"http://arxiv.org/abs/2404.19306v1","category":"cs.LG"}
{"created":"2024-04-30 07:09:47","title":"Boosting generation rate of squeezed single-photon states by generalized photon subtraction","abstract":"In optical quantum information processing with continuous variables, optical non-Gaussian quantum states are essential for universal and fault-tolerant quantum computation. Experimentally, their most typical generation method is photon subtraction (PS) where single-photon detection by an on/off detector probabilistically heralds the generation of squeezed single-photon states. In PS, however, trying to avoid unwanted multi-photon detection inevitably limits the generation rate, hindering the application of squeezed single-photon states. Here, we theoretically show that generalized photon subtraction (GPS), a simple extension of PS, can improve the generation rate while maintaining the quality of the generated states. Furthermore, we experimentally demonstrate the generation rate improvement for 2 dB- and 4 dB-squeezed single-photon states compared to PS, by more than one order of magnitude particularly for the case of 2 dB. Our results will accelerate the application of squeezed single-photon states to more advanced quantum information protocols.","sentences":["In optical quantum information processing with continuous variables, optical non-Gaussian quantum states are essential for universal and fault-tolerant quantum computation.","Experimentally, their most typical generation method is photon subtraction (PS) where single-photon detection by an on/off detector probabilistically heralds the generation of squeezed single-photon states.","In PS, however, trying to avoid unwanted multi-photon detection inevitably limits the generation rate, hindering the application of squeezed single-photon states.","Here, we theoretically show that generalized photon subtraction (GPS), a simple extension of PS, can improve the generation rate while maintaining the quality of the generated states.","Furthermore, we experimentally demonstrate the generation rate improvement for 2 dB- and 4 dB-squeezed single-photon states compared to PS, by more than one order of magnitude particularly for the case of 2 dB. Our results will accelerate the application of squeezed single-photon states to more advanced quantum information protocols."],"url":"http://arxiv.org/abs/2404.19304v1","category":"quant-ph"}
{"created":"2024-04-30 07:07:45","title":"Data Set Terminology of Artificial Intelligence in Medicine: A Historical Review and Recommendation","abstract":"Medicine and artificial intelligence (AI) engineering represent two distinct fields each with decades of published history. With such history comes a set of terminology that has a specific way in which it is applied. However, when two distinct fields with overlapping terminology start to collaborate, miscommunication and misunderstandings can occur. This narrative review aims to give historical context for these terms, accentuate the importance of clarity when these terms are used in medical AI contexts, and offer solutions to mitigate misunderstandings by readers from either field. Through an examination of historical documents, including articles, writing guidelines, and textbooks, this review traces the divergent evolution of terms for data sets and their impact. Initially, the discordant interpretations of the word 'validation' in medical and AI contexts are explored. Then the data sets used for AI evaluation are classified, namely random splitting, cross-validation, temporal, geographic, internal, and external sets. The accurate and standardized description of these data sets is crucial for demonstrating the robustness and generalizability of AI applications in medicine. This review clarifies existing literature to provide a comprehensive understanding of these classifications and their implications in AI evaluation. This review then identifies often misunderstood terms and proposes pragmatic solutions to mitigate terminological confusion. Among these solutions are the use of standardized terminology such as 'training set,' 'validation (or tuning) set,' and 'test set,' and explicit definition of data set splitting terminologies in each medical AI research publication. This review aspires to enhance the precision of communication in medical AI, thereby fostering more effective and transparent research methodologies in this interdisciplinary field.","sentences":["Medicine and artificial intelligence (AI) engineering represent two distinct fields each with decades of published history.","With such history comes a set of terminology that has a specific way in which it is applied.","However, when two distinct fields with overlapping terminology start to collaborate, miscommunication and misunderstandings can occur.","This narrative review aims to give historical context for these terms, accentuate the importance of clarity when these terms are used in medical AI contexts, and offer solutions to mitigate misunderstandings by readers from either field.","Through an examination of historical documents, including articles, writing guidelines, and textbooks, this review traces the divergent evolution of terms for data sets and their impact.","Initially, the discordant interpretations of the word 'validation' in medical and AI contexts are explored.","Then the data sets used for AI evaluation are classified, namely random splitting, cross-validation, temporal, geographic, internal, and external sets.","The accurate and standardized description of these data sets is crucial for demonstrating the robustness and generalizability of AI applications in medicine.","This review clarifies existing literature to provide a comprehensive understanding of these classifications and their implications in AI evaluation.","This review then identifies often misunderstood terms and proposes pragmatic solutions to mitigate terminological confusion.","Among these solutions are the use of standardized terminology such as 'training set,' 'validation (or tuning) set,' and 'test set,' and explicit definition of data set splitting terminologies in each medical AI research publication.","This review aspires to enhance the precision of communication in medical AI, thereby fostering more effective and transparent research methodologies in this interdisciplinary field."],"url":"http://arxiv.org/abs/2404.19303v1","category":"cs.AI"}
{"created":"2024-04-30 07:01:05","title":"Robust Pedestrian Detection via Constructing Versatile Pedestrian Knowledge Bank","abstract":"Pedestrian detection is a crucial field of computer vision research which can be adopted in various real-world applications (e.g., self-driving systems). However, despite noticeable evolution of pedestrian detection, pedestrian representations learned within a detection framework are usually limited to particular scene data in which they were trained. Therefore, in this paper, we propose a novel approach to construct versatile pedestrian knowledge bank containing representative pedestrian knowledge which can be applicable to various detection frameworks and adopted in diverse scenes. We extract generalized pedestrian knowledge from a large-scale pretrained model, and we curate them by quantizing most representative features and guiding them to be distinguishable from background scenes. Finally, we construct versatile pedestrian knowledge bank which is composed of such representations, and then we leverage it to complement and enhance pedestrian features within a pedestrian detection framework. Through comprehensive experiments, we validate the effectiveness of our method, demonstrating its versatility and outperforming state-of-the-art detection performances.","sentences":["Pedestrian detection is a crucial field of computer vision research which can be adopted in various real-world applications (e.g., self-driving systems).","However, despite noticeable evolution of pedestrian detection, pedestrian representations learned within a detection framework are usually limited to particular scene data in which they were trained.","Therefore, in this paper, we propose a novel approach to construct versatile pedestrian knowledge bank containing representative pedestrian knowledge which can be applicable to various detection frameworks and adopted in diverse scenes.","We extract generalized pedestrian knowledge from a large-scale pretrained model, and we curate them by quantizing most representative features and guiding them to be distinguishable from background scenes.","Finally, we construct versatile pedestrian knowledge bank which is composed of such representations, and then we leverage it to complement and enhance pedestrian features within a pedestrian detection framework.","Through comprehensive experiments, we validate the effectiveness of our method, demonstrating its versatility and outperforming state-of-the-art detection performances."],"url":"http://arxiv.org/abs/2404.19299v1","category":"cs.CV"}
{"created":"2024-04-30 06:55:45","title":"Octopus v4: Graph of language models","abstract":"Language models have been effective in a wide range of applications, yet the most sophisticated models are often proprietary. For example, GPT-4 by OpenAI and various models by Anthropic are expensive and consume substantial energy. In contrast, the open-source community has produced competitive models, like Llama3. Furthermore, niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts. This paper introduces a novel approach that employs \\textit{functional tokens} to integrate \\textbf{multiple open-source models}, each optimized for particular tasks. Our newly developed Octopus v4 model leverages \\textit{functional tokens} to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance. Octopus v4, an evolution of the Octopus v1, v2, and v3 models, excels in selection and parameter understanding and reformatting. Additionally, we explore the use of graph as a versatile data structure that effectively coordinates multiple open-source models by harnessing the capabilities of the Octopus model and \\textit{functional tokens}. Use our open-sourced GitHub (\\url{https://www.nexa4ai.com/}) to try Octopus v4 models (\\url{https://huggingface.co/NexaAIDev/Octopus-v4}), and contrite to a larger graph of language models. By activating models less than 10B parameters, we achieved SOTA MMLU score of 74.8 among the same level models.","sentences":["Language models have been effective in a wide range of applications, yet the most sophisticated models are often proprietary.","For example, GPT-4 by OpenAI and various models by Anthropic are expensive and consume substantial energy.","In contrast, the open-source community has produced competitive models, like Llama3.","Furthermore, niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts.","This paper introduces a novel approach that employs \\textit{functional tokens} to integrate \\textbf{multiple open-source models}, each optimized for particular tasks.","Our newly developed Octopus v4 model leverages \\textit{functional tokens} to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance.","Octopus v4, an evolution of the Octopus v1, v2, and v3 models, excels in selection and parameter understanding and reformatting.","Additionally, we explore the use of graph as a versatile data structure that effectively coordinates multiple open-source models by harnessing the capabilities of the Octopus model and \\textit{functional tokens}.","Use our open-sourced GitHub (\\url{https://www.nexa4ai.com/}) to try Octopus v4 models (\\url{https://huggingface.co/NexaAIDev/Octopus-v4}), and contrite to a larger graph of language models.","By activating models less than 10B parameters, we achieved SOTA MMLU score of 74.8 among the same level models."],"url":"http://arxiv.org/abs/2404.19296v1","category":"cs.CL"}
{"created":"2024-04-30 06:48:56","title":"Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning","abstract":"This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS). These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs. For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium. The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed. Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes. To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity. Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment. Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner.","sentences":["This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS).","These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs.","For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium.","The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed.","Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes.","To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity.","Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment.","Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner."],"url":"http://arxiv.org/abs/2404.19292v1","category":"cs.IT"}
{"created":"2024-04-30 16:13:01","title":"Tuning the coherent interaction of an electron qubit and a nuclear magnon","abstract":"A central spin qubit interacting coherently with an ensemble of proximal spins can be used to engineer entangled collective states or a multi-qubit register. Making full use of this many-body platform requires tuning the interaction between the central spin and its spin register. GaAs quantum dots offer a model realization of the central spin system where an electron qubit interacts with multiple ensembles of $\\sim 10^{4}$ nuclear spins. In this work, we demonstrate tuning of the interaction between the electron qubit and the nuclear many-body system in a GaAs quantum dot. The homogeneity of the GaAs system allows us to perform high-precision and isotopically selective nuclear sideband spectroscopy, which reveals the single-nucleus electronic Knight field. Together with time-resolved spectroscopy of the nuclear field, this fully characterizes the electron-nuclear interaction for a priori control. An algorithmic feedback sequence selects the nuclear polarization precisely, which adjusts the electron-nuclear exchange interaction in situ via the electronic g-factor anisotropy. This allows us to tune directly the activation rate of a collective nuclear excitation (magnon) and the coherence time of the electron qubit. Our method is applicable to similar central-spin systems and enables the programmable tuning of coherent interactions in the many-body regime.","sentences":["A central spin qubit interacting coherently with an ensemble of proximal spins can be used to engineer entangled collective states or a multi-qubit register.","Making full use of this many-body platform requires tuning the interaction between the central spin and its spin register.","GaAs quantum dots offer a model realization of the central spin system where an electron qubit interacts with multiple ensembles of $\\sim 10^{4}$ nuclear spins.","In this work, we demonstrate tuning of the interaction between the electron qubit and the nuclear many-body system in a GaAs quantum dot.","The homogeneity of the GaAs system allows us to perform high-precision and isotopically selective nuclear sideband spectroscopy, which reveals the single-nucleus electronic Knight field.","Together with time-resolved spectroscopy of the nuclear field, this fully characterizes the electron-nuclear interaction for a priori control.","An algorithmic feedback sequence selects the nuclear polarization precisely, which adjusts the electron-nuclear exchange interaction in situ via the electronic g-factor anisotropy.","This allows us to tune directly the activation rate of a collective nuclear excitation (magnon) and the coherence time of the electron qubit.","Our method is applicable to similar central-spin systems and enables the programmable tuning of coherent interactions in the many-body regime."],"url":"http://arxiv.org/abs/2404.19679v1","category":"quant-ph"}
{"created":"2024-04-30 16:08:02","title":"The Nature of X-Rays from Young Stellar Objects in the Orion Nebula Cluster -- A Chandra HETGS Legacy Project","abstract":"The Orion Nebula Cluster (ONC) is the closest site of very young ($\\sim$ 1 Myrs) massive star formation. The ONC hosts more than 1600 young and X-ray bright stars with masses ranging from $\\sim$ 0.1 to 35 $M_\\odot$. The Chandra HETGS Orion Legacy Project observed the ONC with the Chandra high energy transmission grating spectrometer (HETGS) for $2.1\\,$Ms. We describe the spectral extraction and cleaning processes necessary to separate overlapping spectra. We obtained 36 high resolution spectra which includes a high brilliance X-ray spectrum of $\\theta^1$ Ori C with over 100 highly significant X-ray lines. The lines show Doppler broadening between 300 and $400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. Higher spectral diffraction orders allow us to resolve line components of high Z He-like triplets in $\\theta^1$ Ori C with unprecedented spectral resolution. Long term light curves spanning $\\sim$20 years show all stars to be highly variable, including the massive stars. Spectral fitting with thermal coronal emission line models reveals that most sources show column densities of up to a few times $10^{22}\\,$cm$^{-2}$ and high coronal temperatures of 10 to 90 MK. We observe a bifurcation of the high temperature component where some stars show a high component of 40 MK, while others show above 60 MK indicating heavy flaring activity. Some lines are resolved with Doppler broadening above our threshold of $\\sim200\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$, up to $500\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. This data set represents the largest collection of HETGS high resolution X-ray spectra from young pre-MS stars in a single star-forming region to date.","sentences":["The Orion Nebula Cluster (ONC) is the closest site of very young ($\\sim$ 1 Myrs) massive star formation.","The ONC hosts more than 1600 young and X-ray bright stars with masses ranging from $\\sim$ 0.1 to 35 $M_\\odot$. The Chandra HETGS Orion Legacy Project observed the ONC with the Chandra high energy transmission grating spectrometer (HETGS) for $2.1\\,$Ms.","We describe the spectral extraction and cleaning processes necessary to separate overlapping spectra.","We obtained 36 high resolution spectra which includes a high brilliance X-ray spectrum of $\\theta^1$ Ori C with over 100 highly significant X-ray lines.","The lines show Doppler broadening between 300 and $400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. Higher spectral diffraction orders allow us to resolve line components of high Z He-like triplets in $\\theta^1$ Ori C with unprecedented spectral resolution.","Long term light curves spanning $\\sim$20 years show all stars to be highly variable, including the massive stars.","Spectral fitting with thermal coronal emission line models reveals that most sources show column densities of up to a few times $10^{22}\\,$cm$^{-2}$ and high coronal temperatures of 10 to 90 MK.","We observe a bifurcation of the high temperature component where some stars show a high component of 40 MK, while others show above 60 MK indicating heavy flaring activity.","Some lines are resolved with Doppler broadening above our threshold of $\\sim200\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$, up to $500\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. This data set represents the largest collection of HETGS high resolution X-ray spectra from young pre-MS stars in a single star-forming region to date."],"url":"http://arxiv.org/abs/2404.19676v1","category":"astro-ph.SR"}
{"created":"2024-04-30 15:45:30","title":"MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation","abstract":"Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from seen training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC. The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24.","sentences":["Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications.","Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC).","Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from seen training distributions but recognizes novel classes sampled from unseen testing distributions.","In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment.","Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks.","To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios.","Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model.","Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning.","The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts.","We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC.","The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24."],"url":"http://arxiv.org/abs/2404.19644v1","category":"cs.CV"}
{"created":"2024-04-30 14:16:45","title":"Automatic Cardiac Pathology Recognition in Echocardiography Images Using Higher Order Dynamic Mode Decomposition and a Vision Transformer for Small Datasets","abstract":"Heart diseases are the main international cause of human defunction. According to the WHO, nearly 18 million people decease each year because of heart diseases. Also considering the increase of medical data, much pressure is put on the health industry to develop systems for early and accurate heart disease recognition. In this work, an automatic cardiac pathology recognition system based on a novel deep learning framework is proposed, which analyses in real-time echocardiography video sequences. The system works in two stages. The first one transforms the data included in a database of echocardiography sequences into a machine-learning-compatible collection of annotated images which can be used in the training stage of any kind of machine learning-based framework, and more specifically with deep learning. This includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the first time to the authors' knowledge, for both data augmentation and feature extraction in the medical field. The second stage is focused on building and training a Vision Transformer (ViT), barely explored in the related literature. The ViT is adapted for an effective training from scratch, even with small datasets. The designed neural network analyses images from an echocardiography sequence to predict the heart state. The results obtained show the superiority of the proposed system and the efficacy of the HODMD algorithm, even outperforming pretrained Convolutional Neural Networks (CNNs), which are so far the method of choice in the literature.","sentences":["Heart diseases are the main international cause of human defunction.","According to the WHO, nearly 18 million people decease each year because of heart diseases.","Also considering the increase of medical data, much pressure is put on the health industry to develop systems for early and accurate heart disease recognition.","In this work, an automatic cardiac pathology recognition system based on a novel deep learning framework is proposed, which analyses in real-time echocardiography video sequences.","The system works in two stages.","The first one transforms the data included in a database of echocardiography sequences into a machine-learning-compatible collection of annotated images which can be used in the training stage of any kind of machine learning-based framework, and more specifically with deep learning.","This includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the first time to the authors' knowledge, for both data augmentation and feature extraction in the medical field.","The second stage is focused on building and training a Vision Transformer (ViT), barely explored in the related literature.","The ViT is adapted for an effective training from scratch, even with small datasets.","The designed neural network analyses images from an echocardiography sequence to predict the heart state.","The results obtained show the superiority of the proposed system and the efficacy of the HODMD algorithm, even outperforming pretrained Convolutional Neural Networks (CNNs), which are so far the method of choice in the literature."],"url":"http://arxiv.org/abs/2404.19579v1","category":"eess.IV"}
{"created":"2024-04-30 12:44:55","title":"First observation of $\u039b_{b}^{0} \\rightarrow \u03a3_c^{(*)++} D^{(*)-} K^{-}$ decays","abstract":"The four decays, $\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{(*)++} D^{(*)-} K^{-}$, are observed for the first time using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of $13\\,\\rm{TeV}$, corresponding to an integrated luminosity of $6\\,\\rm{fb}^{-1}$. By considering the $\\Lambda_b^0 \\rightarrow \\Lambda_c^{+} \\overline{D}^0 K^{-}$ decay as reference channel, the following branching fraction ratios are measured to be,   $$\\frac{\\cal{B} (\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm{D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Lambda_c^{+} \\rm \\overline{D}^0 {K}^{-})}   = {0.282}\\pm{0.016}\\pm{0.016}\\pm{0.005},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm {D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {0.460}\\pm{0.052}\\pm{0.028},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm {D}^{*-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {2.261}\\pm{0.202}\\pm{0.129}\\pm{0.046},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm D^{*-} K^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm D^{-} K^{-})}   = {0.896}\\pm{0.137}\\pm{0.066}\\pm{0.018},$$   where the first uncertainties are statistical, the second are systematic, and the third are due to uncertainties in the branching fractions of intermediate particle decays. These initial observations mark the beginning of pentaquark searches in these modes, with more data set to become available following the LHCb upgrade.","sentences":["The four decays, $\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{(*)++} D^{(*)-} K^{-}$, are observed for the first time using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of $13\\,\\rm{TeV}$, corresponding to an integrated luminosity of $6\\,\\rm{fb}^{-1}$. By considering the $\\Lambda_b^0 \\rightarrow \\Lambda_c^{+} \\overline{D}^0 K^{-}$ decay as reference channel, the following branching fraction ratios are measured to be,   $$\\frac{\\cal{B} (\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm{D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Lambda_c^{+} \\rm \\overline{D}^0 {K}^{-})}   = {0.282}\\pm{0.016}\\pm{0.016}\\pm{0.005},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm {D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow","\\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {0.460}\\pm{0.052}\\pm{0.028},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm {D}^{*-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {2.261}\\pm{0.202}\\pm{0.129}\\pm{0.046},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm D^{*-} K^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm D^{-} K^{-})}   = {0.896}\\pm{0.137}\\pm{0.066}\\pm{0.018},$$   where the first uncertainties are statistical, the second are systematic, and the third are due to uncertainties in the branching fractions of intermediate particle decays.","These initial observations mark the beginning of pentaquark searches in these modes, with more data set to become available following the LHCb upgrade."],"url":"http://arxiv.org/abs/2404.19510v1","category":"hep-ex"}
{"created":"2024-04-30 12:40:52","title":"SPHERE RefPlanets: Search for epsilon Eridani b and warm dust","abstract":"We carried out very deep VLT/SPHERE imaging polarimetry of the nearby system Eps Eri based on 38.5 hours of integration time with a 600 - 900 nm broadband filter to search for polarized scattered light from a planet or from circumstellar dust using AO, coronagraphy, high precision differential polarimetry, and angular differential imaging. We have improved several data reduction and post-processing techniques and also developed new ones to further increase the sensitivity of SPHERE/ZIMPOL. The data provide unprecedented contrast limits, but no significant detection of a point source or an extended signal from circumstellar dust. For each observing epoch, we obtained a point source contrast for the polarized intensity between $2\\cdot 10^{-8}$ and $4\\cdot 10^{-8}$ at the expected separation of the planet Eps Eri b of 1'' near quadrature phase. The polarimetric contrast limits are about six to 50 times better than the intensity limits because polarimetric imaging is much more efficient in speckle suppression. Combining the entire 14-month data set to the search for a planet moving on a Keplerian orbit with the K-Stacker software further improves the contrast limits by a factor of about two, to about $8 \\cdot 10^{-9}$ at 1''. This would allow the detection of a planet with a radius of about 2.5 Jupiter radii. The surface brightness contrast limits achieved for the polarized intensity from an extended scattering region are about 15 mag arcsec$^{-2}$ at 1'', or up to 3 mag arcsec$^{-2}$ deeper than previous limits. For Eps Eri, these limits exclude the presence of a narrow dust ring and they constrain the dust properties. This study shows that the polarimetric contrast limits for reflecting planets with SPHERE/ZIMPOL can be improved to a level $<10^{-8}$ simply by collecting more data over many nights and using the K-Stacker software.","sentences":["We carried out very deep VLT/SPHERE imaging polarimetry of the nearby system Eps Eri based on 38.5 hours of integration time with a 600 - 900 nm broadband filter to search for polarized scattered light from a planet or from circumstellar dust using AO, coronagraphy, high precision differential polarimetry, and angular differential imaging.","We have improved several data reduction and post-processing techniques and also developed new ones to further increase the sensitivity of SPHERE/ZIMPOL.","The data provide unprecedented contrast limits, but no significant detection of a point source or an extended signal from circumstellar dust.","For each observing epoch, we obtained a point source contrast for the polarized intensity between $2\\cdot 10^{-8}$ and $4\\cdot 10^{-8}$ at the expected separation of the planet Eps Eri b of 1'' near quadrature phase.","The polarimetric contrast limits are about six to 50 times better than the intensity limits because polarimetric imaging is much more efficient in speckle suppression.","Combining the entire 14-month data set to the search for a planet moving on a Keplerian orbit with the K-Stacker software further improves the contrast limits by a factor of about two, to about $8 \\cdot 10^{-9}$ at 1''.","This would allow the detection of a planet with a radius of about 2.5 Jupiter radii.","The surface brightness contrast limits achieved for the polarized intensity from an extended scattering region are about 15 mag arcsec$^{-2}$ at 1'', or up to 3 mag arcsec$^{-2}$ deeper than previous limits.","For Eps Eri, these limits exclude the presence of a narrow dust ring and they constrain the dust properties.","This study shows that the polarimetric contrast limits for reflecting planets with SPHERE/ZIMPOL can be improved to a level $<10^{-8}$ simply by collecting more data over many nights and using the K-Stacker software."],"url":"http://arxiv.org/abs/2404.19504v1","category":"astro-ph.EP"}
{"created":"2024-04-30 12:27:49","title":"Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)","abstract":"Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta. Another fundamental premise of the functionalist theory is that \"effect\" is not a monolithic concept. Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call \"effect.\" A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV. Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV. It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect.","sentences":["Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis.","This paper retraces the theory behind the estimator.","It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation.","It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands.","Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison.","Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta.","Another fundamental premise of the functionalist theory is that \"effect\" is not a monolithic concept.","Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call \"effect.\"","A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV.","Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV.","It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect."],"url":"http://arxiv.org/abs/2404.19495v1","category":"stat.AP"}
{"created":"2024-04-30 10:57:37","title":"AoI-aware Sensing Scheduling and Trajectory Optimization for Multi-UAV-assisted Wireless Backscatter Networks","abstract":"This paper considers multiple unmanned aerial vehicles (UAVs) to assist sensing data transmissions from the ground users (GUs) to a remote base station (BS). Each UAV collects sensing data from the GUs and then forwards the sensing data to the remote BS. The GUs first backscatter their data to the UAVs and then all UAVs forward data to the BS by the nonorthogonal multiple access (NOMA) transmissions. We formulate a multi-stage stochastic optimization problem to minimize the long-term time-averaged age-of-information (AoI) by jointly optimizing the GUs' access control, the UAVs' beamforming, and trajectory planning strategies. To solve this problem, we first model the dynamics of the GUs' AoI statuses by virtual queueing systems, and then propose the AoI-aware sensing scheduling and trajectory optimization (AoI-STO) algorithm. This allows us to transform the multi-stage AoI minimization problem into a series of per-slot control problems by using the Lyapunov optimization framework. In each time slot, the GUs' access control, the UAVs' beamforming, and mobility control strategies are updated by using the block coordinate descent (BCD) method according to the instant GUs' AoI statuses. Simulation results reveal that the proposed AoI-STO algorithm can reduce the overall AoI by more than 50%. The GUs' scheduling fairness is also improved greatly by adapting the GUs' access control compared with typical baseline schemes.","sentences":["This paper considers multiple unmanned aerial vehicles (UAVs) to assist sensing data transmissions from the ground users (GUs) to a remote base station (BS).","Each UAV collects sensing data from the GUs and then forwards the sensing data to the remote BS.","The GUs first backscatter their data to the UAVs and then all UAVs forward data to the BS by the nonorthogonal multiple access (NOMA) transmissions.","We formulate a multi-stage stochastic optimization problem to minimize the long-term time-averaged age-of-information (AoI) by jointly optimizing the GUs' access control, the UAVs' beamforming, and trajectory planning strategies.","To solve this problem, we first model the dynamics of the GUs' AoI statuses by virtual queueing systems, and then propose the AoI-aware sensing scheduling and trajectory optimization (AoI-STO) algorithm.","This allows us to transform the multi-stage AoI minimization problem into a series of per-slot control problems by using the Lyapunov optimization framework.","In each time slot, the GUs' access control, the UAVs' beamforming, and mobility control strategies are updated by using the block coordinate descent (BCD) method according to the instant GUs' AoI statuses.","Simulation results reveal that the proposed AoI-STO algorithm can reduce the overall AoI by more than 50%.","The GUs' scheduling fairness is also improved greatly by adapting the GUs' access control compared with typical baseline schemes."],"url":"http://arxiv.org/abs/2404.19449v1","category":"cs.IT"}
{"created":"2024-04-30 10:54:10","title":"Exploring the potential of synthesizing unknown superheavy isotopes via cold-fusion reactions based on the dinuclear system model","abstract":"To assess the potential of cold-fusion for synthesizing superheavy nuclei (SHN) with proton numbers 104-113, we systematically calculated 145 naturally occurring projectile-target combinations within the DNS model. Reactions predominantly show maximum cross-sections in the 1n to 2n channels, peaking near the Coulomb barrier with a sum of barrier and Q-value within 30 MeV. The maximum cross-section occurs below the Bass barrier, suggesting either the Bass model's limitation or significant deformation reducing the effective Coulomb barrier. Our calculations align well with experimental data, revealing that more neutron-rich projectiles slightly enhance fusion, though the effect is minor. For fixed targets (Pb, Bi), evaporation residue cross-sections decrease linearly with increasing projectile proton number, attributed to reduced fusion probability and lower fission barriers in heavier SHN. The touching potential $V_{\\rm in}$ shows a linear trend with the product of projectile-target proton numbers, with neutron-rich systems exhibiting lower $V_{\\rm in}$. Some reactions with $V_{\\rm in} < V_{\\rm S}$ may involve nucleon transfer before capture. Based on the DNS model, we identified optimal combinations and collision energies for synthesizing SHN with significant cross-sections. Collectively, our findings indicate that cold fusion is a promising avenue for creating proton-rich SHN around the drip line in the Z=104-113 region, offering distinct advantages over alternative mechanisms.","sentences":["To assess the potential of cold-fusion for synthesizing superheavy nuclei (SHN) with proton numbers 104-113, we systematically calculated 145 naturally occurring projectile-target combinations within the DNS model.","Reactions predominantly show maximum cross-sections in the 1n to 2n channels, peaking near the Coulomb barrier with a sum of barrier and Q-value within 30 MeV.","The maximum cross-section occurs below the Bass barrier, suggesting either the Bass model's limitation or significant deformation reducing the effective Coulomb barrier.","Our calculations align well with experimental data, revealing that more neutron-rich projectiles slightly enhance fusion, though the effect is minor.","For fixed targets (Pb, Bi), evaporation residue cross-sections decrease linearly with increasing projectile proton number, attributed to reduced fusion probability and lower fission barriers in heavier SHN.","The touching potential $V_{\\rm in}$ shows a linear trend with the product of projectile-target proton numbers, with neutron-rich systems exhibiting lower $V_{\\rm in}$. Some reactions with $V_{\\rm in} < V_{\\rm S}$ may involve nucleon transfer before capture.","Based on the DNS model, we identified optimal combinations and collision energies for synthesizing SHN with significant cross-sections.","Collectively, our findings indicate that cold fusion is a promising avenue for creating proton-rich SHN around the drip line in the Z=104-113 region, offering distinct advantages over alternative mechanisms."],"url":"http://arxiv.org/abs/2404.19446v1","category":"nucl-th"}
{"created":"2024-04-30 09:51:21","title":"Rare Events in Extreme Value Statistics of L\u00e9vy Processes","abstract":"We study rare events in the extreme value statistics of stochastic symmetric jump processes with power tails in the distributions of the jumps, using the big jump principle. The principle states that in the presence of stochastic processes with power tails statistics, if at a certain time a physical quantity takes on a value much larger than its typical value, this large fluctuation is realised through a single macroscopic jump that exceeds the typical scale of the process by several orders of magnitude. In particular, our estimation focuses on the asymptotic behaviour of the tail of the probability distribution of maxima, a fundamental quantity in a wide class of stochastic models used in chemistry to estimate reaction thresholds, in climatology for earthquake risk assessment, in finance for portfolio management, and in ecology for the collective behaviour of species. We determine the analytical form of the probability distribution of rare events in the extreme value statistics of three L\\'evy processes; L\\'evy flights, L\\'evy walks and the L\\'evy-Lorentz gas. For the L\\'evy flights, we re-obtain through the big jump approach recent analytical results, extending their validity. For the L\\'evy-Lorentz gas we show that the topology of the disordered lattice along which the walker moves induces memory effects in its dynamics, which influences the extreme value statistics. Our results are confirmed by extensive numerical simulations.","sentences":["We study rare events in the extreme value statistics of stochastic symmetric jump processes with power tails in the distributions of the jumps, using the big jump principle.","The principle states that in the presence of stochastic processes with power tails statistics, if at a certain time a physical quantity takes on a value much larger than its typical value, this large fluctuation is realised through a single macroscopic jump that exceeds the typical scale of the process by several orders of magnitude.","In particular, our estimation focuses on the asymptotic behaviour of the tail of the probability distribution of maxima, a fundamental quantity in a wide class of stochastic models used in chemistry to estimate reaction thresholds, in climatology for earthquake risk assessment, in finance for portfolio management, and in ecology for the collective behaviour of species.","We determine the analytical form of the probability distribution of rare events in the extreme value statistics of three L\\'evy processes; L\\'evy flights, L\\'evy walks and the L\\'evy-Lorentz gas.","For the L\\'evy flights, we re-obtain through the big jump approach recent analytical results, extending their validity.","For the L\\'evy-Lorentz gas we show that the topology of the disordered lattice along which the walker moves induces memory effects in its dynamics, which influences the extreme value statistics.","Our results are confirmed by extensive numerical simulations."],"url":"http://arxiv.org/abs/2404.19406v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 08:04:06","title":"Design of a Representation Information Repository for the Long-Term Usability of Digital Building Documents","abstract":"The long-term usability of digital building documents is essential for the maintenance and optimization of infrastructure portfolios. It supports the preservation of building-specific knowledge and the cultural heritage hidden within. However, having to do this throughout the lifecycle of a building - or even indefinitely - remains a major challenge. This is especially true for organizations responsible for large collections of digital building documents, such as public administrations or archives. In this article, we first describe the challenges and requirements associated with preservation tasks, and then introduce the concept of so-called representation information within BIM (Building Information Modeling). This type of information is important to give meaning to the stored bit sequences for a particular community. Then, we design a repository for representation information and introduce some so-called 23 BIMcore content elements. Finally, we focus on BIM and the construction sector and explain how the proposed repository can be used to implement the two concepts introduced in the ISO reference model OAIS (Open Archival Information System), namely the representation information and the context information, as well as the concept of significant properties, which has not yet been explicitly modeled in OAIS.","sentences":["The long-term usability of digital building documents is essential for the maintenance and optimization of infrastructure portfolios.","It supports the preservation of building-specific knowledge and the cultural heritage hidden within.","However, having to do this throughout the lifecycle of a building - or even indefinitely - remains a major challenge.","This is especially true for organizations responsible for large collections of digital building documents, such as public administrations or archives.","In this article, we first describe the challenges and requirements associated with preservation tasks, and then introduce the concept of so-called representation information within BIM (Building Information Modeling).","This type of information is important to give meaning to the stored bit sequences for a particular community.","Then, we design a repository for representation information and introduce some so-called 23 BIMcore content elements.","Finally, we focus on BIM and the construction sector and explain how the proposed repository can be used to implement the two concepts introduced in the ISO reference model OAIS (Open Archival Information System), namely the representation information and the context information, as well as the concept of significant properties, which has not yet been explicitly modeled in OAIS."],"url":"http://arxiv.org/abs/2404.19337v1","category":"cs.DL"}
{"created":"2024-04-30 06:59:12","title":"Channel Performance Metrics and Evaluation for XR Head-Mounted Displays with mmWave Arrays","abstract":"Millimeter-wave (mmWave) technology holds the potential to revolutionize head-mounted displays (HMDs) by enabling high-speed wireless communication with nearby processing nodes, where complex video rendering can take place. However, the sparse angular profile of mmWave channels, coupled with the narrow field of view (FoV) of patch-antenna arrays and frequent HMD rotation, can lead to poor performance.   We introduce six channel performance metrics to evaluate the performance of an HMD equipped with mmWave arrays. We analyze the metrics using analytical models, discuss their impact for the application, and apply them to 28 GHz channel sounding data, collected in a conference room using eight HMD patch-antenna arrays, offset by 45 degrees from each other in azimuth.   Our findings confirm that a single array performs poorly due to the narrow FoV, and featuring multiple arrays along the HMD's azimuth is required. Namely, the broader FoV stabilizes channel gain during HMD rotation, lessens the attenuation caused by line of sight (LoS) obstruction, and increases the channel's spatial multiplexing capability. In light of our findings, we conclude that it is imperative to either equip the HMD with multiple arrays or, as an alternative approach, incorporate macroscopic diversity by leveraging distributed access point (AP) infrastructure.","sentences":["Millimeter-wave (mmWave) technology holds the potential to revolutionize head-mounted displays (HMDs) by enabling high-speed wireless communication with nearby processing nodes, where complex video rendering can take place.","However, the sparse angular profile of mmWave channels, coupled with the narrow field of view (FoV) of patch-antenna arrays and frequent HMD rotation, can lead to poor performance.   ","We introduce six channel performance metrics to evaluate the performance of an HMD equipped with mmWave arrays.","We analyze the metrics using analytical models, discuss their impact for the application, and apply them to 28 GHz channel sounding data, collected in a conference room using eight HMD patch-antenna arrays, offset by 45 degrees from each other in azimuth.   ","Our findings confirm that a single array performs poorly due to the narrow FoV, and featuring multiple arrays along the HMD's azimuth is required.","Namely, the broader FoV stabilizes channel gain during HMD rotation, lessens the attenuation caused by line of sight (LoS) obstruction, and increases the channel's spatial multiplexing capability.","In light of our findings, we conclude that it is imperative to either equip the HMD with multiple arrays or, as an alternative approach, incorporate macroscopic diversity by leveraging distributed access point (AP) infrastructure."],"url":"http://arxiv.org/abs/2404.19297v1","category":"eess.SP"}
{"created":"2024-04-30 06:36:43","title":"Training-free Graph Neural Networks and the Power of Labels as Features","abstract":"We propose training-free graph neural networks (TFGNNs), which can be used without training and can also be improved with optional training, for transductive node classification. We first advocate labels as features (LaF), which is an admissible but not explored technique. We show that LaF provably enhances the expressive power of graph neural networks. We design TFGNNs based on this analysis. In the experiments, we confirm that TFGNNs outperform existing GNNs in the training-free setting and converge with much fewer training iterations than traditional GNNs.","sentences":["We propose training-free graph neural networks (TFGNNs), which can be used without training and can also be improved with optional training, for transductive node classification.","We first advocate labels as features (LaF), which is an admissible but not explored technique.","We show that LaF provably enhances the expressive power of graph neural networks.","We design TFGNNs based on this analysis.","In the experiments, we confirm that TFGNNs outperform existing GNNs in the training-free setting and converge with much fewer training iterations than traditional GNNs."],"url":"http://arxiv.org/abs/2404.19288v1","category":"cs.LG"}
{"created":"2024-04-30 06:21:44","title":"Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation","abstract":"Approximate k-Nearest Neighbour (ANN) methods are often used for mining information and aiding machine learning on large scale high-dimensional datasets. ANN methods typically differ in the index structure used for accelerating searches, resulting in various recall/runtime trade-off points. For applications with static datasets, runtime constraints and dataset properties can be used to empirically select an ANN method with suitable operating characteristics. However, for applications with dynamic datasets, which are subject to frequent online changes (like addition of new samples), there is currently no consensus as to which ANN methods are most suitable. Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the frequency and size of index updates. To address this, we empirically evaluate 5 popular ANN methods on two main applications (online data collection and online feature learning) while taking into account these considerations. Two dynamic datasets are used, derived from the SIFT1M dataset with 1 million samples and the DEEP1B dataset with 1 billion samples. The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method. For online data collection, the Hierarchical Navigable Small World Graphs method achieves a consistent speedup over baseline across a wide range of recall rates. For online feature learning, the Scalable Nearest Neighbours method is faster than baseline for recall rates below 75%.","sentences":["Approximate k-Nearest Neighbour (ANN) methods are often used for mining information and aiding machine learning on large scale high-dimensional datasets.","ANN methods typically differ in the index structure used for accelerating searches, resulting in various recall/runtime trade-off points.","For applications with static datasets, runtime constraints and dataset properties can be used to empirically select an ANN method with suitable operating characteristics.","However, for applications with dynamic datasets, which are subject to frequent online changes (like addition of new samples), there is currently no consensus as to which ANN methods are most suitable.","Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the frequency and size of index updates.","To address this, we empirically evaluate 5 popular ANN methods on two main applications (online data collection and online feature learning) while taking into account these considerations.","Two dynamic datasets are used, derived from the SIFT1M dataset with 1 million samples and the DEEP1B dataset with 1 billion samples.","The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method.","For online data collection, the Hierarchical Navigable Small World Graphs method achieves a consistent speedup over baseline across a wide range of recall rates.","For online feature learning, the Scalable Nearest Neighbours method is faster than baseline for recall rates below 75%."],"url":"http://arxiv.org/abs/2404.19284v1","category":"cs.LG"}
{"created":"2024-04-30 06:21:42","title":"MAP-Former: Multi-Agent-Pair Gaussian Joint Prediction","abstract":"There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed. Closing this gap necessitates advancements in prediction beyond current practices. Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents. Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents. Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories. This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene. We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions. Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs.","sentences":["There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed.","Closing this gap necessitates advancements in prediction beyond current practices.","Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents.","Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents.","Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories.","This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene.","We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions.","Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs."],"url":"http://arxiv.org/abs/2404.19283v1","category":"cs.LG"}
{"created":"2024-04-30 06:12:05","title":"Audio-Visual Traffic Light State Detection for Urban Robots","abstract":"We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings. This is a challenging problem because of the visual occlusions and noise from robot locomotion. Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors. The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability. Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion. This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics.","sentences":["We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings.","This is a challenging problem because of the visual occlusions and noise from robot locomotion.","Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors.","The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability.","Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion.","This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics."],"url":"http://arxiv.org/abs/2404.19281v1","category":"cs.RO"}
{"created":"2024-04-30 04:44:19","title":"Persistent Homology generalizations for Social Media Network Analysis","abstract":"This study details an approach for the analysis of social media collected political data through the lens of Topological Data Analysis, with a specific focus on Persistent Homology and the political processes they represent by proposing a set of mathematical generalizations using Gaussian functions to define and analyze these Persistent Homology categories. Three distinct types of Persistent Homologies were recurrent across datasets that had been plotted through retweeting patterns and analyzed through the k-Nearest-Neighbor filtrations. As these Persistent Homologies continued to appear, they were then categorized and dubbed Nuclear, Bipolar, and Multipolar Constellations. Upon investigating the content of these plotted tweets, specific patterns of interaction and political information dissemination were identified, namely Political Personalism and Political Polarization. Through clustering and application of Gaussian density functions, I have mathematically characterized each category, encapsulating their distinctive topological features. The mathematical generalizations of Bipolar, Nuclear, and Multipolar Constellations developed in this study are designed to inspire other political science digital media researchers to utilize these categories as to identify Persistent Homology in datasets derived from various social media platforms, suggesting the broader hypothesis that such structures are bound to be present on political scraped data regardless of the social media it's derived from. This method aims to offer a new perspective in Network Analysis as it allows for an exploration of the underlying shape of the networks formed by retweeting patterns, enhancing the understanding of digital interactions within the sphere of Computational Social Sciences.","sentences":["This study details an approach for the analysis of social media collected political data through the lens of Topological Data Analysis, with a specific focus on Persistent Homology and the political processes they represent by proposing a set of mathematical generalizations using Gaussian functions to define and analyze these Persistent Homology categories.","Three distinct types of Persistent Homologies were recurrent across datasets that had been plotted through retweeting patterns and analyzed through the k-Nearest-Neighbor filtrations.","As these Persistent Homologies continued to appear, they were then categorized and dubbed Nuclear, Bipolar, and Multipolar Constellations.","Upon investigating the content of these plotted tweets, specific patterns of interaction and political information dissemination were identified, namely Political Personalism and Political Polarization.","Through clustering and application of Gaussian density functions, I have mathematically characterized each category, encapsulating their distinctive topological features.","The mathematical generalizations of Bipolar, Nuclear, and Multipolar Constellations developed in this study are designed to inspire other political science digital media researchers to utilize these categories as to identify Persistent Homology in datasets derived from various social media platforms, suggesting the broader hypothesis that such structures are bound to be present on political scraped data regardless of the social media it's derived from.","This method aims to offer a new perspective in Network Analysis as it allows for an exploration of the underlying shape of the networks formed by retweeting patterns, enhancing the understanding of digital interactions within the sphere of Computational Social Sciences."],"url":"http://arxiv.org/abs/2404.19257v1","category":"cs.CY"}
{"created":"2024-04-30 04:41:47","title":"Bias Mitigation via Compensation: A Reinforcement Learning Perspective","abstract":"As AI increasingly integrates with human decision-making, we must carefully consider interactions between the two. In particular, current approaches focus on optimizing individual agent actions but often overlook the nuances of collective intelligence. Group dynamics might require that one agent (e.g., the AI system) compensate for biases and errors in another agent (e.g., the human), but this compensation should be carefully developed. We provide a theoretical framework for algorithmic compensation that synthesizes game theory and reinforcement learning principles to demonstrate the natural emergence of deceptive outcomes from the continuous learning dynamics of agents. We provide simulation results involving Markov Decision Processes (MDP) learning to interact. This work then underpins our ethical analysis of the conditions in which AI agents should adapt to biases and behaviors of other agents in dynamic and complex decision-making environments. Overall, our approach addresses the nuanced role of strategic deception of humans, challenging previous assumptions about its detrimental effects. We assert that compensation for others' biases can enhance coordination and ethical alignment: strategic deception, when ethically managed, can positively shape human-AI interactions.","sentences":["As AI increasingly integrates with human decision-making, we must carefully consider interactions between the two.","In particular, current approaches focus on optimizing individual agent actions but often overlook the nuances of collective intelligence.","Group dynamics might require that one agent (e.g., the AI system) compensate for biases and errors in another agent (e.g., the human), but this compensation should be carefully developed.","We provide a theoretical framework for algorithmic compensation that synthesizes game theory and reinforcement learning principles to demonstrate the natural emergence of deceptive outcomes from the continuous learning dynamics of agents.","We provide simulation results involving Markov Decision Processes (MDP) learning to interact.","This work then underpins our ethical analysis of the conditions in which AI agents should adapt to biases and behaviors of other agents in dynamic and complex decision-making environments.","Overall, our approach addresses the nuanced role of strategic deception of humans, challenging previous assumptions about its detrimental effects.","We assert that compensation for others' biases can enhance coordination and ethical alignment: strategic deception, when ethically managed, can positively shape human-AI interactions."],"url":"http://arxiv.org/abs/2404.19256v1","category":"cs.AI"}
{"created":"2024-04-30 04:19:17","title":"Suvach -- Generated Hindi QA benchmark","abstract":"Current evaluation benchmarks for question answering (QA) in Indic languages often rely on machine translation of existing English datasets. This approach suffers from bias and inaccuracies inherent in machine translation, leading to datasets that may not reflect the true capabilities of EQA models for Indic languages. This paper proposes a new benchmark specifically designed for evaluating Hindi EQA models and discusses the methodology to do the same for any task. This method leverages large language models (LLMs) to generate a high-quality dataset in an extractive setting, ensuring its relevance for the target language. We believe this new resource will foster advancements in Hindi NLP research by providing a more accurate and reliable evaluation tool.","sentences":["Current evaluation benchmarks for question answering (QA) in Indic languages often rely on machine translation of existing English datasets.","This approach suffers from bias and inaccuracies inherent in machine translation, leading to datasets that may not reflect the true capabilities of EQA models for Indic languages.","This paper proposes a new benchmark specifically designed for evaluating Hindi EQA models and discusses the methodology to do the same for any task.","This method leverages large language models (LLMs) to generate a high-quality dataset in an extractive setting, ensuring its relevance for the target language.","We believe this new resource will foster advancements in Hindi NLP research by providing a more accurate and reliable evaluation tool."],"url":"http://arxiv.org/abs/2404.19254v1","category":"cs.CL"}
{"created":"2024-04-30 04:18:21","title":"Learning to Communicate Functional States with Nonverbal Expressions for Improved Human-Robot Collaboration","abstract":"Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction. Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors. In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck). We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions. The proposed approach was evaluated through a user study with 24 participants. The results demonstrate that: 1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot. 2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process. 3. The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state. 4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states.","sentences":["Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction.","Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors.","In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck).","We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions.","The proposed approach was evaluated through a user study with 24 participants.","The results demonstrate that:","1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot.","2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process.","3.","The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state.","4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states."],"url":"http://arxiv.org/abs/2404.19253v1","category":"cs.RO"}
{"created":"2024-04-30 04:01:09","title":"HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning","abstract":"Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. \\href{https://github.com/Clin0212/HydraLoRA}{Code}.","sentences":["Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA.","However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets.","This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance.","Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA.","Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise.","Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases.","\\href{https://github.com/Clin0212/HydraLoRA}{Code}."],"url":"http://arxiv.org/abs/2404.19245v1","category":"cs.CL"}
{"created":"2024-04-30 04:00:15","title":"A University Framework for the Responsible use of Generative AI in Research","abstract":"Generative Artificial Intelligence (generative AI) poses both opportunities and risks for the integrity of research. Universities must guide researchers in using generative AI responsibly, and in navigating a complex regulatory landscape subject to rapid change. By drawing on the experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of generative AI. We provide guidance to help distil the diverse regulatory environment into a principles-based position statement. Further, we explain how a position statement can then serve as a foundation for initiatives in training, communications, infrastructure, and process change. Despite the growing body of literature about AI's impact on academic integrity for undergraduate students, there has been comparatively little attention on the impacts of generative AI for research integrity, and the vital role of institutions in helping to address those challenges. This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework for so doing.","sentences":["Generative Artificial Intelligence (generative AI) poses both opportunities and risks for the integrity of research.","Universities must guide researchers in using generative AI responsibly, and in navigating a complex regulatory landscape subject to rapid change.","By drawing on the experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of generative AI.","We provide guidance to help distil the diverse regulatory environment into a principles-based position statement.","Further, we explain how a position statement can then serve as a foundation for initiatives in training, communications, infrastructure, and process change.","Despite the growing body of literature about AI's impact on academic integrity for undergraduate students, there has been comparatively little attention on the impacts of generative AI for research integrity, and the vital role of institutions in helping to address those challenges.","This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework for so doing."],"url":"http://arxiv.org/abs/2404.19244v1","category":"cs.CY"}
{"created":"2024-04-30 03:31:03","title":"Multi-hop Question Answering over Knowledge Graphs using Large Language Models","abstract":"Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes. Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially. In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops. We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window. We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance.","sentences":["Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges.","Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes.","Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially.","In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops.","We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window.","We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance."],"url":"http://arxiv.org/abs/2404.19234v1","category":"cs.AI"}
{"created":"2024-04-30 03:29:30","title":"GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models","abstract":"Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure cases -- whether they stem from knowledge deficits or issues related to system robustness. To address these challenges, we introduce GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs. This method facilitates the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an evaluation framework that differentiates knowledge gaps from robustness and enables the identification of defective modules. Our empirical results underscore the limitations of current reference-free evaluation approaches and the reliability of GRAMMAR to accurately identify model vulnerabilities.","sentences":["Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base.","However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure cases -- whether they stem from knowledge deficits or issues related to system robustness.","To address these challenges, we introduce GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs.","This method facilitates the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an evaluation framework that differentiates knowledge gaps from robustness and enables the identification of defective modules.","Our empirical results underscore the limitations of current reference-free evaluation approaches and the reliability of GRAMMAR to accurately identify model vulnerabilities."],"url":"http://arxiv.org/abs/2404.19232v1","category":"cs.CL"}
{"created":"2024-04-30 03:17:42","title":"Deep Lead Optimization: Leveraging Generative AI for Structural Modification","abstract":"The idea of using deep-learning-based molecular generation to accelerate discovery of drug candidates has attracted extraordinary attention, and many deep generative models have been developed for automated drug design, termed molecular generation. In general, molecular generation encompasses two main strategies: de novo design, which generates novel molecular structures from scratch, and lead optimization, which refines existing molecules into drug candidates. Among them, lead optimization plays an important role in real-world drug design. For example, it can enable the development of me-better drugs that are chemically distinct yet more effective than the original drugs. It can also facilitate fragment-based drug design, transforming virtual-screened small ligands with low affinity into first-in-class medicines. Despite its importance, automated lead optimization remains underexplored compared to the well-established de novo generative models, due to its reliance on complex biological and chemical knowledge. To bridge this gap, we conduct a systematic review of traditional computational methods for lead optimization, organizing these strategies into four principal sub-tasks with defined inputs and outputs. This review delves into the basic concepts, goals, conventional CADD techniques, and recent advancements in AIDD. Additionally, we introduce a unified perspective based on constrained subgraph generation to harmonize the methodologies of de novo design and lead optimization. Through this lens, de novo design can incorporate strategies from lead optimization to address the challenge of generating hard-to-synthesize molecules; inversely, lead optimization can benefit from the innovations in de novo design by approaching it as a task of generating molecules conditioned on certain substructures.","sentences":["The idea of using deep-learning-based molecular generation to accelerate discovery of drug candidates has attracted extraordinary attention, and many deep generative models have been developed for automated drug design, termed molecular generation.","In general, molecular generation encompasses two main strategies: de novo design, which generates novel molecular structures from scratch, and lead optimization, which refines existing molecules into drug candidates.","Among them, lead optimization plays an important role in real-world drug design.","For example, it can enable the development of me-better drugs that are chemically distinct yet more effective than the original drugs.","It can also facilitate fragment-based drug design, transforming virtual-screened small ligands with low affinity into first-in-class medicines.","Despite its importance, automated lead optimization remains underexplored compared to the well-established de novo generative models, due to its reliance on complex biological and chemical knowledge.","To bridge this gap, we conduct a systematic review of traditional computational methods for lead optimization, organizing these strategies into four principal sub-tasks with defined inputs and outputs.","This review delves into the basic concepts, goals, conventional CADD techniques, and recent advancements in AIDD.","Additionally, we introduce a unified perspective based on constrained subgraph generation to harmonize the methodologies of de novo design and lead optimization.","Through this lens, de novo design can incorporate strategies from lead optimization to address the challenge of generating hard-to-synthesize molecules; inversely, lead optimization can benefit from the innovations in de novo design by approaching it as a task of generating molecules conditioned on certain substructures."],"url":"http://arxiv.org/abs/2404.19230v1","category":"q-bio.BM"}
{"created":"2024-04-30 02:05:18","title":"TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains","abstract":"In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets. It is important to note that existing datasets have not incorporated images or QA pairs, which are two crucial components of TableVQA. As such, the primary objective of this paper is to obtain these necessary components. Specifically, images are sourced either through the application of a \\textit{stylesheet} or by employing the proposed table rendering system. QA pairs are generated by exploiting the large language model (LLM) where the input is a text-formatted table. Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs. We comprehensively compare the performance of various multi-modal large language models (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs from our experiments. Moreover, we discover that the number of vision queries plays a significant role in TableVQA performance. To further analyze the capabilities of MLLMs in comparison to their LLM backbones, we investigate by presenting image-formatted tables to MLLMs and text-formatted tables to LLMs, respectively. Our findings suggest that processing visual inputs is more challenging than text inputs, as evidenced by the lower performance of MLLMs, despite generally requiring higher computational costs than LLMs. The proposed TableVQA-Bench and evaluation codes are available at \\href{https://github.com/naver-ai/tablevqabench}{https://github.com/naver-ai/tablevqabench}.","sentences":["In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets.","It is important to note that existing datasets have not incorporated images or QA pairs, which are two crucial components of TableVQA.","As such, the primary objective of this paper is to obtain these necessary components.","Specifically, images are sourced either through the application of a \\textit{stylesheet} or by employing the proposed table rendering system.","QA pairs are generated by exploiting the large language model (LLM) where the input is a text-formatted table.","Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs.","We comprehensively compare the performance of various multi-modal large language models (MLLMs) on TableVQA-Bench.","GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs from our experiments.","Moreover, we discover that the number of vision queries plays a significant role in TableVQA performance.","To further analyze the capabilities of MLLMs in comparison to their LLM backbones, we investigate by presenting image-formatted tables to MLLMs and text-formatted tables to LLMs, respectively.","Our findings suggest that processing visual inputs is more challenging than text inputs, as evidenced by the lower performance of MLLMs, despite generally requiring higher computational costs than LLMs.","The proposed TableVQA-Bench and evaluation codes are available at \\href{https://github.com/naver-ai/tablevqabench}{https://github.com/naver-ai/tablevqabench}."],"url":"http://arxiv.org/abs/2404.19205v1","category":"cs.CV"}
{"created":"2024-04-30 02:04:49","title":"NeRF-Insert: 3D Local Editing with Multimodal Control Signals","abstract":"We propose NeRF-Insert, a NeRF editing framework that allows users to make high-quality local edits with a flexible level of control. Unlike previous work that relied on image-to-image models, we cast scene editing as an in-painting problem, which encourages the global structure of the scene to be preserved. Moreover, while most existing methods use only textual prompts to condition edits, our framework accepts a combination of inputs of different modalities as reference. More precisely, a user may provide a combination of textual and visual inputs including images, CAD models, and binary image masks for specifying a 3D region. We use generic image generation models to in-paint the scene from multiple viewpoints, and lift the local edits to a 3D-consistent NeRF edit. Compared to previous methods, our results show better visual quality and also maintain stronger consistency with the original NeRF.","sentences":["We propose NeRF-Insert, a NeRF editing framework that allows users to make high-quality local edits with a flexible level of control.","Unlike previous work that relied on image-to-image models, we cast scene editing as an in-painting problem, which encourages the global structure of the scene to be preserved.","Moreover, while most existing methods use only textual prompts to condition edits, our framework accepts a combination of inputs of different modalities as reference.","More precisely, a user may provide a combination of textual and visual inputs including images, CAD models, and binary image masks for specifying a 3D region.","We use generic image generation models to in-paint the scene from multiple viewpoints, and lift the local edits to a 3D-consistent NeRF edit.","Compared to previous methods, our results show better visual quality and also maintain stronger consistency with the original NeRF."],"url":"http://arxiv.org/abs/2404.19204v1","category":"cs.CV"}
{"created":"2024-04-30 01:42:31","title":"Tunable Collective Excitations in Epitaxial Perovskite Nickelates","abstract":"The formation of plasmons through the collective excitation of charge density has generated intense discussions, offering insights to fundamental sciences and potential applications. While the underlying physical principles have been well-established, the effects of multibody interactions and orbital hybridization on plasmonic dynamics remain understudied. In this work, we present the observation of conventional metallic and correlated plasmons in epitaxial La1-xSrxNiO3 (LSNO) films with varying Sr doping concentrations (x = 0, 0.125, 0.25), unveiling their intriguing evolution. Unlike samples at other doping concentrations, the x = 0.125 intermediate doping sample does not exhibit the correlated plasmons despite showing high optical conductivity. Through experimental investigation using spectroscopic ellipsometry and X-ray absorption spectroscopy, that is further supported by theoretical calculations, the O2p-Ni3d orbital hybridization for x = 0.125 is found to be significantly enhanced, alongside a considerable weakening of its effective correlation U*. These factors account for the absence of correlated plasmons and the high optical conductivity observed in LSNO (0.125). Our findings highlight the significant impact of orbital hybridization on the electronic structures and the formation of quasiparticles in strongly correlated systems, thereby marking a notable advancement in plasmonics, emphasizing LSNO's compelling potential as a substitute material in optoelectronic devices.","sentences":["The formation of plasmons through the collective excitation of charge density has generated intense discussions, offering insights to fundamental sciences and potential applications.","While the underlying physical principles have been well-established, the effects of multibody interactions and orbital hybridization on plasmonic dynamics remain understudied.","In this work, we present the observation of conventional metallic and correlated plasmons in epitaxial La1-xSrxNiO3 (LSNO) films with varying Sr doping concentrations (x = 0, 0.125, 0.25), unveiling their intriguing evolution.","Unlike samples at other doping concentrations, the x = 0.125 intermediate doping sample does not exhibit the correlated plasmons despite showing high optical conductivity.","Through experimental investigation using spectroscopic ellipsometry and X-ray absorption spectroscopy, that is further supported by theoretical calculations, the O2p-Ni3d orbital hybridization for x = 0.125 is found to be significantly enhanced, alongside a considerable weakening of its effective correlation U*.","These factors account for the absence of correlated plasmons and the high optical conductivity observed in LSNO (0.125).","Our findings highlight the significant impact of orbital hybridization on the electronic structures and the formation of quasiparticles in strongly correlated systems, thereby marking a notable advancement in plasmonics, emphasizing LSNO's compelling potential as a substitute material in optoelectronic devices."],"url":"http://arxiv.org/abs/2404.19193v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 01:41:03","title":"Mix of Experts Language Model for Named Entity Recognition","abstract":"Named Entity Recognition (NER) is an essential steppingstone in the field of natural language processing. Although promising performance has been achieved by various distantly supervised models, we argue that distant supervision inevitably introduces incomplete and noisy annotations, which may mislead the model training process. To address this issue, we propose a robust NER model named BOND-MoE based on Mixture of Experts (MoE). Instead of relying on a single model for NER prediction, multiple models are trained and ensembled under the Expectation-Maximization (EM) framework, so that noisy supervision can be dramatically alleviated. In addition, we introduce a fair assignment module to balance the document-model assignment process. Extensive experiments on real-world datasets show that the proposed method achieves state-of-the-art performance compared with other distantly supervised NER.","sentences":["Named Entity Recognition (NER) is an essential steppingstone in the field of natural language processing.","Although promising performance has been achieved by various distantly supervised models, we argue that distant supervision inevitably introduces incomplete and noisy annotations, which may mislead the model training process.","To address this issue, we propose a robust NER model named BOND-MoE based on Mixture of Experts (MoE).","Instead of relying on a single model for NER prediction, multiple models are trained and ensembled under the Expectation-Maximization (EM) framework, so that noisy supervision can be dramatically alleviated.","In addition, we introduce a fair assignment module to balance the document-model assignment process.","Extensive experiments on real-world datasets show that the proposed method achieves state-of-the-art performance compared with other distantly supervised NER."],"url":"http://arxiv.org/abs/2404.19192v1","category":"cs.CL"}
{"created":"2024-04-30 00:39:26","title":"Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset","abstract":"The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences' talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model's game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.","sentences":["The dynamic nature of esports makes the situation relatively complicated for average viewers.","Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation.","It will be richer by including diverse multimodal esports information, including audiences' talks/emotions, game audio, and game match event information.","This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline.","Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation.","In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline.","We examine the model's game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach."],"url":"http://arxiv.org/abs/2404.19175v1","category":"cs.CL"}
{"created":"2024-04-30 00:25:44","title":"Explicit Correlation Learning for Generalizable Cross-Modal Deepfake Detection","abstract":"With the rising prevalence of deepfakes, there is a growing interest in developing generalizable detection methods for various types of deepfakes. While effective in their specific modalities, traditional detection methods fall short in addressing the generalizability of detection across diverse cross-modal deepfakes. This paper aims to explicitly learn potential cross-modal correlation to enhance deepfake detection towards various generation scenarios. Our approach introduces a correlation distillation task, which models the inherent cross-modal correlation based on content information. This strategy helps to prevent the model from overfitting merely to audio-visual synchronization. Additionally, we present the Cross-Modal Deepfake Dataset (CMDFD), a comprehensive dataset with four generation methods to evaluate the detection of diverse cross-modal deepfakes. The experimental results on CMDFD and FakeAVCeleb datasets demonstrate the superior generalizability of our method over existing state-of-the-art methods. Our code and data can be found at \\url{https://github.com/ljj898/CMDFD-Dataset-and-Deepfake-Detection}.","sentences":["With the rising prevalence of deepfakes, there is a growing interest in developing generalizable detection methods for various types of deepfakes.","While effective in their specific modalities, traditional detection methods fall short in addressing the generalizability of detection across diverse cross-modal deepfakes.","This paper aims to explicitly learn potential cross-modal correlation to enhance deepfake detection towards various generation scenarios.","Our approach introduces a correlation distillation task, which models the inherent cross-modal correlation based on content information.","This strategy helps to prevent the model from overfitting merely to audio-visual synchronization.","Additionally, we present the Cross-Modal Deepfake Dataset (CMDFD), a comprehensive dataset with four generation methods to evaluate the detection of diverse cross-modal deepfakes.","The experimental results on CMDFD and FakeAVCeleb datasets demonstrate the superior generalizability of our method over existing state-of-the-art methods.","Our code and data can be found at \\url{https://github.com/ljj898/CMDFD-Dataset-and-Deepfake-Detection}."],"url":"http://arxiv.org/abs/2404.19171v1","category":"cs.CV"}
{"created":"2024-04-29 23:53:16","title":"Efficient Mixed-Precision Matrix Factorization of the Inverse Overlap Matrix in Electronic Structure Calculations with AI-Hardware and GPUs","abstract":"In recent years, a new kind of accelerated hardware has gained popularity in the Artificial Intelligence (AI) and Machine Learning (ML) communities which enables extremely high-performance tensor contractions in reduced precision for deep neural network calculations. In this article, we exploit Nvidia Tensor cores, a prototypical example of such AI/ML hardware, to develop a mixed precision approach for computing a dense matrix factorization of the inverse overlap matrix in electronic structure theory, $S^{-1}$. This factorization of $S^{-1}$, written as $ZZ^T=S^{-1}$, is used to transform the general matrix eigenvalue problem into a standard matrix eigenvalue problem. Here we present a mixed precision iterative refinement algorithm where $Z$ is given recursively using matrix-matrix multiplications and can be computed with high performance on Tensor cores. To understand the performance and accuracy of Tensor cores, comparisons are made to GPU-only implementations in single and double precision. Additionally, we propose a non-parametric stopping criteria which is robust in the face of lower precision floating point operations. The algorithm is particularly useful when we have a good initial guess to $Z$, for example, from previous time steps in quantum-mechanical molecular dynamics simulations or from a previous iteration in a geometry optimization.","sentences":["In recent years, a new kind of accelerated hardware has gained popularity in the Artificial Intelligence (AI) and Machine Learning (ML) communities which enables extremely high-performance tensor contractions in reduced precision for deep neural network calculations.","In this article, we exploit Nvidia Tensor cores, a prototypical example of such AI/ML hardware, to develop a mixed precision approach for computing a dense matrix factorization of the inverse overlap matrix in electronic structure theory, $S^{-1}$. This factorization of $S^{-1}$, written as $ZZ^T=S^{-1}$, is used to transform the general matrix eigenvalue problem into a standard matrix eigenvalue problem.","Here we present a mixed precision iterative refinement algorithm where $Z$ is given recursively using matrix-matrix multiplications and can be computed with high performance on Tensor cores.","To understand the performance and accuracy of Tensor cores, comparisons are made to GPU-only implementations in single and double precision.","Additionally, we propose a non-parametric stopping criteria which is robust in the face of lower precision floating point operations.","The algorithm is particularly useful when we have a good initial guess to $Z$, for example, from previous time steps in quantum-mechanical molecular dynamics simulations or from a previous iteration in a geometry optimization."],"url":"http://arxiv.org/abs/2404.19163v1","category":"physics.comp-ph"}
{"created":"2024-04-29 23:14:14","title":"Automated Construction of Theme-specific Knowledge Graphs","abstract":"Despite widespread applications of knowledge graphs (KGs) in various tasks such as question answering and intelligent conversational systems, existing KGs face two major challenges: information granularity and deficiency in timeliness. These hinder considerably the retrieval and analysis of in-context, fine-grained, and up-to-date knowledge from KGs, particularly in highly specialized themes (e.g., specialized scientific research) and rapidly evolving contexts (e.g., breaking news or disaster tracking). To tackle such challenges, we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed from a theme-specific corpus, and design an unsupervised framework for ThemeKG construction (named TKGCon). The framework takes raw theme-specific corpus and generates a high-quality KG that includes salient entities and relations under the theme. Specifically, we start with an entity ontology of the theme from Wikipedia, based on which we then generate candidate relations by Large Language Models (LLMs) to construct a relation ontology. To parse the documents from the theme corpus, we first map the extracted entity pairs to the ontology and retrieve the candidate relations. Finally, we incorporate the context and ontology to consolidate the relations for entity pairs. We observe that directly prompting GPT-4 for theme-specific KG leads to inaccurate entities (such as \"two main types\" as one entity in the query result) and unclear (such as \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In contrast, by constructing the theme-specific KG step by step, our model outperforms GPT-4 and could consistently identify accurate entities and relations. Experimental results also show that our framework excels in evaluations compared with various KG construction baselines.","sentences":["Despite widespread applications of knowledge graphs (KGs) in various tasks such as question answering and intelligent conversational systems, existing KGs face two major challenges: information granularity and deficiency in timeliness.","These hinder considerably the retrieval and analysis of in-context, fine-grained, and up-to-date knowledge from KGs, particularly in highly specialized themes (e.g., specialized scientific research) and rapidly evolving contexts (e.g., breaking news or disaster tracking).","To tackle such challenges, we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed from a theme-specific corpus, and design an unsupervised framework for ThemeKG construction (named TKGCon).","The framework takes raw theme-specific corpus and generates a high-quality KG that includes salient entities and relations under the theme.","Specifically, we start with an entity ontology of the theme from Wikipedia, based on which we then generate candidate relations by Large Language Models (LLMs) to construct a relation ontology.","To parse the documents from the theme corpus, we first map the extracted entity pairs to the ontology and retrieve the candidate relations.","Finally, we incorporate the context and ontology to consolidate the relations for entity pairs.","We observe that directly prompting GPT-4 for theme-specific KG leads to inaccurate entities (such as \"two main types\" as one entity in the query result) and unclear (such as \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\").","In contrast, by constructing the theme-specific KG step by step, our model outperforms GPT-4 and could consistently identify accurate entities and relations.","Experimental results also show that our framework excels in evaluations compared with various KG construction baselines."],"url":"http://arxiv.org/abs/2404.19146v1","category":"cs.AI"}
{"created":"2024-04-29 23:01:03","title":"Workload Intelligence: Punching Holes Through the Cloud Abstraction","abstract":"Today, cloud workloads are essentially opaque to the cloud platform. Typically, the only information the platform receives is the virtual machine (VM) type and possibly a decoration to the type (e.g., the VM is evictable). Similarly, workloads receive little to no information from the platform; generally, workloads might receive telemetry from their VMs or exceptional signals (e.g., shortly before a VM is evicted). The narrow interface between workloads and platforms has several drawbacks: (1) a surge in VM types and decorations in public cloud platforms complicates customer selection; (2) essential workload characteristics (e.g., low availability requirements, high latency tolerance) are often unspecified, hindering platform customization for optimized resource usage and cost savings; and (3) workloads may be unaware of potential optimizations or lack sufficient time to react to platform events.   In this paper, we propose a framework, called Workload Intelligence (WI), for dynamic bi-directional communication between cloud workloads and cloud platform. Via WI, workloads can programmatically adjust their key characteristics, requirements, and even dynamically adapt behaviors like VM priorities. In the other direction, WI allows the platform to programmatically inform workloads about upcoming events, opportunities for optimization, among other scenarios. Because of WI, the cloud platform can drastically simplify its offerings, reduce its costs without fear of violating any workload requirements, and reduce prices to its customers on average by 48.8%.","sentences":["Today, cloud workloads are essentially opaque to the cloud platform.","Typically, the only information the platform receives is the virtual machine (VM) type and possibly a decoration to the type (e.g., the VM is evictable).","Similarly, workloads receive little to no information from the platform; generally, workloads might receive telemetry from their VMs or exceptional signals (e.g., shortly before a VM is evicted).","The narrow interface between workloads and platforms has several drawbacks: (1) a surge in VM types and decorations in public cloud platforms complicates customer selection; (2) essential workload characteristics (e.g., low availability requirements, high latency tolerance) are often unspecified, hindering platform customization for optimized resource usage and cost savings; and (3) workloads may be unaware of potential optimizations or lack sufficient time to react to platform events.   ","In this paper, we propose a framework, called Workload Intelligence (WI), for dynamic bi-directional communication between cloud workloads and cloud platform.","Via WI, workloads can programmatically adjust their key characteristics, requirements, and even dynamically adapt behaviors like VM priorities.","In the other direction, WI allows the platform to programmatically inform workloads about upcoming events, opportunities for optimization, among other scenarios.","Because of WI, the cloud platform can drastically simplify its offerings, reduce its costs without fear of violating any workload requirements, and reduce prices to its customers on average by 48.8%."],"url":"http://arxiv.org/abs/2404.19143v1","category":"cs.DC"}
{"created":"2024-04-29 22:54:35","title":"Micro-Macro Spatial-Temporal Graph-based Encoder-Decoder for Map-Constrained Trajectory Recovery","abstract":"Recovering intermediate missing GPS points in a sparse trajectory, while adhering to the constraints of the road network, could offer deep insights into users' moving behaviors in intelligent transportation systems. Although recent studies have demonstrated the advantages of achieving map-constrained trajectory recovery via an end-to-end manner, they still face two significant challenges. Firstly, existing methods are mostly sequence-based models. It is extremely hard for them to comprehensively capture the micro-semantics of individual trajectory, including the information of each GPS point and the movement between two GPS points. Secondly, existing approaches ignore the impact of the macro-semantics, i.e., the road conditions and the people's shared travel preferences reflected by a group of trajectories. To address the above challenges, we propose a Micro-Macro Spatial-Temporal Graph-based Encoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph to efficiently describe the micro-semantics of trajectory and design a novel message-passing mechanism to learn trajectory representations. Additionally, we extract the macro-semantics of trajectories and further incorporate them into a well-designed graph-based decoder to guide trajectory recovery. Extensive experiments conducted on sparse trajectories with three different sampling intervals that are respectively constructed from two real-world trajectory datasets demonstrate the superiority of our proposed model.","sentences":["Recovering intermediate missing GPS points in a sparse trajectory, while adhering to the constraints of the road network, could offer deep insights into users' moving behaviors in intelligent transportation systems.","Although recent studies have demonstrated the advantages of achieving map-constrained trajectory recovery via an end-to-end manner, they still face two significant challenges.","Firstly, existing methods are mostly sequence-based models.","It is extremely hard for them to comprehensively capture the micro-semantics of individual trajectory, including the information of each GPS point and the movement between two GPS points.","Secondly, existing approaches ignore the impact of the macro-semantics, i.e., the road conditions and the people's shared travel preferences reflected by a group of trajectories.","To address the above challenges, we propose a Micro-Macro Spatial-Temporal Graph-based Encoder-Decoder (MM-STGED).","Specifically, we model each trajectory as a graph to efficiently describe the micro-semantics of trajectory and design a novel message-passing mechanism to learn trajectory representations.","Additionally, we extract the macro-semantics of trajectories and further incorporate them into a well-designed graph-based decoder to guide trajectory recovery.","Extensive experiments conducted on sparse trajectories with three different sampling intervals that are respectively constructed from two real-world trajectory datasets demonstrate the superiority of our proposed model."],"url":"http://arxiv.org/abs/2404.19141v1","category":"cs.LG"}
{"created":"2024-04-29 22:39:55","title":"Evaluating Deep Clustering Algorithms on Non-Categorical 3D CAD Models","abstract":"We introduce the first work on benchmarking and evaluating deep clustering algorithms on large-scale non-categorical 3D CAD models. We first propose a workflow to allow expert mechanical engineers to efficiently annotate 252,648 carefully sampled pairwise CAD model similarities, from a subset of the ABC dataset with 22,968 shapes. Using seven baseline deep clustering methods, we then investigate the fundamental challenges of evaluating clustering methods for non-categorical data. Based on these challenges, we propose a novel and viable ensemble-based clustering comparison approach. This work is the first to directly target the underexplored area of deep clustering algorithms for 3D shapes, and we believe it will be an important building block to analyze and utilize the massive 3D shape collections that are starting to appear in deep geometric computing.","sentences":["We introduce the first work on benchmarking and evaluating deep clustering algorithms on large-scale non-categorical 3D CAD models.","We first propose a workflow to allow expert mechanical engineers to efficiently annotate 252,648 carefully sampled pairwise CAD model similarities, from a subset of the ABC dataset with 22,968 shapes.","Using seven baseline deep clustering methods, we then investigate the fundamental challenges of evaluating clustering methods for non-categorical data.","Based on these challenges, we propose a novel and viable ensemble-based clustering comparison approach.","This work is the first to directly target the underexplored area of deep clustering algorithms for 3D shapes, and we believe it will be an important building block to analyze and utilize the massive 3D shape collections that are starting to appear in deep geometric computing."],"url":"http://arxiv.org/abs/2404.19134v1","category":"cs.CV"}
{"created":"2024-04-29 22:21:24","title":"SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval","abstract":"Knowledge graphs (KGs), which store an extensive number of relational facts (head, relation, tail), serve various applications. While many downstream tasks highly rely on the expressive modeling and predictive embedding of KGs, most of the current KG representation learning methods, where each entity is embedded as a vector in the Euclidean space and each relation is embedded as a transformation, follow an entity ranking protocol. On one hand, such an embedding design cannot capture many-to-many relations. On the other hand, in many retrieval cases, the users wish to get an exact set of answers without any ranking, especially when the results are expected to be precise, e.g., which genes cause an illness. Such scenarios are commonly referred to as \"set retrieval\". This work presents a pioneering study on the KG set retrieval problem. We show that the set retrieval highly depends on expressive modeling of many-to-many relations, and propose a new KG embedding model SpherE to address this problem. SpherE is based on rotational embedding methods, but each entity is embedded as a sphere instead of a vector. While inheriting the high interpretability of rotational-based models, our SpherE can more expressively model one-to-many, many-to-one, and many-to-many relations. Through extensive experiments, we show that our SpherE can well address the set retrieval problem while still having a good predictive ability to infer missing facts. The code is available at https://github.com/Violet24K/SpherE.","sentences":["Knowledge graphs (KGs), which store an extensive number of relational facts (head, relation, tail), serve various applications.","While many downstream tasks highly rely on the expressive modeling and predictive embedding of KGs, most of the current KG representation learning methods, where each entity is embedded as a vector in the Euclidean space and each relation is embedded as a transformation, follow an entity ranking protocol.","On one hand, such an embedding design cannot capture many-to-many relations.","On the other hand, in many retrieval cases, the users wish to get an exact set of answers without any ranking, especially when the results are expected to be precise, e.g., which genes cause an illness.","Such scenarios are commonly referred to as \"set retrieval\".","This work presents a pioneering study on the KG set retrieval problem.","We show that the set retrieval highly depends on expressive modeling of many-to-many relations, and propose a new KG embedding model SpherE to address this problem.","SpherE is based on rotational embedding methods, but each entity is embedded as a sphere instead of a vector.","While inheriting the high interpretability of rotational-based models, our SpherE can more expressively model one-to-many, many-to-one, and many-to-many relations.","Through extensive experiments, we show that our SpherE can well address the set retrieval problem while still having a good predictive ability to infer missing facts.","The code is available at https://github.com/Violet24K/SpherE."],"url":"http://arxiv.org/abs/2404.19130v1","category":"cs.IR"}
{"created":"2024-04-29 20:45:05","title":"Neutral hydrogen filaments in interstellar media: Are they physical?","abstract":"The trending term \"filament\" is extensively used in the interstellar medium (ISM) and the star formation community, and is believed to be one of the most important objects that gauge molecular cloud and star formation. However, the physical definition of these ubiquitous, elongated, high contrast features is poorly defined and still actively debated. Despite the absence of a unified consensus, filaments are believed to be involved in many important physical processes from galaxy structure formation to the emergence of protostellar objects. Therefore, understanding how filaments form, what constrains their growth, and their general physical properties, are extremely important for theorists and observers who study the dynamics of the ISM and consequent star formations. This review serves as a collection of the community's views and develops the concept of \"filaments\" in the context of the ISM and star-forming clouds. Observationally, filaments are seen across the entire sky and often carry an aspect ratio of the order of hundreds. In the context of the ISM, filaments are believed to form by stretching and tearing from magnetized ISM turbulence. ISM filaments are subjected to heating and cooling phases, and are likely to be magnetically aligned. Cold clouds are formed inside ISM due to turbulence instability. This review updates the understanding of ISM filaments in the community.","sentences":["The trending term \"filament\" is extensively used in the interstellar medium (ISM) and the star formation community, and is believed to be one of the most important objects that gauge molecular cloud and star formation.","However, the physical definition of these ubiquitous, elongated, high contrast features is poorly defined and still actively debated.","Despite the absence of a unified consensus, filaments are believed to be involved in many important physical processes from galaxy structure formation to the emergence of protostellar objects.","Therefore, understanding how filaments form, what constrains their growth, and their general physical properties, are extremely important for theorists and observers who study the dynamics of the ISM and consequent star formations.","This review serves as a collection of the community's views and develops the concept of \"filaments\" in the context of the ISM and star-forming clouds.","Observationally, filaments are seen across the entire sky and often carry an aspect ratio of the order of hundreds.","In the context of the ISM, filaments are believed to form by stretching and tearing from magnetized ISM turbulence.","ISM filaments are subjected to heating and cooling phases, and are likely to be magnetically aligned.","Cold clouds are formed inside ISM due to turbulence instability.","This review updates the understanding of ISM filaments in the community."],"url":"http://arxiv.org/abs/2404.19101v1","category":"astro-ph.GA"}
{"created":"2024-04-29 20:43:42","title":"Predicting Fairness of ML Software Configuration","abstract":"This paper investigates the relationships between hyperparameters of machine learning and fairness. Data-driven solutions are increasingly used in critical socio-technical applications where ensuring fairness is important. Rather than explicitly encoding decision logic via control and data structures, the ML developers provide input data, perform some pre-processing, choose ML algorithms, and tune hyperparameters (HPs) to infer a program that encodes the decision logic. Prior works report that the selection of HPs can significantly influence fairness. However, tuning HPs to find an ideal trade-off between accuracy, precision, and fairness has remained an expensive and tedious task. Can we predict fairness of HP configuration for a given dataset? Are the predictions robust to distribution shifts?   We focus on group fairness notions and investigate the HP space of 5 training algorithms. We first find that tree regressors and XGBoots significantly outperformed deep neural networks and support vector machines in accurately predicting the fairness of HPs. When predicting the fairness of ML hyperparameters under temporal distribution shift, the tree regressors outperforms the other algorithms with reasonable accuracy. However, the precision depends on the ML training algorithm, dataset, and protected attributes. For example, the tree regressor model was robust for training data shift from 2014 to 2018 on logistic regression and discriminant analysis HPs with sex as the protected attribute; but not for race and other training algorithms. Our method provides a sound framework to efficiently perform fine-tuning of ML training algorithms and understand the relationships between HPs and fairness.","sentences":["This paper investigates the relationships between hyperparameters of machine learning and fairness.","Data-driven solutions are increasingly used in critical socio-technical applications where ensuring fairness is important.","Rather than explicitly encoding decision logic via control and data structures, the ML developers provide input data, perform some pre-processing, choose ML algorithms, and tune hyperparameters (HPs) to infer a program that encodes the decision logic.","Prior works report that the selection of HPs can significantly influence fairness.","However, tuning HPs to find an ideal trade-off between accuracy, precision, and fairness has remained an expensive and tedious task.","Can we predict fairness of HP configuration for a given dataset?","Are the predictions robust to distribution shifts?   ","We focus on group fairness notions and investigate the HP space of 5 training algorithms.","We first find that tree regressors and XGBoots significantly outperformed deep neural networks and support vector machines in accurately predicting the fairness of HPs.","When predicting the fairness of ML hyperparameters under temporal distribution shift, the tree regressors outperforms the other algorithms with reasonable accuracy.","However, the precision depends on the ML training algorithm, dataset, and protected attributes.","For example, the tree regressor model was robust for training data shift from 2014 to 2018 on logistic regression and discriminant analysis HPs with sex as the protected attribute; but not for race and other training algorithms.","Our method provides a sound framework to efficiently perform fine-tuning of ML training algorithms and understand the relationships between HPs and fairness."],"url":"http://arxiv.org/abs/2404.19100v1","category":"cs.SE"}
{"created":"2024-04-29 20:19:35","title":"Catalyzing Social Interactions in Mixed Reality using ML Recommendation Systems","abstract":"We create an innovative mixed reality-first social recommendation model, utilizing features uniquely collected through mixed reality (MR) systems to promote social interaction, such as gaze recognition, proximity, noise level, congestion level, and conversational intensity. We further extend these models to include right-time features to deliver timely notifications. We measure performance metrics across various models by creating a new intersection of user features, MR features, and right-time features. We create four model types trained on different combinations of the feature classes, where we compare the baseline model trained on the class of user features against the models trained on MR features, right-time features, and a combination of all of the feature classes. Due to limitations in data collection and cost, we observe performance degradation in the right-time, mixed reality, and combination models. Despite these challenges, we introduce optimizations to improve accuracy across all models by over 14 percentage points, where the best performing model achieved 24% greater accuracy.","sentences":["We create an innovative mixed reality-first social recommendation model, utilizing features uniquely collected through mixed reality (MR) systems to promote social interaction, such as gaze recognition, proximity, noise level, congestion level, and conversational intensity.","We further extend these models to include right-time features to deliver timely notifications.","We measure performance metrics across various models by creating a new intersection of user features, MR features, and right-time features.","We create four model types trained on different combinations of the feature classes, where we compare the baseline model trained on the class of user features against the models trained on MR features, right-time features, and a combination of all of the feature classes.","Due to limitations in data collection and cost, we observe performance degradation in the right-time, mixed reality, and combination models.","Despite these challenges, we introduce optimizations to improve accuracy across all models by over 14 percentage points, where the best performing model achieved 24% greater accuracy."],"url":"http://arxiv.org/abs/2404.19095v1","category":"cs.HC"}
{"created":"2024-04-29 20:17:06","title":"Large Language Models as Conversational Movie Recommenders: A User Study","abstract":"This paper explores the effectiveness of using large language models (LLMs) for personalized movie recommendations from users' perspectives in an online field experiment. Our study involves a combination of between-subject prompt and historic consumption assessments, along with within-subject recommendation scenario evaluations. By examining conversation and survey response data from 160 active users, we find that LLMs offer strong recommendation explainability but lack overall personalization, diversity, and user trust. Our results also indicate that different personalized prompting techniques do not significantly affect user-perceived recommendation quality, but the number of movies a user has watched plays a more significant role. Furthermore, LLMs show a greater ability to recommend lesser-known or niche movies. Through qualitative analysis, we identify key conversational patterns linked to positive and negative user interaction experiences and conclude that providing personal context and examples is crucial for obtaining high-quality recommendations from LLMs.","sentences":["This paper explores the effectiveness of using large language models (LLMs) for personalized movie recommendations from users' perspectives in an online field experiment.","Our study involves a combination of between-subject prompt and historic consumption assessments, along with within-subject recommendation scenario evaluations.","By examining conversation and survey response data from 160 active users, we find that LLMs offer strong recommendation explainability but lack overall personalization, diversity, and user trust.","Our results also indicate that different personalized prompting techniques do not significantly affect user-perceived recommendation quality, but the number of movies a user has watched plays a more significant role.","Furthermore, LLMs show a greater ability to recommend lesser-known or niche movies.","Through qualitative analysis, we identify key conversational patterns linked to positive and negative user interaction experiences and conclude that providing personal context and examples is crucial for obtaining high-quality recommendations from LLMs."],"url":"http://arxiv.org/abs/2404.19093v1","category":"cs.IR"}
{"created":"2024-04-29 19:58:34","title":"Deep Reinforcement Learning for Advanced Longitudinal Control and Collision Avoidance in High-Risk Driving Scenarios","abstract":"Existing Advanced Driver Assistance Systems primarily focus on the vehicle directly ahead, often overlooking potential risks from following vehicles. This oversight can lead to ineffective handling of high risk situations, such as high speed, closely spaced, multi vehicle scenarios where emergency braking by one vehicle might trigger a pile up collision. To overcome these limitations, this study introduces a novel deep reinforcement learning based algorithm for longitudinal control and collision avoidance. This proposed algorithm effectively considers the behavior of both leading and following vehicles. Its implementation in simulated high risk scenarios, which involve emergency braking in dense traffic where traditional systems typically fail, has demonstrated the algorithm ability to prevent potential pile up collisions, including those involving heavy duty vehicles.","sentences":["Existing Advanced Driver Assistance Systems primarily focus on the vehicle directly ahead, often overlooking potential risks from following vehicles.","This oversight can lead to ineffective handling of high risk situations, such as high speed, closely spaced, multi vehicle scenarios where emergency braking by one vehicle might trigger a pile up collision.","To overcome these limitations, this study introduces a novel deep reinforcement learning based algorithm for longitudinal control and collision avoidance.","This proposed algorithm effectively considers the behavior of both leading and following vehicles.","Its implementation in simulated high risk scenarios, which involve emergency braking in dense traffic where traditional systems typically fail, has demonstrated the algorithm ability to prevent potential pile up collisions, including those involving heavy duty vehicles."],"url":"http://arxiv.org/abs/2404.19087v1","category":"cs.RO"}
{"created":"2024-04-29 19:51:06","title":"Stability of Quantum Computers","abstract":"Quantum computing's potential is immense, promising super-polynomial reductions in execution time, energy use, and memory requirements compared to classical computers. This technology has the power to revolutionize scientific applications such as simulating many-body quantum systems for molecular structure understanding, factorization of large integers, enhance machine learning, and in the process, disrupt industries like telecommunications, material science, pharmaceuticals and artificial intelligence. However, quantum computing's potential is curtailed by noise, further complicated by non-stationary noise parameter distributions across time and qubits. This dissertation focuses on the persistent issue of noise in quantum computing, particularly non-stationarity of noise parameters in transmon processors.","sentences":["Quantum computing's potential is immense, promising super-polynomial reductions in execution time, energy use, and memory requirements compared to classical computers.","This technology has the power to revolutionize scientific applications such as simulating many-body quantum systems for molecular structure understanding, factorization of large integers, enhance machine learning, and in the process, disrupt industries like telecommunications, material science, pharmaceuticals and artificial intelligence.","However, quantum computing's potential is curtailed by noise, further complicated by non-stationary noise parameter distributions across time and qubits.","This dissertation focuses on the persistent issue of noise in quantum computing, particularly non-stationarity of noise parameters in transmon processors."],"url":"http://arxiv.org/abs/2404.19082v1","category":"quant-ph"}
{"created":"2024-04-29 19:43:10","title":"Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights","abstract":"This study examines the extent to which U.S. federal agencies responded to and implemented the principles outlined in the White House's October 2022 \"Blueprint for an AI Bill of Rights.\" The Blueprint provided a framework for the ethical governance of artificial intelligence systems, organized around five core principles: safety and effectiveness, protection against algorithmic discrimination, data privacy, notice and explanation about AI systems, and human alternatives and fallback.   Through an analysis of publicly available records across 15 federal departments, the authors found limited evidence that the Blueprint directly influenced agency actions after its release. Only five departments explicitly mentioned the Blueprint, while 12 took steps aligned with one or more of its principles. However, much of this work appeared to have precedents predating the Blueprint or motivations disconnected from it, such as compliance with prior executive orders on trustworthy AI. Departments' activities often emphasized priorities like safety, accountability and transparency that overlapped with Blueprint principles, but did not necessarily stem from it.   The authors conclude that the non-binding Blueprint seems to have had minimal impact on shaping the U.S. government's approach to ethical AI governance in its first year. Factors like public concerns after high-profile AI releases and obligations to follow direct executive orders likely carried more influence over federal agencies. More rigorous study would be needed to definitively assess the Blueprint's effects within the federal bureaucracy and broader society.","sentences":["This study examines the extent to which U.S. federal agencies responded to and implemented the principles outlined in the White House's October 2022 \"Blueprint for an AI Bill of Rights.\"","The Blueprint provided a framework for the ethical governance of artificial intelligence systems, organized around five core principles: safety and effectiveness, protection against algorithmic discrimination, data privacy, notice and explanation about AI systems, and human alternatives and fallback.   ","Through an analysis of publicly available records across 15 federal departments, the authors found limited evidence that the Blueprint directly influenced agency actions after its release.","Only five departments explicitly mentioned the Blueprint, while 12 took steps aligned with one or more of its principles.","However, much of this work appeared to have precedents predating the Blueprint or motivations disconnected from it, such as compliance with prior executive orders on trustworthy AI.","Departments' activities often emphasized priorities like safety, accountability and transparency that overlapped with Blueprint principles, but did not necessarily stem from it.   ","The authors conclude that the non-binding Blueprint seems to have had minimal impact on shaping the U.S. government's approach to ethical AI governance in its first year.","Factors like public concerns after high-profile AI releases and obligations to follow direct executive orders likely carried more influence over federal agencies.","More rigorous study would be needed to definitively assess the Blueprint's effects within the federal bureaucracy and broader society."],"url":"http://arxiv.org/abs/2404.19076v1","category":"cs.CY"}
{"created":"2024-04-29 19:41:51","title":"Distributed Stochastic Optimization of a Neural Representation Network for Time-Space Tomography Reconstruction","abstract":"4D time-space reconstruction of dynamic events or deforming objects using X-ray computed tomography (CT) is an extremely ill-posed inverse problem. Existing approaches assume that the object remains static for the duration of several tens or hundreds of X-ray projection measurement images (reconstruction of consecutive limited-angle CT scans). However, this is an unrealistic assumption for many in-situ experiments that causes spurious artifacts and inaccurate morphological reconstructions of the object. To solve this problem, we propose to perform a 4D time-space reconstruction using a distributed implicit neural representation (DINR) network that is trained using a novel distributed stochastic training algorithm. Our DINR network learns to reconstruct the object at its output by iterative optimization of its network parameters such that the measured projection images best match the output of the CT forward measurement model. We use a continuous time and space forward measurement model that is a function of the DINR outputs at a sparsely sampled set of continuous valued object coordinates. Unlike existing state-of-the-art neural representation architectures that forward and back propagate through dense voxel grids that sample the object's entire time-space coordinates, we only propagate through the DINR at a small subset of object coordinates in each iteration resulting in an order-of-magnitude reduction in memory and compute for training. DINR leverages distributed computation across several compute nodes and GPUs to produce high-fidelity 4D time-space reconstructions even for extremely large CT data sizes. We use both simulated parallel-beam and experimental cone-beam X-ray CT datasets to demonstrate the superior performance of our approach.","sentences":["4D time-space reconstruction of dynamic events or deforming objects using X-ray computed tomography (CT) is an extremely ill-posed inverse problem.","Existing approaches assume that the object remains static for the duration of several tens or hundreds of X-ray projection measurement images (reconstruction of consecutive limited-angle CT scans).","However, this is an unrealistic assumption for many in-situ experiments that causes spurious artifacts and inaccurate morphological reconstructions of the object.","To solve this problem, we propose to perform a 4D time-space reconstruction using a distributed implicit neural representation (DINR) network that is trained using a novel distributed stochastic training algorithm.","Our DINR network learns to reconstruct the object at its output by iterative optimization of its network parameters such that the measured projection images best match the output of the CT forward measurement model.","We use a continuous time and space forward measurement model that is a function of the DINR outputs at a sparsely sampled set of continuous valued object coordinates.","Unlike existing state-of-the-art neural representation architectures that forward and back propagate through dense voxel grids that sample the object's entire time-space coordinates, we only propagate through the DINR at a small subset of object coordinates in each iteration resulting in an order-of-magnitude reduction in memory and compute for training.","DINR leverages distributed computation across several compute nodes and GPUs to produce high-fidelity 4D time-space reconstructions even for extremely large CT data sizes.","We use both simulated parallel-beam and experimental cone-beam X-ray CT datasets to demonstrate the superior performance of our approach."],"url":"http://arxiv.org/abs/2404.19075v1","category":"eess.IV"}
{"created":"2024-04-29 19:28:35","title":"Blind Spots and Biases: Exploring the Role of Annotator Cognitive Biases in NLP","abstract":"With the rapid proliferation of artificial intelligence, there is growing concern over its potential to exacerbate existing biases and societal disparities and introduce novel ones. This issue has prompted widespread attention from academia, policymakers, industry, and civil society. While evidence suggests that integrating human perspectives can mitigate bias-related issues in AI systems, it also introduces challenges associated with cognitive biases inherent in human decision-making. Our research focuses on reviewing existing methodologies and ongoing investigations aimed at understanding annotation attributes that contribute to bias.","sentences":["With the rapid proliferation of artificial intelligence, there is growing concern over its potential to exacerbate existing biases and societal disparities and introduce novel ones.","This issue has prompted widespread attention from academia, policymakers, industry, and civil society.","While evidence suggests that integrating human perspectives can mitigate bias-related issues in AI systems, it also introduces challenges associated with cognitive biases inherent in human decision-making.","Our research focuses on reviewing existing methodologies and ongoing investigations aimed at understanding annotation attributes that contribute to bias."],"url":"http://arxiv.org/abs/2404.19071v1","category":"cs.HC"}
{"created":"2024-04-29 19:12:42","title":"HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models","abstract":"Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans. In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions. This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization. We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.","sentences":["Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans.","In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions.","This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization.","We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task.","HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training."],"url":"http://arxiv.org/abs/2404.19065v1","category":"cs.AI"}
{"created":"2024-04-29 19:04:17","title":"Self-reverting vortices in chiral active matter","abstract":"There is currently a strong interest in the collective behavior of chiral active particles that can propel and rotate themselves. In the presence of alignment interactions for many chiral particles, chiral self-propulsion can induce vortex patterns in the velocity fields. However, these emerging patterns are non-permanent, and do not induce global vorticity. Here we combine theoretical arguments and computer simulations to predict a so-far unknown class of collective behavior. We show that, for chiral active particles, vortices with significant dynamical coherence emerge spontaneously. They originate from the interplay between attraction interactions and chirality in the absence of alignment interactions. Depending on parameters, the vortices can either feature a constant vorticity or a vorticity that oscillates periodically in time, resulting in self-reverting vortices. Our results may guide future experiments to realize customized collective phenomena such as spontaneously rotating gears and patterns with a self-reverting order.","sentences":["There is currently a strong interest in the collective behavior of chiral active particles that can propel and rotate themselves.","In the presence of alignment interactions for many chiral particles, chiral self-propulsion can induce vortex patterns in the velocity fields.","However, these emerging patterns are non-permanent, and do not induce global vorticity.","Here we combine theoretical arguments and computer simulations to predict a so-far unknown class of collective behavior.","We show that, for chiral active particles, vortices with significant dynamical coherence emerge spontaneously.","They originate from the interplay between attraction interactions and chirality in the absence of alignment interactions.","Depending on parameters, the vortices can either feature a constant vorticity or a vorticity that oscillates periodically in time, resulting in self-reverting vortices.","Our results may guide future experiments to realize customized collective phenomena such as spontaneously rotating gears and patterns with a self-reverting order."],"url":"http://arxiv.org/abs/2404.19062v1","category":"cond-mat.soft"}
{"created":"2024-04-29 19:01:55","title":"Modular, Hierarchical Machine Learning for Sequential Goal Completion","abstract":"Given a maze populated with different objects, one may task a robot with a sequential goal completion task, e.g. 1) pick up a key then 2) unlock the door then 3) unlock the treasure chest. A typical machine learning (ML) solution would involve a monolithically trained artificial neural network (ANN). However, if the sequence of goals or the goals themselves change, then the ANN must be significantly (or, at worst, completely) retrained. Instead of a monolithic ANN, a modular ML component would be 1) independently optimizable (task-agnostic) and 2) arbitrarily reconfigurable with other ML modules. This work describes a modular, hierarchical ML framework by integrating two emerging ML techniques: 1) cognitive map learners (CML) and 2) hyperdimensional computing (HDC). A CML is a collection of three single layer ANNs (matrices) collaboratively trained to learn the topology of an abstract graph. Here, two CMLs were constructed, one describing locations on in 2D physical space and the other the relative distribution of objects found in this space. Each CML node states was encoded as a high-dimensional vector to utilize HDC, an ML algebra, for symbolic reasoning over these high-dimensional symbol vectors. In this way, each sub-goal above was described by algebraic equations of CML node states. Multiple, independently trained CMLs were subsequently assembled together to navigate a maze to solve a sequential goal task. Critically, changes to these goals required only localized changes in the CML-HDC architecture, as opposed to a global ANN retraining scheme. This framework therefore enabled a more traditional engineering approach to ML, akin to digital logic design.","sentences":["Given a maze populated with different objects, one may task a robot with a sequential goal completion task, e.g. 1) pick up a key then 2) unlock the door then 3) unlock the treasure chest.","A typical machine learning (ML) solution would involve a monolithically trained artificial neural network (ANN).","However, if the sequence of goals or the goals themselves change, then the ANN must be significantly (or, at worst, completely) retrained.","Instead of a monolithic ANN, a modular ML component would be 1) independently optimizable (task-agnostic) and 2) arbitrarily reconfigurable with other ML modules.","This work describes a modular, hierarchical ML framework by integrating two emerging ML techniques: 1) cognitive map learners (CML) and 2) hyperdimensional computing (HDC).","A CML is a collection of three single layer ANNs (matrices) collaboratively trained to learn the topology of an abstract graph.","Here, two CMLs were constructed, one describing locations on in 2D physical space and the other the relative distribution of objects found in this space.","Each CML node states was encoded as a high-dimensional vector to utilize HDC, an ML algebra, for symbolic reasoning over these high-dimensional symbol vectors.","In this way, each sub-goal above was described by algebraic equations of CML node states.","Multiple, independently trained CMLs were subsequently assembled together to navigate a maze to solve a sequential goal task.","Critically, changes to these goals required only localized changes in the CML-HDC architecture, as opposed to a global ANN retraining scheme.","This framework therefore enabled a more traditional engineering approach to ML, akin to digital logic design."],"url":"http://arxiv.org/abs/2404.19060v1","category":"cs.NE"}
{"created":"2024-04-29 18:45:51","title":"Assembling Modular, Hierarchical Cognitive Map Learners with Hyperdimensional Computing","abstract":"Cognitive map learners (CML) are a collection of separate yet collaboratively trained single-layer artificial neural networks (matrices), which navigate an abstract graph by learning internal representations of the node states, edge actions, and edge action availabilities. A consequence of this atypical segregation of information is that the CML performs near-optimal path planning between any two graph node states. However, the CML does not learn when or why to transition from one node to another. This work created CMLs with node states expressed as high dimensional vectors consistent with hyperdimensional computing (HDC), a form of symbolic machine learning (ML). This work evaluated HDC-based CMLs as ML modules, capable of receiving external inputs and computing output responses which are semantically meaningful for other HDC-based modules. Several CMLs were prepared independently then repurposed to solve the Tower of Hanoi puzzle without retraining these CMLs and without explicit reference to their respective graph topologies. This work suggests a template for building levels of biologically plausible cognitive abstraction and orchestration.","sentences":["Cognitive map learners (CML) are a collection of separate yet collaboratively trained single-layer artificial neural networks (matrices), which navigate an abstract graph by learning internal representations of the node states, edge actions, and edge action availabilities.","A consequence of this atypical segregation of information is that the CML performs near-optimal path planning between any two graph node states.","However, the CML does not learn when or why to transition from one node to another.","This work created CMLs with node states expressed as high dimensional vectors consistent with hyperdimensional computing (HDC), a form of symbolic machine learning (ML).","This work evaluated HDC-based CMLs as ML modules, capable of receiving external inputs and computing output responses which are semantically meaningful for other HDC-based modules.","Several CMLs were prepared independently then repurposed to solve the Tower of Hanoi puzzle without retraining these CMLs and without explicit reference to their respective graph topologies.","This work suggests a template for building levels of biologically plausible cognitive abstraction and orchestration."],"url":"http://arxiv.org/abs/2404.19051v1","category":"cs.NE"}
{"created":"2024-04-29 18:40:01","title":"A Framework for Real-time Safeguarding the Text Generation of Large Language","abstract":"Large Language Models (LLMs) have significantly advanced natural language processing (NLP) tasks but also pose ethical and societal risks due to their propensity to generate harmful content. To address this, various approaches have been developed to safeguard LLMs from producing unsafe content. However, existing methods have limitations, including the need for training specific control models and proactive intervention during text generation, that lead to quality degradation and increased computational overhead. To mitigate those limitations, we propose LLMSafeGuard, a lightweight framework to safeguard LLM text generation in real-time. LLMSafeGuard integrates an external validator into the beam search algorithm during decoding, rejecting candidates that violate safety constraints while allowing valid ones to proceed. We introduce a similarity based validation approach, simplifying constraint introduction and eliminating the need for control model training. Additionally, LLMSafeGuard employs a context-wise timing selection strategy, intervening LLMs only when necessary. We evaluate LLMSafe-Guard on two tasks, detoxification and copyright safeguarding, and demonstrate its superior performance over SOTA baselines. For instance, LLMSafeGuard reduces the average toxic score of. LLM output by 29.7% compared to the best baseline meanwhile preserving similar linguistic quality as natural output in detoxification task. Similarly, in the copyright task, LLMSafeGuard decreases the Longest Common Subsequence (LCS) by 56.2% compared to baselines. Moreover, our context-wise timing selection strategy reduces inference time by at least 24% meanwhile maintaining comparable effectiveness as validating each time step. LLMSafeGuard also offers tunable parameters to balance its effectiveness and efficiency.","sentences":["Large Language Models (LLMs) have significantly advanced natural language processing (NLP) tasks but also pose ethical and societal risks due to their propensity to generate harmful content.","To address this, various approaches have been developed to safeguard LLMs from producing unsafe content.","However, existing methods have limitations, including the need for training specific control models and proactive intervention during text generation, that lead to quality degradation and increased computational overhead.","To mitigate those limitations, we propose LLMSafeGuard, a lightweight framework to safeguard LLM text generation in real-time.","LLMSafeGuard integrates an external validator into the beam search algorithm during decoding, rejecting candidates that violate safety constraints while allowing valid ones to proceed.","We introduce a similarity based validation approach, simplifying constraint introduction and eliminating the need for control model training.","Additionally, LLMSafeGuard employs a context-wise timing selection strategy, intervening LLMs only when necessary.","We evaluate LLMSafe-Guard on two tasks, detoxification and copyright safeguarding, and demonstrate its superior performance over SOTA baselines.","For instance, LLMSafeGuard reduces the average toxic score of.","LLM output by 29.7% compared to the best baseline meanwhile preserving similar linguistic quality as natural output in detoxification task.","Similarly, in the copyright task, LLMSafeGuard decreases the Longest Common Subsequence (LCS) by 56.2% compared to baselines.","Moreover, our context-wise timing selection strategy reduces inference time by at least 24% meanwhile maintaining comparable effectiveness as validating each time step.","LLMSafeGuard also offers tunable parameters to balance its effectiveness and efficiency."],"url":"http://arxiv.org/abs/2404.19048v1","category":"cs.CL"}
{"created":"2024-04-29 18:35:45","title":"Maritime Vessel Tank Inspection using Aerial Robots: Experience from the field and dataset release","abstract":"This paper presents field results and lessons learned from the deployment of aerial robots inside ship ballast tanks. Vessel tanks including ballast tanks and cargo holds present dark, dusty environments having simultaneously very narrow openings and wide open spaces that create several challenges for autonomous navigation and inspection operations. We present a system for vessel tank inspection using an aerial robot along with its autonomy modules. We show the results of autonomous exploration and visual inspection in 3 ships spanning across 7 distinct types of sections of the ballast tanks. Additionally, we comment on the lessons learned from the field and possible directions for future work. Finally, we release a dataset consisting of the data from these missions along with data collected with a handheld sensor stick.","sentences":["This paper presents field results and lessons learned from the deployment of aerial robots inside ship ballast tanks.","Vessel tanks including ballast tanks and cargo holds present dark, dusty environments having simultaneously very narrow openings and wide open spaces that create several challenges for autonomous navigation and inspection operations.","We present a system for vessel tank inspection using an aerial robot along with its autonomy modules.","We show the results of autonomous exploration and visual inspection in 3 ships spanning across 7 distinct types of sections of the ballast tanks.","Additionally, we comment on the lessons learned from the field and possible directions for future work.","Finally, we release a dataset consisting of the data from these missions along with data collected with a handheld sensor stick."],"url":"http://arxiv.org/abs/2404.19045v1","category":"cs.RO"}
{"created":"2024-04-29 18:24:55","title":"Embedded Representation Learning Network for Animating Styled Video Portrait","abstract":"The talking head generation recently attracted considerable attention due to its widespread application prospects, especially for digital avatars and 3D animation design. Inspired by this practical demand, several works explored Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these methods based on NeRF face two challenges: (1) Difficulty in generating style-controllable talking heads. (2) Displacement artifacts around the neck in rendered images. To overcome these two challenges, we propose a novel generative paradigm \\textit{Embedded Representation Learning Network} (ERLNet) with two learning stages. First, the \\textit{ audio-driven FLAME} (ADF) module is constructed to produce facial expression and head pose sequences synchronized with content audio and style video. Second, given the sequence deduced by the ADF, one novel \\textit{dual-branch fusion NeRF} (DBF-NeRF) explores these contents to render the final images. Extensive empirical studies demonstrate that the collaboration of these two stages effectively facilitates our method to render a more realistic talking head than the existing algorithms.","sentences":["The talking head generation recently attracted considerable attention due to its widespread application prospects, especially for digital avatars and 3D animation design.","Inspired by this practical demand, several works explored Neural Radiance Fields (NeRF) to synthesize the talking heads.","However, these methods based on NeRF face two challenges: (1) Difficulty in generating style-controllable talking heads.","(2) Displacement artifacts around the neck in rendered images.","To overcome these two challenges, we propose a novel generative paradigm \\textit{Embedded Representation Learning Network} (ERLNet) with two learning stages.","First, the \\textit{ audio-driven FLAME} (ADF) module is constructed to produce facial expression and head pose sequences synchronized with content audio and style video.","Second, given the sequence deduced by the ADF, one novel \\textit{dual-branch fusion NeRF} (DBF-NeRF) explores these contents to render the final images.","Extensive empirical studies demonstrate that the collaboration of these two stages effectively facilitates our method to render a more realistic talking head than the existing algorithms."],"url":"http://arxiv.org/abs/2404.19038v1","category":"cs.CV"}
{"created":"2024-04-29 18:16:13","title":"Machine Unlearning for Document Classification","abstract":"Document understanding models have recently demonstrated remarkable performance by leveraging extensive collections of user documents. However, since documents often contain large amounts of personal data, their usage can pose a threat to user privacy and weaken the bonds of trust between humans and AI services. In response to these concerns, legislation advocating ``the right to be forgotten\" has recently been proposed, allowing users to request the removal of private information from computer systems and neural network models. A novel approach, known as machine unlearning, has emerged to make AI models forget about a particular class of data. In our research, we explore machine unlearning for document classification problems, representing, to the best of our knowledge, the first investigation into this area. Specifically, we consider a realistic scenario where a remote server houses a well-trained model and possesses only a small portion of training data. This setup is designed for efficient forgetting manipulation. This work represents a pioneering step towards the development of machine unlearning methods aimed at addressing privacy concerns in document analysis applications. Our code is publicly available at \\url{https://github.com/leitro/MachineUnlearning-DocClassification}.","sentences":["Document understanding models have recently demonstrated remarkable performance by leveraging extensive collections of user documents.","However, since documents often contain large amounts of personal data, their usage can pose a threat to user privacy and weaken the bonds of trust between humans and AI services.","In response to these concerns, legislation advocating ``the right to be forgotten\" has recently been proposed, allowing users to request the removal of private information from computer systems and neural network models.","A novel approach, known as machine unlearning, has emerged to make AI models forget about a particular class of data.","In our research, we explore machine unlearning for document classification problems, representing, to the best of our knowledge, the first investigation into this area.","Specifically, we consider a realistic scenario where a remote server houses a well-trained model and possesses only a small portion of training data.","This setup is designed for efficient forgetting manipulation.","This work represents a pioneering step towards the development of machine unlearning methods aimed at addressing privacy concerns in document analysis applications.","Our code is publicly available at \\url{https://github.com/leitro/MachineUnlearning-DocClassification}."],"url":"http://arxiv.org/abs/2404.19031v1","category":"cs.CV"}
{"created":"2024-04-29 18:00:25","title":"ALP-ine quests at the LHC: hunting axion-like particles via peaks and dips in $t \\bar{t}$ production","abstract":"We present an analysis of the sensitivity of current and future LHC searches for new spin-0 particles in top-anti-top-quark ($t\\bar{t}$) final states, focusing on generic axion-like particles (ALPs) that are coupled to top quarks and gluons. As a first step, we derive new limits on the effective ALP Lagrangian in terms of the Wilson coefficients $c_t$ and $c_{\\tilde{G}}$ based on the results of the CMS search using $35.9$ fb$^{-1}$ of data, collected at $\\sqrt{s} = 13$ TeV. We then investigate how the production of an ALP with generic couplings to gluons and top quarks can be distinguished from the production of a pseudoscalar which couples to gluons exclusively via a top-quark loop. To this end, we make use of the invariant $t\\bar{t}$ mass distribution and angular correlations that are sensitive to the $t\\bar{t}$ spin correlation. Using a mass of 400 GeV as an example, we find that already the data collected during Run 2 and Run 3 of the LHC provides an interesting sensitivity to the underlying nature of a possible new particle. We also analyze the prospects for data anticipated to be collected during the high-luminosity phase of the LHC. Finally, we compare the limits obtained from the $t \\bar t$ searches to existing experimental bounds from LHC searches for narrow di-photon resonances, from measurements of the production of four top quarks, and from global analyses of ALP-SMEFT interference effects.","sentences":["We present an analysis of the sensitivity of current and future LHC searches for new spin-0 particles in top-anti-top-quark ($t\\bar{t}$) final states, focusing on generic axion-like particles (ALPs) that are coupled to top quarks and gluons.","As a first step, we derive new limits on the effective ALP Lagrangian in terms of the Wilson coefficients $c_t$ and $c_{\\tilde{G}}$ based on the results of the CMS search using $35.9$ fb$^{-1}$ of data, collected at $\\sqrt{s} = 13$ TeV. We then investigate how the production of an ALP with generic couplings to gluons and top quarks can be distinguished from the production of a pseudoscalar which couples to gluons exclusively via a top-quark loop.","To this end, we make use of the invariant $t\\bar{t}$ mass distribution and angular correlations that are sensitive to the $t\\bar{t}$ spin correlation.","Using a mass of 400 GeV as an example, we find that already the data collected during Run 2 and Run 3 of the LHC provides an interesting sensitivity to the underlying nature of a possible new particle.","We also analyze the prospects for data anticipated to be collected during the high-luminosity phase of the LHC.","Finally, we compare the limits obtained from the $t \\bar t$ searches to existing experimental bounds from LHC searches for narrow di-photon resonances, from measurements of the production of four top quarks, and from global analyses of ALP-SMEFT interference effects."],"url":"http://arxiv.org/abs/2404.19014v1","category":"hep-ph"}
{"created":"2024-04-29 18:00:03","title":"How Did We Get Here? Summarizing Conversation Dynamics","abstract":"Throughout a conversation, the way participants interact with each other is in constant flux: their tones may change, they may resort to different strategies to convey their points, or they might alter their interaction patterns. An understanding of these dynamics can complement that of the actual facts and opinions discussed, offering a more holistic view of the trajectory of the conversation: how it arrived at its current state and where it is likely heading.   In this work, we introduce the task of summarizing the dynamics of conversations, by constructing a dataset of human-written summaries, and exploring several automated baselines. We evaluate whether such summaries can capture the trajectory of conversations via an established downstream task: forecasting whether an ongoing conversation will eventually derail into toxic behavior. We show that they help both humans and automated systems with this forecasting task. Humans make predictions three times faster, and with greater confidence, when reading the summaries than when reading the transcripts. Furthermore, automated forecasting systems are more accurate when constructing, and then predicting based on, summaries of conversation dynamics, compared to directly predicting on the transcripts.","sentences":["Throughout a conversation, the way participants interact with each other is in constant flux: their tones may change, they may resort to different strategies to convey their points, or they might alter their interaction patterns.","An understanding of these dynamics can complement that of the actual facts and opinions discussed, offering a more holistic view of the trajectory of the conversation: how it arrived at its current state and where it is likely heading.   ","In this work, we introduce the task of summarizing the dynamics of conversations, by constructing a dataset of human-written summaries, and exploring several automated baselines.","We evaluate whether such summaries can capture the trajectory of conversations via an established downstream task: forecasting whether an ongoing conversation will eventually derail into toxic behavior.","We show that they help both humans and automated systems with this forecasting task.","Humans make predictions three times faster, and with greater confidence, when reading the summaries than when reading the transcripts.","Furthermore, automated forecasting systems are more accurate when constructing, and then predicting based on, summaries of conversation dynamics, compared to directly predicting on the transcripts."],"url":"http://arxiv.org/abs/2404.19007v1","category":"cs.CL"}
{"created":"2024-04-29 18:00:00","title":"Axion Searches Go Deep Underground","abstract":"We propose to investigate the time modulation of radioisotope decays deep underground as a method to explore axion dark matter. In this work, we focus on the $\\alpha$-decay of heavy isotopes and develop a theoretical description for the $\\theta$-dependence of $\\alpha$-decay half-lives, which enables us to predict the time variation of $\\alpha$-radioactivity in response to an oscillating axion dark matter background. To probe this scenario, we have recently constructed and installed a setup deep underground at the Gran Sasso Laboratory, based on the $\\alpha$-decay of Americium-241. This prototype experiment, named RadioAxion-$\\alpha$, will allow us to explore a broad range of oscillation's periods, from a few micro-seconds up to one year, thus providing competitive limits on the axion decay constant across 13 orders of magnitude in the axion mass, ranging from $10^{-9}$ eV to $10^{-20}$ eV after one month of data collection, and down to $10^{-22}$ eV after three years.","sentences":["We propose to investigate the time modulation of radioisotope decays deep underground as a method to explore axion dark matter.","In this work, we focus on the $\\alpha$-decay of heavy isotopes and develop a theoretical description for the $\\theta$-dependence of $\\alpha$-decay half-lives, which enables us to predict the time variation of $\\alpha$-radioactivity in response to an oscillating axion dark matter background.","To probe this scenario, we have recently constructed and installed a setup deep underground at the Gran Sasso Laboratory, based on the $\\alpha$-decay of Americium-241.","This prototype experiment, named RadioAxion-$\\alpha$, will allow us to explore a broad range of oscillation's periods, from a few micro-seconds up to one year, thus providing competitive limits on the axion decay constant across 13 orders of magnitude in the axion mass, ranging from $10^{-9}$ eV to $10^{-20}$ eV after one month of data collection, and down to $10^{-22}$ eV after three years."],"url":"http://arxiv.org/abs/2404.18993v1","category":"hep-ph"}
{"created":"2024-04-29 16:59:13","title":"Search for heavy neutral Higgs bosons decaying into a top quark pair in 140 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"A search for heavy pseudo-scalar ($A$) and scalar ($H$) Higgs bosons decaying into a top-quark pair ($t\\bar{t}$) has been performed with 140 fb$^{-1}$ of proton-proton collision data collected by the ATLAS experiment at the Large Hadron Collider at a centre-of-mass energy of $\\sqrt{s}=13$ TeV. Interference effects between the signal process and Standard Model (SM) $t\\bar{t}$ production are taken into account. Final states with exactly one or exactly two electrons or muons are considered. No significant deviation from the SM prediction is observed. The results of the search are interpreted in the context of a two-Higgs-doublet model (2HDM) of type II in the alignment limit with mass-degenerate pseudo-scalar and scalar Higgs bosons ($m_A = m_H$) and the hMSSM parameterisation of the minimal supersymmetric extension of the Standard Model. Ratios of the two vacuum expectation values, $\\tan\\beta$, smaller than 3.49 (3.16) are excluded at 95% confidence level for $m_A = m_H = 400$ GeV in the 2HDM (hMSSM). Masses up to 1240 GeV are excluded for the lowest tested $\\tan\\beta$ value of 0.4 in the 2HDM. In the hMSSM, masses up to 950 GeV are excluded for $\\tan\\beta=1.0$. In addition, generic exclusion limits are derived separately for single scalar and pseudo-scalar states for different choices of their mass and total width.","sentences":["A search for heavy pseudo-scalar ($A$) and scalar ($H$) Higgs bosons decaying into a top-quark pair ($t\\bar{t}$) has been performed with 140 fb$^{-1}$ of proton-proton collision data collected by the ATLAS experiment at the Large Hadron Collider at a centre-of-mass energy of $\\sqrt{s}=13$ TeV. Interference effects between the signal process and Standard Model (SM) $t\\bar{t}$ production are taken into account.","Final states with exactly one or exactly two electrons or muons are considered.","No significant deviation from the SM prediction is observed.","The results of the search are interpreted in the context of a two-Higgs-doublet model (2HDM) of type II in the alignment limit with mass-degenerate pseudo-scalar and scalar Higgs bosons ($m_A = m_H$) and the hMSSM parameterisation of the minimal supersymmetric extension of the Standard Model.","Ratios of the two vacuum expectation values, $\\tan\\beta$, smaller than 3.49 (3.16) are excluded at 95% confidence level for $m_A = m_H = 400$ GeV in the 2HDM (hMSSM).","Masses up to 1240 GeV are excluded for the lowest tested $\\tan\\beta$ value of 0.4 in the 2HDM.","In the hMSSM, masses up to 950 GeV are excluded for $\\tan\\beta=1.0$. In addition, generic exclusion limits are derived separately for single scalar and pseudo-scalar states for different choices of their mass and total width."],"url":"http://arxiv.org/abs/2404.18986v1","category":"hep-ex"}
{"created":"2024-04-29 15:19:05","title":"Can ChatGPT Make Explanatory Inferences? Benchmarks for Abductive Reasoning","abstract":"Explanatory inference is the creation and evaluation of hypotheses that provide explanations, and is sometimes known as abduction or abductive inference. Generative AI is a new set of artificial intelligence models based on novel algorithms for generating text, images, and sounds. This paper proposes a set of benchmarks for assessing the ability of AI programs to perform explanatory inference, and uses them to determine the extent to which ChatGPT, a leading generative AI model, is capable of making explanatory inferences. Tests on the benchmarks reveal that ChatGPT performs creative and evaluative inferences in many domains, although it is limited to verbal and visual modalities. Claims that ChatGPT and similar models are incapable of explanation, understanding, causal reasoning, meaning, and creativity are rebutted.","sentences":["Explanatory inference is the creation and evaluation of hypotheses that provide explanations, and is sometimes known as abduction or abductive inference.","Generative AI is a new set of artificial intelligence models based on novel algorithms for generating text, images, and sounds.","This paper proposes a set of benchmarks for assessing the ability of AI programs to perform explanatory inference, and uses them to determine the extent to which ChatGPT, a leading generative AI model, is capable of making explanatory inferences.","Tests on the benchmarks reveal that ChatGPT performs creative and evaluative inferences in many domains, although it is limited to verbal and visual modalities.","Claims that ChatGPT and similar models are incapable of explanation, understanding, causal reasoning, meaning, and creativity are rebutted."],"url":"http://arxiv.org/abs/2404.18982v1","category":"cs.AI"}
{"created":"2024-04-29 15:18:26","title":"Decoding Radiologists' Intentions: A Novel System for Accurate Region Identification in Chest X-ray Image Analysis","abstract":"In the realm of chest X-ray (CXR) image analysis, radiologists meticulously examine various regions, documenting their observations in reports. The prevalence of errors in CXR diagnoses, particularly among inexperienced radiologists and hospital residents, underscores the importance of understanding radiologists' intentions and the corresponding regions of interest. This understanding is crucial for correcting mistakes by guiding radiologists to the accurate regions of interest, especially in the diagnosis of chest radiograph abnormalities. In response to this imperative, we propose a novel system designed to identify the primary intentions articulated by radiologists in their reports and the corresponding regions of interest in CXR images. This system seeks to elucidate the visual context underlying radiologists' textual findings, with the potential to rectify errors made by less experienced practitioners and direct them to precise regions of interest. Importantly, the proposed system can be instrumental in providing constructive feedback to inexperienced radiologists or junior residents in the hospital, bridging the gap in face-to-face communication. The system represents a valuable tool for enhancing diagnostic accuracy and fostering continuous learning within the medical community.","sentences":["In the realm of chest X-ray (CXR) image analysis, radiologists meticulously examine various regions, documenting their observations in reports.","The prevalence of errors in CXR diagnoses, particularly among inexperienced radiologists and hospital residents, underscores the importance of understanding radiologists' intentions and the corresponding regions of interest.","This understanding is crucial for correcting mistakes by guiding radiologists to the accurate regions of interest, especially in the diagnosis of chest radiograph abnormalities.","In response to this imperative, we propose a novel system designed to identify the primary intentions articulated by radiologists in their reports and the corresponding regions of interest in CXR images.","This system seeks to elucidate the visual context underlying radiologists' textual findings, with the potential to rectify errors made by less experienced practitioners and direct them to precise regions of interest.","Importantly, the proposed system can be instrumental in providing constructive feedback to inexperienced radiologists or junior residents in the hospital, bridging the gap in face-to-face communication.","The system represents a valuable tool for enhancing diagnostic accuracy and fostering continuous learning within the medical community."],"url":"http://arxiv.org/abs/2404.18981v1","category":"eess.IV"}
{"created":"2024-04-29 14:53:48","title":"Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs","abstract":"There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments. However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent's ability to generalize skills across tasks. In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs). We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model's general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents' performance and generalization. To support the development and evaluation of these agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations. Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions. In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task. Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop high-performing agents for open-ended learning environments.","sentences":["There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments.","However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent's ability to generalize skills across tasks.","In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs).","We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model's general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents' performance and generalization.","To support the development and evaluation of these agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations.","Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions.","In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task.","Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop high-performing agents for open-ended learning environments."],"url":"http://arxiv.org/abs/2404.18978v1","category":"cs.LG"}
{"created":"2024-04-29 14:45:28","title":"Foundations of Multisensory Artificial Intelligence","abstract":"Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the machine learning foundations of multisensory AI. In the first part, we present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task. These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets, design principled approaches to learn these interactions, and analyze whether their model has succeeded in learning. In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities. We introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas, followed by the cross-modal attention and multimodal transformer architectures that now underpin many of today's multimodal foundation models. Scaling these architectures on MultiBench enables the creation of general-purpose multisensory AI systems, and we discuss our collaborative efforts in applying these models for real-world impact in affective computing, mental health, cancer prognosis, and robotics. Finally, we conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multisensory AI.","sentences":["Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents.","By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the machine learning foundations of multisensory AI.","In the first part, we present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task.","These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets, design principled approaches to learn these interactions, and analyze whether their model has succeeded in learning.","In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities.","We introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas, followed by the cross-modal attention and multimodal transformer architectures that now underpin many of today's multimodal foundation models.","Scaling these architectures on MultiBench enables the creation of general-purpose multisensory AI systems, and we discuss our collaborative efforts in applying these models for real-world impact in affective computing, mental health, cancer prognosis, and robotics.","Finally, we conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multisensory AI."],"url":"http://arxiv.org/abs/2404.18976v1","category":"cs.LG"}
{"created":"2024-04-29 14:39:15","title":"M3H: Multimodal Multitask Machine Learning for Healthcare","abstract":"Recent breakthroughs in AI are poised to fundamentally enhance our study and understanding of healthcare. The development of an integrated many-to-many framework that leverages multiple data modality inputs for the analytical modeling of multiple medical tasks, is critical for a unified understanding of modern medicine. In this work, we introduce M3H, an explainable Multimodal Multitask Machine Learning for Healthcare framework that consolidates learning from diverse multimodal inputs across a broad spectrum of medical task categories and machine learning problem classes. The modular design of the framework ensures its generalizable data processing, task definition, and rapid model prototyping, applicable to both clinical and operational healthcare settings. We evaluate the M3H framework by validating models trained from four modalities (tabular, time-series, language, and vision) on 41 medical tasks across 4 machine learning problem classes. Our results demonstrate that M3H consistently produces multitask models that outperform canonical single-task models (by 1.1- 37.2%) across 37 disease diagnoses from 16 medical departments, three hospital operation forecasts, and one patient phenotyping task: spanning ML problem classes of supervised binary classification, multiclass classification, regression, and clustering. Additionally, the framework introduces a novel attention mechanism to balance self-exploitation (focus on learning source task), and cross-exploration (encourage learning from other tasks). Furthermore, M3H provides explainability insights on how joint learning of additional tasks impacts the learning of source task using a proposed TIM score, shedding light into the dynamics of task interdependencies. Its adaptable architecture facilitates the customization and integration, establishing it as a robust and scalable candidate solution for future AI-driven healthcare systems.","sentences":["Recent breakthroughs in AI are poised to fundamentally enhance our study and understanding of healthcare.","The development of an integrated many-to-many framework that leverages multiple data modality inputs for the analytical modeling of multiple medical tasks, is critical for a unified understanding of modern medicine.","In this work, we introduce M3H, an explainable Multimodal Multitask Machine Learning for Healthcare framework that consolidates learning from diverse multimodal inputs across a broad spectrum of medical task categories and machine learning problem classes.","The modular design of the framework ensures its generalizable data processing, task definition, and rapid model prototyping, applicable to both clinical and operational healthcare settings.","We evaluate the M3H framework by validating models trained from four modalities (tabular, time-series, language, and vision) on 41 medical tasks across 4 machine learning problem classes.","Our results demonstrate that M3H consistently produces multitask models that outperform canonical single-task models (by 1.1- 37.2%) across 37 disease diagnoses from 16 medical departments, three hospital operation forecasts, and one patient phenotyping task: spanning ML problem classes of supervised binary classification, multiclass classification, regression, and clustering.","Additionally, the framework introduces a novel attention mechanism to balance self-exploitation (focus on learning source task), and cross-exploration (encourage learning from other tasks).","Furthermore, M3H provides explainability insights on how joint learning of additional tasks impacts the learning of source task using a proposed TIM score, shedding light into the dynamics of task interdependencies.","Its adaptable architecture facilitates the customization and integration, establishing it as a robust and scalable candidate solution for future AI-driven healthcare systems."],"url":"http://arxiv.org/abs/2404.18975v1","category":"cs.LG"}
{"created":"2024-04-29 14:17:06","title":"The Convergence of AI and Synthetic Biology: The Looming Deluge","abstract":"The convergence of artificial intelligence (AI) and synthetic biology is rapidly accelerating the pace of biological discovery and engineering. AI techniques, such as large language models and biological design tools, are enabling the automated design, build, test, and learning cycles for engineered biological systems. This convergence promises to democratize synthetic biology and unlock novel applications across domains from medicine to environmental sustainability. However, it also poses significant risks around reliability, dual use, and governance. The opacity of AI models, the deskilling of workforces, and the outdated nature of current regulatory frameworks present challenges in ensuring responsible development. Urgent attention is needed to update governance structures, integrate human oversight into increasingly automated workflows, and foster a culture of responsibility among the growing community of bioengineers. Only by proactively addressing these issues can we realize the transformative potential of AI-driven synthetic biology while mitigating its risks.","sentences":["The convergence of artificial intelligence (AI) and synthetic biology is rapidly accelerating the pace of biological discovery and engineering.","AI techniques, such as large language models and biological design tools, are enabling the automated design, build, test, and learning cycles for engineered biological systems.","This convergence promises to democratize synthetic biology and unlock novel applications across domains from medicine to environmental sustainability.","However, it also poses significant risks around reliability, dual use, and governance.","The opacity of AI models, the deskilling of workforces, and the outdated nature of current regulatory frameworks present challenges in ensuring responsible development.","Urgent attention is needed to update governance structures, integrate human oversight into increasingly automated workflows, and foster a culture of responsibility among the growing community of bioengineers.","Only by proactively addressing these issues can we realize the transformative potential of AI-driven synthetic biology while mitigating its risks."],"url":"http://arxiv.org/abs/2404.18973v1","category":"q-bio.OT"}
{"created":"2024-04-29 13:47:04","title":"Credible, Unreliable or Leaked?: Evidence Verification for Enhanced Automated Fact-checking","abstract":"Automated fact-checking (AFC) is garnering increasing attention by researchers aiming to help fact-checkers combat the increasing spread of misinformation online. While many existing AFC methods incorporate external information from the Web to help examine the veracity of claims, they often overlook the importance of verifying the source and quality of collected \"evidence\". One overlooked challenge involves the reliance on \"leaked evidence\", information gathered directly from fact-checking websites and used to train AFC systems, resulting in an unrealistic setting for early misinformation detection. Similarly, the inclusion of information from unreliable sources can undermine the effectiveness of AFC systems. To address these challenges, we present a comprehensive approach to evidence verification and filtering. We create the \"CREDible, Unreliable or LEaked\" (CREDULE) dataset, which consists of 91,632 articles classified as Credible, Unreliable and Fact checked (Leaked). Additionally, we introduce the EVidence VERification Network (EVVER-Net), trained on CREDULE to detect leaked and unreliable evidence in both short and long texts. EVVER-Net can be used to filter evidence collected from the Web, thus enhancing the robustness of end-to-end AFC systems. We experiment with various language models and show that EVVER-Net can demonstrate impressive performance of up to 91.5% and 94.4% accuracy, while leveraging domain credibility scores along with short or long texts, respectively. Finally, we assess the evidence provided by widely-used fact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and VERITE, some of which exhibit concerning rates of leaked and unreliable evidence.","sentences":["Automated fact-checking (AFC) is garnering increasing attention by researchers aiming to help fact-checkers combat the increasing spread of misinformation online.","While many existing AFC methods incorporate external information from the Web to help examine the veracity of claims, they often overlook the importance of verifying the source and quality of collected \"evidence\".","One overlooked challenge involves the reliance on \"leaked evidence\", information gathered directly from fact-checking websites and used to train AFC systems, resulting in an unrealistic setting for early misinformation detection.","Similarly, the inclusion of information from unreliable sources can undermine the effectiveness of AFC systems.","To address these challenges, we present a comprehensive approach to evidence verification and filtering.","We create the \"CREDible, Unreliable or LEaked\" (CREDULE) dataset, which consists of 91,632 articles classified as Credible, Unreliable and Fact checked (Leaked).","Additionally, we introduce the EVidence VERification Network (EVVER-Net), trained on CREDULE to detect leaked and unreliable evidence in both short and long texts.","EVVER-Net can be used to filter evidence collected from the Web, thus enhancing the robustness of end-to-end AFC systems.","We experiment with various language models and show that EVVER-Net can demonstrate impressive performance of up to 91.5% and 94.4% accuracy, while leveraging domain credibility scores along with short or long texts, respectively.","Finally, we assess the evidence provided by widely-used fact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and VERITE, some of which exhibit concerning rates of leaked and unreliable evidence."],"url":"http://arxiv.org/abs/2404.18971v1","category":"cs.CL"}
{"created":"2024-04-29 13:19:31","title":"Sequential model confidence sets","abstract":"In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models' performances and come with time-uniform, nonasymptotic coverage guarantees.","sentences":["In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best.","Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence.","Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size.","However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes.","In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods.","Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence.","Sequential model confidence sets allow to continuously monitor the models' performances and come with time-uniform, nonasymptotic coverage guarantees."],"url":"http://arxiv.org/abs/2404.18678v1","category":"stat.ME"}
{"created":"2024-04-30 17:59:02","title":"Exact Universal Characterization of Chiral-Symmetric Higher-Order Topological Phases","abstract":"Utilizing a series of Bott indices formulated through polynomials of position operators, we establish a comprehensive framework for characterizing topological zero-energy corner states in systems with chiral symmetry. Our framework covers systems with arbitrary shape, including topological phases that are not characterizable by previously proposed invariants such as multipole moments or multipole chiral numbers. A key feature of our framework is its ability to capture the real-space pattern of zero-energy corner states. We provide a rigorous analytical proof of its higher-order correspondence. To demonstrate the effectiveness of our theory, we examine several model systems with representative patterns of zero-energy corner states that previous frameworks fail to classify.","sentences":["Utilizing a series of Bott indices formulated through polynomials of position operators, we establish a comprehensive framework for characterizing topological zero-energy corner states in systems with chiral symmetry.","Our framework covers systems with arbitrary shape, including topological phases that are not characterizable by previously proposed invariants such as multipole moments or multipole chiral numbers.","A key feature of our framework is its ability to capture the real-space pattern of zero-energy corner states.","We provide a rigorous analytical proof of its higher-order correspondence.","To demonstrate the effectiveness of our theory, we examine several model systems with representative patterns of zero-energy corner states that previous frameworks fail to classify."],"url":"http://arxiv.org/abs/2404.19757v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 17:55:11","title":"curvedSpaceSim: A framework for simulating particles interacting along geodesics","abstract":"A large number of powerful, high-quality, and open-source simulation packages exist to efficiently perform molecular dynamics simulations, and their prevalence has greatly accelerated discoveries across a wide range of scientific domains. These packages typically simulate particles in free (Euclidean) space, with options to specify a variety of boundary conditions. While more exotic, many physical systems are constrained to and interact across curved surfaces, such as organisms moving across the landscape, colloids pinned at curved fluid-fluid interfaces, and layers of epithelial cells forming highly curved tissues. The calculation of distances and the updating of equations of motion in idealized geometries (namely, on surfaces of constant curvature) can be done analytically, but it is much more challenging to efficiently perform molecular-dynamics-like simulations on arbitrarily curved surfaces. This article discusses a simulation framework which combines tools from particle-based simulations with recent work in discrete differential geometry to model particles that interact via geodesic distances and move on an arbitrarily curved surface. We present computational cost estimates for a variety of surface complexities with and without various algorithmic specializations (e.g., restrictions to short-range interaction potentials, or multi-threaded parallelization). Our flexible and extensible framework is set up to easily handle both equilibrium and non-equilibrium dynamics, and will enable researchers to access time- and particle-number-scales previously inaccessible.","sentences":["A large number of powerful, high-quality, and open-source simulation packages exist to efficiently perform molecular dynamics simulations, and their prevalence has greatly accelerated discoveries across a wide range of scientific domains.","These packages typically simulate particles in free (Euclidean) space, with options to specify a variety of boundary conditions.","While more exotic, many physical systems are constrained to and interact across curved surfaces, such as organisms moving across the landscape, colloids pinned at curved fluid-fluid interfaces, and layers of epithelial cells forming highly curved tissues.","The calculation of distances and the updating of equations of motion in idealized geometries (namely, on surfaces of constant curvature) can be done analytically, but it is much more challenging to efficiently perform molecular-dynamics-like simulations on arbitrarily curved surfaces.","This article discusses a simulation framework which combines tools from particle-based simulations with recent work in discrete differential geometry to model particles that interact via geodesic distances and move on an arbitrarily curved surface.","We present computational cost estimates for a variety of surface complexities with and without various algorithmic specializations (e.g., restrictions to short-range interaction potentials, or multi-threaded parallelization).","Our flexible and extensible framework is set up to easily handle both equilibrium and non-equilibrium dynamics, and will enable researchers to access time- and particle-number-scales previously inaccessible."],"url":"http://arxiv.org/abs/2404.19751v1","category":"cond-mat.soft"}
{"created":"2024-04-30 17:54:16","title":"Scale-Robust Timely Asynchronous Decentralized Learning","abstract":"We consider an asynchronous decentralized learning system, which consists of a network of connected devices trying to learn a machine learning model without any centralized parameter server. The users in the network have their own local training data, which is used for learning across all the nodes in the network. The learning method consists of two processes, evolving simultaneously without any necessary synchronization. The first process is the model update, where the users update their local model via a fixed number of stochastic gradient descent steps. The second process is model mixing, where the users communicate with each other via randomized gossiping to exchange their models and average them to reach consensus. In this work, we investigate the staleness criteria for such a system, which is a sufficient condition for convergence of individual user models. We show that for network scaling, i.e., when the number of user devices $n$ is very large, if the gossip capacity of individual users scales as $\\Omega(\\log n)$, we can guarantee the convergence of user models in finite time. Furthermore, we show that the bounded staleness can only be guaranteed by any distributed opportunistic scheme by $\\Omega(n)$ scaling.","sentences":["We consider an asynchronous decentralized learning system, which consists of a network of connected devices trying to learn a machine learning model without any centralized parameter server.","The users in the network have their own local training data, which is used for learning across all the nodes in the network.","The learning method consists of two processes, evolving simultaneously without any necessary synchronization.","The first process is the model update, where the users update their local model via a fixed number of stochastic gradient descent steps.","The second process is model mixing, where the users communicate with each other via randomized gossiping to exchange their models and average them to reach consensus.","In this work, we investigate the staleness criteria for such a system, which is a sufficient condition for convergence of individual user models.","We show that for network scaling, i.e., when the number of user devices $n$ is very large, if the gossip capacity of individual users scales as $\\Omega(\\log n)$, we can guarantee the convergence of user models in finite time.","Furthermore, we show that the bounded staleness can only be guaranteed by any distributed opportunistic scheme by $\\Omega(n)$ scaling."],"url":"http://arxiv.org/abs/2404.19749v1","category":"cs.IT"}
{"created":"2024-04-30 17:51:36","title":"Obstruction Complexes in Grid Homology","abstract":"Recently, Manolescu-Sarkar constructed a stable homotopy type for link Floer homology, which uses grid homology and accounts for all domains that do not pass through a specific square. In doing so, they produced an obstruction chain complex of the grid diagram with that square removed. We define the obstruction chain complex of the full grid, without the square removed, and compute its homology. Though this homology is too complicated to immediately extend the Manolescu-Sarkar construction, we give results about the existence of sign assignments in grid homology.","sentences":["Recently, Manolescu-Sarkar constructed a stable homotopy type for link Floer homology, which uses grid homology and accounts for all domains that do not pass through a specific square.","In doing so, they produced an obstruction chain complex of the grid diagram with that square removed.","We define the obstruction chain complex of the full grid, without the square removed, and compute its homology.","Though this homology is too complicated to immediately extend the Manolescu-Sarkar construction, we give results about the existence of sign assignments in grid homology."],"url":"http://arxiv.org/abs/2404.19747v1","category":"math.GT"}
{"created":"2024-04-30 17:47:30","title":"Analyzing Transport Policies in Developing Countries with ABM","abstract":"Deciphering travel behavior and mode choices is a critical aspect of effective urban transportation system management, particularly in developing countries where unique socio-economic and cultural conditions complicate decision-making. Agent-based simulations offer a valuable tool for modeling transportation systems, enabling a nuanced understanding and policy impact evaluation. This work aims to shed light on the effects of transport policies and analyzes travel behavior by simulating agents making mode choices for their daily commutes. Agents gather information from the environment and their social network to assess the optimal transport option based on personal satisfaction criteria. Our findings, stemming from simulating a free-fare policy for public transit in a developing-country city, reveal a significant influence on decision-making, fostering public service use while positively influencing pollution levels, accident rates, and travel speed.","sentences":["Deciphering travel behavior and mode choices is a critical aspect of effective urban transportation system management, particularly in developing countries where unique socio-economic and cultural conditions complicate decision-making.","Agent-based simulations offer a valuable tool for modeling transportation systems, enabling a nuanced understanding and policy impact evaluation.","This work aims to shed light on the effects of transport policies and analyzes travel behavior by simulating agents making mode choices for their daily commutes.","Agents gather information from the environment and their social network to assess the optimal transport option based on personal satisfaction criteria.","Our findings, stemming from simulating a free-fare policy for public transit in a developing-country city, reveal a significant influence on decision-making, fostering public service use while positively influencing pollution levels, accident rates, and travel speed."],"url":"http://arxiv.org/abs/2404.19745v1","category":"cs.MA"}
{"created":"2024-04-30 17:32:53","title":"On the derivatives of the Liouville currents","abstract":"The Liouville map, introduced by Bonahon, assigns to each point in the Teichm\\\"uller space a natural Radon measure on the space of geodesics of the base surface. The Liouville map is real analytic and it even extends to a holomorphic map of a neighborhood of the Teichm\\\"uller space in the Quasi-Fuchsian space of an arbitrary conformally hyperbolic Riemann surface. The earthquake paths and by their extension quake-bends, introduced by Thurston, are particularly nice real-analytic and holomorphic paths in the Teichm\\\"uller and the Quasi-Fuchsian space, respectively. We find a geometric expression for the derivative of the Liouville map along earthquake paths.","sentences":["The Liouville map, introduced by Bonahon, assigns to each point in the Teichm\\\"uller space a natural Radon measure on the space of geodesics of the base surface.","The Liouville map is real analytic and it even extends to a holomorphic map of a neighborhood of the Teichm\\\"uller space in the Quasi-Fuchsian space of an arbitrary conformally hyperbolic Riemann surface.","The earthquake paths and by their extension quake-bends, introduced by Thurston, are particularly nice real-analytic and holomorphic paths in the Teichm\\\"uller and the Quasi-Fuchsian space, respectively.","We find a geometric expression for the derivative of the Liouville map along earthquake paths."],"url":"http://arxiv.org/abs/2404.19736v1","category":"math.CV"}
{"created":"2024-04-30 17:23:07","title":"Classification of simple 0-dimensional isolated complete intersection singularities","abstract":"The aim of this article is the classification of simple 0-dimensional isolated complete intersection singularities in positive characteristic. As usual, a singularity is called simple or 0-modal if there are only finitely many isomorphism classes of singularities into which the given singularity can deform. The notion of simpleness was introduced by V. I. Arnold and the classification of low modality singularities has become a fundamental task in singularity theory. Simple complex analytic isolated complete intersection singularities (ICIS) were classified by M. Giusti. However, the classification in positive characteristic requires different methods and is much more involved. The final result is nevertheless similar to the classification in characteristic 0 with some additional normal forms in low characteristic. The theoretical results in this paper mainly concern families of ICIS that are formal in the fiber and algebraic in the base (formal deformation theory is not sufficient). In particular, we give a definition of modality in this situation and prove its semicontinuity.","sentences":["The aim of this article is the classification of simple 0-dimensional isolated complete intersection singularities in positive characteristic.","As usual, a singularity is called simple or 0-modal if there are only finitely many isomorphism classes of singularities into which the given singularity can deform.","The notion of simpleness was introduced by V. I. Arnold and the classification of low modality singularities has become a fundamental task in singularity theory.","Simple complex analytic isolated complete intersection singularities (ICIS) were classified by M. Giusti.","However, the classification in positive characteristic requires different methods and is much more involved.","The final result is nevertheless similar to the classification in characteristic 0 with some additional normal forms in low characteristic.","The theoretical results in this paper mainly concern families of ICIS that are formal in the fiber and algebraic in the base (formal deformation theory is not sufficient).","In particular, we give a definition of modality in this situation and prove its semicontinuity."],"url":"http://arxiv.org/abs/2404.19728v1","category":"math.AG"}
{"created":"2024-04-30 17:19:30","title":"Sound and Complete Proof Rules for Probabilistic Termination","abstract":"Termination is a fundamental question in the analysis of probabilistic imperative programs. We consider the qualitative and quantitative probabilistic termination problems for an imperative programming model with discrete probabilistic choice and demonic bounded nondeterminism. The qualitative question asks if the program terminates almost surely, no matter how nondeterminism is resolved; the quantitative question asks for a bound on the probability of termination. Despite a long and rich literature on the topic, no sound and relatively complete proof systems were known for this problem. We provide the first sound and relatively complete proof rules for proving qualitative and quantitative termination in the assertion language of arithmetic. Our proof rules use supermartingales as estimates of likelihood of the prgroam's evolution - the key insight is to use appropriately defined finite-state sub-instances. Our completeness result shows how to construct a suitable supermartingales from an almost-surely terminating program. We also show that proofs of termination in many existing proof systems can be transformed to proofs in our system, pointing to its applicability in practice. As an application of our proof rule, we show a proof of almost sure termination for the two-dimensional random walker.","sentences":["Termination is a fundamental question in the analysis of probabilistic imperative programs.","We consider the qualitative and quantitative probabilistic termination problems for an imperative programming model with discrete probabilistic choice and demonic bounded nondeterminism.","The qualitative question asks if the program terminates almost surely, no matter how nondeterminism is resolved; the quantitative question asks for a bound on the probability of termination.","Despite a long and rich literature on the topic, no sound and relatively complete proof systems were known for this problem.","We provide the first sound and relatively complete proof rules for proving qualitative and quantitative termination in the assertion language of arithmetic.","Our proof rules use supermartingales as estimates of likelihood of the prgroam's evolution - the key insight is to use appropriately defined finite-state sub-instances.","Our completeness result shows how to construct a suitable supermartingales from an almost-surely terminating program.","We also show that proofs of termination in many existing proof systems can be transformed to proofs in our system, pointing to its applicability in practice.","As an application of our proof rule, we show a proof of almost sure termination for the two-dimensional random walker."],"url":"http://arxiv.org/abs/2404.19724v1","category":"cs.LO"}
{"created":"2024-04-30 17:10:25","title":"Automated, Reliable, and Efficient Continental-Scale Replication of 7.3 Petabytes of Climate Simulation Data: A Case Study","abstract":"We report on our experiences replicating 7.3 petabytes (PB) of Earth System Grid Federation (ESGF) climate simulation data from Lawrence Livermore National Laboratory (LLNL) in California to Argonne National Laboratory (ANL) in Illinois and Oak Ridge National Laboratory (ORNL) in Tennessee. This movement of some 29 million files, twice, undertaken in order to establish new ESGF nodes at ANL and ORNL, was performed largely automatically by a simple replication tool, a script that invoked Globus to transfer large bundles of files while tracking progress in a database. Under the covers, Globus organized transfers to make efficient use of the high-speed Energy Sciences network (ESnet) and the data transfer nodes deployed at participating sites, and also addressed security, integrity checking, and recovery from a variety of transient failures. This success demonstrates the considerable benefits that can accrue from the adoption of performant data replication infrastructure.","sentences":["We report on our experiences replicating 7.3 petabytes (PB) of Earth System Grid Federation (ESGF) climate simulation data from Lawrence Livermore National Laboratory (LLNL) in California to Argonne National Laboratory (ANL) in Illinois and Oak Ridge National Laboratory (ORNL) in Tennessee.","This movement of some 29 million files, twice, undertaken in order to establish new ESGF nodes at ANL and ORNL, was performed largely automatically by a simple replication tool, a script that invoked Globus to transfer large bundles of files while tracking progress in a database.","Under the covers, Globus organized transfers to make efficient use of the high-speed Energy Sciences network (ESnet) and the data transfer nodes deployed at participating sites, and also addressed security, integrity checking, and recovery from a variety of transient failures.","This success demonstrates the considerable benefits that can accrue from the adoption of performant data replication infrastructure."],"url":"http://arxiv.org/abs/2404.19717v1","category":"cs.DC"}
{"created":"2024-04-30 17:06:20","title":"ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text data on social disorders in children and adolescents","abstract":"This paper describes our participation in Task 3 and Task 5 of the #SMM4H (Social Media Mining for Health) 2024 Workshop, explicitly targeting the classification challenges within tweet data. Task 3 is a multi-class classification task centered on tweets discussing the impact of outdoor environments on symptoms of social anxiety. Task 5 involves a binary classification task focusing on tweets reporting medical disorders in children. We applied transfer learning from pre-trained encoder-decoder models such as BART-base and T5-small to identify the labels of a set of given tweets. We also presented some data augmentation methods to see their impact on the model performance. Finally, the systems obtained the best F1 score of 0.627 in Task 3 and the best F1 score of 0.841 in Task 5.","sentences":["This paper describes our participation in Task 3 and Task 5 of the #SMM4H (Social Media Mining for Health) 2024 Workshop, explicitly targeting the classification challenges within tweet data.","Task 3 is a multi-class classification task centered on tweets discussing the impact of outdoor environments on symptoms of social anxiety.","Task 5 involves a binary classification task focusing on tweets reporting medical disorders in children.","We applied transfer learning from pre-trained encoder-decoder models such as BART-base and T5-small to identify the labels of a set of given tweets.","We also presented some data augmentation methods to see their impact on the model performance.","Finally, the systems obtained the best F1 score of 0.627 in Task 3 and the best F1 score of 0.841 in Task 5."],"url":"http://arxiv.org/abs/2404.19714v1","category":"cs.CL"}
{"created":"2024-04-30 17:01:18","title":"SRG/ART-XC discovery of SRGAJ144459.2-604207: a well-tempered bursting accreting millisecond X-ray pulsar","abstract":"We report on the discovery of the new accreting millisecond X-ray pulsar SRGAJ144459.2-604207 using the SRG/ART-XC data. The source was observed twice in February 2024 during the declining phase of the outburst. Timing analysis revealed a coherent signal near 447.8~Hz modulated by the Doppler effect due to the orbital motion. The derived parameters for the binary system are consistent with the circular orbit with a period of $\\sim5.2$~h. The pulse profiles of the persistent emission, showing a sine-like part during half a period with a plateau in between, can well be modelled by emission from two circular spots partially eclipsed by the accretion disk. Additionally, during our 133~ks exposure observations, we detected 19 thermonuclear X-ray bursts. All bursts have similar shapes and energetics, and do not show any signs of photospheric radius expansion. The burst rate decreases linearly from one per $\\sim$1.6~h at the beginning of observations to one per $\\sim$2.2~h at the end and anticorrelates with the persistent flux. Spectral evolution during the bursts is consistent with the models of the neutron star atmospheres heated by accretion and imply a neutron star radius of 11--12~km and the distance to the source of 8--9~kpc. We also detected pulsations during the bursts and showed that the pulse profiles differ substantially from those observed in the persistent emission. However, we could not find a simple physical model explaining the pulse profiles detected during the bursts.","sentences":["We report on the discovery of the new accreting millisecond X-ray pulsar SRGAJ144459.2-604207 using the SRG/ART-XC data.","The source was observed twice in February 2024 during the declining phase of the outburst.","Timing analysis revealed a coherent signal near 447.8~Hz modulated by the Doppler effect due to the orbital motion.","The derived parameters for the binary system are consistent with the circular orbit with a period of $\\sim5.2$~h.","The pulse profiles of the persistent emission, showing a sine-like part during half a period with a plateau in between, can well be modelled by emission from two circular spots partially eclipsed by the accretion disk.","Additionally, during our 133~ks exposure observations, we detected 19 thermonuclear X-ray bursts.","All bursts have similar shapes and energetics, and do not show any signs of photospheric radius expansion.","The burst rate decreases linearly from one per $\\sim$1.6~h at the beginning of observations to one per $\\sim$2.2~h at the end and anticorrelates with the persistent flux.","Spectral evolution during the bursts is consistent with the models of the neutron star atmospheres heated by accretion and imply a neutron star radius of 11--12~km and the distance to the source of 8--9~kpc.","We also detected pulsations during the bursts and showed that the pulse profiles differ substantially from those observed in the persistent emission.","However, we could not find a simple physical model explaining the pulse profiles detected during the bursts."],"url":"http://arxiv.org/abs/2404.19709v1","category":"astro-ph.HE"}
{"created":"2024-04-30 16:54:59","title":"RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting","abstract":"We propose RTG-SLAM, a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. RTG-SLAM features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of real large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.","sentences":["We propose RTG-SLAM, a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting.","RTG-SLAM features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme.","We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors.","By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost.","For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors and with large depth errors.","We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable.","We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians.","In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time.","We show real-time reconstructions of a variety of real large scenes.","Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy."],"url":"http://arxiv.org/abs/2404.19706v1","category":"cs.CV"}
{"created":"2024-04-30 16:46:36","title":"Dynamics of particle aggregation in dewetting films of complex liquids","abstract":"We consider the dynamic wetting and dewetting processes of films and droplets of complex liquids on planar surfaces, focusing on the case of colloidal suspensions, where the particle interactions can be sufficiently attractive to cause agglomeration of the colloids within the film. This leads to an interesting array of dynamic behaviours within the liquid and of the liquid-air interface. Incorporating concepts from thermodynamics and using the thin-film approximation, we construct a model consisting of a pair of coupled partial differential equations that represent the evolution of the liquid film and the effective colloidal height profiles. We determine the relevant phase behaviour of the uniform system, including finding associated binodal and spinodal curves, helping to uncover how the emerging behaviour depends on the particle interactions. Performing a linear stability analysis of our system enables us to identify parameter regimes where agglomerates form, which we independently confirm through numerical simulations and continuation of steady states, to construct bifurcation diagrams. We obtain various dynamics such as uniform colloidal profiles in an unstable situation evolving into agglomerates and thus elucidate the interplay between dewetting and particle aggregation in complex liquids on surfaces.","sentences":["We consider the dynamic wetting and dewetting processes of films and droplets of complex liquids on planar surfaces, focusing on the case of colloidal suspensions, where the particle interactions can be sufficiently attractive to cause agglomeration of the colloids within the film.","This leads to an interesting array of dynamic behaviours within the liquid and of the liquid-air interface.","Incorporating concepts from thermodynamics and using the thin-film approximation, we construct a model consisting of a pair of coupled partial differential equations that represent the evolution of the liquid film and the effective colloidal height profiles.","We determine the relevant phase behaviour of the uniform system, including finding associated binodal and spinodal curves, helping to uncover how the emerging behaviour depends on the particle interactions.","Performing a linear stability analysis of our system enables us to identify parameter regimes where agglomerates form, which we independently confirm through numerical simulations and continuation of steady states, to construct bifurcation diagrams.","We obtain various dynamics such as uniform colloidal profiles in an unstable situation evolving into agglomerates and thus elucidate the interplay between dewetting and particle aggregation in complex liquids on surfaces."],"url":"http://arxiv.org/abs/2404.19701v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 16:46:20","title":"Comparing Multivariate Distributions: A Novel Approach Using Optimal Transport-based Plots","abstract":"Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets. Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data. In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data. Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets. Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets. Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour. We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously.","sentences":["Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets.","Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data.","In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data.","Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets.","Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets.","Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour.","We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously."],"url":"http://arxiv.org/abs/2404.19700v1","category":"stat.ME"}
{"created":"2024-04-30 16:44:56","title":"Structural properties of Krylov subspaces and applications to unbounded self-adjoint operators","abstract":"This paper presents a study of the inherent structural properties of Krylov subspaces, in particular for the self-adjoint class of operators, and how they relate with the important phenomenon of `Krylov solvability' of linear inverse problems. Owing to the complexity of the problem in the unbounded setting, recently developed perturbative techniques are used that exploit the use of the weak topology on $\\mathcal{H}$. We also make a strong connection between the approximative properties of the Krylov subspace and the famous Hamburger problem of moments, in particular the determinacy condition.","sentences":["This paper presents a study of the inherent structural properties of Krylov subspaces, in particular for the self-adjoint class of operators, and how they relate with the important phenomenon of `Krylov solvability' of linear inverse problems.","Owing to the complexity of the problem in the unbounded setting, recently developed perturbative techniques are used that exploit the use of the weak topology on $\\mathcal{H}$. We also make a strong connection between the approximative properties of the Krylov subspace and the famous Hamburger problem of moments, in particular the determinacy condition."],"url":"http://arxiv.org/abs/2404.19698v1","category":"math.FA"}
{"created":"2024-04-30 16:29:32","title":"The density of Gabor systems in expansible locally compact abelian groups","abstract":"We investigate the reproducing properties of Gabor systems within the context of expansible groups. These properties are established in terms of density conditions. The concept of density that we employ mirrors the well-known Beurling density defined in Euclidean space, which is made possible due to the expansive structure. Along the way, for groups with an open and compact subgroup, we demonstrate that modulation spaces are continuously embedded in Wiener spaces. Utilizing this result, we derive the Bessel condition of Gabor systems. Additionally, we construct Gabor orthonormal bases with arbitrarily small or large densities, enabling us to conclude that a Comparison Theorem, such as the one proven to be valid in the Euclidean case, cannot hold in this context. Finally, we establish that Gabor frames possess the Homogeneous Approximation Property.","sentences":["We investigate the reproducing properties of Gabor systems within the context of expansible groups.","These properties are established in terms of density conditions.","The concept of density that we employ mirrors the well-known Beurling density defined in Euclidean space, which is made possible due to the expansive structure.","Along the way, for groups with an open and compact subgroup, we demonstrate that modulation spaces are continuously embedded in Wiener spaces.","Utilizing this result, we derive the Bessel condition of Gabor systems.","Additionally, we construct Gabor orthonormal bases with arbitrarily small or large densities, enabling us to conclude that a Comparison Theorem, such as the one proven to be valid in the Euclidean case, cannot hold in this context.","Finally, we establish that Gabor frames possess the Homogeneous Approximation Property."],"url":"http://arxiv.org/abs/2404.19688v1","category":"math.FA"}
{"created":"2024-04-30 16:24:43","title":"Enhanced twist-and-turn dynamics of spin squeezing in internal bosonic Josephson junctions","abstract":"The twist-and-turn dynamics of spin squeezing results from the interplay of the (nonlinear) one-axis-twisting- and the (linear) transverse-field turning term in the underlying Hamiltonian, both with constant (time-independent) respective coupling strengths. Using the methods of shortcuts to adiabaticity (STA) and their recently proposed enhanced version (eSTA), we demonstrate here that dynamics of this type can be utilized for a fast and robust preparation of spin-squeezed states in internal bosonic Josephson junctions -- condensates of cold bosonic atoms in two different internal (hyperfine) states (single-boson modes). Assuming that the initial state of this system is the coherent spin state with all the bosons in the equal superposition of the two single-boson modes and that the nonlinear-coupling strength in this system remains constant, we set out to determine the time-dependence of the linear-coupling strength using the STA and eSTA approaches. We then quantitatively characterize the modified twist-and-turn dynamics in this system by evaluating the coherent spin-squeezing- and number-squeezing parameters, as well as the fidelity of the target spin-squeezed states. In this manner, we show that the eSTA approach allows for a particularly robust experimental realization of strongly spin-squeezed states in this system, consistently outperforming its adiabatic and STA-based counterparts, even for systems with several hundred particles.","sentences":["The twist-and-turn dynamics of spin squeezing results from the interplay of the (nonlinear) one-axis-twisting- and the (linear) transverse-field turning term in the underlying Hamiltonian, both with constant (time-independent) respective coupling strengths.","Using the methods of shortcuts to adiabaticity (STA) and their recently proposed enhanced version (eSTA), we demonstrate here that dynamics of this type can be utilized for a fast and robust preparation of spin-squeezed states in internal bosonic Josephson junctions -- condensates of cold bosonic atoms in two different internal (hyperfine) states (single-boson modes).","Assuming that the initial state of this system is the coherent spin state with all the bosons in the equal superposition of the two single-boson modes and that the nonlinear-coupling strength in this system remains constant, we set out to determine the time-dependence of the linear-coupling strength using the STA and eSTA approaches.","We then quantitatively characterize the modified twist-and-turn dynamics in this system by evaluating the coherent spin-squeezing- and number-squeezing parameters, as well as the fidelity of the target spin-squeezed states.","In this manner, we show that the eSTA approach allows for a particularly robust experimental realization of strongly spin-squeezed states in this system, consistently outperforming its adiabatic and STA-based counterparts, even for systems with several hundred particles."],"url":"http://arxiv.org/abs/2404.19685v1","category":"quant-ph"}
{"created":"2024-04-30 16:06:07","title":"Derivative learning of tensorial quantities -- Predicting finite temperature infrared spectra from first principles","abstract":"We develop a strategy that integrates machine learning and first-principles calculations to achieve technical accurate predictions of infrared spectra. Specifically, the methodology allows to predict infrared spectra for complex systems at finite temperatures. The method's effectiveness is demonstrated in challenging scenarios, such as the analysis of water and the organic-inorganic halide perovskite MAPbI$_{3}$, where our results consistently align with experimental data. A distinctive feature of the methodology is the incorporation of derivative learning, which proves indispensable for obtaining accurate polarization data in bulk materials and facilitates the training of a machine learning surrogate model of the polarization adapted to rotational and translational symmetries. We achieve polarisation prediction accuracies of about 1 % by training only on the predicted Born effective charges.","sentences":["We develop a strategy that integrates machine learning and first-principles calculations to achieve technical accurate predictions of infrared spectra.","Specifically, the methodology allows to predict infrared spectra for complex systems at finite temperatures.","The method's effectiveness is demonstrated in challenging scenarios, such as the analysis of water and the organic-inorganic halide perovskite MAPbI$_{3}$, where our results consistently align with experimental data.","A distinctive feature of the methodology is the incorporation of derivative learning, which proves indispensable for obtaining accurate polarization data in bulk materials and facilitates the training of a machine learning surrogate model of the polarization adapted to rotational and translational symmetries.","We achieve polarisation prediction accuracies of about 1 % by training only on the predicted Born effective charges."],"url":"http://arxiv.org/abs/2404.19674v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 16:05:06","title":"Magnetic trapping of an ultracold $^{39}$K-$^{40}$K mixture with a versatile potassium laser system","abstract":"We present a dual isotope magneto-optical trap (MOT), simultaneous sub-Doppler laser cooling, and magnetic trapping of a spin-polarized $^{39}$K-$^{40}$K Bose-Fermi mixture realized in a single-chamber setup with an unenriched potassium dispenser as the source of atoms. We are able to magnetically confine more than $2.2\\times10^5$ fermions ($F=9/2\\,m_F=9/2$) and $1.4\\times10^7$ bosons ($F=2\\,m_F=2$) with a lifetime exceeding 1.2 s. For this work, we have developed a versatile laser tailored for sub-Doppler cooling of all naturally occurring potassium isotopes and their mixtures. This laser system incorporates innovative features, such as the capability to select an isotope by activating or deactivating specific acousto-optic modulators that control the light seeding tapered amplifiers. Switching between isotopes takes $\\sim$1 $\\mathrm{\\mu}$s without any mechanical adjustment of the components. As a final step in characterizing the laser system, we demonstrate sub-Doppler cooling of $^{41}$K.","sentences":["We present a dual isotope magneto-optical trap (MOT), simultaneous sub-Doppler laser cooling, and magnetic trapping of a spin-polarized $^{39}$K-$^{40}$K Bose-Fermi mixture realized in a single-chamber setup with an unenriched potassium dispenser as the source of atoms.","We are able to magnetically confine more than $2.2\\times10^5$ fermions ($F=9/2\\,m_F=9/2$) and $1.4\\times10^7$ bosons ($F=2\\,m_F=2$) with a lifetime exceeding 1.2 s. For this work, we have developed a versatile laser tailored for sub-Doppler cooling of all naturally occurring potassium isotopes and their mixtures.","This laser system incorporates innovative features, such as the capability to select an isotope by activating or deactivating specific acousto-optic modulators that control the light seeding tapered amplifiers.","Switching between isotopes takes $\\sim$1 $\\mathrm{\\mu}$s without any mechanical adjustment of the components.","As a final step in characterizing the laser system, we demonstrate sub-Doppler cooling of $^{41}$K."],"url":"http://arxiv.org/abs/2404.19670v1","category":"physics.atom-ph"}
{"created":"2024-04-30 15:57:41","title":"Constrained maximization of conformal capacity","abstract":"We consider constellations of disks which are unions of disjoint hyperbolic disks in the unit disk with fixed radii and unfixed centers. We study the problem of maximizing the conformal capacity of a constellation under constraints on the centers in two cases. In the first case the constraint is that the centers are at most at distance $R \\in(0,1)$ from the origin and in the second case it is required that the centers are on the subsegment $[-R,R]$ of a diameter of the unit disk. We study also similar types of constellations with hyperbolic segments instead of the hyperbolic disks. Our computational experiments suggest that a dispersion phenomenon occurs: the disks/segments go as close to the unit circle as possible under these constraints and stay as far as possible from each other. The computation of capacity reduces to the Dirichlet problem for the Laplace equation which we solve with a fast boundary integral equation method. The results are double-checked with the $hp$-FEM method.","sentences":["We consider constellations of disks which are unions of disjoint hyperbolic disks in the unit disk with fixed radii and unfixed centers.","We study the problem of maximizing the conformal capacity of a constellation under constraints on the centers in two cases.","In the first case the constraint is that the centers are at most at distance $R \\in(0,1)$ from the origin and in the second case it is required that the centers are on the subsegment $[-R,R]$ of a diameter of the unit disk.","We study also similar types of constellations with hyperbolic segments instead of the hyperbolic disks.","Our computational experiments suggest that a dispersion phenomenon occurs: the disks/segments go as close to the unit circle as possible under these constraints and stay as far as possible from each other.","The computation of capacity reduces to the Dirichlet problem for the Laplace equation which we solve with a fast boundary integral equation method.","The results are double-checked with the $hp$-FEM method."],"url":"http://arxiv.org/abs/2404.19663v1","category":"math.CV"}
{"created":"2024-04-30 15:45:53","title":"Best polynomial approximation for non-autonomous linear ODEs in the $\\star$-product framework","abstract":"We present the first formulation of the optimal polynomial approximation of the solution of linear non-autonomous systems of ODEs in the framework of the so-called $\\star$-product. This product is the basis of new approaches for the solution of such ODEs, both in the analytical and the numerical sense. The paper shows how to formally state the problem and derives upper bounds for its error.","sentences":["We present the first formulation of the optimal polynomial approximation of the solution of linear non-autonomous systems of ODEs in the framework of the so-called $\\star$-product.","This product is the basis of new approaches for the solution of such ODEs, both in the analytical and the numerical sense.","The paper shows how to formally state the problem and derives upper bounds for its error."],"url":"http://arxiv.org/abs/2404.19645v1","category":"math.CA"}
{"created":"2024-04-30 15:44:57","title":"Cybersecurity Pathways Towards CE-Certified Autonomous Forestry Machines","abstract":"The increased importance of cybersecurity in autonomous machinery is becoming evident in the forestry domain. Forestry worksites are becoming more complex with the involvement of multiple systems and system of systems. Hence, there is a need to investigate how to address cybersecurity challenges for autonomous systems of systems in the forestry domain. Using a literature review and adapting standards from similar domains, as well as collaborative sessions with domain experts, we identify challenges towards CE-certified autonomous forestry machines focusing on cybersecurity and safety. Furthermore, we discuss the relationship between safety and cybersecurity risk assessment and their relation to AI, highlighting the need for a holistic methodology for their assurance.","sentences":["The increased importance of cybersecurity in autonomous machinery is becoming evident in the forestry domain.","Forestry worksites are becoming more complex with the involvement of multiple systems and system of systems.","Hence, there is a need to investigate how to address cybersecurity challenges for autonomous systems of systems in the forestry domain.","Using a literature review and adapting standards from similar domains, as well as collaborative sessions with domain experts, we identify challenges towards CE-certified autonomous forestry machines focusing on cybersecurity and safety.","Furthermore, we discuss the relationship between safety and cybersecurity risk assessment and their relation to AI, highlighting the need for a holistic methodology for their assurance."],"url":"http://arxiv.org/abs/2404.19643v1","category":"cs.SE"}
{"created":"2024-04-30 15:35:03","title":"SEArch: an execution infrastructure for service-based software systems","abstract":"The shift from monolithic applications to composition of distributed software initiated in the early twentieth, is based on the vision of software-as-service. This vision, found in many technologies such as RESTful APIs, advocates globally available services cooperating through an infrastructure providing (access to) distributed computational resources. Choreographies can support this vision by abstracting away local computation and rendering interoperability with message-passing: cooperation is achieved by sending and receiving messages. Following this choreographic paradigm, we develop SEArch, after Service Execution Architecture, a language-independent execution infrastructure capable of performing transparent dynamic reconfiguration of software artefacts. Choreographic mechanisms are used in SEArch to specify interoperability contracts, thus providing the support needed for automatic discovery and binding of services at runtime.","sentences":["The shift from monolithic applications to composition of distributed software initiated in the early twentieth, is based on the vision of software-as-service.","This vision, found in many technologies such as RESTful APIs, advocates globally available services cooperating through an infrastructure providing (access to) distributed computational resources.","Choreographies can support this vision by abstracting away local computation and rendering interoperability with message-passing: cooperation is achieved by sending and receiving messages.","Following this choreographic paradigm, we develop SEArch, after Service Execution Architecture, a language-independent execution infrastructure capable of performing transparent dynamic reconfiguration of software artefacts.","Choreographic mechanisms are used in SEArch to specify interoperability contracts, thus providing the support needed for automatic discovery and binding of services at runtime."],"url":"http://arxiv.org/abs/2404.19633v1","category":"cs.SE"}
{"created":"2024-04-30 15:24:53","title":"Machine learning of continuous and discrete variational ODEs with convergence guarantee and uncertainty quantification","abstract":"The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data. The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design. A rigorous proof of convergence as the distance between observation data points converges to zero is provided. Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques. We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification. The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces.","sentences":["The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data.","The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design.","A rigorous proof of convergence as the distance between observation data points converges to zero is provided.","Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques.","We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification.","The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces."],"url":"http://arxiv.org/abs/2404.19626v1","category":"math.NA"}
{"created":"2024-04-30 15:23:36","title":"Dual-Port Grid-Forming Interconnecting Power Converters in Hybrid AC/DC Grids","abstract":"Interconnecting power converters (IPC) are the main elements enabling the interconnection of multiple high-voltage alternating current (HVAC) and high-voltage direct current (HVDC) subgrids. These converters can be classified either as grid-forming or grid-following. These roles can be assigned to both ac and dc terminals. This work compares state-of-the-art single-port grid-forming and grid-following control schemes with a dual-port grid-forming control scheme, which can simultaneously form a stable voltage on the ac and the dc sides. The dual-port grid-forming small-signal stability and dynamic behavior under fluctuations in the power flow are studied and compared against state-of-the-art control architectures. Moreover, the dual-port control scheme is validated and tested on a down-scaled laboratory platform with several transient events.","sentences":["Interconnecting power converters (IPC) are the main elements enabling the interconnection of multiple high-voltage alternating current (HVAC) and high-voltage direct current (HVDC) subgrids.","These converters can be classified either as grid-forming or grid-following.","These roles can be assigned to both ac and dc terminals.","This work compares state-of-the-art single-port grid-forming and grid-following control schemes with a dual-port grid-forming control scheme, which can simultaneously form a stable voltage on the ac and the dc sides.","The dual-port grid-forming small-signal stability and dynamic behavior under fluctuations in the power flow are studied and compared against state-of-the-art control architectures.","Moreover, the dual-port control scheme is validated and tested on a down-scaled laboratory platform with several transient events."],"url":"http://arxiv.org/abs/2404.19625v1","category":"eess.SY"}
{"created":"2024-04-30 15:20:41","title":"Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference","abstract":"Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We further develop two new estimators for estimating the proposed ideal loss. We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effect are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.","sentences":["Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection.","Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect.","To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect.","On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect.","We further develop two new estimators for estimating the proposed ideal loss.","We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effect are present, while the existing methods are biased.","Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.19620v1","category":"cs.LG"}
{"created":"2024-04-30 15:19:51","title":"Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture","abstract":"Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.","sentences":["Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default.","We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically.","In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics.","With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied.","In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture.","Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware.","This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system.","Code is available at https://xinyu-yi.github.io/PNP/."],"url":"http://arxiv.org/abs/2404.19619v1","category":"cs.GR"}
{"created":"2024-04-30 15:16:13","title":"More luminous red novae that require jets","abstract":"I study two intermediate luminosity optical transients (ILOTs) classified as luminous red novae (LRNe) and argue that their modeling with a common envelope evolution (CEE) without jets encounters challenges. LRNe are ILOTs powered by violent binary interaction. Although popular in the literature is to assume a CEE is the cause of LRNe, I here repeat an old claim that many LRNe are powered by grazing envelope evolution (GEE) events; the GEE might end in a CEE or a detached binary system. I find that the LRN AT 2021biy might have continued to experience mass ejection episodes after its eruption and, therefore, might not suffered a full CEE during the outburst. This adds to an earlier finding that a jet-less model does not account for some of its properties. I find that a suggested jet-less CEE model for the LRN AT 2019zhd does not reproduce its photosphere radius evolution. These results that challenge jet-less models of two LRNe strengthen a previous claim that jets play major roles in powering ILOTs and shaping their ejecta and that in many LRNe, the more compact companion launches the jets during a GEE.","sentences":["I study two intermediate luminosity optical transients (ILOTs) classified as luminous red novae (LRNe) and argue that their modeling with a common envelope evolution (CEE) without jets encounters challenges.","LRNe are ILOTs powered by violent binary interaction.","Although popular in the literature is to assume a CEE is the cause of LRNe, I here repeat an old claim that many LRNe are powered by grazing envelope evolution (GEE) events; the GEE might end in a CEE or a detached binary system.","I find that the LRN AT 2021biy might have continued to experience mass ejection episodes after its eruption and, therefore, might not suffered a full CEE during the outburst.","This adds to an earlier finding that a jet-less model does not account for some of its properties.","I find that a suggested jet-less CEE model for the LRN AT 2019zhd does not reproduce its photosphere radius evolution.","These results that challenge jet-less models of two LRNe strengthen a previous claim that jets play major roles in powering ILOTs and shaping their ejecta and that in many LRNe, the more compact companion launches the jets during a GEE."],"url":"http://arxiv.org/abs/2404.19617v1","category":"astro-ph.SR"}
{"created":"2024-04-30 15:13:57","title":"SemiPL: A Semi-supervised Method for Event Sound Source Localization","abstract":"In recent years, Event Sound Source Localization has been widely applied in various fields. Recent works typically relying on the contrastive learning framework show impressive performance. However, all work is based on large relatively simple datasets. It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services. In this paper, we apply the existing model to a more complex dataset, explore the influence of parameters on the model, and propose a semi-supervised improvement method SemiPL. With the increase in data quantity and the influence of label quality, self-supervised learning will be an unstoppable trend. The experiment shows that the parameter adjustment will positively affect the existing model. In particular, SSPL achieved an improvement of 12.2% cIoU and 0.56% AUC in Chaotic World compared to the results provided. The code is available at: https://github.com/ly245422/SSPL","sentences":["In recent years, Event Sound Source Localization has been widely applied in various fields.","Recent works typically relying on the contrastive learning framework show impressive performance.","However, all work is based on large relatively simple datasets.","It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services.","In this paper, we apply the existing model to a more complex dataset, explore the influence of parameters on the model, and propose a semi-supervised improvement method SemiPL.","With the increase in data quantity and the influence of label quality, self-supervised learning will be an unstoppable trend.","The experiment shows that the parameter adjustment will positively affect the existing model.","In particular, SSPL achieved an improvement of 12.2% cIoU and 0.56% AUC in Chaotic World compared to the results provided.","The code is available at: https://github.com/ly245422/SSPL"],"url":"http://arxiv.org/abs/2404.19615v1","category":"cs.CV"}
{"created":"2024-04-30 15:11:34","title":"Quantum Cloud Computing: Trends and Challenges","abstract":"Quantum computing (QC) is a new paradigm that will revolutionize various areas of computing, especially cloud computing. QC, still in its infancy, is a costly technology capable of operating in highly isolated environments due to its rapid response to environmental factors. For this reason, it is still a challenging technology for researchers to reach. Integrating QC into an isolated remote server, like a cloud, and making it available to users can overcome these problems. Furthermore, experts predict that QC, with its ability to swiftly resolve complex and computationally intensive operations, will offer significant benefits in systems that process large amounts of data, like cloud computing. This article presents the vision and challenges for the quantum cloud computing (QCC) paradigm that will emerge with the integration of quantum and cloud computing. Next, we present the advantages of QC over classical computing applications. We analyze the effects of QC on cloud systems, such as cost, security, and scalability. Besides all of these advantages, we highlight research gaps in QCC, such as qubit stability and efficient resource allocation. This article identifies QCC's advantages and challenges for future research, highlighting research gaps.","sentences":["Quantum computing (QC) is a new paradigm that will revolutionize various areas of computing, especially cloud computing.","QC, still in its infancy, is a costly technology capable of operating in highly isolated environments due to its rapid response to environmental factors.","For this reason, it is still a challenging technology for researchers to reach.","Integrating QC into an isolated remote server, like a cloud, and making it available to users can overcome these problems.","Furthermore, experts predict that QC, with its ability to swiftly resolve complex and computationally intensive operations, will offer significant benefits in systems that process large amounts of data, like cloud computing.","This article presents the vision and challenges for the quantum cloud computing (QCC) paradigm that will emerge with the integration of quantum and cloud computing.","Next, we present the advantages of QC over classical computing applications.","We analyze the effects of QC on cloud systems, such as cost, security, and scalability.","Besides all of these advantages, we highlight research gaps in QCC, such as qubit stability and efficient resource allocation.","This article identifies QCC's advantages and challenges for future research, highlighting research gaps."],"url":"http://arxiv.org/abs/2404.19612v1","category":"cs.DC"}
{"created":"2024-04-30 14:52:56","title":"Log prismatic $F$-crystals and purity","abstract":"Our goal is to study $p$-adic local systems on a rigid-analytic variety with semistable formal model. We prove that such a local system is semistable if and only if so are its restrictions to the points corresponding to the irreducible components of the special fiber. For this, the main body of the paper concerns analytic prismatic $F$-crystals on the absolute logarithmic prismatic site of a semistable $p$-adic log formal scheme. Analyzing Breuil-Kisin log prisms, we obtain a prismatic purity theorem and deduce the above purity theorem for semistable local systems.","sentences":["Our goal is to study $p$-adic local systems on a rigid-analytic variety with semistable formal model.","We prove that such a local system is semistable if and only if so are its restrictions to the points corresponding to the irreducible components of the special fiber.","For this, the main body of the paper concerns analytic prismatic $F$-crystals on the absolute logarithmic prismatic site of a semistable $p$-adic log formal scheme.","Analyzing Breuil-Kisin log prisms, we obtain a prismatic purity theorem and deduce the above purity theorem for semistable local systems."],"url":"http://arxiv.org/abs/2404.19603v1","category":"math.NT"}
{"created":"2024-04-30 14:49:26","title":"Parity and time-reversal symmetry violation in diatomic molecules: LaO, LaS and LuO","abstract":"The violation of parity (P) and time-reversal (T) symmetry is enhanced in the LaS, LaO and LuO molecules due to the existence of states of opposite parity with small energy differences and the presence of heavy nuclei. We calculate the molecular enhancement for the P, T-violating electron electric dipole moment ($W_{\\mathrm{d}}$), scalar-pseudoscalar nucleon-electron interaction ($W_{\\mathrm{s}}$), nuclear magnetic quadrupole moment ($W_{\\mathrm{M}}$), and for the nuclear spin-dependent P-violating anapole moment ($W_{\\mathrm{A}}$). We use the relativistic 4-components coupled cluster method and perform a systematic study to estimate the associated uncertainties in our approach. We find that the individual contribution of each computational parameter to the total uncertainty in a system is approximately the same for all the calculated enhancement factors, summing up to a total uncertainty of $\\sim7$\\%. We discuss the energy shifts and matrix elements associated with the calculated molecular enhancement factors and relate them to higher-energy P- and P, T- violating interactions.","sentences":["The violation of parity (P) and time-reversal (T) symmetry is enhanced in the LaS, LaO and LuO molecules due to the existence of states of opposite parity with small energy differences and the presence of heavy nuclei.","We calculate the molecular enhancement for the P, T-violating electron electric dipole moment ($W_{\\mathrm{d}}$), scalar-pseudoscalar nucleon-electron interaction ($W_{\\mathrm{s}}$), nuclear magnetic quadrupole moment ($W_{\\mathrm{M}}$), and for the nuclear spin-dependent P-violating anapole moment ($W_{\\mathrm{A}}$).","We use the relativistic 4-components coupled cluster method and perform a systematic study to estimate the associated uncertainties in our approach.","We find that the individual contribution of each computational parameter to the total uncertainty in a system is approximately the same for all the calculated enhancement factors, summing up to a total uncertainty of $\\sim7$\\%.","We discuss the energy shifts and matrix elements associated with the calculated molecular enhancement factors and relate them to higher-energy P- and P, T- violating interactions."],"url":"http://arxiv.org/abs/2404.19599v1","category":"physics.atom-ph"}
{"created":"2024-04-30 14:26:14","title":"Boundary effect and quantum phases in spin chains","abstract":"Boundary effect is a widespread idea in many-body theories. However, it is more of a conceptual notion than a rigorously defined physical quantity. One can quantify the boundary effect by comparing two ground states of the same physical model, which differ only slightly in system size. Here, we analyze the quantity, which we call a boundary effect function, for an XXZ spin-1/2 model using density matrix renormalization group calculations. We find that three quantum phases of the model manifest as different functional forms of the boundary effect function. As a result, the quantum phase transition of the model is associated with a nonanalytic change of the boundary effect function. This work thus provides and concretizes a novel perspective on the relationship between bulk and boundary properties of ground states.","sentences":["Boundary effect is a widespread idea in many-body theories.","However, it is more of a conceptual notion than a rigorously defined physical quantity.","One can quantify the boundary effect by comparing two ground states of the same physical model, which differ only slightly in system size.","Here, we analyze the quantity, which we call a boundary effect function, for an XXZ spin-1/2 model using density matrix renormalization group calculations.","We find that three quantum phases of the model manifest as different functional forms of the boundary effect function.","As a result, the quantum phase transition of the model is associated with a nonanalytic change of the boundary effect function.","This work thus provides and concretizes a novel perspective on the relationship between bulk and boundary properties of ground states."],"url":"http://arxiv.org/abs/2404.19588v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 14:22:33","title":"Integrating Visuo-tactile Sensing with Haptic Feedback for Teleoperated Robot Manipulation","abstract":"Telerobotics enables humans to overcome spatial constraints and allows them to physically interact with the environment in remote locations. However, the sensory feedback provided by the system to the operator is often purely visual, limiting the operator's dexterity in manipulation tasks. In this work, we address this issue by equipping the robot's end-effector with high-resolution visuotactile GelSight sensors. Using low-cost MANUS-Gloves, we provide the operator with haptic feedback about forces acting at the points of contact in the form of vibration signals. We propose two different methods for estimating these forces; one based on estimating the movement of markers on the sensor surface and one deep-learning approach. Additionally, we integrate our system into a virtual-reality teleoperation pipeline in which a human operator controls both arms of a Tiago robot while receiving visual and haptic feedback. We believe that integrating haptic feedback is a crucial step for dexterous manipulation in teleoperated robotic systems.","sentences":["Telerobotics enables humans to overcome spatial constraints and allows them to physically interact with the environment in remote locations.","However, the sensory feedback provided by the system to the operator is often purely visual, limiting the operator's dexterity in manipulation tasks.","In this work, we address this issue by equipping the robot's end-effector with high-resolution visuotactile GelSight sensors.","Using low-cost MANUS-Gloves, we provide the operator with haptic feedback about forces acting at the points of contact in the form of vibration signals.","We propose two different methods for estimating these forces; one based on estimating the movement of markers on the sensor surface and one deep-learning approach.","Additionally, we integrate our system into a virtual-reality teleoperation pipeline in which a human operator controls both arms of a Tiago robot while receiving visual and haptic feedback.","We believe that integrating haptic feedback is a crucial step for dexterous manipulation in teleoperated robotic systems."],"url":"http://arxiv.org/abs/2404.19585v1","category":"cs.RO"}
{"created":"2024-04-30 14:15:16","title":"New EVENODD+ Codes with More Flexible Parameters and Lower Complexity","abstract":"EVENODD+ codes are binary maximum distance separable (MDS) array codes for correcting double disk failures in RAID-6 with asymptotically optimal encoding/decoding/update complexities. However, the number of bits stored in each disk of EVENODD+ codes should be an odd number minus one. In this paper, we present a new construction of EVENODD+ codes that have more flexible parameters. The number of bits stored in each disk of our codes is an odd minus one times any positive integer. Moreover, our codes not only have asymptotically optimal encoding/decoding/update complexities but also have lower encoding/decoding/update complexities than the existing EVENODD+ codes.","sentences":["EVENODD+ codes are binary maximum distance separable (MDS) array codes for correcting double disk failures in RAID-6 with asymptotically optimal encoding/decoding/update complexities.","However, the number of bits stored in each disk of EVENODD+ codes should be an odd number minus one.","In this paper, we present a new construction of EVENODD+ codes that have more flexible parameters.","The number of bits stored in each disk of our codes is an odd minus one times any positive integer.","Moreover, our codes not only have asymptotically optimal encoding/decoding/update complexities but also have lower encoding/decoding/update complexities than the existing EVENODD+ codes."],"url":"http://arxiv.org/abs/2404.19578v1","category":"cs.IT"}
{"created":"2024-04-30 14:09:14","title":"A Spatio-Temporal based Frame Indexing Algorithm for QoS Improvement in Live Low-Motion Video Streaming","abstract":"Real-time video life streaming of events over a network continued to gain more popularity among the populace. However, there is need to ensure the judicious utilization of allocated bandwidth without compromising the Quality of Service (QoS) of the system. In this regard, this paper presents an approach based on spatio-temporal frame indexing that detects and eliminate redundancy within and across captured frame, prior transmission from the server to clients. The standard and local low motion videos were the two scenarios considered in evaluating the performance of the proposed algorithm. Results obtained showed that the proposed approach achieved an improvement of 5.13%, 15.8% and 5%, 15.6% improvement in terms of the buffer size and compression ratio. Though with a tradeoff of the frame-built time, where both the standard and local frame indexing outperforms the proposed scheme with 10.8% and 8.71% respectively.","sentences":["Real-time video life streaming of events over a network continued to gain more popularity among the populace.","However, there is need to ensure the judicious utilization of allocated bandwidth without compromising the Quality of Service (QoS) of the system.","In this regard, this paper presents an approach based on spatio-temporal frame indexing that detects and eliminate redundancy within and across captured frame, prior transmission from the server to clients.","The standard and local low motion videos were the two scenarios considered in evaluating the performance of the proposed algorithm.","Results obtained showed that the proposed approach achieved an improvement of 5.13%, 15.8% and 5%, 15.6% improvement in terms of the buffer size and compression ratio.","Though with a tradeoff of the frame-built time, where both the standard and local frame indexing outperforms the proposed scheme with 10.8% and 8.71% respectively."],"url":"http://arxiv.org/abs/2404.19574v1","category":"cs.CV"}
{"created":"2024-04-30 14:03:07","title":"Morphodynamics of chloroplast network control light-avoidance response in the non-motile dinoflagellate Pyrocystis lunula","abstract":"Photosynthetic algae play a significant role in oceanic carbon capture. Their performance, however, is constantly challenged by fluctuations in environmental light conditions. Here, we show that the non-motile single-celled marine dinoflagellate Pyrocystis lunula can internally contract its chloroplast network in response to light. By exposing the cell to various physiological light conditions and applying temporal illumination sequences, we find that network morphodynamics follows simple rules, as established in a mathematical model. Our analysis of the chloroplast structure reveals that its unusual reticulated morphology constitutes properties similar to auxetic metamaterials, facilitating drastic deformations for light-avoidance, while confined by the cell wall. Our study shows how the topologically complex network of chloroplasts is crucial in supporting the dinoflagellate's adaptation to varying light conditions, thereby facilitating essential life-sustaining processes.","sentences":["Photosynthetic algae play a significant role in oceanic carbon capture.","Their performance, however, is constantly challenged by fluctuations in environmental light conditions.","Here, we show that the non-motile single-celled marine dinoflagellate Pyrocystis lunula can internally contract its chloroplast network in response to light.","By exposing the cell to various physiological light conditions and applying temporal illumination sequences, we find that network morphodynamics follows simple rules, as established in a mathematical model.","Our analysis of the chloroplast structure reveals that its unusual reticulated morphology constitutes properties similar to auxetic metamaterials, facilitating drastic deformations for light-avoidance, while confined by the cell wall.","Our study shows how the topologically complex network of chloroplasts is crucial in supporting the dinoflagellate's adaptation to varying light conditions, thereby facilitating essential life-sustaining processes."],"url":"http://arxiv.org/abs/2404.19570v1","category":"physics.bio-ph"}
{"created":"2024-04-30 13:55:26","title":"Relativity with or without light and Maxwell","abstract":"The complex relationship between Einstein's second postulate and the Maxwell electromagnetic theory is elucidated. A simple deduction of the main results of the Ignatowski approach to the theory of relativity is given. The peculiar status of the principle of relativity among the Maxwellians is illustrated.","sentences":["The complex relationship between Einstein's second postulate and the Maxwell electromagnetic theory is elucidated.","A simple deduction of the main results of the Ignatowski approach to the theory of relativity is given.","The peculiar status of the principle of relativity among the Maxwellians is illustrated."],"url":"http://arxiv.org/abs/2404.19566v1","category":"physics.class-ph"}
{"created":"2024-04-30 13:51:44","title":"Time, Travel, and Energy in the Uniform Dispersion Problem","abstract":"We investigate the algorithmic problem of uniformly dispersing a swarm of robots in an unknown, gridlike environment. In this setting, our goal is to comprehensively study the relationships between performance metrics and robot capabilities. We introduce a formal model comparing dispersion algorithms based on makespan, traveled distance, energy consumption, sensing, communication, and memory. Using this framework, we classify several uniform dispersion algorithms according to their capability requirements and performance. We prove that while makespan and travel can be minimized in all environments, energy cannot, as long as the swarm's sensing range is bounded. In contrast, we show that energy can be minimized even by simple, ``ant-like\" robots in synchronous settings and asymptotically minimized in asynchronous settings, provided the environment is topologically simply connected. Our findings offer insights into fundamental limitations that arise when designing swarm robotics systems for exploring unknown environments, highlighting the impact of environment's topology on the feasibility of energy-efficient dispersion.","sentences":["We investigate the algorithmic problem of uniformly dispersing a swarm of robots in an unknown, gridlike environment.","In this setting, our goal is to comprehensively study the relationships between performance metrics and robot capabilities.","We introduce a formal model comparing dispersion algorithms based on makespan, traveled distance, energy consumption, sensing, communication, and memory.","Using this framework, we classify several uniform dispersion algorithms according to their capability requirements and performance.","We prove that while makespan and travel can be minimized in all environments, energy cannot, as long as the swarm's sensing range is bounded.","In contrast, we show that energy can be minimized even by simple, ``ant-like\" robots in synchronous settings and asymptotically minimized in asynchronous settings, provided the environment is topologically simply connected.","Our findings offer insights into fundamental limitations that arise when designing swarm robotics systems for exploring unknown environments, highlighting the impact of environment's topology on the feasibility of energy-efficient dispersion."],"url":"http://arxiv.org/abs/2404.19564v1","category":"cs.RO"}
{"created":"2024-04-30 13:31:37","title":"Entanglement-assisted phase estimation algorithm for calculating dynamical response functions","abstract":"Dynamical response functions are fundamental quantities to describe the excited-state properties in quantum many-body systems. Quantum algorithms have been proposed to evaluate these quantities by means of quantum phase estimation (QPE), where the energy spectra are directly extracted from the QPE measurement outcomes in the frequency domain. Accurate estimation of excitation energies and transition probabilities with these QPE-based approaches is, however, challenging because of the problem of spectral leakage (or peak broadening) which is inherent in the QPE algorithm. To overcome this issue, in this work we consider an extension of the QPE-based approach adopting the optimal entangled input states, which is known to achieve the Heisenberg-limited scaling for the estimation precision. We demonstrate that with this method the peaks in the calculated energy spectra are more localized than those calculated by the original QPE-based approaches, suggesting the mitigation of the spectral leakage problem. By analyzing the probability distribution with the entangled phase estimation, we propose a simple scheme to better estimate both the transition energies and the corresponding transition probabilities of the peaks of interest in the spectra. The validity of our prescription is demonstrated by numerical simulations in various quantum many-body problems: the spectral function of a simple electron-plasmon model in condensed-matter physics, the dipole transitions of the H$_2$O molecule in quantum chemistry, and the electromagnetic transitions of the $^6$Li nucleus in nuclear physics.","sentences":["Dynamical response functions are fundamental quantities to describe the excited-state properties in quantum many-body systems.","Quantum algorithms have been proposed to evaluate these quantities by means of quantum phase estimation (QPE), where the energy spectra are directly extracted from the QPE measurement outcomes in the frequency domain.","Accurate estimation of excitation energies and transition probabilities with these QPE-based approaches is, however, challenging because of the problem of spectral leakage (or peak broadening) which is inherent in the QPE algorithm.","To overcome this issue, in this work we consider an extension of the QPE-based approach adopting the optimal entangled input states, which is known to achieve the Heisenberg-limited scaling for the estimation precision.","We demonstrate that with this method the peaks in the calculated energy spectra are more localized than those calculated by the original QPE-based approaches, suggesting the mitigation of the spectral leakage problem.","By analyzing the probability distribution with the entangled phase estimation, we propose a simple scheme to better estimate both the transition energies and the corresponding transition probabilities of the peaks of interest in the spectra.","The validity of our prescription is demonstrated by numerical simulations in various quantum many-body problems: the spectral function of a simple electron-plasmon model in condensed-matter physics, the dipole transitions of the H$_2$O molecule in quantum chemistry, and the electromagnetic transitions of the $^6$Li nucleus in nuclear physics."],"url":"http://arxiv.org/abs/2404.19554v1","category":"quant-ph"}
{"created":"2024-04-30 13:23:27","title":"Formation of Rydberg Crystals Induced by Quantum Melting in One-Dimension","abstract":"Quantum fluctuations in frustrated systems can lead to the emergence of complex many-body phases. However, the role of quantum fluctuations in frustration-free lattices is less explored and could provide an interesting avenue for exploring new physics, and perhaps easier to realize compared to frustrated lattice systems. Using Rydberg atoms with tunable interactions as a platform, we leverage strong van der Waals interactions and obtain a constrained model in one dimension with non-local fluctuations given by dipolar interactions alongside local fluctuations. The combined effect of such processes leads to intrinsically quantum-ordered Rydberg crystals through the order-by-disorder mechanism. Finite-size analyses indicate that combined fluctuations drive the transition from disordered to ordered phases, contrary to the expected direction. We provide a theoretical description to understand the physics of order-by-disorder in one-dimensional systems, which are typically seen only in higher dimensions.","sentences":["Quantum fluctuations in frustrated systems can lead to the emergence of complex many-body phases.","However, the role of quantum fluctuations in frustration-free lattices is less explored and could provide an interesting avenue for exploring new physics, and perhaps easier to realize compared to frustrated lattice systems.","Using Rydberg atoms with tunable interactions as a platform, we leverage strong van der Waals interactions and obtain a constrained model in one dimension with non-local fluctuations given by dipolar interactions alongside local fluctuations.","The combined effect of such processes leads to intrinsically quantum-ordered Rydberg crystals through the order-by-disorder mechanism.","Finite-size analyses indicate that combined fluctuations drive the transition from disordered to ordered phases, contrary to the expected direction.","We provide a theoretical description to understand the physics of order-by-disorder in one-dimensional systems, which are typically seen only in higher dimensions."],"url":"http://arxiv.org/abs/2404.19551v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 13:17:49","title":"Cool-core, X-ray cavities and cold front revealed in RXCJ0352.9+1941 cluster by Chandra and GMRT observations","abstract":"This paper presents a comprehensive analysis of 30 ks Chandra and 46.8 ks (13 Hr) 1.4 GHz GMRT radio data on the cool-core cluster RXCJ0352.9+1941 with an objective to investigate AGN activities at its core. This study confirms a pair of X-ray cavities at projected distances of about 10.30 kpc and 20.80 kpc, respectively, on the NW and SE of the X-ray peak. GMRT L band (1.4 GHz) data revealed a bright radio source associated with the core of this cluster hosting multiple jet-like emissions. The spatial association of the X-ray cavities with the inner pair of radio jets confirm their origin due to AGN outbursts. The 1.4 GHz radio power ${\\rm 7.4 \\pm 0.8 \\times 10^{39} \\, erg\\, s^{-1}}$ is correlated with the mechanical power stored in the X-ray cavities ($\\sim7.90\\times 10^{44}$ erg s$^{-1}$), implying that the power injected by radio jets in the ICM is sufficient enough to offset the radiative losses. The X-shaped morphology of diffuse radio emission seems to be comprised of two pairs of orthogonal radio jets, likely formed due to a spin-flip of jets due to the merger of two systems. The X-ray surface brightness analysis of the ICM in its environment revealed two non-uniform, extended spiral-like emission structures on either side of the core, pointing towards the sloshing of gas due to a minor merger and might have resulted in a cold front at $\\sim$31 arcsec (62 kpc) with a temperature jump of 1.44 keV.","sentences":["This paper presents a comprehensive analysis of 30 ks Chandra and 46.8 ks (13 Hr) 1.4 GHz GMRT radio data on the cool-core cluster RXCJ0352.9+1941 with an objective to investigate AGN activities at its core.","This study confirms a pair of X-ray cavities at projected distances of about 10.30 kpc and 20.80 kpc, respectively, on the NW and SE of the X-ray peak.","GMRT L band (1.4 GHz) data revealed a bright radio source associated with the core of this cluster hosting multiple jet-like emissions.","The spatial association of the X-ray cavities with the inner pair of radio jets confirm their origin due to AGN outbursts.","The 1.4 GHz radio power ${\\rm 7.4 \\pm 0.8 \\times 10^{39} \\, erg\\, s^{-1}}$ is correlated with the mechanical power stored in the X-ray cavities ($\\sim7.90\\times 10^{44}$ erg s$^{-1}$), implying that the power injected by radio jets in the ICM is sufficient enough to offset the radiative losses.","The X-shaped morphology of diffuse radio emission seems to be comprised of two pairs of orthogonal radio jets, likely formed due to a spin-flip of jets due to the merger of two systems.","The X-ray surface brightness analysis of the ICM in its environment revealed two non-uniform, extended spiral-like emission structures on either side of the core, pointing towards the sloshing of gas due to a minor merger and might have resulted in a cold front at $\\sim$31 arcsec (62 kpc) with a temperature jump of 1.44 keV."],"url":"http://arxiv.org/abs/2404.19549v1","category":"astro-ph.HE"}
{"created":"2024-04-30 13:16:05","title":"Distributed Traffic Signal Control via Coordinated Maximum Pressure-plus-Penalty","abstract":"This paper develops an adaptive traffic control policy inspired by Maximum Pressure (MP) while imposing coordination across intersections. The proposed Coordinated Maximum Pressure-plus-Penalty (CMPP) control policy features a local objective for each intersection that consists of the total pressure within the neighborhood and a penalty accounting for the queue capacities and continuous green time for certain movements. The corresponding control task is reformulated as a distributed optimization problem and solved via two customized algorithms: one based on the alternating direction method of multipliers (ADMM) and the other follows a greedy heuristic augmented with a majority vote. CMPP not only provides a theoretical guarantee of queuing network stability but also outperforms several benchmark controllers in simulations on a large-scale real traffic network with lower average travel and waiting time per vehicle, as well as less network congestion. Furthermore, CPMM with the greedy algorithm enjoys comparable computational efficiency as fully decentralized controllers without significantly compromising the control performance, which highlights its great potential for real-world deployment.","sentences":["This paper develops an adaptive traffic control policy inspired by Maximum Pressure (MP) while imposing coordination across intersections.","The proposed Coordinated Maximum Pressure-plus-Penalty (CMPP) control policy features a local objective for each intersection that consists of the total pressure within the neighborhood and a penalty accounting for the queue capacities and continuous green time for certain movements.","The corresponding control task is reformulated as a distributed optimization problem and solved via two customized algorithms: one based on the alternating direction method of multipliers (ADMM) and the other follows a greedy heuristic augmented with a majority vote.","CMPP not only provides a theoretical guarantee of queuing network stability but also outperforms several benchmark controllers in simulations on a large-scale real traffic network with lower average travel and waiting time per vehicle, as well as less network congestion.","Furthermore, CPMM with the greedy algorithm enjoys comparable computational efficiency as fully decentralized controllers without significantly compromising the control performance, which highlights its great potential for real-world deployment."],"url":"http://arxiv.org/abs/2404.19547v1","category":"eess.SY"}
{"created":"2024-04-30 13:15:19","title":"Discrete de-Rham complex involving a discontinuous finite element space for velocities: the case of periodic straight triangular and Cartesian meshes","abstract":"The aim of this article is to derive discontinuous finite elements vector spaces which can be put in a discrete de-Rham complex for which an harmonic gap property may be proven. First, discontinuous finite element spaces inspired by classical N{\\'e}d{\\'e}lec or Raviart-Thomas conforming space are considered, and we prove that by relaxing the normal or tangential constraint, discontinuous spaces ensuring the harmonic gap property can be built. Then the triangular case is addressed, for which we prove that such a property holds for the classical discontinuous finite element space for vectors. On Cartesian meshes, this result does not hold for the classical discontinuous finite element space for vectors. We then show how to use the de-Rham complex found for triangular meshes for enriching the finite element space on Cartesian meshes in order to recover a de-Rham complex, on which the same harmonic gap property is proven.","sentences":["The aim of this article is to derive discontinuous finite elements vector spaces which can be put in a discrete de-Rham complex for which an harmonic gap property may be proven.","First, discontinuous finite element spaces inspired by classical N{\\'e}d{\\'e}lec or Raviart-Thomas conforming space are considered, and we prove that by relaxing the normal or tangential constraint, discontinuous spaces ensuring the harmonic gap property can be built.","Then the triangular case is addressed, for which we prove that such a property holds for the classical discontinuous finite element space for vectors.","On Cartesian meshes, this result does not hold for the classical discontinuous finite element space for vectors.","We then show how to use the de-Rham complex found for triangular meshes for enriching the finite element space on Cartesian meshes in order to recover a de-Rham complex, on which the same harmonic gap property is proven."],"url":"http://arxiv.org/abs/2404.19545v1","category":"math.NA"}
{"created":"2024-04-30 13:12:36","title":"Physics-Informed Machine Learning On Polar Ice: A Survey","abstract":"The mass loss of the polar ice sheets contributes considerably to ongoing sea-level rise and changing ocean circulation, leading to coastal flooding and risking the homes and livelihoods of tens of millions of people globally. To address the complex problem of ice behavior, physical models and data-driven models have been proposed in the literature. Although traditional physical models can guarantee physically meaningful results, they have limitations in producing high-resolution results. On the other hand, data-driven approaches require large amounts of high-quality and labeled data, which is rarely available in the polar regions. Hence, as a promising framework that leverages the advantages of physical models and data-driven methods, physics-informed machine learning (PIML) has been widely studied in recent years. In this paper, we review the existing algorithms of PIML, provide our own taxonomy based on the methods of combining physics and data-driven approaches, and analyze the advantages of PIML in the aspects of accuracy and efficiency. Further, our survey discusses some current challenges and highlights future opportunities, including PIML on sea ice studies, PIML with different combination methods and backbone networks, and neural operator methods.","sentences":["The mass loss of the polar ice sheets contributes considerably to ongoing sea-level rise and changing ocean circulation, leading to coastal flooding and risking the homes and livelihoods of tens of millions of people globally.","To address the complex problem of ice behavior, physical models and data-driven models have been proposed in the literature.","Although traditional physical models can guarantee physically meaningful results, they have limitations in producing high-resolution results.","On the other hand, data-driven approaches require large amounts of high-quality and labeled data, which is rarely available in the polar regions.","Hence, as a promising framework that leverages the advantages of physical models and data-driven methods, physics-informed machine learning (PIML) has been widely studied in recent years.","In this paper, we review the existing algorithms of PIML, provide our own taxonomy based on the methods of combining physics and data-driven approaches, and analyze the advantages of PIML in the aspects of accuracy and efficiency.","Further, our survey discusses some current challenges and highlights future opportunities, including PIML on sea ice studies, PIML with different combination methods and backbone networks, and neural operator methods."],"url":"http://arxiv.org/abs/2404.19536v1","category":"cs.LG"}
{"created":"2024-04-30 13:03:22","title":"Tree P{\u00f3}lya Splitting distributions for multivariate count data","abstract":"In this article, we develop a new class of multivariate distributions adapted for count data, called Tree P{\\'o}lya Splitting. This class results from the combination of a univariate distribution and singular multivariate distributions along a fixed partition tree. As we will demonstrate, these distributions are flexible, allowing for the modeling of complex dependencies (positive, negative, or null) at the observation level. Specifically, we present the theoretical properties of Tree P{\\'o}lya Splitting distributions by focusing primarily on marginal distributions, factorial moments, and dependency structures (covariance and correlations). The abundance of 17 species of Trichoptera recorded at 49 sites is used, on one hand, to illustrate the theoretical properties developed in this article on a concrete case, and on the other hand, to demonstrate the interest of this type of models, notably by comparing them to classical approaches in ecology or microbiome.","sentences":["In this article, we develop a new class of multivariate distributions adapted for count data, called Tree P{\\'o}lya Splitting.","This class results from the combination of a univariate distribution and singular multivariate distributions along a fixed partition tree.","As we will demonstrate, these distributions are flexible, allowing for the modeling of complex dependencies (positive, negative, or null) at the observation level.","Specifically, we present the theoretical properties of Tree P{\\'o}lya Splitting distributions by focusing primarily on marginal distributions, factorial moments, and dependency structures (covariance and correlations).","The abundance of 17 species of Trichoptera recorded at 49 sites is used, on one hand, to illustrate the theoretical properties developed in this article on a concrete case, and on the other hand, to demonstrate the interest of this type of models, notably by comparing them to classical approaches in ecology or microbiome."],"url":"http://arxiv.org/abs/2404.19528v1","category":"math.ST"}
{"created":"2024-04-30 12:50:20","title":"TRAC: a tool for data-aware coordination (with an application to smart contracts)","abstract":"We propose TRAC, a tool for the specification and verification of coordinated multiparty distributed systems. Relying on finite-state machines (FSMs) where transition labels look like Hoare triples, \\thetool can specify the coordination of the participants of a distributed protocol for instance an execution model akin blockchain smart contracts (SCs). In fact, the transitions of our FSMs yield guards, and assignments over data variables, and with participants binders. The latter allow us to model scenarios with an unbounded number of participants which can vary at run-time. We introduce a notion of well-formedness to rule out meaningless or problematic specifications. This notion is verified with TRAC and demonstrated on several case studies borrowed from the smart contracts domain. Then, we evaluate the performance of TRAC using a set of randomised examples, studying the correlations between the features supported and the time taken to decide well-formedness.","sentences":["We propose TRAC, a tool for the specification and verification of coordinated multiparty distributed systems.","Relying on finite-state machines (FSMs) where transition labels look like Hoare triples, \\thetool can specify the coordination of the participants of a distributed protocol for instance an execution model akin blockchain smart contracts (SCs).","In fact, the transitions of our FSMs yield guards, and assignments over data variables, and with participants binders.","The latter allow us to model scenarios with an unbounded number of participants which can vary at run-time.","We introduce a notion of well-formedness to rule out meaningless or problematic specifications.","This notion is verified with TRAC and demonstrated on several case studies borrowed from the smart contracts domain.","Then, we evaluate the performance of TRAC using a set of randomised examples, studying the correlations between the features supported and the time taken to decide well-formedness."],"url":"http://arxiv.org/abs/2404.19523v1","category":"cs.LO"}
{"created":"2024-04-30 12:50:18","title":"Stable properties under weakly geometrically flat maps","abstract":"In this note we show that a weakly geometrically flat map $\\pi$ : M $\\rightarrow$ N between pure dimensional complex spaces has the local lifting property for cycles. From this result we also deduce that, under these hypotheses, several properties of M are transferred to N.","sentences":["In this note we show that a weakly geometrically flat map $\\pi$ : M $\\rightarrow$ N between pure dimensional complex spaces has the local lifting property for cycles.","From this result we also deduce that, under these hypotheses, several properties of M are transferred to N."],"url":"http://arxiv.org/abs/2404.19522v1","category":"math.CV"}
{"created":"2024-04-30 12:50:03","title":"Passivation of Clustered DC Microgrids with Non-Monotone Loads","abstract":"In this paper, we consider the problem of voltage stability in DC networks containing uncertain loads with non-monotone incremental impedances and where the steady-state power availability is restricted to a subset of the buses in the network. We propose controllers for powered buses that guarantee voltage regulation and output strictly equilibrium independent passivity (OS-EIP) of the controlled buses, while buses without power are equipped with controllers that dampen their transient behaviour. The OS-EIP of a cluster containing both bus types is verified through a linear matrix inequality (LMI) condition, and the asymptotic stability of the overall microgrid with uncertain, non-monotone loads is ensured by interconnecting the OS-EIP clusters. By further employing singular perturbation theory, we show that the OS-EIP property of the clusters is robust against certain network parameter and topology changes.","sentences":["In this paper, we consider the problem of voltage stability in DC networks containing uncertain loads with non-monotone incremental impedances and where the steady-state power availability is restricted to a subset of the buses in the network.","We propose controllers for powered buses that guarantee voltage regulation and output strictly equilibrium independent passivity (OS-EIP) of the controlled buses, while buses without power are equipped with controllers that dampen their transient behaviour.","The OS-EIP of a cluster containing both bus types is verified through a linear matrix inequality (LMI) condition, and the asymptotic stability of the overall microgrid with uncertain, non-monotone loads is ensured by interconnecting the OS-EIP clusters.","By further employing singular perturbation theory, we show that the OS-EIP property of the clusters is robust against certain network parameter and topology changes."],"url":"http://arxiv.org/abs/2404.19520v1","category":"eess.SY"}
{"created":"2024-04-30 12:47:42","title":"Inexact subgradient methods for semialgebraic functions","abstract":"Motivated by the widespread use of approximate derivatives in machine learning and optimization, we study inexact subgradient methods with non-vanishing additive errors and step sizes. In the nonconvex semialgebraic setting, under boundedness assumptions, we prove that the method provides points that eventually fluctuate close to the critical set at a distance proportional to $\\epsilon^\\rho$ where $\\epsilon$ is the error in subgradient evaluation and $\\rho$ relates to the geometry of the problem. In the convex setting, we provide complexity results for the averaged values. We also obtain byproducts of independent interest, such as descent-like lemmas for nonsmooth nonconvex problems and some results on the limit of affine interpolants of differential inclusions.","sentences":["Motivated by the widespread use of approximate derivatives in machine learning and optimization, we study inexact subgradient methods with non-vanishing additive errors and step sizes.","In the nonconvex semialgebraic setting, under boundedness assumptions, we prove that the method provides points that eventually fluctuate close to the critical set at a distance proportional to $\\epsilon^\\rho$ where $\\epsilon$ is the error in subgradient evaluation and $\\rho$ relates to the geometry of the problem.","In the convex setting, we provide complexity results for the averaged values.","We also obtain byproducts of independent interest, such as descent-like lemmas for nonsmooth nonconvex problems and some results on the limit of affine interpolants of differential inclusions."],"url":"http://arxiv.org/abs/2404.19517v1","category":"math.OC"}
{"created":"2024-04-30 12:46:21","title":"Dimensional crossover in Kardar-Parisi-Zhang growth","abstract":"Two-dimensional (2D) KPZ growth is usually investigated on substrates of lateral sizes $L_x=L_y$, so that $L_x$ and the correlation length ($\\xi$) are the only relevant lengths determining the scaling behavior. However, in cylindrical geometry, as well as in flat rectangular substrates $L_x \\neq L_y$ and, thus, the surfaces can become correlated in a single direction, when $\\xi \\sim L_x \\ll L_y$. From extensive simulations of several KPZ models, we demonstrate that this yields a dimensional crossover in their dynamics, with the roughness scaling as $W \\sim t^{\\beta_{\\text{2D}}}$ for $t \\ll t_c$ and $W \\sim t^{\\beta_{\\text{1D}}}$ for $t \\gg t_c$, where $t_c \\sim L_x^{1/z_{2\\text{D}}}$. The height distributions (HDs) also cross over from the 2D flat [cylindrical] HD to the asymptotic Tracy-Widom GOE [GUE] distribution. Moreover, 2D-to-1D crossovers are found also in the asymptotic growth velocity and in the steady state regime of flat systems, where a family of universal HDs exists, interpolating between the 2D and 1D ones as $L_y/L_x$ increases. Importantly, the crossover scalings are fully determined and indicate a possible way to solve 2D KPZ models.","sentences":["Two-dimensional (2D) KPZ growth is usually investigated on substrates of lateral sizes $L_x=L_y$, so that $L_x$ and the correlation length ($\\xi$) are the only relevant lengths determining the scaling behavior.","However, in cylindrical geometry, as well as in flat rectangular substrates $L_x \\neq L_y$ and, thus, the surfaces can become correlated in a single direction, when $\\xi \\sim L_x \\ll L_y$. From extensive simulations of several KPZ models, we demonstrate that this yields a dimensional crossover in their dynamics, with the roughness scaling as $W \\sim t^{\\beta_{\\text{2D}}}$ for $t \\ll t_c$ and $W \\sim t^{\\beta_{\\text{1D}}}$ for $t \\gg t_c$, where $t_c \\sim L_x^{1/z_{2\\text{D}}}$.","The height distributions (HDs) also cross over from the 2D flat [cylindrical] HD to the asymptotic Tracy-Widom GOE [GUE] distribution.","Moreover, 2D-to-1D crossovers are found also in the asymptotic growth velocity and in the steady state regime of flat systems, where a family of universal HDs exists, interpolating between the 2D and 1D ones as $L_y/L_x$ increases.","Importantly, the crossover scalings are fully determined and indicate a possible way to solve 2D KPZ models."],"url":"http://arxiv.org/abs/2404.19516v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 12:43:11","title":"Temporal Graph ODEs for Irregularly-Sampled Time Series","abstract":"Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations. To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced. We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks.","sentences":["Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations.","To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced.","We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks."],"url":"http://arxiv.org/abs/2404.19508v1","category":"cs.LG"}
{"created":"2024-04-30 12:41:00","title":"Context-Aware Machine Translation with Source Coreference Explanation","abstract":"Despite significant improvements in enhancing the quality of translation, context-aware machine translation (MT) models underperform in many cases. One of the main reasons is that they fail to utilize the correct features from context when the context is too long or their models are overly complex. This can lead to the explain-away effect, wherein the models only consider features easier to explain predictions, resulting in inaccurate translations. To address this issue, we propose a model that explains the decisions made for translation by predicting coreference features in the input. We construct a model for input coreference by exploiting contextual features from both the input and translation output representations on top of an existing MT model. We evaluate and analyze our method in the WMT document-level translation task of English-German dataset, the English-Russian dataset, and the multilingual TED talk dataset, demonstrating an improvement of over 1.0 BLEU score when compared with other context-aware models.","sentences":["Despite significant improvements in enhancing the quality of translation, context-aware machine translation (MT) models underperform in many cases.","One of the main reasons is that they fail to utilize the correct features from context when the context is too long or their models are overly complex.","This can lead to the explain-away effect, wherein the models only consider features easier to explain predictions, resulting in inaccurate translations.","To address this issue, we propose a model that explains the decisions made for translation by predicting coreference features in the input.","We construct a model for input coreference by exploiting contextual features from both the input and translation output representations on top of an existing MT model.","We evaluate and analyze our method in the WMT document-level translation task of English-German dataset, the English-Russian dataset, and the multilingual TED talk dataset, demonstrating an improvement of over 1.0 BLEU score when compared with other context-aware models."],"url":"http://arxiv.org/abs/2404.19505v1","category":"cs.CL"}
{"created":"2024-04-30 12:37:53","title":"Interplay between Vector-like Lepton and Seesaw Mechanism:Oblique Corrections","abstract":"The non-vanishing neutrino mass strongly hints the existence of right-handed neutrinos (RHNs), singlets of the standard model (SM). However, they are highly decoupled from the SM and difficult to probe. In this work, we consider the Majorana RHNs from the type-I seesaw mechanism may well mix with the heavy neutral lepton dwelling in certain vector-like lepton (VLL), thus acquiring a sizable electroweak charge. Such a simple scenario yields many interesting consequences, and the imprint on oblique corrections, well expected from the mass splitting between components of VLL by virtue of VLL-RHN mixing, is our focus here. We analytically calculate the Peskin-Takeuchi parameters S, T and U with full details, carefully treating the Majorana loop to obtain the self consistent expressions free of divergence. Then, we constrain on the VLL-RHN system which only gives a sizable $T$ parameter using the PDG-2021 data and CDF-II data, separately, by imposing $T\\lesssim{\\cal O}(0.1)$. It is found that for the RHN and VLL below the TeV scale, with a properly large mixing, stands in the frontier of the electroweak precision test such as W-boson mass.","sentences":["The non-vanishing neutrino mass strongly hints the existence of right-handed neutrinos (RHNs), singlets of the standard model (SM).","However, they are highly decoupled from the SM and difficult to probe.","In this work, we consider the Majorana RHNs from the type-I seesaw mechanism may well mix with the heavy neutral lepton dwelling in certain vector-like lepton (VLL), thus acquiring a sizable electroweak charge.","Such a simple scenario yields many interesting consequences, and the imprint on oblique corrections, well expected from the mass splitting between components of VLL by virtue of VLL-RHN mixing, is our focus here.","We analytically calculate the Peskin-Takeuchi parameters S, T and U with full details, carefully treating the Majorana loop to obtain the self consistent expressions free of divergence.","Then, we constrain on the VLL-RHN system which only gives a sizable $T$ parameter using the PDG-2021 data and CDF-II data, separately, by imposing $T\\lesssim{\\cal O}(0.1)$.","It is found that for the RHN and VLL below the TeV scale, with a properly large mixing, stands in the frontier of the electroweak precision test such as W-boson mass."],"url":"http://arxiv.org/abs/2404.19502v1","category":"hep-ph"}
{"created":"2024-04-30 12:31:03","title":"Light Cone Cancellation for Variational Quantum Eigensolver Ansatz","abstract":"Variational Quantum Algorithms (VQAs) represent a class of algorithms that utilize a hybrid approach, combining classical and quantum computing techniques. In this approach, classical computers serve as optimizers that update circuit parameters to find approximate solutions to complex problems. In this study, we apply a method known as Light Cone Cancellation (LCC) to optimize variational circuits, effectively reducing the required number of qubits and gates for circuit simulation. We then evaluate the performance of LCC one of the VQAs -- the Variational Quantum Eigensolver (VQE) -- to address the Max-Cut problem. Compared with the Quantum Approximate Optimization Algorithm (QAOA), VQE offers greater degrees of freedom at lower circuit depths. By applying LCC to VQE, we can shift the complexity of circuit simulation from the number of qubits to the number of edges in the graph, i.e., from exponential time to polynomial time. This enables us to solve large problems up to 50 vertices, without actually simulating the entire circuit. From our simulation in a 7-qubit and a 27-qubit noisy devices, we show that LCC yields higher approximation ratios than those cases without LCC, implying that the effect of noise is reduced when LCC is applied.","sentences":["Variational Quantum Algorithms (VQAs) represent a class of algorithms that utilize a hybrid approach, combining classical and quantum computing techniques.","In this approach, classical computers serve as optimizers that update circuit parameters to find approximate solutions to complex problems.","In this study, we apply a method known as Light Cone Cancellation (LCC) to optimize variational circuits, effectively reducing the required number of qubits and gates for circuit simulation.","We then evaluate the performance of LCC one of the VQAs -- the Variational Quantum Eigensolver (VQE) -- to address the Max-Cut problem.","Compared with the Quantum Approximate Optimization Algorithm (QAOA), VQE offers greater degrees of freedom at lower circuit depths.","By applying LCC to VQE, we can shift the complexity of circuit simulation from the number of qubits to the number of edges in the graph, i.e., from exponential time to polynomial time.","This enables us to solve large problems up to 50 vertices, without actually simulating the entire circuit.","From our simulation in a 7-qubit and a 27-qubit noisy devices, we show that LCC yields higher approximation ratios than those cases without LCC, implying that the effect of noise is reduced when LCC is applied."],"url":"http://arxiv.org/abs/2404.19497v1","category":"quant-ph"}
{"created":"2024-04-30 12:23:37","title":"Reducing Communication Overhead in the IoT-Edge-Cloud Continuum: A Survey on Protocols and Data Reduction Strategies","abstract":"The adoption of the Internet of Things (IoT) deployments has led to a sharp increase in network traffic as a vast number of IoT devices communicate with each other and IoT services through the IoT-edge-cloud continuum. This network traffic increase poses a major challenge to the global communications infrastructure since it hinders communication performance and also puts significant strain on the energy consumption of IoT devices. To address these issues, efficient and collaborative IoT solutions which enable information exchange while reducing the transmitted data and associated network traffic are crucial. This survey provides a comprehensive overview of the communication technologies and protocols as well as data reduction strategies that contribute to this goal. First, we present a comparative analysis of prevalent communication technologies in the IoT domain, highlighting their unique characteristics and exploring the potential for protocol composition and joint usage to enhance overall communication efficiency within the IoT-edge-cloud continuum. Next, we investigate various data traffic reduction techniques tailored to the IoT-edge-cloud context and evaluate their applicability and effectiveness on resource-constrained and devices. Finally, we investigate the emerging concepts that have the potential to further reduce the communication overhead in the IoT-edge-cloud continuum, including cross-layer optimization strategies and Edge AI techniques for IoT data reduction. The paper offers a comprehensive roadmap for developing efficient and scalable solutions across the layers of the IoT-edge-cloud continuum that are beneficial for real-time processing to alleviate network congestion in complex IoT environments.","sentences":["The adoption of the Internet of Things (IoT) deployments has led to a sharp increase in network traffic as a vast number of IoT devices communicate with each other and IoT services through the IoT-edge-cloud continuum.","This network traffic increase poses a major challenge to the global communications infrastructure since it hinders communication performance and also puts significant strain on the energy consumption of IoT devices.","To address these issues, efficient and collaborative IoT solutions which enable information exchange while reducing the transmitted data and associated network traffic are crucial.","This survey provides a comprehensive overview of the communication technologies and protocols as well as data reduction strategies that contribute to this goal.","First, we present a comparative analysis of prevalent communication technologies in the IoT domain, highlighting their unique characteristics and exploring the potential for protocol composition and joint usage to enhance overall communication efficiency within the IoT-edge-cloud continuum.","Next, we investigate various data traffic reduction techniques tailored to the IoT-edge-cloud context and evaluate their applicability and effectiveness on resource-constrained and devices.","Finally, we investigate the emerging concepts that have the potential to further reduce the communication overhead in the IoT-edge-cloud continuum, including cross-layer optimization strategies and Edge AI techniques for IoT data reduction.","The paper offers a comprehensive roadmap for developing efficient and scalable solutions across the layers of the IoT-edge-cloud continuum that are beneficial for real-time processing to alleviate network congestion in complex IoT environments."],"url":"http://arxiv.org/abs/2404.19492v1","category":"cs.NI"}
{"created":"2024-04-30 12:18:47","title":"EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision","abstract":"Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud. As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems. However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications. While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators. In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras. It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme enabling the low-latency execution of multi-layer GNNs. We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge.","sentences":["Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud.","As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems.","However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications.","While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators.","In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras.","It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme enabling the low-latency execution of multi-layer GNNs.","We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge."],"url":"http://arxiv.org/abs/2404.19489v1","category":"cs.CV"}
{"created":"2024-04-30 12:15:50","title":"Finetuning greedy kernel models by exchange algorithms","abstract":"Kernel based approximation offers versatile tools for high-dimensional approximation, which can especially be leveraged for surrogate modeling. For this purpose, both \"knot insertion\" and \"knot removal\" approaches aim at choosing a suitable subset of the data, in order to obtain a sparse but nevertheless accurate kernel model. In the present work, focussing on kernel based interpolation, we aim at combining these two approaches to further improve the accuracy of kernel models, without increasing the computational complexity of the final kernel model. For this, we introduce a class of kernel exchange algorithms (KEA). The resulting KEA algorithm can be used for finetuning greedy kernel surrogate models, allowing for an reduction of the error up to 86.4% (17.2% on average) in our experiments.","sentences":["Kernel based approximation offers versatile tools for high-dimensional approximation, which can especially be leveraged for surrogate modeling.","For this purpose, both \"knot insertion\" and \"knot removal\" approaches aim at choosing a suitable subset of the data, in order to obtain a sparse but nevertheless accurate kernel model.","In the present work, focussing on kernel based interpolation, we aim at combining these two approaches to further improve the accuracy of kernel models, without increasing the computational complexity of the final kernel model.","For this, we introduce a class of kernel exchange algorithms (KEA).","The resulting KEA algorithm can be used for finetuning greedy kernel surrogate models, allowing for an reduction of the error up to 86.4% (17.2% on average) in our experiments."],"url":"http://arxiv.org/abs/2404.19487v1","category":"cs.LG"}
{"created":"2024-04-30 11:46:14","title":"Hybrid Bit and Semantic Communications","abstract":"Semantic communication technology is regarded as a method surpassing the Shannon limit of bit transmission, capable of effectively enhancing transmission efficiency. However, current approaches that directly map content to transmission symbols are challenging to deploy in practice, imposing significant limitations on the development of semantic communication. To address this challenge, we propose a hybrid bit and semantic communication system, named HybridBSC, in which encoded semantic information is inserted into bit information for transmission via conventional digital communication systems utilizing same spectrum resources. The system can be easily deployed using existing communication architecture to achieve bit and semantic information transmission. Particularly, we design a semantic insertion and extraction scheme to implement this strategy. Furthermore, we conduct experimental validation based on the pluto-based software defined radio (SDR) platform in a real wireless channel, demonstrating that the proposed strategy can simultaneously transmit semantic and bit information.","sentences":["Semantic communication technology is regarded as a method surpassing the Shannon limit of bit transmission, capable of effectively enhancing transmission efficiency.","However, current approaches that directly map content to transmission symbols are challenging to deploy in practice, imposing significant limitations on the development of semantic communication.","To address this challenge, we propose a hybrid bit and semantic communication system, named HybridBSC, in which encoded semantic information is inserted into bit information for transmission via conventional digital communication systems utilizing same spectrum resources.","The system can be easily deployed using existing communication architecture to achieve bit and semantic information transmission.","Particularly, we design a semantic insertion and extraction scheme to implement this strategy.","Furthermore, we conduct experimental validation based on the pluto-based software defined radio (SDR) platform in a real wireless channel, demonstrating that the proposed strategy can simultaneously transmit semantic and bit information."],"url":"http://arxiv.org/abs/2404.19477v1","category":"eess.SP"}
{"created":"2024-04-30 11:23:31","title":"Continual Model-based Reinforcement Learning for Data Efficient Wireless Network Optimisation","abstract":"We present a method that addresses the pain point of long lead-time required to deploy cell-level parameter optimisation policies to new wireless network sites. Given a sequence of action spaces represented by overlapping subsets of cell-level configuration parameters provided by domain experts, we formulate throughput optimisation as Continual Reinforcement Learning of control policies. Simulation results suggest that the proposed system is able to shorten the end-to-end deployment lead-time by two-fold compared to a reinitialise-and-retrain baseline without any drop in optimisation gain.","sentences":["We present a method that addresses the pain point of long lead-time required to deploy cell-level parameter optimisation policies to new wireless network sites.","Given a sequence of action spaces represented by overlapping subsets of cell-level configuration parameters provided by domain experts, we formulate throughput optimisation as Continual Reinforcement Learning of control policies.","Simulation results suggest that the proposed system is able to shorten the end-to-end deployment lead-time by two-fold compared to a reinitialise-and-retrain baseline without any drop in optimisation gain."],"url":"http://arxiv.org/abs/2404.19462v1","category":"cs.LG"}
{"created":"2024-04-30 11:14:46","title":"Computing Borel complexity of some geometrical properties in Banach spaces","abstract":"We compute the Borel complexity of some classes of Banach spaces such as different versions of diameter two properties, spaces satisfying the Daugavet equation or spaces with an octahedral norm. In most of the above cases our computation is even optimal, which completes the research done during the last years around this topic for isomorphism classes of Banach spaces.","sentences":["We compute the Borel complexity of some classes of Banach spaces such as different versions of diameter two properties, spaces satisfying the Daugavet equation or spaces with an octahedral norm.","In most of the above cases our computation is even optimal, which completes the research done during the last years around this topic for isomorphism classes of Banach spaces."],"url":"http://arxiv.org/abs/2404.19457v1","category":"math.FA"}
{"created":"2024-04-30 11:10:25","title":"Structural Parameters for Dense Temporal Graphs","abstract":"Temporal graphs provide a useful model for many real-world networks. Unfortunately the majority of algorithmic problems we might consider on such graphs are intractable. There has been recent progress in defining structural parameters which describe tractable cases by simultaneously restricting the underlying structure and the times at which edges appear in the graph. These all rely on the temporal graph being sparse in some sense. We introduce temporal analogues of three increasingly restrictive static graph parameters -- cliquewidth, modular-width and neighbourhood diversity -- which take small values for highly structured temporal graphs, even if a large number of edges are active at each timestep. The computational problems solvable efficiently when the temporal cliquewidth of the input graph is bounded form a subset of those solvable efficiently when the temporal modular-width is bounded, which is in turn a subset of problems efficiently solvable when the temporal neighbourhood diversity is bounded. By considering specific temporal graph problems, we demonstrate that (up to standard complexity theoretic assumptions) these inclusions are strict.","sentences":["Temporal graphs provide a useful model for many real-world networks.","Unfortunately the majority of algorithmic problems we might consider on such graphs are intractable.","There has been recent progress in defining structural parameters which describe tractable cases by simultaneously restricting the underlying structure and the times at which edges appear in the graph.","These all rely on the temporal graph being sparse in some sense.","We introduce temporal analogues of three increasingly restrictive static graph parameters -- cliquewidth, modular-width and neighbourhood diversity -- which take small values for highly structured temporal graphs, even if a large number of edges are active at each timestep.","The computational problems solvable efficiently when the temporal cliquewidth of the input graph is bounded form a subset of those solvable efficiently when the temporal modular-width is bounded, which is in turn a subset of problems efficiently solvable when the temporal neighbourhood diversity is bounded.","By considering specific temporal graph problems, we demonstrate that (up to standard complexity theoretic assumptions)","these inclusions are strict."],"url":"http://arxiv.org/abs/2404.19453v1","category":"cs.DM"}
{"created":"2024-04-30 11:09:47","title":"How to Sustainably Monitor ML-Enabled Systems? Accuracy and Energy Efficiency Tradeoffs in Concept Drift Detection","abstract":"ML-enabled systems that are deployed in a production environment typically suffer from decaying model prediction quality through concept drift, i.e., a gradual change in the statistical characteristics of a certain real-world domain. To combat this, a simple solution is to periodically retrain ML models, which unfortunately can consume a lot of energy. One recommended tactic to improve energy efficiency is therefore to systematically monitor the level of concept drift and only retrain when it becomes unavoidable. Different methods are available to do this, but we know very little about their concrete impact on the tradeoff between accuracy and energy efficiency, as these methods also consume energy themselves.   To address this, we therefore conducted a controlled experiment to study the accuracy vs. energy efficiency tradeoff of seven common methods for concept drift detection. We used five synthetic datasets, each in a version with abrupt and one with gradual drift, and trained six different ML models as base classifiers. Based on a full factorial design, we tested 420 combinations (7 drift detectors * 5 datasets * 2 types of drift * 6 base classifiers) and compared energy consumption and drift detection accuracy.   Our results indicate that there are three types of detectors: a) detectors that sacrifice energy efficiency for detection accuracy (KSWIN), b) balanced detectors that consume low to medium energy with good accuracy (HDDM_W, ADWIN), and c) detectors that consume very little energy but are unusable in practice due to very poor accuracy (HDDM_A, PageHinkley, DDM, EDDM). By providing rich evidence for this energy efficiency tactic, our findings support ML practitioners in choosing the best suited method of concept drift detection for their ML-enabled systems.","sentences":["ML-enabled systems that are deployed in a production environment typically suffer from decaying model prediction quality through concept drift, i.e., a gradual change in the statistical characteristics of a certain real-world domain.","To combat this, a simple solution is to periodically retrain ML models, which unfortunately can consume a lot of energy.","One recommended tactic to improve energy efficiency is therefore to systematically monitor the level of concept drift and only retrain when it becomes unavoidable.","Different methods are available to do this, but we know very little about their concrete impact on the tradeoff between accuracy and energy efficiency, as these methods also consume energy themselves.   ","To address this, we therefore conducted a controlled experiment to study the accuracy vs. energy efficiency tradeoff of seven common methods for concept drift detection.","We used five synthetic datasets, each in a version with abrupt and one with gradual drift, and trained six different ML models as base classifiers.","Based on a full factorial design, we tested 420 combinations (7 drift detectors * 5 datasets * 2 types of drift * 6 base classifiers) and compared energy consumption and drift detection accuracy.   ","Our results indicate that there are three types of detectors: a) detectors that sacrifice energy efficiency for detection accuracy (KSWIN), b) balanced detectors that consume low to medium energy with good accuracy (HDDM_W, ADWIN), and c) detectors that consume very little energy but are unusable in practice due to very poor accuracy (HDDM_A, PageHinkley, DDM, EDDM).","By providing rich evidence for this energy efficiency tactic, our findings support ML practitioners in choosing the best suited method of concept drift detection for their ML-enabled systems."],"url":"http://arxiv.org/abs/2404.19452v1","category":"cs.LG"}
{"created":"2024-04-30 10:46:26","title":"Rediscussion of eclipsing binaries. Paper XIX. The long-period solar-type system V454 Aurigae","abstract":"V454 Aur is an eclipsing binary system containing two solar-type stars on an orbit of relatively long period (P = 27.02 d) and large eccentricity (e = 0.381). Eclipses were detected using data from the Hipparcos satellite, and a high-quality double-lined spectroscopic orbit has been presented by Griffin (2001). The NASA Transiting Exoplanet Survey Satellite (TESS) has observed the system during eight sectors, capturing ten eclipses in their entirety. V454 Aur is unusual in that the primary star - the star eclipsed at the deeper minimum - is less massive, smaller \\emph{and} cooler than its companion. This phenomenon can occur in certain configurations of eccentric orbits when the stars are closer together at the primary eclipse, causing a larger area to be eclipsed than at the secondary. We use the radial velocity measurements from Griffin and the light curves from TESS to determine the masses and radii of the component stars for the first time, finding masses of 1.034 +/- 0.006 Msun and 1.161 +/- 0.008 Msun, and radii of 0.979 +/- 0.003 Rsun and 1.211 +/- 0.003 Rsun. Our measurement of the distance to the system is consistent with that from the Gaia DR3 parallax. A detailed spectroscopic study to determine chemical abundances and more precise temperatures is encouraged. Finally, we present equations to derive the effective temperatures of the stars from the inferred temperature of the system as a whole, plus the ratio of the radii and either the surface brightness or light ratio of the stars.","sentences":["V454 Aur is an eclipsing binary system containing two solar-type stars on an orbit of relatively long period (P = 27.02 d) and large eccentricity (e = 0.381).","Eclipses were detected using data from the Hipparcos satellite, and a high-quality double-lined spectroscopic orbit has been presented by Griffin (2001).","The NASA Transiting Exoplanet Survey Satellite (TESS) has observed the system during eight sectors, capturing ten eclipses in their entirety.","V454 Aur is unusual in that the primary star - the star eclipsed at the deeper minimum - is less massive, smaller \\emph{and} cooler than its companion.","This phenomenon can occur in certain configurations of eccentric orbits when the stars are closer together at the primary eclipse, causing a larger area to be eclipsed than at the secondary.","We use the radial velocity measurements from Griffin and the light curves from TESS to determine the masses and radii of the component stars for the first time, finding masses of 1.034 +/- 0.006 Msun and 1.161 +/- 0.008 Msun, and radii of 0.979 +/- 0.003 Rsun and 1.211 +/- 0.003 Rsun.","Our measurement of the distance to the system is consistent with that from the Gaia DR3 parallax.","A detailed spectroscopic study to determine chemical abundances and more precise temperatures is encouraged.","Finally, we present equations to derive the effective temperatures of the stars from the inferred temperature of the system as a whole, plus the ratio of the radii and either the surface brightness or light ratio of the stars."],"url":"http://arxiv.org/abs/2404.19443v1","category":"astro-ph.SR"}
{"created":"2024-04-30 10:44:33","title":"ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers","abstract":"Existing neural audio codecs usually sacrifice computational complexity for audio quality. They build the feature transformation layers mainly on convolutional blocks, which are not inherently appropriate for capturing local redundancies of audio signals. As compensation, either adversarial losses from a discriminator or a large number of model parameters are required to improve the codec. To that end, we propose Efficient Speech Codec (ESC), a lightweight parameter-efficient codec laid on cross-scale residual vector quantization and transformers. Our model leverages mirrored hierarchical window-attention transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance codebook utilization, we design a learning paradigm that involves a pre-training stage to assist with codec training. Extensive results show that ESC can achieve high audio quality with much lower complexity, which is a prospective alternative in place of existing codecs.","sentences":["Existing neural audio codecs usually sacrifice computational complexity for audio quality.","They build the feature transformation layers mainly on convolutional blocks, which are not inherently appropriate for capturing local redundancies of audio signals.","As compensation, either adversarial losses from a discriminator or a large number of model parameters are required to improve the codec.","To that end, we propose Efficient Speech Codec (ESC), a lightweight parameter-efficient codec laid on cross-scale residual vector quantization and transformers.","Our model leverages mirrored hierarchical window-attention transformer blocks and performs step-wise decoding from coarse-to-fine feature representations.","To enhance codebook utilization, we design a learning paradigm that involves a pre-training stage to assist with codec training.","Extensive results show that ESC can achieve high audio quality with much lower complexity, which is a prospective alternative in place of existing codecs."],"url":"http://arxiv.org/abs/2404.19441v1","category":"cs.SD"}
{"created":"2024-04-30 10:41:23","title":"Neuro-Vision to Language: Image Reconstruction and Language enabled Interaction via Brain Recordings","abstract":"Decoding non-invasive brain recordings is crucial for advancing our understanding of human cognition, yet faces challenges from individual differences and complex neural signal representations. Traditional methods require custom models and extensive trials, and lack interpretability in visual reconstruction tasks. Our framework integrating integrates 3D brain structures with visual semantics by Vision Transformer 3D. The unified feature extractor aligns fMRI features with multiple levels of visual embeddings efficiently, removing the need for individual-specific models and allowing extraction from single-trial data. This extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with various fMRI-image related textual data to support multimodal large model development. The integration with LLMs enhances decoding capabilities, enabling tasks like brain captioning, question-answering, detailed descriptions, complex reasoning, and visual reconstruction. Our approach not only shows superior performance across these tasks but also precisely identifies and manipulates language-based concepts within brain signals, enhancing interpretability and providing deeper neural process insights. These advances significantly broaden non-invasive brain decoding applicability in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.","sentences":["Decoding non-invasive brain recordings is crucial for advancing our understanding of human cognition, yet faces challenges from individual differences and complex neural signal representations.","Traditional methods require custom models and extensive trials, and lack interpretability in visual reconstruction tasks.","Our framework integrating integrates 3D brain structures with visual semantics by Vision Transformer","3D.","The unified feature extractor aligns fMRI features with multiple levels of visual embeddings efficiently, removing the need for individual-specific models and allowing extraction from single-trial data.","This extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs).","Additionally, we have enhanced the fMRI dataset with various fMRI-image related textual data to support multimodal large model development.","The integration with LLMs enhances decoding capabilities, enabling tasks like brain captioning, question-answering, detailed descriptions, complex reasoning, and visual reconstruction.","Our approach not only shows superior performance across these tasks but also precisely identifies and manipulates language-based concepts within brain signals, enhancing interpretability and providing deeper neural process insights.","These advances significantly broaden non-invasive brain decoding applicability in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models."],"url":"http://arxiv.org/abs/2404.19438v2","category":"cs.NE"}
{"created":"2024-04-30 10:29:54","title":"Fine-tuning the Microstructure and Photophysical Characteristics of Fluorescent Conjugated Copolymers Using Photoalignment and Liquid-crystalline Ordering","abstract":"Replicating the microstructure and near-unity excitation energy transfer efficiency in natural light-harvesting complexes (LHCs) remains a major challenge for synthetic energy-harvesting devices. Biological photosynthesis can spontaneously regulate the active ensembles of involved energy absorbing and funnelling chlorophyll-containing proteins in response to fluctuating sunlight. Here we utilize liquid crystalline (LC) ordering to fine-tune the polymer packing and photophysical properties in liquid crystalline conjugated polymer (LCCP) films for LHC biomimicry and optimizing photoluminescence quantum efficiency (PLQE). We show that the long-range orientational ordering present in a LC phase of poly(9,9-dioctylfluorene-co-benzothiadiazole) (F8BT) stabilizes a small fraction of randomly-oriented F8BT nanocrystals dispersed in an amorphous matrix of disordered F8BT chains, hence resembling a self-doped host-guest system whereby excitation energy funnelling and PLQE are reinforced significantly by three-dimensional donor-to-acceptor Forster resonance energy transfer (FRET) and dominant intrachain emission in the nano-crystalline acceptor. Furthermore, the photoalignment of nematic F8BT layers is combined to fabricate long-sought large-area-extended monodomains which exhibit >60% crystallinity and ~20 nm-long interchain packing order, whilst also promoting linearly polarized emission, a new band-edge absorption species, and an extra emissive interchain excited state. Our micro-PL spectral results support the feasibility of making use of self-doped F8BT nematic films for bio-mimicry of certain structural basis and light-harvesting properties of naturally occurring LHCs.","sentences":["Replicating the microstructure and near-unity excitation energy transfer efficiency in natural light-harvesting complexes (LHCs) remains a major challenge for synthetic energy-harvesting devices.","Biological photosynthesis can spontaneously regulate the active ensembles of involved energy absorbing and funnelling chlorophyll-containing proteins in response to fluctuating sunlight.","Here we utilize liquid crystalline (LC) ordering to fine-tune the polymer packing and photophysical properties in liquid crystalline conjugated polymer (LCCP) films for LHC biomimicry and optimizing photoluminescence quantum efficiency (PLQE).","We show that the long-range orientational ordering present in a LC phase of poly(9,9-dioctylfluorene-co-benzothiadiazole) (F8BT) stabilizes a small fraction of randomly-oriented F8BT nanocrystals dispersed in an amorphous matrix of disordered F8BT chains, hence resembling a self-doped host-guest system whereby excitation energy funnelling and PLQE are reinforced significantly by three-dimensional donor-to-acceptor Forster resonance energy transfer (FRET) and dominant intrachain emission in the nano-crystalline acceptor.","Furthermore, the photoalignment of nematic F8BT layers is combined to fabricate long-sought large-area-extended monodomains which exhibit >60% crystallinity and ~20 nm-long interchain packing order, whilst also promoting linearly polarized emission, a new band-edge absorption species, and an extra emissive interchain excited state.","Our micro-PL spectral results support the feasibility of making use of self-doped F8BT nematic films for bio-mimicry of certain structural basis and light-harvesting properties of naturally occurring LHCs."],"url":"http://arxiv.org/abs/2404.19435v1","category":"cond-mat.soft"}
{"created":"2024-04-30 10:29:15","title":"The Arens-Michael envelope of a solvable Lie algebra is a homological epimorphism","abstract":"The Arens-Michael envelope of the universal enveloping algebra of a finite-dimensional complex Lie algebra is a homological epimorphism if and only if the Lie algebra is solvable. The necessity was proved by Pirkovskii in [Proc. Amer. Math. Soc. 134, 2621--2631, 2006]. We prove the sufficiency.","sentences":["The Arens-Michael envelope of the universal enveloping algebra of a finite-dimensional complex Lie algebra is a homological epimorphism if and only if the Lie algebra is solvable.","The necessity was proved by Pirkovskii in [Proc.","Amer.","Math.","Soc. 134, 2621--2631, 2006].","We prove the sufficiency."],"url":"http://arxiv.org/abs/2404.19433v1","category":"math.FA"}
{"created":"2024-04-30 10:26:04","title":"Integrated Sensing and Communications for Unsourced Random Access: Fundamental Limits","abstract":"This work considers the problem of integrated sensing and communication (ISAC) with a massive number of unsourced and uncoordinated users. In the proposed model, known as the unsourced ISAC system (UNISAC), all active communication and sensing users share a short frame to transmit their signals, without requiring scheduling with the base station (BS). Hence, the signal received from each user is affected by significant interference from numerous interfering users, making it challenging to extract the transmitted signals. UNISAC aims to decode the transmitted message sequences from communication users while simultaneously detect active sensing users, regardless of the identity of the decoded and detected users. In this paper, we derive an achievable performance limit for UNISAC and demonstrate its superiority over conventional approaches such as ALOHA, time-division multiple access, treating interference as noise, and multiple signal classification. Through numerical simulations, we validate the UNISAC's effectiveness in detecting and decoding a large number of users.","sentences":["This work considers the problem of integrated sensing and communication (ISAC) with a massive number of unsourced and uncoordinated users.","In the proposed model, known as the unsourced ISAC system (UNISAC), all active communication and sensing users share a short frame to transmit their signals, without requiring scheduling with the base station (BS).","Hence, the signal received from each user is affected by significant interference from numerous interfering users, making it challenging to extract the transmitted signals.","UNISAC aims to decode the transmitted message sequences from communication users while simultaneously detect active sensing users, regardless of the identity of the decoded and detected users.","In this paper, we derive an achievable performance limit for UNISAC and demonstrate its superiority over conventional approaches such as ALOHA, time-division multiple access, treating interference as noise, and multiple signal classification.","Through numerical simulations, we validate the UNISAC's effectiveness in detecting and decoding a large number of users."],"url":"http://arxiv.org/abs/2404.19431v2","category":"cs.IT"}
{"created":"2024-04-30 10:21:14","title":"S\u00f5najaht: Definition Embeddings and Semantic Search for Reverse Dictionary Creation","abstract":"We present an information retrieval based reverse dictionary system using modern pre-trained language models and approximate nearest neighbors search algorithms. The proposed approach is applied to an existing Estonian language lexicon resource, S\\~onaveeb (word web), with the purpose of enhancing and enriching it by introducing cross-lingual reverse dictionary functionality powered by semantic search.   The performance of the system is evaluated using both an existing labeled English dataset of words and definitions that is extended to contain also Estonian and Russian translations, and a novel unlabeled evaluation approach that extracts the evaluation data from the lexicon resource itself using synonymy relations.   Evaluation results indicate that the information retrieval based semantic search approach without any model training is feasible, producing median rank of 1 in the monolingual setting and median rank of 2 in the cross-lingual setting using the unlabeled evaluation approach, with models trained for cross-lingual retrieval and including Estonian in their training data showing superior performance in our particular task.","sentences":["We present an information retrieval based reverse dictionary system using modern pre-trained language models and approximate nearest neighbors search algorithms.","The proposed approach is applied to an existing Estonian language lexicon resource, S\\~onaveeb (word web), with the purpose of enhancing and enriching it by introducing cross-lingual reverse dictionary functionality powered by semantic search.   ","The performance of the system is evaluated using both an existing labeled English dataset of words and definitions that is extended to contain also Estonian and Russian translations, and a novel unlabeled evaluation approach that extracts the evaluation data from the lexicon resource itself using synonymy relations.   ","Evaluation results indicate that the information retrieval based semantic search approach without any model training is feasible, producing median rank of 1 in the monolingual setting and median rank of 2 in the cross-lingual setting using the unlabeled evaluation approach, with models trained for cross-lingual retrieval and including Estonian in their training data showing superior performance in our particular task."],"url":"http://arxiv.org/abs/2404.19430v1","category":"cs.CL"}
{"created":"2024-04-30 10:17:21","title":"Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping","abstract":"The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters. However, it faces the challenge of extended all-to-all communication latency during the training process. Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation. Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements. In our study, we extend the scope of this challenge by considering overlap at the broader training graph level. During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining. In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations. We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training. Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%. Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions.","sentences":["The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters.","However, it faces the challenge of extended all-to-all communication latency during the training process.","Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation.","Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements.","In our study, we extend the scope of this challenge by considering overlap at the broader training graph level.","During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining.","In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations.","We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training.","Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%.","Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.19429v1","category":"cs.DC"}
{"created":"2024-04-30 10:12:21","title":"On wave systems with antisymmetric potential in dimension d >= 4 and well-posedness for (half-)wave maps","abstract":"We prove a priori estimates for wave systems of the type \\[ \\partial_{tt} u - \\Delta u = \\Omega \\cdot du + F(u) \\quad \\text{in $\\mathbb{R}^d \\times \\mathbb{R}$} \\]   where $d \\geq 4$ and $\\Omega$ is a suitable antisymmetric potential. We show that the assumptions on $\\Omega$ are applicable to wave- and half-wave maps, the latter by means of the Krieger-Sire reduction. We thus obtain well-posedness of those equations for small initial data in $\\dot{H}^{\\frac{d}{2}}(\\mathbb{R}^d)$.","sentences":["We prove a priori estimates for wave systems of the type \\[ \\partial_{tt} u - \\Delta u = \\Omega \\cdot du + F(u) \\quad \\text{in $\\mathbb{R}^d \\times \\mathbb{R}$} \\]   where $d \\geq 4$ and $\\Omega$ is a suitable antisymmetric potential.","We show that the assumptions on $\\Omega$ are applicable to wave- and half-wave maps, the latter by means of the Krieger-Sire reduction.","We thus obtain well-posedness of those equations for small initial data in $\\dot{H}^{\\frac{d}{2}}(\\mathbb{R}^d)$."],"url":"http://arxiv.org/abs/2404.19421v1","category":"math.AP"}
{"created":"2024-04-30 10:01:05","title":"Gauge invariant discretization of Chern-Simons couplings","abstract":"We discretize Chern-Simons couplings in gauge invariant way. We obtain (p+q)-forms representing Chern-Simons couplings on (p + q)-simplexes from wedge products of p- and q-forms on p- and q-simplexes, respectively, where p- and q-simplexes form (p+q)-simplexes by having a common vertex. We show that the Chern-Simons couplings on simplicial complexes reduce to Chern-Simons couplings on the manifolds in a continuum limit. Moreover, we prove that a typical discretized Chern-Simons term that has the Chern-Simons coupling is gauge invariant.","sentences":["We discretize Chern-Simons couplings in gauge invariant way.","We obtain (p+q)-forms representing Chern-Simons couplings on (p + q)-simplexes from wedge products of p- and q-forms on p- and q-simplexes, respectively, where p- and q-simplexes form (p+q)-simplexes by having a common vertex.","We show that the Chern-Simons couplings on simplicial complexes reduce to Chern-Simons couplings on the manifolds in a continuum limit.","Moreover, we prove that a typical discretized Chern-Simons term that has the Chern-Simons coupling is gauge invariant."],"url":"http://arxiv.org/abs/2404.19413v1","category":"hep-lat"}
{"created":"2024-04-30 10:00:39","title":"Enhancing Robotic Adaptability: Integrating Unsupervised Trajectory Segmentation and Conditional ProMPs for Dynamic Learning Environments","abstract":"We propose a novel framework for enhancing robotic adaptability and learning efficiency, which integrates unsupervised trajectory segmentation with adaptive probabilistic movement primitives (ProMPs). By employing a cutting-edge deep learning architecture that combines autoencoders and Recurrent Neural Networks (RNNs), our approach autonomously pinpoints critical transitional points in continuous, unlabeled motion data, thus significantly reducing dependence on extensively labeled datasets. This innovative method dynamically adjusts motion trajectories using conditional variables, significantly enhancing the flexibility and accuracy of robotic actions under dynamic conditions while also reducing the computational overhead associated with traditional robotic programming methods. Our experimental validation demonstrates superior learning efficiency and adaptability compared to existing techniques, paving the way for advanced applications in industrial and service robotics.","sentences":["We propose a novel framework for enhancing robotic adaptability and learning efficiency, which integrates unsupervised trajectory segmentation with adaptive probabilistic movement primitives (ProMPs).","By employing a cutting-edge deep learning architecture that combines autoencoders and Recurrent Neural Networks (RNNs), our approach autonomously pinpoints critical transitional points in continuous, unlabeled motion data, thus significantly reducing dependence on extensively labeled datasets.","This innovative method dynamically adjusts motion trajectories using conditional variables, significantly enhancing the flexibility and accuracy of robotic actions under dynamic conditions while also reducing the computational overhead associated with traditional robotic programming methods.","Our experimental validation demonstrates superior learning efficiency and adaptability compared to existing techniques, paving the way for advanced applications in industrial and service robotics."],"url":"http://arxiv.org/abs/2404.19412v1","category":"cs.RO"}
{"created":"2024-04-30 09:59:34","title":"Composite antiferromagnetic and orbital order with altermagnetic properties at a cuprate/manganite interface","abstract":"Heterostructures from complex oxides allow one to combine various electronic and magnetic orders as to induce new quantum states. A prominent example is the coupling between superconducting and magnetic orders in multilayers from high-Tc cuprates and manganites. A key role is played here by the interfacial CuO2 layer whose distinct properties remain to be fully understood. Here, we study with resonant inelastic X-ray scattering (RIXS) the magnon excitations of this interfacial CuO2 layer. In particular, we show that the underlying antiferromagnetic exchange interaction at the interface is strongly suppressed to J ~ 70 meV, as compared to J ~ 130 meV for the CuO2 layers away from the interface. Moreover, we observe an anomalous momentum dependence of the intensity of the interfacial magnon mode and show that it suggests that the antiferromagnetic order is accompanied by a particular kind of orbital order that yields a so-called altermagnetic state. Such a two-dimensional altermagnet has recently been predicted to enable new spintronic applications and superconducting proximity effects.","sentences":["Heterostructures from complex oxides allow one to combine various electronic and magnetic orders as to induce new quantum states.","A prominent example is the coupling between superconducting and magnetic orders in multilayers from high-Tc cuprates and manganites.","A key role is played here by the interfacial CuO2 layer whose distinct properties remain to be fully understood.","Here, we study with resonant inelastic X-ray scattering (RIXS) the magnon excitations of this interfacial CuO2 layer.","In particular, we show that the underlying antiferromagnetic exchange interaction at the interface is strongly suppressed to J ~ 70 meV, as compared to J ~ 130 meV for the CuO2 layers away from the interface.","Moreover, we observe an anomalous momentum dependence of the intensity of the interfacial magnon mode and show that it suggests that the antiferromagnetic order is accompanied by a particular kind of orbital order that yields a so-called altermagnetic state.","Such a two-dimensional altermagnet has recently been predicted to enable new spintronic applications and superconducting proximity effects."],"url":"http://arxiv.org/abs/2404.19410v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 09:56:03","title":"Comparison of two numerical methods for Riemannian cubic polynomials on Stiefel manifolds","abstract":"In this paper we compare two numerical methods to integrate Riemannian cubic polynomials on the Stiefel manifold $\\textbf{St}_{n,k}$. The first one is the adjusted de Casteljau algorithm, and the second one is a symplectic integrator constructed through discretization maps. In particular, we choose the cases of $n=3$ together with $k=1$ and $k=2$. The first case is diffeomorphic to the sphere and the quasi-geodesics appearing in the adjusted de Casteljau algorithm are actually geodesics. The second case is an example where we have a pure quasi-geodesic different from a geodesic. We provide a numerical comparison of both methods and discuss the obtained results to highlight the benefits of each method.","sentences":["In this paper we compare two numerical methods to integrate Riemannian cubic polynomials on the Stiefel manifold $\\textbf{St}_{n,k}$.","The first one is the adjusted de Casteljau algorithm, and the second one is a symplectic integrator constructed through discretization maps.","In particular, we choose the cases of $n=3$ together with $k=1$ and $k=2$. The first case is diffeomorphic to the sphere and the quasi-geodesics appearing in the adjusted de Casteljau algorithm are actually geodesics.","The second case is an example where we have a pure quasi-geodesic different from a geodesic.","We provide a numerical comparison of both methods and discuss the obtained results to highlight the benefits of each method."],"url":"http://arxiv.org/abs/2404.19407v1","category":"math.OC"}
{"created":"2024-04-30 09:51:02","title":"Exploring the role of mean-field potentials and short-range wave function behavior in the adiabatic connection","abstract":"In this article, we explore the construction of Hamiltonians with long-range interactions and their corrections using the short-range behavior of the wave function. A key aspect of our investigation is the examination of the one-particle potential, kept constant in our previous work, and the effects of its optimization on the adiabatic connection.   Our methodology involves the use of a parameter-dependent potential dependent on a single parameter to facilitate practical computations. We analyze the energy errors and densities in a two-electron system (harmonium) under various conditions, employing different confinement potentials and interaction parameters. The study reveals that while the mean-field potential improves the expectation value of the physical Hamiltonian, it does not necessarily improve the energy of the system within the bounds of chemical accuracy.   We also delve into the impact of density variations in adiabatic connections, challenging the common assumption that a mean field improves results. Our findings indicate that as long as energy errors remain within chemical accuracy, the mean field does not significantly outperform a bare potential. This observation is attributed to the effectiveness of corrections based on the short-range behavior of the wave function, a universal characteristic that diminishes the distinction between using a mean field or not.","sentences":["In this article, we explore the construction of Hamiltonians with long-range interactions and their corrections using the short-range behavior of the wave function.","A key aspect of our investigation is the examination of the one-particle potential, kept constant in our previous work, and the effects of its optimization on the adiabatic connection.   ","Our methodology involves the use of a parameter-dependent potential dependent on a single parameter to facilitate practical computations.","We analyze the energy errors and densities in a two-electron system (harmonium) under various conditions, employing different confinement potentials and interaction parameters.","The study reveals that while the mean-field potential improves the expectation value of the physical Hamiltonian, it does not necessarily improve the energy of the system within the bounds of chemical accuracy.   ","We also delve into the impact of density variations in adiabatic connections, challenging the common assumption that a mean field improves results.","Our findings indicate that as long as energy errors remain within chemical accuracy, the mean field does not significantly outperform a bare potential.","This observation is attributed to the effectiveness of corrections based on the short-range behavior of the wave function, a universal characteristic that diminishes the distinction between using a mean field or not."],"url":"http://arxiv.org/abs/2404.19405v1","category":"physics.chem-ph"}
{"created":"2024-04-30 09:48:00","title":"Complexity of Round-Robin Allocation with Potentially Noisy Queries","abstract":"We study the complexity of a fundamental algorithm for fairly allocating indivisible items, the round-robin algorithm. For $n$ agents and $m$ items, we show that the algorithm can be implemented in time $O(nm\\log(m/n))$ in the worst case. If the agents' preferences are uniformly random, we establish an improved (expected) running time of $O(nm + m\\log m)$. On the other hand, assuming comparison queries between items, we prove that $\\Omega(nm + m\\log m)$ queries are necessary to implement the algorithm, even when randomization is allowed. We also derive bounds in noise models where the answers to queries are incorrect with some probability. Our proofs involve novel applications of tools from multi-armed bandit, information theory, as well as posets and linear extensions.","sentences":["We study the complexity of a fundamental algorithm for fairly allocating indivisible items, the round-robin algorithm.","For $n$ agents and $m$ items, we show that the algorithm can be implemented in time $O(nm\\log(m/n))$ in the worst case.","If the agents' preferences are uniformly random, we establish an improved (expected) running time of $O(nm + m\\log m)$.","On the other hand, assuming comparison queries between items, we prove that $\\Omega(nm + m\\log m)$ queries are necessary to implement the algorithm, even when randomization is allowed.","We also derive bounds in noise models where the answers to queries are incorrect with some probability.","Our proofs involve novel applications of tools from multi-armed bandit, information theory, as well as posets and linear extensions."],"url":"http://arxiv.org/abs/2404.19402v1","category":"cs.GT"}
{"created":"2024-04-30 09:36:14","title":"ZSMILES: an approach for efficient SMILES storage for random access in Virtual Screening","abstract":"Virtual screening is a technique used in drug discovery to select the most promising molecules to test in a lab. To perform virtual screening, we need a large set of molecules as input, and storing these molecules can become an issue. In fact, extreme-scale high-throughput virtual screening applications require a big dataset of input molecules and produce an even bigger dataset as output. These molecules' databases occupy tens of TB of storage space, and domain experts frequently sample a small portion of this data. In this context, SMILES is a popular data format for storing large sets of molecules since it requires significantly less space to represent molecules than other formats (e.g., MOL2, SDF). This paper proposes an efficient dictionary-based approach to compress SMILES-based datasets. This approach takes advantage of domain knowledge to provide a readable output with separable SMILES, enabling random access. We examine the benefits of storing these datasets using ZSMILES to reduce the cold storage footprint in HPC systems. The main contributions concern a custom dictionary-based approach and a data pre-processing step. From experimental results, we can notice how ZSMILES leverage domain knowledge to compress x1.13 more than state of the art in similar scenarios and up to $0.29$ compression ratio. We tested a CUDA version of ZSMILES targetting NVIDIA's GPUs, showing a potential speedup of 7x.","sentences":["Virtual screening is a technique used in drug discovery to select the most promising molecules to test in a lab.","To perform virtual screening, we need a large set of molecules as input, and storing these molecules can become an issue.","In fact, extreme-scale high-throughput virtual screening applications require a big dataset of input molecules and produce an even bigger dataset as output.","These molecules' databases occupy tens of TB of storage space, and domain experts frequently sample a small portion of this data.","In this context, SMILES is a popular data format for storing large sets of molecules since it requires significantly less space to represent molecules than other formats (e.g., MOL2, SDF).","This paper proposes an efficient dictionary-based approach to compress SMILES-based datasets.","This approach takes advantage of domain knowledge to provide a readable output with separable SMILES, enabling random access.","We examine the benefits of storing these datasets using ZSMILES to reduce the cold storage footprint in HPC systems.","The main contributions concern a custom dictionary-based approach and a data pre-processing step.","From experimental results, we can notice how ZSMILES leverage domain knowledge to compress x1.13 more than state of the art in similar scenarios and up to $0.29$ compression ratio.","We tested a CUDA version of ZSMILES targetting NVIDIA's GPUs, showing a potential speedup of 7x."],"url":"http://arxiv.org/abs/2404.19391v1","category":"cs.CE"}
{"created":"2024-04-30 09:31:44","title":"EL meteorites do date the giant planet instability","abstract":"In our recent work, we combined dynamical simulations, meteoritic data and thermal models as well as asteroid observations to argue that the current parent body of the EL meteorites was implanted into the asteroid belt not earlier than 60 Myr after the beginning of the Solar System and that the most likely capture mechanism was the giant planet orbital instability. In the study \"The link between Athor and EL meteorites does not constrain the timing of the giant planet instability\" that appeared in arXiv, Izidoro and collaborators argue that the implantation of Athor into the asteroid belt does not necessarily require that the giant planet orbital instability occurred at the implantation time. Here we provide further arguments that, in the end, the giant planet instability is still the most likely dynamical process to implant asteroid Athor into the asteroid main belt between 60 and 100 Myr after the beginning of the Solar System.","sentences":["In our recent work, we combined dynamical simulations, meteoritic data and thermal models as well as asteroid observations to argue that the current parent body of the EL meteorites was implanted into the asteroid belt not earlier than 60 Myr after the beginning of the Solar System and that the most likely capture mechanism was the giant planet orbital instability.","In the study \"The link between Athor and EL meteorites does not constrain the timing of the giant planet instability\" that appeared in arXiv, Izidoro and collaborators argue that the implantation of Athor into the asteroid belt does not necessarily require that the giant planet orbital instability occurred at the implantation time.","Here we provide further arguments that, in the end, the giant planet instability is still the most likely dynamical process to implant asteroid Athor into the asteroid main belt between 60 and 100 Myr after the beginning of the Solar System."],"url":"http://arxiv.org/abs/2404.19390v1","category":"astro-ph.EP"}
{"created":"2024-04-30 09:20:58","title":"Weighted Feedback-Based Quantum Algorithm for Excited States Calculation","abstract":"Drawing inspiration from the Lyapunov control technique for quantum systems, feedback-based quantum algorithms have been proposed for calculating the ground states of Hamiltonians. In this work, we consider extending these algorithms to tackle calculating excited states. Inspired by the weighted subspace-search variational quantum eigensolver algorithm, we propose a novel weighted feedback-based quantum algorithm for excited state calculation. We show that depending on how we design the weights and the feedback law, we can prepare the $p$th excited state or lowest energy states up to the $p$th excited state. Through an application in quantum chemistry, we show the effectiveness of the proposed algorithm, evaluating its efficacy via numerical simulations.","sentences":["Drawing inspiration from the Lyapunov control technique for quantum systems, feedback-based quantum algorithms have been proposed for calculating the ground states of Hamiltonians.","In this work, we consider extending these algorithms to tackle calculating excited states.","Inspired by the weighted subspace-search variational quantum eigensolver algorithm, we propose a novel weighted feedback-based quantum algorithm for excited state calculation.","We show that depending on how we design the weights and the feedback law, we can prepare the $p$th excited state or lowest energy states up to the $p$th excited state.","Through an application in quantum chemistry, we show the effectiveness of the proposed algorithm, evaluating its efficacy via numerical simulations."],"url":"http://arxiv.org/abs/2404.19386v1","category":"quant-ph"}
{"created":"2024-04-30 09:05:41","title":"Entanglement Signature of the Superradiant Quantum Phase Transition","abstract":"Entanglement and quantum correlations between atoms are not usually considered key ingredients of the superradiant phase transition. Here we consider the Tavis-Cummings model, a solvable system of two-levels atoms, coupled with a single-mode quantized electromagnetic field. This system undergoes a superradiant phase transition, even in a finite-size framework, accompanied by a spontaneous symmetry breaking, and an infinite sequence of energy level crossings. We find approximated expressions for the ground state, its energy, and the position of the level crossings, valid in the limit of a very large number of photons with respect to that of the atoms. In that same limit, we find that the number of photons scales quadratically with the coupling strength, and linearly with the system size, providing a new insight into the superradiance phenomenon. Resorting to novel multipartite measures, we then demonstrate that this quantum phase transition is accompanied by a crossover in the quantum correlations and entanglement between the atoms (qubits). The latters therefore represent suited order parameters for this transition. Finally, we show that these properties of the quantum phase transition persist in the thermodynamic limit.","sentences":["Entanglement and quantum correlations between atoms are not usually considered key ingredients of the superradiant phase transition.","Here we consider the Tavis-Cummings model, a solvable system of two-levels atoms, coupled with a single-mode quantized electromagnetic field.","This system undergoes a superradiant phase transition, even in a finite-size framework, accompanied by a spontaneous symmetry breaking, and an infinite sequence of energy level crossings.","We find approximated expressions for the ground state, its energy, and the position of the level crossings, valid in the limit of a very large number of photons with respect to that of the atoms.","In that same limit, we find that the number of photons scales quadratically with the coupling strength, and linearly with the system size, providing a new insight into the superradiance phenomenon.","Resorting to novel multipartite measures, we then demonstrate that this quantum phase transition is accompanied by a crossover in the quantum correlations and entanglement between the atoms (qubits).","The latters therefore represent suited order parameters for this transition.","Finally, we show that these properties of the quantum phase transition persist in the thermodynamic limit."],"url":"http://arxiv.org/abs/2404.19373v1","category":"quant-ph"}
{"created":"2024-04-30 08:59:00","title":"Fairness in AI: challenges in bridging the gap between algorithms and law","abstract":"In this paper we examine algorithmic fairness from the perspective of law aiming to identify best practices and strategies for the specification and adoption of fairness definitions and algorithms in real-world systems and use cases. We start by providing a brief introduction of current anti-discrimination law in the European Union and the United States and discussing the concepts of bias and fairness from an legal and ethical viewpoint. We then proceed by presenting a set of algorithmic fairness definitions by example, aiming to communicate their objectives to non-technical audiences. Then, we introduce a set of core criteria that need to be taken into account when selecting a specific fairness definition for real-world use case applications. Finally, we enumerate a set of key considerations and best practices for the design and employment of fairness methods on real-world AI applications","sentences":["In this paper we examine algorithmic fairness from the perspective of law aiming to identify best practices and strategies for the specification and adoption of fairness definitions and algorithms in real-world systems and use cases.","We start by providing a brief introduction of current anti-discrimination law in the European Union and the United States and discussing the concepts of bias and fairness from an legal and ethical viewpoint.","We then proceed by presenting a set of algorithmic fairness definitions by example, aiming to communicate their objectives to non-technical audiences.","Then, we introduce a set of core criteria that need to be taken into account when selecting a specific fairness definition for real-world use case applications.","Finally, we enumerate a set of key considerations and best practices for the design and employment of fairness methods on real-world AI applications"],"url":"http://arxiv.org/abs/2404.19371v1","category":"cs.CY"}
{"created":"2024-04-30 08:51:45","title":"Parametric estimation and LAN property of the birth-death-move process with mutations","abstract":"A birth-death-move process with mutations is a Markov model for a system of marked particles in interaction, that move over time, with births and deaths. In addition the mark of each particle may also change, which constitutes a mutation. Assuming a parametric form for this model, we derive its likelihood expression and prove its local asymptotic normality. The efficiency and asymptotic distribution of the maximum likelihood estimator, with an explicit expression of its covariance matrix, is deduced. The underlying technical assumptions are showed to be satisfied by several natural parametric specifications. As an application, we leverage this model to analyse the joint dynamics of two types of proteins in a living cell, that are involved in the exocytosis process. Our approach enables to quantify the so-called colocalization phenomenon, answering an important question in microbiology.","sentences":["A birth-death-move process with mutations is a Markov model for a system of marked particles in interaction, that move over time, with births and deaths.","In addition the mark of each particle may also change, which constitutes a mutation.","Assuming a parametric form for this model, we derive its likelihood expression and prove its local asymptotic normality.","The efficiency and asymptotic distribution of the maximum likelihood estimator, with an explicit expression of its covariance matrix, is deduced.","The underlying technical assumptions are showed to be satisfied by several natural parametric specifications.","As an application, we leverage this model to analyse the joint dynamics of two types of proteins in a living cell, that are involved in the exocytosis process.","Our approach enables to quantify the so-called colocalization phenomenon, answering an important question in microbiology."],"url":"http://arxiv.org/abs/2404.19367v1","category":"math.ST"}
{"created":"2024-04-30 08:45:55","title":"Global solution for the stochastic nonlinear Schr\u00f6dinger system with quadratic interaction in four dimensions","abstract":"We discuss the global existence of solutions to a system of stochastic Schr\\\"odinger equations with multiplicative noise. Our setting of the quadratic nonlinear terms in dimension 4 is $L^2$-critical. We treat the solutions under the ground state. We estimate the time derivative of the quantity of energy by using the cancellation of the cubic terms in the spatial derivative of the solution.","sentences":["We discuss the global existence of solutions to a system of stochastic Schr\\\"odinger equations with multiplicative noise.","Our setting of the quadratic nonlinear terms in dimension 4 is $L^2$-critical.","We treat the solutions under the ground state.","We estimate the time derivative of the quantity of energy by using the cancellation of the cubic terms in the spatial derivative of the solution."],"url":"http://arxiv.org/abs/2404.19362v1","category":"math.AP"}
{"created":"2024-04-30 08:45:16","title":"Large Language Model Informed Patent Image Retrieval","abstract":"In patent prosecution, image-based retrieval systems for identifying similarities between current patent images and prior art are pivotal to ensure the novelty and non-obviousness of patent applications. Despite their growing popularity in recent years, existing attempts, while effective at recognizing images within the same patent, fail to deliver practical value due to their limited generalizability in retrieving relevant prior art. Moreover, this task inherently involves the challenges posed by the abstract visual features of patent images, the skewed distribution of image classifications, and the semantic information of image descriptions. Therefore, we propose a language-informed, distribution-aware multimodal approach to patent image feature learning, which enriches the semantic understanding of patent image by integrating Large Language Models and improves the performance of underrepresented classes with our proposed distribution-aware contrastive losses. Extensive experiments on DeepPatent2 dataset show that our proposed method achieves state-of-the-art or comparable performance in image-based patent retrieval with mAP +53.3%, Recall@10 +41.8%, and MRR@10 +51.9%. Furthermore, through an in-depth user analysis, we explore our model in aiding patent professionals in their image retrieval efforts, highlighting the model's real-world applicability and effectiveness.","sentences":["In patent prosecution, image-based retrieval systems for identifying similarities between current patent images and prior art are pivotal to ensure the novelty and non-obviousness of patent applications.","Despite their growing popularity in recent years, existing attempts, while effective at recognizing images within the same patent, fail to deliver practical value due to their limited generalizability in retrieving relevant prior art.","Moreover, this task inherently involves the challenges posed by the abstract visual features of patent images, the skewed distribution of image classifications, and the semantic information of image descriptions.","Therefore, we propose a language-informed, distribution-aware multimodal approach to patent image feature learning, which enriches the semantic understanding of patent image by integrating Large Language Models and improves the performance of underrepresented classes with our proposed distribution-aware contrastive losses.","Extensive experiments on DeepPatent2 dataset show that our proposed method achieves state-of-the-art or comparable performance in image-based patent retrieval with mAP +53.3%, Recall@10 +41.8%, and MRR@10 +51.9%.","Furthermore, through an in-depth user analysis, we explore our model in aiding patent professionals in their image retrieval efforts, highlighting the model's real-world applicability and effectiveness."],"url":"http://arxiv.org/abs/2404.19360v1","category":"cs.CV"}
{"created":"2024-04-30 08:38:09","title":"Interest Clock: Time Perception in Real-Time Streaming Recommendation System","abstract":"User preferences follow a dynamic pattern over a day, e.g., at 8 am, a user might prefer to read news, while at 8 pm, they might prefer to watch movies. Time modeling aims to enable recommendation systems to perceive time changes to capture users' dynamic preferences over time, which is an important and challenging problem in recommendation systems. Especially, streaming recommendation systems in the industry, with only available samples of the current moment, present greater challenges for time modeling. There is still a lack of effective time modeling methods for streaming recommendation systems. In this paper, we propose an effective and universal method Interest Clock to perceive time information in recommendation systems. Interest Clock first encodes users' time-aware preferences into a clock (hour-level personalized features) and then uses Gaussian distribution to smooth and aggregate them into the final interest clock embedding according to the current time for the final prediction. By arming base models with Interest Clock, we conduct online A/B tests, obtaining +0.509% and +0.758% improvements on user active days and app duration respectively. Besides, the extended offline experiments show improvements as well. Interest Clock has been deployed on Douyin Music App.","sentences":["User preferences follow a dynamic pattern over a day, e.g., at 8 am, a user might prefer to read news, while at 8 pm, they might prefer to watch movies.","Time modeling aims to enable recommendation systems to perceive time changes to capture users' dynamic preferences over time, which is an important and challenging problem in recommendation systems.","Especially, streaming recommendation systems in the industry, with only available samples of the current moment, present greater challenges for time modeling.","There is still a lack of effective time modeling methods for streaming recommendation systems.","In this paper, we propose an effective and universal method Interest Clock to perceive time information in recommendation systems.","Interest Clock first encodes users' time-aware preferences into a clock (hour-level personalized features) and then uses Gaussian distribution to smooth and aggregate them into the final interest clock embedding according to the current time for the final prediction.","By arming base models with Interest Clock, we conduct online A/B tests, obtaining +0.509% and +0.758% improvements on user active days and app duration respectively.","Besides, the extended offline experiments show improvements as well.","Interest Clock has been deployed on Douyin Music App."],"url":"http://arxiv.org/abs/2404.19357v1","category":"cs.IR"}
{"created":"2024-04-30 08:37:53","title":"A Concept for Semi-Automatic Configuration of Sufficiently Valid Simulation Setups for Automated Driving Systems","abstract":"As simulation is increasingly used in scenario-based approaches to test Automated Driving Systems, the credibility of simulation results is a major concern. Arguably, credibility depends on the validity of the simulation setup and simulation models. When selecting appropriate simulation models, a trade-off must be made between validity, often connected to the model's fidelity, and cost of computation. However, due to the large number of test cases, expert-based methods to create sufficiently valid simulation setups seem infeasible. We propose using design contracts in order to semi-automatically compose simulation setups for given test cases from simulation models and to derive requirements for the simulation models, supporting separation of concerns between simulation model developers and users. Simulation model contracts represent their validity domains by capturing a validity guarantee and the associated operating conditions in an assumption. We then require the composition of the simulation model contracts to refine a test case contract. The latter contract captures the operating conditions of the test case in its assumption and validity requirements in its guarantee. Based on this idea, we present a framework that supports the compositional configuration of simulation setups based on the contracts and a method to derive runtime monitors for these simulation setups.","sentences":["As simulation is increasingly used in scenario-based approaches to test Automated Driving Systems, the credibility of simulation results is a major concern.","Arguably, credibility depends on the validity of the simulation setup and simulation models.","When selecting appropriate simulation models, a trade-off must be made between validity, often connected to the model's fidelity, and cost of computation.","However, due to the large number of test cases, expert-based methods to create sufficiently valid simulation setups seem infeasible.","We propose using design contracts in order to semi-automatically compose simulation setups for given test cases from simulation models and to derive requirements for the simulation models, supporting separation of concerns between simulation model developers and users.","Simulation model contracts represent their validity domains by capturing a validity guarantee and the associated operating conditions in an assumption.","We then require the composition of the simulation model contracts to refine a test case contract.","The latter contract captures the operating conditions of the test case in its assumption and validity requirements in its guarantee.","Based on this idea, we present a framework that supports the compositional configuration of simulation setups based on the contracts and a method to derive runtime monitors for these simulation setups."],"url":"http://arxiv.org/abs/2404.19356v1","category":"eess.SY"}
{"created":"2024-04-30 08:33:52","title":"PEFSL: A deployment Pipeline for Embedded Few-Shot Learning on a FPGA SoC","abstract":"This paper tackles the challenges of implementing few-shot learning on embedded systems, specifically FPGA SoCs, a vital approach for adapting to diverse classification tasks, especially when the costs of data acquisition or labeling prove to be prohibitively high. Our contributions encompass the development of an end-to-end open-source pipeline for a few-shot learning platform for object classification on a FPGA SoCs. The pipeline is built on top of the Tensil open-source framework, facilitating the design, training, evaluation, and deployment of DNN backbones tailored for few-shot learning. Additionally, we showcase our work's potential by building and deploying a low-power, low-latency demonstrator trained on the MiniImageNet dataset with a dataflow architecture. The proposed system has a latency of 30 ms while consuming 6.2 W on the PYNQ-Z1 board.","sentences":["This paper tackles the challenges of implementing few-shot learning on embedded systems, specifically FPGA SoCs, a vital approach for adapting to diverse classification tasks, especially when the costs of data acquisition or labeling prove to be prohibitively high.","Our contributions encompass the development of an end-to-end open-source pipeline for a few-shot learning platform for object classification on a FPGA SoCs.","The pipeline is built on top of the Tensil open-source framework, facilitating the design, training, evaluation, and deployment of DNN backbones tailored for few-shot learning.","Additionally, we showcase our work's potential by building and deploying a low-power, low-latency demonstrator trained on the MiniImageNet dataset with a dataflow architecture.","The proposed system has a latency of 30 ms while consuming 6.2 W on the PYNQ-Z1 board."],"url":"http://arxiv.org/abs/2404.19354v1","category":"cs.AR"}
{"created":"2024-04-30 08:18:26","title":"Quasi-determinant and right eigenvalues of dual quaternion matrices","abstract":"Dual quaternion/complex matrices have important applications in brain science and multi-agent formation control. In this paper, we first study some basic properties of determinants of dual complex matrices, including Sturm theorem and Bloomfield-Watson inequality for dual complex matrices. Then, we show that every eigenvalue of a dual complex matrix must be the root of the characteristic polynomial of this matrix. With the help of the determinants of dual complex matrices, we introduce the concept of quasi-determinants of dual quaternion matrices, and show that every right eigenvalue of a dual quaternion matrix must be the root of the quasi-characteristic polynomial of this matrix, as well as the quasi-determinant of a dual quaternion Hermitian matrix is equivalent to the product of the square of the magnitudes of all eigenvalues. Our results are helpful for the further study of dual quaternion matrix theory, and their applications.","sentences":["Dual quaternion/complex matrices have important applications in brain science and multi-agent formation control.","In this paper, we first study some basic properties of determinants of dual complex matrices, including Sturm theorem and Bloomfield-Watson inequality for dual complex matrices.","Then, we show that every eigenvalue of a dual complex matrix must be the root of the characteristic polynomial of this matrix.","With the help of the determinants of dual complex matrices, we introduce the concept of quasi-determinants of dual quaternion matrices, and show that every right eigenvalue of a dual quaternion matrix must be the root of the quasi-characteristic polynomial of this matrix, as well as the quasi-determinant of a dual quaternion Hermitian matrix is equivalent to the product of the square of the magnitudes of all eigenvalues.","Our results are helpful for the further study of dual quaternion matrix theory, and their applications."],"url":"http://arxiv.org/abs/2404.19348v1","category":"math.RA"}
{"created":"2024-04-30 08:09:24","title":"Data-adaptive structural change-point detection via isolation","abstract":"In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals. The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors.","sentences":["In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence.","The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals.","We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals.","The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability.","Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval.","The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy.","The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors."],"url":"http://arxiv.org/abs/2404.19344v1","category":"stat.ME"}
{"created":"2024-04-30 08:08:06","title":"Quantum Modelling of Magnetism in Strongly Correlated Materials: Evaluating Constrained DFT and LDA+$U$+$J$ for Y114","abstract":"Transition-metal compounds represent a fascinating playground for exploring the intricate relationship between structural distortions, electronic properties, and magnetic behaviour, holding significant promise for technological advancements. Among these compounds, YBaCo$_4$O$_{7}$ (Y114) is attractive due to its manifestation of a ferrimagnetic component at low temperature intertwined with distortion effect due to the charge disproportionation on Co ions, exerting profound impact on its magnetic properties. In this perspective paper, we study the structural and magnetic intricacies of the Y114 crystal. Traditionally, the investigation of such materials has relied heavily on computational modelling using density-functional theory (DFT) with the on-site Coulomb interaction correction $U$ (DFT+$U$) based on the Hubbard model (sometimes including Hund's exchange coupling parameter $J$, DFT+$U$+$J$) to unravel their complexities. Herein, we analysed the spurious effects of magnetic-moment delocalisation and spillover to non-magnetic ions in the lattice on electronic structure and magnetic properties of Y114. To overcome this problem we have applied constrained DFT (cDFT) based on the potential self-consistency approach, and comprehensively explore the Y114 crystal's characteristics in its ferrimagnetic order. We find that cDFT yields magnetic moments of Co ions much closer to the experimental values than LDA+$U$+$J$ with the parameters $U$ and $J$ fitted to reproduce experimental lattice constants. cDFT allows for an accurate prediction of magnetic properties using oxidation states of magnetic ions as well-defined parameters. Through this perspective, we not only enhance our understanding of the magnetic interactions in Y114 crystal, but also pave the way for future investigations into magnetic materials.","sentences":["Transition-metal compounds represent a fascinating playground for exploring the intricate relationship between structural distortions, electronic properties, and magnetic behaviour, holding significant promise for technological advancements.","Among these compounds, YBaCo$_4$O$_{7}$ (Y114) is attractive due to its manifestation of a ferrimagnetic component at low temperature intertwined with distortion effect due to the charge disproportionation on Co ions, exerting profound impact on its magnetic properties.","In this perspective paper, we study the structural and magnetic intricacies of the Y114 crystal.","Traditionally, the investigation of such materials has relied heavily on computational modelling using density-functional theory (DFT) with the on-site Coulomb interaction correction $U$ (DFT+$U$) based on the Hubbard model (sometimes including Hund's exchange coupling parameter $J$, DFT+$U$+$J$) to unravel their complexities.","Herein, we analysed the spurious effects of magnetic-moment delocalisation and spillover to non-magnetic ions in the lattice on electronic structure and magnetic properties of Y114.","To overcome this problem we have applied constrained DFT (cDFT) based on the potential self-consistency approach, and comprehensively explore the Y114 crystal's characteristics in its ferrimagnetic order.","We find that cDFT yields magnetic moments of Co ions much closer to the experimental values than LDA+$U$+$J$ with the parameters $U$ and $J$ fitted to reproduce experimental lattice constants.","cDFT allows for an accurate prediction of magnetic properties using oxidation states of magnetic ions as well-defined parameters.","Through this perspective, we not only enhance our understanding of the magnetic interactions in Y114 crystal, but also pave the way for future investigations into magnetic materials."],"url":"http://arxiv.org/abs/2404.19343v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 08:06:02","title":"Revisiting the Constraint on Equation of State of Neutron Star based on the Binary Neutron Star Mergers","abstract":"The merger of neutron star (NS)-NS binary can form different production of the compact remnant, among which the supramassive NS (SMNS) could create an internal plateau and the followed steep decay marks the collapse of the SMNS. The proportion of SMNS and the corresponding collapse-time are often used to constrain the NS equation of state (EoS). This paper revisits this topic by considering the effect of an accretion disk on the compact remnant, which is not considered in previous works. Compared with previous works, the collapse-time distribution (peaks $\\sim$100 s) of the SMNSs formed from NS-NS merger is almost unaffected by the initial surface magnetic ($B_{{\\rm s},i}$) of NS, but the total energy output of the magnetic dipole radiation from the SMNSs depends on $B_{{\\rm s},i}$ significantly. Coupling the constraints from the SMNS fraction, we exclude some EoSs and obtain three candidate EoSs, i.e., DD2, ENG, and MPA1. By comparing the distributions of the collapse-time and the luminosity of the internal plateau (in the short gamma-ray bursts) for those from observations with those obtained based on the three candidate EoSs, it is shown that only the EoS of ENG is favored. Our sample based on the ENG EOS and a mass distribution motivated by Galactic systems suggests that approximately $99\\%$ of NS-NS mergers collapse to form a black hole within $10^7$s. This includes scenarios forming a BH promptly ($36.5\\%$), a SMNS ($60.7\\%$), or a stable NS that transitions into a BH or a SMNS following accretion ($2.1\\%$). It also indicates that the remnants for GW170817 and GW190425, and the second object of GW190814 are more likely to be BHs.","sentences":["The merger of neutron star (NS)-NS binary can form different production of the compact remnant, among which the supramassive NS (SMNS) could create an internal plateau and the followed steep decay marks the collapse of the SMNS.","The proportion of SMNS and the corresponding collapse-time are often used to constrain the NS equation of state (EoS).","This paper revisits this topic by considering the effect of an accretion disk on the compact remnant, which is not considered in previous works.","Compared with previous works, the collapse-time distribution (peaks $\\sim$100 s) of the SMNSs formed from NS-NS merger is almost unaffected by the initial surface magnetic ($B_{{\\rm s},i}$) of NS, but the total energy output of the magnetic dipole radiation from the SMNSs depends on $B_{{\\rm s},i}$ significantly.","Coupling the constraints from the SMNS fraction, we exclude some EoSs and obtain three candidate EoSs, i.e., DD2, ENG, and MPA1.","By comparing the distributions of the collapse-time and the luminosity of the internal plateau (in the short gamma-ray bursts) for those from observations with those obtained based on the three candidate EoSs, it is shown that only the EoS of ENG is favored.","Our sample based on the ENG EOS and a mass distribution motivated by Galactic systems suggests that approximately $99\\%$ of NS-NS mergers collapse to form a black hole within $10^7$s.","This includes scenarios forming a BH promptly ($36.5\\%$), a SMNS ($60.7\\%$), or a stable NS that transitions into a BH or a SMNS following accretion ($2.1\\%$).","It also indicates that the remnants for GW170817 and GW190425, and the second object of GW190814 are more likely to be BHs."],"url":"http://arxiv.org/abs/2404.19340v1","category":"astro-ph.HE"}
{"created":"2024-04-30 08:00:17","title":"Multi-Scale Heterogeneity-Aware Hypergraph Representation for Histopathology Whole Slide Images","abstract":"Survival prediction is a complex ordinal regression task that aims to predict the survival coefficient ranking among a cohort of patients, typically achieved by analyzing patients' whole slide images. Existing deep learning approaches mainly adopt multiple instance learning or graph neural networks under weak supervision. Most of them are unable to uncover the diverse interactions between different types of biological entities(\\textit{e.g.}, cell cluster and tissue block) across multiple scales, while such interactions are crucial for patient survival prediction. In light of this, we propose a novel multi-scale heterogeneity-aware hypergraph representation framework. Specifically, our framework first constructs a multi-scale heterogeneity-aware hypergraph and assigns each node with its biological entity type. It then mines diverse interactions between nodes on the graph structure to obtain a global representation. Experimental results demonstrate that our method outperforms state-of-the-art approaches on three benchmark datasets. Code is publicly available at \\href{https://github.com/Hanminghao/H2GT}{https://github.com/Hanminghao/H2GT}.","sentences":["Survival prediction is a complex ordinal regression task that aims to predict the survival coefficient ranking among a cohort of patients, typically achieved by analyzing patients' whole slide images.","Existing deep learning approaches mainly adopt multiple instance learning or graph neural networks under weak supervision.","Most of them are unable to uncover the diverse interactions between different types of biological entities(\\textit{e.g.}, cell cluster and tissue block) across multiple scales, while such interactions are crucial for patient survival prediction.","In light of this, we propose a novel multi-scale heterogeneity-aware hypergraph representation framework.","Specifically, our framework first constructs a multi-scale heterogeneity-aware hypergraph and assigns each node with its biological entity type.","It then mines diverse interactions between nodes on the graph structure to obtain a global representation.","Experimental results demonstrate that our method outperforms state-of-the-art approaches on three benchmark datasets.","Code is publicly available at \\href{https://github.com/Hanminghao/H2GT}{https://github.com/Hanminghao/H2GT}."],"url":"http://arxiv.org/abs/2404.19334v1","category":"cs.CV"}
{"created":"2024-04-30 07:37:48","title":"Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten Text Recognition","abstract":"In recent advances in automatic text recognition (ATR), deep neural networks have demonstrated the ability to implicitly capture language statistics, potentially reducing the need for traditional language models. This study directly addresses whether explicit language models, specifically n-gram models, still contribute to the performance of state-of-the-art deep learning architectures in the field of handwriting recognition. We evaluate two prominent neural network architectures, PyLaia and DAN, with and without the integration of explicit n-gram language models. Our experiments on three datasets - IAM, RIMES, and NorHand v2 - at both line and page level, investigate optimal parameters for n-gram models, including their order, weight, smoothing methods and tokenization level. The results show that incorporating character or subword n-gram models significantly improves the performance of ATR models on all datasets, challenging the notion that deep learning models alone are sufficient for optimal performance. In particular, the combination of DAN with a character language model outperforms current benchmarks, confirming the value of hybrid approaches in modern document analysis systems.","sentences":["In recent advances in automatic text recognition (ATR), deep neural networks have demonstrated the ability to implicitly capture language statistics, potentially reducing the need for traditional language models.","This study directly addresses whether explicit language models, specifically n-gram models, still contribute to the performance of state-of-the-art deep learning architectures in the field of handwriting recognition.","We evaluate two prominent neural network architectures, PyLaia and DAN, with and without the integration of explicit n-gram language models.","Our experiments on three datasets - IAM, RIMES, and NorHand v2 - at both line and page level, investigate optimal parameters for n-gram models, including their order, weight, smoothing methods and tokenization level.","The results show that incorporating character or subword n-gram models significantly improves the performance of ATR models on all datasets, challenging the notion that deep learning models alone are sufficient for optimal performance.","In particular, the combination of DAN with a character language model outperforms current benchmarks, confirming the value of hybrid approaches in modern document analysis systems."],"url":"http://arxiv.org/abs/2404.19317v1","category":"cs.CV"}
{"created":"2024-04-30 07:33:51","title":"Modeling Orthographic Variation in Occitan's Dialects","abstract":"Effectively normalizing textual data poses a considerable challenge, especially for low-resource languages lacking standardized writing systems. In this study, we fine-tuned a multilingual model with data from several Occitan dialects and conducted a series of experiments to assess the model's representations of these dialects. For evaluation purposes, we compiled a parallel lexicon encompassing four Occitan dialects. Intrinsic evaluations of the model's embeddings revealed that surface similarity between the dialects strengthened representations. When the model was further fine-tuned for part-of-speech tagging and Universal Dependency parsing, its performance was robust to dialectical variation, even when trained solely on part-of-speech data from a single dialect. Our findings suggest that large multilingual models minimize the need for spelling normalization during pre-processing.","sentences":["Effectively normalizing textual data poses a considerable challenge, especially for low-resource languages lacking standardized writing systems.","In this study, we fine-tuned a multilingual model with data from several Occitan dialects and conducted a series of experiments to assess the model's representations of these dialects.","For evaluation purposes, we compiled a parallel lexicon encompassing four Occitan dialects.","Intrinsic evaluations of the model's embeddings revealed that surface similarity between the dialects strengthened representations.","When the model was further fine-tuned for part-of-speech tagging and Universal Dependency parsing, its performance was robust to dialectical variation, even when trained solely on part-of-speech data from a single dialect.","Our findings suggest that large multilingual models minimize the need for spelling normalization during pre-processing."],"url":"http://arxiv.org/abs/2404.19315v1","category":"cs.CL"}
{"created":"2024-04-30 07:29:40","title":"Does Whisper understand Swiss German? An automatic, qualitative, and human evaluation","abstract":"Whisper is a state-of-the-art automatic speech recognition (ASR) model (Radford et al., 2022). Although Swiss German dialects are allegedly not part of Whisper's training data, preliminary experiments showed that Whisper can transcribe Swiss German quite well, with the output being a speech translation into Standard German. To gain a better understanding of Whisper's performance on Swiss German, we systematically evaluate it using automatic, qualitative, and human evaluation. We test its performance on three existing test sets: SwissDial (Dogan-Sch\\\"onberger et al., 2021), STT4SG-350 (Pl\\\"uss et al., 2023), and Swiss Parliaments Corpus (Pl\\\"uss et al., 2021). In addition, we create a new test set for this work, based on short mock clinical interviews.   For automatic evaluation, we used word error rate (WER) and BLEU. In the qualitative analysis, we discuss Whisper's strengths and weaknesses and anylyze some output examples. For the human evaluation, we conducted a survey with 28 participants who were asked to evaluate Whisper's performance.   All of our evaluations suggest that Whisper is a viable ASR system for Swiss German, so long as the Standard German output is desired.","sentences":["Whisper is a state-of-the-art automatic speech recognition (ASR) model (Radford et al., 2022).","Although Swiss German dialects are allegedly not part of Whisper's training data, preliminary experiments showed that Whisper can transcribe Swiss German quite well, with the output being a speech translation into Standard German.","To gain a better understanding of Whisper's performance on Swiss German, we systematically evaluate it using automatic, qualitative, and human evaluation.","We test its performance on three existing test sets: SwissDial (Dogan-Sch\\\"onberger et al., 2021), STT4SG-350 (Pl\\\"uss et al., 2023), and Swiss Parliaments Corpus (Pl\\\"uss et al., 2021).","In addition, we create a new test set for this work, based on short mock clinical interviews.   ","For automatic evaluation, we used word error rate (WER) and BLEU.","In the qualitative analysis, we discuss Whisper's strengths and weaknesses and anylyze some output examples.","For the human evaluation, we conducted a survey with 28 participants who were asked to evaluate Whisper's performance.   ","All of our evaluations suggest that Whisper is a viable ASR system for Swiss German, so long as the Standard German output is desired."],"url":"http://arxiv.org/abs/2404.19310v1","category":"cs.CL"}
{"created":"2024-04-30 07:24:32","title":"Enhancing GUI Exploration Coverage of Android Apps with Deep Link-Integrated Monkey","abstract":"Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting. Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries. This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages. To address this, we utilize Android's deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method. This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey). Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages. We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances. We conduct experiments to evaluate Delm's effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection. The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2\\% activity coverage, 21.13\\% method coverage, and 23.81\\% crash detection.","sentences":["Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting.","Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries.","This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages.","To address this, we utilize Android's deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method.","This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey).","Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages.","We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances.","We conduct experiments to evaluate Delm's effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection.","The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2\\% activity coverage, 21.13\\% method coverage, and 23.81\\% crash detection."],"url":"http://arxiv.org/abs/2404.19307v1","category":"cs.SE"}
{"created":"2024-04-30 07:17:29","title":"Species of structure and physical dimensions","abstract":"This study addresses the often underestimated importance of physical dimensions and units in the formal reconstruction of physical theories, focusing on structuralist approaches that use the concept of ``species of structure\" as a meta-mathematical tool. We are pursuing an approach that goes back to a suggestion by T.~Tao. It involves the representation of fundamental physical quantities by one-dimensional real ordered vector spaces, while derived quantities are formulated using concepts from linear algebra, e.~g.~tensor products and dual spaces. As an introduction, the theory of Ohm's law is considered. We then formulate a reconstruction of the calculus of physical dimensions, including Buckingham's $\\Pi$-theorem. Furthermore, an application of this method to the Newtonian theory of gravitating systems consisting of point particles is demonstrated, emphasizing the role of the automorphism group.","sentences":["This study addresses the often underestimated importance of physical dimensions and units in the formal reconstruction of physical theories, focusing on structuralist approaches that use the concept of ``species of structure\" as a meta-mathematical tool.","We are pursuing an approach that goes back to a suggestion by T.~Tao.","It involves the representation of fundamental physical quantities by one-dimensional real ordered vector spaces, while derived quantities are formulated using concepts from linear algebra, e.~g.~tensor products and dual spaces.","As an introduction, the theory of Ohm's law is considered.","We then formulate a reconstruction of the calculus of physical dimensions, including Buckingham's $\\Pi$-theorem.","Furthermore, an application of this method to the Newtonian theory of gravitating systems consisting of point particles is demonstrated, emphasizing the role of the automorphism group."],"url":"http://arxiv.org/abs/2404.19305v1","category":"math-ph"}
{"created":"2024-04-30 06:49:15","title":"Two atoms in a harmonic trap with spin-orbital-angular-momentum coupling","abstract":"We study the problem of two harmonically trapped atoms in the presence of spin-orbital-angular-momentum coupling. The two-body energy spectrum is numerically calculated by utilizing the exact diagonalization method. We analyze how the degeneracy of energy levels is lifted under the interplay between the interatomic interaction and spin-orbital-angular-momentum coupling. The exact numerical results show an excellent agreement with that of perturbation theory in the weak-interaction limit, as well as that in the absence of spin-orbital-angular-momentum coupling. The properties of correlations between the two atoms are also discussed with respect to the interaction strength. The findings in this work may provide valuable insights into few-body physics subjected to spin-orbital-angular-momentum coupling, and the possible experimental detection of spectrum functions in many-body systems, such as radio-frequency spectroscopy.","sentences":["We study the problem of two harmonically trapped atoms in the presence of spin-orbital-angular-momentum coupling.","The two-body energy spectrum is numerically calculated by utilizing the exact diagonalization method.","We analyze how the degeneracy of energy levels is lifted under the interplay between the interatomic interaction and spin-orbital-angular-momentum coupling.","The exact numerical results show an excellent agreement with that of perturbation theory in the weak-interaction limit, as well as that in the absence of spin-orbital-angular-momentum coupling.","The properties of correlations between the two atoms are also discussed with respect to the interaction strength.","The findings in this work may provide valuable insights into few-body physics subjected to spin-orbital-angular-momentum coupling, and the possible experimental detection of spectrum functions in many-body systems, such as radio-frequency spectroscopy."],"url":"http://arxiv.org/abs/2404.19293v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 06:47:04","title":"Dynamic Human Trust Modeling of Autonomous Agents With Varying Capability and Strategy","abstract":"Objective We model the dynamic trust of human subjects in a human-autonomy-teaming screen-based task.   Background Trust is an emerging area of study in human-robot collaboration. Many studies have looked at the issue of robot performance as a sole predictor of human trust, but this could underestimate the complexity of the interaction.   Method Subjects were paired with autonomous agents to search an on-screen grid to determine the number of outlier objects. In each trial, a different autonomous agent with a preassigned capability used one of three search strategies and then reported the number of outliers it found as a fraction of its capability. Then, the subject reported their total outlier estimate. Human subjects then evaluated statements about the agent's behavior, reliability, and their trust in the agent.   Results 80 subjects were recruited. Self-reported trust was modeled using Ordinary Least Squares, but the group that interacted with varying capability agents on a short time order produced a better performing ARIMAX model. Models were cross-validated between groups and found a moderate improvement in the next trial trust prediction.   Conclusion A time series modeling approach reveals the effects of temporal ordering of agent performance on estimated trust. Recency bias may affect how subjects weigh the contribution of strategy or capability to trust. Understanding the connections between agent behavior, agent performance, and human trust is crucial to improving human-robot collaborative tasks.   Application The modeling approach in this study demonstrates the need to represent autonomous agent characteristics over time to capture changes in human trust.","sentences":["Objective We model the dynamic trust of human subjects in a human-autonomy-teaming screen-based task.   ","Background Trust is an emerging area of study in human-robot collaboration.","Many studies have looked at the issue of robot performance as a sole predictor of human trust, but this could underestimate the complexity of the interaction.   ","Method Subjects were paired with autonomous agents to search an on-screen grid to determine the number of outlier objects.","In each trial, a different autonomous agent with a preassigned capability used one of three search strategies and then reported the number of outliers it found as a fraction of its capability.","Then, the subject reported their total outlier estimate.","Human subjects then evaluated statements about the agent's behavior, reliability, and their trust in the agent.   ","Results 80 subjects were recruited.","Self-reported trust was modeled using Ordinary Least Squares, but the group that interacted with varying capability agents on a short time order produced a better performing ARIMAX model.","Models were cross-validated between groups and found a moderate improvement in the next trial trust prediction.   ","Conclusion A time series modeling approach reveals the effects of temporal ordering of agent performance on estimated trust.","Recency bias may affect how subjects weigh the contribution of strategy or capability to trust.","Understanding the connections between agent behavior, agent performance, and human trust is crucial to improving human-robot collaborative tasks.   ","Application","The modeling approach in this study demonstrates the need to represent autonomous agent characteristics over time to capture changes in human trust."],"url":"http://arxiv.org/abs/2404.19291v1","category":"cs.HC"}
{"created":"2024-04-30 06:27:05","title":"Three-dimensional plasmoid-mediated reconnection and turbulence in Hall magnetohydrodynamics","abstract":"Plasmoid instability accelerates reconnection in collisional plasmas by transforming a laminar reconnection layer into numerous plasmoids connected by secondary current sheets in two dimensions (2D) and by fostering self-generated turbulent reconnection in three dimensions (3D). In large-scale astrophysical and space systems, plasmoid instability likely initiates in the collisional regime but may transition into the collisionless regime as the fragmentation of the current sheet progresses toward kinetic scales. Hall MHD models are widely regarded as a simplified yet effective representation of the transition from collisional to collisionless reconnection. However, plasmoid instability in 2D Hall MHD simulations often leads to a single-X-line reconnection configuration, which significantly differs from fully kinetic particle-in-cell simulation results. This study shows that single-X-line reconnection is less likely to occur in 3D compared to 2D. Moreover, depending on the Lundquist number and the ratio between the system size and the kinetic scale, Hall MHD can also realize 3D self-generated turbulent reconnection. We analyze the features of the self-generated turbulent state, including the energy power spectra and the scale dependence of turbulent eddy anisotropy.","sentences":["Plasmoid instability accelerates reconnection in collisional plasmas by transforming a laminar reconnection layer into numerous plasmoids connected by secondary current sheets in two dimensions (2D) and by fostering self-generated turbulent reconnection in three dimensions (3D).","In large-scale astrophysical and space systems, plasmoid instability likely initiates in the collisional regime but may transition into the collisionless regime as the fragmentation of the current sheet progresses toward kinetic scales.","Hall MHD models are widely regarded as a simplified yet effective representation of the transition from collisional to collisionless reconnection.","However, plasmoid instability in 2D Hall MHD simulations often leads to a single-X-line reconnection configuration, which significantly differs from fully kinetic particle-in-cell simulation results.","This study shows that single-X-line reconnection is less likely to occur in 3D compared to 2D. Moreover, depending on the Lundquist number and the ratio between the system size and the kinetic scale, Hall MHD can also realize 3D self-generated turbulent reconnection.","We analyze the features of the self-generated turbulent state, including the energy power spectra and the scale dependence of turbulent eddy anisotropy."],"url":"http://arxiv.org/abs/2404.19285v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 05:54:40","title":"Bridge to Non-Barrier Communication: Gloss-Prompted Fine-grained Cued Speech Gesture Generation with Diffusion Model","abstract":"Cued Speech (CS) is an advanced visual phonetic encoding system that integrates lip reading with hand codings, enabling people with hearing impairments to communicate efficiently. CS video generation aims to produce specific lip and gesture movements of CS from audio or text inputs. The main challenge is that given limited CS data, we strive to simultaneously generate fine-grained hand and finger movements, as well as lip movements, meanwhile the two kinds of movements need to be asynchronously aligned. Existing CS generation methods are fragile and prone to poor performance due to template-based statistical models and careful hand-crafted pre-processing to fit the models. Therefore, we propose a novel Gloss-prompted Diffusion-based CS Gesture generation framework (called GlossDiff). Specifically, to integrate additional linguistic rules knowledge into the model. we first introduce a bridging instruction called \\textbf{Gloss}, which is an automatically generated descriptive text to establish a direct and more delicate semantic connection between spoken language and CS gestures. Moreover, we first suggest rhythm is an important paralinguistic feature for CS to improve the communication efficacy. Therefore, we propose a novel Audio-driven Rhythmic Module (ARM) to learn rhythm that matches audio speech. Moreover, in this work, we design, record, and publish the first Chinese CS dataset with four CS cuers. Extensive experiments demonstrate that our method quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods. We release the code and data at https://glossdiff.github.io/.","sentences":["Cued Speech (CS) is an advanced visual phonetic encoding system that integrates lip reading with hand codings, enabling people with hearing impairments to communicate efficiently.","CS video generation aims to produce specific lip and gesture movements of CS from audio or text inputs.","The main challenge is that given limited CS data, we strive to simultaneously generate fine-grained hand and finger movements, as well as lip movements, meanwhile the two kinds of movements need to be asynchronously aligned.","Existing CS generation methods are fragile and prone to poor performance due to template-based statistical models and careful hand-crafted pre-processing to fit the models.","Therefore, we propose a novel Gloss-prompted Diffusion-based CS Gesture generation framework (called GlossDiff).","Specifically, to integrate additional linguistic rules knowledge into the model.","we first introduce a bridging instruction called \\textbf{Gloss}, which is an automatically generated descriptive text to establish a direct and more delicate semantic connection between spoken language and CS gestures.","Moreover, we first suggest rhythm is an important paralinguistic feature for CS to improve the communication efficacy.","Therefore, we propose a novel Audio-driven Rhythmic Module (ARM) to learn rhythm that matches audio speech.","Moreover, in this work, we design, record, and publish the first Chinese CS dataset with four CS cuers.","Extensive experiments demonstrate that our method quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods.","We release the code and data at https://glossdiff.github.io/."],"url":"http://arxiv.org/abs/2404.19277v1","category":"cs.CV"}
{"created":"2024-04-30 05:51:21","title":"C2FDrone: Coarse-to-Fine Drone-to-Drone Detection using Vision Transformer Networks","abstract":"A vision-based drone-to-drone detection system is crucial for various applications like collision avoidance, countering hostile drones, and search-and-rescue operations. However, detecting drones presents unique challenges, including small object sizes, distortion, occlusion, and real-time processing requirements. Current methods integrating multi-scale feature fusion and temporal information have limitations in handling extreme blur and minuscule objects. To address this, we propose a novel coarse-to-fine detection strategy based on vision transformers. We evaluate our approach on three challenging drone-to-drone detection datasets, achieving F1 score enhancements of 7%, 3%, and 1% on the FL-Drones, AOT, and NPS-Drones datasets, respectively. Additionally, we demonstrate real-time processing capabilities by deploying our model on an edge-computing device. Our code will be made publicly available.","sentences":["A vision-based drone-to-drone detection system is crucial for various applications like collision avoidance, countering hostile drones, and search-and-rescue operations.","However, detecting drones presents unique challenges, including small object sizes, distortion, occlusion, and real-time processing requirements.","Current methods integrating multi-scale feature fusion and temporal information have limitations in handling extreme blur and minuscule objects.","To address this, we propose a novel coarse-to-fine detection strategy based on vision transformers.","We evaluate our approach on three challenging drone-to-drone detection datasets, achieving F1 score enhancements of 7%, 3%, and 1% on the FL-Drones, AOT, and NPS-Drones datasets, respectively.","Additionally, we demonstrate real-time processing capabilities by deploying our model on an edge-computing device.","Our code will be made publicly available."],"url":"http://arxiv.org/abs/2404.19276v1","category":"cs.CV"}
{"created":"2024-04-30 05:43:30","title":"AdapTics: A Toolkit for Creative Design and Integration of Real-Time Adaptive Mid-Air Ultrasound Tactons","abstract":"Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback. Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime. These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable. This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons-dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs. A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications. We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers.","sentences":["Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback.","Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime.","These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable.","This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons-dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs.","A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications.","We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers."],"url":"http://arxiv.org/abs/2404.19275v1","category":"cs.HC"}
{"created":"2024-04-30 05:41:49","title":"Statistical Mechanics Calculations Using Variational Autoregressive Networks and Quantum Annealing","abstract":"In statistical mechanics, computing the partition function is generally difficult. An approximation method using a variational autoregressive network (VAN) has been proposed recently. This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples. The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution. When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field.","sentences":["In statistical mechanics, computing the partition function is generally difficult.","An approximation method using a variational autoregressive network (VAN) has been proposed recently.","This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples.","The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution.","When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field."],"url":"http://arxiv.org/abs/2404.19274v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-30 05:33:37","title":"Regularity and long-time behavior of global weak solutions to a coupled Cahn-Hilliard system: the off-critical case","abstract":"We consider a diffuse interface model that describes the macro- and micro-phase separation processes of a polymer mixture. The resulting system consists of a Cahn-Hilliard equation and a Cahn-Hilliard-Oono type equation endowed with the singular Flory-Huggins potential. For the initial boundary value problem in a bounded smooth domain of $\\mathbb{R}^d$ ($d\\in\\{2,3\\}$) with homogeneous Neumann boundary conditions for the phase functions as well as chemical potentials, we study the regularity and long-time behavior of global weak solutions in the off-critical case, i.e., the mass is not conserved during the micro-phase separation of diblock copolymers. By investigating an auxiliary system with viscous regularizations, we show that every global weak solution regularizes instantaneously for $t>0$. In two dimensions, we obtain the instantaneous strict separation property under a mild growth condition on the first derivative of potential functions near pure phases $\\pm 1$, while in three dimensions, we establish the eventual strict separation property for sufficiently large time. Finally, we prove that every global weak solution converges to a single equilibrium as $t\\to +\\infty$.","sentences":["We consider a diffuse interface model that describes the macro- and micro-phase separation processes of a polymer mixture.","The resulting system consists of a Cahn-Hilliard equation and a Cahn-Hilliard-Oono type equation endowed with the singular Flory-Huggins potential.","For the initial boundary value problem in a bounded smooth domain of $\\mathbb{R}^d$ ($d\\in\\{2,3\\}$) with homogeneous Neumann boundary conditions for the phase functions as well as chemical potentials, we study the regularity and long-time behavior of global weak solutions in the off-critical case, i.e., the mass is not conserved during the micro-phase separation of diblock copolymers.","By investigating an auxiliary system with viscous regularizations, we show that every global weak solution regularizes instantaneously for $t>0$. In two dimensions, we obtain the instantaneous strict separation property under a mild growth condition on the first derivative of potential functions near pure phases $\\pm 1$, while in three dimensions, we establish the eventual strict separation property for sufficiently large time.","Finally, we prove that every global weak solution converges to a single equilibrium as $t\\to +\\infty$."],"url":"http://arxiv.org/abs/2404.19271v1","category":"math.AP"}
{"created":"2024-04-30 05:27:09","title":"Electromagnetic response of spinful Majorana fermions","abstract":"A remarkable feature of topological superconductors is the emergence of Majorana fermions in electron systems. Whereas the emergent Majorana fermions share the self-anti-particle property with Majorana fermions in particle physics, they may have essentially different electromagnetic properties. In this paper, we argue the electromagnetic response of spinful Majorana fermions in topological superconductors. We present a general theory of the electromagnetic response of spinful Majorana fermions in topological superconductors and clarify how the pairing symmetry is encoded in the electromagnetic response. As an application, we predict the sublattice-dependent dipole (Ising)-type magnetic response of corner Majorana fermions in iron-based superconductors.","sentences":["A remarkable feature of topological superconductors is the emergence of Majorana fermions in electron systems.","Whereas the emergent Majorana fermions share the self-anti-particle property with Majorana fermions in particle physics, they may have essentially different electromagnetic properties.","In this paper, we argue the electromagnetic response of spinful Majorana fermions in topological superconductors.","We present a general theory of the electromagnetic response of spinful Majorana fermions in topological superconductors and clarify how the pairing symmetry is encoded in the electromagnetic response.","As an application, we predict the sublattice-dependent dipole (Ising)-type magnetic response of corner Majorana fermions in iron-based superconductors."],"url":"http://arxiv.org/abs/2404.19269v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 05:11:32","title":"Mapping New Realities: Ground Truth Image Creation with Pix2Pix Image-to-Image Translation","abstract":"Generative Adversarial Networks (GANs) have significantly advanced image processing, with Pix2Pix being a notable framework for image-to-image translation. This paper explores a novel application of Pix2Pix to transform abstract map images into realistic ground truth images, addressing the scarcity of such images crucial for domains like urban planning and autonomous vehicle training. We detail the Pix2Pix model's utilization for generating high-fidelity datasets, supported by a dataset of paired map and aerial images, and enhanced by a tailored training regimen. The results demonstrate the model's capability to accurately render complex urban features, establishing its efficacy and potential for broad real-world applications.","sentences":["Generative Adversarial Networks (GANs) have significantly advanced image processing, with Pix2Pix being a notable framework for image-to-image translation.","This paper explores a novel application of Pix2Pix to transform abstract map images into realistic ground truth images, addressing the scarcity of such images crucial for domains like urban planning and autonomous vehicle training.","We detail the Pix2Pix model's utilization for generating high-fidelity datasets, supported by a dataset of paired map and aerial images, and enhanced by a tailored training regimen.","The results demonstrate the model's capability to accurately render complex urban features, establishing its efficacy and potential for broad real-world applications."],"url":"http://arxiv.org/abs/2404.19265v2","category":"cs.CV"}
{"created":"2024-04-30 05:10:59","title":"DiffuseLoco: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets","abstract":"This work introduces DiffuseLoco, a framework for training multi-skill diffusion-based policies for dynamic legged locomotion from offline datasets, enabling real-time control of diverse skills on robots in the real world. Offline learning at scale has led to breakthroughs in computer vision, natural language processing, and robotic manipulation domains. However, scaling up learning for legged robot locomotion, especially with multiple skills in a single policy, presents significant challenges for prior online reinforcement learning methods. To address this challenge, we propose a novel, scalable framework that leverages diffusion models to directly learn from offline multimodal datasets with a diverse set of locomotion skills. With design choices tailored for real-time control in dynamical systems, including receding horizon control and delayed inputs, DiffuseLoco is capable of reproducing multimodality in performing various locomotion skills, zero-shot transfer to real quadrupedal robots, and it can be deployed on edge computing devices. Furthermore, DiffuseLoco demonstrates free transitions between skills and robustness against environmental variations. Through extensive benchmarking in real-world experiments, DiffuseLoco exhibits better stability and velocity tracking performance compared to prior reinforcement learning and non-diffusion-based behavior cloning baselines. The design choices are validated via comprehensive ablation studies. This work opens new possibilities for scaling up learning-based legged locomotion controllers through the scaling of large, expressive models and diverse offline datasets.","sentences":["This work introduces DiffuseLoco, a framework for training multi-skill diffusion-based policies for dynamic legged locomotion from offline datasets, enabling real-time control of diverse skills on robots in the real world.","Offline learning at scale has led to breakthroughs in computer vision, natural language processing, and robotic manipulation domains.","However, scaling up learning for legged robot locomotion, especially with multiple skills in a single policy, presents significant challenges for prior online reinforcement learning methods.","To address this challenge, we propose a novel, scalable framework that leverages diffusion models to directly learn from offline multimodal datasets with a diverse set of locomotion skills.","With design choices tailored for real-time control in dynamical systems, including receding horizon control and delayed inputs, DiffuseLoco is capable of reproducing multimodality in performing various locomotion skills, zero-shot transfer to real quadrupedal robots, and it can be deployed on edge computing devices.","Furthermore, DiffuseLoco demonstrates free transitions between skills and robustness against environmental variations.","Through extensive benchmarking in real-world experiments, DiffuseLoco exhibits better stability and velocity tracking performance compared to prior reinforcement learning and non-diffusion-based behavior cloning baselines.","The design choices are validated via comprehensive ablation studies.","This work opens new possibilities for scaling up learning-based legged locomotion controllers through the scaling of large, expressive models and diverse offline datasets."],"url":"http://arxiv.org/abs/2404.19264v1","category":"cs.RO"}
{"created":"2024-04-30 05:08:54","title":"mm-Wave and sub-THz Chip-to-Package Transitions for Communications Systems","abstract":"This work presents mm-Wave and sub-THz chip-to-package transitions for communications systems. To date, reported transitions either have high loss, typically 3 to 4 dB, or require high cost packages to support very fine bump pitches and low loss materials. We analyze the impact of transitions on a high frequency, wide bandwidth communication system and present the design of a chip-to-package transition in two different commercial packaging technologies. The proposed transitions achieve <1 dB loss in both technologies, validating the design methodology.","sentences":["This work presents mm-Wave and sub-THz chip-to-package transitions for communications systems.","To date, reported transitions either have high loss, typically 3 to 4 dB, or require high cost packages to support very fine bump pitches and low loss materials.","We analyze the impact of transitions on a high frequency, wide bandwidth communication system and present the design of a chip-to-package transition in two different commercial packaging technologies.","The proposed transitions achieve <1 dB loss in both technologies, validating the design methodology."],"url":"http://arxiv.org/abs/2404.19263v1","category":"eess.SY"}
{"created":"2024-04-30 04:22:11","title":"On a Family of Relaxed Gradient Descent Methods for Quadratic Minimization","abstract":"This paper studies the convergence properties of a family of Relaxed $\\ell$-Minimal Gradient Descent methods for quadratic optimization; the family includes the omnipresent Steepest Descent method, as well as the Minimal Gradient method. Simple proofs are provided that show, in an appropriately chosen norm, the gradient and the distance of the iterates from optimality converge linearly, for all members of the family. Moreover, the function values decrease linearly, and iteration complexity results are provided. All theoretical results hold when (fixed) relaxation is employed. It is also shown that, given a fixed overhead and storage budget, every Relaxed $\\ell$-Minimal Gradient Descent method can be implemented using exactly one matrix vector product. Numerical experiments are presented that illustrate the benefits of relaxation across the family.","sentences":["This paper studies the convergence properties of a family of Relaxed $\\ell$-Minimal Gradient Descent methods for quadratic optimization; the family includes the omnipresent Steepest Descent method, as well as the Minimal Gradient method.","Simple proofs are provided that show, in an appropriately chosen norm, the gradient and the distance of the iterates from optimality converge linearly, for all members of the family.","Moreover, the function values decrease linearly, and iteration complexity results are provided.","All theoretical results hold when (fixed) relaxation is employed.","It is also shown that, given a fixed overhead and storage budget, every Relaxed $\\ell$-Minimal Gradient Descent method can be implemented using exactly one matrix vector product.","Numerical experiments are presented that illustrate the benefits of relaxation across the family."],"url":"http://arxiv.org/abs/2404.19255v1","category":"math.OC"}
{"created":"2024-04-30 04:16:55","title":"Exploiting Hatred by Targets for Hate Speech Detection on Vietnamese Social Media Texts","abstract":"The growth of social networks makes toxic content spread rapidly. Hate speech detection is a task to help decrease the number of harmful comments. With the diversity in the hate speech created by users, it is necessary to interpret the hate speech besides detecting it. Hence, we propose a methodology to construct a system for targeted hate speech detection from online streaming texts from social media. We first introduce the ViTHSD - a targeted hate speech detection dataset for Vietnamese Social Media Texts. The dataset contains 10K comments, each comment is labeled to specific targets with three levels: clean, offensive, and hate. There are 5 targets in the dataset, and each target is labeled with the corresponding level manually by humans with strict annotation guidelines. The inter-annotator agreement obtained from the dataset is 0.45 by Cohen's Kappa index, which is indicated as a moderate level. Then, we construct a baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained language model to leverage the power of text representation of BERTology. Finally, we suggest a methodology to integrate the baseline model for targeted hate speech detection into the online streaming system for practical application in preventing hateful and offensive content on social media.","sentences":["The growth of social networks makes toxic content spread rapidly.","Hate speech detection is a task to help decrease the number of harmful comments.","With the diversity in the hate speech created by users, it is necessary to interpret the hate speech besides detecting it.","Hence, we propose a methodology to construct a system for targeted hate speech detection from online streaming texts from social media.","We first introduce the ViTHSD - a targeted hate speech detection dataset for Vietnamese Social Media Texts.","The dataset contains 10K comments, each comment is labeled to specific targets with three levels: clean, offensive, and hate.","There are 5 targets in the dataset, and each target is labeled with the corresponding level manually by humans with strict annotation guidelines.","The inter-annotator agreement obtained from the dataset is 0.45 by Cohen's Kappa index, which is indicated as a moderate level.","Then, we construct a baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained language model to leverage the power of text representation of BERTology.","Finally, we suggest a methodology to integrate the baseline model for targeted hate speech detection into the online streaming system for practical application in preventing hateful and offensive content on social media."],"url":"http://arxiv.org/abs/2404.19252v1","category":"cs.CL"}
{"created":"2024-04-30 04:16:44","title":"Quantum control in the presence of strongly coupled non-Markovian noise","abstract":"Controlling quantum systems under correlated non-Markovian noise, particularly when strongly coupled, poses significant challenges in the development of quantum technologies. Traditional quantum control strategies, heavily reliant on precise models, often fail under these conditions. Here, we address the problem by utilizing a data-driven graybox model, which integrates machine learning structures with physics-based elements. We demonstrate single-qubit control, implementing a universal gate set as well as a random gate set, achieving high fidelity under unknown, strongly-coupled non-Markovian non-Gaussian noise, significantly outperforming traditional methods. Our method is applicable to all open finite-dimensional quantum systems, regardless of the type of noise or the strength of the coupling.","sentences":["Controlling quantum systems under correlated non-Markovian noise, particularly when strongly coupled, poses significant challenges in the development of quantum technologies.","Traditional quantum control strategies, heavily reliant on precise models, often fail under these conditions.","Here, we address the problem by utilizing a data-driven graybox model, which integrates machine learning structures with physics-based elements.","We demonstrate single-qubit control, implementing a universal gate set as well as a random gate set, achieving high fidelity under unknown, strongly-coupled non-Markovian non-Gaussian noise, significantly outperforming traditional methods.","Our method is applicable to all open finite-dimensional quantum systems, regardless of the type of noise or the strength of the coupling."],"url":"http://arxiv.org/abs/2404.19251v1","category":"quant-ph"}
{"created":"2024-04-30 04:03:31","title":"Logistic Map Pseudo Random Number Generator in FPGA","abstract":"This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.","sentences":["This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution.","The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver.","These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output.","This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design."],"url":"http://arxiv.org/abs/2404.19246v1","category":"cs.CR"}
{"created":"2024-04-30 03:58:19","title":"A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems","abstract":"Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li's distortion model and traditional Brown's distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.","sentences":["Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems.","Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated.","In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process.","In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations.","The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern.","The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li's distortion model and traditional Brown's distortion model.","Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction.","The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method."],"url":"http://arxiv.org/abs/2404.19242v1","category":"cs.CV"}
{"created":"2024-04-30 03:55:29","title":"Surface energy and elementary excitations of the XYZ spin chain with integrable open boundary fields","abstract":"We study the thermodynamic limit of the anisotropic XYZ spin chain with non-diagonal integrable open boundary conditions. Although the $U(1)$-symmetry is broken, by using the new parametrization scheme, we exactly obtain the surface energy and the excitation energy of the system, which has solved the difficulty in the inhomogeneous $T-Q$ relation. With the boundary parameters in the regions making the Hamiltonian Hermitian, we have obtained the distribution patterns of the zero roots of the eigenvalue of the transfer matrix for the ground state and the excited ones. We find that the surface and excitation energies depend on the parities of sites number $N$, due to the long-range Neel order in the bulk. The spontaneous magnetization and easy-axis for all the regions of boundary parameters are studied. We also obtain the physical quantities in the thermodynamic limit of boundary XXZ model by taking the triangular limit.","sentences":["We study the thermodynamic limit of the anisotropic XYZ spin chain with non-diagonal integrable open boundary conditions.","Although the $U(1)$-symmetry is broken, by using the new parametrization scheme, we exactly obtain the surface energy and the excitation energy of the system, which has solved the difficulty in the inhomogeneous $T-Q$ relation.","With the boundary parameters in the regions making the Hamiltonian Hermitian, we have obtained the distribution patterns of the zero roots of the eigenvalue of the transfer matrix for the ground state and the excited ones.","We find that the surface and excitation energies depend on the parities of sites number $N$, due to the long-range Neel order in the bulk.","The spontaneous magnetization and easy-axis for all the regions of boundary parameters are studied.","We also obtain the physical quantities in the thermodynamic limit of boundary XXZ model by taking the triangular limit."],"url":"http://arxiv.org/abs/2404.19240v1","category":"math-ph"}
{"created":"2024-04-30 03:52:00","title":"Pilot Contamination in Massive MIMO Systems: Challenges and Future Prospects","abstract":"Massive multiple input multiple output (M-MIMO) technology plays a pivotal role in fifth-generation (5G) and beyond communication systems, offering a wide range of benefits, from increased spectral efficiency (SE) to enhanced energy efficiency and higher reliability. However, these advantages are contingent upon precise channel state information (CSI) availability at the base station (BS). Ensuring precise CSI is challenging due to the constrained size of the coherence interval and the resulting limitations on pilot sequence length. Therefore, reusing pilot sequences in adjacent cells introduces pilot contamination, hindering SE enhancement. This paper reviews recent advancements and addresses research challenges in mitigating pilot contamination and improving channel estimation, categorizing the existing research into three broader categories: pilot assignment schemes, advanced signal processing methods, and advanced channel estimation techniques. Salient representative pilot mitigation/assignment techniques are analyzed and compared in each category. Lastly, possible future research directions are discussed.","sentences":["Massive multiple input multiple output (M-MIMO) technology plays a pivotal role in fifth-generation (5G) and beyond communication systems, offering a wide range of benefits, from increased spectral efficiency (SE) to enhanced energy efficiency and higher reliability.","However, these advantages are contingent upon precise channel state information (CSI) availability at the base station (BS).","Ensuring precise CSI is challenging due to the constrained size of the coherence interval and the resulting limitations on pilot sequence length.","Therefore, reusing pilot sequences in adjacent cells introduces pilot contamination, hindering SE enhancement.","This paper reviews recent advancements and addresses research challenges in mitigating pilot contamination and improving channel estimation, categorizing the existing research into three broader categories: pilot assignment schemes, advanced signal processing methods, and advanced channel estimation techniques.","Salient representative pilot mitigation/assignment techniques are analyzed and compared in each category.","Lastly, possible future research directions are discussed."],"url":"http://arxiv.org/abs/2404.19238v1","category":"cs.IT"}
{"created":"2024-04-30 03:32:37","title":"On the Effect of Bounded Rationality in Electricity Markets","abstract":"Nash equilibrium is a common solution concept that captures the strategic interaction in electricity market analysis. However, it requires a fundamental but impractical assumption that all market participants are fully rational, which implies unlimited computational resources and cognitive abilities. To tackle the limitation, level-k reasoning is proposed and studied to model the bounded rational behaviors. In this paper, we consider a Cournot competition in electricity markets with two suppliers both following level-k reasoning. One is a self-interested firm and the other serves as a benevolent social planner. First, we observe that the optimal strategy of the social planner is to be of a particular rationality level. Being less or more rational may both result in reduced social welfare. Then, we investigate the effect of bounded rationality on social welfare performance and find that it could largely deviate from that at the Nash equilibrium point. Finally, we characterize optimal, mean maximizing and max-min strategies for the benevolent social planner, when having access to different information. The numerical experiments further demonstrate and validate our findings.","sentences":["Nash equilibrium is a common solution concept that captures the strategic interaction in electricity market analysis.","However, it requires a fundamental but impractical assumption that all market participants are fully rational, which implies unlimited computational resources and cognitive abilities.","To tackle the limitation, level-k reasoning is proposed and studied to model the bounded rational behaviors.","In this paper, we consider a Cournot competition in electricity markets with two suppliers both following level-k reasoning.","One is a self-interested firm and the other serves as a benevolent social planner.","First, we observe that the optimal strategy of the social planner is to be of a particular rationality level.","Being less or more rational may both result in reduced social welfare.","Then, we investigate the effect of bounded rationality on social welfare performance and find that it could largely deviate from that at the Nash equilibrium point.","Finally, we characterize optimal, mean maximizing and max-min strategies for the benevolent social planner, when having access to different information.","The numerical experiments further demonstrate and validate our findings."],"url":"http://arxiv.org/abs/2404.19236v1","category":"cs.GT"}
{"created":"2024-04-30 03:22:56","title":"Evolution of static to dynamic mechanical behavior in topological nonreciprocal robotic metamaterials","abstract":"Based on the Maxwell-Beatty reciprocity theorem, static non-reciprocity has been realized by using nonlinearity, but this non-reciprocity has strict restrictions on input amplitude and structure size(number of units). Here, we design a robotic metamaterial with two components of displacement and rotation, which uses active control to add external forces on the units to break reciprocity at the level of the interactions between the units. We show analytically and simulatively that breaking reciprocity at the level of the interactions directly leads to a strong asymmetric response of displacement in a static system, this displacement-specific characteristic not only has no restrictions on size, input amplitude, and suitable geometric asymmetry, but also can be transferred to rotation by coupling under large deformation. After the evolution from statics to dynamics, asymmetric transmission and unidirectional amplification of vector solitons are both implemented in this system. Our research uncovers the evolution of static non-reciprocity to dynamic non-reciprocity while building a bridge between non-reciprocity physics and soliton science.","sentences":["Based on the Maxwell-Beatty reciprocity theorem, static non-reciprocity has been realized by using nonlinearity, but this non-reciprocity has strict restrictions on input amplitude and structure size(number of units).","Here, we design a robotic metamaterial with two components of displacement and rotation, which uses active control to add external forces on the units to break reciprocity at the level of the interactions between the units.","We show analytically and simulatively that breaking reciprocity at the level of the interactions directly leads to a strong asymmetric response of displacement in a static system, this displacement-specific characteristic not only has no restrictions on size, input amplitude, and suitable geometric asymmetry, but also can be transferred to rotation by coupling under large deformation.","After the evolution from statics to dynamics, asymmetric transmission and unidirectional amplification of vector solitons are both implemented in this system.","Our research uncovers the evolution of static non-reciprocity to dynamic non-reciprocity while building a bridge between non-reciprocity physics and soliton science."],"url":"http://arxiv.org/abs/2404.19231v1","category":"cond-mat.soft"}
{"created":"2024-04-30 03:15:47","title":"On the Hodge Structures of Global Smoothings of Normal Crossing Varieties","abstract":"Let $f:X \\rightarrow \\Delta $ be a one-parameter semistable degeneration of $m$-dimensional compact complex manifolds. Assume that each component of the central fiber $X_0$ is K\\\"ahler. Then, we provide a criterion for a general fiber to satisfy the $\\partial\\overline{\\partial}$-lemma and a formula to compute the Hodge index on the middle cohomology of the general fiber in terms of the topological conditions/invariants on the central fiber.   We apply our theorem to several examples, including the global smoothing of $m$-fold ODPs, Hashimoto-Sano's non-K\\\"ahler Calabi-Yau threefolds, and Sano's non-K\\\"ahler Calabi-Yau $m$-folds.   To deal with the last example, we also prove a Lefschetz-type theorem for the cohomology of the fiber product of two Lefschetz fibrations over $\\mathbb{P}^1$ with disjoint critical locus.","sentences":["Let $f:X \\rightarrow \\Delta $ be a one-parameter semistable degeneration of $m$-dimensional compact complex manifolds.","Assume that each component of the central fiber $X_0$ is K\\\"ahler.","Then, we provide a criterion for a general fiber to satisfy the $\\partial\\overline{\\partial}$-lemma and a formula to compute the Hodge index on the middle cohomology of the general fiber in terms of the topological conditions/invariants on the central fiber.   ","We apply our theorem to several examples, including the global smoothing of $m$-fold ODPs, Hashimoto-Sano's non-K\\\"ahler Calabi-Yau threefolds, and Sano's non-K\\\"ahler Calabi-Yau $m$-folds.   ","To deal with the last example, we also prove a Lefschetz-type theorem for the cohomology of the fiber product of two Lefschetz fibrations over $\\mathbb{P}^1$ with disjoint critical locus."],"url":"http://arxiv.org/abs/2404.19229v1","category":"math.AG"}
{"created":"2024-04-30 03:07:11","title":"A Survey of Deep Learning Based Software Refactoring","abstract":"Refactoring is one of the most important activities in software engineering which is used to improve the quality of a software system. With the advancement of deep learning techniques, researchers are attempting to apply deep learning techniques to software refactoring. Consequently, dozens of deep learning-based refactoring approaches have been proposed. However, there is a lack of comprehensive reviews on such works as well as a taxonomy for deep learning-based refactoring. To this end, in this paper, we present a survey on deep learning-based software refactoring. We classify related works into five categories according to the major tasks they cover. Among these categories, we further present key aspects (i.e., code smell types, refactoring types, training strategies, and evaluation) to give insight into the details of the technologies that have supported refactoring through deep learning. The classification indicates that there is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep learning techniques have been used for the detection of code smells and the recommendation of refactoring solutions as found in 56.25\\% and 33.33\\% of the literature respectively. In contrast, only 6.25\\% and 4.17\\% were towards the end-to-end code transformation as refactoring and the mining of refactorings, respectively. Notably, we found no literature representation for the quality assurance for refactoring. We also observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method level whereas classes and variables attracted minimal attention. Finally, we discuss the challenges and limitations associated with the employment of deep learning-based refactorings and present some potential research opportunities for future work.","sentences":["Refactoring is one of the most important activities in software engineering which is used to improve the quality of a software system.","With the advancement of deep learning techniques, researchers are attempting to apply deep learning techniques to software refactoring.","Consequently, dozens of deep learning-based refactoring approaches have been proposed.","However, there is a lack of comprehensive reviews on such works as well as a taxonomy for deep learning-based refactoring.","To this end, in this paper, we present a survey on deep learning-based software refactoring.","We classify related works into five categories according to the major tasks they cover.","Among these categories, we further present key aspects (i.e., code smell types, refactoring types, training strategies, and evaluation) to give insight into the details of the technologies that have supported refactoring through deep learning.","The classification indicates that there is an imbalance in the adoption of deep learning techniques for the process of refactoring.","Most of the deep learning techniques have been used for the detection of code smells and the recommendation of refactoring solutions as found in 56.25\\% and 33.33\\% of the literature respectively.","In contrast, only 6.25\\% and 4.17\\% were towards the end-to-end code transformation as refactoring and the mining of refactorings, respectively.","Notably, we found no literature representation for the quality assurance for refactoring.","We also observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method level whereas classes and variables attracted minimal attention.","Finally, we discuss the challenges and limitations associated with the employment of deep learning-based refactorings and present some potential research opportunities for future work."],"url":"http://arxiv.org/abs/2404.19226v1","category":"cs.SE"}
{"created":"2024-04-30 02:51:29","title":"Temporal Logic Resilience for Dynamical Systems","abstract":"We consider the notion of resilience for cyber-physical systems, that is, the ability of the system to withstand adverse events while maintaining acceptable functionality. We use finite temporal logic to express the requirements on the acceptable functionality and define the resilience metric as the maximum disturbance under which the system satisfies the temporal requirements. We fix a parameterized template for the set of disturbances and form a robust optimization problem under the system dynamics and the temporal specifications to find the maximum value of the parameter. Additionally, we introduce two novel classes of specifications: closed and convex finite temporal logics specifications, offering a comprehensive analysis of the resilience metric within these specific frameworks. From a computational standpoint, we present an exact solution for linear systems and exact-time reachability and finite-horizon safety, complemented by an approximate solution for finite-horizon reachability. Extending our findings to nonlinear systems, we leverage linear approximations and SMT-based approaches to offer viable computational methodologies. The theoretical results are demonstrated on the temperature regulation of buildings, adaptive cruise control and DC motors.","sentences":["We consider the notion of resilience for cyber-physical systems, that is, the ability of the system to withstand adverse events while maintaining acceptable functionality.","We use finite temporal logic to express the requirements on the acceptable functionality and define the resilience metric as the maximum disturbance under which the system satisfies the temporal requirements.","We fix a parameterized template for the set of disturbances and form a robust optimization problem under the system dynamics and the temporal specifications to find the maximum value of the parameter.","Additionally, we introduce two novel classes of specifications: closed and convex finite temporal logics specifications, offering a comprehensive analysis of the resilience metric within these specific frameworks.","From a computational standpoint, we present an exact solution for linear systems and exact-time reachability and finite-horizon safety, complemented by an approximate solution for finite-horizon reachability.","Extending our findings to nonlinear systems, we leverage linear approximations and SMT-based approaches to offer viable computational methodologies.","The theoretical results are demonstrated on the temperature regulation of buildings, adaptive cruise control and DC motors."],"url":"http://arxiv.org/abs/2404.19223v1","category":"eess.SY"}
{"created":"2024-04-30 02:50:09","title":"Cycles of Well-Linked Sets and an Elementary Bound for the Directed Grid Theorem","abstract":"In 2015, Kawarabayashi and Kreutzer proved the directed grid theorem confirming a conjecture by Reed, Johnson, Robertson, Seymour, and Thomas from the mid-nineties. The theorem states the existence of a function $f$ such that every digraph of directed tree-width $f(k)$ contains a cylindrical grid of order $k$ as a butterfly minor, but the given function grows non-elementarily with the size of the grid minor.   In this paper we present an alternative proof of the directed grid theorem which is conceptually much simpler, more modular in its composition and also improves the upper bound for the function $f$ to a power tower of height 22.   Our proof is inspired by the breakthrough result of Chekuri and Chuzhoy, who proved a polynomial bound for the excluded grid theorem for undirected graphs. We translate a key concept of their proof to directed graphs by introducing \\emph{cycles of well-linked sets (CWS)}, and show that any digraph of high directed tree-width contains a large CWS, which in turn contains a large cylindrical grid, improving the result due to Kawarabayashi and Kreutzer from an non-elementary to an elementary function.   An immediate application of our result is an improvement of the bound for Younger's conjecture proved by Reed, Robertson, Seymour and Thomas (1996) from a non-elementary to an elementary function. The same improvement applies to other types of Erd\\H{o}s-P\\'osa style problems on directed graphs. To the best of our knowledge this is the first significant improvement on the bound for Younger's conjecture since it was proved in 1996.   We believe that the theoretical tools we developed may find applications beyond the directed grid theorem, in a similar way as the path-of-sets-system framework due to Chekuri and Chuzhoy (2016) did (see for example Hatzel, Komosa, Pilipczuk and Sorge (2022); Chekuri and Chuzhoy (2015); Chuzhoy and Nimavat (2019)).","sentences":["In 2015, Kawarabayashi and Kreutzer proved the directed grid theorem confirming a conjecture by Reed, Johnson, Robertson, Seymour, and Thomas from the mid-nineties.","The theorem states the existence of a function $f$ such that every digraph of directed tree-width $f(k)$ contains a cylindrical grid of order $k$ as a butterfly minor, but the given function grows non-elementarily with the size of the grid minor.   ","In this paper we present an alternative proof of the directed grid theorem which is conceptually much simpler, more modular in its composition and also improves the upper bound for the function $f$ to a power tower of height 22.   ","Our proof is inspired by the breakthrough result of Chekuri and Chuzhoy, who proved a polynomial bound for the excluded grid theorem for undirected graphs.","We translate a key concept of their proof to directed graphs by introducing \\emph{cycles of well-linked sets (CWS)}, and show that any digraph of high directed tree-width contains a large CWS, which in turn contains a large cylindrical grid, improving the result due to Kawarabayashi and Kreutzer from an non-elementary to an elementary function.   ","An immediate application of our result is an improvement of the bound for Younger's conjecture proved by Reed, Robertson, Seymour and Thomas (1996) from a non-elementary to an elementary function.","The same improvement applies to other types of Erd\\H{o}s-P\\'osa style problems on directed graphs.","To the best of our knowledge this is the first significant improvement on the bound for Younger's conjecture since it was proved in 1996.   ","We believe that the theoretical tools we developed may find applications beyond the directed grid theorem, in a similar way as the path-of-sets-system framework due to Chekuri and Chuzhoy (2016) did (see for example Hatzel, Komosa, Pilipczuk and Sorge (2022);","Chekuri and Chuzhoy (2015); Chuzhoy and Nimavat (2019))."],"url":"http://arxiv.org/abs/2404.19222v1","category":"cs.DM"}
{"created":"2024-04-30 02:22:52","title":"Triply efficient shadow tomography","abstract":"Given copies of a quantum state $\\rho$, a shadow tomography protocol aims to learn all expectation values from a fixed set of observables, to within a given precision $\\epsilon$. We say that a shadow tomography protocol is triply efficient if it is sample- and time-efficient, and only employs measurements that entangle a constant number of copies of $\\rho$ at a time. The classical shadows protocol based on random single-copy measurements is triply efficient for the set of local Pauli observables. This and other protocols based on random single-copy Clifford measurements can be understood as arising from fractional colorings of a graph $G$ that encodes the commutation structure of the set of observables. Here we describe a framework for two-copy shadow tomography that uses an initial round of Bell measurements to reduce to a fractional coloring problem in an induced subgraph of $G$ with bounded clique number. This coloring problem can be addressed using techniques from graph theory known as chi-boundedness. Using this framework we give the first triply efficient shadow tomography scheme for the set of local fermionic observables, which arise in a broad class of interacting fermionic systems in physics and chemistry. We also give a triply efficient scheme for the set of all $n$-qubit Pauli observables. Our protocols for these tasks use two-copy measurements, which is necessary: sample-efficient schemes are provably impossible using only single-copy measurements. Finally, we give a shadow tomography protocol that compresses an $n$-qubit quantum state into a $\\text{poly}(n)$-sized classical representation, from which one can extract the expected value of any of the $4^n$ Pauli observables in $\\text{poly}(n)$ time, up to a small constant error.","sentences":["Given copies of a quantum state $\\rho$, a shadow tomography protocol aims to learn all expectation values from a fixed set of observables, to within a given precision $\\epsilon$. We say that a shadow tomography protocol is triply efficient if it is sample- and time-efficient, and only employs measurements that entangle a constant number of copies of $\\rho$ at a time.","The classical shadows protocol based on random single-copy measurements is triply efficient for the set of local Pauli observables.","This and other protocols based on random single-copy Clifford measurements can be understood as arising from fractional colorings of a graph $G$ that encodes the commutation structure of the set of observables.","Here we describe a framework for two-copy shadow tomography that uses an initial round of Bell measurements to reduce to a fractional coloring problem in an induced subgraph of $G$ with bounded clique number.","This coloring problem can be addressed using techniques from graph theory known as chi-boundedness.","Using this framework we give the first triply efficient shadow tomography scheme for the set of local fermionic observables, which arise in a broad class of interacting fermionic systems in physics and chemistry.","We also give a triply efficient scheme for the set of all $n$-qubit Pauli observables.","Our protocols for these tasks use two-copy measurements, which is necessary: sample-efficient schemes are provably impossible using only single-copy measurements.","Finally, we give a shadow tomography protocol that compresses an $n$-qubit quantum state into a $\\text{poly}(n)$-sized classical representation, from which one can extract the expected value of any of the $4^n$ Pauli observables in $\\text{poly}(n)$ time, up to a small constant error."],"url":"http://arxiv.org/abs/2404.19211v1","category":"quant-ph"}
{"created":"2024-04-30 17:38:15","title":"Almost Envy-Freeness under Weakly Lexicographic Preferences","abstract":"In fair division of indivisible items, domain restriction has played a key role in escaping from negative results and providing structural insights into the computational and axiomatic boundaries of fairness. One notable subdomain of additive preferences, the lexicographic domain, has yielded several positive results in dealing with goods, chores, and mixtures thereof. However, the majority of work within this domain primarily consider strict linear orders over items, which do not allow the modeling of more expressive preferences that contain indifferences (ties). We investigate the most prominent fairness notions of envy-freeness up to any (EFX) or some (EF1) item under weakly lexicographic preferences. For the goods-only setting, we develop an algorithm that can be customized to guarantee EF1, EFX, maximin share (MMS), or a combination thereof, along the efficiency notion of Pareto optimality (PO). From the conceptual perspective, we propose techniques such as preference graphs and potential envy that are independently of interest when dealing with ties. Finally, we demonstrate challenges in dealing with chores and highlight key algorithmic and axiomatic differences of finding EFX solutions with the goods-only setting. Nevertheless, we show that there is an algorithm that always returns an EF1 and PO allocation for the chores-only instances.","sentences":["In fair division of indivisible items, domain restriction has played a key role in escaping from negative results and providing structural insights into the computational and axiomatic boundaries of fairness.","One notable subdomain of additive preferences, the lexicographic domain, has yielded several positive results in dealing with goods, chores, and mixtures thereof.","However, the majority of work within this domain primarily consider strict linear orders over items, which do not allow the modeling of more expressive preferences that contain indifferences (ties).","We investigate the most prominent fairness notions of envy-freeness up to any (EFX) or some (EF1) item under weakly lexicographic preferences.","For the goods-only setting, we develop an algorithm that can be customized to guarantee EF1, EFX, maximin share (MMS), or a combination thereof, along the efficiency notion of Pareto optimality (PO).","From the conceptual perspective, we propose techniques such as preference graphs and potential envy that are independently of interest when dealing with ties.","Finally, we demonstrate challenges in dealing with chores and highlight key algorithmic and axiomatic differences of finding EFX solutions with the goods-only setting.","Nevertheless, we show that there is an algorithm that always returns an EF1 and PO allocation for the chores-only instances."],"url":"http://arxiv.org/abs/2404.19740v1","category":"cs.GT"}
{"created":"2024-04-30 17:27:21","title":"An evaluation of the BALROG and RoboBA algorithms for determining the position of Fermi/GBM GRBs","abstract":"The Fermi/GBM instrument is a vital source of detections of gamma-ray bursts and has an increasingly important role to play in understanding gravitational-wave transients. In both cases, its impact is increased by accurate positions with reliable uncertainties. We evaluate the RoboBA and BALROG algorithms for determining the position of gamma-ray bursts detected by the Fermi/GBM instrument. We construct a sample of 54 bursts with detections both by Swift/BAT and by Fermi/GBM. We then compare the positions predicted by RoboBA and BALROG with the positions measured by BAT, which we can assume to be the true position. We find that RoboBA and BALROG are similarly precise for bright bursts whose uncertainties are dominated by systematic errors, but RoboBA performs better for faint bursts whose uncertainties are dominated by statistical noise. We further find that the uncertainties in the positions predicted by RoboBA are consistent with the distribution of position errors, whereas BALROG seems to be underestimating the uncertainties by a factor of about two. Additionally, we consider the implications of these results for the follow-up of the optical afterglows of Fermi/GBM bursts. In particular, for the DDOTI wide-field imager we conclude that a single pointing is best. Our sample would allow a similar study to be carried out for other telescopes.","sentences":["The Fermi/GBM instrument is a vital source of detections of gamma-ray bursts and has an increasingly important role to play in understanding gravitational-wave transients.","In both cases, its impact is increased by accurate positions with reliable uncertainties.","We evaluate the RoboBA and BALROG algorithms for determining the position of gamma-ray bursts detected by the Fermi/GBM instrument.","We construct a sample of 54 bursts with detections both by Swift/BAT and by Fermi/GBM.","We then compare the positions predicted by RoboBA and BALROG with the positions measured by BAT, which we can assume to be the true position.","We find that RoboBA and BALROG are similarly precise for bright bursts whose uncertainties are dominated by systematic errors, but RoboBA performs better for faint bursts whose uncertainties are dominated by statistical noise.","We further find that the uncertainties in the positions predicted by RoboBA are consistent with the distribution of position errors, whereas BALROG seems to be underestimating the uncertainties by a factor of about two.","Additionally, we consider the implications of these results for the follow-up of the optical afterglows of Fermi/GBM bursts.","In particular, for the DDOTI wide-field imager we conclude that a single pointing is best.","Our sample would allow a similar study to be carried out for other telescopes."],"url":"http://arxiv.org/abs/2404.19732v1","category":"astro-ph.HE"}
{"created":"2024-04-30 17:06:42","title":"Controlled Spalling of Single Crystal 4H-SiC Bulk Substrates","abstract":"We detail several scientific and engineering innovations which enable the controlled spalling of 10 - 50 micron thick films of single crystal 4H silicon carbide (4H-SiC) from bulk substrates. 4H-SiC's properties, including high thermal conductivity and a wide bandgap, make it an ideal candidate for high-temperature, high-voltage power electronic devices. Moreover, 4H-SiC has been shown to be an excellent host of solid-state atomic defect qubits for quantum computing and quantum networking. Because 4H-SiC single crystal substrates are expensive (due to long growth times and limited yield), techniques for removal and transfer of layers in the tens-of-microns thickness range are highly desirable for substrate reuse and heterogenous integration of separated layers. In this work we utilize novel approaches for stressor layer thickness control and spalling crack initiation to demonstrate controlled spalling of 4H-SiC, the highest fracture toughness material spalled to date. Additionally, we demonstrate substrate re-use, bonding of the spalled films to carrier substrates, and explore the spin coherence of the spalled films. In preliminary studies we are able to achieve coherent spin control of neutral divacancy ($VV^{0}$) qubit ensembles and measure a spin T2* of 0.581 $\\mu$s in such spalled films.","sentences":["We detail several scientific and engineering innovations which enable the controlled spalling of 10 - 50 micron thick films of single crystal 4H silicon carbide (4H-SiC) from bulk substrates.","4H-SiC's properties, including high thermal conductivity and a wide bandgap, make it an ideal candidate for high-temperature, high-voltage power electronic devices.","Moreover, 4H-SiC has been shown to be an excellent host of solid-state atomic defect qubits for quantum computing and quantum networking.","Because 4H-SiC single crystal substrates are expensive (due to long growth times and limited yield), techniques for removal and transfer of layers in the tens-of-microns thickness range are highly desirable for substrate reuse and heterogenous integration of separated layers.","In this work we utilize novel approaches for stressor layer thickness control and spalling crack initiation to demonstrate controlled spalling of 4H-SiC, the highest fracture toughness material spalled to date.","Additionally, we demonstrate substrate re-use, bonding of the spalled films to carrier substrates, and explore the spin coherence of the spalled films.","In preliminary studies we are able to achieve coherent spin control of neutral divacancy ($VV^{0}$) qubit ensembles and measure a spin T2* of 0.581 $\\mu$s in such spalled films."],"url":"http://arxiv.org/abs/2404.19716v1","category":"physics.app-ph"}
{"created":"2024-04-30 17:01:20","title":"A rank decomposition for the topological classification of neural representations","abstract":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces.   As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.   Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","sentences":["Neural networks can be thought of as applying a transformation to an input dataset.","The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems.","In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset.","Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces.   ","As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight.","We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change.","As the width increases, the homology groups of the input manifold become more likely to be preserved.","We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.   ","Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on."],"url":"http://arxiv.org/abs/2404.19710v1","category":"cs.LG"}
{"created":"2024-04-30 16:59:38","title":"Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models","abstract":"Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty. The introduced methods are implemented to the accompanying R package sstvars.","sentences":["Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian.","We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes.","Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy.","In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty.","The introduced methods are implemented to the accompanying R package sstvars."],"url":"http://arxiv.org/abs/2404.19707v1","category":"econ.EM"}
{"created":"2024-04-30 16:21:47","title":"Exponential localization for eigensections of the Bochner-Schr\u00f6dinger operator","abstract":"We study asymptotic spectral properties of the Bochner-Schr\\\"odinger operator $H_{p}=\\frac 1p\\Delta^{L^p\\otimes E}+V$ on high tensor powers of a Hermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a Riemannian manifold $X$ of bounded geometry under assumption that the curvature form of $L$ is non-degenerate. At an arbitrary point $x_0$ of $X$ the operator $H_p$ can be approximated by a model operator $\\mathcal H^{(x_0)}$, which is a Schr\\\"odinger operator with constant magnetic field. For large $p$, the spectrum of $H_p$ asymptotically coincides, up to order $p^{-1/4}$, with the union of the spectra of the model operators $\\mathcal H^{(x_0)}$ over $X$. We show that, if the union of the spectra of $\\mathcal H^{(x_0)}$ over the complement of a compact subset of $X$ has a gap, then the spectrum of $H_{p}$ in the gap is discrete and the corresponding eigensections decay exponentially away the compact subset.","sentences":["We study asymptotic spectral properties of the Bochner-Schr\\\"odinger operator $H_{p}=\\frac 1p\\Delta^{L^p\\otimes E}+V$ on high tensor powers of a Hermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a Riemannian manifold $X$ of bounded geometry under assumption that the curvature form of $L$ is non-degenerate.","At an arbitrary point $x_0$ of $X$ the operator $H_p$ can be approximated by a model operator $\\mathcal H^{(x_0)}$, which is a Schr\\\"odinger operator with constant magnetic field.","For large $p$, the spectrum of $H_p$ asymptotically coincides, up to order $p^{-1/4}$, with the union of the spectra of the model operators $\\mathcal H^{(x_0)}$ over $X$. We show that, if the union of the spectra of $\\mathcal H^{(x_0)}$ over the complement of a compact subset of $X$ has a gap, then the spectrum of $H_{p}$ in the gap is discrete and the corresponding eigensections decay exponentially away the compact subset."],"url":"http://arxiv.org/abs/2404.19684v1","category":"math.SP"}
{"created":"2024-04-30 16:11:39","title":"Density-wave-like gap evolution in La$_3$Ni$_2$O$_7$ under high pressure revealed by ultrafast optical spectroscopy","abstract":"We explore the quasiparticle dynamics in bilayer nickelate La$_3$Ni$_2$O$_7$ crystal using ultrafast optical pump-probe spectroscopy at high pressure up to 34.2 GPa. At ambient pressure, the temperature dependence of relaxation indicates appearance of phonon bottleneck effect due to the opening of density-wave-like gap at 151 K. By analyzing the data with RT model, we identified the energy scale of the gap to be 70 meV at ambient pressure. The relaxation bottleneck effect is suppressed gradually by the pressure and disappears around 26 GPa. At high pressure above 29.4 GPa, we discover a new density-wave like order with transition temperature of $\\sim$130 K. Our results not only provide the first experimental evidence of the density-wave like gap evolution under high pressure, but also offering insight into the underline interplay between the density wave order and superconductivity in pressured La$_3$Ni$_2$O$_7$.","sentences":["We explore the quasiparticle dynamics in bilayer nickelate La$_3$Ni$_2$O$_7$ crystal using ultrafast optical pump-probe spectroscopy at high pressure up to 34.2 GPa.","At ambient pressure, the temperature dependence of relaxation indicates appearance of phonon bottleneck effect due to the opening of density-wave-like gap at 151 K. By analyzing the data with RT model, we identified the energy scale of the gap to be 70 meV at ambient pressure.","The relaxation bottleneck effect is suppressed gradually by the pressure and disappears around 26 GPa.","At high pressure above 29.4 GPa, we discover a new density-wave like order with transition temperature of $\\sim$130 K. Our results not only provide the first experimental evidence of the density-wave like gap evolution under high pressure, but also offering insight into the underline interplay between the density wave order and superconductivity in pressured La$_3$Ni$_2$O$_7$."],"url":"http://arxiv.org/abs/2404.19678v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 15:47:29","title":"Exploring the hierarchy of quantum correlations under thermal effects in two gravitational cat states","abstract":"In this article, we investigate the hierarchy of quantum correlations between two gravitational cats states (modeled by two qubits). We use concurrence to quantify the entanglement between the two gravitational cat states. Quantum steering is employed to measure the steerabilities. We consider geometric quantum discord to quantify quantum correlations beyond entanglement. We show that the concurrence persists even when steerability is lost under thermal effects. We also show that the temperature influences the degree of quantum correlations between the two gravitational cat states. Besides, when the energy difference between the ground state and the first excited level becomes significant, the states become separable.","sentences":["In this article, we investigate the hierarchy of quantum correlations between two gravitational cats states (modeled by two qubits).","We use concurrence to quantify the entanglement between the two gravitational cat states.","Quantum steering is employed to measure the steerabilities.","We consider geometric quantum discord to quantify quantum correlations beyond entanglement.","We show that the concurrence persists even when steerability is lost under thermal effects.","We also show that the temperature influences the degree of quantum correlations between the two gravitational cat states.","Besides, when the energy difference between the ground state and the first excited level becomes significant, the states become separable."],"url":"http://arxiv.org/abs/2404.19648v1","category":"quant-ph"}
{"created":"2024-04-30 14:59:42","title":"Strong minimal model theorem and Massey products","abstract":"Kadeishvili's minimal model theorem establishes the existence of an $A_\\infty$-structure, unique up to isomorphism, on the cohomology of a dg associative algebra, which captures its homotopy type. In this note we prove the existence of minimal models that are unique up to isotopy, a stronger result obviously known to T. Kadeishvili and certainly to others, yet seemingly overlooked by mankind. We will explore how this stronger result can help in the study of Massey products. First, we show that the attempts to extract a local information from the ternary operation $\\mu_3$ of our minimal model leads directly to the rediscovery of the triple Massey product. The motto is: \"The triple Massey product is an invariant manifestation of $\\mu_3$.\" We then prove that, under reasonable assumptions, the higher Massey product $\\langle x_1,\\ldots,x_n\\rangle$ equals the set of all values $\\mu_n(x_1,\\ldots,x_n)$, where $\\mu_n$ runs over the $n$-ary products of our minimal models. We believe that this note will help to elucidate the still somewhat enigmatic relationship between minimal models and Massey products.","sentences":["Kadeishvili's minimal model theorem establishes the existence of an $A_\\infty$-structure, unique up to isomorphism, on the cohomology of a dg associative algebra, which captures its homotopy type.","In this note we prove the existence of minimal models that are unique up to isotopy, a stronger result obviously known to T. Kadeishvili and certainly to others, yet seemingly overlooked by mankind.","We will explore how this stronger result can help in the study of Massey products.","First, we show that the attempts to extract a local information from the ternary operation $\\mu_3$ of our minimal model leads directly to the rediscovery of the triple Massey product.","The motto is: \"The triple Massey product is an invariant manifestation of $\\mu_3$.\" We then prove that, under reasonable assumptions, the higher Massey product $\\langle x_1,\\ldots,x_n\\rangle$ equals the set of all values $\\mu_n(x_1,\\ldots,x_n)$, where $\\mu_n$ runs over the $n$-ary products of our minimal models.","We believe that this note will help to elucidate the still somewhat enigmatic relationship between minimal models and Massey products."],"url":"http://arxiv.org/abs/2404.19607v1","category":"math.AT"}
{"created":"2024-04-30 14:57:10","title":"Electronic structure of the surface superconducting Weyl semimetal PtBi$_2$","abstract":"Trigonal PtBi$_2$ is a layered semimetal without inversion symmetry, featuring 12 Weyl points in the vicinity of the Fermi energy. Its topological Fermi arcs were recently shown to superconduct at low temperatures where bulk superconductivity is absent. Here, we perform first-principles calculations to investigate in detail the bulk and surface electronic structure of PtBi$_2$, and obtain the spin texture as well as the momentum-dependent localization of the arcs. Motivated by the experimentally observed recovery of inversion symmetry under pressure or upon doping, we interpolate between the two structures and determine the energy and momentum dependence of the Weyl nodes. For deeper insights into the surface superconductivity of PtBi$_2$, we construct a symmetry-adapted effective four-band model that accurately reproduces the Weyl points of PtBi$_2$. We supplement this model with an analysis of the symmetry-allowed pairings between the Fermi arcs, which naturally mix spin-singlet and spin-triplet channels. Moreover, the presence of surface-only superconductivity facilitates an intrinsic superconductor-semimetal-superconductor Josephson junction, with the semimetallic phase sandwiched between the two superconducting surfaces. For a phase difference of $\\pi$, zero-energy Andreev bound states develop between the two terminations.","sentences":["Trigonal PtBi$_2$ is a layered semimetal without inversion symmetry, featuring 12 Weyl points in the vicinity of the Fermi energy.","Its topological Fermi arcs were recently shown to superconduct at low temperatures where bulk superconductivity is absent.","Here, we perform first-principles calculations to investigate in detail the bulk and surface electronic structure of PtBi$_2$, and obtain the spin texture as well as the momentum-dependent localization of the arcs.","Motivated by the experimentally observed recovery of inversion symmetry under pressure or upon doping, we interpolate between the two structures and determine the energy and momentum dependence of the Weyl nodes.","For deeper insights into the surface superconductivity of PtBi$_2$, we construct a symmetry-adapted effective four-band model that accurately reproduces the Weyl points of PtBi$_2$. We supplement this model with an analysis of the symmetry-allowed pairings between the Fermi arcs, which naturally mix spin-singlet and spin-triplet channels.","Moreover, the presence of surface-only superconductivity facilitates an intrinsic superconductor-semimetal-superconductor Josephson junction, with the semimetallic phase sandwiched between the two superconducting surfaces.","For a phase difference of $\\pi$, zero-energy Andreev bound states develop between the two terminations."],"url":"http://arxiv.org/abs/2404.19606v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 14:51:41","title":"Recovery of the X-ray polarization of Swift J1727.8$-$1613 after the soft to hard spectral transition","abstract":"We report on the detection of X-ray polarization in the black-hole X-ray binary Swift J1727.8$-$1613 during its dim hard spectral state by the Imaging X-ray Polarimetry Explorer (IXPE). This is the first detection of the X-ray polarization at the transition from the soft to the hard state in an X-ray binary. We find a 2$-$8 keV averaged polarization degree of (3.3 ${\\pm}$ 0.4) % and the corresponding polarization angle of 3{\\deg} ${\\pm}$ 4{\\deg}, which matches with the polarization detected during the rising stage of the outburst, in September$-$October 2023, within 1${\\sigma}$ uncertainty. The observational campaign complements previous studies of this source and enables comparison of the X-ray polarization properties of a single transient across the X-ray hardness-intensity diagram. The complete recovery of X-ray polarization properties, including energy dependence, follows a dramatic drop of the X-ray polarization during the soft state. The new IXPE observations in the dim hard state at the reverse transition indicate that the accretion properties, including the geometry of the corona, appear to be strikingly similar to the bright hard state during the outburst rise even though the X-ray luminosities differ by two orders of magnitude.","sentences":["We report on the detection of X-ray polarization in the black-hole X-ray binary Swift J1727.8$-$1613 during its dim hard spectral state by the Imaging X-ray Polarimetry Explorer (IXPE).","This is the first detection of the X-ray polarization at the transition from the soft to the hard state in an X-ray binary.","We find a 2$-$8 keV averaged polarization degree of (3.3 ${\\pm}$ 0.4) % and the corresponding polarization angle of 3{\\deg} ${\\pm}$ 4{\\deg}, which matches with the polarization detected during the rising stage of the outburst, in September$-$October 2023, within 1${\\sigma}$ uncertainty.","The observational campaign complements previous studies of this source and enables comparison of the X-ray polarization properties of a single transient across the X-ray hardness-intensity diagram.","The complete recovery of X-ray polarization properties, including energy dependence, follows a dramatic drop of the X-ray polarization during the soft state.","The new IXPE observations in the dim hard state at the reverse transition indicate that the accretion properties, including the geometry of the corona, appear to be strikingly similar to the bright hard state during the outburst rise even though the X-ray luminosities differ by two orders of magnitude."],"url":"http://arxiv.org/abs/2404.19601v1","category":"astro-ph.HE"}
{"created":"2024-04-30 14:43:57","title":"Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning","abstract":"The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. However, the impact of backdoor attacks on multilingual models remains under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data in one or two languages can affect the outputs in languages whose instruction-tuning data was not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5, BLOOM, and GPT-3.5-turbo, with high attack success rates, surpassing 95% in several languages across various scenarios. Alarmingly, our findings also indicate that larger models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments show that triggers can still work even after paraphrasing, and the backdoor mechanism proves highly effective in cross-lingual response settings across 25 languages, achieving an average attack success rate of 50%. Our study aims to highlight the vulnerabilities and significant security risks present in current multilingual LLMs, underscoring the emergent need for targeted security measures.","sentences":["The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs.","However, the impact of backdoor attacks on multilingual models remains under-explored.","Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data in one or two languages can affect the outputs in languages whose instruction-tuning data was not poisoned.","Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5, BLOOM, and GPT-3.5-turbo, with high attack success rates, surpassing 95% in several languages across various scenarios.","Alarmingly, our findings also indicate that larger models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma.","Moreover, our experiments show that triggers can still work even after paraphrasing, and the backdoor mechanism proves highly effective in cross-lingual response settings across 25 languages, achieving an average attack success rate of 50%.","Our study aims to highlight the vulnerabilities and significant security risks present in current multilingual LLMs, underscoring the emergent need for targeted security measures."],"url":"http://arxiv.org/abs/2404.19597v1","category":"cs.CL"}
{"created":"2024-04-30 14:19:06","title":"Leveraging Label Information for Stealthy Data Stealing in Vertical Federated Learning","abstract":"We develop DMAVFL, a novel attack strategy that evades current detection mechanisms. The key idea is to integrate a discriminator with auxiliary classifier that takes a full advantage of the label information (which was completely ignored in previous attacks): on one hand, label information helps to better characterize embeddings of samples from distinct classes, yielding an improved reconstruction performance; on the other hand, computing malicious gradients with label information better mimics the honest training, making the malicious gradients indistinguishable from the honest ones, and the attack much more stealthy. Our comprehensive experiments demonstrate that DMAVFL significantly outperforms existing attacks, and successfully circumvents SOTA defenses for malicious attacks. Additional ablation studies and evaluations on other defenses further underscore the robustness and effectiveness of DMAVFL.","sentences":["We develop DMAVFL, a novel attack strategy that evades current detection mechanisms.","The key idea is to integrate a discriminator with auxiliary classifier that takes a full advantage of the label information (which was completely ignored in previous attacks): on one hand, label information helps to better characterize embeddings of samples from distinct classes, yielding an improved reconstruction performance; on the other hand, computing malicious gradients with label information better mimics the honest training, making the malicious gradients indistinguishable from the honest ones, and the attack much more stealthy.","Our comprehensive experiments demonstrate that DMAVFL significantly outperforms existing attacks, and successfully circumvents SOTA defenses for malicious attacks.","Additional ablation studies and evaluations on other defenses further underscore the robustness and effectiveness of DMAVFL."],"url":"http://arxiv.org/abs/2404.19582v1","category":"cs.LG"}
{"created":"2024-04-30 14:07:11","title":"Self-assembling of multilayered polymorphs with ion beams","abstract":"Polymorphism contributes to the diversity of nature, so that even materials having identical chemical compositions exhibit variations in properties because of different lattice symmetries. Thus, if stacked together into multilayers, polymorphs may work as an alternative approach to the sequential deposition of layers with different chemical compositions. However, selective polymorph crystallization during conventional thin film synthesis is not trivial; e.g. opting for step-like changes of temperature and/or pressure correlated with switching from one polymorph to another during synthesis is tricky, since it may cause degradation of the structural quality. In the present work, applying the disorder-induced ordering approach we fabricated such multilayered polymorph structures using ion beams. We show that during ion irradiation of gallium oxide, the dynamic annealing of disorder may be tuned towards self-assembling of several polymorph interfaces, consistently with theoretical modelling. Specifically, we demonstrated multilayers with two polymorph interface repetitions obtained in one ion beam assisted fabrication step. Importantly, single crystal structure of the polymorphs was maintained in between interfaces exhibiting repeatable crystallographic relationships, correlating with optical cross-sectional maps. This data paves the way for enhancing functionalities in materials with not previously thought capabilities of ion beam technology.","sentences":["Polymorphism contributes to the diversity of nature, so that even materials having identical chemical compositions exhibit variations in properties because of different lattice symmetries.","Thus, if stacked together into multilayers, polymorphs may work as an alternative approach to the sequential deposition of layers with different chemical compositions.","However, selective polymorph crystallization during conventional thin film synthesis is not trivial; e.g. opting for step-like changes of temperature and/or pressure correlated with switching from one polymorph to another during synthesis is tricky, since it may cause degradation of the structural quality.","In the present work, applying the disorder-induced ordering approach we fabricated such multilayered polymorph structures using ion beams.","We show that during ion irradiation of gallium oxide, the dynamic annealing of disorder may be tuned towards self-assembling of several polymorph interfaces, consistently with theoretical modelling.","Specifically, we demonstrated multilayers with two polymorph interface repetitions obtained in one ion beam assisted fabrication step.","Importantly, single crystal structure of the polymorphs was maintained in between interfaces exhibiting repeatable crystallographic relationships, correlating with optical cross-sectional maps.","This data paves the way for enhancing functionalities in materials with not previously thought capabilities of ion beam technology."],"url":"http://arxiv.org/abs/2404.19572v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 14:05:17","title":"A note on background independence in $\\text{AdS}_3$ string theory","abstract":"In this note, we comment on the path integral formulation of string theory on $\\mathcal{M}\\times\\text{S}^3\\times\\mathbb{T}^4$ where $\\mathcal{M}$ is any hyperbolic 3-manifold. In the special case of $k=1$ NS-NS flux, we provide a covariant description of the worldsheet theory and argue that the path integral depends only on the details of the conformal boundary $\\partial\\mathcal{M}$, making the background independence of this theory manifest. We provide a simple path integral argument that the path integral localizes onto holomorphic covering maps from the worldsheet to the boundary. For closed manifolds $\\mathcal{M}$, the gravitational path integral is argued to be trivial. This implies that the bulk gravitational theory has precisely one state in its Hilbert space. Finally, we comment on the effect of continuous deformations of the worldsheet theory which introduce non-minimal string tension.","sentences":["In this note, we comment on the path integral formulation of string theory on $\\mathcal{M}\\times\\text{S}^3\\times\\mathbb{T}^4$ where $\\mathcal{M}$ is any hyperbolic 3-manifold.","In the special case of $k=1$ NS-NS flux, we provide a covariant description of the worldsheet theory and argue that the path integral depends only on the details of the conformal boundary $\\partial\\mathcal{M}$, making the background independence of this theory manifest.","We provide a simple path integral argument that the path integral localizes onto holomorphic covering maps from the worldsheet to the boundary.","For closed manifolds $\\mathcal{M}$, the gravitational path integral is argued to be trivial.","This implies that the bulk gravitational theory has precisely one state in its Hilbert space.","Finally, we comment on the effect of continuous deformations of the worldsheet theory which introduce non-minimal string tension."],"url":"http://arxiv.org/abs/2404.19571v1","category":"hep-th"}
{"created":"2024-04-30 13:42:01","title":"Computational study of numerical flux schemes for mesoscale atmospheric flows in a Finite Volume framework","abstract":"We develop, and implement in a Finite Volume environment, a density-based approach for the Euler equations written in conservative form using density, momentum, and total energy as variables. Under simplifying assumptions, these equations are used to describe non-hydrostatic atmospheric flow. The well-balancing of the approach is ensured by a local hydrostatic reconstruction updated in runtime during the simulation to keep the numerical error under control. To approximate the solution of the Riemann problem, we consider four methods: Roe-Pike, HLLC, AUSM+-up and HLLC-AUSM. We assess our density-based approach and compare the accuracy of these four approximated Riemann solvers using two two classical benchmarks, namely the smooth rising thermal bubble and the density current.","sentences":["We develop, and implement in a Finite Volume environment, a density-based approach for the Euler equations written in conservative form using density, momentum, and total energy as variables.","Under simplifying assumptions, these equations are used to describe non-hydrostatic atmospheric flow.","The well-balancing of the approach is ensured by a local hydrostatic reconstruction updated in runtime during the simulation to keep the numerical error under control.","To approximate the solution of the Riemann problem, we consider four methods: Roe-Pike, HLLC, AUSM+-up and HLLC-AUSM.","We assess our density-based approach and compare the accuracy of these four approximated Riemann solvers using two two classical benchmarks, namely the smooth rising thermal bubble and the density current."],"url":"http://arxiv.org/abs/2404.19559v1","category":"math.NA"}
{"created":"2024-04-30 13:13:08","title":"Evaluation of An Indoor Localization Engine","abstract":"Pedestrian Indoor localization based on modalities available in modern smartphones have been widely studied in literature and many of the specific challenges have been addressed. However, very few approaches consider the whole problem and proposed solutions are very often evaluated under very limited scenarios. We propose a fusion engine for localization that makes use of various data provided by a smartphone (Inertial sensors, pressure sensors, Wi-Fi, BLE, GNSS, map etc.) to provide a fused localization that is robust under harsh conditions (poor RSS coverage, device position change etc.). Moreover, our solution has been evaluated for hardware integration and tested over a large database including more than 250 experiments representing different scenarios, showing feasibility of lightweight implementation and good results over various conditions.","sentences":["Pedestrian Indoor localization based on modalities available in modern smartphones have been widely studied in literature and many of the specific challenges have been addressed.","However, very few approaches consider the whole problem and proposed solutions are very often evaluated under very limited scenarios.","We propose a fusion engine for localization that makes use of various data provided by a smartphone (Inertial sensors, pressure sensors, Wi-Fi, BLE, GNSS, map etc.) to provide a fused localization that is robust under harsh conditions (poor RSS coverage, device position change etc.).","Moreover, our solution has been evaluated for hardware integration and tested over a large database including more than 250 experiments representing different scenarios, showing feasibility of lightweight implementation and good results over various conditions."],"url":"http://arxiv.org/abs/2404.19538v1","category":"eess.SP"}
{"created":"2024-04-30 12:45:41","title":"A Smartphone-Based Method for Assessing Tomato Nutrient Status through Trichome Density Measurement","abstract":"Accurately assessing tomato plant nutrient status is crucial for maintaining high yields. Consequently, accurately identifying fertilizer-induced stress through the morphological traits of tomato plants has become a critical agricultural challenge. Research and development efforts have focused on developing noninvasive diagnostic tools for nutrition that leverage a combination of morphological traits and advanced sensor technologies. Given these advancements, detecting fertilizer stress by observing morphological traits near the growth points of tomatoes is still a significant challenge. To address this challenge, we developed a simple and cost-effective smartphone-based method for measuring trichome density. This method involves transferring trichomes from the surface of a leaf onto cellophane tape and capturing images using a smartphone. The images are processed using computer vision techniques to calculate the trichome density. To assess the efficacy of this method, we performed experiments on hydroponically grown tomato plants subjected to varying fertilizer concentrations. Our results indicate that our novel method for measuring trichome density accurately reflects fertilizer stress in tomato plants. The predictive performance of our model, as evaluated by the mean area under the precision recall curve, was 0.824, despite variations in the measurement data caused by differences in optical conditions. This study introduces an innovative approach for designing diagnostic devices for detecting fertilizer stress in plants by considering the surface structures of plants. Our proposed method represents a straightforward, efficient, and economical approach for evaluating the nutrient status of tomato plants and has the potential to overcome the limitations of conventional noncontact optical methods.","sentences":["Accurately assessing tomato plant nutrient status is crucial for maintaining high yields.","Consequently, accurately identifying fertilizer-induced stress through the morphological traits of tomato plants has become a critical agricultural challenge.","Research and development efforts have focused on developing noninvasive diagnostic tools for nutrition that leverage a combination of morphological traits and advanced sensor technologies.","Given these advancements, detecting fertilizer stress by observing morphological traits near the growth points of tomatoes is still a significant challenge.","To address this challenge, we developed a simple and cost-effective smartphone-based method for measuring trichome density.","This method involves transferring trichomes from the surface of a leaf onto cellophane tape and capturing images using a smartphone.","The images are processed using computer vision techniques to calculate the trichome density.","To assess the efficacy of this method, we performed experiments on hydroponically grown tomato plants subjected to varying fertilizer concentrations.","Our results indicate that our novel method for measuring trichome density accurately reflects fertilizer stress in tomato plants.","The predictive performance of our model, as evaluated by the mean area under the precision recall curve, was 0.824, despite variations in the measurement data caused by differences in optical conditions.","This study introduces an innovative approach for designing diagnostic devices for detecting fertilizer stress in plants by considering the surface structures of plants.","Our proposed method represents a straightforward, efficient, and economical approach for evaluating the nutrient status of tomato plants and has the potential to overcome the limitations of conventional noncontact optical methods."],"url":"http://arxiv.org/abs/2404.19513v1","category":"cs.CV"}
{"created":"2024-04-30 12:45:34","title":"Comparison of the high-order Runge-Kutta discontinuous Galerkin method and gas-kinetic scheme for inviscid compressible flow simulations","abstract":"The Runge--Kutta discontinuous Galerkin (RKDG) method is a high-order technique for addressing hyperbolic conservation laws, which has been refined over recent decades and is effective in handling shock discontinuities. Despite its advancements, the RKDG method faces challenges, such as stringent constraints on the explicit time-step size and reduced robustness when dealing with strong discontinuities. On the other hand, the Gas-Kinetic Scheme (GKS) based on a high-order gas evolution model also delivers significant accuracy and stability in solving hyperbolic conservation laws through refined spatial and temporal discretizations. Unlike RKDG, GKS allows for more flexible CFL number constraints and features an advanced flow evolution mechanism at cell interfaces. Additionally, GKS' compact spatial reconstruction enhances the accuracy of the method and its ability to capture stable strong discontinuities effectively. In this study, we conduct a thorough examination of the RKDG method using various numerical fluxes and the GKS method employing both compact and non-compact spatial reconstructions. Both methods are applied under the framework of explicit time discretization and are tested solely in inviscid scenarios. We will present numerous numerical tests and provide a comparative analysis of the outcomes derived from these two computational approaches.","sentences":["The Runge--Kutta discontinuous Galerkin (RKDG) method is a high-order technique for addressing hyperbolic conservation laws, which has been refined over recent decades and is effective in handling shock discontinuities.","Despite its advancements, the RKDG method faces challenges, such as stringent constraints on the explicit time-step size and reduced robustness when dealing with strong discontinuities.","On the other hand, the Gas-Kinetic Scheme (GKS) based on a high-order gas evolution model also delivers significant accuracy and stability in solving hyperbolic conservation laws through refined spatial and temporal discretizations.","Unlike RKDG, GKS allows for more flexible CFL number constraints and features an advanced flow evolution mechanism at cell interfaces.","Additionally, GKS' compact spatial reconstruction enhances the accuracy of the method and its ability to capture stable strong discontinuities effectively.","In this study, we conduct a thorough examination of the RKDG method using various numerical fluxes and the GKS method employing both compact and non-compact spatial reconstructions.","Both methods are applied under the framework of explicit time discretization and are tested solely in inviscid scenarios.","We will present numerous numerical tests and provide a comparative analysis of the outcomes derived from these two computational approaches."],"url":"http://arxiv.org/abs/2404.19512v1","category":"math.NA"}
{"created":"2024-04-30 12:42:05","title":"Choosing a consultant in a dynamic investment problem","abstract":"Consider a dynamic decision-making scenario where at every stage the investor has to choose between investing in one of two projects or gathering more information. At each stage, the investor may seek counsel from one of several consultants, who, for a fixed cost, provide partial information about the realized state. We explore the optimal strategy and its dependence on the belief and the consultation cost. Our analysis reveals that if one of the consultants discloses the state with a nonzero probability, this consultant will be used in any optimal strategy, provided the consultation cost is sufficiently small.","sentences":["Consider a dynamic decision-making scenario where at every stage the investor has to choose between investing in one of two projects or gathering more information.","At each stage, the investor may seek counsel from one of several consultants, who, for a fixed cost, provide partial information about the realized state.","We explore the optimal strategy and its dependence on the belief and the consultation cost.","Our analysis reveals that if one of the consultants discloses the state with a nonzero probability, this consultant will be used in any optimal strategy, provided the consultation cost is sufficiently small."],"url":"http://arxiv.org/abs/2404.19507v1","category":"cs.IT"}
{"created":"2024-04-30 12:40:05","title":"Kuroda's Translation for Higher-Order Logic","abstract":"In 1951, Kuroda defined an embedding of classical first-order logic into intuitionistic logic, such that a formula and its translation are equivalent in classical logic. Recently, Brown and Rizkallah extended this translation to higher-order logic, but did not prove the classical equivalence, and showed that the embedding fails in the presence of functional extensionality. We prove that functional extensionality and propositional extensionality are sufficient to derive the classical equivalence between a higher-order formula and its translation. We emphasize a condition under which Kuroda's translation works with functional extensionality.","sentences":["In 1951, Kuroda defined an embedding of classical first-order logic into intuitionistic logic, such that a formula and its translation are equivalent in classical logic.","Recently, Brown and Rizkallah extended this translation to higher-order logic, but did not prove the classical equivalence, and showed that the embedding fails in the presence of functional extensionality.","We prove that functional extensionality and propositional extensionality are sufficient to derive the classical equivalence between a higher-order formula and its translation.","We emphasize a condition under which Kuroda's translation works with functional extensionality."],"url":"http://arxiv.org/abs/2404.19503v1","category":"cs.LO"}
{"created":"2024-04-30 12:30:48","title":"Online and Offline Robust Multivariate Linear Regression","abstract":"We consider the robust estimation of the parameters of multivariate Gaussian linear regression models. To this aim we consider robust version of the usual (Mahalanobis) least-square criterion, with or without Ridge regularization. We introduce two methods each considered contrast: (i) online stochastic gradient descent algorithms and their averaged versions and (ii) offline fix-point algorithms. Under weak assumptions, we prove the asymptotic normality of the resulting estimates. Because the variance matrix of the noise is usually unknown, we propose to plug a robust estimate of it in the Mahalanobis-based stochastic gradient descent algorithms. We show, on synthetic data, the dramatic gain in terms of robustness of the proposed estimates as compared to the classical least-square ones. Well also show the computational efficiency of the online versions of the proposed algorithms. All the proposed algorithms are implemented in the R package RobRegression available on CRAN.","sentences":["We consider the robust estimation of the parameters of multivariate Gaussian linear regression models.","To this aim we consider robust version of the usual (Mahalanobis) least-square criterion, with or without Ridge regularization.","We introduce two methods each considered contrast: (i) online stochastic gradient descent algorithms and their averaged versions and (ii) offline fix-point algorithms.","Under weak assumptions, we prove the asymptotic normality of the resulting estimates.","Because the variance matrix of the noise is usually unknown, we propose to plug a robust estimate of it in the Mahalanobis-based stochastic gradient descent algorithms.","We show, on synthetic data, the dramatic gain in terms of robustness of the proposed estimates as compared to the classical least-square ones.","Well also show the computational efficiency of the online versions of the proposed algorithms.","All the proposed algorithms are implemented in the R package RobRegression available on CRAN."],"url":"http://arxiv.org/abs/2404.19496v1","category":"math.ST"}
{"created":"2024-04-30 11:40:07","title":"Quantum Relaxation for Solving Multiple Knapsack Problems","abstract":"Combinatorial problems are a common challenge in business, requiring finding optimal solutions under specified constraints. While significant progress has been made with variational approaches such as QAOA, most problems addressed are unconstrained (such as Max-Cut). In this study, we investigate a hybrid quantum-classical method for constrained optimization problems, particularly those with knapsack constraints that occur frequently in financial and supply chain applications. Our proposed method relies firstly on relaxations to local quantum Hamiltonians, defined through commutative maps. Drawing inspiration from quantum random access code (QRAC) concepts, particularly Quantum Random Access Optimizer (QRAO), we explore QRAO's potential in solving large constrained optimization problems. We employ classical techniques like Linear Relaxation as a presolve mechanism to handle constraints and cope further with scalability. We compare our approach with QAOA and present the final results for a real-world procurement optimization problem: a significant sized multi-knapsack-constrained problem.","sentences":["Combinatorial problems are a common challenge in business, requiring finding optimal solutions under specified constraints.","While significant progress has been made with variational approaches such as QAOA, most problems addressed are unconstrained (such as Max-Cut).","In this study, we investigate a hybrid quantum-classical method for constrained optimization problems, particularly those with knapsack constraints that occur frequently in financial and supply chain applications.","Our proposed method relies firstly on relaxations to local quantum Hamiltonians, defined through commutative maps.","Drawing inspiration from quantum random access code (QRAC) concepts, particularly Quantum Random Access Optimizer (QRAO), we explore QRAO's potential in solving large constrained optimization problems.","We employ classical techniques like Linear Relaxation as a presolve mechanism to handle constraints and cope further with scalability.","We compare our approach with QAOA and present the final results for a real-world procurement optimization problem: a significant sized multi-knapsack-constrained problem."],"url":"http://arxiv.org/abs/2404.19474v1","category":"quant-ph"}
{"created":"2024-04-30 11:37:33","title":"Multi-label Classification under Uncertainty: A Tree-based Conformal Prediction Approach","abstract":"Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously. The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing. The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure. The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \\textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate. The prediction sets are thus formed based on the testing outcomes of these two procedures. We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures. We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification.","sentences":["Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously.","The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing.","The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure.","The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \\textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate.","The prediction sets are thus formed based on the testing outcomes of these two procedures.","We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures.","We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification."],"url":"http://arxiv.org/abs/2404.19472v1","category":"stat.ME"}
{"created":"2024-04-30 11:16:37","title":"Simple loss-tolerant protocol for GHZ-state distribution in a quantum network","abstract":"Distributed quantum entanglement plays a crucial role in realizing networks that connect quantum devices. However, sharing entanglement between distant nodes by means of photons is a challenging process primary due to unavoidable losses in the linking channels. In this paper, we propose a simple loss-tolerant protocol for the Greenberger-Horne-Zeilinger state distribution. We analyze the distribution rate under feasible experimental conditions and demonstrate the advantages of rate-loss scaling with respect to direct transmission. Our protocol does not use quantum repeaters and is achievable with current quantum optics technology. The result has direct application to tasks such as conference key agreement or distributed sensing. Moreover, it reduces the requirements for implementing distributed quantum error correction codes such as the surface code.","sentences":["Distributed quantum entanglement plays a crucial role in realizing networks that connect quantum devices.","However, sharing entanglement between distant nodes by means of photons is a challenging process primary due to unavoidable losses in the linking channels.","In this paper, we propose a simple loss-tolerant protocol for the Greenberger-Horne-Zeilinger state distribution.","We analyze the distribution rate under feasible experimental conditions and demonstrate the advantages of rate-loss scaling with respect to direct transmission.","Our protocol does not use quantum repeaters and is achievable with current quantum optics technology.","The result has direct application to tasks such as conference key agreement or distributed sensing.","Moreover, it reduces the requirements for implementing distributed quantum error correction codes such as the surface code."],"url":"http://arxiv.org/abs/2404.19458v1","category":"quant-ph"}
{"created":"2024-04-30 09:47:35","title":"The strong CP problem revisited and solved by the gauge group topology","abstract":"We exploit the non-perturbative result that the $\\theta$ angle which defines the vacuum structure is not a $c$-number free parameter, as suggested by the instanton semi-classical approximation, but instead one of the points of the spectrum of the central operator $\\tilde{\\theta}$ which describes the gauge group topology. Hence, the value of such an angle should not be \\textit{a priori} fixed, but rather be determined, as any quantum operator, by the infinite volume limit of the functional integral, where the fermionic mass term uniquely fixes the phase, with $< \\tilde{\\theta} > = \\theta_M$, the mass angle. Such an equality is stable under radiative corrections performed before the infinite volume limit of the functional integral.   The mechanisms is carefully controlled in the massive Schwinger model with attention to the infrared problems, to the volume effects induced by boundary terms and with a careful discussion of the infinite volume limit of the functional integral. The extension to the QCD case relies on the choice of modified APS boundary conditions which do not break chiral symmetry, allowing for the crucial role of the fermionic mass term in uniquely determining the phase, with a solution of the strong $CP$ problem.","sentences":["We exploit the non-perturbative result that the $\\theta$ angle which defines the vacuum structure is not a $c$-number free parameter, as suggested by the instanton semi-classical approximation, but instead one of the points of the spectrum of the central operator $\\tilde{\\theta}$ which describes the gauge group topology.","Hence, the value of such an angle should not be \\textit{a priori} fixed, but rather be determined, as any quantum operator, by the infinite volume limit of the functional integral, where the fermionic mass term uniquely fixes the phase, with $< \\tilde{\\theta} > = \\theta_M$, the mass angle.","Such an equality is stable under radiative corrections performed before the infinite volume limit of the functional integral.   ","The mechanisms is carefully controlled in the massive Schwinger model with attention to the infrared problems, to the volume effects induced by boundary terms and with a careful discussion of the infinite volume limit of the functional integral.","The extension to the QCD case relies on the choice of modified APS boundary conditions which do not break chiral symmetry, allowing for the crucial role of the fermionic mass term in uniquely determining the phase, with a solution of the strong $CP$ problem."],"url":"http://arxiv.org/abs/2404.19400v1","category":"hep-th"}
{"created":"2024-04-30 09:45:43","title":"L\u00e9vy processes resurrected in the positive half-line","abstract":"A L\\'evy processes resurrected in the positive half-line is a Markov process obtained by removing successively all jumps that make it negative. A natural question, given this construction, is whether the resulting process is absorbed at 0 or not. We first describe the law of the resurrected process in terms of that of the initial L\\'evy process. Then in many important classes of L\\'evy processes, we give conditions for absorption and conditions for non absorption bearing on the characteristics of the initial L\\'evy process.","sentences":["A L\\'evy processes resurrected in the positive half-line is a Markov process obtained by removing successively all jumps that make it negative.","A natural question, given this construction, is whether the resulting process is absorbed at 0 or not.","We first describe the law of the resurrected process in terms of that of the initial L\\'evy process.","Then in many important classes of L\\'evy processes, we give conditions for absorption and conditions for non absorption bearing on the characteristics of the initial L\\'evy process."],"url":"http://arxiv.org/abs/2404.19399v1","category":"math.PR"}
{"created":"2024-04-30 09:38:00","title":"Convergence analysis of the transformed gradient projection algorithms on compact matrix manifolds","abstract":"In this paper, to address the optimization problem on a compact matrix manifold, we introduce a novel algorithmic framework called the Transformed Gradient Projection (TGP) algorithm, using the projection onto this compact matrix manifold. Compared with the existing algorithms, the key innovation in our approach lies in the utilization of a new class of search directions and various stepsizes, including the Armijo, nonmonotone Armijo, and fixed stepsizes, to guide the selection of the next iterate. Our framework offers flexibility by encompassing the classical gradient projection algorithms as special cases, and intersecting the retraction-based line-search algorithms. Notably, our focus is on the Stiefel or Grassmann manifold, revealing that many existing algorithms in the literature can be seen as specific instances within our proposed framework, and this algorithmic framework also induces several new special cases. Then, we conduct a thorough exploration of the convergence properties of these algorithms, considering various search directions and stepsizes. To achieve this, we extensively analyze the geometric properties of the projection onto compact matrix manifolds, allowing us to extend classical inequalities related to retractions from the literature. Building upon these insights, we establish the weak convergence, convergence rate, and global convergence of TGP algorithms under three distinct stepsizes. In cases where the compact matrix manifold is the Stiefel or Grassmann manifold, our convergence results either encompass or surpass those found in the literature. Finally, through a series of numerical experiments, we observe that the TGP algorithms, owing to their increased flexibility in choosing search directions, outperform classical gradient projection and retraction-based line-search algorithms in several scenarios.","sentences":["In this paper, to address the optimization problem on a compact matrix manifold, we introduce a novel algorithmic framework called the Transformed Gradient Projection (TGP) algorithm, using the projection onto this compact matrix manifold.","Compared with the existing algorithms, the key innovation in our approach lies in the utilization of a new class of search directions and various stepsizes, including the Armijo, nonmonotone Armijo, and fixed stepsizes, to guide the selection of the next iterate.","Our framework offers flexibility by encompassing the classical gradient projection algorithms as special cases, and intersecting the retraction-based line-search algorithms.","Notably, our focus is on the Stiefel or Grassmann manifold, revealing that many existing algorithms in the literature can be seen as specific instances within our proposed framework, and this algorithmic framework also induces several new special cases.","Then, we conduct a thorough exploration of the convergence properties of these algorithms, considering various search directions and stepsizes.","To achieve this, we extensively analyze the geometric properties of the projection onto compact matrix manifolds, allowing us to extend classical inequalities related to retractions from the literature.","Building upon these insights, we establish the weak convergence, convergence rate, and global convergence of TGP algorithms under three distinct stepsizes.","In cases where the compact matrix manifold is the Stiefel or Grassmann manifold, our convergence results either encompass or surpass those found in the literature.","Finally, through a series of numerical experiments, we observe that the TGP algorithms, owing to their increased flexibility in choosing search directions, outperform classical gradient projection and retraction-based line-search algorithms in several scenarios."],"url":"http://arxiv.org/abs/2404.19392v1","category":"math.OC"}
{"created":"2024-04-30 09:25:49","title":"Electronic decoupling and hole-doping of graphene nanoribbons on metal substrates by chloride intercalation","abstract":"Atomically precise graphene nanoribbons (GNRs) have a wide range of electronic properties that depend sensitively on their chemical structure. Several types of GNRs have been synthesized on metal surfaces through selective surface-catalyzed reactions. The resulting GNRs are adsorbed on the metal surface, which may lead to hybridization between the GNR orbitals and those of the substrate. This makes investigation of the intrinsic electronic properties of GNRs more difficult, and also rules out capacitive gating. Here we demonstrate the formation of a dielectric gold chloride adlayer that can intercalate underneath GNRs on the Au(111) surface. The intercalated gold chloride adlayer electronically decouples the GNRs from the metal and leads to a substantial hole doping of the GNRs. Our results introduce an easily accessible tool in the in situ characterization of GNRs grown on Au(111) that allows for exploration of their electronic properties in a heavily hole-doped regime.","sentences":["Atomically precise graphene nanoribbons (GNRs) have a wide range of electronic properties that depend sensitively on their chemical structure.","Several types of GNRs have been synthesized on metal surfaces through selective surface-catalyzed reactions.","The resulting GNRs are adsorbed on the metal surface, which may lead to hybridization between the GNR orbitals and those of the substrate.","This makes investigation of the intrinsic electronic properties of GNRs more difficult, and also rules out capacitive gating.","Here we demonstrate the formation of a dielectric gold chloride adlayer that can intercalate underneath GNRs on the Au(111) surface.","The intercalated gold chloride adlayer electronically decouples the GNRs from the metal and leads to a substantial hole doping of the GNRs.","Our results introduce an easily accessible tool in the in situ characterization of GNRs grown on Au(111) that allows for exploration of their electronic properties in a heavily hole-doped regime."],"url":"http://arxiv.org/abs/2404.19389v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 08:17:06","title":"The influence of non-Newtonian behaviors of blood on the hemodynamics past a bileaflet mechanical heart valve","abstract":"This study employs extensive three-dimensional direct numerical simulations (DNS) to investigate the influence of blood non-Newtonian behaviors on the hemodynamics around a bileaflet mechanical heart valve under both steady inflow and physiologically realistic pulsatile flow conditions. Under steady inflow conditions, the study reveals that blood rheology impacts velocity and pressure field variations, as well as the values of clinically important surface and time-averaged parameters like wall shear stress (WSS) and pressure recovery. Notably, this influence is most pronounced at low Reynolds numbers, gradually diminishing as the Reynolds number increases. For instance, surface-averaged WSS values obtained with the non-Newtonian shear-thinning power-law model exceed those obtained with the Newtonian model. At $Re = 750$, this difference reaches around 67\\%, reducing to less than 1\\% at $Re = 5000$. Correspondingly, pressure recovery downstream of the valve leaflets is lower for the shear-thinning blood than the constant viscosity one, with the difference decreasing as the Reynolds number increases. On the other hand, in pulsatile flow conditions, jets formed between the leaflets and the valve housing wall are shorter than steady inflow conditions. Additionally, surface-averaged wall shear stress and blood damage (BD) parameter values are higher (with differences more than 13\\% and 47\\%, respectively) during the peak stage of the cardiac cycle, especially for blood exhibiting non-Newtonian yield stress characteristics compared to the shear-thinning or constant viscosity characteristics. Therefore, blood non-Newtonian behaviors, including shear-thinning and yield stress behaviors, exert a considerable influence on the hemodynamics around a mechanical heart valve.","sentences":["This study employs extensive three-dimensional direct numerical simulations (DNS) to investigate the influence of blood non-Newtonian behaviors on the hemodynamics around a bileaflet mechanical heart valve under both steady inflow and physiologically realistic pulsatile flow conditions.","Under steady inflow conditions, the study reveals that blood rheology impacts velocity and pressure field variations, as well as the values of clinically important surface and time-averaged parameters like wall shear stress (WSS) and pressure recovery.","Notably, this influence is most pronounced at low Reynolds numbers, gradually diminishing as the Reynolds number increases.","For instance, surface-averaged WSS values obtained with the non-Newtonian shear-thinning power-law model exceed those obtained with the Newtonian model.","At $Re = 750$, this difference reaches around 67\\%, reducing to less than 1\\% at $Re = 5000$.","Correspondingly, pressure recovery downstream of the valve leaflets is lower for the shear-thinning blood than the constant viscosity one, with the difference decreasing as the Reynolds number increases.","On the other hand, in pulsatile flow conditions, jets formed between the leaflets and the valve housing wall are shorter than steady inflow conditions.","Additionally, surface-averaged wall shear stress and blood damage (BD) parameter values are higher (with differences more than 13\\% and 47\\%, respectively) during the peak stage of the cardiac cycle, especially for blood exhibiting non-Newtonian yield stress characteristics compared to the shear-thinning or constant viscosity characteristics.","Therefore, blood non-Newtonian behaviors, including shear-thinning and yield stress behaviors, exert a considerable influence on the hemodynamics around a mechanical heart valve."],"url":"http://arxiv.org/abs/2404.19347v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 08:01:49","title":"StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation","abstract":"Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and always exhibits large variability among different runs. Such property makes prompt tuning highly unreliable and vulnerable to poorly constructed prompts, which limits its extension to more real-world applications. To tackle this issue, we propose to treat the hard prompt and soft prompt as separate inputs to mitigate noise brought by the prompt initialization. Furthermore, we optimize soft prompts with contrastive learning for utilizing class-aware information in the training process to maintain model performance. Experimental results demonstrate that \\sysname outperforms state-of-the-art methods by 7.20% in accuracy and reduces the standard deviation by 2.02 on average. Furthermore, extensive experiments underscore its robustness and stability across 7 datasets covering various tasks.","sentences":["Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity.","However, this approach largely depends on the quality of prompt initialization, and always exhibits large variability among different runs.","Such property makes prompt tuning highly unreliable and vulnerable to poorly constructed prompts, which limits its extension to more real-world applications.","To tackle this issue, we propose to treat the hard prompt and soft prompt as separate inputs to mitigate noise brought by the prompt initialization.","Furthermore, we optimize soft prompts with contrastive learning for utilizing class-aware information in the training process to maintain model performance.","Experimental results demonstrate that \\sysname outperforms state-of-the-art methods by 7.20% in accuracy and reduces the standard deviation by 2.02 on average.","Furthermore, extensive experiments underscore its robustness and stability across 7 datasets covering various tasks."],"url":"http://arxiv.org/abs/2404.19335v1","category":"cs.CL"}
{"created":"2024-04-30 07:55:45","title":"Fusing Depthwise and Pointwise Convolutions for Efficient Inference on GPUs","abstract":"Depthwise and pointwise convolutions have fewer parameters and perform fewer operations than standard convolutions. As a result, they have become increasingly used in various compact DNNs, including convolutional neural networks (CNNs) and vision transformers (ViTs). However, they have a lower compute-to-memory-access ratio than standard convolutions, making their memory accesses often the performance bottleneck. This paper explores fusing depthwise and pointwise convolutions to overcome the memory access bottleneck. The focus is on fusing these operators on GPUs. The prior art on GPU-based fusion suffers from one or more of the following: (1) fusing either a convolution with an element-wise or multiple non-convolutional operators, (2) not explicitly optimizing for memory accesses, (3) not supporting depthwise convolutions. This paper proposes Fused Convolutional Modules (FCMs), a set of novel fused depthwise and pointwise GPU kernels. FCMs significantly reduce pointwise and depthwise convolutions memory accesses, improving execution time and energy efficiency. To evaluate the trade-offs associated with fusion and determine which convolutions are beneficial to fuse and the optimal FCM parameters, we propose FusePlanner. FusePlanner consists of cost models to estimate the memory accesses of depthwise, pointwise, and FCM kernels given GPU characteristics. Our experiments on three GPUs using representative CNNs and ViTs demonstrate that FCMs save up to 83% of the memory accesses and achieve speedups of up to 3.7x compared to cuDNN. Complete model implementations of various CNNs using our modules outperform TVMs' achieving speedups of up to 1.8x and saving up to two-thirds of the energy.","sentences":["Depthwise and pointwise convolutions have fewer parameters and perform fewer operations than standard convolutions.","As a result, they have become increasingly used in various compact DNNs, including convolutional neural networks (CNNs) and vision transformers (ViTs).","However, they have a lower compute-to-memory-access ratio than standard convolutions, making their memory accesses often the performance bottleneck.","This paper explores fusing depthwise and pointwise convolutions to overcome the memory access bottleneck.","The focus is on fusing these operators on GPUs.","The prior art on GPU-based fusion suffers from one or more of the following: (1) fusing either a convolution with an element-wise or multiple non-convolutional operators, (2) not explicitly optimizing for memory accesses, (3) not supporting depthwise convolutions.","This paper proposes Fused Convolutional Modules (FCMs), a set of novel fused depthwise and pointwise GPU kernels.","FCMs significantly reduce pointwise and depthwise convolutions memory accesses, improving execution time and energy efficiency.","To evaluate the trade-offs associated with fusion and determine which convolutions are beneficial to fuse and the optimal FCM parameters, we propose FusePlanner.","FusePlanner consists of cost models to estimate the memory accesses of depthwise, pointwise, and FCM kernels given GPU characteristics.","Our experiments on three GPUs using representative CNNs and ViTs demonstrate that FCMs save up to 83% of the memory accesses and achieve speedups of up to 3.7x compared to cuDNN.","Complete model implementations of various CNNs using our modules outperform TVMs' achieving speedups of up to 1.8x and saving up to two-thirds of the energy."],"url":"http://arxiv.org/abs/2404.19331v1","category":"cs.PF"}
{"created":"2024-04-30 07:50:29","title":"LVOS: A Benchmark for Large-scale Long-term Video Object Segmentation","abstract":"Video object segmentation (VOS) aims to distinguish and track target objects in a video. Despite the excellent performance achieved by off-the-shell VOS models, existing VOS benchmarks mainly focus on short-term videos lasting about 5 seconds, where objects remain visible most of the time. However, these benchmarks poorly represent practical applications, and the absence of long-term datasets restricts further investigation of VOS in realistic scenarios. Thus, we propose a novel benchmark named LVOS, comprising 720 videos with 296,401 frames and 407,945 high-quality annotations. Videos in LVOS last 1.14 minutes on average, approximately 5 times longer than videos in existing datasets. Each video includes various attributes, especially challenges deriving from the wild, such as long-term reappearing and cross-temporal similar objects. Compared to previous benchmarks, our LVOS better reflects VOS models' performance in real scenarios. Based on LVOS, we evaluate 20 existing VOS models under 4 different settings and conduct a comprehensive analysis. On LVOS, these models suffer a large performance drop, highlighting the challenge of achieving precise tracking and segmentation in real-world scenarios. Attribute-based analysis indicates that key factor to accuracy decline is the increased video length, emphasizing LVOS's crucial role. We hope our LVOS can advance development of VOS in real scenes. Data and code are available at https://lingyihongfd.github.io/lvos.github.io/.","sentences":["Video object segmentation (VOS) aims to distinguish and track target objects in a video.","Despite the excellent performance achieved by off-the-shell VOS models, existing VOS benchmarks mainly focus on short-term videos lasting about 5 seconds, where objects remain visible most of the time.","However, these benchmarks poorly represent practical applications, and the absence of long-term datasets restricts further investigation of VOS in realistic scenarios.","Thus, we propose a novel benchmark named LVOS, comprising 720 videos with 296,401 frames and 407,945 high-quality annotations.","Videos in LVOS last 1.14 minutes on average, approximately 5 times longer than videos in existing datasets.","Each video includes various attributes, especially challenges deriving from the wild, such as long-term reappearing and cross-temporal similar objects.","Compared to previous benchmarks, our LVOS better reflects VOS models' performance in real scenarios.","Based on LVOS, we evaluate 20 existing VOS models under 4 different settings and conduct a comprehensive analysis.","On LVOS, these models suffer a large performance drop, highlighting the challenge of achieving precise tracking and segmentation in real-world scenarios.","Attribute-based analysis indicates that key factor to accuracy decline is the increased video length, emphasizing LVOS's crucial role.","We hope our LVOS can advance development of VOS in real scenes.","Data and code are available at https://lingyihongfd.github.io/lvos.github.io/."],"url":"http://arxiv.org/abs/2404.19326v2","category":"cs.CV"}
{"created":"2024-04-30 07:48:43","title":"Correcting for confounding in longitudinal experiments: positioning non-linear mixed effects modeling as implementation of standardization using latent conditional exchangeability","abstract":"Non-linear mixed effects modeling and simulation (NLME M&S) is evaluated to be used for standardization with longitudinal data in presence of confounders. Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients. We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model. Our motivation is that in pharmacometrics NLME M&S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens. Such a comparison is a causal question sometimes referred to as causal prediction. Nonetheless, NLME M&S is rarely positioned as a method for causal prediction.   As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule. Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram. From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes. Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates.","sentences":["Non-linear mixed effects modeling and simulation (NLME M&S) is evaluated to be used for standardization with longitudinal data in presence of confounders.","Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients.","We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model.","Our motivation is that in pharmacometrics NLME M&S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens.","Such a comparison is a causal question sometimes referred to as causal prediction.","Nonetheless, NLME M&S is rarely positioned as a method for causal prediction.   ","As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule.","Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram.","From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes.","Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates."],"url":"http://arxiv.org/abs/2404.19325v1","category":"stat.ME"}
{"created":"2024-04-30 07:41:37","title":"Observation of strain-rate softening behavior in jammed granular media","abstract":"The strain-rate sensitivity of confined granular materials has been widely explored, with most findings exhibiting rate-strengthening behaviors. This study, however, reveals a distinct rate-softening behavior across a certain strain rate range based on triaxial tests on particle clusters of various materials with different surface properties, particle sizes, shapes, and stiffness. This softening effect is especially pronounced in the case of common rice particles. By examining the behavior of rice particles under different confining pressure and surface conditions, and directly measuring the frictional coefficient across various loading rates, we find that the reduction in surface frictional coefficient with the increasing strain rate predominantly contributes to this rate-softening behavior. This conclusion is validated by results from Finite Element Method (FEM) simulations. Additionally, we identify confining pressure as a critical factor regulating the normal stress between particles, and thereby enhancing frictional behavior. Rheometer tests reveal that the shear modulus exhibits a similar rate-softening trend. This study of rate-softening behavior in granular materials enhances our understanding of the mechanisms during their deformation under confining pressure. It also suggests that local inter-particle tribology significantly impacts overall granular behavior.","sentences":["The strain-rate sensitivity of confined granular materials has been widely explored, with most findings exhibiting rate-strengthening behaviors.","This study, however, reveals a distinct rate-softening behavior across a certain strain rate range based on triaxial tests on particle clusters of various materials with different surface properties, particle sizes, shapes, and stiffness.","This softening effect is especially pronounced in the case of common rice particles.","By examining the behavior of rice particles under different confining pressure and surface conditions, and directly measuring the frictional coefficient across various loading rates, we find that the reduction in surface frictional coefficient with the increasing strain rate predominantly contributes to this rate-softening behavior.","This conclusion is validated by results from Finite Element Method (FEM) simulations.","Additionally, we identify confining pressure as a critical factor regulating the normal stress between particles, and thereby enhancing frictional behavior.","Rheometer tests reveal that the shear modulus exhibits a similar rate-softening trend.","This study of rate-softening behavior in granular materials enhances our understanding of the mechanisms during their deformation under confining pressure.","It also suggests that local inter-particle tribology significantly impacts overall granular behavior."],"url":"http://arxiv.org/abs/2404.19321v1","category":"cond-mat.soft"}
{"created":"2024-04-30 07:41:03","title":"Strong enhancement of magnetic coercivity induced by uniaxial stress","abstract":"The performance of permanent magnets is intricately tied to their magnetic hysteresis loop. In this study, we investigate the heavy-fermion ferromagnet CeAgSb$_2$ through magnetization measurements under uniaxial stress. We observe a 2400 % increase in magnetic coercivity with just a modest stress of approximately 1 kbar. This effect persists even after pressure release, attributable to stress-induced defects that efficiently pin domain walls. Other magnetic properties such as ordering temperature and saturation moment exhibit only weak pressure dependencies and display full reversibility. Our findings offer a promising route for increasing coercive field strength and enhancing the energy product in ferromagnetic materials and are potentially applicable to a broad spectrum of commercial or emerging magnetic applications.","sentences":["The performance of permanent magnets is intricately tied to their magnetic hysteresis loop.","In this study, we investigate the heavy-fermion ferromagnet CeAgSb$_2$ through magnetization measurements under uniaxial stress.","We observe a 2400 % increase in magnetic coercivity with just a modest stress of approximately 1 kbar.","This effect persists even after pressure release, attributable to stress-induced defects that efficiently pin domain walls.","Other magnetic properties such as ordering temperature and saturation moment exhibit only weak pressure dependencies and display full reversibility.","Our findings offer a promising route for increasing coercive field strength and enhancing the energy product in ferromagnetic materials and are potentially applicable to a broad spectrum of commercial or emerging magnetic applications."],"url":"http://arxiv.org/abs/2404.19320v1","category":"cond-mat.str-el"}
{"created":"2024-04-30 07:40:35","title":"Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget","abstract":"Compared to standard language model (LM) pretraining (i.e., from scratch), Knowledge Distillation (KD) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model. As such, KD in LM pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch. Scaling laws of LM pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)-and under a fixed computation budget, smaller models are able be process more data than larger models. We thus hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget. To test this, we compare pretraining from scratch against several KD strategies for masked language modeling (MLM) in a fair experimental setup, with respect to amount of computation as well as pretraining data. Downstream results on GLUE, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary KD under a fixed computation budget, more sophisticated KD strategies, namely TinyBERT (Jiao et al., 2020) and MiniLM (Wang et al., 2023), outperform it by a notable margin. We further find that KD yields larger gains over pretraining from scratch when the data must be repeated under the fixed computation budget.","sentences":["Compared to standard language model (LM) pretraining (i.e., from scratch), Knowledge Distillation (KD) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model.","As such, KD in LM pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch.","Scaling laws of LM pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)-and under a fixed computation budget, smaller models are able be process more data than larger models.","We thus hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget.","To test this, we compare pretraining from scratch against several KD strategies for masked language modeling (MLM) in a fair experimental setup, with respect to amount of computation as well as pretraining data.","Downstream results on GLUE, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary KD under a fixed computation budget, more sophisticated KD strategies, namely TinyBERT (Jiao et al., 2020) and MiniLM (Wang et al., 2023), outperform it by a notable margin.","We further find that KD yields larger gains over pretraining from scratch when the data must be repeated under the fixed computation budget."],"url":"http://arxiv.org/abs/2404.19319v1","category":"cs.CL"}
{"created":"2024-04-30 07:07:35","title":"Dissociative recombination of the CH$^+$ molecular ion at low energy","abstract":"The reactive collisions of the CH$^+$ molecular ion with electrons is studied in the framework of the multichannel quantum defect theory, taking into account the contribution of the core-excited Rydberg states. In addition to the $X^1\\Sigma^+$ ground state of the ion, we also consider the contribution to the dynamics of the $a^3\\Pi$ and $A^1\\Pi$ excited states of CH$^+$. Our results - in the case of the dissociative recombination in good agreement with the storage ring measurements - rely on decisive improvements - complete account of the ionisation channels and accurate evaluation of the reaction matrix - of a previously used model.","sentences":["The reactive collisions of the CH$^+$ molecular ion with electrons is studied in the framework of the multichannel quantum defect theory, taking into account the contribution of the core-excited Rydberg states.","In addition to the $X^1\\Sigma^+$ ground state of the ion, we also consider the contribution to the dynamics of the $a^3\\Pi$ and $A^1\\Pi$ excited states of CH$^+$. Our results - in the case of the dissociative recombination in good agreement with the storage ring measurements - rely on decisive improvements - complete account of the ionisation channels and accurate evaluation of the reaction matrix - of a previously used model."],"url":"http://arxiv.org/abs/2404.19302v1","category":"astro-ph.IM"}
{"created":"2024-04-30 07:04:23","title":"Statistics and explainability: a fruitful alliance","abstract":"In this paper, we propose standard statistical tools as a solution to commonly highlighted problems in the explainability literature. Indeed, leveraging statistical estimators allows for a proper definition of explanations, enabling theoretical guarantees and the formulation of evaluation metrics to quantitatively assess the quality of explanations. This approach circumvents, among other things, the subjective human assessment currently prevalent in the literature. Moreover, we argue that uncertainty quantification is essential for providing robust and trustworthy explanations, and it can be achieved in this framework through classical statistical procedures such as the bootstrap. However, it is crucial to note that while Statistics offers valuable contributions, it is not a panacea for resolving all the challenges. Future research avenues could focus on open problems, such as defining a purpose for the explanations or establishing a statistical framework for counterfactual or adversarial scenarios.","sentences":["In this paper, we propose standard statistical tools as a solution to commonly highlighted problems in the explainability literature.","Indeed, leveraging statistical estimators allows for a proper definition of explanations, enabling theoretical guarantees and the formulation of evaluation metrics to quantitatively assess the quality of explanations.","This approach circumvents, among other things, the subjective human assessment currently prevalent in the literature.","Moreover, we argue that uncertainty quantification is essential for providing robust and trustworthy explanations, and it can be achieved in this framework through classical statistical procedures such as the bootstrap.","However, it is crucial to note that while Statistics offers valuable contributions, it is not a panacea for resolving all the challenges.","Future research avenues could focus on open problems, such as defining a purpose for the explanations or establishing a statistical framework for counterfactual or adversarial scenarios."],"url":"http://arxiv.org/abs/2404.19301v1","category":"stat.ML"}
{"created":"2024-04-30 06:39:04","title":"On Improving the Algorithm-, Model-, and Data- Efficiency of Self-Supervised Learning","abstract":"Self-supervised learning (SSL) has developed rapidly in recent years. However, most of the mainstream methods are computationally expensive and rely on two (or more) augmentations for each image to construct positive pairs. Moreover, they mainly focus on large models and large-scale datasets, which lack flexibility and feasibility in many practical applications. In this paper, we propose an efficient single-branch SSL method based on non-parametric instance discrimination, aiming to improve the algorithm, model, and data efficiency of SSL. By analyzing the gradient formula, we correct the update rule of the memory bank with improved performance. We further propose a novel self-distillation loss that minimizes the KL divergence between the probability distribution and its square root version. We show that this alleviates the infrequent updating problem in instance discrimination and greatly accelerates convergence. We systematically compare the training overhead and performance of different methods in different scales of data, and under different backbones. Experimental results show that our method outperforms various baselines with significantly less overhead, and is especially effective for limited amounts of data and small models.","sentences":["Self-supervised learning (SSL) has developed rapidly in recent years.","However, most of the mainstream methods are computationally expensive and rely on two (or more) augmentations for each image to construct positive pairs.","Moreover, they mainly focus on large models and large-scale datasets, which lack flexibility and feasibility in many practical applications.","In this paper, we propose an efficient single-branch SSL method based on non-parametric instance discrimination, aiming to improve the algorithm, model, and data efficiency of SSL.","By analyzing the gradient formula, we correct the update rule of the memory bank with improved performance.","We further propose a novel self-distillation loss that minimizes the KL divergence between the probability distribution and its square root version.","We show that this alleviates the infrequent updating problem in instance discrimination and greatly accelerates convergence.","We systematically compare the training overhead and performance of different methods in different scales of data, and under different backbones.","Experimental results show that our method outperforms various baselines with significantly less overhead, and is especially effective for limited amounts of data and small models."],"url":"http://arxiv.org/abs/2404.19289v1","category":"cs.CV"}
{"created":"2024-04-30 06:34:21","title":"Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective","abstract":"Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks. While prior research has primarily concentrated on improving the adversarial robustness of image encoders to guard against attacks on images, the exploration of text-based and multimodal attacks has largely been overlooked. In this work, we initiate the first known and comprehensive effort to study adapting vision-language models for adversarial robustness under the multimodal attack. Firstly, we introduce a multimodal attack strategy and investigate the impact of different attacks. We then propose a multimodal contrastive adversarial training loss, aligning the clean and adversarial text embeddings with the adversarial and clean visual features, to enhance the adversarial robustness of both image and text encoders of CLIP. Extensive experiments on 15 datasets across two tasks demonstrate that our method significantly improves the adversarial robustness of CLIP. Interestingly, we find that the model fine-tuned against multimodal adversarial attacks exhibits greater robustness than its counterpart fine-tuned solely against image-based attacks, even in the context of image attacks, which may open up new possibilities for enhancing the security of VLMs.","sentences":["Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks.","While prior research has primarily concentrated on improving the adversarial robustness of image encoders to guard against attacks on images, the exploration of text-based and multimodal attacks has largely been overlooked.","In this work, we initiate the first known and comprehensive effort to study adapting vision-language models for adversarial robustness under the multimodal attack.","Firstly, we introduce a multimodal attack strategy and investigate the impact of different attacks.","We then propose a multimodal contrastive adversarial training loss, aligning the clean and adversarial text embeddings with the adversarial and clean visual features, to enhance the adversarial robustness of both image and text encoders of CLIP.","Extensive experiments on 15 datasets across two tasks demonstrate that our method significantly improves the adversarial robustness of CLIP.","Interestingly, we find that the model fine-tuned against multimodal adversarial attacks exhibits greater robustness than its counterpart fine-tuned solely against image-based attacks, even in the context of image attacks, which may open up new possibilities for enhancing the security of VLMs."],"url":"http://arxiv.org/abs/2404.19287v1","category":"cs.CV"}
{"created":"2024-04-30 05:31:19","title":"Structure and dynamics of amphiphilic patchy cubes in a nanoslit under shear","abstract":"Patchy nanocubes are intriguing materials with simple shapes and space-filling and multidirectional bonding properties. Previous studies have revealed various mesoscopic structures such as colloidal crystals in the solid regime and rod-like or fractal-like aggregates in the liquid regime of the phase diagram. Recent studies have also shown that mesoscopic structural properties, such as average cluster size $\\left\\langle M \\right\\rangle$ and orientational order, in amphiphilic nanocube suspensions are associated with macroscopic viscosity changes, mainly owing to differences in cluster shape among patch arrangements. Although many studies have been conducted on the self-assembled structures of nanocubes in bulk, little is known about their self-assembly in nanoscale spaces or structural changes under shear. In this study, we investigated mixtures of one- and two-patch amphiphilic nanocubes confined in two flat parallel plates at rest and under shear using molecular dynamics simulations coupled with multiparticle collision dynamics. We considered two different patch arrangements for the two-patch particles and two different slit widths $H$ to determine the degree of confinement in constant-volume fractions in the liquid regime of the phase diagram. We revealed two unique cluster morphologies that have not been previously observed under bulk conditions. At rest, the size of the rod-like aggregates increased with decreasing $H$, whereas that of the fractal-like aggregates remained constant. Under weak shear with strong confinement, the rod-like aggregates maintained a larger $\\left\\langle M \\right\\rangle$ than the fractal-like aggregates, which were more rigid and maintained a larger $\\left\\langle M \\right\\rangle$ than the rod-like aggregates under bulk conditions.","sentences":["Patchy nanocubes are intriguing materials with simple shapes and space-filling and multidirectional bonding properties.","Previous studies have revealed various mesoscopic structures such as colloidal crystals in the solid regime and rod-like or fractal-like aggregates in the liquid regime of the phase diagram.","Recent studies have also shown that mesoscopic structural properties, such as average cluster size $\\left\\langle M \\right\\rangle$ and orientational order, in amphiphilic nanocube suspensions are associated with macroscopic viscosity changes, mainly owing to differences in cluster shape among patch arrangements.","Although many studies have been conducted on the self-assembled structures of nanocubes in bulk, little is known about their self-assembly in nanoscale spaces or structural changes under shear.","In this study, we investigated mixtures of one- and two-patch amphiphilic nanocubes confined in two flat parallel plates at rest and under shear using molecular dynamics simulations coupled with multiparticle collision dynamics.","We considered two different patch arrangements for the two-patch particles and two different slit widths $H$ to determine the degree of confinement in constant-volume fractions in the liquid regime of the phase diagram.","We revealed two unique cluster morphologies that have not been previously observed under bulk conditions.","At rest, the size of the rod-like aggregates increased with decreasing $H$, whereas that of the fractal-like aggregates remained constant.","Under weak shear with strong confinement, the rod-like aggregates maintained a larger $\\left\\langle M \\right\\rangle$ than the fractal-like aggregates, which were more rigid and maintained a larger $\\left\\langle M \\right\\rangle$ than the rod-like aggregates under bulk conditions."],"url":"http://arxiv.org/abs/2404.19270v1","category":"cond-mat.soft"}
{"created":"2024-04-30 05:05:07","title":"Length and torsion dependence of thermal conductivity in twisted graphene nanoribbons","abstract":"Research on the physical properties of materials at the nanoscale is crucial for the development of breakthrough nanotechnologies. One of the key properties to consider is the ability to conduct heat, i.e., its thermal conductivity. Graphene is a remarkable nanostructure with exceptional physical properties, including one of the highest thermal conductivities (TC) ever measured. Graphene nanoribbons (GNRs) share most fundamental properties with graphene, with the added benefit of having a controllable electronic bandgap. One method to achieve such control is by twisting the GNR, which can tailor its electronic properties, as well as change their TC. Here, we revisit the dependence of the TC of twisted GNRs (TGNRs) on the number of applied turns to the GNR by calculating more precise and mathematically well defined geometric parameters related to the TGNR shape, namely, its twist and writhe. We show that the dependence of the TC on twist is not a simple function of the number of turns initially applied to a straight GNR. In fact, we show that the TC of TGNRs requires at least two parameters to be properly described. Our conclusions are supported by atomistic molecular dynamics simulations to obtain the TC of suspended TGNRs prepared under different values of initially applied turns and different sizes of their suspended part. Among possible choices of parameter pairs, we show that TC can be appropriately described by the initial number of turns and the initial twist density of the TGNRs.","sentences":["Research on the physical properties of materials at the nanoscale is crucial for the development of breakthrough nanotechnologies.","One of the key properties to consider is the ability to conduct heat, i.e., its thermal conductivity.","Graphene is a remarkable nanostructure with exceptional physical properties, including one of the highest thermal conductivities (TC) ever measured.","Graphene nanoribbons (GNRs) share most fundamental properties with graphene, with the added benefit of having a controllable electronic bandgap.","One method to achieve such control is by twisting the GNR, which can tailor its electronic properties, as well as change their TC.","Here, we revisit the dependence of the TC of twisted GNRs (TGNRs) on the number of applied turns to the GNR by calculating more precise and mathematically well defined geometric parameters related to the TGNR shape, namely, its twist and writhe.","We show that the dependence of the TC on twist is not a simple function of the number of turns initially applied to a straight GNR.","In fact, we show that the TC of TGNRs requires at least two parameters to be properly described.","Our conclusions are supported by atomistic molecular dynamics simulations to obtain the TC of suspended TGNRs prepared under different values of initially applied turns and different sizes of their suspended part.","Among possible choices of parameter pairs, we show that TC can be appropriately described by the initial number of turns and the initial twist density of the TGNRs."],"url":"http://arxiv.org/abs/2404.19262v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 04:48:31","title":"Deconfinement to confinement by generalizing BRST symmetry on the sphere","abstract":"Recently it has been shown that the theory in the quadratic gauge on 4-sphere, $\\mathbb{S}^{4}$ consists of two phases namely, the confined and the deconfined phases. A suitable finite field dependent BRST (FFBRST) transformation interrelates two different gauge fixed theories. In this paper, we use the FFBRST technique on the curved space for the first time and elaborate a novel application of it. We propose two different formulations of this technique that transform the deconfined phase action on sphere to the confined phase action on sphere inside the quadratic gauge. Both proposed passages change the phase with BRST invariance to the phase without BRST invariance unlike usual connections where the FFBRST operation leave the BRST symmetry intact and there is a unique field theoretic essence of them, which makes them particularly important to study. Thus, the two different field redefinitions act as a new mechanism that execute phase transition between two real QCD phases on 4-sphere other than ghost condensation process.","sentences":["Recently it has been shown that the theory in the quadratic gauge on 4-sphere, $\\mathbb{S}^{4}$ consists of two phases namely, the confined and the deconfined phases.","A suitable finite field dependent BRST (FFBRST) transformation interrelates two different gauge fixed theories.","In this paper, we use the FFBRST technique on the curved space for the first time and elaborate a novel application of it.","We propose two different formulations of this technique that transform the deconfined phase action on sphere to the confined phase action on sphere inside the quadratic gauge.","Both proposed passages change the phase with BRST invariance to the phase without BRST invariance unlike usual connections where the FFBRST operation leave the BRST symmetry intact and there is a unique field theoretic essence of them, which makes them particularly important to study.","Thus, the two different field redefinitions act as a new mechanism that execute phase transition between two real QCD phases on 4-sphere other than ghost condensation process."],"url":"http://arxiv.org/abs/2404.19258v1","category":"hep-th"}
{"created":"2024-04-30 04:13:14","title":"Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair","abstract":"In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes. The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes. While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features. To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features. We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair). Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample. To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model. The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity.","sentences":["In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes.","The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes.","While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features.","To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features.","We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair).","Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample.","To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model.","The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity."],"url":"http://arxiv.org/abs/2404.19250v1","category":"cs.CV"}
{"created":"2024-04-30 03:56:21","title":"Joint Pricing and Matching for Resource Allocation Platforms via Min-cost Flow Problem","abstract":"Stochastic matching is the stochastic version of the well-known matching problem, which consists in maximizing the rewards of a matching under a set of probability distributions associated with the nodes and edges. In most stochastic matching problems, the probability distributions inherent in the nodes and edges are set a priori and are not controllable. However, many resource allocation platforms can control the probability distributions by changing prices. For example, a rideshare platform can control the distribution of the number of requesters by setting the fare to maximize the reward of a taxi-requester matching. Although several methods for optimizing price have been developed, optimizations in consideration of the matching problem are still in its infancy. In this paper, we tackle the problem of optimizing price in the consideration of the resulting bipartite graph matching, given the effect of the price on the probabilistic uncertainty in the graph. Even though our problem involves hard to evaluate objective values and is non-convex, we construct a (1-1/e)-approximation algorithm under the assumption that a convex min-cost flow problem can be solved exactly.","sentences":["Stochastic matching is the stochastic version of the well-known matching problem, which consists in maximizing the rewards of a matching under a set of probability distributions associated with the nodes and edges.","In most stochastic matching problems, the probability distributions inherent in the nodes and edges are set a priori and are not controllable.","However, many resource allocation platforms can control the probability distributions by changing prices.","For example, a rideshare platform can control the distribution of the number of requesters by setting the fare to maximize the reward of a taxi-requester matching.","Although several methods for optimizing price have been developed, optimizations in consideration of the matching problem are still in its infancy.","In this paper, we tackle the problem of optimizing price in the consideration of the resulting bipartite graph matching, given the effect of the price on the probabilistic uncertainty in the graph.","Even though our problem involves hard to evaluate objective values and is non-convex, we construct a (1-1/e)-approximation algorithm under the assumption that a convex min-cost flow problem can be solved exactly."],"url":"http://arxiv.org/abs/2404.19241v1","category":"math.OC"}
{"created":"2024-04-30 03:15:04","title":"Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information","abstract":"Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this work, we provide a theoretical understanding of the symmetric InfoNCE loss through the lens of the pointwise mutual information and show that encoders that achieve the optimal similarity in the pretraining provide a good representation for downstream classification tasks under mild assumptions. Based on our theoretical results, we also propose a new similarity metric for multimodal contrastive learning by utilizing a nonlinear kernel to enrich the capability. To verify the effectiveness of the proposed method, we demonstrate pretraining of multimodal representation models on the Conceptual Caption datasets and evaluate zero-shot classification and linear classification on common benchmark datasets.","sentences":["Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications.","The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning.","In this work, we provide a theoretical understanding of the symmetric InfoNCE loss through the lens of the pointwise mutual information and show that encoders that achieve the optimal similarity in the pretraining provide a good representation for downstream classification tasks under mild assumptions.","Based on our theoretical results, we also propose a new similarity metric for multimodal contrastive learning by utilizing a nonlinear kernel to enrich the capability.","To verify the effectiveness of the proposed method, we demonstrate pretraining of multimodal representation models on the Conceptual Caption datasets and evaluate zero-shot classification and linear classification on common benchmark datasets."],"url":"http://arxiv.org/abs/2404.19228v1","category":"cs.LG"}
{"created":"2024-04-30 02:37:29","title":"FOTS: A Fast Optical Tactile Simulator for Sim2Real Learning of Tactile-motor Robot Manipulation Skills","abstract":"Simulation is a widely used tool in robotics to reduce hardware consumption and gather large-scale data. Despite previous efforts to simulate optical tactile sensors, there remain challenges in efficiently synthesizing images and replicating marker motion under different contact loads. In this work, we propose a fast optical tactile simulator, named FOTS, for simulating optical tactile sensors. We utilize multi-layer perceptron mapping and planar shadow generation to simulate the optical response, while employing marker distribution approximation to simulate the motion of surface markers caused by the elastomer deformation. Experimental results demonstrate that FOTS outperforms other methods in terms of image generation quality and rendering speed, achieving 28.6 fps for optical simulation and 326.1 fps for marker motion simulation on a single CPU without GPU acceleration. In addition, we integrate the FOTS simulation model with physical engines like MuJoCo, and the peg-in-hole task demonstrates the effectiveness of our method in achieving zero-shot Sim2Real learning of tactile-motor robot manipulation skills. Our code is available at https://github.com/Rancho-zhao/FOTS.","sentences":["Simulation is a widely used tool in robotics to reduce hardware consumption and gather large-scale data.","Despite previous efforts to simulate optical tactile sensors, there remain challenges in efficiently synthesizing images and replicating marker motion under different contact loads.","In this work, we propose a fast optical tactile simulator, named FOTS, for simulating optical tactile sensors.","We utilize multi-layer perceptron mapping and planar shadow generation to simulate the optical response, while employing marker distribution approximation to simulate the motion of surface markers caused by the elastomer deformation.","Experimental results demonstrate that FOTS outperforms other methods in terms of image generation quality and rendering speed, achieving 28.6 fps for optical simulation and 326.1 fps for marker motion simulation on a single CPU without GPU acceleration.","In addition, we integrate the FOTS simulation model with physical engines like MuJoCo, and the peg-in-hole task demonstrates the effectiveness of our method in achieving zero-shot Sim2Real learning of tactile-motor robot manipulation skills.","Our code is available at https://github.com/Rancho-zhao/FOTS."],"url":"http://arxiv.org/abs/2404.19217v2","category":"cs.RO"}
{"created":"2024-04-30 02:32:30","title":"An Invariance Principle of 1D KPZ with Robin Boundary Conditions","abstract":"We consider a discrete one-dimensional random interface on the half-space whose height at any positive point is defined as a sum of an independent random noise and a function of the heights at its two closest neighbours. In 2022, Adhikari and Chatterjee proved for the full-space model that assuming the function is equivariant, symmetric, and at least six times differentiable in a neighbourhood of zero, as the variance of the noise variables goes to zero, the height function converges to the Cole-Hopf solution of the 1D KPZ equation under a parabolic rescaling on full space. In this paper, we obtained the same convergence result for such a random interface model with Robin boundary conditions.","sentences":["We consider a discrete one-dimensional random interface on the half-space whose height at any positive point is defined as a sum of an independent random noise and a function of the heights at its two closest neighbours.","In 2022, Adhikari and Chatterjee proved for the full-space model that assuming the function is equivariant, symmetric, and at least six times differentiable in a neighbourhood of zero, as the variance of the noise variables goes to zero, the height function converges to the Cole-Hopf solution of the 1D KPZ equation under a parabolic rescaling on full space.","In this paper, we obtained the same convergence result for such a random interface model with Robin boundary conditions."],"url":"http://arxiv.org/abs/2404.19215v1","category":"math.PR"}
{"created":"2024-04-30 02:16:15","title":"Periodic Event-Triggered Boundary Control of Neuron Growth with Actuation at Soma","abstract":"Exploring novel strategies for the regulation of axon growth, we introduce a periodic event-triggered control (PETC) to enhance the practical implementation of the associated PDE backstepping control law. Neurological injuries may impair neuronal function, but therapies like Chondroitinase ABC (ChABC) have shown promise in improving axon elongation by influencing the extracellular matrix. This matrix, composed of extracellular macromolecules and minerals, regulates tubulin protein concentration, potentially aiding in neuronal recovery. The concentration and spatial distribution of tubulin influence axon elongation dynamics. Recent research explores feedback control strategies for this model, leading to the development of an event-triggering control (CETC) approach. In this approach, the control law updates when the monitored triggering condition is met, reducing actuation resource consumption. Through the meticulous redesign of the triggering mechanism, we introduce a periodic event-triggering control (PETC), updating control inputs at specific intervals, but evaluating the event-trigger only periodically, an ideal tool for standard time-sliced actuators like ChABC. PETC is a step forward to the design of practically feasible feedback laws for the neuron growth process. The PETC strategy establishes an upper bound on event triggers between periodic examinations, ensuring convergence and preventing Zeno behavior. Through Lyapunov analysis, we demonstrate the local exponential convergence of the system with the periodic event-triggering mechanism in the $L^2$-norm sense. Numerical examples are presented to confirm the theoretical findings.","sentences":["Exploring novel strategies for the regulation of axon growth, we introduce a periodic event-triggered control (PETC) to enhance the practical implementation of the associated PDE backstepping control law.","Neurological injuries may impair neuronal function, but therapies like Chondroitinase ABC (ChABC) have shown promise in improving axon elongation by influencing the extracellular matrix.","This matrix, composed of extracellular macromolecules and minerals, regulates tubulin protein concentration, potentially aiding in neuronal recovery.","The concentration and spatial distribution of tubulin influence axon elongation dynamics.","Recent research explores feedback control strategies for this model, leading to the development of an event-triggering control (CETC) approach.","In this approach, the control law updates when the monitored triggering condition is met, reducing actuation resource consumption.","Through the meticulous redesign of the triggering mechanism, we introduce a periodic event-triggering control (PETC), updating control inputs at specific intervals, but evaluating the event-trigger only periodically, an ideal tool for standard time-sliced actuators like ChABC.","PETC is a step forward to the design of practically feasible feedback laws for the neuron growth process.","The PETC strategy establishes an upper bound on event triggers between periodic examinations, ensuring convergence and preventing Zeno behavior.","Through Lyapunov analysis, we demonstrate the local exponential convergence of the system with the periodic event-triggering mechanism in the $L^2$-norm sense.","Numerical examples are presented to confirm the theoretical findings."],"url":"http://arxiv.org/abs/2404.19206v1","category":"math.OC"}
{"created":"2024-04-30 02:00:45","title":"Thermal Performance of a Liquid-cooling Assisted Thin Wickless Vapor Chamber","abstract":"The ever-increasing need for power consumption in electronic devices, coupled with the requirement for thinner size, calls for the development of efficient heat spreading components. Vapor chambers (VCs), because of their ability to effectively spread heat over a large area by two-phase heat transfer, seem ideal for such applications. However, creating thin and efficient vapor chambers that work over a wide range of power inputs is a persisting challenge. VCs that use wicks for circulating the phase changing media, suffer from capillary restrictions, dry-out, clogging, increase in size and weight, and can often be costly. Recent developments in wick-free wettability patterned vapor chambers replace traditional wicks with laser-fabricated wickless components. An experimental setup allows for fast testing and experimental evaluation of water-charged VCs with liquid-assisted cooling. The sealed chamber can maintain vacuum for long durations, and can be used for testing of very thin wick-free VCs. This work extends our previous study by decreasing overall thickness of the wick-free VC down to 3 mm and evaluates its performance. Furthermore, the impact of wettability patterns on VC performance is investigated, by carrying out experiments both in non-patterned and patterned VCs. Experiments are first carried out on a wick-free VC with no wettability patterns and comprising of an entirely superhydrophilic evaporator coupled with a hydrophobic condenser. Thereafter, wettability patterns that aid the rapid return of water to the heated site on the evaporator and improve condensation on the condenser of the vapor chamber are implemented. The thermal characteristics show that the patterned VCs outperform the non-patterned VCs under all scenarios. The patterned VCs exhibit low thermal resistance independent of fluid charging ratio withstanding higher power inputs without thermal dry-outs.","sentences":["The ever-increasing need for power consumption in electronic devices, coupled with the requirement for thinner size, calls for the development of efficient heat spreading components.","Vapor chambers (VCs), because of their ability to effectively spread heat over a large area by two-phase heat transfer, seem ideal for such applications.","However, creating thin and efficient vapor chambers that work over a wide range of power inputs is a persisting challenge.","VCs that use wicks for circulating the phase changing media, suffer from capillary restrictions, dry-out, clogging, increase in size and weight, and can often be costly.","Recent developments in wick-free wettability patterned vapor chambers replace traditional wicks with laser-fabricated wickless components.","An experimental setup allows for fast testing and experimental evaluation of water-charged VCs with liquid-assisted cooling.","The sealed chamber can maintain vacuum for long durations, and can be used for testing of very thin wick-free VCs.","This work extends our previous study by decreasing overall thickness of the wick-free VC down to 3 mm and evaluates its performance.","Furthermore, the impact of wettability patterns on VC performance is investigated, by carrying out experiments both in non-patterned and patterned VCs.","Experiments are first carried out on a wick-free VC with no wettability patterns and comprising of an entirely superhydrophilic evaporator coupled with a hydrophobic condenser.","Thereafter, wettability patterns that aid the rapid return of water to the heated site on the evaporator and improve condensation on the condenser of the vapor chamber are implemented.","The thermal characteristics show that the patterned VCs outperform the non-patterned VCs under all scenarios.","The patterned VCs exhibit low thermal resistance independent of fluid charging ratio withstanding higher power inputs without thermal dry-outs."],"url":"http://arxiv.org/abs/2404.19203v1","category":"physics.app-ph"}
{"created":"2024-04-30 02:00:28","title":"Dihadron helicity correlation in photon-nucleus collisions","abstract":"The helicity correlation of two back-to-back hadrons is a powerful tool that makes it possible to probe the longitudinal spin transfer, $G_{1L}$, in unpolarized hadronic collisions. In this work, we investigate the helicity correlation of back-to-back dihadrons produced in photon-nucleus collisions with both space-like and quasireal photons and explore its potential in understanding the flavor dependence of spin-dependent fragmentation functions. We present helicity amplitudes of partonic scatterings with both virtual and real photons and make numerical predictions for the dihadron helicity correlations at the future Electron Ion Collider experiment and the current RHIC/LHC ultra-peripheral collision experiment. Future experimental measurements can also illuminate the fragmentation function of circularly polarized gluons.","sentences":["The helicity correlation of two back-to-back hadrons is a powerful tool that makes it possible to probe the longitudinal spin transfer, $G_{1L}$, in unpolarized hadronic collisions.","In this work, we investigate the helicity correlation of back-to-back dihadrons produced in photon-nucleus collisions with both space-like and quasireal photons and explore its potential in understanding the flavor dependence of spin-dependent fragmentation functions.","We present helicity amplitudes of partonic scatterings with both virtual and real photons and make numerical predictions for the dihadron helicity correlations at the future Electron Ion Collider experiment and the current RHIC/LHC ultra-peripheral collision experiment.","Future experimental measurements can also illuminate the fragmentation function of circularly polarized gluons."],"url":"http://arxiv.org/abs/2404.19202v1","category":"hep-ph"}
{"created":"2024-04-30 01:59:25","title":"Global Search Optics: Automatically Exploring Optimal Solutions to Compact Computational Imaging Systems","abstract":"The popularity of mobile vision creates a demand for advanced compact computational imaging systems, which call for the development of both a lightweight optical system and an effective image reconstruction model. Recently, joint design pipelines come to the research forefront, where the two significant components are simultaneously optimized via data-driven learning to realize the optimal system design. However, the effectiveness of these designs largely depends on the initial setup of the optical system, complicated by a non-convex solution space that impedes reaching a globally optimal solution. In this work, we present Global Search Optics (GSO) to automatically design compact computational imaging systems through two parts: (i) Fused Optimization Method for Automatic Optical Design (OptiFusion), which searches for diverse initial optical systems under certain design specifications; and (ii) Efficient Physic-aware Joint Optimization (EPJO), which conducts parallel joint optimization of initial optical systems and image reconstruction networks with the consideration of physical constraints, culminating in the selection of the optimal solution. Extensive experimental results on the design of three-piece (3P) sphere computational imaging systems illustrate that the GSO serves as a transformative end-to-end lens design paradigm for superior global optimal structure searching ability, which provides compact computational imaging systems with higher imaging quality compared to traditional methods. The source code will be made publicly available at https://github.com/wumengshenyou/GSO.","sentences":["The popularity of mobile vision creates a demand for advanced compact computational imaging systems, which call for the development of both a lightweight optical system and an effective image reconstruction model.","Recently, joint design pipelines come to the research forefront, where the two significant components are simultaneously optimized via data-driven learning to realize the optimal system design.","However, the effectiveness of these designs largely depends on the initial setup of the optical system, complicated by a non-convex solution space that impedes reaching a globally optimal solution.","In this work, we present Global Search Optics (GSO) to automatically design compact computational imaging systems through two parts: (i) Fused Optimization Method for Automatic Optical Design (OptiFusion), which searches for diverse initial optical systems under certain design specifications; and (ii) Efficient Physic-aware Joint Optimization (EPJO), which conducts parallel joint optimization of initial optical systems and image reconstruction networks with the consideration of physical constraints, culminating in the selection of the optimal solution.","Extensive experimental results on the design of three-piece (3P) sphere computational imaging systems illustrate that the GSO serves as a transformative end-to-end lens design paradigm for superior global optimal structure searching ability, which provides compact computational imaging systems with higher imaging quality compared to traditional methods.","The source code will be made publicly available at https://github.com/wumengshenyou/GSO."],"url":"http://arxiv.org/abs/2404.19201v1","category":"eess.IV"}
{"created":"2024-04-30 01:50:14","title":"$CP$-violating observables of four-body $B_{(s)} \\to (\u03c0\u03c0)(K\\bar{K})$ decays in perturbative QCD","abstract":"In this work, we investigate six helicity amplitudes of the four-body $B_{(s)} \\to (\\pi\\pi)(K\\bar{K})$ decays in the perturbative QCD (PQCD) approach. The $\\pi\\pi$ invariant mass spectrum is dominated by the vector resonance $\\rho(770)$ together with scalar resonance $f_0(980)$, while the vector resonance $\\phi(1020)$ and scalar resonance $f_0(980)$ are expected to contribute in the $K\\bar{K}$ invariant mass range. We extract the two-body branching ratios ${\\cal B}(B_{(s)}\\to \\rho\\phi)$ from the corresponding four-body decays $B_{(s)}\\to \\rho\\phi\\to (\\pi\\pi)(K \\bar K)$. The predicted ${\\cal B}(B^0_{s}\\to \\rho\\phi)$ agrees well with the current experimental data within errors. The longitudinal polarization fractions of the $B_{(s)}\\to \\rho\\phi$ decays are found to be as large as $90\\%$, basically consistent with the previous two-body predictions within uncertainties. In addition, the triple-product asymmetries (TPAs) of the considered decays are also presented for the first time. Since the $B_s^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ decay is induced by both tree and penguin operators, the values of the ${\\cal A}^{\\rm CP}_{\\rm dir}$ and ${\\cal A}^{1}_{\\text{T-true}}$ are calculated to be $(21.8^{+2.7}_{-3.3})\\%$ and $(-10.23^{+1.73}_{-1.56})\\%$ respectively. While for pure penguin decays $B^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ and $B^+\\to \\rho^+\\phi\\to(\\pi^+\\pi^0)(K^+K^-)$, both the direct $CP$ asymmetries and ``true\" TPAs are naturally expected to be zero in the standard model (SM). The ``fake\" TPAs requiring no weak phase difference are usually none zero for all considered decay channels. The sizable ``fake\" ${\\cal A}^{1}_{\\text{T-fake}}=(-20.92^{+6.26}_{-2.80})\\%$ of the $B^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ decay is predicted in the PQCD approach, which provides valuable information on the final-state interactions.Our predictions can be tested by the future experiments.","sentences":["In this work, we investigate six helicity amplitudes of the four-body $B_{(s)} \\to (\\pi\\pi)(K\\bar{K})$ decays in the perturbative QCD (PQCD) approach.","The $\\pi\\pi$ invariant mass spectrum is dominated by the vector resonance $\\rho(770)$ together with scalar resonance $f_0(980)$, while the vector resonance $\\phi(1020)$ and scalar resonance $f_0(980)$ are expected to contribute in the $K\\bar{K}$ invariant mass range.","We extract the two-body branching ratios ${\\cal B}(B_{(s)}\\to \\rho\\phi)$ from the corresponding four-body decays $B_{(s)}\\to \\rho\\phi\\to (\\pi\\pi)(K \\bar K)$.","The predicted ${\\cal B}(B^0_{s}\\to \\rho\\phi)$ agrees well with the current experimental data within errors.","The longitudinal polarization fractions of the $B_{(s)}\\to \\rho\\phi$ decays are found to be as large as $90\\%$, basically consistent with the previous two-body predictions within uncertainties.","In addition, the triple-product asymmetries (TPAs) of the considered decays are also presented for the first time.","Since the $B_s^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ decay is induced by both tree and penguin operators, the values of the ${\\cal A}^{\\rm CP}_{\\rm dir}$ and ${\\cal A}^{1}_{\\text{T-true}}$ are calculated to be $(21.8^{+2.7}_{-3.3})\\%$ and $(-10.23^{+1.73}_{-1.56})\\%$ respectively.","While for pure penguin decays $B^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ and $B^+\\to \\rho^+\\phi\\to(\\pi^+\\pi^0)(K^+K^-)$, both the direct $CP$ asymmetries and ``true\" TPAs are naturally expected to be zero in the standard model (SM).","The ``fake\" TPAs requiring no weak phase difference are usually none zero for all considered decay channels.","The sizable ``fake\" ${\\cal A}^{1}_{\\text{T-fake}}=(-20.92^{+6.26}_{-2.80})\\%$ of the $B^0\\to \\rho^0\\phi\\to(\\pi^+\\pi^-)(K^+K^-)$ decay is predicted in the PQCD approach, which provides valuable information on the final-state interactions.","Our predictions can be tested by the future experiments."],"url":"http://arxiv.org/abs/2404.19198v1","category":"hep-ph"}
{"created":"2024-04-30 01:49:02","title":"Does the Electron EDM Preclude Electroweak Baryogenesis ?","abstract":"Electroweak baryogenesis (EWBG) constitutes a theoretically compelling and experimentally testable mechanism for explaining the origin of the baryon asymmetry of the universe (BAU). New results for the electric dipole moment (EDM) of the electron place significant constraints on the beyond Standard Model CP-violation needed for successful EWBG. We show how new developments in EWBG quantum transport theory that include CP-violating sources first order in gradients imply more relaxed EDM constraints than implied by previous approximation formulations. Consequently, EWBG remains viable even in light of present EDM bounds. We also illustrate how these developments enable a more realistic treatment of CP-conserving interactions that can also have a decisive impact on the predicted BAU.","sentences":["Electroweak baryogenesis (EWBG) constitutes a theoretically compelling and experimentally testable mechanism for explaining the origin of the baryon asymmetry of the universe (BAU).","New results for the electric dipole moment (EDM) of the electron place significant constraints on the beyond Standard Model CP-violation needed for successful EWBG.","We show how new developments in EWBG quantum transport theory that include CP-violating sources first order in gradients imply more relaxed EDM constraints than implied by previous approximation formulations.","Consequently, EWBG remains viable even in light of present EDM bounds.","We also illustrate how these developments enable a more realistic treatment of CP-conserving interactions that can also have a decisive impact on the predicted BAU."],"url":"http://arxiv.org/abs/2404.19197v1","category":"hep-ph"}
{"created":"2024-04-30 01:35:27","title":"Assessing the safety benefits of CACC+ based coordination of connected and autonomous vehicle platoons in emergency braking scenarios","abstract":"Ensuring safety is the most important factor in connected and autonomous vehicles, especially in emergency braking situations. As such, assessing the safety benefits of one information topology over other is a necessary step towards evaluating and ensuring safety. In this paper, we compare the safety benefits of a cooperative adaptive cruise control which utilizes information from one predecessor vehicle (CACC) with the one that utilizes information from multiple predecessors (CACC+) for the maintenance of spacing under an emergency braking scenario. A constant time headway policy is employed for maintenance of spacing (that includes a desired standstill spacing distance and a velocity dependent spacing distance) between the vehicles in the platoon. The considered emergency braking scenario consists of braking of the leader vehicle of the platoon at its maximum deceleration and that of the following vehicles to maintain the spacing as per CACC or CACC+. By focusing on the standstill spacing distance and utilizing Monte Carlo simulations, we assess the safety benefits of CACC+ over CACC by utilizing the following safety metrics: (1) probability of collision, (2) expected number of collisions, and (3) severity of collision (defined as the relative velocity of the two vehicles at impact). We present and provide discussion of these results.","sentences":["Ensuring safety is the most important factor in connected and autonomous vehicles, especially in emergency braking situations.","As such, assessing the safety benefits of one information topology over other is a necessary step towards evaluating and ensuring safety.","In this paper, we compare the safety benefits of a cooperative adaptive cruise control which utilizes information from one predecessor vehicle (CACC) with the one that utilizes information from multiple predecessors (CACC+) for the maintenance of spacing under an emergency braking scenario.","A constant time headway policy is employed for maintenance of spacing (that includes a desired standstill spacing distance and a velocity dependent spacing distance) between the vehicles in the platoon.","The considered emergency braking scenario consists of braking of the leader vehicle of the platoon at its maximum deceleration and that of the following vehicles to maintain the spacing as per CACC or CACC+.","By focusing on the standstill spacing distance and utilizing Monte Carlo simulations, we assess the safety benefits of CACC+ over CACC by utilizing the following safety metrics: (1) probability of collision, (2) expected number of collisions, and (3) severity of collision (defined as the relative velocity of the two vehicles at impact).","We present and provide discussion of these results."],"url":"http://arxiv.org/abs/2404.19189v1","category":"eess.SY"}
{"created":"2024-04-30 01:35:14","title":"Maximum bound principle and original energy dissipation of arbitrarily high-order rescaled exponential time differencing Runge-Kutta schemes for Allen--Cahn equations","abstract":"The energy dissipation law and the maximum bound principle are two critical physical properties of the Allen--Cahn equations. While many existing time-stepping methods are known to preserve the energy dissipation law, most apply to a modified form of energy. In this work, we demonstrate that, when the nonlinear term of the Allen--Cahn equation is Lipschitz continuous, a class of arbitrarily high-order exponential time differencing Runge--Kutta (ETDRK) schemes preserve the original energy dissipation property, under a mild step-size constraint. Additionally, we guarantee the Lipschitz condition on the nonlinear term by applying a rescaling post-processing technique, which ensures that the numerical solution unconditionally satisfies the maximum bound principle. Consequently, our proposed schemes maintain both the original energy dissipation law and the maximum bound principle and can achieve arbitrarily high-order accuracy. We also establish an optimal error estimate for the proposed schemes. Some numerical experiments are carried out to verify our theoretical results.","sentences":["The energy dissipation law and the maximum bound principle are two critical physical properties of the Allen--Cahn equations.","While many existing time-stepping methods are known to preserve the energy dissipation law, most apply to a modified form of energy.","In this work, we demonstrate that, when the nonlinear term of the Allen--Cahn equation is Lipschitz continuous, a class of arbitrarily high-order exponential time differencing Runge--Kutta (ETDRK) schemes preserve the original energy dissipation property, under a mild step-size constraint.","Additionally, we guarantee the Lipschitz condition on the nonlinear term by applying a rescaling post-processing technique, which ensures that the numerical solution unconditionally satisfies the maximum bound principle.","Consequently, our proposed schemes maintain both the original energy dissipation law and the maximum bound principle and can achieve arbitrarily high-order accuracy.","We also establish an optimal error estimate for the proposed schemes.","Some numerical experiments are carried out to verify our theoretical results."],"url":"http://arxiv.org/abs/2404.19188v1","category":"math.NA"}
{"created":"2024-04-30 01:27:12","title":"CONTUNER: Singing Voice Beautifying with Pitch and Expressiveness Condition","abstract":"Singing voice beautifying is a novel task that has application value in people's daily life, aiming to correct the pitch of the singing voice and improve the expressiveness without changing the original timbre and content. Existing methods rely on paired data or only concentrate on the correction of pitch. However, professional songs and amateur songs from the same person are hard to obtain, and singing voice beautifying doesn't only contain pitch correction but other aspects like emotion and rhythm. Since we propose a fast and high-fidelity singing voice beautifying system called ConTuner, a diffusion model combined with the modified condition to generate the beautified Mel-spectrogram, where the modified condition is composed of optimized pitch and expressiveness. For pitch correction, we establish a mapping relationship from MIDI, spectrum envelope to pitch. To make amateur singing more expressive, we propose the expressiveness enhancer in the latent space to convert amateur vocal tone to professional. ConTuner achieves a satisfactory beautification effect on both Mandarin and English songs. Ablation study demonstrates that the expressiveness enhancer and generator-based accelerate method in ConTuner are effective.","sentences":["Singing voice beautifying is a novel task that has application value in people's daily life, aiming to correct the pitch of the singing voice and improve the expressiveness without changing the original timbre and content.","Existing methods rely on paired data or only concentrate on the correction of pitch.","However, professional songs and amateur songs from the same person are hard to obtain, and singing voice beautifying doesn't only contain pitch correction but other aspects like emotion and rhythm.","Since we propose a fast and high-fidelity singing voice beautifying system called ConTuner, a diffusion model combined with the modified condition to generate the beautified Mel-spectrogram, where the modified condition is composed of optimized pitch and expressiveness.","For pitch correction, we establish a mapping relationship from MIDI, spectrum envelope to pitch.","To make amateur singing more expressive, we propose the expressiveness enhancer in the latent space to convert amateur vocal tone to professional.","ConTuner achieves a satisfactory beautification effect on both Mandarin and English songs.","Ablation study demonstrates that the expressiveness enhancer and generator-based accelerate method in ConTuner are effective."],"url":"http://arxiv.org/abs/2404.19187v1","category":"cs.SD"}
{"created":"2024-04-30 01:19:39","title":"Distributionally Robust Optimization with Multimodal Decision-Dependent Ambiguity Sets","abstract":"We consider a two-stage distributionally robust optimization (DRO) model with multimodal uncertainty, where both the mode probabilities and uncertainty distributions could be affected by the first-stage decisions. To address this setting, we propose a generic framework by introducing a $\\phi$-divergence based ambiguity set to characterize the decision-dependent mode probabilities and further consider both moment-based and Wasserstein distance-based ambiguity sets to characterize the uncertainty distribution under each mode. We identify two special $\\phi$-divergence examples (variation distance and $\\chi^2$-distance) and provide specific forms of decision dependence relationships under which we can derive tractable reformulations. Furthermore, we investigate the benefits of considering multimodality in a DRO model compared to a single-modal counterpart through an analytical analysis. We provide a computational study over the facility location problem to illustrate our results, which demonstrate that omission of multimodality and decision-dependent uncertainties within DRO frameworks result in inadequately performing solutions with worse in-sample and out-of-sample performances under various settings.","sentences":["We consider a two-stage distributionally robust optimization (DRO) model with multimodal uncertainty, where both the mode probabilities and uncertainty distributions could be affected by the first-stage decisions.","To address this setting, we propose a generic framework by introducing a $\\phi$-divergence based ambiguity set to characterize the decision-dependent mode probabilities and further consider both moment-based and Wasserstein distance-based ambiguity sets to characterize the uncertainty distribution under each mode.","We identify two special $\\phi$-divergence examples (variation distance and $\\chi^2$-distance) and provide specific forms of decision dependence relationships under which we can derive tractable reformulations.","Furthermore, we investigate the benefits of considering multimodality in a DRO model compared to a single-modal counterpart through an analytical analysis.","We provide a computational study over the facility location problem to illustrate our results, which demonstrate that omission of multimodality and decision-dependent uncertainties within DRO frameworks result in inadequately performing solutions with worse in-sample and out-of-sample performances under various settings."],"url":"http://arxiv.org/abs/2404.19185v1","category":"math.OC"}
{"created":"2024-04-30 01:19:27","title":"Tunable plasmonic properties of spatially overlapping asymmetric nanoparticle dimers","abstract":"In this work, the plasmonic properties of nanoparticle dimers with optical responses over a wide spectral range have been investigated by varying the inter-particle gap, dimer geometry, gap morphology, nanoparticle composition, and refractive index of the surrounding medium. In particular, we have theoretically investigated the plasmonic properties of spatially overlapping symmetric gold nanodisks, shape-asymmetric gold nanodisk nanoplates, and compositionally asymmetric gold-silver nanodisk dimers by varying the gap separation from touching to overlapping regime. In such a configuration, we have observed the appearance of a dominant bonding dimer plasmon (BDP) mode that blue-shifts as gap separation turns from touching to overlapping. In addition, it is found that asymmetric dimer produces a broader resonance shift compared to symmetric dimer because of the hybridization of bright and dark plasmon modes, making it a viable option for sensing applications. It is also found that blue shifting of the plasmon mode occurred by changing the gap morphology of the contacting region of the dimer for fixed nanoparticle size and dimer overlapping. Moreover, we explored the influence of overlapping nanoparticle dimer thickness and observed a notable resonance shift by varying the thickness of the nanoparticle dimer. Finally, based on this tunable resonance shift, we explored the sensing applications of bonding dimer plasmon mode with optimized geometries. Thus, the computed figure of merit (FOM) of the overlapping symmetric, shape-asymmetric, and compositionally asymmetric nanoparticle dimers were found to be 1.55, 2.08, and 3.04, respectively, and comparative advantages among the three configurations with implications for surface-based sensing have been thoroughly discussed.","sentences":["In this work, the plasmonic properties of nanoparticle dimers with optical responses over a wide spectral range have been investigated by varying the inter-particle gap, dimer geometry, gap morphology, nanoparticle composition, and refractive index of the surrounding medium.","In particular, we have theoretically investigated the plasmonic properties of spatially overlapping symmetric gold nanodisks, shape-asymmetric gold nanodisk nanoplates, and compositionally asymmetric gold-silver nanodisk dimers by varying the gap separation from touching to overlapping regime.","In such a configuration, we have observed the appearance of a dominant bonding dimer plasmon (BDP) mode that blue-shifts as gap separation turns from touching to overlapping.","In addition, it is found that asymmetric dimer produces a broader resonance shift compared to symmetric dimer because of the hybridization of bright and dark plasmon modes, making it a viable option for sensing applications.","It is also found that blue shifting of the plasmon mode occurred by changing the gap morphology of the contacting region of the dimer for fixed nanoparticle size and dimer overlapping.","Moreover, we explored the influence of overlapping nanoparticle dimer thickness and observed a notable resonance shift by varying the thickness of the nanoparticle dimer.","Finally, based on this tunable resonance shift, we explored the sensing applications of bonding dimer plasmon mode with optimized geometries.","Thus, the computed figure of merit (FOM) of the overlapping symmetric, shape-asymmetric, and compositionally asymmetric nanoparticle dimers were found to be 1.55, 2.08, and 3.04, respectively, and comparative advantages among the three configurations with implications for surface-based sensing have been thoroughly discussed."],"url":"http://arxiv.org/abs/2404.19184v1","category":"physics.optics"}
{"created":"2024-04-30 01:02:15","title":"Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics","abstract":"Transformers have supplanted Recurrent Neural Networks as the dominant architecture for both natural language processing tasks and, despite criticisms of cognitive implausibility, for modelling the effect of predictability on online human language comprehension. However, two recently developed recurrent neural network architectures, RWKV and Mamba, appear to perform natural language tasks comparably to or better than transformers of equivalent scale. In this paper, we show that contemporary recurrent models are now also able to match - and in some cases, exceed - performance of comparably sized transformers at modeling online human language comprehension. This suggests that transformer language models are not uniquely suited to this task, and opens up new directions for debates about the extent to which architectural features of language models make them better or worse models of human language comprehension.","sentences":["Transformers have supplanted Recurrent Neural Networks as the dominant architecture for both natural language processing tasks and, despite criticisms of cognitive implausibility, for modelling the effect of predictability on online human language comprehension.","However, two recently developed recurrent neural network architectures, RWKV and Mamba, appear to perform natural language tasks comparably to or better than transformers of equivalent scale.","In this paper, we show that contemporary recurrent models are now also able to match - and in some cases, exceed - performance of comparably sized transformers at modeling online human language comprehension.","This suggests that transformer language models are not uniquely suited to this task, and opens up new directions for debates about the extent to which architectural features of language models make them better or worse models of human language comprehension."],"url":"http://arxiv.org/abs/2404.19178v1","category":"cs.CL"}
{"created":"2024-04-30 01:00:57","title":"Detecting Spectral Breaks in Spiked Covariance Models","abstract":"In this paper, the key objects of interest are the sequential covariance matrices $\\mathbf{S}_{n,t}$ and their largest eigenvalues. Here, the matrix $\\mathbf{S}_{n,t}$ is computed as the empirical covariance associated with observations $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_{ \\lfloor nt \\rfloor } \\}$, for $t\\in [0,1]$. The observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ are assumed to be i.i.d. $p$-dimensional vectors with zero mean, and a covariance matrix that is a fixed-rank perturbation of the identity matrix. Treating $\\{ \\mathbf{S}_{n,t}\\}_{t \\in [0,1]}$ as a matrix-valued stochastic process indexed by $t$, we study the behavior of the largest eigenvalues of $\\mathbf{S}_{n,t}$, as $t$ varies, with $n$ and $p$ increasing simultaneously, so that $p/n \\to y \\in (0,1)$. As a key contribution of this work, we establish the weak convergence of the stochastic process corresponding to the sample spiked eigenvalues, if their population counterparts exceed the critical phase-transition threshold. Our analysis of the limiting process is fully comprehensive revealing, in general, non-Gaussian limiting processes.   As an application, we consider a class of change-point problems, where the interest is in detecting structural breaks in the covariance caused by a change in magnitude of the spiked eigenvalues. For this purpose, we propose two different maximal statistics corresponding to centered spiked eigenvalues of the sequential covariances. We show the existence of limiting null distributions for these statistics, and prove consistency of the test under fixed alternatives. Moreover, we compare the behavior of the proposed tests through a simulation study.","sentences":["In this paper, the key objects of interest are the sequential covariance matrices $\\mathbf{S}_{n,t}$ and their largest eigenvalues.","Here, the matrix $\\mathbf{S}_{n,t}$ is computed as the empirical covariance associated with observations $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_{ \\lfloor nt \\rfloor } \\}$, for $t\\in [0,1]$. The observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ are assumed to be i.i.d.","$p$-dimensional vectors with zero mean, and a covariance matrix that is a fixed-rank perturbation of the identity matrix.","Treating $\\{ \\mathbf{S}_{n,t}\\}_{t \\in","[0,1]}$ as a matrix-valued stochastic process indexed by $t$, we study the behavior of the largest eigenvalues of $\\mathbf{S}_{n,t}$, as $t$ varies, with $n$ and $p$ increasing simultaneously, so that $p/n \\to y \\in (0,1)$. As a key contribution of this work, we establish the weak convergence of the stochastic process corresponding to the sample spiked eigenvalues, if their population counterparts exceed the critical phase-transition threshold.","Our analysis of the limiting process is fully comprehensive revealing, in general, non-Gaussian limiting processes.   ","As an application, we consider a class of change-point problems, where the interest is in detecting structural breaks in the covariance caused by a change in magnitude of the spiked eigenvalues.","For this purpose, we propose two different maximal statistics corresponding to centered spiked eigenvalues of the sequential covariances.","We show the existence of limiting null distributions for these statistics, and prove consistency of the test under fixed alternatives.","Moreover, we compare the behavior of the proposed tests through a simulation study."],"url":"http://arxiv.org/abs/2404.19176v1","category":"math.ST"}
{"created":"2024-04-30 00:16:59","title":"PEVA-Net: Prompt-Enhanced View Aggregation Network for Zero/Few-Shot Multi-View 3D Shape Recognition","abstract":"Large vision-language models have impressively promote the performance of 2D visual recognition under zero/few-shot scenarios. In this paper, we focus on exploiting the large vision-language model, i.e., CLIP, to address zero/few-shot 3D shape recognition based on multi-view representations. The key challenge for both tasks is to generate a discriminative descriptor of the 3D shape represented by multiple view images under the scenarios of either without explicit training (zero-shot 3D shape recognition) or training with a limited number of data (few-shot 3D shape recognition). We analyze that both tasks are relevant and can be considered simultaneously. Specifically, leveraging the descriptor which is effective for zero-shot inference to guide the tuning of the aggregated descriptor under the few-shot training can significantly improve the few-shot learning efficacy. Hence, we propose Prompt-Enhanced View Aggregation Network (PEVA-Net) to simultaneously address zero/few-shot 3D shape recognition. Under the zero-shot scenario, we propose to leverage the prompts built up from candidate categories to enhance the aggregation process of multiple view-associated visual features. The resulting aggregated feature serves for effective zero-shot recognition of the 3D shapes. Under the few-shot scenario, we first exploit a transformer encoder to aggregate the view-associated visual features into a global descriptor. To tune the encoder, together with the main classification loss, we propose a self-distillation scheme via a feature distillation loss by treating the zero-shot descriptor as the guidance signal for the few-shot descriptor. This scheme can significantly enhance the few-shot learning efficacy.","sentences":["Large vision-language models have impressively promote the performance of 2D visual recognition under zero/few-shot scenarios.","In this paper, we focus on exploiting the large vision-language model, i.e., CLIP, to address zero/few-shot 3D shape recognition based on multi-view representations.","The key challenge for both tasks is to generate a discriminative descriptor of the 3D shape represented by multiple view images under the scenarios of either without explicit training (zero-shot 3D shape recognition) or training with a limited number of data (few-shot 3D shape recognition).","We analyze that both tasks are relevant and can be considered simultaneously.","Specifically, leveraging the descriptor which is effective for zero-shot inference to guide the tuning of the aggregated descriptor under the few-shot training can significantly improve the few-shot learning efficacy.","Hence, we propose Prompt-Enhanced View Aggregation Network (PEVA-Net) to simultaneously address zero/few-shot 3D shape recognition.","Under the zero-shot scenario, we propose to leverage the prompts built up from candidate categories to enhance the aggregation process of multiple view-associated visual features.","The resulting aggregated feature serves for effective zero-shot recognition of the 3D shapes.","Under the few-shot scenario, we first exploit a transformer encoder to aggregate the view-associated visual features into a global descriptor.","To tune the encoder, together with the main classification loss, we propose a self-distillation scheme via a feature distillation loss by treating the zero-shot descriptor as the guidance signal for the few-shot descriptor.","This scheme can significantly enhance the few-shot learning efficacy."],"url":"http://arxiv.org/abs/2404.19168v1","category":"cs.CV"}
{"created":"2024-04-30 00:12:57","title":"Advancing low-field MRI with a universal denoising imaging transformer: Towards fast and high-quality imaging","abstract":"Recent developments in low-field (LF) magnetic resonance imaging (MRI) systems present remarkable opportunities for affordable and widespread MRI access. A robust denoising method to overcome the intrinsic low signal-noise-ratio (SNR) barrier is critical to the success of LF MRI. However, current data-driven MRI denoising methods predominantly handle magnitude images and rely on customized models with constrained data diversity and quantity, which exhibit limited generalizability in clinical applications across diverse MRI systems, pulse sequences, and organs. In this study, we present ImT-MRD: a complex-valued imaging transformer trained on a vast number of clinical MRI scans aiming at universal MR denoising at LF systems. Compared with averaging multiple-repeated scans for higher image SNR, the model obtains better image quality from fewer repetitions, demonstrating its capability for accelerating scans under various clinical settings. Moreover, with its complex-valued image input, the model can denoise intermediate results before advanced post-processing and prepare high-quality data for further MRI research. By delivering universal and accurate denoising across clinical and research tasks, our model holds great promise to expedite the evolution of LF MRI for accessible and equal biomedical applications.","sentences":["Recent developments in low-field (LF) magnetic resonance imaging (MRI) systems present remarkable opportunities for affordable and widespread MRI access.","A robust denoising method to overcome the intrinsic low signal-noise-ratio (SNR) barrier is critical to the success of LF MRI.","However, current data-driven MRI denoising methods predominantly handle magnitude images and rely on customized models with constrained data diversity and quantity, which exhibit limited generalizability in clinical applications across diverse MRI systems, pulse sequences, and organs.","In this study, we present ImT-MRD: a complex-valued imaging transformer trained on a vast number of clinical MRI scans aiming at universal MR denoising at LF systems.","Compared with averaging multiple-repeated scans for higher image SNR, the model obtains better image quality from fewer repetitions, demonstrating its capability for accelerating scans under various clinical settings.","Moreover, with its complex-valued image input, the model can denoise intermediate results before advanced post-processing and prepare high-quality data for further MRI research.","By delivering universal and accurate denoising across clinical and research tasks, our model holds great promise to expedite the evolution of LF MRI for accessible and equal biomedical applications."],"url":"http://arxiv.org/abs/2404.19167v1","category":"eess.IV"}
{"created":"2024-04-30 00:08:20","title":"Large CP violation in charmed baryon decays","abstract":"We present a systematic numerical study of CP violation in two-body weak decays of antitriplet charmed baryons. In the standard model, the $\\Delta c =1$ quark level interactions inducing CP violation for the relevant decays can be grouped into two types, proportional to the CKM matrix elements $\\lambda_q= V_{uq}V^*_{cq}$ with $q=d$ and $q=b$, respectively. Recent studies have shown that with $SU(3)_F$ flavor symmetry, the decay amplitudes, including their strong phases, can be determined by data if contributions from $\\lambda_b$ are neglected. However, terms proportional to $\\lambda_b$ must be retained to create interferences that induce CP violation. Some of the $\\lambda_b$ terms can be recovered in the framework of $SU(3)_F$ flavor symmetry, and the CP-violating rate asymmetry $A_{CP}$ is predicted to be of the order of ${\\cal O}(10^{-4})$. We find that final state re-scattering effects, which link $\\lambda_d$ and $\\lambda_b$ terms, can help recover the missing terms. The re-scattering effects can enhance CP violation by an order of magnitude, with $A_{CP} (\\Xi_c^0 \\to p K^-) - A_{CP} (\\Xi_c^0 \\to \\Sigma^+ \\pi^-)$ being as large as $1.87 \\times 10^{-3}$. This makes it promising to observe CP violation for the first time in baryon decays.","sentences":["We present a systematic numerical study of CP violation in two-body weak decays of antitriplet charmed baryons.","In the standard model, the $\\Delta c =1$ quark level interactions inducing CP violation for the relevant decays can be grouped into two types, proportional to the CKM matrix elements $\\lambda_q= V_{uq}V^*_{cq}$ with $q=d$ and $q=b$, respectively.","Recent studies have shown that with $SU(3)_F$ flavor symmetry, the decay amplitudes, including their strong phases, can be determined by data if contributions from $\\lambda_b$ are neglected.","However, terms proportional to $\\lambda_b$ must be retained to create interferences that induce CP violation.","Some of the $\\lambda_b$ terms can be recovered in the framework of $SU(3)_F$ flavor symmetry, and the CP-violating rate asymmetry $A_{CP}$ is predicted to be of the order of ${\\cal O}(10^{-4})$. We find that final state re-scattering effects, which link $\\lambda_d$ and $\\lambda_b$ terms, can help recover the missing terms.","The re-scattering effects can enhance CP violation by an order of magnitude, with $A_{CP} (\\Xi_c^0 \\to p K^-) - A_{CP} (\\Xi_c^0 \\to \\Sigma^+ \\pi^-)$ being as large as $1.87 \\times 10^{-3}$.","This makes it promising to observe CP violation for the first time in baryon decays."],"url":"http://arxiv.org/abs/2404.19166v1","category":"hep-ph"}
{"created":"2024-04-29 23:49:19","title":"What Drives Performance in Multilingual Language Models?","abstract":"This study investigates the factors influencing the performance of multilingual large language models (MLLMs) across diverse languages. We study 6 MLLMs, including masked language models, autoregressive models, and instruction-tuned LLMs, on the SIB-200 dataset, a topic classification dataset encompassing 204 languages. Our analysis considers three scenarios: ALL languages, SEEN languages (present in the model's pretraining data), and UNSEEN languages (not present or documented in the model's pretraining data in any meaningful way). We examine the impact of factors such as pretraining data size, general resource availability, language family, and script type on model performance. Decision tree analysis reveals that pretraining data size is the most influential factor for SEEN languages. However, interestingly, script type and language family are crucial for UNSEEN languages, highlighting the importance of cross-lingual transfer learning. Notably, model size and architecture do not significantly alter the most important features identified. Our findings provide valuable insights into the strengths and limitations of current MLLMs and hope to guide the development of more effective and equitable multilingual NLP systems.","sentences":["This study investigates the factors influencing the performance of multilingual large language models (MLLMs) across diverse languages.","We study 6 MLLMs, including masked language models, autoregressive models, and instruction-tuned LLMs, on the SIB-200 dataset, a topic classification dataset encompassing 204 languages.","Our analysis considers three scenarios: ALL languages, SEEN languages (present in the model's pretraining data), and UNSEEN languages (not present or documented in the model's pretraining data in any meaningful way).","We examine the impact of factors such as pretraining data size, general resource availability, language family, and script type on model performance.","Decision tree analysis reveals that pretraining data size is the most influential factor for SEEN languages.","However, interestingly, script type and language family are crucial for UNSEEN languages, highlighting the importance of cross-lingual transfer learning.","Notably, model size and architecture do not significantly alter the most important features identified.","Our findings provide valuable insights into the strengths and limitations of current MLLMs and hope to guide the development of more effective and equitable multilingual NLP systems."],"url":"http://arxiv.org/abs/2404.19159v1","category":"cs.CL"}
{"created":"2024-04-29 23:38:58","title":"Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian Processes to Deep Neural Networks","abstract":"Large neural networks trained on large datasets have become the dominant paradigm in machine learning. These systems rely on maximum likelihood point estimates of their parameters, precluding them from expressing model uncertainty. This may result in overconfident predictions and it prevents the use of deep learning models for sequential decision making. This thesis develops scalable methods to equip neural networks with model uncertainty. In particular, we leverage the linearised Laplace approximation to equip pre-trained neural networks with the uncertainty estimates provided by their tangent linear models. This turns the problem of Bayesian inference in neural networks into one of Bayesian inference in conjugate Gaussian-linear models. Alas, the cost of this remains cubic in either the number of network parameters or in the number of observations times output dimensions. By assumption, neither are tractable. We address this intractability by using stochastic gradient descent (SGD) -- the workhorse algorithm of deep learning -- to perform posterior sampling in linear models and their convex duals: Gaussian processes. With this, we turn back to linearised neural networks, finding the linearised Laplace approximation to present a number of incompatibilities with modern deep learning practices -- namely, stochastic optimisation, early stopping and normalisation layers -- when used for hyperparameter learning. We resolve these and construct a sample-based EM algorithm for scalable hyperparameter learning with linearised neural networks. We apply the above methods to perform linearised neural network inference with ResNet-50 (25M parameters) trained on Imagenet (1.2M observations and 1000 output dimensions). Additionally, we apply our methods to estimate uncertainty for 3d tomographic reconstructions obtained with the deep image prior network.","sentences":["Large neural networks trained on large datasets have become the dominant paradigm in machine learning.","These systems rely on maximum likelihood point estimates of their parameters, precluding them from expressing model uncertainty.","This may result in overconfident predictions and it prevents the use of deep learning models for sequential decision making.","This thesis develops scalable methods to equip neural networks with model uncertainty.","In particular, we leverage the linearised Laplace approximation to equip pre-trained neural networks with the uncertainty estimates provided by their tangent linear models.","This turns the problem of Bayesian inference in neural networks into one of Bayesian inference in conjugate Gaussian-linear models.","Alas, the cost of this remains cubic in either the number of network parameters or in the number of observations times output dimensions.","By assumption, neither are tractable.","We address this intractability by using stochastic gradient descent (SGD) -- the workhorse algorithm of deep learning -- to perform posterior sampling in linear models and their convex duals: Gaussian processes.","With this, we turn back to linearised neural networks, finding the linearised Laplace approximation to present a number of incompatibilities with modern deep learning practices -- namely, stochastic optimisation, early stopping and normalisation layers -- when used for hyperparameter learning.","We resolve these and construct a sample-based EM algorithm for scalable hyperparameter learning with linearised neural networks.","We apply the above methods to perform linearised neural network inference with ResNet-50 (25M parameters) trained on Imagenet (1.2M observations and 1000 output dimensions).","Additionally, we apply our methods to estimate uncertainty for 3d tomographic reconstructions obtained with the deep image prior network."],"url":"http://arxiv.org/abs/2404.19157v1","category":"stat.ML"}
{"created":"2024-04-29 23:36:38","title":"RTF: Region-based Table Filling Method for Relational Triple Extraction","abstract":"Relational triple extraction is crucial work for the automatic construction of knowledge graphs. Existing methods only construct shallow representations from a token or token pair-level. However, previous works ignore local spatial dependencies of relational triples, resulting in a weakness of entity pair boundary detection. To tackle this problem, we propose a novel Region-based Table Filling method (RTF). We devise a novel region-based tagging scheme and bi-directional decoding strategy, which regard each relational triple as a region on the relation-specific table, and identifies triples by determining two endpoints of each region. We also introduce convolution to construct region-level table representations from a spatial perspective which makes triples easier to be captured. In addition, we share partial tagging scores among different relations to improve learning efficiency of relation classifier. Experimental results show that our method achieves state-of-the-art with better generalization capability on three variants of two widely used benchmark datasets.","sentences":["Relational triple extraction is crucial work for the automatic construction of knowledge graphs.","Existing methods only construct shallow representations from a token or token pair-level.","However, previous works ignore local spatial dependencies of relational triples, resulting in a weakness of entity pair boundary detection.","To tackle this problem, we propose a novel Region-based Table Filling method (RTF).","We devise a novel region-based tagging scheme and bi-directional decoding strategy, which regard each relational triple as a region on the relation-specific table, and identifies triples by determining two endpoints of each region.","We also introduce convolution to construct region-level table representations from a spatial perspective which makes triples easier to be captured.","In addition, we share partial tagging scores among different relations to improve learning efficiency of relation classifier.","Experimental results show that our method achieves state-of-the-art with better generalization capability on three variants of two widely used benchmark datasets."],"url":"http://arxiv.org/abs/2404.19154v1","category":"cs.CL"}
{"created":"2024-04-29 23:26:30","title":"SAGS: Structure-Aware 3D Gaussian Splatting","abstract":"Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene's geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page https://eververas.github.io/SAGS/.","sentences":["Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods.","Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives.","However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts.","In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets.","SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene's geometry.","Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\\times$ size reduction without the reliance on any compression strategies.","Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size.","Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps.","Project page https://eververas.github.io/SAGS/."],"url":"http://arxiv.org/abs/2404.19149v1","category":"cs.CV"}
{"created":"2024-04-29 23:08:03","title":"Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty","abstract":"Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \\textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \\textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \\textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.","sentences":["Bootstrap is a popular methodology for simulating input uncertainty.","However, it can be computationally expensive when the number of samples is large.","We propose a new approach called \\textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications.","We decomposes the target being simulated into two parts: the \\textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \\textit{orthogonal part} which is easier to be simulated.","We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval."],"url":"http://arxiv.org/abs/2404.19145v2","category":"stat.ME"}
{"created":"2024-04-29 23:05:03","title":"A Locally Robust Semiparametric Approach to Examiner IV Designs","abstract":"I propose a locally robust semiparametric framework for estimating causal effects using the popular examiner IV design, in the presence of many examiners and possibly many covariates relative to the sample size. The key ingredient of this approach is an orthogonal moment function that is robust to biases and local misspecification from the first step estimation of the examiner IV. I derive the orthogonal moment function and show that it delivers multiple robustness where the outcome model or at least one of the first step components is misspecified but the estimating equation remains valid. The proposed framework not only allows for estimation of the examiner IV in the presence of many examiners and many covariates relative to sample size, using a wide range of nonparametric and machine learning techniques including LASSO, Dantzig, neural networks and random forests, but also delivers root-n consistent estimation of the parameter of interest under mild assumptions.","sentences":["I propose a locally robust semiparametric framework for estimating causal effects using the popular examiner IV design, in the presence of many examiners and possibly many covariates relative to the sample size.","The key ingredient of this approach is an orthogonal moment function that is robust to biases and local misspecification from the first step estimation of the examiner IV.","I derive the orthogonal moment function and show that it delivers multiple robustness where the outcome model or at least one of the first step components is misspecified but the estimating equation remains valid.","The proposed framework not only allows for estimation of the examiner IV in the presence of many examiners and many covariates relative to sample size, using a wide range of nonparametric and machine learning techniques including LASSO, Dantzig, neural networks and random forests, but also delivers root-n consistent estimation of the parameter of interest under mild assumptions."],"url":"http://arxiv.org/abs/2404.19144v1","category":"econ.EM"}
{"created":"2024-04-29 23:00:27","title":"Purcell enhanced optical refrigeration","abstract":"Optical refrigeration of solids with anti-Stokes fluorescence has been widely explored as a vibration-free cryogenic cooling technology. A minimum temperature of 87 K has been demonstrated with rare-earth ion doped crystals using optical refrigeration. However, the depletion of the upper-lying energy levels in the ground state manifold hinders further cooling to below liquid nitrogen (LN$_2$) temperatures, confining its applications. In this work, we introduce a Purcell enhanced optical refrigeration method to circumvent this limitation. This approach enhances the emission of high energy photons by coupling to a nearby nanocavity, blue shifting the mean emission wavelength. Such Purcell enhanced emission facilitates cooling starting from a lower energy level in the ground state manifold, which exhibits a higher occupation below LN$_2$ temperatures. Using our experimentally measured optical coefficients, our theoretical analysis predicts a minimum achievable temperature of 38 K for a Yb$^{3+}$:YLiF$_{4}$ nanocrystal near a cavity under realistic conditions. The proposed method is applicable to other rare-earth ion doped materials and semiconductors, and will have applications in creating superconducting and other quantum devices with solid-state cooling.","sentences":["Optical refrigeration of solids with anti-Stokes fluorescence has been widely explored as a vibration-free cryogenic cooling technology.","A minimum temperature of 87 K has been demonstrated with rare-earth ion doped crystals using optical refrigeration.","However, the depletion of the upper-lying energy levels in the ground state manifold hinders further cooling to below liquid nitrogen (LN$_2$) temperatures, confining its applications.","In this work, we introduce a Purcell enhanced optical refrigeration method to circumvent this limitation.","This approach enhances the emission of high energy photons by coupling to a nearby nanocavity, blue shifting the mean emission wavelength.","Such Purcell enhanced emission facilitates cooling starting from a lower energy level in the ground state manifold, which exhibits a higher occupation below LN$_2$ temperatures.","Using our experimentally measured optical coefficients, our theoretical analysis predicts a minimum achievable temperature of 38 K for a Yb$^{3+}$:YLiF$_{4}$ nanocrystal near a cavity under realistic conditions.","The proposed method is applicable to other rare-earth ion doped materials and semiconductors, and will have applications in creating superconducting and other quantum devices with solid-state cooling."],"url":"http://arxiv.org/abs/2404.19142v1","category":"physics.optics"}
{"created":"2024-04-29 22:06:17","title":"Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM","abstract":"Vision and Language Models (VLMs) continue to demonstrate remarkable zero-shot (ZS) performance across various tasks. However, many probing studies have revealed that even the best-performing VLMs struggle to capture aspects of compositional scene understanding, lacking the ability to properly ground and localize linguistic phrases in images. Recent VLM advancements include scaling up both model and dataset sizes, additional training objectives and levels of supervision, and variations in the model architectures. To characterize the grounding ability of VLMs, such as phrase grounding, referring expressions comprehension, and relationship understanding, Pointing Game has been used as an evaluation metric for datasets with bounding box annotations. In this paper, we introduce a novel suite of quantitative metrics that utilize GradCAM activations to rigorously evaluate the grounding capabilities of pre-trained VLMs like CLIP, BLIP, and ALBEF. These metrics offer an explainable and quantifiable approach for a more detailed comparison of the zero-shot capabilities of VLMs and enable measuring models' grounding uncertainty. This characterization reveals interesting tradeoffs between the size of the model, the dataset size, and their performance.","sentences":["Vision and Language Models (VLMs) continue to demonstrate remarkable zero-shot (ZS) performance across various tasks.","However, many probing studies have revealed that even the best-performing VLMs struggle to capture aspects of compositional scene understanding, lacking the ability to properly ground and localize linguistic phrases in images.","Recent VLM advancements include scaling up both model and dataset sizes, additional training objectives and levels of supervision, and variations in the model architectures.","To characterize the grounding ability of VLMs, such as phrase grounding, referring expressions comprehension, and relationship understanding, Pointing Game has been used as an evaluation metric for datasets with bounding box annotations.","In this paper, we introduce a novel suite of quantitative metrics that utilize GradCAM activations to rigorously evaluate the grounding capabilities of pre-trained VLMs like CLIP, BLIP, and ALBEF.","These metrics offer an explainable and quantifiable approach for a more detailed comparison of the zero-shot capabilities of VLMs and enable measuring models' grounding uncertainty.","This characterization reveals interesting tradeoffs between the size of the model, the dataset size, and their performance."],"url":"http://arxiv.org/abs/2404.19128v1","category":"cs.CV"}
{"created":"2024-04-29 22:05:29","title":"A model-free subdata selection method for classification","abstract":"Subdata selection is a study of methods that select a small representative sample of the big data, the analysis of which is fast and statistically efficient. The existing subdata selection methods assume that the big data can be reasonably modeled using an underlying model, such as a (multinomial) logistic regression for classification problems. These methods work extremely well when the underlying modeling assumption is correct but often yield poor results otherwise. In this paper, we propose a model-free subdata selection method for classification problems, and the resulting subdata is called PED subdata. The PED subdata uses decision trees to find a partition of the data, followed by selecting an appropriate sample from each component of the partition. Random forests are used for analyzing the selected subdata. Our method can be employed for a general number of classes in the response and for both categorical and continuous predictors. We show analytically that the PED subdata results in a smaller Gini than a uniform subdata. Further, we demonstrate that the PED subdata has higher classification accuracy than other competing methods through extensive simulated and real datasets.","sentences":["Subdata selection is a study of methods that select a small representative sample of the big data, the analysis of which is fast and statistically efficient.","The existing subdata selection methods assume that the big data can be reasonably modeled using an underlying model, such as a (multinomial) logistic regression for classification problems.","These methods work extremely well when the underlying modeling assumption is correct but often yield poor results otherwise.","In this paper, we propose a model-free subdata selection method for classification problems, and the resulting subdata is called PED subdata.","The PED subdata uses decision trees to find a partition of the data, followed by selecting an appropriate sample from each component of the partition.","Random forests are used for analyzing the selected subdata.","Our method can be employed for a general number of classes in the response and for both categorical and continuous predictors.","We show analytically that the PED subdata results in a smaller Gini than a uniform subdata.","Further, we demonstrate that the PED subdata has higher classification accuracy than other competing methods through extensive simulated and real datasets."],"url":"http://arxiv.org/abs/2404.19127v1","category":"stat.ME"}
{"created":"2024-04-29 21:34:04","title":"Identification and estimation of causal effects using non-concurrent controls in platform trials","abstract":"Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce a unique challenge with the use of non-concurrent controls, raising questions about how to efficiently utilize their data to estimate treatment effects. Specifically, what estimands should be used to evaluate the causal effect of a treatment versus control? Under what assumptions can these estimands be identified and estimated? Do we achieve any efficiency gains? In this paper, we use structural causal models and counterfactuals to clarify estimands and formalize their identification in the presence of non-concurrent controls in platform trials. We also provide outcome regression, inverse probability weighting, and doubly robust estimators for their estimation. We discuss efficiency gains, demonstrate their performance in a simulation study, and apply them to the ACTT platform trial, resulting in a 20% improvement in precision.","sentences":["Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure.","Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout.","This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable.","While flexible, platform trials introduce a unique challenge with the use of non-concurrent controls, raising questions about how to efficiently utilize their data to estimate treatment effects.","Specifically, what estimands should be used to evaluate the causal effect of a treatment versus control?","Under what assumptions can these estimands be identified and estimated?","Do we achieve any efficiency gains?","In this paper, we use structural causal models and counterfactuals to clarify estimands and formalize their identification in the presence of non-concurrent controls in platform trials.","We also provide outcome regression, inverse probability weighting, and doubly robust estimators for their estimation.","We discuss efficiency gains, demonstrate their performance in a simulation study, and apply them to the ACTT platform trial, resulting in a 20% improvement in precision."],"url":"http://arxiv.org/abs/2404.19118v1","category":"stat.ME"}
{"created":"2024-04-29 21:23:29","title":"EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars","abstract":"Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model's capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets.","sentences":["Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach.","The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain.","We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions.","To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model's capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   ","Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   ","We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets."],"url":"http://arxiv.org/abs/2404.19110v1","category":"cs.CV"}
{"created":"2024-04-29 20:57:05","title":"Optimal tradeoffs for estimating Pauli observables","abstract":"We revisit the problem of Pauli shadow tomography: given copies of an unknown $n$-qubit quantum state $\\rho$, estimate $\\text{tr}(P\\rho)$ for some set of Pauli operators $P$ to within additive error $\\epsilon$. This has been a popular testbed for exploring the advantage of protocols with quantum memory over those without: with enough memory to measure two copies at a time, one can use Bell sampling to estimate $|\\text{tr}(P\\rho)|$ for all $P$ using $O(n/\\epsilon^4)$ copies, but with $k\\le n$ qubits of memory, $\\Omega(2^{(n-k)/3})$ copies are needed.   These results leave open several natural questions. How does this picture change in the physically relevant setting where one only needs to estimate a certain subset of Paulis? What is the optimal dependence on $\\epsilon$? What is the optimal tradeoff between quantum memory and sample complexity?   We answer all of these questions. For any subset $A$ of Paulis and any family of measurement strategies, we completely characterize the optimal sample complexity, up to $\\log |A|$ factors. We show any protocol that makes $\\text{poly}(n)$-copy measurements must make $\\Omega(1/\\epsilon^4)$ measurements. For any protocol that makes $\\text{poly}(n)$-copy measurements and only has $k < n$ qubits of memory, we show that $\\widetilde{\\Theta}(\\min\\{2^n/\\epsilon^2, 2^{n-k}/\\epsilon^4\\})$ copies are necessary and sufficient.   The protocols we propose can also estimate the actual values $\\text{tr}(P\\rho)$, rather than just their absolute values as in prior work. Additionally, as a byproduct of our techniques, we establish tight bounds for the task of purity testing and show that it exhibits an intriguing phase transition not present in the memory-sample tradeoff for Pauli shadow tomography.","sentences":["We revisit the problem of Pauli shadow tomography: given copies of an unknown $n$-qubit quantum state $\\rho$, estimate $\\text{tr}(P\\rho)$ for some set of Pauli operators $P$ to within additive error $\\epsilon$. This has been a popular testbed for exploring the advantage of protocols with quantum memory over those without: with enough memory to measure two copies at a time, one can use Bell sampling to estimate $|\\text{tr}(P\\rho)|$ for all $P$ using $O(n/\\epsilon^4)$ copies, but with $k\\le n$ qubits of memory, $\\Omega(2^{(n-k)/3})$ copies are needed.   ","These results leave open several natural questions.","How does this picture change in the physically relevant setting where one only needs to estimate a certain subset of Paulis?","What is the optimal dependence on $\\epsilon$?","What is the optimal tradeoff between quantum memory and sample complexity?   ","We answer all of these questions.","For any subset $A$ of Paulis and any family of measurement strategies, we completely characterize the optimal sample complexity, up to $\\log |A|$ factors.","We show any protocol that makes $\\text{poly}(n)$-copy measurements must make $\\Omega(1/\\epsilon^4)$ measurements.","For any protocol that makes $\\text{poly}(n)$-copy measurements and only has $k < n$ qubits of memory, we show that $\\widetilde{\\Theta}(\\min\\{2^n/\\epsilon^2, 2^{n-k}/\\epsilon^4\\})$ copies are necessary and sufficient.   ","The protocols we propose can also estimate the actual values $\\text{tr}(P\\rho)$, rather than just their absolute values as in prior work.","Additionally, as a byproduct of our techniques, we establish tight bounds for the task of purity testing and show that it exhibits an intriguing phase transition not present in the memory-sample tradeoff for Pauli shadow tomography."],"url":"http://arxiv.org/abs/2404.19105v1","category":"quant-ph"}
{"created":"2024-04-29 20:05:46","title":"Electronic correlations, layer distinction, and electron doping in the alternating single-layer trilayer La$_{3}$Ni$_{2}$O$_{7}$ polymorph","abstract":"We employ a density-functional theory plus dynamical mean-field theory framework to investigate the correlated electronic structure of the alternating single-layer trilayer (1313) polymorph of La$_3$Ni$_2$O$_7$ under pressure. We find that La$_3$Ni$_2$O$_7$-1313 exhibits electronically distinct layers in terms of their correlations. At ambient pressure, the single-layer is in a Mott insulating regime and the low-energy physics is dominated by the trilayer block. Under pressure, the single-layer exhibits orbital-selective physics closing the gap allowing charge flow into the trilayer block. This change in effective doping of the trilayer block is likely linked to the higher T$_c$ obtained in La$_3$Ni$_2$O$_7$-1313 ($\\sim$ 80 K) when compared to the nominal trilayer La$_4$Ni$_3$O$_{10}$ ($\\sim$ 30 K). We conclude that correlation-driven layer differentiation is crucial in the La$_3$Ni$_2$O$_7$-1313 polymorph and that its low-energy physics aligns closely with the trilayer La$_4$Ni$_3$O$_{10}$ (rather than the conventional bilayer La$_3$Ni$_2$O$_7$), in spite of the apparent differences in nominal filling.","sentences":["We employ a density-functional theory plus dynamical mean-field theory framework to investigate the correlated electronic structure of the alternating single-layer trilayer (1313) polymorph of La$_3$Ni$_2$O$_7$ under pressure.","We find that La$_3$Ni$_2$O$_7$-1313 exhibits electronically distinct layers in terms of their correlations.","At ambient pressure, the single-layer is in a Mott insulating regime and the low-energy physics is dominated by the trilayer block.","Under pressure, the single-layer exhibits orbital-selective physics closing the gap allowing charge flow into the trilayer block.","This change in effective doping of the trilayer block is likely linked to the higher T$_c$ obtained in La$_3$Ni$_2$O$_7$-1313 ($\\sim$ 80 K) when compared to the nominal trilayer La$_4$Ni$_3$O$_{10}$ ($\\sim$ 30 K).","We conclude that correlation-driven layer differentiation is crucial in the La$_3$Ni$_2$O$_7$-1313 polymorph and that its low-energy physics aligns closely with the trilayer La$_4$Ni$_3$O$_{10}$ (rather than the conventional bilayer La$_3$Ni$_2$O$_7$), in spite of the apparent differences in nominal filling."],"url":"http://arxiv.org/abs/2404.19089v1","category":"cond-mat.str-el"}
{"created":"2024-04-29 19:26:25","title":"Reinforcement Learning Driven Cooperative Ball Balance in Rigidly Coupled Drones","abstract":"Multi-drone cooperative transport (CT) problem has been widely studied in the literature. However, limited work exists on control of such systems in the presence of time-varying uncertainties, such as the time-varying center of gravity (CG). This paper presents a leader-follower approach for the control of a multi-drone CT system with time-varying CG. The leader uses a traditional Proportional-Integral-Derivative (PID) controller, and in contrast, the follower uses a deep reinforcement learning (RL) controller using only local information and minimal leader information. Extensive simulation results are presented, showing the effectiveness of the proposed method over a previously developed adaptive controller and for variations in the mass of the objects being transported and CG speeds. Preliminary experimental work also demonstrates ball balance (depicting moving CG) on a stick/rod lifted by two Crazyflie drones cooperatively.","sentences":["Multi-drone cooperative transport (CT) problem has been widely studied in the literature.","However, limited work exists on control of such systems in the presence of time-varying uncertainties, such as the time-varying center of gravity (CG).","This paper presents a leader-follower approach for the control of a multi-drone CT system with time-varying CG.","The leader uses a traditional Proportional-Integral-Derivative (PID) controller, and in contrast, the follower uses a deep reinforcement learning (RL) controller using only local information and minimal leader information.","Extensive simulation results are presented, showing the effectiveness of the proposed method over a previously developed adaptive controller and for variations in the mass of the objects being transported and CG speeds.","Preliminary experimental work also demonstrates ball balance (depicting moving CG) on a stick/rod lifted by two Crazyflie drones cooperatively."],"url":"http://arxiv.org/abs/2404.19070v1","category":"cs.RO"}
{"created":"2024-04-29 18:51:17","title":"Plan of Thoughts: Heuristic-Guided Problem Solving with Large Language Models","abstract":"While language models (LMs) offer significant capability in zero-shot reasoning tasks across a wide range of domains, they do not perform satisfactorily in problems which requires multi-step reasoning. Previous approaches to mitigate this involves breaking a larger, multi-step task into sub-tasks and asking the language model to generate proposals (\"thoughts\") for each sub-task and using exhaustive planning approaches such as DFS to compose a solution. In this work, we leverage this idea to introduce two new contributions: first, we formalize a planning-based approach to perform multi-step problem solving with LMs via Partially Observable Markov Decision Processes (POMDPs), with the LM's own reflections about the value of a state used as a search heuristic; second, leveraging the online POMDP solver POMCP, we demonstrate a superior success rate of 89.4% on the Game of 24 task as compared to existing approaches while also offering better anytime performance characteristics than fixed tree-search which is used previously. Taken together, these contributions allow modern LMs to decompose and solve larger-scale reasoning tasks more effectively.","sentences":["While language models (LMs) offer significant capability in zero-shot reasoning tasks across a wide range of domains, they do not perform satisfactorily in problems which requires multi-step reasoning.","Previous approaches to mitigate this involves breaking a larger, multi-step task into sub-tasks and asking the language model to generate proposals (\"thoughts\") for each sub-task and using exhaustive planning approaches such as DFS to compose a solution.","In this work, we leverage this idea to introduce two new contributions: first, we formalize a planning-based approach to perform multi-step problem solving with LMs via Partially Observable Markov Decision Processes (POMDPs), with the LM's own reflections about the value of a state used as a search heuristic; second, leveraging the online POMDP solver POMCP, we demonstrate a superior success rate of 89.4% on the Game of 24 task as compared to existing approaches while also offering better anytime performance characteristics than fixed tree-search which is used previously.","Taken together, these contributions allow modern LMs to decompose and solve larger-scale reasoning tasks more effectively."],"url":"http://arxiv.org/abs/2404.19055v1","category":"cs.CL"}
{"created":"2024-04-29 18:49:06","title":"Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes","abstract":"The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \\textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.","sentences":["The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice.","Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density.","In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \\textit{any} continuous, integrable spectral density.","In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence.","As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop.","We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer."],"url":"http://arxiv.org/abs/2404.19053v1","category":"stat.CO"}
{"created":"2024-04-29 18:39:24","title":"Continuous feedback protocols for cooling and trapping a quantum harmonic oscillator","abstract":"Quantum technologies and experiments often require preparing systems in low-temperature states. Here, we investigate cooling schemes using feedback protocols modeled with a Quantum Fokker-Planck Master Equation (QFPME) recently derived by Annby-Andersson et. al. (Phys. Rev. Lett. 129, 050401, 2022). This equation describes systems under continuous weak measurements, with feedback based on the outcome of these measurements. We apply this formalism to study the cooling and trapping of a harmonic oscillator for several protocols based on position and/or momentum measurements. We find that the protocols can cool the oscillator down to, or close to, the ground state for suitable choices of parameters. Our analysis provides an analytically solvable case study of quantum measurement and feedback and illustrates the application of the QFPME to continuous quantum systems.","sentences":["Quantum technologies and experiments often require preparing systems in low-temperature states.","Here, we investigate cooling schemes using feedback protocols modeled with a Quantum Fokker-Planck Master Equation (QFPME) recently derived by Annby-Andersson et.","al. (Phys. Rev. Lett.","129, 050401, 2022).","This equation describes systems under continuous weak measurements, with feedback based on the outcome of these measurements.","We apply this formalism to study the cooling and trapping of a harmonic oscillator for several protocols based on position and/or momentum measurements.","We find that the protocols can cool the oscillator down to, or close to, the ground state for suitable choices of parameters.","Our analysis provides an analytically solvable case study of quantum measurement and feedback and illustrates the application of the QFPME to continuous quantum systems."],"url":"http://arxiv.org/abs/2404.19047v1","category":"quant-ph"}
{"created":"2024-04-29 18:35:52","title":"Peculiar velocities in Friedmann universes with nonzero spatial curvature","abstract":"We extend the earlier linear studies of cosmological peculiar velocities to Friedmann universes with nonzero spatial curvature. In the process, we also compare our results with those obtained in cosmologies with Euclidean spatial sections. Employing relativistic cosmological perturbation theory, we first provide the differential formulae governing the evolution of peculiar velocities on all Friedmann backgrounds. The technical complexities of the curved models, however, mean that analytic solutions are possible only in special, though characteristic, moments in the lifetime of these universes. Nevertheless, our solutions exhibit persistent patterns that make us confident enough to generalise them. Thus, we confirm earlier claims that, compared to the Newtonian studies, the relativistic analysis supports considerably stronger linear growth-rates for peculiar-velocity perturbations. This result holds irrespective of the background curvature. Moreover, for positive curvature, the peculiar growth-rate is found to be faster than that obtained in a spatially flat Friedman universe. In contrast, linear peculiar velocities appear to grow at a slower pace when their Friedmann host is spatially open. Extrapolating them to the present, our results seem to suggest faster bulk peculiar motions in overdense, rather than in underdense, regions of the universe.","sentences":["We extend the earlier linear studies of cosmological peculiar velocities to Friedmann universes with nonzero spatial curvature.","In the process, we also compare our results with those obtained in cosmologies with Euclidean spatial sections.","Employing relativistic cosmological perturbation theory, we first provide the differential formulae governing the evolution of peculiar velocities on all Friedmann backgrounds.","The technical complexities of the curved models, however, mean that analytic solutions are possible only in special, though characteristic, moments in the lifetime of these universes.","Nevertheless, our solutions exhibit persistent patterns that make us confident enough to generalise them.","Thus, we confirm earlier claims that, compared to the Newtonian studies, the relativistic analysis supports considerably stronger linear growth-rates for peculiar-velocity perturbations.","This result holds irrespective of the background curvature.","Moreover, for positive curvature, the peculiar growth-rate is found to be faster than that obtained in a spatially flat Friedman universe.","In contrast, linear peculiar velocities appear to grow at a slower pace when their Friedmann host is spatially open.","Extrapolating them to the present, our results seem to suggest faster bulk peculiar motions in overdense, rather than in underdense, regions of the universe."],"url":"http://arxiv.org/abs/2404.19046v1","category":"gr-qc"}
{"created":"2024-04-29 18:33:17","title":"Improving Interpretability of Deep Active Learning for Flood Inundation Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite Imagery","abstract":"Flood inundation mapping is a critical task for responding to the increasing risk of flooding linked to global warming. Significant advancements of deep learning in recent years have triggered its extensive applications, including flood inundation mapping. To cope with the time-consuming and labor-intensive data labeling process in supervised learning, deep active learning strategies are one of the feasible approaches. However, there remains limited exploration into the interpretability of how deep active learning strategies operate, with a specific focus on flood inundation mapping in the field of remote sensing. In this study, we introduce a novel framework of Interpretable Deep Active Learning for Flood inundation Mapping (IDAL-FIM), specifically in terms of class ambiguity of multi-spectral satellite images. In the experiments, we utilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we employ five acquisition functions, which are the random, K-means, BALD, entropy, and margin acquisition functions. Based on the experimental results, we demonstrate that two proposed class ambiguity indices are effective variables to interpret the deep active learning by establishing statistically significant correlation with the predictive uncertainty of the deep learning model at the tile level. Then, we illustrate the behaviors of deep active learning through visualizing two-dimensional density plots and providing interpretations regarding the operation of deep active learning, in flood inundation mapping.","sentences":["Flood inundation mapping is a critical task for responding to the increasing risk of flooding linked to global warming.","Significant advancements of deep learning in recent years have triggered its extensive applications, including flood inundation mapping.","To cope with the time-consuming and labor-intensive data labeling process in supervised learning, deep active learning strategies are one of the feasible approaches.","However, there remains limited exploration into the interpretability of how deep active learning strategies operate, with a specific focus on flood inundation mapping in the field of remote sensing.","In this study, we introduce a novel framework of Interpretable Deep Active Learning for Flood inundation Mapping (IDAL-FIM), specifically in terms of class ambiguity of multi-spectral satellite images.","In the experiments, we utilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout.","In addition, we employ five acquisition functions, which are the random, K-means, BALD, entropy, and margin acquisition functions.","Based on the experimental results, we demonstrate that two proposed class ambiguity indices are effective variables to interpret the deep active learning by establishing statistically significant correlation with the predictive uncertainty of the deep learning model at the tile level.","Then, we illustrate the behaviors of deep active learning through visualizing two-dimensional density plots and providing interpretations regarding the operation of deep active learning, in flood inundation mapping."],"url":"http://arxiv.org/abs/2404.19043v1","category":"cs.CV"}
{"created":"2024-04-29 18:23:30","title":"Non-resonant electric quantum control of individual on-surface spins","abstract":"Quantum control techniques play an important role in manipulating and harnessing the properties of different quantum systems, including isolated atoms. Here, we propose to achieve quantum control over a single on-surface atomic spin using Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interferometry implemented with Scanning Tunneling Microscopy (STM). Specifically, we model how the application of time-dependent, non-resonant AC electric fields across the STM tip-surface gap makes it possible to achieve precise quantum state manipulation in an isolated Fe atom on a MgO/Ag(100) surface. We propose a protocol to combine Landau Zener tunneling with LZSM interferometry that permits one to measure the quantum spin tunneling of an individual Fe atom. The proposed experiments can be implemented with ESR-STM instrumentation, opening a new venue in the research of on-surface single spin control.","sentences":["Quantum control techniques play an important role in manipulating and harnessing the properties of different quantum systems, including isolated atoms.","Here, we propose to achieve quantum control over a single on-surface atomic spin using Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interferometry implemented with Scanning Tunneling Microscopy (STM).","Specifically, we model how the application of time-dependent, non-resonant AC electric fields across the STM tip-surface gap makes it possible to achieve precise quantum state manipulation in an isolated Fe atom on a MgO/Ag(100) surface.","We propose a protocol to combine Landau Zener tunneling with LZSM interferometry that permits one to measure the quantum spin tunneling of an individual Fe atom.","The proposed experiments can be implemented with ESR-STM instrumentation, opening a new venue in the research of on-surface single spin control."],"url":"http://arxiv.org/abs/2404.19036v1","category":"quant-ph"}
{"created":"2024-04-29 18:23:00","title":"Improved pressure-gradient sensor for the prediction of separation onset in RANS models","abstract":"We improve upon two key aspects of the Menter shear stress transport (SST) turbulence model: (1) We propose a more robust adverse pressure gradient sensor based on the strength of the pressure gradient in the direction of the local mean flow; (2) We propose two alternative eddy viscosity models to be used in the adverse pressure gradient regions identified by our sensor. Direct numerical simulations of the Boeing Gaussian bump are used to identify the terms in the baseline SST model that need correction, and a posteriori Reynolds-averaged Navier-Stokes calculations are used to calibrate coefficient values, leading to a model that is both physics driven and data informed. The two sensor-equipped models are applied to two thick airfoils representative of modern wind turbine applications, the FFA-W3-301 and the DU00-W-212, with maximum thicknesses of 30% and 20% of their chord lengths, respectively. While the baseline SST model predicts stall (onset of separation) $3^\\circ$ to $5^\\circ$ late for all cases considered, the proposed models predict stall within the margins of experimental uncertainty, which greatly improves the prediction of the maximum lift generated. For the FFA airfoil, the models also improve the prediction of the linear region of the lift curve likely due to their improved prediction of a pressure-side separation at low angles of attack. The models are shown to generalize well across the two airfoil geometries (despite their difference in thickness) and across almost a factor of 10 in variations in chord-based Reynolds numbers from $1.6\\times10^6$ to $1.5\\times10^7$.","sentences":["We improve upon two key aspects of the Menter shear stress transport (SST) turbulence model: (1) We propose a more robust adverse pressure gradient sensor based on the strength of the pressure gradient in the direction of the local mean flow; (2) We propose two alternative eddy viscosity models to be used in the adverse pressure gradient regions identified by our sensor.","Direct numerical simulations of the Boeing Gaussian bump are used to identify the terms in the baseline SST model that need correction, and a posteriori Reynolds-averaged Navier-Stokes calculations are used to calibrate coefficient values, leading to a model that is both physics driven and data informed.","The two sensor-equipped models are applied to two thick airfoils representative of modern wind turbine applications, the FFA-W3-301 and the DU00-W-212, with maximum thicknesses of 30% and 20% of their chord lengths, respectively.","While the baseline SST model predicts stall (onset of separation) $3^\\circ$ to $5^\\circ$ late for all cases considered, the proposed models predict stall within the margins of experimental uncertainty, which greatly improves the prediction of the maximum lift generated.","For the FFA airfoil, the models also improve the prediction of the linear region of the lift curve likely due to their improved prediction of a pressure-side separation at low angles of attack.","The models are shown to generalize well across the two airfoil geometries (despite their difference in thickness) and across almost a factor of 10 in variations in chord-based Reynolds numbers from $1.6\\times10^6$ to $1.5\\times10^7$."],"url":"http://arxiv.org/abs/2404.19035v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 18:04:24","title":"Information literacy development and assessment at school level: a systematic review of the literature","abstract":"Information literacy (IL) involves a group of competences and fundamental skills in the 21st century. Today, society operates around information, which is challenging considering the vast amount of content available online. People must be capable of searching, critically assessing, making sense of, and communicating information. This set of competences must be properly developed since childhood, especially if considering early age access to online resources. To better understand the evolution and current status of IL development and assessment at school (K-12) level, we conducted a systematic literature review based on the guidelines established by the PRISMA statement. Our review led us to an initial set of 1,234 articles, from which 53 passed the inclusion criteria. These articles were used to address six research questions focused on IL definitions, skills, standards, and assessment tools. Our review shows IL evolution over the years and how it has been formalisedthrough definitions and standards. These findings reveal key gaps that must be addressed in order to advance the field further. Keywords: Elementary education, Information literacy, Secondary education, 21st Century abilities.","sentences":["Information literacy (IL) involves a group of competences and fundamental skills in the 21st century.","Today, society operates around information, which is challenging considering the vast amount of content available online.","People must be capable of searching, critically assessing, making sense of, and communicating information.","This set of competences must be properly developed since childhood, especially if considering early age access to online resources.","To better understand the evolution and current status of IL development and assessment at school (K-12) level, we conducted a systematic literature review based on the guidelines established by the PRISMA statement.","Our review led us to an initial set of 1,234 articles, from which 53 passed the inclusion criteria.","These articles were used to address six research questions focused on IL definitions, skills, standards, and assessment tools.","Our review shows IL evolution over the years and how it has been formalisedthrough definitions and standards.","These findings reveal key gaps that must be addressed in order to advance the field further.","Keywords: Elementary education, Information literacy, Secondary education, 21st Century abilities."],"url":"http://arxiv.org/abs/2404.19020v1","category":"cs.IR"}
{"created":"2024-04-29 18:03:01","title":"MAGAZ3NE: Massive, Extremely Dusty Galaxies at $z\\sim2$ Lead to Photometric Overestimation of Number Densities of the Most Massive Galaxies at $3<z<4$","abstract":"We present rest-frame optical spectra from Keck/MOSFIRE and Keck/NIRES of 16 candidate ultramassive galaxies targeted as part of the Massive Ancient Galaxies at $z>3$ Near-Infrared (MAGAZ3NE) Survey. These candidates were selected to have photometric redshifts $3\\lesssim z_{\\rm phot}<4$, photometric stellar masses log($M$/M$_\\odot$)$>11.7$, and well-sampled photometric spectral energy distributions (SEDs) from the UltraVISTA and VIDEO surveys. In contrast to previous spectroscopic observations of blue star-forming and post-starburst ultramassive galaxies, candidates in this sample have very red SEDs implying significant dust attenuation, old stellar ages, and/or active galactic nuclei (AGN). Of these galaxies, eight are revealed to be heavily dust-obscured $2.0<z<2.7$ galaxies with strong emission lines, some showing broad features indicative of AGN, three are Type I AGN hosts at $z>3$, one is a $z\\sim1.2$ dusty galaxy, and four galaxies do not have a confirmed spectroscopic redshift. In fact, none of the sample has |$z_{\\rm spec}-z_{\\rm phot}$|$<0.5$, suggesting difficulties for photometric redshift programs in fitting similarly red SEDs. The prevalence of these red interloper galaxies suggests that the number densities of high-mass galaxies are overestimated at $z\\gtrsim3$ in large photometric surveys, helping to resolve the `impossibly early galaxy problem' and leading to much better agreement with cosmological galaxy simulations. A more complete spectroscopic survey of ultramassive galaxies is required to pin down the uncertainties on their number densities in the early universe.","sentences":["We present rest-frame optical spectra from Keck/MOSFIRE and Keck/NIRES of 16 candidate ultramassive galaxies targeted as part of the Massive Ancient Galaxies at $z>3$ Near-Infrared (MAGAZ3NE) Survey.","These candidates were selected to have photometric redshifts $3\\lesssim z_{\\rm phot}<4$, photometric stellar masses log($M$/M$_\\odot$)$>11.7$, and well-sampled photometric spectral energy distributions (SEDs) from the UltraVISTA and VIDEO surveys.","In contrast to previous spectroscopic observations of blue star-forming and post-starburst ultramassive galaxies, candidates in this sample have very red SEDs implying significant dust attenuation, old stellar ages, and/or active galactic nuclei (AGN).","Of these galaxies, eight are revealed to be heavily dust-obscured $2.0<z<2.7$ galaxies with strong emission lines, some showing broad features indicative of AGN, three are Type I AGN hosts at $z>3$, one is a $z\\sim1.2$ dusty galaxy, and four galaxies do not have a confirmed spectroscopic redshift.","In fact, none of the sample has |$z_{\\rm spec}-z_{\\rm phot}$|$<0.5$, suggesting difficulties for photometric redshift programs in fitting similarly red SEDs.","The prevalence of these red interloper galaxies suggests that the number densities of high-mass galaxies are overestimated at $z\\gtrsim3$ in large photometric surveys, helping to resolve the `impossibly early galaxy problem' and leading to much better agreement with cosmological galaxy simulations.","A more complete spectroscopic survey of ultramassive galaxies is required to pin down the uncertainties on their number densities in the early universe."],"url":"http://arxiv.org/abs/2404.19018v1","category":"astro-ph.GA"}
{"created":"2024-04-29 18:02:00","title":"$\\mathcal{N} = 2$ superconformal higher-spin multiplets and their hypermultiplet couplings","abstract":"We construct an off-shell $\\mathcal{N}=2$ superconformal cubic vertex for the hypermultiplet coupled to an arbitrary integer higher spin ${\\bf s}$ gauge $\\mathcal{N}=2$ supermultiplet % in flatfour-dimensional space. in a general $\\mathcal{N}=2$ conformal supergravity background. We heavily use $\\mathcal{N}=2, 4D$ harmonic superspace that provides an unconstrained superfield Lagrangian description. We start with $\\mathcal{N}=2$ global superconformal symmetry transformations of the free hypermultiplet model and require invariance of the cubic vertices of general form under these transformations and their gauged version. As a result, we deduce $\\mathcal{N}=2, 4D$ unconstrained analytic superconformal gauge potentials for an arbitrary integer ${\\bf s}$. These are the basic ingredients of the approach under consideration. We describe the properties of the gauge potentials, derive the corresponding superconformal and gauge transformation laws, and inspect the off-shell contents of the thus obtained $\\mathcal{N}=2$ superconformal higher-spin ${\\bf s}$ multiplets in the Wess-Zumino gauges. The spin ${\\bf s}$ multiplet involves $8(2{\\bf s} -1)_B + 8(2{\\bf s}-1)_F$ essential off-shell degrees of freedom. The cubic vertex has the generic structure higher spin gauge superfields $\\times$ hypermultiplet supercurrents. We present the explicit form of the relevant supercurrents.","sentences":["We construct an off-shell $\\mathcal{N}=2$ superconformal cubic vertex for the hypermultiplet coupled to an arbitrary integer higher spin ${\\bf s}$ gauge $\\mathcal{N}=2$ supermultiplet % in flatfour-dimensional space.","in a general $\\mathcal{N}=2$ conformal supergravity background.","We heavily use $\\mathcal{N}=2, 4D$ harmonic superspace that provides an unconstrained superfield Lagrangian description.","We start with $\\mathcal{N}=2$ global superconformal symmetry transformations of the free hypermultiplet model and require invariance of the cubic vertices of general form under these transformations and their gauged version.","As a result, we deduce $\\mathcal{N}=2, 4D$ unconstrained analytic superconformal gauge potentials for an arbitrary integer ${\\bf s}$. These are the basic ingredients of the approach under consideration.","We describe the properties of the gauge potentials, derive the corresponding superconformal and gauge transformation laws, and inspect the off-shell contents of the thus obtained $\\mathcal{N}=2$ superconformal higher-spin ${\\bf s}$ multiplets in the Wess-Zumino gauges.","The spin ${\\bf s}$ multiplet involves $8(2{\\bf s} -1)_B + 8(2{\\bf s}-1)_F$ essential off-shell degrees of freedom.","The cubic vertex has the generic structure higher spin gauge superfields $\\times$ hypermultiplet supercurrents.","We present the explicit form of the relevant supercurrents."],"url":"http://arxiv.org/abs/2404.19016v1","category":"hep-th"}
{"created":"2024-04-29 18:00:19","title":"Transitionless Quantum Driving of the Tomonaga-Luttinger Liquid","abstract":"Shortcuts to adiabaticity (STA) make the fast preparation of many-body states possible, circumventing the limitations of adiabatic strategies. We propose a fast STA protocol for generating interacting states in the Tomonaga-Luttinger liquid by counter-diabatic driving, stirring the dynamics with an auxiliary control field. To this end, we exploit the equivalence between the time-dependent Tomonaga-Luttinger liquid and an ensemble of quantum oscillators with driven mass and frequency. We specify the closed-form expression of the counterdiabatic control and demonstrate its efficiency in suppressing excitations.","sentences":["Shortcuts to adiabaticity (STA) make the fast preparation of many-body states possible, circumventing the limitations of adiabatic strategies.","We propose a fast STA protocol for generating interacting states in the Tomonaga-Luttinger liquid by counter-diabatic driving, stirring the dynamics with an auxiliary control field.","To this end, we exploit the equivalence between the time-dependent Tomonaga-Luttinger liquid and an ensemble of quantum oscillators with driven mass and frequency.","We specify the closed-form expression of the counterdiabatic control and demonstrate its efficiency in suppressing excitations."],"url":"http://arxiv.org/abs/2404.19013v1","category":"quant-ph"}
{"created":"2024-04-29 18:00:12","title":"Synthesizing the Born rule with reinforcement learning","abstract":"According to the subjective Bayesian interpretation of quantum theory (QBism), quantum mechanics is a tool that an agent would be wise to use when making bets about natural phenomena. In particular, the Born rule is understood to be a decision-making norm, an ideal which one should strive to meet even if usually falling short in practice. What is required for an agent to make decisions that conform to quantum mechanics? Here we investigate how a realistic (hence non-ideal) agent might deviate from the Born rule in its decisions. To do so we simulate a simple agent as a reinforcement-learning algorithm that makes `bets' on the outputs of a symmetric informationally-complete measurement (SIC) and adjusts its decisions in order to maximize its expected return. We quantify how far the algorithm's decision-making behavior departs from the ideal form of the Born rule and investigate the limiting factors. We propose an experimental implementation of the scenario using heralded single photons.","sentences":["According to the subjective Bayesian interpretation of quantum theory (QBism), quantum mechanics is a tool that an agent would be wise to use when making bets about natural phenomena.","In particular, the Born rule is understood to be a decision-making norm, an ideal which one should strive to meet even if usually falling short in practice.","What is required for an agent to make decisions that conform to quantum mechanics?","Here we investigate how a realistic (hence non-ideal) agent might deviate from the Born rule in its decisions.","To do so we simulate a simple agent as a reinforcement-learning algorithm that makes `bets' on the outputs of a symmetric informationally-complete measurement (SIC) and adjusts its decisions in order to maximize its expected return.","We quantify how far the algorithm's decision-making behavior departs from the ideal form of the Born rule and investigate the limiting factors.","We propose an experimental implementation of the scenario using heralded single photons."],"url":"http://arxiv.org/abs/2404.19011v1","category":"quant-ph"}
{"created":"2024-04-29 18:00:04","title":"Strongly vs. weakly coupled in-medium showers: energy stopping in large-$N_f$ QED","abstract":"Inside a medium, showers originating from a very high-energy particle may develop via medium-induced splitting processes such as hard bremsstrahlung or pair production. During shower development, two consecutive splittings sometimes overlap quantum mechanically, so that they cannot be treated independently. Some of these effects can be absorbed into an effective value of a medium parameter known as $\\hat q$. Previous calculations (with certain simplifying assumptions) have found that, after adjusting the value of $\\hat q$, the leftover effect of overlapping splittings is quite small for purely gluonic large-$N_c$ showers but is very much larger for large-$N_f$ QED showers, at comparable values of $N\\alpha$. Those works did not quite make for apples-to-apples comparisons: the gluon shower work investigated energy deposition from a gluon-initiated shower, whereas the QED work investigated charge-deposition from an electron-initiated shower. As a first step to tighten up the comparison, this paper investigates energy deposition in the QED case. Along the way, we develop a framework that should be useful in the future to explore whether the very small effect of overlapping splitting in purely gluonic showers is an artifact of having ignored quarks.","sentences":["Inside a medium, showers originating from a very high-energy particle may develop via medium-induced splitting processes such as hard bremsstrahlung or pair production.","During shower development, two consecutive splittings sometimes overlap quantum mechanically, so that they cannot be treated independently.","Some of these effects can be absorbed into an effective value of a medium parameter known as $\\hat q$. Previous calculations (with certain simplifying assumptions) have found that, after adjusting the value of $\\hat q$, the leftover effect of overlapping splittings is quite small for purely gluonic large-$N_c$ showers but is very much larger for large-$N_f$ QED showers, at comparable values of $N\\alpha$. Those works did not quite make for apples-to-apples comparisons: the gluon shower work investigated energy deposition from a gluon-initiated shower, whereas the QED work investigated charge-deposition from an electron-initiated shower.","As a first step to tighten up the comparison, this paper investigates energy deposition in the QED case.","Along the way, we develop a framework that should be useful in the future to explore whether the very small effect of overlapping splitting in purely gluonic showers is an artifact of having ignored quarks."],"url":"http://arxiv.org/abs/2404.19008v1","category":"hep-ph"}
{"created":"2024-04-29 18:00:01","title":"Planet Hunters TESS V: a planetary system around a binary star, including a mini-Neptune in the habitable zone","abstract":"We report on the discovery and validation of a transiting long-period mini-Neptune orbiting a bright (V = 9.0 mag) G dwarf (TOI 4633; R = 1.05 RSun, M = 1.10 MSun). The planet was identified in data from the Transiting Exoplanet Survey Satellite by citizen scientists taking part in the Planet Hunters TESS project. Modeling of the transit events yields an orbital period of 271.9445 +/- 0.0040 days and radius of 3.2 +/- 0.20 REarth. The Earth-like orbital period and an incident flux of 1.56 +/- 0.2 places it in the optimistic habitable zone around the star. Doppler spectroscopy of the system allowed us to place an upper mass limit on the transiting planet and revealed a non-transiting planet candidate in the system with a period of 34.15 +/- 0.15 days. Furthermore, the combination of archival data dating back to 1905 with new high angular resolution imaging revealed a stellar companion orbiting the primary star with an orbital period of around 230 years and an eccentricity of about 0.9. The long period of the transiting planet, combined with the high eccentricity and close approach of the companion star makes this a valuable system for testing the formation and stability of planets in binary systems.","sentences":["We report on the discovery and validation of a transiting long-period mini-Neptune orbiting a bright (V = 9.0 mag) G dwarf (TOI 4633; R = 1.05 RSun, M = 1.10 MSun).","The planet was identified in data from the Transiting Exoplanet Survey Satellite by citizen scientists taking part in the Planet Hunters TESS project.","Modeling of the transit events yields an orbital period of 271.9445 +/- 0.0040 days and radius of 3.2 +/- 0.20 REarth.","The Earth-like orbital period and an incident flux of 1.56 +/- 0.2 places it in the optimistic habitable zone around the star.","Doppler spectroscopy of the system allowed us to place an upper mass limit on the transiting planet and revealed a non-transiting planet candidate in the system with a period of 34.15 +/- 0.15 days.","Furthermore, the combination of archival data dating back to 1905 with new high angular resolution imaging revealed a stellar companion orbiting the primary star with an orbital period of around 230 years and an eccentricity of about 0.9.","The long period of the transiting planet, combined with the high eccentricity and close approach of the companion star makes this a valuable system for testing the formation and stability of planets in binary systems."],"url":"http://arxiv.org/abs/2404.18997v1","category":"astro-ph.EP"}
{"created":"2024-04-30 15:51:05","title":"Masked Multi-Query Slot Attention for Unsupervised Object Discovery","abstract":"Unsupervised object discovery is becoming an essential line of research for tackling recognition problems that require decomposing an image into entities, such as semantic segmentation and object detection. Recently, object-centric methods that leverage self-supervision have gained popularity, due to their simplicity and adaptability to different settings and conditions. However, those methods do not exploit effective techniques already employed in modern self-supervised approaches. In this work, we consider an object-centric approach in which DINO ViT features are reconstructed via a set of queried representations called slots. Based on that, we propose a masking scheme on input features that selectively disregards the background regions, inducing our model to focus more on salient objects during the reconstruction phase. Moreover, we extend the slot attention to a multi-query approach, allowing the model to learn multiple sets of slots, producing more stable masks. During training, these multiple sets of slots are learned independently while, at test time, these sets are merged through Hungarian matching to obtain the final slots. Our experimental results and ablations on the PASCAL-VOC 2012 dataset show the importance of each component and highlight how their combination consistently improves object localization. Our source code is available at: https://github.com/rishavpramanik/maskedmultiqueryslot","sentences":["Unsupervised object discovery is becoming an essential line of research for tackling recognition problems that require decomposing an image into entities, such as semantic segmentation and object detection.","Recently, object-centric methods that leverage self-supervision have gained popularity, due to their simplicity and adaptability to different settings and conditions.","However, those methods do not exploit effective techniques already employed in modern self-supervised approaches.","In this work, we consider an object-centric approach in which DINO ViT features are reconstructed via a set of queried representations called slots.","Based on that, we propose a masking scheme on input features that selectively disregards the background regions, inducing our model to focus more on salient objects during the reconstruction phase.","Moreover, we extend the slot attention to a multi-query approach, allowing the model to learn multiple sets of slots, producing more stable masks.","During training, these multiple sets of slots are learned independently while, at test time, these sets are merged through Hungarian matching to obtain the final slots.","Our experimental results and ablations on the PASCAL-VOC 2012 dataset show the importance of each component and highlight how their combination consistently improves object localization.","Our source code is available at: https://github.com/rishavpramanik/maskedmultiqueryslot"],"url":"http://arxiv.org/abs/2404.19654v1","category":"cs.CV"}
{"created":"2024-04-30 14:28:08","title":"Acceptance Tests of more than 10 000 Photomultiplier Tubes for the multi-PMT Digital Optical Modules of the IceCube Upgrade","abstract":"More than 10,000 photomultiplier tubes (PMTs) with a diameter of 80 mm will be installed in multi-PMT Digital Optical Modules (mDOMs) of the IceCube Upgrade. These have been tested and pre-calibrated at two sites. A throughput of more than 1000 PMTs per week with both sites was achieved with a modular design of the testing facilities and highly automated testing procedures. The testing facilities can easily be adapted to other PMTs, such that they can, e.g., be re-used for testing the PMTs for IceCube-Gen2. Single photoelectron response, high voltage dependence, time resolution, prepulse, late pulse, afterpulse probabilities, and dark rates were measured for each PMT. We describe the design of the testing facilities, the testing procedures, and the results of the acceptance tests.","sentences":["More than 10,000 photomultiplier tubes (PMTs) with a diameter of 80 mm will be installed in multi-PMT Digital Optical Modules (mDOMs) of the IceCube Upgrade.","These have been tested and pre-calibrated at two sites.","A throughput of more than 1000 PMTs per week with both sites was achieved with a modular design of the testing facilities and highly automated testing procedures.","The testing facilities can easily be adapted to other PMTs, such that they can, e.g., be re-used for testing the PMTs for IceCube-Gen2.","Single photoelectron response, high voltage dependence, time resolution, prepulse, late pulse, afterpulse probabilities, and dark rates were measured for each PMT.","We describe the design of the testing facilities, the testing procedures, and the results of the acceptance tests."],"url":"http://arxiv.org/abs/2404.19589v1","category":"astro-ph.IM"}
{"created":"2024-04-30 11:49:29","title":"SpecstatOR: Speckle statistics-based iOCT Segmentation Network for Ophthalmic Surgery","abstract":"This paper presents an innovative approach to intraoperative Optical Coherence Tomography (iOCT) image segmentation in ophthalmic surgery, leveraging statistical analysis of speckle patterns to incorporate statistical pathology-specific prior knowledge. Our findings indicate statistically different speckle patterns within the retina and between retinal layers and surgical tools, facilitating the segmentation of previously unseen data without the necessity for manual labeling. The research involves fitting various statistical distributions to iOCT data, enabling the differentiation of different ocular structures and surgical tools. The proposed segmentation model aims to refine the statistical findings based on prior tissue understanding to leverage statistical and biological knowledge. Incorporating statistical parameters, physical analysis of light-tissue interaction, and deep learning informed by biological structures enhance segmentation accuracy, offering potential benefits to real-time applications in ophthalmic surgical procedures. The study demonstrates the adaptability and precision of using Gamma distribution parameters and the derived binary maps as sole inputs for segmentation, notably enhancing the model's inference performance on unseen data.","sentences":["This paper presents an innovative approach to intraoperative Optical Coherence Tomography (iOCT) image segmentation in ophthalmic surgery, leveraging statistical analysis of speckle patterns to incorporate statistical pathology-specific prior knowledge.","Our findings indicate statistically different speckle patterns within the retina and between retinal layers and surgical tools, facilitating the segmentation of previously unseen data without the necessity for manual labeling.","The research involves fitting various statistical distributions to iOCT data, enabling the differentiation of different ocular structures and surgical tools.","The proposed segmentation model aims to refine the statistical findings based on prior tissue understanding to leverage statistical and biological knowledge.","Incorporating statistical parameters, physical analysis of light-tissue interaction, and deep learning informed by biological structures enhance segmentation accuracy, offering potential benefits to real-time applications in ophthalmic surgical procedures.","The study demonstrates the adaptability and precision of using Gamma distribution parameters and the derived binary maps as sole inputs for segmentation, notably enhancing the model's inference performance on unseen data."],"url":"http://arxiv.org/abs/2404.19481v1","category":"eess.IV"}
{"created":"2024-04-30 11:27:02","title":"Effect of detachment on Magnum-PSI ELM-like pulses: I. Direct observations and qualitative results","abstract":"Conditions similar to those at the end of the divertor leg in a tokamak were replicated in the linear plasma machine Magnum-PSI. The neutral pressure in the target chamber is then increased to cause the target to transition from an attached to a detached state. Superimposed to this steady state regime, ELM-like pulses are reproduced, resulting in a sudden increase in plasma temperature and density, such that the heat flux increases transiently by half an order of magnitude. Visible light emission, target thermography, and Thomson scattering are used to demonstrate that the higher the neutral pressure the more energy is removed from the ELM-like pulse in the volume. If the neutral pressure is sufficiently high, the ELM-like pulse can be prevented from affecting the target and the plasma energy is fully dissipated in the volume instead (ID 4 in Table 1). The visible light images allow the division of the ELM-plasma interaction process of ELM energy dissipation into 3 \"stages\" ranging from no dissipation to full dissipation (the target plasma is detached). In the second publication related to this study, spectroscopic data is analysed with a Bayesian approach, to acquire insights into the significance of molecular processes in dissipating the plasma energy and particles.","sentences":["Conditions similar to those at the end of the divertor leg in a tokamak were replicated in the linear plasma machine Magnum-PSI.","The neutral pressure in the target chamber is then increased to cause the target to transition from an attached to a detached state.","Superimposed to this steady state regime, ELM-like pulses are reproduced, resulting in a sudden increase in plasma temperature and density, such that the heat flux increases transiently by half an order of magnitude.","Visible light emission, target thermography, and Thomson scattering are used to demonstrate that the higher the neutral pressure the more energy is removed from the ELM-like pulse in the volume.","If the neutral pressure is sufficiently high, the ELM-like pulse can be prevented from affecting the target and the plasma energy is fully dissipated in the volume instead (ID 4 in Table 1).","The visible light images allow the division of the ELM-plasma interaction process of ELM energy dissipation into 3 \"stages\" ranging from no dissipation to full dissipation (the target plasma is detached).","In the second publication related to this study, spectroscopic data is analysed with a Bayesian approach, to acquire insights into the significance of molecular processes in dissipating the plasma energy and particles."],"url":"http://arxiv.org/abs/2404.19464v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 11:18:18","title":"Adaptive Gaussian Process Regression for Bayesian inverse problems","abstract":"We introduce a novel adaptive Gaussian Process Regression (GPR) methodology for efficient construction of surrogate models for Bayesian inverse problems with expensive forward model evaluations. An adaptive design strategy focuses on optimizing both the positioning and simulation accuracy of training data in order to reduce the computational cost of simulating training data without compromising the fidelity of the posterior distributions of parameters. The method interleaves a goal-oriented active learning algorithm selecting evaluation points and tolerances based on the expected impact on the Kullback-Leibler divergence of surrogated and true posterior with a Markov Chain Monte Carlo sampling of the posterior. The performance benefit of the adaptive approach is demonstrated for two simple test problems.","sentences":["We introduce a novel adaptive Gaussian Process Regression (GPR) methodology for efficient construction of surrogate models for Bayesian inverse problems with expensive forward model evaluations.","An adaptive design strategy focuses on optimizing both the positioning and simulation accuracy of training data in order to reduce the computational cost of simulating training data without compromising the fidelity of the posterior distributions of parameters.","The method interleaves a goal-oriented active learning algorithm selecting evaluation points and tolerances based on the expected impact on the Kullback-Leibler divergence of surrogated and true posterior with a Markov Chain Monte Carlo sampling of the posterior.","The performance benefit of the adaptive approach is demonstrated for two simple test problems."],"url":"http://arxiv.org/abs/2404.19459v1","category":"math.NA"}
{"created":"2024-04-30 10:11:03","title":"Active Dendrites Enable Efficient Continual Learning in Time-To-First-Spike Neural Networks","abstract":"While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks. This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner. Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally. By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites. Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset. Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices. Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy.","sentences":["While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks.","This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner.","Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally.","By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites.","Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset.","Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices.","Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy."],"url":"http://arxiv.org/abs/2404.19419v1","category":"cs.NE"}
{"created":"2024-04-30 08:55:01","title":"Evaluating Telugu Proficiency in Large Language Models_ A Comparative Analysis of ChatGPT and Gemini","abstract":"The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English. This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs. Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu. The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning. By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction. Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations. This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities.","sentences":["The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English.","This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.","Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu.","The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning.","By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction.","Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations.","This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities."],"url":"http://arxiv.org/abs/2404.19369v1","category":"cs.CL"}
{"created":"2024-04-30 07:52:36","title":"End-to-end information extraction in handwritten documents: Understanding Paris marriage records from 1880 to 1940","abstract":"The EXO-POPP project aims to establish a comprehensive database comprising 300,000 marriage records from Paris and its suburbs, spanning the years 1880 to 1940, which are preserved in over 130,000 scans of double pages. Each marriage record may encompass up to 118 distinct types of information that require extraction from plain text. In this paper, we introduce the M-POPP dataset, a subset of the M-POPP database with annotations for full-page text recognition and information extraction in both handwritten and printed documents, and which is now publicly available. We present a fully end-to-end architecture adapted from the DAN, designed to perform both handwritten text recognition and information extraction directly from page images without the need for explicit segmentation. We showcase the information extraction capabilities of this architecture by achieving a new state of the art for full-page Information Extraction on Esposalles and we use this architecture as a baseline for the M-POPP dataset. We also assess and compare how different encoding strategies for named entities in the text affect the performance of jointly recognizing handwritten text and extracting information, from full pages.","sentences":["The EXO-POPP project aims to establish a comprehensive database comprising 300,000 marriage records from Paris and its suburbs, spanning the years 1880 to 1940, which are preserved in over 130,000 scans of double pages.","Each marriage record may encompass up to 118 distinct types of information that require extraction from plain text.","In this paper, we introduce the M-POPP dataset, a subset of the M-POPP database with annotations for full-page text recognition and information extraction in both handwritten and printed documents, and which is now publicly available.","We present a fully end-to-end architecture adapted from the DAN, designed to perform both handwritten text recognition and information extraction directly from page images without the need for explicit segmentation.","We showcase the information extraction capabilities of this architecture by achieving a new state of the art for full-page Information Extraction on Esposalles and we use this architecture as a baseline for the M-POPP dataset.","We also assess and compare how different encoding strategies for named entities in the text affect the performance of jointly recognizing handwritten text and extracting information, from full pages."],"url":"http://arxiv.org/abs/2404.19329v1","category":"cs.CV"}
{"created":"2024-04-30 07:34:42","title":"QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering","abstract":"Extractive Question Answering (EQA) in Machine Reading Comprehension (MRC) often faces the challenge of dealing with semantically identical but format-variant inputs. Our work introduces a novel approach, called the ``Query Latent Semantic Calibrator (QLSC)'', designed as an auxiliary module for existing MRC models. We propose a unique scaling strategy to capture latent semantic center features of queries. These features are then seamlessly integrated into traditional query and passage embeddings using an attention mechanism. By deepening the comprehension of the semantic queries-passage relationship, our approach diminishes sensitivity to variations in text format and boosts the model's capability in pinpointing accurate answers. Experimental results on robust Question-Answer datasets confirm that our approach effectively handles format-variant but semantically identical queries, highlighting the effectiveness and adaptability of our proposed method.","sentences":["Extractive Question Answering (EQA) in Machine Reading Comprehension (MRC) often faces the challenge of dealing with semantically identical but format-variant inputs.","Our work introduces a novel approach, called the ``Query Latent Semantic Calibrator (QLSC)'', designed as an auxiliary module for existing MRC models.","We propose a unique scaling strategy to capture latent semantic center features of queries.","These features are then seamlessly integrated into traditional query and passage embeddings using an attention mechanism.","By deepening the comprehension of the semantic queries-passage relationship, our approach diminishes sensitivity to variations in text format and boosts the model's capability in pinpointing accurate answers.","Experimental results on robust Question-Answer datasets confirm that our approach effectively handles format-variant but semantically identical queries, highlighting the effectiveness and adaptability of our proposed method."],"url":"http://arxiv.org/abs/2404.19316v1","category":"cs.CL"}
{"created":"2024-04-30 06:51:30","title":"Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement","abstract":"The main function of depth completion is to compensate for an insufficient and unpredictable number of sparse depth measurements of hardware sensors. However, existing research on depth completion assumes that the sparsity -- the number of points or LiDAR lines -- is fixed for training and testing. Hence, the completion performance drops severely when the number of sparse depths changes significantly. To address this issue, we propose the sparsity-adaptive depth refinement (SDR) framework, which refines monocular depth estimates using sparse depth points. For SDR, we propose the masked spatial propagation network (MSPN) to perform SDR with a varying number of sparse depths effectively by gradually propagating sparse depth information throughout the entire depth map. Experimental results demonstrate that MPSN achieves state-of-the-art performance on both SDR and conventional depth completion scenarios.","sentences":["The main function of depth completion is to compensate for an insufficient and unpredictable number of sparse depth measurements of hardware sensors.","However, existing research on depth completion assumes that the sparsity -- the number of points or LiDAR lines -- is fixed for training and testing.","Hence, the completion performance drops severely when the number of sparse depths changes significantly.","To address this issue, we propose the sparsity-adaptive depth refinement (SDR) framework, which refines monocular depth estimates using sparse depth points.","For SDR, we propose the masked spatial propagation network (MSPN) to perform SDR with a varying number of sparse depths effectively by gradually propagating sparse depth information throughout the entire depth map.","Experimental results demonstrate that MPSN achieves state-of-the-art performance on both SDR and conventional depth completion scenarios."],"url":"http://arxiv.org/abs/2404.19294v1","category":"cs.CV"}
{"created":"2024-04-30 06:33:07","title":"Soft Prompt Generation for Domain Generalization","abstract":"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt, which are not optimal for specific domains. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which acts as a learning vector that undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt and residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains, potentially compromising the transferability of the prompts. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely \\textbf{S}oft \\textbf{P}rompt \\textbf{G}eneration (SPG). To the best of our knowledge, we are the first to introduce the generative model into prompt learning in VLMs and explore its potential for producing soft prompts by relying solely on the generative model, ensuring the diversity of prompts. Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt labels for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that our proposed SPG achieves state-of-the-art performance. The code will be available soon.","sentences":["Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt, which are not optimal for specific domains.","To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which acts as a learning vector that undergoes fine-tuning based on specific domain data.","Prior prompt learning methods primarily learn a fixed prompt and residuled prompt from training samples.","However, the learned prompts lack diversity and ignore information about unseen domains, potentially compromising the transferability of the prompts.","In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely \\textbf{S}oft \\textbf{P}rompt \\textbf{G}eneration (SPG).","To the best of our knowledge, we are the first to introduce the generative model into prompt learning in VLMs and explore its potential for producing soft prompts by relying solely on the generative model, ensuring the diversity of prompts.","Specifically, SPG consists of a two-stage training phase and an inference phase.","During the training phase, we introduce soft prompt labels for each domain, aiming to incorporate the generative model domain knowledge.","During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain.","Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that our proposed SPG achieves state-of-the-art performance.","The code will be available soon."],"url":"http://arxiv.org/abs/2404.19286v1","category":"cs.CV"}
{"created":"2024-04-30 06:14:51","title":"Dual Dynamic Threshold Adjustment Strategy for Deep Metric Learning","abstract":"Loss functions and sample mining strategies are essential components in deep metric learning algorithms. However, the existing loss function or mining strategy often necessitate the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative. The threshold provides a stable numerical standard for determining whether to retain the pairs. It is a vital parameter to reduce the redundant sample pairs participating in training. Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches. Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold. Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy. We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples. AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs. This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds. We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS). Experimental results show that our algorithm achieves competitive performance on CUB200, Cars196, and SOP datasets.","sentences":["Loss functions and sample mining strategies are essential components in deep metric learning algorithms.","However, the existing loss function or mining strategy often necessitate the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative.","The threshold provides a stable numerical standard for determining whether to retain the pairs.","It is a vital parameter to reduce the redundant sample pairs participating in training.","Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches.","Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold.","Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy.","We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods.","ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples.","AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs.","This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds.","We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS).","Experimental results show that our algorithm achieves competitive performance on CUB200, Cars196, and SOP datasets."],"url":"http://arxiv.org/abs/2404.19282v1","category":"cs.MM"}
{"created":"2024-04-30 04:12:59","title":"A Nonnested Augmented Subspace Method for Kohn-Sham Equation","abstract":"In this paper, a novel adaptive finite element method is proposed to solve the Kohn-Sham equation based on the moving mesh (nonnested mesh) adaptive technique and the augmented subspace method. Different from the classical self-consistent field iterative algorithm which requires to solve the Kohn-Sham equation directly in each adaptive finite element space, our algorithm transforms the Kohn-Sham equation into some linear boundary value problems of the same scale in each adaptive finite element space, and then the wavefunctions derived from the linear boundary value problems are corrected by solving a small-scale Kohn-Sham equation defined in a low-dimensional augmented subspace. Since the new algorithm avoids solving large-scale Kohn-Sham equation directly, a significant improvement for the solving efficiency can be obtained. In addition, the adaptive moving mesh technique is used to generate the nonnested adaptive mesh for the nonnested augmented subspace method according to the singularity of the approximate wavefunctions. The modified Hessian matrix of the approximate wavefunctions is used as the metric matrix to redistribute the mesh. Through the moving mesh adaptive technique, the redistributed mesh is almost optimal. A number of numerical experiments are carried out to verify the efficiency and the accuracy of the proposed algorithm.","sentences":["In this paper, a novel adaptive finite element method is proposed to solve the Kohn-Sham equation based on the moving mesh (nonnested mesh) adaptive technique and the augmented subspace method.","Different from the classical self-consistent field iterative algorithm which requires to solve the Kohn-Sham equation directly in each adaptive finite element space, our algorithm transforms the Kohn-Sham equation into some linear boundary value problems of the same scale in each adaptive finite element space, and then the wavefunctions derived from the linear boundary value problems are corrected by solving a small-scale Kohn-Sham equation defined in a low-dimensional augmented subspace.","Since the new algorithm avoids solving large-scale Kohn-Sham equation directly, a significant improvement for the solving efficiency can be obtained.","In addition, the adaptive moving mesh technique is used to generate the nonnested adaptive mesh for the nonnested augmented subspace method according to the singularity of the approximate wavefunctions.","The modified Hessian matrix of the approximate wavefunctions is used as the metric matrix to redistribute the mesh.","Through the moving mesh adaptive technique, the redistributed mesh is almost optimal.","A number of numerical experiments are carried out to verify the efficiency and the accuracy of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.19249v1","category":"math.NA"}
{"created":"2024-04-30 04:12:36","title":"Transition Rate Scheduling for Quantization-Aware Training","abstract":"Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations. It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers. We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT. Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states. This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions. It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually. We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels. Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly. Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT. Experimental results demonstrate the effectiveness of our approach on standard benchmarks.","sentences":["Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations.","It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers.","We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT.","Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states.","This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions.","It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually.","We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels.","Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly.","Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT.","Experimental results demonstrate the effectiveness of our approach on standard benchmarks."],"url":"http://arxiv.org/abs/2404.19248v1","category":"cs.CV"}
{"created":"2024-04-30 01:16:53","title":"MACO: Exploring GEMM Acceleration on a Loosely-Coupled Multi-core Processor","abstract":"General-purpose processor vendors have integrated customized accelerator in their products due to the widespread use of General Matrix-Matrix Multiplication (GEMM) kernels. However, it remains a challenge to further improve the flexibilityand scalability of these GEMM-enhanced processors to cater to the emerging large-scale GEMM workloads. In this paper we propose MACO, a novel loosely-coupled multi-core general-purpose architecture optimized for GEMM-related applications. To enhance the programmability and flexibility of MACO, the paper introduces a tile-based instruction set architecture. Additionally, the paper presents techniques such as hardware-assisted data prefetching and locking, and predictive address translation to further enhance the computational efficiency of MACO for GEMM workloads. The experimental results demonstrate that MACO exhibits good scalability, achieving an average computational efficiency of 90% across multiple cores. Furthermore, evaluations on state-of-the-art deep neural networks show that MACO can achieve up to 1.1 TFLOPS with 88% computational efficiency, indicating its adaptivity to deep learning workloads.","sentences":["General-purpose processor vendors have integrated customized accelerator in their products due to the widespread use of General Matrix-Matrix Multiplication (GEMM) kernels.","However, it remains a challenge to further improve the flexibilityand scalability of these GEMM-enhanced processors to cater to the emerging large-scale GEMM workloads.","In this paper we propose MACO, a novel loosely-coupled multi-core general-purpose architecture optimized for GEMM-related applications.","To enhance the programmability and flexibility of MACO, the paper introduces a tile-based instruction set architecture.","Additionally, the paper presents techniques such as hardware-assisted data prefetching and locking, and predictive address translation to further enhance the computational efficiency of MACO for GEMM workloads.","The experimental results demonstrate that MACO exhibits good scalability, achieving an average computational efficiency of 90% across multiple cores.","Furthermore, evaluations on state-of-the-art deep neural networks show that MACO can achieve up to 1.1 TFLOPS with 88% computational efficiency, indicating its adaptivity to deep learning workloads."],"url":"http://arxiv.org/abs/2404.19180v1","category":"cs.AR"}
{"created":"2024-04-30 01:13:24","title":"Align-Free Multi-Plane Phase Retrieval","abstract":"The multi-plane phase retrieval method provides a budget-friendly and effective way to perform phase imaging, yet it often encounters alignment challenges due to shifts along the optical axis in experiments. Traditional methods, such as employing beamsplitters instead of mechanical stage movements or adjusting focus using tunable light sources, add complexity to the setup required for multi-plane phase retrieval. Attempts to address these issues computationally face difficulties due to the variable impact of diffraction, which renders conventional homography techniques inadequate. In our research, we introduce a novel Adaptive Cascade Calibrated (ACC) strategy for multi-plane phase retrieval that overcomes misalignment issues. This technique detects feature points within the refocused sample space and calculates the transformation matrix for neighboring planes on-the-fly to digitally adjust measurements, facilitating alignment-free multi-plane phase retrieval. This approach not only avoids the need for complex and expensive optical hardware but also simplifies the imaging setup, reducing overall costs. The effectiveness of our method is validated through simulations and real-world optical experiments.","sentences":["The multi-plane phase retrieval method provides a budget-friendly and effective way to perform phase imaging, yet it often encounters alignment challenges due to shifts along the optical axis in experiments.","Traditional methods, such as employing beamsplitters instead of mechanical stage movements or adjusting focus using tunable light sources, add complexity to the setup required for multi-plane phase retrieval.","Attempts to address these issues computationally face difficulties due to the variable impact of diffraction, which renders conventional homography techniques inadequate.","In our research, we introduce a novel Adaptive Cascade Calibrated (ACC) strategy for multi-plane phase retrieval that overcomes misalignment issues.","This technique detects feature points within the refocused sample space and calculates the transformation matrix for neighboring planes on-the-fly to digitally adjust measurements, facilitating alignment-free multi-plane phase retrieval.","This approach not only avoids the need for complex and expensive optical hardware but also simplifies the imaging setup, reducing overall costs.","The effectiveness of our method is validated through simulations and real-world optical experiments."],"url":"http://arxiv.org/abs/2404.18946v1","category":"physics.optics"}
{"created":"2024-04-29 21:25:59","title":"Source-Free Domain Adaptation of Weakly-Supervised Object Localization Models for Histology","abstract":"Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images. Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations. A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type. In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons. SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks. In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy. They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment. Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification.","sentences":["Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images.","Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations.","A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type.","In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons.","SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks.","In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy.","They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment.","Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification."],"url":"http://arxiv.org/abs/2404.19113v1","category":"cs.CV"}
{"created":"2024-04-29 20:22:33","title":"Data-Driven Min-Max MPC for Linear Systems: Robustness and Adaptation","abstract":"Data-driven controllers design is an important research problem, in particular when data is corrupted by the noise. In this paper, we propose a data-driven min-max model predictive control (MPC) scheme using noisy input-state data for unknown linear time-invariant (LTI) system. The unknown system matrices are characterized by a set-membership representation using the noisy input-state data. Leveraging this representation, we derive an upper bound on the worst-case cost and determine the corresponding optimal state-feedback control law through a semidefinite program (SDP). We prove that the resulting closed-loop system is robustly stabilized and satisfies the input and state constraints. Further, we propose an adaptive data-driven min-max MPC scheme which exploits additional online input-state data to improve closed-loop performance. Numerical examples show the effectiveness of the proposed methods.","sentences":["Data-driven controllers design is an important research problem, in particular when data is corrupted by the noise.","In this paper, we propose a data-driven min-max model predictive control (MPC) scheme using noisy input-state data for unknown linear time-invariant (LTI) system.","The unknown system matrices are characterized by a set-membership representation using the noisy input-state data.","Leveraging this representation, we derive an upper bound on the worst-case cost and determine the corresponding optimal state-feedback control law through a semidefinite program (SDP).","We prove that the resulting closed-loop system is robustly stabilized and satisfies the input and state constraints.","Further, we propose an adaptive data-driven min-max MPC scheme which exploits additional online input-state data to improve closed-loop performance.","Numerical examples show the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.19096v1","category":"eess.SY"}
{"created":"2024-04-29 18:13:23","title":"Adaptive Regulated Sparsity Promoting Approach for Data-Driven Modeling and Control of Grid-Connected Solar Photovoltaic Generation","abstract":"This paper aims to introduce a new statistical learning technique based on sparsity promoting for data-driven modeling and control of solar photovoltaic (PV) systems. Compared with conventional sparse regression techniques that might introduce computational complexities when the number of candidate functions increases, an innovative algorithm, named adaptive regulated sparse regression (ARSR) is proposed that adaptively regulates the hyperparameter weights of candidate functions to best represent the dynamics of PV systems. Utilizing this algorithm, open-loop and closed-loop models of single-stage and two-stage PV systems are obtained from measurements and are utilized for control design purposes. Moreover, it is demonstrated that the proposed data-driven approach can successfully be employed for fault analysis studies, which distinguishes its capabilities compared with other data-driven techniques. Finally, the proposed approach is validated through real-time simulations.","sentences":["This paper aims to introduce a new statistical learning technique based on sparsity promoting for data-driven modeling and control of solar photovoltaic (PV) systems.","Compared with conventional sparse regression techniques that might introduce computational complexities when the number of candidate functions increases, an innovative algorithm, named adaptive regulated sparse regression (ARSR) is proposed that adaptively regulates the hyperparameter weights of candidate functions to best represent the dynamics of PV systems.","Utilizing this algorithm, open-loop and closed-loop models of single-stage and two-stage PV systems are obtained from measurements and are utilized for control design purposes.","Moreover, it is demonstrated that the proposed data-driven approach can successfully be employed for fault analysis studies, which distinguishes its capabilities compared with other data-driven techniques.","Finally, the proposed approach is validated through real-time simulations."],"url":"http://arxiv.org/abs/2404.19028v1","category":"eess.SY"}
{"created":"2024-04-29 18:07:47","title":"Multi-Page Document Visual Question Answering using Self-Attention Scoring Mechanism","abstract":"Documents are 2-dimensional carriers of written communication, and as such their interpretation requires a multi-modal approach where textual and visual information are efficiently combined. Document Visual Question Answering (Document VQA), due to this multi-modal nature, has garnered significant interest from both the document understanding and natural language processing communities. The state-of-the-art single-page Document VQA methods show impressive performance, yet in multi-page scenarios, these methods struggle. They have to concatenate all pages into one large page for processing, demanding substantial GPU resources, even for evaluation. In this work, we propose a novel method and efficient training strategy for multi-page Document VQA tasks. In particular, we employ a visual-only document representation, leveraging the encoder from a document understanding model, Pix2Struct. Our approach utilizes a self-attention scoring mechanism to generate relevance scores for each document page, enabling the retrieval of pertinent pages. This adaptation allows us to extend single-page Document VQA models to multi-page scenarios without constraints on the number of pages during evaluation, all with minimal demand for GPU resources. Our extensive experiments demonstrate not only achieving state-of-the-art performance without the need for Optical Character Recognition (OCR), but also sustained performance in scenarios extending to documents of nearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA dataset. Our code is publicly available at \\url{https://github.com/leitro/SelfAttnScoring-MPDocVQA}.","sentences":["Documents are 2-dimensional carriers of written communication, and as such their interpretation requires a multi-modal approach where textual and visual information are efficiently combined.","Document Visual Question Answering (Document VQA), due to this multi-modal nature, has garnered significant interest from both the document understanding and natural language processing communities.","The state-of-the-art single-page Document VQA methods show impressive performance, yet in multi-page scenarios, these methods struggle.","They have to concatenate all pages into one large page for processing, demanding substantial GPU resources, even for evaluation.","In this work, we propose a novel method and efficient training strategy for multi-page Document VQA tasks.","In particular, we employ a visual-only document representation, leveraging the encoder from a document understanding model, Pix2Struct.","Our approach utilizes a self-attention scoring mechanism to generate relevance scores for each document page, enabling the retrieval of pertinent pages.","This adaptation allows us to extend single-page Document VQA models to multi-page scenarios without constraints on the number of pages during evaluation, all with minimal demand for GPU resources.","Our extensive experiments demonstrate not only achieving state-of-the-art performance without the need for Optical Character Recognition (OCR), but also sustained performance in scenarios extending to documents of nearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA dataset.","Our code is publicly available at \\url{https://github.com/leitro/SelfAttnScoring-MPDocVQA}."],"url":"http://arxiv.org/abs/2404.19024v1","category":"cs.CV"}
{"created":"2024-04-29 18:04:30","title":"Enhancing Autonomous Vehicle Design and Testing: A Comprehensive Review of AR and VR Integration","abstract":"This comprehensive literature review explores the potential of Augmented Reality and Virtual Reality technologies to enhance the design and testing of autonomous vehicles. By analyzing existing research, the review aims to identify how AR and VR can be leveraged to improve various aspects of autonomous vehicle development, including: creating more realistic and comprehensive testing environments, facilitating the design of user centered interfaces, and safely evaluating driver behavior in complex scenarios. Ultimately, the review highlights AR and VR utilization as a key driver in the development of adaptable testing environments, fostering more dependable autonomous vehicle technology, and ultimately propelling significant advancements within the field.","sentences":["This comprehensive literature review explores the potential of Augmented Reality and Virtual Reality technologies to enhance the design and testing of autonomous vehicles.","By analyzing existing research, the review aims to identify how AR and VR can be leveraged to improve various aspects of autonomous vehicle development, including: creating more realistic and comprehensive testing environments, facilitating the design of user centered interfaces, and safely evaluating driver behavior in complex scenarios.","Ultimately, the review highlights AR and VR utilization as a key driver in the development of adaptable testing environments, fostering more dependable autonomous vehicle technology, and ultimately propelling significant advancements within the field."],"url":"http://arxiv.org/abs/2404.19021v1","category":"cs.HC"}
{"created":"2024-04-29 17:54:19","title":"Timely Status Updates in Slotted ALOHA Network With Energy Harvesting","abstract":"We investigate the age of information (AoI) in a scenario where energy-harvesting devices send status updates to a gateway following the slotted ALOHA protocol and receive no feedback. We let the devices adjust the transmission probabilities based on their current battery level. Using a Markovian analysis, we derive analytically the average AoI. We further provide an approximate analysis for accurate and easy-to-compute approximations of both the average AoI and the age-violation probability (AVP), i.e., the probability that the AoI exceeds a given threshold. We also analyze the average throughput. Via numerical results, we investigate two baseline strategies: transmit a new update whenever possible to exploit every opportunity to reduce the AoI, and transmit only when sufficient energy is available to increase the chance of successful decoding. The two strategies are beneficial for low and high update-generation rates, respectively. We show that an optimized policy that balances the two strategies outperforms them significantly in terms of both AoI metrics and throughput. Finally, we show the benefit of decoding multiple packets in a slot using successive interference cancellation and adapting the transmission probability based on both the current battery level and the time elapsed since the last transmission.","sentences":["We investigate the age of information (AoI) in a scenario where energy-harvesting devices send status updates to a gateway following the slotted ALOHA protocol and receive no feedback.","We let the devices adjust the transmission probabilities based on their current battery level.","Using a Markovian analysis, we derive analytically the average AoI. We further provide an approximate analysis for accurate and easy-to-compute approximations of both the average AoI and the age-violation probability (AVP), i.e., the probability that the AoI exceeds a given threshold.","We also analyze the average throughput.","Via numerical results, we investigate two baseline strategies: transmit a new update whenever possible to exploit every opportunity to reduce the AoI, and transmit only when sufficient energy is available to increase the chance of successful decoding.","The two strategies are beneficial for low and high update-generation rates, respectively.","We show that an optimized policy that balances the two strategies outperforms them significantly in terms of both AoI metrics and throughput.","Finally, we show the benefit of decoding multiple packets in a slot using successive interference cancellation and adapting the transmission probability based on both the current battery level and the time elapsed since the last transmission."],"url":"http://arxiv.org/abs/2404.18990v1","category":"cs.IT"}
{"created":"2024-04-29 17:49:49","title":"Cyberbully and Online Harassment: Issues Associated with Digital Wellbeing","abstract":"As digital technology becomes increasingly embedded in daily life, its impact on social interactions has become a critical area of study, particularly concerning cyberbullying. This meta-analysis investigates the dual role of technology in cyberbullying both as a catalyst that can exacerbate the issue and as a potential solution. Cyberbullying, characterized by the use of digital platforms to harass, threaten, or humiliate individuals, poses significant challenges to mental and social wellbeing. This research synthesizes empirical findings from diverse studies to evaluate how innovative technological interventions, such as content monitoring algorithms, anonymous reporting systems, and educational initiatives integrated within digital platforms, contribute to reducing the prevalence of cyberbullying. The study focuses on the effectiveness of these interventions in various settings, highlighting the need for adaptive strategies that respond to the dynamic digital landscape. By offering a comprehensive overview of the current state of cyberbullying and the efficacy of technology based solutions, this analysis provides valuable insights for stakeholders, including educators, policymakers, and technology developers, aiming to enhance digital wellbeing and create safer online environments. The findings underscore the importance of leveraging technology not only as a medium of communication but also as a strategic tool to combat the negative impacts of cyberbullying, thus promoting a more inclusive and respectful digital world.","sentences":["As digital technology becomes increasingly embedded in daily life, its impact on social interactions has become a critical area of study, particularly concerning cyberbullying.","This meta-analysis investigates the dual role of technology in cyberbullying both as a catalyst that can exacerbate the issue and as a potential solution.","Cyberbullying, characterized by the use of digital platforms to harass, threaten, or humiliate individuals, poses significant challenges to mental and social wellbeing.","This research synthesizes empirical findings from diverse studies to evaluate how innovative technological interventions, such as content monitoring algorithms, anonymous reporting systems, and educational initiatives integrated within digital platforms, contribute to reducing the prevalence of cyberbullying.","The study focuses on the effectiveness of these interventions in various settings, highlighting the need for adaptive strategies that respond to the dynamic digital landscape.","By offering a comprehensive overview of the current state of cyberbullying and the efficacy of technology based solutions, this analysis provides valuable insights for stakeholders, including educators, policymakers, and technology developers, aiming to enhance digital wellbeing and create safer online environments.","The findings underscore the importance of leveraging technology not only as a medium of communication but also as a strategic tool to combat the negative impacts of cyberbullying, thus promoting a more inclusive and respectful digital world."],"url":"http://arxiv.org/abs/2404.18989v1","category":"cs.CY"}
{"created":"2024-04-29 16:30:59","title":"Evidence for Plasmoid-mediated Magnetic Reconnection during a Small-scale Flare in the Partially Ionized Low Solar Atmosphere","abstract":"Magnetic reconnection plays a crucial role in the energy release process for different kinds of solar eruptions and activities. The rapid solar eruption requires a fast reconnection model. Plasmoid instability in the reconnecting current sheets is one of the most acceptable fast reconnection mechanisms for explaining the explosive events in the magnetohydrodynamics (MHD) scale, which is also a potential bridge between the macroscopic MHD reconnection process and microscale dissipations. Plenty of high resolution observations indicate that the plasmoid-like structures exist in the high temperature solar corona, but such evidences are very rare in the lower solar atmosphere with partially ionized plasmas. Utilizing joint observations from the Goode Solar Telescope (GST) and the Solar Dynamics Observatory (SDO), we discovered a small-scale eruptive phenomenon in NOAA AR 13085, characterized by clear reconnection cusp structures, supported by Nonlinear Force-Free Field (NLFFF) extrapolation results. The plasmoid-like structures with a size about 150 km were observed to be ejected downward from the current sheet at a maximum velocity of 24 km$\\cdot$s$^{-1}$ in the H$\\alpha$ line wing images, followed by enhanced emissions at around the post flare loop region in multiple wave lengths. Our 2.5D high-resolution MHD simulations further reproduced such a phenomenon and revealed reconnection fine structures. These results provide comprehensive evidences for the plasmoid mediated reconnection in partially ionized plasmas, and suggest an unified reconnection model for solar flares with different length scales from the lower chromosphere to corona.","sentences":["Magnetic reconnection plays a crucial role in the energy release process for different kinds of solar eruptions and activities.","The rapid solar eruption requires a fast reconnection model.","Plasmoid instability in the reconnecting current sheets is one of the most acceptable fast reconnection mechanisms for explaining the explosive events in the magnetohydrodynamics (MHD) scale, which is also a potential bridge between the macroscopic MHD reconnection process and microscale dissipations.","Plenty of high resolution observations indicate that the plasmoid-like structures exist in the high temperature solar corona, but such evidences are very rare in the lower solar atmosphere with partially ionized plasmas.","Utilizing joint observations from the Goode Solar Telescope (GST) and the Solar Dynamics Observatory (SDO), we discovered a small-scale eruptive phenomenon in NOAA AR 13085, characterized by clear reconnection cusp structures, supported by Nonlinear Force-Free Field (NLFFF) extrapolation results.","The plasmoid-like structures with a size about 150 km were observed to be ejected downward from the current sheet at a maximum velocity of 24 km$\\cdot$s$^{-1}$ in the H$\\alpha$ line wing images, followed by enhanced emissions at around the post flare loop region in multiple wave lengths.","Our 2.5D high-resolution MHD simulations further reproduced such a phenomenon and revealed reconnection fine structures.","These results provide comprehensive evidences for the plasmoid mediated reconnection in partially ionized plasmas, and suggest an unified reconnection model for solar flares with different length scales from the lower chromosphere to corona."],"url":"http://arxiv.org/abs/2404.18983v1","category":"astro-ph.SR"}
{"created":"2024-04-29 14:52:38","title":"Computational Job Market Analysis with Natural Language Processing","abstract":"[Abridged Abstract]   Recent technological advances underscore labor market dynamics, yielding significant consequences for employment prospects and increasing job vacancy data across platforms and languages. Aggregating such data holds potential for valuable insights into labor market demands, new skills emergence, and facilitating job matching for various stakeholders. However, despite prevalent insights in the private sector, transparent language technology systems and data for this domain are lacking. This thesis investigates Natural Language Processing (NLP) technology for extracting relevant information from job descriptions, identifying challenges including scarcity of training data, lack of standardized annotation guidelines, and shortage of effective extraction methods from job ads. We frame the problem, obtaining annotated data, and introducing extraction methodologies. Our contributions include job description datasets, a de-identification dataset, and a novel active learning algorithm for efficient model training. We propose skill extraction using weak supervision, a taxonomy-aware pre-training methodology adapting multilingual language models to the job market domain, and a retrieval-augmented model leveraging multiple skill extraction datasets to enhance overall performance. Finally, we ground extracted information within a designated taxonomy.","sentences":["[Abridged Abstract]   Recent technological advances underscore labor market dynamics, yielding significant consequences for employment prospects and increasing job vacancy data across platforms and languages.","Aggregating such data holds potential for valuable insights into labor market demands, new skills emergence, and facilitating job matching for various stakeholders.","However, despite prevalent insights in the private sector, transparent language technology systems and data for this domain are lacking.","This thesis investigates Natural Language Processing (NLP) technology for extracting relevant information from job descriptions, identifying challenges including scarcity of training data, lack of standardized annotation guidelines, and shortage of effective extraction methods from job ads.","We frame the problem, obtaining annotated data, and introducing extraction methodologies.","Our contributions include job description datasets, a de-identification dataset, and a novel active learning algorithm for efficient model training.","We propose skill extraction using weak supervision, a taxonomy-aware pre-training methodology adapting multilingual language models to the job market domain, and a retrieval-augmented model leveraging multiple skill extraction datasets to enhance overall performance.","Finally, we ground extracted information within a designated taxonomy."],"url":"http://arxiv.org/abs/2404.18977v1","category":"cs.CL"}
{"created":"2024-04-29 05:55:23","title":"An Aggregation-Free Federated Learning for Tackling Data Heterogeneity","abstract":"The performance of Federated Learning (FL) hinges on the effectiveness of utilizing knowledge from distributed datasets. Traditional FL methods adopt an aggregate-then-adapt framework, where clients update local models based on a global model aggregated by the server from the previous training round. This process can cause client drift, especially with significant cross-client data heterogeneity, impacting model performance and convergence of the FL algorithm. To address these challenges, we introduce FedAF, a novel aggregation-free FL algorithm. In this framework, clients collaboratively learn condensed data by leveraging peer knowledge, the server subsequently trains the global model using the condensed data and soft labels received from the clients. FedAF inherently avoids the issue of client drift, enhances the quality of condensed data amid notable data heterogeneity, and improves the global model performance. Extensive numerical studies on several popular benchmark datasets show FedAF surpasses various state-of-the-art FL algorithms in handling label-skew and feature-skew data heterogeneity, leading to superior global model accuracy and faster convergence.","sentences":["The performance of Federated Learning (FL) hinges on the effectiveness of utilizing knowledge from distributed datasets.","Traditional FL methods adopt an aggregate-then-adapt framework, where clients update local models based on a global model aggregated by the server from the previous training round.","This process can cause client drift, especially with significant cross-client data heterogeneity, impacting model performance and convergence of the FL algorithm.","To address these challenges, we introduce FedAF, a novel aggregation-free FL algorithm.","In this framework, clients collaboratively learn condensed data by leveraging peer knowledge, the server subsequently trains the global model using the condensed data and soft labels received from the clients.","FedAF inherently avoids the issue of client drift, enhances the quality of condensed data amid notable data heterogeneity, and improves the global model performance.","Extensive numerical studies on several popular benchmark datasets show FedAF surpasses various state-of-the-art FL algorithms in handling label-skew and feature-skew data heterogeneity, leading to superior global model accuracy and faster convergence."],"url":"http://arxiv.org/abs/2404.18962v1","category":"cs.CV"}
{"created":"2024-04-30 17:11:12","title":"The lazy (NTK) and rich ($\u03bc$P) regimes: a gentle tutorial","abstract":"A central theme of the modern machine learning paradigm is that larger neural networks achieve better performance on a variety of metrics. Theoretical analyses of these overparameterized models have recently centered around studying very wide neural networks. In this tutorial, we provide a nonrigorous but illustrative derivation of the following fact: in order to train wide networks effectively, there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights. This degree of freedom controls the richness of training behavior: at minimum, the wide network trains lazily like a kernel machine, and at maximum, it exhibits feature learning in the so-called $\\mu$P regime. In this paper, we explain this richness scale, synthesize recent research results into a coherent whole, offer new perspectives and intuitions, and provide empirical evidence supporting our claims. In doing so, we hope to encourage further study of the richness scale, as it may be key to developing a scientific theory of feature learning in practical deep neural networks.","sentences":["A central theme of the modern machine learning paradigm is that larger neural networks achieve better performance on a variety of metrics.","Theoretical analyses of these overparameterized models have recently centered around studying very wide neural networks.","In this tutorial, we provide a nonrigorous but illustrative derivation of the following fact: in order to train wide networks effectively, there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights.","This degree of freedom controls the richness of training behavior: at minimum, the wide network trains lazily like a kernel machine, and at maximum, it exhibits feature learning in the so-called $\\mu$P regime.","In this paper, we explain this richness scale, synthesize recent research results into a coherent whole, offer new perspectives and intuitions, and provide empirical evidence supporting our claims.","In doing so, we hope to encourage further study of the richness scale, as it may be key to developing a scientific theory of feature learning in practical deep neural networks."],"url":"http://arxiv.org/abs/2404.19719v1","category":"cs.LG"}
{"created":"2024-04-30 16:29:44","title":"Continuum limit of $p$-biharmonic equations on graphs","abstract":"This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph. The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity. We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions. The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations. The $L^\\infty$ estimates of solutions are also obtained as a byproduct.","sentences":["This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph.","The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity.","We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions.","The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations.","The $L^\\infty$ estimates of solutions are also obtained as a byproduct."],"url":"http://arxiv.org/abs/2404.19689v1","category":"math.AP"}
{"created":"2024-04-30 16:26:23","title":"On the lack of selection for the transport equation over a dense set of vector fields","abstract":"The purpose of this work is to demonstrate that the lack of selection by smooth regularisation for the continuity equation with a bounded, divergence-free vector field as demonstrated in \\cite{DeLellis_Giri22} by De Lellis and Giri takes place over a dense set of vector fields. More precisely, we construct a set of bounded vector fields $D$ dense in $L^p_{loc}([0,2]\\times \\R^2;\\R^2)$ such that for each vector field $\\bb\\in D$, there are two smooth regularisations of $\\bb$, for which the unique solution of the Cauchy problem for the continuity equation along each regularisation converges to two distinct solutions of the Cauchy problem along $\\bb$.","sentences":["The purpose of this work is to demonstrate that the lack of selection by smooth regularisation for the continuity equation with a bounded, divergence-free vector field as demonstrated in \\cite{DeLellis_Giri22} by De Lellis and Giri takes place over a dense set of vector fields.","More precisely, we construct a set of bounded vector fields $D$ dense in $L^p_{loc}([0,2]\\times \\R^2;\\R^2)$ such that for each vector field $\\bb\\in D$, there are two smooth regularisations of $\\bb$, for which the unique solution of the Cauchy problem for the continuity equation along each regularisation converges to two distinct solutions of the Cauchy problem along $\\bb$."],"url":"http://arxiv.org/abs/2404.19687v1","category":"math.AP"}
{"created":"2024-04-30 16:06:04","title":"Neural Controlled Differential Equations with Quantum Hidden Evolutions","abstract":"We introduce a class of neural controlled differential equation inspired by quantum mechanics. Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schr\\\"{o}dinger equation. Specifically, the hidden state represents the wave function, and its collapse leads to an interpretation of the classification probability. We implement and compare the results of four variants of NQDEs on a toy spiral classification problem.","sentences":["We introduce a class of neural controlled differential equation inspired by quantum mechanics.","Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schr\\\"{o}dinger equation.","Specifically, the hidden state represents the wave function, and its collapse leads to an interpretation of the classification probability.","We implement and compare the results of four variants of NQDEs on a toy spiral classification problem."],"url":"http://arxiv.org/abs/2404.19673v1","category":"cs.LG"}
{"created":"2024-04-30 15:35:27","title":"Thermal Fluid Closures and Pressure Anisotropies in Numerical Simulations of Plasma Wakefield Acceleration","abstract":"We investigate the dynamics of plasma-based acceleration processes with collisionless particle dynamics and non negligible thermal effects. We aim at assessing the applicability of fluid-like models, obtained by suitable closure assumptions applied to the relativistic kinetic equations, thus not suffering of statistical noise, even in presence of a finite temperature. The work here presented focuses on the characterization of pressure anisotropies, which crucially depend on the adopted closure scheme, and hence are useful to discern the appropriate thermal fluid model. To this aim, simulation results of spatially resolved fluid models with different thermal closure assumptions are compared with the results of particle-in-cell (PIC) simulations at changing temperature and amplitude of plasma oscillations.","sentences":["We investigate the dynamics of plasma-based acceleration processes with collisionless particle dynamics and non negligible thermal effects.","We aim at assessing the applicability of fluid-like models, obtained by suitable closure assumptions applied to the relativistic kinetic equations, thus not suffering of statistical noise, even in presence of a finite temperature.","The work here presented focuses on the characterization of pressure anisotropies, which crucially depend on the adopted closure scheme, and hence are useful to discern the appropriate thermal fluid model.","To this aim, simulation results of spatially resolved fluid models with different thermal closure assumptions are compared with the results of particle-in-cell (PIC) simulations at changing temperature and amplitude of plasma oscillations."],"url":"http://arxiv.org/abs/2404.19635v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 14:55:57","title":"Data-Driven Invertible Neural Surrogates of Atmospheric Transmission","abstract":"We present a framework for inferring an atmospheric transmission profile from a spectral scene. This framework leverages a lightweight, physics-based simulator that is automatically tuned - by virtue of autodifferentiation and differentiable programming - to construct a surrogate atmospheric profile to model the observed data. We demonstrate utility of the methodology by (i) performing atmospheric correction, (ii) recasting spectral data between various modalities (e.g. radiance and reflectance at the surface and at the sensor), and (iii) inferring atmospheric transmission profiles, such as absorbing bands and their relative magnitudes.","sentences":["We present a framework for inferring an atmospheric transmission profile from a spectral scene.","This framework leverages a lightweight, physics-based simulator that is automatically tuned - by virtue of autodifferentiation and differentiable programming - to construct a surrogate atmospheric profile to model the observed data.","We demonstrate utility of the methodology by (i) performing atmospheric correction, (ii) recasting spectral data between various modalities (e.g. radiance and reflectance at the surface and at the sensor), and (iii) inferring atmospheric transmission profiles, such as absorbing bands and their relative magnitudes."],"url":"http://arxiv.org/abs/2404.19605v1","category":"cs.LG"}
{"created":"2024-04-30 14:21:27","title":"Catalan percolation","abstract":"In Catalan percolation, all nearest-neighbor edges $\\{i,i+1\\}$ along $\\mathbb Z$ are initially occupied, and all other edges are open independently with probability $p$. Open edges $\\{i,j\\}$ are occupied if some pair of edges $\\{i,k\\}$ and $\\{k,j\\}$, with $i<k<j$, become occupied. This model was introduced by Gravner and the third author, in the context of polluted graph bootstrap percolation.   We prove that the critical $p_{\\mathrm c}$ is strictly between that of oriented site percolation on $\\mathbb Z^2$ and the Catalan growth rate $1/4$. Our main result shows that an enhanced oriented percolation model, with non-decaying infinite-range dependency, has a strictly smaller critical parameter than the classical model. This is reminiscent of the work of Duminil-Copin, Hil\\'ario, Kozma and Sidoravicius on brochette percolation. Our proof differs, however, in that we do not use Aizenman--Grimmett enhancements or differential inequalities. Two key ingredients are the work of Hil\\'ario, S\\'a, Sanchis and Teixeira on stretched lattices, and the Russo--Seymour--Welsh result for oriented percolation by Duminil-Copin, Tassion and Teixeira.","sentences":["In Catalan percolation, all nearest-neighbor edges $\\{i,i+1\\}$ along $\\mathbb Z$ are initially occupied, and all other edges are open independently with probability $p$. Open edges $\\{i,j\\}$ are occupied if some pair of edges $\\{i,k\\}$ and $\\{k,j\\}$, with $i<k<j$, become occupied.","This model was introduced by Gravner and the third author, in the context of polluted graph bootstrap percolation.   ","We prove that the critical $p_{\\mathrm c}$ is strictly between that of oriented site percolation on $\\mathbb Z^2$ and the Catalan growth rate $1/4$. Our main result shows that an enhanced oriented percolation model, with non-decaying infinite-range dependency, has a strictly smaller critical parameter than the classical model.","This is reminiscent of the work of Duminil-Copin, Hil\\'ario, Kozma and Sidoravicius on brochette percolation.","Our proof differs, however, in that we do not use Aizenman--Grimmett enhancements or differential inequalities.","Two key ingredients are the work of Hil\\'ario, S\\'a, Sanchis and Teixeira on stretched lattices, and the Russo--Seymour--Welsh result for oriented percolation by Duminil-Copin, Tassion and Teixeira."],"url":"http://arxiv.org/abs/2404.19583v1","category":"math.PR"}
{"created":"2024-04-30 14:11:10","title":"Topology in a Su--Schrieffer--Heeger plasmonic crystal","abstract":"In this paper we study the topology of the bands of a polaritonic crystal composed of graphene and of a metallic grating. We first derive a Kronig--Penney type of equation for the polaritonic bands as function of the Bloch wavevector and discuss the propagation of the surface plasmon polaritons on the polaritonic crystal using a transfer-matrix approach. Secondly, we reformulate the problem as a tight-binding model that resembles the Su--Schrieffer-Heeger (SSH) Hamiltonian, one difference being that the hopping amplitudes are, in this case, energy dependent. In possession of the tight-binding equations it is a simple task to determine the topology of the bands. Similarly to the SSH model, we show that there is a tunable parameter that induces topological phase transitions from trivial to non-trivial. In our case, it is the distance $d$ between the graphene sheet and the metallic grating. We note that $d$ is a parameter that can be easily tuned experimentally simply by controlling the thickness of the spacer between the grating and the graphene sheet. It is then experimentally feasible to engineer devices with the required topological properties.","sentences":["In this paper we study the topology of the bands of a polaritonic crystal composed of graphene and of a metallic grating.","We first derive a Kronig--Penney type of equation for the polaritonic bands as function of the Bloch wavevector and discuss the propagation of the surface plasmon polaritons on the polaritonic crystal using a transfer-matrix approach.","Secondly, we reformulate the problem as a tight-binding model that resembles the Su--Schrieffer-Heeger (SSH) Hamiltonian, one difference being that the hopping amplitudes are, in this case, energy dependent.","In possession of the tight-binding equations it is a simple task to determine the topology of the bands.","Similarly to the SSH model, we show that there is a tunable parameter that induces topological phase transitions from trivial to non-trivial.","In our case, it is the distance $d$ between the graphene sheet and the metallic grating.","We note that $d$ is a parameter that can be easily tuned experimentally simply by controlling the thickness of the spacer between the grating and the graphene sheet.","It is then experimentally feasible to engineer devices with the required topological properties."],"url":"http://arxiv.org/abs/2404.19576v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 13:55:30","title":"Causal Perception Inspired Representation Learning for Trustworthy Image Quality Assessment","abstract":"Despite great success in modeling visual perception, deep neural network based image quality assessment (IQA) still remains unreliable in real-world applications due to its vulnerability to adversarial perturbations and the inexplicit black-box structure. In this paper, we propose to build a trustworthy IQA model via Causal Perception inspired Representation Learning (CPRL), and a score reflection attack method for IQA model. More specifically, we assume that each image is composed of Causal Perception Representation (CPR) and non-causal perception representation (N-CPR). CPR serves as the causation of the subjective quality label, which is invariant to the imperceptible adversarial perturbations. Inversely, N-CPR presents spurious associations with the subjective quality label, which may significantly change with the adversarial perturbations. To extract the CPR from each input image, we develop a soft ranking based channel-wise activation function to mediate the causally sufficient (beneficial for high prediction accuracy) and necessary (beneficial for high robustness) deep features, and based on intervention employ minimax game to optimize. Experiments on four benchmark databases show that the proposed CPRL method outperforms many state-of-the-art adversarial defense methods and provides explicit model interpretation.","sentences":["Despite great success in modeling visual perception, deep neural network based image quality assessment (IQA) still remains unreliable in real-world applications due to its vulnerability to adversarial perturbations and the inexplicit black-box structure.","In this paper, we propose to build a trustworthy IQA model via Causal Perception inspired Representation Learning (CPRL), and a score reflection attack method for IQA model.","More specifically, we assume that each image is composed of Causal Perception Representation (CPR) and non-causal perception representation (N-CPR).","CPR serves as the causation of the subjective quality label, which is invariant to the imperceptible adversarial perturbations.","Inversely, N-CPR presents spurious associations with the subjective quality label, which may significantly change with the adversarial perturbations.","To extract the CPR from each input image, we develop a soft ranking based channel-wise activation function to mediate the causally sufficient (beneficial for high prediction accuracy) and necessary (beneficial for high robustness) deep features, and based on intervention employ minimax game to optimize.","Experiments on four benchmark databases show that the proposed CPRL method outperforms many state-of-the-art adversarial defense methods and provides explicit model interpretation."],"url":"http://arxiv.org/abs/2404.19567v1","category":"cs.CV"}
{"created":"2024-04-30 13:39:26","title":"Neural Dynamic Data Valuation","abstract":"Data constitute the foundational component of the data economy and its marketplaces. Efficient and fair data valuation has emerged as a topic of significant interest.\\ Many approaches based on marginal contribution have shown promising results in various downstream tasks. However, they are well known to be computationally expensive as they require training a large number of utility functions, which are used to evaluate the usefulness or value of a given dataset for a specific purpose. As a result, it has been recognized as infeasible to apply these methods to a data marketplace involving large-scale datasets. Consequently, a critical issue arises: how can the re-training of the utility function be avoided? To address this issue, we propose a novel data valuation method from the perspective of optimal control, named the neural dynamic data valuation (NDDV). Our method has solid theoretical interpretations to accurately identify the data valuation via the sensitivity of the data optimal control state. In addition, we implement a data re-weighting strategy to capture the unique features of data points, ensuring fairness through the interaction between data points and the mean-field states. Notably, our method requires only training once to estimate the value of all data points, significantly improving the computational efficiency. We conduct comprehensive experiments using different datasets and tasks. The results demonstrate that the proposed NDDV method outperforms the existing state-of-the-art data valuation methods in accurately identifying data points with either high or low values and is more computationally efficient.","sentences":["Data constitute the foundational component of the data economy and its marketplaces.","Efficient and fair data valuation has emerged as a topic of significant interest.\\ Many approaches based on marginal contribution have shown promising results in various downstream tasks.","However, they are well known to be computationally expensive as they require training a large number of utility functions, which are used to evaluate the usefulness or value of a given dataset for a specific purpose.","As a result, it has been recognized as infeasible to apply these methods to a data marketplace involving large-scale datasets.","Consequently, a critical issue arises: how can the re-training of the utility function be avoided?","To address this issue, we propose a novel data valuation method from the perspective of optimal control, named the neural dynamic data valuation (NDDV).","Our method has solid theoretical interpretations to accurately identify the data valuation via the sensitivity of the data optimal control state.","In addition, we implement a data re-weighting strategy to capture the unique features of data points, ensuring fairness through the interaction between data points and the mean-field states.","Notably, our method requires only training once to estimate the value of all data points, significantly improving the computational efficiency.","We conduct comprehensive experiments using different datasets and tasks.","The results demonstrate that the proposed NDDV method outperforms the existing state-of-the-art data valuation methods in accurately identifying data points with either high or low values and is more computationally efficient."],"url":"http://arxiv.org/abs/2404.19557v1","category":"stat.ML"}
{"created":"2024-04-30 12:36:28","title":"Well-posedness of McKean-Vlasov SDEs with density-dependent drift","abstract":"In this paper, we study the well-posedness of McKean-Vlasov stochastic differential equations (SDE) whose drift depends pointwisely on marginal density and satisfies a condition about local integrability in time-space variables. The drift is assumed to be Lipschitz continuous in distribution variable with respect to Wasserstein metric $W_p$. Our approach is by approximation with mollified SDEs. We establish a new estimate about H{\\\"o}lder continuity in time of marginal density. Then we deduce that the marginal distributions (resp. marginal densities) of the mollified SDEs converge in $W_p$ (resp. topology of compact convergence) to the solution of the Fokker-Planck equation associated with the density-dependent SDE. We prove strong existence of a solution. Weak and strong uniqueness are obtained when $p=1$, the drift coefficient is bounded, and the diffusion coefficient is distribution free.","sentences":["In this paper, we study the well-posedness of McKean-Vlasov stochastic differential equations (SDE) whose drift depends pointwisely on marginal density and satisfies a condition about local integrability in time-space variables.","The drift is assumed to be Lipschitz continuous in distribution variable with respect to Wasserstein metric $W_p$. Our approach is by approximation with mollified SDEs.","We establish a new estimate about H{\\\"o}lder continuity in time of marginal density.","Then we deduce that the marginal distributions (resp.","marginal densities) of the mollified SDEs converge in $W_p$ (resp.","topology of compact convergence) to the solution of the Fokker-Planck equation associated with the density-dependent SDE.","We prove strong existence of a solution.","Weak and strong uniqueness are obtained when $p=1$, the drift coefficient is bounded, and the diffusion coefficient is distribution free."],"url":"http://arxiv.org/abs/2404.19499v1","category":"math.PR"}
{"created":"2024-04-30 12:22:50","title":"Fokker-Planck equation for McKean-Vlasov SPDEs driven by time-space Brownian sheet","abstract":"In this paper, we consider a McKean-Vlasov (mean-field) stochastic partial differential equations (SPDEs) driven by a Brownian sheet. We study the propagation of chaos for a space-time Ornstein-Uhlenbeck SPDE type. Subsequently, we prove the existence and uniqueness of a nonlinear McKean-Vlasov SPDE. Finally, we establish a Fokker-Planck equation for the law of the solution of the McKean-Vlasov type SPDE driven by a time-space Brownian sheet, and we provide some examples to illustrate the results obtained.","sentences":["In this paper, we consider a McKean-Vlasov (mean-field) stochastic partial differential equations (SPDEs) driven by a Brownian sheet.","We study the propagation of chaos for a space-time Ornstein-Uhlenbeck SPDE type.","Subsequently, we prove the existence and uniqueness of a nonlinear McKean-Vlasov SPDE.","Finally, we establish a Fokker-Planck equation for the law of the solution of the McKean-Vlasov type SPDE driven by a time-space Brownian sheet, and we provide some examples to illustrate the results obtained."],"url":"http://arxiv.org/abs/2404.19490v1","category":"math.PR"}
{"created":"2024-04-30 08:48:07","title":"Navigating Brain Language Representations: A Comparative Analysis of Neural Language Models and Psychologically Plausible Models","abstract":"Neural language models, particularly large-scale ones, have been consistently proven to be most effective in predicting brain neural activity across a range of studies. However, previous research overlooked the comparison of these models with psychologically plausible ones. Moreover, evaluations were reliant on limited, single-modality, and English cognitive datasets. To address these questions, we conducted an analysis comparing encoding performance of various neural language models and psychologically plausible models. Our study utilized extensive multi-modal cognitive datasets, examining bilingual word and discourse levels. Surprisingly, our findings revealed that psychologically plausible models outperformed neural language models across diverse contexts, encompassing different modalities such as fMRI and eye-tracking, and spanning languages from English to Chinese. Among psychologically plausible models, the one incorporating embodied information emerged as particularly exceptional. This model demonstrated superior performance at both word and discourse levels, exhibiting robust prediction of brain activation across numerous regions in both English and Chinese.","sentences":["Neural language models, particularly large-scale ones, have been consistently proven to be most effective in predicting brain neural activity across a range of studies.","However, previous research overlooked the comparison of these models with psychologically plausible ones.","Moreover, evaluations were reliant on limited, single-modality, and English cognitive datasets.","To address these questions, we conducted an analysis comparing encoding performance of various neural language models and psychologically plausible models.","Our study utilized extensive multi-modal cognitive datasets, examining bilingual word and discourse levels.","Surprisingly, our findings revealed that psychologically plausible models outperformed neural language models across diverse contexts, encompassing different modalities such as fMRI and eye-tracking, and spanning languages from English to Chinese.","Among psychologically plausible models, the one incorporating embodied information emerged as particularly exceptional.","This model demonstrated superior performance at both word and discourse levels, exhibiting robust prediction of brain activation across numerous regions in both English and Chinese."],"url":"http://arxiv.org/abs/2404.19364v1","category":"cs.CL"}
{"created":"2024-04-30 07:59:47","title":"Super-resolution by converting evanescent waves in microsphere to propagating and transfer function from its surface to nano-jet","abstract":"The EM waves transmitted through a thin object with fine structures is observed, by microsphere located above the object. While the waves include both evanescent and propagating waves, the high resolution is obtained by the evanescent ones, including the information on the fine structures of the object. Description of this process is divided into two parts: a) The super resolution is analyzed by using Helmholtz equation for the evanescent waves transmitted from the object to the microsphere surface. b) Using boundary condition, the electric fields on the inner surface of the microsphere includes both the evanescent and propagating waves. The transmission of these waves to a nano jet is produced by a transfer function, including convolution between the spatial modes of the evanescent waves with those of the microsphere, which increases the conversion of the evanescent waves to propagating waves, and thus increase the resolution.","sentences":["The EM waves transmitted through a thin object with fine structures is observed, by microsphere located above the object.","While the waves include both evanescent and propagating waves, the high resolution is obtained by the evanescent ones, including the information on the fine structures of the object.","Description of this process is divided into two parts: a)","The super resolution is analyzed by using Helmholtz equation for the evanescent waves transmitted from the object to the microsphere surface.","b) Using boundary condition, the electric fields on the inner surface of the microsphere includes both the evanescent and propagating waves.","The transmission of these waves to a nano jet is produced by a transfer function, including convolution between the spatial modes of the evanescent waves with those of the microsphere, which increases the conversion of the evanescent waves to propagating waves, and thus increase the resolution."],"url":"http://arxiv.org/abs/2404.19333v1","category":"physics.optics"}
{"created":"2024-04-30 06:55:12","title":"Collisional dynamics of symmetric two-dimensional quantum droplets","abstract":"The collisional dynamics of two symmetric droplets with equal intraspecies scattering lengths and particle number density for each component is studied by solving the corresponding extended Gross-Pitaevskii equation in two dimensions by including a logarithmic correction term in the usual contact interaction. We find the merging droplet after collision experiences a quadrupole oscillation in its shape and the oscillation period is found to be independent of the incidental momentum for small droplets. With increasing collision momentum the colliding droplets may separate into two, or even more, and finally into small pieces of droplets. For these dynamical phases, we manage to present boundaries determined by the remnant particle number in the central area and the damped oscillation of the quadrupole mode. A stability peak for the existence of droplets emerges at the critical particle number $N_c \\simeq 48$ for the quasi-Gaussian and flat-top shapes of the droplets.","sentences":["The collisional dynamics of two symmetric droplets with equal intraspecies scattering lengths and particle number density for each component is studied by solving the corresponding extended Gross-Pitaevskii equation in two dimensions by including a logarithmic correction term in the usual contact interaction.","We find the merging droplet after collision experiences a quadrupole oscillation in its shape and the oscillation period is found to be independent of the incidental momentum for small droplets.","With increasing collision momentum the colliding droplets may separate into two, or even more, and finally into small pieces of droplets.","For these dynamical phases, we manage to present boundaries determined by the remnant particle number in the central area and the damped oscillation of the quadrupole mode.","A stability peak for the existence of droplets emerges at the critical particle number $N_c \\simeq 48$ for the quasi-Gaussian and flat-top shapes of the droplets."],"url":"http://arxiv.org/abs/2404.19295v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 05:18:06","title":"Flow by Gauss Curvature to the Orlicz Minkowski Problem for q-torsional rigidity","abstract":"The Minkowski problem for torsional rigidity ($2$-torsional rigidity) was firstly studied by Colesanti and Fimiani \\cite{CA} using variational method. Moreover, Hu, Liu and Ma \\cite{HJ00} also studied this problem by method of curvature flow and obtain the existence of smooth solution. In addition, the Minkowski problem for $2$-torsional rigidity was also extended to $L_p$ version and Orlicz version.   Recently, Hu and Zhang \\cite{HJ2} introduced the concept of Orlicz mixed $q$-torsional rigidity and obtained Orlicz $q$-torsional measure through the variational method for $q>1$. Specially, they established the functional Orlicz Brunn-Minkowski inequality and the functional Orlicz Minkowski inequality.   Motivated by the remarkable work by Hu and Zhang in \\cite{HJ2}, we can propose the Orlicz Minkowksi problem for $q$-torsional rigidity, and then confirm the existence of smooth even solutions to the Orlicz Minkowski problem for $q$-torsional rigidity with $q>1$ by method of a Gauss curvature flow.","sentences":["The Minkowski problem for torsional rigidity ($2$-torsional rigidity) was firstly studied by Colesanti and Fimiani \\cite{CA} using variational method.","Moreover, Hu, Liu and Ma \\cite{HJ00} also studied this problem by method of curvature flow and obtain the existence of smooth solution.","In addition, the Minkowski problem for $2$-torsional rigidity was also extended to $L_p$ version and Orlicz version.   ","Recently, Hu and Zhang \\cite{HJ2} introduced the concept of Orlicz mixed $q$-torsional rigidity and obtained Orlicz $q$-torsional measure through the variational method for $q>1$. Specially, they established the functional Orlicz Brunn-Minkowski inequality and the functional Orlicz Minkowski inequality.   ","Motivated by the remarkable work by Hu and Zhang in \\cite{HJ2}, we can propose the Orlicz Minkowksi problem for $q$-torsional rigidity, and then confirm the existence of smooth even solutions to the Orlicz Minkowski problem for $q$-torsional rigidity with $q>1$ by method of a Gauss curvature flow."],"url":"http://arxiv.org/abs/2404.19266v1","category":"math.DG"}
{"created":"2024-04-30 04:54:15","title":"High dimensional analysis reveals conservative sharpening and a stochastic edge of stability","abstract":"Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization.","sentences":["Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime.","There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability.","Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening.","We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown.","We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues.","We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization."],"url":"http://arxiv.org/abs/2404.19261v1","category":"cs.LG"}
{"created":"2024-04-30 03:55:19","title":"Central elements of the degenerate quantum general linear group","abstract":"We construct central elements of the degenerate quantum general linear group introduced by Cheng, Wang and Zhang. In particular, we give an explicit formula for the quantum Casimir element. Our method is based on the explicit $L$ operators. Moreover, we construct a universal $L$ operator, which is a spectral parameter-dependent solution of the quantum Yang-Baxter equation in the tensor product of the degenerate quantum general linear group and the endomorphism ring of its natural representation. This construction leads to the FRT approach to the degenerate quantum general linear group.","sentences":["We construct central elements of the degenerate quantum general linear group introduced by Cheng, Wang and Zhang.","In particular, we give an explicit formula for the quantum Casimir element.","Our method is based on the explicit $L$ operators.","Moreover, we construct a universal $L$ operator, which is a spectral parameter-dependent solution of the quantum Yang-Baxter equation in the tensor product of the degenerate quantum general linear group and the endomorphism ring of its natural representation.","This construction leads to the FRT approach to the degenerate quantum general linear group."],"url":"http://arxiv.org/abs/2404.19239v1","category":"math.QA"}
{"created":"2024-04-30 03:06:50","title":"Electromagnetic field and chaotic charged-particle motion around hairy black holes in Horndeski gravity","abstract":"The Wald vector potential is an exact solution of the source-less Maxwell equations regarding an electromagnetic field of a vacuum uncharged black hole like the Kerr background black hole in an asymptotically uniform magnetic field. However, it is not if the black hole is a nonvacuum solution in a theory of modified gravity with extra fields or a charged Kerr-Newman spacetime. To satisfy the source-less Maxwell equations in this case, the Wald vector potential must be modified and generalized appropriately. Following this idea, we derive an expression for the vector potential of an electromagnetic field surrounding a hairy black hole in the Horndeski modified gravity theory. Explicit symplectic integrators with excellent long-term behaviour are used to simulate the motion of charged particles around the hairy black hole immersed in the external magnetic field. The recurrence plot method based on the recurrence quantification analysis uses diagonal structures parallel to the main diagonal to show regular dynamics, but adopts no diagonal structures to indicate chaotic dynamics. The method is efficient to detect chaos from order in the curved spacetime, as the Poincare map and the fast Lyapunov indicator are.","sentences":["The Wald vector potential is an exact solution of the source-less Maxwell equations regarding an electromagnetic field of a vacuum uncharged black hole like the Kerr background black hole in an asymptotically uniform magnetic field.","However, it is not if the black hole is a nonvacuum solution in a theory of modified gravity with extra fields or a charged Kerr-Newman spacetime.","To satisfy the source-less Maxwell equations in this case, the Wald vector potential must be modified and generalized appropriately.","Following this idea, we derive an expression for the vector potential of an electromagnetic field surrounding a hairy black hole in the Horndeski modified gravity theory.","Explicit symplectic integrators with excellent long-term behaviour are used to simulate the motion of charged particles around the hairy black hole immersed in the external magnetic field.","The recurrence plot method based on the recurrence quantification analysis uses diagonal structures parallel to the main diagonal to show regular dynamics, but adopts no diagonal structures to indicate chaotic dynamics.","The method is efficient to detect chaos from order in the curved spacetime, as the Poincare map and the fast Lyapunov indicator are."],"url":"http://arxiv.org/abs/2404.19225v1","category":"gr-qc"}
{"created":"2024-04-30 02:30:05","title":"EfficientASR: Speech Recognition Network Compression via Attention Redundancy and Chunk-Level FFN Optimization","abstract":"In recent years, Transformer networks have shown remarkable performance in speech recognition tasks. However, their deployment poses challenges due to high computational and storage resource requirements. To address this issue, a lightweight model called EfficientASR is proposed in this paper, aiming to enhance the versatility of Transformer models. EfficientASR employs two primary modules: Shared Residual Multi-Head Attention (SRMHA) and Chunk-Level Feedforward Networks (CFFN). The SRMHA module effectively reduces redundant computations in the network, while the CFFN module captures spatial knowledge and reduces the number of parameters. The effectiveness of the EfficientASR model is validated on two public datasets, namely Aishell-1 and HKUST. Experimental results demonstrate a 36% reduction in parameters compared to the baseline Transformer network, along with improvements of 0.3% and 0.2% in Character Error Rate (CER) on the Aishell-1 and HKUST datasets, respectively.","sentences":["In recent years, Transformer networks have shown remarkable performance in speech recognition tasks.","However, their deployment poses challenges due to high computational and storage resource requirements.","To address this issue, a lightweight model called EfficientASR is proposed in this paper, aiming to enhance the versatility of Transformer models.","EfficientASR employs two primary modules: Shared Residual Multi-Head Attention (SRMHA) and Chunk-Level Feedforward Networks (CFFN).","The SRMHA module effectively reduces redundant computations in the network, while the CFFN module captures spatial knowledge and reduces the number of parameters.","The effectiveness of the EfficientASR model is validated on two public datasets, namely Aishell-1 and HKUST.","Experimental results demonstrate a 36% reduction in parameters compared to the baseline Transformer network, along with improvements of 0.3% and 0.2% in Character Error Rate (CER) on the Aishell-1 and HKUST datasets, respectively."],"url":"http://arxiv.org/abs/2404.19214v1","category":"cs.SD"}
{"created":"2024-04-30 02:29:38","title":"Nonexistence of Majorana fermions in Kerr-Newman type spacetimes with nontrivial charge","abstract":"In this short paper, we study Majorana fermions in terms of a modified Chandrasekhar's separation for the Dirac equation in Kerr-Newman-type spacetimes. If the electric charge or magnetic charge is nonzero, we show that nontrivial differentiable time-periodic Majorana fermions do not exist outside the event horizon in Kerr-Newman and Kerr-Newman-AdS spacetimes, and between the event horizon and the cosmological horizon in Kerr-Newman-dS spacetime.","sentences":["In this short paper, we study Majorana fermions in terms of a modified Chandrasekhar's separation for the Dirac equation in Kerr-Newman-type spacetimes.","If the electric charge or magnetic charge is nonzero, we show that nontrivial differentiable time-periodic Majorana fermions do not exist outside the event horizon in Kerr-Newman and Kerr-Newman-AdS spacetimes, and between the event horizon and the cosmological horizon in Kerr-Newman-dS spacetime."],"url":"http://arxiv.org/abs/2404.19213v1","category":"gr-qc"}
{"created":"2024-04-30 02:23:37","title":"EAD-VC: Enhancing Speech Auto-Disentanglement for Voice Conversion with IFUB Estimator and Joint Text-Guided Consistent Learning","abstract":"Using unsupervised learning to disentangle speech into content, rhythm, pitch, and timbre for voice conversion has become a hot research topic. Existing works generally take into account disentangling speech components through human-crafted bottleneck features which can not achieve sufficient information disentangling, while pitch and rhythm may still be mixed together. There is a risk of information overlap in the disentangling process which results in less speech naturalness. To overcome such limits, we propose a two-stage model to disentangle speech representations in a self-supervised manner without a human-crafted bottleneck design, which uses the Mutual Information (MI) with the designed upper bound estimator (IFUB) to separate overlapping information between speech components. Moreover, we design a Joint Text-Guided Consistent (TGC) module to guide the extraction of speech content and eliminate timbre leakage issues. Experiments show that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness, speech naturalness, and similarity. Audio samples can be found at https://largeaudiomodel.com/eadvc.","sentences":["Using unsupervised learning to disentangle speech into content, rhythm, pitch, and timbre for voice conversion has become a hot research topic.","Existing works generally take into account disentangling speech components through human-crafted bottleneck features which can not achieve sufficient information disentangling, while pitch and rhythm may still be mixed together.","There is a risk of information overlap in the disentangling process which results in less speech naturalness.","To overcome such limits, we propose a two-stage model to disentangle speech representations in a self-supervised manner without a human-crafted bottleneck design, which uses the Mutual Information (MI) with the designed upper bound estimator (IFUB) to separate overlapping information between speech components.","Moreover, we design a Joint Text-Guided Consistent (TGC) module to guide the extraction of speech content and eliminate timbre leakage issues.","Experiments show that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness, speech naturalness, and similarity.","Audio samples can be found at https://largeaudiomodel.com/eadvc."],"url":"http://arxiv.org/abs/2404.19212v1","category":"cs.SD"}
{"created":"2024-04-30 02:22:31","title":"AdaOper: Energy-efficient and Responsive Concurrent DNN Inference on Mobile Devices","abstract":"Deep neural network (DNN) has driven extensive applications in mobile technology. However, for long-running mobile apps like voice assistants or video applications on smartphones, energy efficiency is critical for battery-powered devices. The rise of heterogeneous processors in mobile devices today has introduced new challenges for optimizing energy efficiency. Our key insight is that partitioning computations across different processors for parallelism and speedup doesn't necessarily correlate with energy consumption optimization and may even increase it. To address this, we present AdaOper, an energy-efficient concurrent DNN inference system. It optimizes energy efficiency on mobile heterogeneous processors while maintaining responsiveness. AdaOper includes a runtime energy profiler that dynamically adjusts operator partitioning to optimize energy efficiency based on dynamic device conditions. We conduct preliminary experiments, which show that AdaOper reduces energy consumption by 16.88% compared to the existing concurrent method while ensuring real-time performance.","sentences":["Deep neural network (DNN) has driven extensive applications in mobile technology.","However, for long-running mobile apps like voice assistants or video applications on smartphones, energy efficiency is critical for battery-powered devices.","The rise of heterogeneous processors in mobile devices today has introduced new challenges for optimizing energy efficiency.","Our key insight is that partitioning computations across different processors for parallelism and speedup doesn't necessarily correlate with energy consumption optimization and may even increase it.","To address this, we present AdaOper, an energy-efficient concurrent DNN inference system.","It optimizes energy efficiency on mobile heterogeneous processors while maintaining responsiveness.","AdaOper includes a runtime energy profiler that dynamically adjusts operator partitioning to optimize energy efficiency based on dynamic device conditions.","We conduct preliminary experiments, which show that AdaOper reduces energy consumption by 16.88% compared to the existing concurrent method while ensuring real-time performance."],"url":"http://arxiv.org/abs/2404.19209v1","category":"cs.DC"}
{"created":"2024-04-30 01:45:45","title":"Updated observational constraints on spatially-flat and non-flat $\u039b$CDM and XCDM cosmological models","abstract":"We study 6 LCDM models, with 4 allowing for non-flat geometry and 3 allowing for a non-unity lensing consistency parameter $A_L$. We also study 6 XCDM models with a dynamical dark energy density X-fluid with equation of state $w$. For the non-flat models we use two different primordial power spectra, Planck $P(q)$ and new $P(q)$. These models are tested against: Planck 2018 CMB power spectra (P18) and lensing potential power spectrum (lensing), and an updated compilation of BAO, SNIa, $H(z)$, and $f\\sigma_8$ data [non-CMB data]. P18 data favor closed geometry for the LCDM and XCDM models and $w<-1$ (phantom-like dark energy) for the XCDM models while non-CMB data favor open geometry for the LCDM models and closed geometry and $w>-1$ (quintessence-like dark energy) for the XCDM models. When P18 and non-CMB data are jointly analyzed there is weak evidence for open geometry and moderate evidence for quintessence-like dark energy. Regardless of data used, $A_L>1$ is always favored. The XCDM model constraints obtained from CMB data and from non-CMB data are incompatible, ruling out the 3 $A_L = 1$ XCDM models at $> 3\\sigma$. In the 9 models not ruled out, for the P18+lensing+non-CMB data set we find little deviation from flat geometry and moderate deviation from $w=-1$. In all 6 non-flat models (not ruled out), open geometry is mildly favored, and in all 3 XCDM+$A_L$ models (not ruled out) quintessence-like dark energy is moderately favored (by at most $1.6 \\sigma$). In the $A_L = 1$ non-flat LCDM cases, we find for P18+lensing+non-CMB data $\\Omega_k = 0.0009 \\pm 0.0017$ [$0.0008 \\pm 0.0017$] for the Planck [new] $P(q)$ model, favoring open geometry. The flat LCDM model remains the simplest (largely) observationally-consistent cosmological model. Our cosmological parameter constraints obtained for the flat LCDM model (and other models) are the most restrictive results to date (Abridged).","sentences":["We study 6 LCDM models, with 4 allowing for non-flat geometry and 3 allowing for a non-unity lensing consistency parameter $A_L$. We also study 6 XCDM models with a dynamical dark energy density X-fluid with equation of state $w$. For the non-flat models we use two different primordial power spectra, Planck $P(q)$ and new $P(q)$.","These models are tested against: Planck 2018 CMB power spectra (P18) and lensing potential power spectrum (lensing), and an updated compilation of BAO, SNIa, $H(z)$, and $f\\sigma_8$ data [non-CMB data].","P18 data favor closed geometry for the LCDM and XCDM models and $w<-1$ (phantom-like dark energy) for the XCDM models while non-CMB data favor open geometry for the LCDM models and closed geometry and $w>-1$ (quintessence-like dark energy) for the XCDM models.","When P18 and non-CMB data are jointly analyzed there is weak evidence for open geometry and moderate evidence for quintessence-like dark energy.","Regardless of data used, $A_L>1$ is always favored.","The XCDM model constraints obtained from CMB data and from non-CMB data are incompatible, ruling out the 3 $A_L = 1$ XCDM models at $> 3\\sigma$. In the 9 models not ruled out, for the P18+lensing+non-CMB data set we find little deviation from flat geometry and moderate deviation from $w=-1$. In all 6 non-flat models (not ruled out), open geometry is mildly favored, and in all 3 XCDM+$A_L$ models (not ruled out) quintessence-like dark energy is moderately favored (by at most $1.6 \\sigma$).","In the $A_L = 1$ non-flat LCDM cases, we find for P18+lensing+non-CMB data $\\Omega_k = 0.0009 \\pm 0.0017$ [$0.0008 \\pm 0.0017$] for the Planck [new] $P(q)$ model, favoring open geometry.","The flat LCDM model remains the simplest (largely) observationally-consistent cosmological model.","Our cosmological parameter constraints obtained for the flat LCDM model (and other models) are the most restrictive results to date (Abridged)."],"url":"http://arxiv.org/abs/2404.19194v1","category":"astro-ph.CO"}
{"created":"2024-04-30 01:01:43","title":"The moduli space of left-invariant metrics on six-dimensional characteristically solvable nilmanifolds","abstract":"A real Lie algebra is said to be characteristically solvable if its derivation algebra is solvable. We explicitly determine the moduli space of left-invariant metrics, up to isometric automorphism, for $6$-dimensional nilmanifolds whose associated Lie algebra is characteristically solvable. We also compute the corresponding full isometry groups. For each left-invariant metric on these nilmanifolds we compute the index and distribution of symmetry. In particular, we find the first known examples of Lie groups which do not admit a left-invariant metric with positive index of symmetry. As an application we study the index of symmetry of nilsoliton metrics on characteristically solvable Lie algebras. We prove that nilsoliton metrics detect the existence of left-invariant metrics with positive index of symmetry.","sentences":["A real Lie algebra is said to be characteristically solvable if its derivation algebra is solvable.","We explicitly determine the moduli space of left-invariant metrics, up to isometric automorphism, for $6$-dimensional nilmanifolds whose associated Lie algebra is characteristically solvable.","We also compute the corresponding full isometry groups.","For each left-invariant metric on these nilmanifolds we compute the index and distribution of symmetry.","In particular, we find the first known examples of Lie groups which do not admit a left-invariant metric with positive index of symmetry.","As an application we study the index of symmetry of nilsoliton metrics on characteristically solvable Lie algebras.","We prove that nilsoliton metrics detect the existence of left-invariant metrics with positive index of symmetry."],"url":"http://arxiv.org/abs/2404.19177v1","category":"math.DG"}
{"created":"2024-04-30 00:37:55","title":"XFeat: Accelerated Features for Lightweight Image Matching","abstract":"We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.","sentences":["We introduce a lightweight and accurate architecture for resource-efficient visual correspondence.","Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features.","Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices.","In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network.","Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality.","Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors.","XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization.","We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations.","Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24."],"url":"http://arxiv.org/abs/2404.19174v1","category":"cs.CV"}
{"created":"2024-04-30 00:02:34","title":"DelGrad: Exact gradients in spiking networks for learning transmission delays and weights","abstract":"Spiking neural networks (SNNs) inherently rely on the timing of signals for representing and processing information. Transmission delays play an important role in shaping these temporal characteristics. Recent work has demonstrated the substantial advantages of learning these delays along with synaptic weights, both in terms of accuracy and memory efficiency. However, these approaches suffer from drawbacks in terms of precision and efficiency, as they operate in discrete time and with approximate gradients, while also requiring membrane potential recordings for calculating parameter updates. To alleviate these issues, we propose an analytical approach for calculating exact loss gradients with respect to both synaptic weights and delays in an event-based fashion. The inclusion of delays emerges naturally within our proposed formalism, enriching the model's search space with a temporal dimension. Our algorithm is purely based on the timing of individual spikes and does not require access to other variables such as membrane potentials. We explicitly compare the impact on accuracy and parameter efficiency of different types of delays - axonal, dendritic and synaptic. Furthermore, while previous work on learnable delays in SNNs has been mostly confined to software simulations, we demonstrate the functionality and benefits of our approach on the BrainScaleS-2 neuromorphic platform.","sentences":["Spiking neural networks (SNNs) inherently rely on the timing of signals for representing and processing information.","Transmission delays play an important role in shaping these temporal characteristics.","Recent work has demonstrated the substantial advantages of learning these delays along with synaptic weights, both in terms of accuracy and memory efficiency.","However, these approaches suffer from drawbacks in terms of precision and efficiency, as they operate in discrete time and with approximate gradients, while also requiring membrane potential recordings for calculating parameter updates.","To alleviate these issues, we propose an analytical approach for calculating exact loss gradients with respect to both synaptic weights and delays in an event-based fashion.","The inclusion of delays emerges naturally within our proposed formalism, enriching the model's search space with a temporal dimension.","Our algorithm is purely based on the timing of individual spikes and does not require access to other variables such as membrane potentials.","We explicitly compare the impact on accuracy and parameter efficiency of different types of delays - axonal, dendritic and synaptic.","Furthermore, while previous work on learnable delays in SNNs has been mostly confined to software simulations, we demonstrate the functionality and benefits of our approach on the BrainScaleS-2 neuromorphic platform."],"url":"http://arxiv.org/abs/2404.19165v1","category":"cs.NE"}
{"created":"2024-04-29 23:21:17","title":"Enhancing Brazilian Sign Language Recognition through Skeleton Image Representation","abstract":"Effective communication is paramount for the inclusion of deaf individuals in society. However, persistent communication barriers due to limited Sign Language (SL) knowledge hinder their full participation. In this context, Sign Language Recognition (SLR) systems have been developed to improve communication between signing and non-signing individuals. In particular, there is the problem of recognizing isolated signs (Isolated Sign Language Recognition, ISLR) of great relevance in the development of vision-based SL search engines, learning tools, and translation systems. This work proposes an ISLR approach where body, hands, and facial landmarks are extracted throughout time and encoded as 2-D images. These images are processed by a convolutional neural network, which maps the visual-temporal information into a sign label. Experimental results demonstrate that our method surpassed the state-of-the-art in terms of performance metrics on two widely recognized datasets in Brazilian Sign Language (LIBRAS), the primary focus of this study. In addition to being more accurate, our method is more time-efficient and easier to train due to its reliance on a simpler network architecture and solely RGB data as input.","sentences":["Effective communication is paramount for the inclusion of deaf individuals in society.","However, persistent communication barriers due to limited Sign Language (SL) knowledge hinder their full participation.","In this context, Sign Language Recognition (SLR) systems have been developed to improve communication between signing and non-signing individuals.","In particular, there is the problem of recognizing isolated signs (Isolated Sign Language Recognition, ISLR) of great relevance in the development of vision-based SL search engines, learning tools, and translation systems.","This work proposes an ISLR approach where body, hands, and facial landmarks are extracted throughout time and encoded as 2-D images.","These images are processed by a convolutional neural network, which maps the visual-temporal information into a sign label.","Experimental results demonstrate that our method surpassed the state-of-the-art in terms of performance metrics on two widely recognized datasets in Brazilian Sign Language (LIBRAS), the primary focus of this study.","In addition to being more accurate, our method is more time-efficient and easier to train due to its reliance on a simpler network architecture and solely RGB data as input."],"url":"http://arxiv.org/abs/2404.19148v1","category":"cs.CV"}
{"created":"2024-04-29 23:18:07","title":"Asymptotically conserved charges and 2-kink collision in quasi-integrable potential KdV models","abstract":"We study a particular deformation of the potential KdV model (pKdV) and construct the quasi-conservation laws by a direct method. The charge densities, differing from their integrable counterpart with homogeneous degree terms, exhibit mixed scale dimension terms. The modifications of the charges around the soliton interaction regions are examined by numerically simulating some representative anomalies. We show numerically the elastic scattering of two kinks for a wide range of values of the deformation parameters. It is discussed an anomaly cancellation mechanism to define an exact conservation law of the usual pKdV model, and a renormalization procedure is introduced for some divergent charges by subtructing the continuous linear background contribution. The KdV-type equations are quite ubiquitous in several areas of non-linear science, such as the study of General Relativity in $AdS_{3}$, Bose-Einstein condensates, superconductivity and fluid dynamics.","sentences":["We study a particular deformation of the potential KdV model (pKdV) and construct the quasi-conservation laws by a direct method.","The charge densities, differing from their integrable counterpart with homogeneous degree terms, exhibit mixed scale dimension terms.","The modifications of the charges around the soliton interaction regions are examined by numerically simulating some representative anomalies.","We show numerically the elastic scattering of two kinks for a wide range of values of the deformation parameters.","It is discussed an anomaly cancellation mechanism to define an exact conservation law of the usual pKdV model, and a renormalization procedure is introduced for some divergent charges by subtructing the continuous linear background contribution.","The KdV-type equations are quite ubiquitous in several areas of non-linear science, such as the study of General Relativity in $AdS_{3}$, Bose-Einstein condensates, superconductivity and fluid dynamics."],"url":"http://arxiv.org/abs/2404.19147v1","category":"hep-th"}
{"created":"2024-04-29 22:42:01","title":"On Rational Recursion for Holonomic Sequences","abstract":"It was recently conjectured that every component of a discrete rational dynamical system is a solution to an algebraic difference equation that is linear in its highest-shift term (a quasi-linear equation). Holonomic sequences are trivially seen as solutions to such dynamical systems. We prove that the conjecture holds for holonomic sequences and propose two algorithms for converting holonomic recurrence equations into such quasi-linear equations. The two algorithms differ in their efficiency and the minimality of orders in their outputs.","sentences":["It was recently conjectured that every component of a discrete rational dynamical system is a solution to an algebraic difference equation that is linear in its highest-shift term (a quasi-linear equation).","Holonomic sequences are trivially seen as solutions to such dynamical systems.","We prove that the conjecture holds for holonomic sequences and propose two algorithms for converting holonomic recurrence equations into such quasi-linear equations.","The two algorithms differ in their efficiency and the minimality of orders in their outputs."],"url":"http://arxiv.org/abs/2404.19136v1","category":"cs.SC"}
{"created":"2024-04-29 22:32:26","title":"Parameterized Wasserstein Gradient Flow","abstract":"We develop a fast and scalable numerical approach to solve Wasserstein gradient flows (WGFs), particularly suitable for high-dimensional cases. Our approach is to use general reduced-order models, like deep neural networks, to parameterize the push-forward maps such that they can push a simple reference density to the one solving the given WGF. The new dynamical system is called parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter space equipped with a pullback Wasserstein metric. Our numerical scheme can approximate the solutions of WGFs for general energy functionals effectively, without requiring spatial discretization or nonconvex optimization procedures, thus avoiding some limitations of classical numerical methods and more recent deep-learning-based approaches. A comprehensive analysis of the approximation errors measured by Wasserstein distance is also provided in this work. Numerical experiments show promising computational efficiency and verified accuracy on various WGF examples using our approach.","sentences":["We develop a fast and scalable numerical approach to solve Wasserstein gradient flows (WGFs), particularly suitable for high-dimensional cases.","Our approach is to use general reduced-order models, like deep neural networks, to parameterize the push-forward maps such that they can push a simple reference density to the one solving the given WGF.","The new dynamical system is called parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter space equipped with a pullback Wasserstein metric.","Our numerical scheme can approximate the solutions of WGFs for general energy functionals effectively, without requiring spatial discretization or nonconvex optimization procedures, thus avoiding some limitations of classical numerical methods and more recent deep-learning-based approaches.","A comprehensive analysis of the approximation errors measured by Wasserstein distance is also provided in this work.","Numerical experiments show promising computational efficiency and verified accuracy on various WGF examples using our approach."],"url":"http://arxiv.org/abs/2404.19133v1","category":"math.NA"}
{"created":"2024-04-29 22:03:02","title":"Compositional Factorization of Visual Scenes with Convolutional Sparse Coding and Resonator Networks","abstract":"We propose a system for visual scene analysis and recognition based on encoding the sparse, latent feature-representation of an image into a high-dimensional vector that is subsequently factorized to parse scene content. The sparse feature representation is learned from image statistics via convolutional sparse coding, while scene parsing is performed by a resonator network. The integration of sparse coding with the resonator network increases the capacity of distributed representations and reduces collisions in the combinatorial search space during factorization. We find that for this problem the resonator network is capable of fast and accurate vector factorization, and we develop a confidence-based metric that assists in tracking the convergence of the resonator network.","sentences":["We propose a system for visual scene analysis and recognition based on encoding the sparse, latent feature-representation of an image into a high-dimensional vector that is subsequently factorized to parse scene content.","The sparse feature representation is learned from image statistics via convolutional sparse coding, while scene parsing is performed by a resonator network.","The integration of sparse coding with the resonator network increases the capacity of distributed representations and reduces collisions in the combinatorial search space during factorization.","We find that for this problem the resonator network is capable of fast and accurate vector factorization, and we develop a confidence-based metric that assists in tracking the convergence of the resonator network."],"url":"http://arxiv.org/abs/2404.19126v1","category":"cs.CV"}
{"created":"2024-04-29 21:37:40","title":"Computing distances on Riemann surfaces","abstract":"Riemann surfaces are among the simplest and most basic geometric objects. They appear as key players in many branches of physics, mathematics, and other sciences. Despite their widespread significance, how to compute distances between pairs of points on compact Riemann surfaces is surprisingly unknown, unless the surface is a sphere or a torus. This is because on higher-genus surfaces, the distance formula involves an infimum over infinitely many terms, so it cannot be evaluated in practice. Here we derive a computable distance formula for a broad class of Riemann surfaces. The formula reduces the infimum to a minimum over an explicit set consisting of finitely many terms. We also develop a distance computation algorithm, which cannot be expressed as a formula, but which is more computationally efficient on surfaces with high genuses. We illustrate both the formula and the algorithm in application to generalized Bolza surfaces, which are a particular class of highly symmetric compact Riemann surfaces of any genus greater than 1.","sentences":["Riemann surfaces are among the simplest and most basic geometric objects.","They appear as key players in many branches of physics, mathematics, and other sciences.","Despite their widespread significance, how to compute distances between pairs of points on compact Riemann surfaces is surprisingly unknown, unless the surface is a sphere or a torus.","This is because on higher-genus surfaces, the distance formula involves an infimum over infinitely many terms, so it cannot be evaluated in practice.","Here we derive a computable distance formula for a broad class of Riemann surfaces.","The formula reduces the infimum to a minimum over an explicit set consisting of finitely many terms.","We also develop a distance computation algorithm, which cannot be expressed as a formula, but which is more computationally efficient on surfaces with high genuses.","We illustrate both the formula and the algorithm in application to generalized Bolza surfaces, which are a particular class of highly symmetric compact Riemann surfaces of any genus greater than 1."],"url":"http://arxiv.org/abs/2404.19120v1","category":"math.GT"}
{"created":"2024-04-29 21:26:18","title":"Enhancing IoT Security: A Novel Feature Engineering Approach for ML-Based Intrusion Detection Systems","abstract":"The integration of Internet of Things (IoT) applications in our daily lives has led to a surge in data traffic, posing significant security challenges. IoT applications using cloud and edge computing are at higher risk of cyberattacks because of the expanded attack surface from distributed edge and cloud services, the vulnerability of IoT devices, and challenges in managing security across interconnected systems leading to oversights. This led to the rise of ML-based solutions for intrusion detection systems (IDSs), which have proven effective in enhancing network security and defending against diverse threats. However, ML-based IDS in IoT systems encounters challenges, particularly from noisy, redundant, and irrelevant features in varied IoT datasets, potentially impacting its performance. Therefore, reducing such features becomes crucial to enhance system performance and minimize computational costs. This paper focuses on improving the effectiveness of ML-based IDS at the edge level by introducing a novel method to find a balanced trade-off between cost and accuracy through the creation of informative features in a two-tier edge-user IoT environment. A hybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming algorithm is utilized for this purpose. Three IoT intrusion detection datasets, namely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the proposed approach.","sentences":["The integration of Internet of Things (IoT) applications in our daily lives has led to a surge in data traffic, posing significant security challenges.","IoT applications using cloud and edge computing are at higher risk of cyberattacks because of the expanded attack surface from distributed edge and cloud services, the vulnerability of IoT devices, and challenges in managing security across interconnected systems leading to oversights.","This led to the rise of ML-based solutions for intrusion detection systems (IDSs), which have proven effective in enhancing network security and defending against diverse threats.","However, ML-based IDS in IoT systems encounters challenges, particularly from noisy, redundant, and irrelevant features in varied IoT datasets, potentially impacting its performance.","Therefore, reducing such features becomes crucial to enhance system performance and minimize computational costs.","This paper focuses on improving the effectiveness of ML-based IDS at the edge level by introducing a novel method to find a balanced trade-off between cost and accuracy through the creation of informative features in a two-tier edge-user IoT environment.","A hybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming algorithm is utilized for this purpose.","Three IoT intrusion detection datasets, namely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the proposed approach."],"url":"http://arxiv.org/abs/2404.19114v1","category":"cs.CR"}
{"created":"2024-04-29 21:25:25","title":"Hidden Synergy: $L_1$ Weight Normalization and 1-Path-Norm Regularization","abstract":"We present PSiLON Net, an MLP architecture that uses $L_1$ weight normalization for each weight vector and shares the length parameter across the layer. The 1-path-norm provides a bound for the Lipschitz constant of a neural network and reflects on its generalizability, and we show how PSiLON Net's design drastically simplifies the 1-path-norm, while providing an inductive bias towards efficient learning and near-sparse parameters. We propose a pruning method to achieve exact sparsity in the final stages of training, if desired. To exploit the inductive bias of residual networks, we present a simplified residual block, leveraging concatenated ReLU activations. For networks constructed with such blocks, we prove that considering only a subset of possible paths in the 1-path-norm is sufficient to bound the Lipschitz constant. Using the 1-path-norm and this improved bound as regularizers, we conduct experiments in the small data regime using overparameterized PSiLON Nets and PSiLON ResNets, demonstrating reliable optimization and strong performance.","sentences":["We present PSiLON Net, an MLP architecture that uses $L_1$ weight normalization for each weight vector and shares the length parameter across the layer.","The 1-path-norm provides a bound for the Lipschitz constant of a neural network and reflects on its generalizability, and we show how PSiLON Net's design drastically simplifies the 1-path-norm, while providing an inductive bias towards efficient learning and near-sparse parameters.","We propose a pruning method to achieve exact sparsity in the final stages of training, if desired.","To exploit the inductive bias of residual networks, we present a simplified residual block, leveraging concatenated ReLU activations.","For networks constructed with such blocks, we prove that considering only a subset of possible paths in the 1-path-norm is sufficient to bound the Lipschitz constant.","Using the 1-path-norm and this improved bound as regularizers, we conduct experiments in the small data regime using overparameterized PSiLON Nets and PSiLON ResNets, demonstrating reliable optimization and strong performance."],"url":"http://arxiv.org/abs/2404.19112v1","category":"cs.LG"}
{"created":"2024-04-29 21:25:19","title":"H\u00f6lder regularity for degenerate parabolic double-phase equations","abstract":"We prove that bounded weak solutions to degenerate parabolic double-phase equations of $p$-Laplace type are locally H\\\"older continuous. The proof is based on phase analysis and methods for the $p$-Laplace equation. In particular, the phase analysis determines whether the double-phase equation is locally similar to the $p$-Laplace or the $q$-Laplace equation.","sentences":["We prove that bounded weak solutions to degenerate parabolic double-phase equations of $p$-Laplace type are locally H\\\"older continuous.","The proof is based on phase analysis and methods for the $p$-Laplace equation.","In particular, the phase analysis determines whether the double-phase equation is locally similar to the $p$-Laplace or the $q$-Laplace equation."],"url":"http://arxiv.org/abs/2404.19111v1","category":"math.AP"}
{"created":"2024-04-29 21:19:41","title":"The Shape of Money Laundering: Subgraph Representation Learning on the Blockchain with the Elliptic2 Dataset","abstract":"Subgraph representation learning is a technique for analyzing local structures (or shapes) within complex networks. Enabled by recent developments in scalable Graph Neural Networks (GNNs), this approach encodes relational information at a subgroup level (multiple connected nodes) rather than at a node level of abstraction. We posit that certain domain applications, such as anti-money laundering (AML), are inherently subgraph problems and mainstream graph techniques have been operating at a suboptimal level of abstraction. This is due in part to the scarcity of annotated datasets of real-world size and complexity, as well as the lack of software tools for managing subgraph GNN workflows at scale. To enable work in fundamental algorithms as well as domain applications in AML and beyond, we introduce Elliptic2, a large graph dataset containing 122K labeled subgraphs of Bitcoin clusters within a background graph consisting of 49M node clusters and 196M edge transactions. The dataset provides subgraphs known to be linked to illicit activity for learning the set of \"shapes\" that money laundering exhibits in cryptocurrency and accurately classifying new criminal activity. Along with the dataset we share our graph techniques, software tooling, promising early experimental results, and new domain insights already gleaned from this approach. Taken together, we find immediate practical value in this approach and the potential for a new standard in anti-money laundering and forensic analytics in cryptocurrencies and other financial networks.","sentences":["Subgraph representation learning is a technique for analyzing local structures (or shapes) within complex networks.","Enabled by recent developments in scalable Graph Neural Networks (GNNs), this approach encodes relational information at a subgroup level (multiple connected nodes) rather than at a node level of abstraction.","We posit that certain domain applications, such as anti-money laundering (AML), are inherently subgraph problems and mainstream graph techniques have been operating at a suboptimal level of abstraction.","This is due in part to the scarcity of annotated datasets of real-world size and complexity, as well as the lack of software tools for managing subgraph GNN workflows at scale.","To enable work in fundamental algorithms as well as domain applications in AML and beyond, we introduce Elliptic2, a large graph dataset containing 122K labeled subgraphs of Bitcoin clusters within a background graph consisting of 49M node clusters and 196M edge transactions.","The dataset provides subgraphs known to be linked to illicit activity for learning the set of \"shapes\" that money laundering exhibits in cryptocurrency and accurately classifying new criminal activity.","Along with the dataset we share our graph techniques, software tooling, promising early experimental results, and new domain insights already gleaned from this approach.","Taken together, we find immediate practical value in this approach and the potential for a new standard in anti-money laundering and forensic analytics in cryptocurrencies and other financial networks."],"url":"http://arxiv.org/abs/2404.19109v2","category":"cs.LG"}
{"created":"2024-04-29 21:19:12","title":"Real-Time Convolutional Neural Network-Based Star Detection and Centroiding Method for CubeSat Star Tracker","abstract":"Star trackers are one of the most accurate celestial sensors used for absolute attitude determination. The devices detect stars in captured images and accurately compute their projected centroids on an imaging focal plane with subpixel precision. Traditional algorithms for star detection and centroiding often rely on threshold adjustments for star pixel detection and pixel brightness weighting for centroid computation. However, challenges like high sensor noise and stray light can compromise algorithm performance. This article introduces a Convolutional Neural Network (CNN)-based approach for star detection and centroiding, tailored to address the issues posed by noisy star tracker images in the presence of stray light and other artifacts. Trained using simulated star images overlayed with real sensor noise and stray light, the CNN produces both a binary segmentation map distinguishing star pixels from the background and a distance map indicating each pixel's proximity to the nearest star centroid. Leveraging this distance information alongside pixel coordinates transforms centroid calculations into a set of trilateration problems solvable via the least squares method. Our method employs efficient UNet variants for the underlying CNN architectures, and the variants' performances are evaluated. Comprehensive testing has been undertaken with synthetic image evaluations, hardware-in-the-loop assessments, and night sky tests. The tests consistently demonstrated that our method outperforms several existing algorithms in centroiding accuracy and exhibits superior resilience to high sensor noise and stray light interference. An additional benefit of our algorithms is that they can be executed in real-time on low-power edge AI processors.","sentences":["Star trackers are one of the most accurate celestial sensors used for absolute attitude determination.","The devices detect stars in captured images and accurately compute their projected centroids on an imaging focal plane with subpixel precision.","Traditional algorithms for star detection and centroiding often rely on threshold adjustments for star pixel detection and pixel brightness weighting for centroid computation.","However, challenges like high sensor noise and stray light can compromise algorithm performance.","This article introduces a Convolutional Neural Network (CNN)-based approach for star detection and centroiding, tailored to address the issues posed by noisy star tracker images in the presence of stray light and other artifacts.","Trained using simulated star images overlayed with real sensor noise and stray light, the CNN produces both a binary segmentation map distinguishing star pixels from the background and a distance map indicating each pixel's proximity to the nearest star centroid.","Leveraging this distance information alongside pixel coordinates transforms centroid calculations into a set of trilateration problems solvable via the least squares method.","Our method employs efficient UNet variants for the underlying CNN architectures, and the variants' performances are evaluated.","Comprehensive testing has been undertaken with synthetic image evaluations, hardware-in-the-loop assessments, and night sky tests.","The tests consistently demonstrated that our method outperforms several existing algorithms in centroiding accuracy and exhibits superior resilience to high sensor noise and stray light interference.","An additional benefit of our algorithms is that they can be executed in real-time on low-power edge AI processors."],"url":"http://arxiv.org/abs/2404.19108v1","category":"cs.CV"}
{"created":"2024-04-29 21:00:15","title":"Bilayer graphene in periodic and quasiperiodic magnetic superlattices","abstract":"Starting from the effective Hamiltonian arising from the tight binding model, we study the behaviour of low-lying excitations for bilayer graphene placed in periodic external magnetic fields by using irreducible second order supersymmetry transformations. The coupled system of equations describing these excitations is reduced to a pair of periodic Schr\\\"odinger Hamiltonians intertwined by a second order differential operator. The direct implementation of more general second-order supersymmetry transformations allows to create nonsingular Schr\\\"odinger potentials with periodicity defects and bound states embedded in the forbidden bands, which turn out to be associated to quasiperiodic magnetic superlattices.","sentences":["Starting from the effective Hamiltonian arising from the tight binding model, we study the behaviour of low-lying excitations for bilayer graphene placed in periodic external magnetic fields by using irreducible second order supersymmetry transformations.","The coupled system of equations describing these excitations is reduced to a pair of periodic Schr\\\"odinger Hamiltonians intertwined by a second order differential operator.","The direct implementation of more general second-order supersymmetry transformations allows to create nonsingular Schr\\\"odinger potentials with periodicity defects and bound states embedded in the forbidden bands, which turn out to be associated to quasiperiodic magnetic superlattices."],"url":"http://arxiv.org/abs/2404.19106v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 20:32:45","title":"Non-explosion solutions for a class of stochastic physical diffusion oscillators","abstract":"In this work, we are interested in problems that are related to the physical phenomena of diffusion. We will focus on the theoretical aspect of the study, such as existence, uniqueness and non-explosive solutions. We will weaken the conditions imposed on the coefficients of the stochastic differential equations (SDE) that model some diffusion phenomena of mechanics. The work will be based on a general non-explosion criterion and we will obtain sufficient conditions so that the solution for a certain class of diffusions does not explode. We will construct Lyapunov functions that ensure the non-explosion of the solutions. Two important oscillators, namely the Duffing and the Van Der Pol oscillators, belong to this class. The Euler-Maruyama method is applied to these two oscillators to give us a simulation solution for them.","sentences":["In this work, we are interested in problems that are related to the physical phenomena of diffusion.","We will focus on the theoretical aspect of the study, such as existence, uniqueness and non-explosive solutions.","We will weaken the conditions imposed on the coefficients of the stochastic differential equations (SDE) that model some diffusion phenomena of mechanics.","The work will be based on a general non-explosion criterion and we will obtain sufficient conditions so that the solution for a certain class of diffusions does not explode.","We will construct Lyapunov functions that ensure the non-explosion of the solutions.","Two important oscillators, namely the Duffing and the Van Der Pol oscillators, belong to this class.","The Euler-Maruyama method is applied to these two oscillators to give us a simulation solution for them."],"url":"http://arxiv.org/abs/2404.19099v1","category":"math.DS"}
{"created":"2024-04-29 20:19:25","title":"In-Context Symbolic Regression: Leveraging Language Models for Function Discovery","abstract":"Symbolic Regression (SR) is a task which aims to extract the mathematical expression underlying a set of empirical observations. Transformer-based methods trained on SR datasets detain the current state-of-the-art in this task, while the application of Large Language Models (LLMs) to SR remains unexplored. This work investigates the integration of pre-trained LLMs into the SR pipeline, utilizing an approach that iteratively refines a functional form based on the prediction error it achieves on the observation set, until it reaches convergence. Our method leverages LLMs to propose an initial set of possible functions based on the observations, exploiting their strong pre-training prior. These functions are then iteratively refined by the model itself and by an external optimizer for their coefficients. The process is repeated until the results are satisfactory. We then analyze Vision-Language Models in this context, exploring the inclusion of plots as visual inputs to aid the optimization process. Our findings reveal that LLMs are able to successfully recover good symbolic equations that fit the given data, outperforming SR baselines based on Genetic Programming, with the addition of images in the input showing promising results for the most complex benchmarks.","sentences":["Symbolic Regression (SR) is a task which aims to extract the mathematical expression underlying a set of empirical observations.","Transformer-based methods trained on SR datasets detain the current state-of-the-art in this task, while the application of Large Language Models (LLMs) to SR remains unexplored.","This work investigates the integration of pre-trained LLMs into the SR pipeline, utilizing an approach that iteratively refines a functional form based on the prediction error it achieves on the observation set, until it reaches convergence.","Our method leverages LLMs to propose an initial set of possible functions based on the observations, exploiting their strong pre-training prior.","These functions are then iteratively refined by the model itself and by an external optimizer for their coefficients.","The process is repeated until the results are satisfactory.","We then analyze Vision-Language Models in this context, exploring the inclusion of plots as visual inputs to aid the optimization process.","Our findings reveal that LLMs are able to successfully recover good symbolic equations that fit the given data, outperforming SR baselines based on Genetic Programming, with the addition of images in the input showing promising results for the most complex benchmarks."],"url":"http://arxiv.org/abs/2404.19094v1","category":"cs.CL"}
{"created":"2024-04-29 20:15:26","title":"On the Schwartz estimate for Hodge Laplacians on semisimple Lie groups","abstract":"In this paper, we prove Schwartz estimates for Hodge Laplacian and Dirac operators on semisimple Lie groups. Alongside, we gives a version of Kuga lemma for its Lie algebra cohomology. This is a generalization of similar results on symmetric spaces. The main purpose of such estimates is to study the heat problem not only in the scalar case, but also for sections of vector bundles on homogeneous spaces using Fourier analysis.","sentences":["In this paper, we prove Schwartz estimates for Hodge Laplacian and Dirac operators on semisimple Lie groups.","Alongside, we gives a version of Kuga lemma for its Lie algebra cohomology.","This is a generalization of similar results on symmetric spaces.","The main purpose of such estimates is to study the heat problem not only in the scalar case, but also for sections of vector bundles on homogeneous spaces using Fourier analysis."],"url":"http://arxiv.org/abs/2404.19091v1","category":"math.DG"}
{"created":"2024-04-29 20:00:42","title":"New look at Milnor Spheres","abstract":"S. Donaldson disproved the smooth $\\mathrm{h}$-cobordism conjecture in dimension $4$ by studying invariants coming from the moduli space of connections on $\\mathrm{SO}(3)$-bundles over smooth $4$-dimension manifolds $X$. In this paper, we reverse his point of view in dimension $8$. Namely, we consider fibrations coming from diffeomorphisms classes of homotopy Hopf manifolds $\\Sigma^7\\times\\mathrm{S}^1$, i.e., $\\Sigma^7$ is a homotopy sphere, seeing the bases as moduli spaces for additional structures.   Our overarching objective is to establish a correspondence between the smooth invariants of Kervaire--Milnor and categorical invariants. One interprets pairwise non-diffeomorphic smooth structures on $\\Sigma^7\\times\\mathrm{S}^1$ in terms of non-rationality. This paper's contributions are diverse, each carrying its significance. We believe profound connections exist between the themes addressed, which connect hypersurface singularity theory, Homological Mirror Symmetry, Topological Modular forms ($\\mathrm{tmf}$), and Spherical T-duality.","sentences":["S. Donaldson disproved the smooth $\\mathrm{h}$-cobordism conjecture in dimension $4$ by studying invariants coming from the moduli space of connections on $\\mathrm{SO}(3)$-bundles over smooth $4$-dimension manifolds $X$. In this paper, we reverse his point of view in dimension $8$. Namely, we consider fibrations coming from diffeomorphisms classes of homotopy Hopf manifolds $\\Sigma^7\\times\\mathrm{S}^1$, i.e., $\\Sigma^7$ is a homotopy sphere, seeing the bases as moduli spaces for additional structures.   ","Our overarching objective is to establish a correspondence between the smooth invariants of Kervaire--Milnor and categorical invariants.","One interprets pairwise non-diffeomorphic smooth structures on $\\Sigma^7\\times\\mathrm{S}^1$ in terms of non-rationality.","This paper's contributions are diverse, each carrying its significance.","We believe profound connections exist between the themes addressed, which connect hypersurface singularity theory, Homological Mirror Symmetry, Topological Modular forms ($\\mathrm{tmf}$), and Spherical T-duality."],"url":"http://arxiv.org/abs/2404.19088v1","category":"math.DG"}
{"created":"2024-04-29 19:55:33","title":"Nonlinear microrheology with time-dependent forces -- Application to recoils in viscoelastic fluids","abstract":"This work presents a theoretical analysis of the motion of a tracer colloid driven by a time-dependent force through a viscoelastic fluid. The recoil of the colloid after application of a strong force is determined. It provides insights into the elastic forces stored locally in the fluid and their weakening by plastic processes. We generalize the mode coupling theory of microrheology to include time-dependent forces. After deriving the equations of motion for the tracer correlator and simplifying to a schematic model we apply the theory to a switch-off force protocol that features the recoiling of the tracer after cessation of the driving. We also include Langevin dynamics simulations to compare to the results of the theory. A non-monotonic trend of the recoil amplitude is found in the theory and confirmed in the simulations. The linear-response approximation is also verified in the small-force regime. While the overall agreement between simulation and theory is good, simulation shows that the theory predicts a too strong non-monotonous dependence of the recoil distance on the applied force.","sentences":["This work presents a theoretical analysis of the motion of a tracer colloid driven by a time-dependent force through a viscoelastic fluid.","The recoil of the colloid after application of a strong force is determined.","It provides insights into the elastic forces stored locally in the fluid and their weakening by plastic processes.","We generalize the mode coupling theory of microrheology to include time-dependent forces.","After deriving the equations of motion for the tracer correlator and simplifying to a schematic model we apply the theory to a switch-off force protocol that features the recoiling of the tracer after cessation of the driving.","We also include Langevin dynamics simulations to compare to the results of the theory.","A non-monotonic trend of the recoil amplitude is found in the theory and confirmed in the simulations.","The linear-response approximation is also verified in the small-force regime.","While the overall agreement between simulation and theory is good, simulation shows that the theory predicts a too strong non-monotonous dependence of the recoil distance on the applied force."],"url":"http://arxiv.org/abs/2404.19085v1","category":"cond-mat.soft"}
{"created":"2024-04-29 19:48:06","title":"$N=(2,2)$ superfields and geometry revisited","abstract":"We take a fresh look at the relation between generalised K\\\"ahler geometry and $N=(2,2)$ supersymmetric sigma models in two dimensions formulated in terms of $(2,2)$ superfields. Dual formulations in terms of different kinds of superfield are combined to give a formulation with a doubled target space and both the original superfield and the dual superfield. For K\\\"ahler geometry, we show that this doubled geometry is Donaldson's deformation of the holomorphic cotangent bundle of the original K\\\"ahler manifold. This doubled formulation gives an elegant geometric reformulation of the equations of motion. We interpret the equations of motion as the intersection of two Lagrangian submanifolds (or of a Lagrangian submanifold with an isotropic one)in the infinite dimensional symplectic supermanifold which is the analogue of phase space. We then consider further extensions of this formalism, including one in which the geometry is quadrupled, and discuss their geometry.","sentences":["We take a fresh look at the relation between generalised K\\\"ahler geometry and $N=(2,2)$ supersymmetric sigma models in two dimensions formulated in terms of $(2,2)$ superfields.","Dual formulations in terms of different kinds of superfield are combined to give a formulation with a doubled target space and both the original superfield and the dual superfield.","For K\\\"ahler geometry, we show that this doubled geometry is Donaldson's deformation of the holomorphic cotangent bundle of the original K\\\"ahler manifold.","This doubled formulation gives an elegant geometric reformulation of the equations of motion.","We interpret the equations of motion as the intersection of two Lagrangian submanifolds (or of a Lagrangian submanifold with an isotropic one)in the infinite dimensional symplectic supermanifold which is the analogue of phase space.","We then consider further extensions of this formalism, including one in which the geometry is quadrupled, and discuss their geometry."],"url":"http://arxiv.org/abs/2404.19079v1","category":"hep-th"}
{"created":"2024-04-29 19:18:52","title":"Revolutionizing Traffic Sign Recognition: Unveiling the Potential of Vision Transformers","abstract":"This research introduces an innovative method for Traffic Sign Recognition (TSR) by leveraging deep learning techniques, with a particular emphasis on Vision Transformers. TSR holds a vital role in advancing driver assistance systems and autonomous vehicles. Traditional TSR approaches, reliant on manual feature extraction, have proven to be labor-intensive and costly. Moreover, methods based on shape and color have inherent limitations, including susceptibility to various factors and changes in lighting conditions. This study explores three variants of Vision Transformers (PVT, TNT, LNL) and six convolutional neural networks (AlexNet, ResNet, VGG16, MobileNet, EfficientNet, GoogleNet) as baseline models. To address the shortcomings of traditional methods, a novel pyramid EATFormer backbone is proposed, amalgamating Evolutionary Algorithms (EAs) with the Transformer architecture. The introduced EA-based Transformer block captures multi-scale, interactive, and individual information through its components: Feed-Forward Network, Global and Local Interaction, and Multi-Scale Region Aggregation modules. Furthermore, a Modulated Deformable MSA module is introduced to dynamically model irregular locations. Experimental evaluations on the GTSRB and BelgiumTS datasets demonstrate the efficacy of the proposed approach in enhancing both prediction speed and accuracy. This study concludes that Vision Transformers hold significant promise in traffic sign classification and contributes a fresh algorithmic framework for TSR. These findings set the stage for the development of precise and dependable TSR algorithms, benefiting driver assistance systems and autonomous vehicles.","sentences":["This research introduces an innovative method for Traffic Sign Recognition (TSR) by leveraging deep learning techniques, with a particular emphasis on Vision Transformers.","TSR holds a vital role in advancing driver assistance systems and autonomous vehicles.","Traditional TSR approaches, reliant on manual feature extraction, have proven to be labor-intensive and costly.","Moreover, methods based on shape and color have inherent limitations, including susceptibility to various factors and changes in lighting conditions.","This study explores three variants of Vision Transformers (PVT, TNT, LNL) and six convolutional neural networks (AlexNet, ResNet, VGG16, MobileNet, EfficientNet, GoogleNet) as baseline models.","To address the shortcomings of traditional methods, a novel pyramid EATFormer backbone is proposed, amalgamating Evolutionary Algorithms (EAs) with the Transformer architecture.","The introduced EA-based Transformer block captures multi-scale, interactive, and individual information through its components: Feed-Forward Network, Global and Local Interaction, and Multi-Scale Region Aggregation modules.","Furthermore, a Modulated Deformable MSA module is introduced to dynamically model irregular locations.","Experimental evaluations on the GTSRB and BelgiumTS datasets demonstrate the efficacy of the proposed approach in enhancing both prediction speed and accuracy.","This study concludes that Vision Transformers hold significant promise in traffic sign classification and contributes a fresh algorithmic framework for TSR.","These findings set the stage for the development of precise and dependable TSR algorithms, benefiting driver assistance systems and autonomous vehicles."],"url":"http://arxiv.org/abs/2404.19066v1","category":"cs.CV"}
{"created":"2024-04-29 18:51:56","title":"Anisotropic Quark Stars in Modified $f(R,T)$ Gravity utilizing Tolman V potential","abstract":"Alternative gravity theory is currently an incredibly significant technique for addressing some enduring experimental difficulties, such as the universe's dark region. They may also be employed in celestial cosmology, producing results that are a stage beyond those found using Einstein's General Relativity. In this study, we examine the characteristics of anisotropic spherically symmetric stellar structures in the context of modified $f(R,T)$ gravity. In order to explain the distinctive characteristics of compact objects, we investigate how the fluid distribution in the star model is affected by the MIT bag model equation of state. By using Tolman V metric potentials, we establish the field equations, and by employing the experimental data of the three observed stars, we identify the values of unknown parameters. By using a realistic $f(R,T)$ model, we investigate the effect of the energy density, anisotropic factor, transversal and radial pressure within the cores of the aforementioned stars for a particular amount of the Bag constant. Further, we examine the stability of the cosmic structure and the physical validity of our suggested model via equilibrium conditions, energy, and causality parameters. To conclude, the physical conditions are fulfilled by our model, and the magnitude of the Bag constant agrees with the experimental data, demonstrating the model's feasibility.","sentences":["Alternative gravity theory is currently an incredibly significant technique for addressing some enduring experimental difficulties, such as the universe's dark region.","They may also be employed in celestial cosmology, producing results that are a stage beyond those found using Einstein's General Relativity.","In this study, we examine the characteristics of anisotropic spherically symmetric stellar structures in the context of modified $f(R,T)$ gravity.","In order to explain the distinctive characteristics of compact objects, we investigate how the fluid distribution in the star model is affected by the MIT bag model equation of state.","By using Tolman V metric potentials, we establish the field equations, and by employing the experimental data of the three observed stars, we identify the values of unknown parameters.","By using a realistic $f(R,T)$ model, we investigate the effect of the energy density, anisotropic factor, transversal and radial pressure within the cores of the aforementioned stars for a particular amount of the Bag constant.","Further, we examine the stability of the cosmic structure and the physical validity of our suggested model via equilibrium conditions, energy, and causality parameters.","To conclude, the physical conditions are fulfilled by our model, and the magnitude of the Bag constant agrees with the experimental data, demonstrating the model's feasibility."],"url":"http://arxiv.org/abs/2404.19056v1","category":"gr-qc"}
{"created":"2024-04-29 18:50:55","title":"Bounds to the Basset-Boussinesq force on particle laden stratified flows","abstract":"The Basset-Boussinesq force is often perfunctory neglected when studying small inertial particles in turbulence. The force results from the diffusion of vorticity from the particles, and as it depends on the particles past history, it complicates the dynamics by turning their equations of motion into integro-differential equations. However, this force is of the same order as other viscous forces acting on the particles, and beyond convenience, the reasons for neglecting it are unclear. We derive strict bounds for the magnitude of the Basset-Boussinesq force in stably stratified flows, in contexts of interest for geophysical turbulence. The bounds are validated by direct numerical simulations. The Basset-Boussinesq force can be neglected when a buoyancy Stokes number $\\textrm{Sb} = N \\tau_p$ is small, where $N$ is the flow Brunt-V\\\"ais\\\"al\\\"a frequency and $\\tau_p$ is the particles Stokes time. For sufficiently strong stratification, or particles with large inertia, this force must be considered in the dynamics.","sentences":["The Basset-Boussinesq force is often perfunctory neglected when studying small inertial particles in turbulence.","The force results from the diffusion of vorticity from the particles, and as it depends on the particles past history, it complicates the dynamics by turning their equations of motion into integro-differential equations.","However, this force is of the same order as other viscous forces acting on the particles, and beyond convenience, the reasons for neglecting it are unclear.","We derive strict bounds for the magnitude of the Basset-Boussinesq force in stably stratified flows, in contexts of interest for geophysical turbulence.","The bounds are validated by direct numerical simulations.","The Basset-Boussinesq force can be neglected when a buoyancy Stokes number $\\textrm{Sb} = N \\tau_p$ is small, where $N$ is the flow Brunt-V\\\"ais\\\"al\\\"a frequency and $\\tau_p$ is the particles Stokes time.","For sufficiently strong stratification, or particles with large inertia, this force must be considered in the dynamics."],"url":"http://arxiv.org/abs/2404.19054v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 18:30:27","title":"Multiplicity results for fully nonlinear elliptic equations with natural gradient growth","abstract":"In this paper, we prove a theorem concerning the existence of three solutions for the following boundary value problem: \\begin{equation*} -\\mathcal{M}_{\\lambda,\\Lambda}^+(D^2u)-\\Gamma|Du|^2=f(u)~~~\\text{in}\\ \\Omega, u=0~~~\\text{on}\\ \\partial\\Omega, \\end{equation*} where $f:[0,\\infty]\\to[0,\\infty]$ is a $C^{\\alpha}$ function and $\\Omega$ denotes a bounded, smooth domain in $\\mathbb{R}^N$. By constructing two ordered pairs of sub and supersolutions for a specific class of $f$ exhibiting sublinear growth, we further establish the existence of three positive solutions to the aforementioned boundary value problem.","sentences":["In this paper, we prove a theorem concerning the existence of three solutions for the following boundary value problem: \\begin{equation*} -\\mathcal{M}_{\\lambda,\\Lambda}^+(D^2u)-\\Gamma|Du|^2=f(u)~~~\\text{in}\\ \\Omega, u=0~~~\\text{on}\\ \\partial\\Omega, \\end{equation*} where $f:[0,\\infty]\\to[0,\\infty]$ is a $C^{\\alpha}$ function and $\\Omega$ denotes a bounded, smooth domain in $\\mathbb{R}^N$. By constructing two ordered pairs of sub and supersolutions for a specific class of $f$ exhibiting sublinear growth, we further establish the existence of three positive solutions to the aforementioned boundary value problem."],"url":"http://arxiv.org/abs/2404.19042v1","category":"math.AP"}
{"created":"2024-04-29 18:29:11","title":"Stochastic dynamics of two-compartment models with regulatory mechanisms for hematopoiesis","abstract":"We present an asymptotic analysis of a stochastic two-compartmental cell proliferation system with regulatory mechanisms. We model the system as a state-dependent birth and death process. Proliferation of hematopoietic stem cells (HSCs) is regulated by population density of HSC-derived clones and differentiation of HSC is regulated by population density of HSCs. By scaling up the initial population, we show the density of dynamics converges in distribution to the solution of a system of ordinary differential equations. The system of ODE has a unique non-trivial equilibrium that is globally stable. Furthermore, we show the scaled fluctuation of the population converges in law to a linear diffusion. With initial data being Gaussian, the limit is a Gauss-Markov process and we prove the process will stabilize exponentially fast in the 2-Wasserstein metric. We apply our results to analyze and compare two regulatory mechanisms in the hematopoietic system. Simulations are conducted to verify our large-scale and long-time approximation of the dynamics. We demonstrate some regulatory mechanisms are efficient (converge to steady state rapidly) but not effective (have large fluctuation around the steady state).","sentences":["We present an asymptotic analysis of a stochastic two-compartmental cell proliferation system with regulatory mechanisms.","We model the system as a state-dependent birth and death process.","Proliferation of hematopoietic stem cells (HSCs) is regulated by population density of HSC-derived clones and differentiation of HSC is regulated by population density of HSCs.","By scaling up the initial population, we show the density of dynamics converges in distribution to the solution of a system of ordinary differential equations.","The system of ODE has a unique non-trivial equilibrium that is globally stable.","Furthermore, we show the scaled fluctuation of the population converges in law to a linear diffusion.","With initial data being Gaussian, the limit is a Gauss-Markov process and we prove the process will stabilize exponentially fast in the 2-Wasserstein metric.","We apply our results to analyze and compare two regulatory mechanisms in the hematopoietic system.","Simulations are conducted to verify our large-scale and long-time approximation of the dynamics.","We demonstrate some regulatory mechanisms are efficient (converge to steady state rapidly) but not effective (have large fluctuation around the steady state)."],"url":"http://arxiv.org/abs/2404.19041v1","category":"q-bio.PE"}
{"created":"2024-04-29 18:26:22","title":"Hyperbolic 3-manifolds with uniform spectral gap for coclosed 1-forms","abstract":"We study two quantifications of being a homology sphere for hyperbolic 3-manifolds, one geometric and one topological: the spectral gap for the Laplacian on coclosed 1-forms and the size of the first torsion homology group. We first construct a sequence of closed hyperbolic integer homology spheres with volume tending to infinity and a uniform coclosed 1-form spectral gap. This answers a question asked by Lin--Lipnowski. We also find sequences of hyperbolic rational homology spheres with the same properties that geometrically converge to a tame limit manifold. Moreover, we show that any such sequence must have unbounded torsion homology growth. Finally we show that a sequence of closed hyperbolic rational homology 3-spheres with uniformly bounded rank and a uniform coclosed 1-form spectral gap must have torsion homology that grows exponentially in volume.","sentences":["We study two quantifications of being a homology sphere for hyperbolic 3-manifolds, one geometric and one topological: the spectral gap for the Laplacian on coclosed 1-forms and the size of the first torsion homology group.","We first construct a sequence of closed hyperbolic integer homology spheres with volume tending to infinity and a uniform coclosed 1-form spectral gap.","This answers a question asked by Lin--Lipnowski.","We also find sequences of hyperbolic rational homology spheres with the same properties that geometrically converge to a tame limit manifold.","Moreover, we show that any such sequence must have unbounded torsion homology growth.","Finally we show that a sequence of closed hyperbolic rational homology 3-spheres with uniformly bounded rank and a uniform coclosed 1-form spectral gap must have torsion homology that grows exponentially in volume."],"url":"http://arxiv.org/abs/2404.19039v1","category":"math.GT"}
{"created":"2024-04-29 18:24:41","title":"BSMPT v3 A Tool for Phase Transitions and Primordial Gravitational Waves in Extended Higgs Sectors","abstract":"Strong first-order phase transitions (SFOPT) during the evolution of the Higgs potential in the early universe not only allow for the dynamical generation of the observed matter-antimatter asymmetry, they can also source a stochastic gravitational wave (GW) background possibly detectable with future space-based gravitational waves interferometers. As SFOPTs are phenomenologically incompatible with the Standard Model (SM) Higgs sector, the observation of GWs from SFOPTs provides an exciting interplay between cosmology and particle physics in the search for new physics. With the C++ code BSMPTv3, we present for the first time a tool that performs the whole chain from the particle physics model to the gravitational wave spectrum. Extending the previous versions BSMPTv1 and v2, it traces the phases of beyond-SM (BSM) Higgs potentials and is capable of treating multiple vacuum directions and multi-step phase transitions. During the tracing, it checks for discrete symmetries, flat directions, and electroweak symmetry restoration, and finally reports the transition history. The transition probability from the false to the true vacuum is obtained from the solution of the bounce equation which allows for the calculation of the nucleation, percolation and completion temperatures. The peak amplitude and frequency of the GWs originating from sound waves and turbulence, are evaluated after the calculation of the thermal parameters at the transition temperature, and finally the signal-to-noise ratio at LISA is provided. The code BSMPTv3 is a powerful self-contained tool that comes more than timely and will be of great benefit for investigations of the vacuum structure of the early universe of not only simple but also complicated Higgs potentials involving several vacuum directions, with exciting applications in the search for new physics.","sentences":["Strong first-order phase transitions (SFOPT) during the evolution of the Higgs potential in the early universe not only allow for the dynamical generation of the observed matter-antimatter asymmetry, they can also source a stochastic gravitational wave (GW) background possibly detectable with future space-based gravitational waves interferometers.","As SFOPTs are phenomenologically incompatible with the Standard Model (SM) Higgs sector, the observation of GWs from SFOPTs provides an exciting interplay between cosmology and particle physics in the search for new physics.","With the C++ code BSMPTv3, we present for the first time a tool that performs the whole chain from the particle physics model to the gravitational wave spectrum.","Extending the previous versions BSMPTv1 and v2, it traces the phases of beyond-SM (BSM) Higgs potentials and is capable of treating multiple vacuum directions and multi-step phase transitions.","During the tracing, it checks for discrete symmetries, flat directions, and electroweak symmetry restoration, and finally reports the transition history.","The transition probability from the false to the true vacuum is obtained from the solution of the bounce equation which allows for the calculation of the nucleation, percolation and completion temperatures.","The peak amplitude and frequency of the GWs originating from sound waves and turbulence, are evaluated after the calculation of the thermal parameters at the transition temperature, and finally the signal-to-noise ratio at LISA is provided.","The code BSMPTv3 is a powerful self-contained tool that comes more than timely and will be of great benefit for investigations of the vacuum structure of the early universe of not only simple but also complicated Higgs potentials involving several vacuum directions, with exciting applications in the search for new physics."],"url":"http://arxiv.org/abs/2404.19037v1","category":"hep-ph"}
{"created":"2024-04-29 18:21:06","title":"Traveling waves near Poiseuille flow for the 2D Euler equation","abstract":"In this paper we reveal the existence of a large family of new, nontrivial and Lipschitz traveling waves for the 2D Euler equation at an arbitrarily small distance from the Poiseuille flow in $H^s$, with $s<3/2$, at the level of the vorticity.","sentences":["In this paper we reveal the existence of a large family of new, nontrivial and Lipschitz traveling waves for the 2D Euler equation at an arbitrarily small distance from the Poiseuille flow in $H^s$, with $s<3/2$, at the level of the vorticity."],"url":"http://arxiv.org/abs/2404.19034v1","category":"math.AP"}
{"created":"2024-04-29 18:16:52","title":"Fermionic Machine Learning","abstract":"We introduce fermionic machine learning (FermiML), a machine learning framework based on fermionic quantum computation. FermiML models are expressed in terms of parameterized matchgate circuits, a restricted class of quantum circuits that map exactly to systems of free Majorana fermions. The FermiML framework allows for building fermionic counterparts of any quantum machine learning (QML) model based on parameterized quantum circuits, including models that produce highly entangled quantum states. Importantly, matchgate circuits are efficiently simulable classically, thus rendering FermiML a flexible framework for utility benchmarks of QML methods on large real-world datasets. We initiate the exploration of FermiML by benchmarking it against unrestricted PQCs in the context of classification with random quantum kernels. Through experiments on standard datasets (Digits and Wisconsin Breast Cancer), we demonstrate that FermiML kernels are on-par with unrestricted PQC kernels in classification tasks using support-vector machines. Furthermore, we find that FermiML kernels outperform their unrestricted candidates on multi-class classification, including on datasets with several tens of relevant features. We thus show how FermiML enables us to explore regimes previously inaccessible to QML methods.","sentences":["We introduce fermionic machine learning (FermiML), a machine learning framework based on fermionic quantum computation.","FermiML models are expressed in terms of parameterized matchgate circuits, a restricted class of quantum circuits that map exactly to systems of free Majorana fermions.","The FermiML framework allows for building fermionic counterparts of any quantum machine learning (QML) model based on parameterized quantum circuits, including models that produce highly entangled quantum states.","Importantly, matchgate circuits are efficiently simulable classically, thus rendering FermiML a flexible framework for utility benchmarks of QML methods on large real-world datasets.","We initiate the exploration of FermiML by benchmarking it against unrestricted PQCs in the context of classification with random quantum kernels.","Through experiments on standard datasets (Digits and Wisconsin Breast Cancer), we demonstrate that FermiML kernels are on-par with unrestricted PQC kernels in classification tasks using support-vector machines.","Furthermore, we find that FermiML kernels outperform their unrestricted candidates on multi-class classification, including on datasets with several tens of relevant features.","We thus show how FermiML enables us to explore regimes previously inaccessible to QML methods."],"url":"http://arxiv.org/abs/2404.19032v1","category":"quant-ph"}
{"created":"2024-04-29 18:15:36","title":"A High-Fidelity Methodology for Particle-Resolved Direct Numerical Simulations","abstract":"We present a novel computational method for direct numerical simulations of particle-laden flows with fully-resolved particles (PR-DNS). The method is based on the recently developed Volume-Filtering Immersed Boundary method [Dave et al, Journal of Computational Physics, 487:112136, 2023] derived by volume-filtering the transport equations. This approach is mathematically and physically rigorous, in contrast to other PR-DNS methods which rely on ad-hoc numerical schemes to impose no-slip boundary conditions on the surface of particles. With the present PR-DNS strategy, we show that the ratio of filter size to particle diameter acts as a parameter that controls the level of fidelity. In the limit where this ratio is very small, a well-resolved PR-DNS is obtained. Conversely, when the ratio of filter size to particle diameter is large, a classic point-particle method is obtained. The discretization of the filtered equations is discussed and compared to other PR-DNS strategies based on direct-forcing immersed boundary methods. Numerical examples with sedimenting resolved particles are discussed.","sentences":["We present a novel computational method for direct numerical simulations of particle-laden flows with fully-resolved particles (PR-DNS).","The method is based on the recently developed Volume-Filtering Immersed Boundary method [Dave et al, Journal of Computational Physics, 487:112136, 2023] derived by volume-filtering the transport equations.","This approach is mathematically and physically rigorous, in contrast to other PR-DNS methods which rely on ad-hoc numerical schemes to impose no-slip boundary conditions on the surface of particles.","With the present PR-DNS strategy, we show that the ratio of filter size to particle diameter acts as a parameter that controls the level of fidelity.","In the limit where this ratio is very small, a well-resolved PR-DNS is obtained.","Conversely, when the ratio of filter size to particle diameter is large, a classic point-particle method is obtained.","The discretization of the filtered equations is discussed and compared to other PR-DNS strategies based on direct-forcing immersed boundary methods.","Numerical examples with sedimenting resolved particles are discussed."],"url":"http://arxiv.org/abs/2404.19030v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 18:10:12","title":"MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing","abstract":"Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing.","sentences":["Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications.","However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair).","In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations.","Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details.","To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts.","For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting.","A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions.","Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks.","Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing."],"url":"http://arxiv.org/abs/2404.19026v1","category":"cs.CV"}
{"created":"2024-04-29 18:09:28","title":"Unsupervised Binary Code Translation with Application to Code Similarity Detection and Vulnerability Discovery","abstract":"Binary code analysis has immense importance in the research domain of software security. Today, software is very often compiled for various Instruction Set Architectures (ISAs). As a result, cross-architecture binary code analysis has become an emerging problem. Recently, deep learning-based binary analysis has shown promising success. It is widely known that training a deep learning model requires a massive amount of data. However, for some low-resource ISAs, an adequate amount of data is hard to find, preventing deep learning from being widely adopted for binary analysis. To overcome the data scarcity problem and facilitate cross-architecture binary code analysis, we propose to apply the ideas and techniques in Neural Machine Translation (NMT) to binary code analysis. Our insight is that a binary, after disassembly, is represented in some assembly language. Given a binary in a low-resource ISA, we translate it to a binary in a high-resource ISA (e.g., x86). Then we can use a model that has been trained on the high-resource ISA to test the translated binary. We have implemented the model called UNSUPERBINTRANS, and conducted experiments to evaluate its performance. Specifically, we conducted two downstream tasks, including code similarity detection and vulnerability discovery. In both tasks, we achieved high accuracies.","sentences":["Binary code analysis has immense importance in the research domain of software security.","Today, software is very often compiled for various Instruction Set Architectures (ISAs).","As a result, cross-architecture binary code analysis has become an emerging problem.","Recently, deep learning-based binary analysis has shown promising success.","It is widely known that training a deep learning model requires a massive amount of data.","However, for some low-resource ISAs, an adequate amount of data is hard to find, preventing deep learning from being widely adopted for binary analysis.","To overcome the data scarcity problem and facilitate cross-architecture binary code analysis, we propose to apply the ideas and techniques in Neural Machine Translation (NMT) to binary code analysis.","Our insight is that a binary, after disassembly, is represented in some assembly language.","Given a binary in a low-resource ISA, we translate it to a binary in a high-resource ISA (e.g., x86).","Then we can use a model that has been trained on the high-resource ISA to test the translated binary.","We have implemented the model called UNSUPERBINTRANS, and conducted experiments to evaluate its performance.","Specifically, we conducted two downstream tasks, including code similarity detection and vulnerability discovery.","In both tasks, we achieved high accuracies."],"url":"http://arxiv.org/abs/2404.19025v1","category":"cs.SE"}
{"created":"2024-04-29 18:00:25","title":"Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler Solutions","abstract":"Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\\circ$ scenes by employing the above regularizations.","sentences":["Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes.","Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation.","However, both implicit and explicit radiance fields require dense sampling of images in the given scene.","Their performance degrades significantly when only a sparse set of views is available.","Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views.","The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset.","While the former may provide only sparse supervision, the latter may suffer from generalization issues.","As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field.","Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields.","We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario.","Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions.","By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field.","We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\\circ$ scenes by employing the above regularizations."],"url":"http://arxiv.org/abs/2404.19015v1","category":"cs.CV"}
{"created":"2024-04-29 18:00:02","title":"Chirality-induced spin selectivity by variable-range hopping along DNA double helix","abstract":"We here present a variable-range hopping model to describe the chirality-induced spin selectivity along the DNA double helix. In this model, DNA is considered as a one-dimensional disordered system, where electrons are transported by chiral phonon-assisted hopping between localized states. Owing to the coupling between the electron spin and the vorticity of chiral phonons, electric toroidal monopole appears in the charge-to-spin conductances as a manifestation of true chirality. Our model quantitatively explains the temperature dependence of the spin polarization observed in experiments.","sentences":["We here present a variable-range hopping model to describe the chirality-induced spin selectivity along the DNA double helix.","In this model, DNA is considered as a one-dimensional disordered system, where electrons are transported by chiral phonon-assisted hopping between localized states.","Owing to the coupling between the electron spin and the vorticity of chiral phonons, electric toroidal monopole appears in the charge-to-spin conductances as a manifestation of true chirality.","Our model quantitatively explains the temperature dependence of the spin polarization observed in experiments."],"url":"http://arxiv.org/abs/2404.19000v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 18:00:00","title":"Unifying Simulation and Inference with Normalizing Flows","abstract":"There have been many applications of deep neural networks to detector calibrations and a growing number of studies that propose deep generative models as automated fast detector simulators. We show that these two tasks can be unified by using maximum likelihood estimation (MLE) from conditional generative models for energy regression. Unlike direct regression techniques, the MLE approach is prior-independent and non-Gaussian resolutions can be determined from the shape of the likelihood near the maximum. Using an ATLAS-like calorimeter simulation, we demonstrate this concept in the context of calorimeter energy calibration.","sentences":["There have been many applications of deep neural networks to detector calibrations and a growing number of studies that propose deep generative models as automated fast detector simulators.","We show that these two tasks can be unified by using maximum likelihood estimation (MLE) from conditional generative models for energy regression.","Unlike direct regression techniques, the MLE approach is prior-independent and non-Gaussian resolutions can be determined from the shape of the likelihood near the maximum.","Using an ATLAS-like calorimeter simulation, we demonstrate this concept in the context of calorimeter energy calibration."],"url":"http://arxiv.org/abs/2404.18992v1","category":"hep-ph"}
{"created":"2024-04-29 17:07:15","title":"Definition of vortex boundary using stagnation pressure","abstract":"A novel method is proposed to identify vortex boundary and center of rotation based on tubular surfaces of constant stagnation pressure and minimum of the stagnation pressure gradient. The method is derived from Crocco's theorem, which ensures that the gradient of stagnation pressure is orthogonal to both the velocity and vorticity vectors. The method is Galilean invariant, requires little processing and is robust. It enables visualization of complex turbulent flows and provides a physically consistent definition of vortex boundaries for quantitative analyses. This vortex boundary is a material surface that is representative of the kinematics of the flow by construction, constitutes a vortex tube, ensures conservation of circulation in the inviscid limit and provides a unique relation to the conservation of momentum equations and vortex loads.","sentences":["A novel method is proposed to identify vortex boundary and center of rotation based on tubular surfaces of constant stagnation pressure and minimum of the stagnation pressure gradient.","The method is derived from Crocco's theorem, which ensures that the gradient of stagnation pressure is orthogonal to both the velocity and vorticity vectors.","The method is Galilean invariant, requires little processing and is robust.","It enables visualization of complex turbulent flows and provides a physically consistent definition of vortex boundaries for quantitative analyses.","This vortex boundary is a material surface that is representative of the kinematics of the flow by construction, constitutes a vortex tube, ensures conservation of circulation in the inviscid limit and provides a unique relation to the conservation of momentum equations and vortex loads."],"url":"http://arxiv.org/abs/2404.18987v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 15:57:18","title":"PCA for Point Processes","abstract":"We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo\\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.","sentences":["We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level.","By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes.","The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis.","Key theoretical contributions include establishing a Karhunen-Lo\\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures.","We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns.","We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved.","We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness.","Our method is implemented in the pppca R-package."],"url":"http://arxiv.org/abs/2404.19661v1","category":"stat.ME"}
{"created":"2024-04-30 15:56:16","title":"Regularization of Riemannian optimization: Application to process tomography and quantum machine learning","abstract":"Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels. In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches. Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible. We apply the method to quantum process tomography and a quantum machine learning problem. Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography. Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data.","sentences":["Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels.","In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches.","Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible.","We apply the method to quantum process tomography and a quantum machine learning problem.","Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography.","Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data."],"url":"http://arxiv.org/abs/2404.19659v1","category":"quant-ph"}
{"created":"2024-04-30 14:42:55","title":"Perceptual Constancy Constrained Single Opinion Score Calibration for Image Quality Assessment","abstract":"In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS). Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS. More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images. Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality. Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3). Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively. Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available.","sentences":["In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS).","Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS.","More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images.","Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality.","Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3).","Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively.","Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available."],"url":"http://arxiv.org/abs/2404.19595v1","category":"cs.CV"}
{"created":"2024-04-30 13:50:20","title":"Short term vs. long term: optimization of microswimmer navigation on different time horizons","abstract":"We use reinforcement learning to find strategies that allow microswimmers in turbulence to avoid regions of large strain. This question is motivated by the hypothesis that swimming microorganisms tend to avoid such regions to minimise the risk of predation. We ask which local cues a microswimmer must measure to efficiently avoid such straining regions. We find that it can succeed without directional information, merely by measuring the magnitude of the local strain. However, the swimmer avoids straining regions more efficiently if it can measure the sign of local strain gradients. We compare our results with those of an earlier study [Mousavi et al. arxiv:2309.09641] where a short-time expansion was used to find optimal strategies. We find that the short-time strategies work well in some cases but not in others. We derive a new theory that explains when the time-horizon matters for our optimisation problem, and when it does not. We find the strategy with best performance when the time-horizon coincides with the correlation time of the turbulent fluctuations. We also explain how the update frequency (the frequency at which the swimmer updates its state) affects the found strategies. We find that higher update frequencies yield better performance, as long as the time between updates is smaller than the correlation time of the flow.","sentences":["We use reinforcement learning to find strategies that allow microswimmers in turbulence to avoid regions of large strain.","This question is motivated by the hypothesis that swimming microorganisms tend to avoid such regions to minimise the risk of predation.","We ask which local cues a microswimmer must measure to efficiently avoid such straining regions.","We find that it can succeed without directional information, merely by measuring the magnitude of the local strain.","However, the swimmer avoids straining regions more efficiently if it can measure the sign of local strain gradients.","We compare our results with those of an earlier study [Mousavi et al. arxiv:2309.09641] where a short-time expansion was used to find optimal strategies.","We find that the short-time strategies work well in some cases but not in others.","We derive a new theory that explains when the time-horizon matters for our optimisation problem, and when it does not.","We find the strategy with best performance when the time-horizon coincides with the correlation time of the turbulent fluctuations.","We also explain how the update frequency (the frequency at which the swimmer updates its state) affects the found strategies.","We find that higher update frequencies yield better performance, as long as the time between updates is smaller than the correlation time of the flow."],"url":"http://arxiv.org/abs/2404.19561v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 11:31:07","title":"Bayesian Functional Connectivity and Graph Convolutional Network for Working Memory Load Classification","abstract":"Brain responses related to working memory originate from distinct brain areas and oscillate at different frequencies. EEG signals with high temporal correlation can effectively capture these responses. Therefore, estimating the functional connectivity of EEG for working memory protocols in different frequency bands plays a significant role in analyzing the brain dynamics with increasing memory and cognitive loads, which remains largely unexplored. The present study introduces a Bayesian structure learning algorithm to learn the functional connectivity of EEG in sensor space. Next, the functional connectivity graphs are taken as input to the graph convolutional network to classify the working memory loads. The intrasubject (subject-specific) classification performed on 154 subjects for six different verbal working memory loads produced the highest classification accuracy of 96% and average classification accuracy of 89%, outperforming state-of-the-art classification models proposed in the literature. Furthermore, the proposed Bayesian structure learning algorithm is compared with state-of-the-art functional connectivity estimation methods through intersubject and intrasubject statistical analysis of variance. The results also show that the alpha and theta bands have better classification accuracy than the beta band.","sentences":["Brain responses related to working memory originate from distinct brain areas and oscillate at different frequencies.","EEG signals with high temporal correlation can effectively capture these responses.","Therefore, estimating the functional connectivity of EEG for working memory protocols in different frequency bands plays a significant role in analyzing the brain dynamics with increasing memory and cognitive loads, which remains largely unexplored.","The present study introduces a Bayesian structure learning algorithm to learn the functional connectivity of EEG in sensor space.","Next, the functional connectivity graphs are taken as input to the graph convolutional network to classify the working memory loads.","The intrasubject (subject-specific) classification performed on 154 subjects for six different verbal working memory loads produced the highest classification accuracy of 96% and average classification accuracy of 89%, outperforming state-of-the-art classification models proposed in the literature.","Furthermore, the proposed Bayesian structure learning algorithm is compared with state-of-the-art functional connectivity estimation methods through intersubject and intrasubject statistical analysis of variance.","The results also show that the alpha and theta bands have better classification accuracy than the beta band."],"url":"http://arxiv.org/abs/2404.19467v1","category":"cs.LG"}
{"created":"2024-04-30 11:24:50","title":"Enhancing Physical Layer Security with Deep SIMO Auto-Encoder and RF Impairments Modeling","abstract":"This paper presents a novel approach to achieving secure wireless communication by leveraging the inherent characteristics of wireless channels through end-to-end learning using a single-input-multiple-output (SIMO) autoencoder (AE). To ensure a more realistic signal transmission, we derive the signal model that captures all radio frequency (RF) hardware impairments to provide reliable and secure communication. Performance evaluations against traditional linear decoders, such as zero-forcing (ZR) and linear minimum mean square error (LMMSE), and the optimal nonlinear decoder, maximum likelihood (ML), demonstrate that the AE-based SIMO model exhibits superior bit error rate (BER) performance, but with a substantial gap even in the presence of RF hardware impairments. Additionally, the proposed model offers enhanced security features, preventing potential eavesdroppers from intercepting transmitted information and leveraging RF impairments for augmented physical layer security and device identification. These findings underscore the efficacy of the proposed end-to-end learning approach in achieving secure and robust wireless communication.","sentences":["This paper presents a novel approach to achieving secure wireless communication by leveraging the inherent characteristics of wireless channels through end-to-end learning using a single-input-multiple-output (SIMO) autoencoder (AE).","To ensure a more realistic signal transmission, we derive the signal model that captures all radio frequency (RF) hardware impairments to provide reliable and secure communication.","Performance evaluations against traditional linear decoders, such as zero-forcing (ZR) and linear minimum mean square error (LMMSE), and the optimal nonlinear decoder, maximum likelihood (ML), demonstrate that the AE-based SIMO model exhibits superior bit error rate (BER) performance, but with a substantial gap even in the presence of RF hardware impairments.","Additionally, the proposed model offers enhanced security features, preventing potential eavesdroppers from intercepting transmitted information and leveraging RF impairments for augmented physical layer security and device identification.","These findings underscore the efficacy of the proposed end-to-end learning approach in achieving secure and robust wireless communication."],"url":"http://arxiv.org/abs/2404.19463v1","category":"eess.SP"}
{"created":"2024-04-30 11:19:05","title":"AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples","abstract":"Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than 100 attack implementations with a total of over 800 different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.","sentences":["Adversarial examples are typically optimized with gradient-based attacks.","While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models.","This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others.","In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks.","To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences.","We then introduce our framework, which evaluates their effectiveness and efficiency.","We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget.","Our extensive experimental analysis compares more than 100 attack implementations with a total of over 800 different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches.","Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all.","We release AttackBench as a publicly available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples."],"url":"http://arxiv.org/abs/2404.19460v1","category":"cs.LG"}
{"created":"2024-04-30 06:02:59","title":"Quater-GCN: Enhancing 3D Human Pose Estimation with Orientation and Semi-supervised Training","abstract":"3D human pose estimation is a vital task in computer vision, involving the prediction of human joint positions from images or videos to reconstruct a skeleton of a human in three-dimensional space. This technology is pivotal in various fields, including animation, security, human-computer interaction, and automotive safety, where it promotes both technological progress and enhanced human well-being. The advent of deep learning significantly advances the performance of 3D pose estimation by incorporating temporal information for predicting the spatial positions of human joints. However, traditional methods often fall short as they primarily focus on the spatial coordinates of joints and overlook the orientation and rotation of the connecting bones, which are crucial for a comprehensive understanding of human pose in 3D space. To address these limitations, we introduce Quater-GCN (Q-GCN), a directed graph convolutional network tailored to enhance pose estimation by orientation. Q-GCN excels by not only capturing the spatial dependencies among node joints through their coordinates but also integrating the dynamic context of bone rotations in 2D space. This approach enables a more sophisticated representation of human poses by also regressing the orientation of each bone in 3D space, moving beyond mere coordinate prediction. Furthermore, we complement our model with a semi-supervised training strategy that leverages unlabeled data, addressing the challenge of limited orientation ground truth data. Through comprehensive evaluations, Q-GCN has demonstrated outstanding performance against current state-of-the-art methods.","sentences":["3D human pose estimation is a vital task in computer vision, involving the prediction of human joint positions from images or videos to reconstruct a skeleton of a human in three-dimensional space.","This technology is pivotal in various fields, including animation, security, human-computer interaction, and automotive safety, where it promotes both technological progress and enhanced human well-being.","The advent of deep learning significantly advances the performance of 3D pose estimation by incorporating temporal information for predicting the spatial positions of human joints.","However, traditional methods often fall short as they primarily focus on the spatial coordinates of joints and overlook the orientation and rotation of the connecting bones, which are crucial for a comprehensive understanding of human pose in 3D space.","To address these limitations, we introduce Quater-GCN (Q-GCN), a directed graph convolutional network tailored to enhance pose estimation by orientation.","Q-GCN excels by not only capturing the spatial dependencies among node joints through their coordinates but also integrating the dynamic context of bone rotations in 2D space.","This approach enables a more sophisticated representation of human poses by also regressing the orientation of each bone in 3D space, moving beyond mere coordinate prediction.","Furthermore, we complement our model with a semi-supervised training strategy that leverages unlabeled data, addressing the challenge of limited orientation ground truth data.","Through comprehensive evaluations, Q-GCN has demonstrated outstanding performance against current state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.19279v1","category":"cs.CV"}
{"created":"2024-04-30 04:11:21","title":"Improved AutoEncoder with LSTM module and KL divergence","abstract":"The task of anomaly detection is to separate anomalous data from normal data in the dataset. Models such as deep convolutional autoencoder (CAE) network and deep supporting vector data description (SVDD) model have been universally employed and have demonstrated significant success in detecting anomalies. However, the over-reconstruction ability of CAE network for anomalous data can easily lead to high false negative rate in detecting anomalous data. On the other hand, the deep SVDD model has the drawback of feature collapse, which leads to a decrease of detection accuracy for anomalies. To address these problems, we propose the Improved AutoEncoder with LSTM module and Kullback-Leibler divergence (IAE-LSTM-KL) model in this paper. An LSTM network is added after the encoder to memorize feature representations of normal data. In the meanwhile, the phenomenon of feature collapse can also be mitigated by penalizing the featured input to SVDD module via KL divergence. The efficacy of the IAE-LSTM-KL model is validated through experiments on both synthetic and real-world datasets. Experimental results show that IAE-LSTM-KL model yields higher detection accuracy for anomalies. In addition, it is also found that the IAE-LSTM-KL model demonstrates enhanced robustness to contaminated outliers in the dataset.","sentences":["The task of anomaly detection is to separate anomalous data from normal data in the dataset.","Models such as deep convolutional autoencoder (CAE) network and deep supporting vector data description (SVDD) model have been universally employed and have demonstrated significant success in detecting anomalies.","However, the over-reconstruction ability of CAE network for anomalous data can easily lead to high false negative rate in detecting anomalous data.","On the other hand, the deep SVDD model has the drawback of feature collapse, which leads to a decrease of detection accuracy for anomalies.","To address these problems, we propose the Improved AutoEncoder with LSTM module and Kullback-Leibler divergence (IAE-LSTM-KL) model in this paper.","An LSTM network is added after the encoder to memorize feature representations of normal data.","In the meanwhile, the phenomenon of feature collapse can also be mitigated by penalizing the featured input to SVDD module via KL divergence.","The efficacy of the IAE-LSTM-KL model is validated through experiments on both synthetic and real-world datasets.","Experimental results show that IAE-LSTM-KL model yields higher detection accuracy for anomalies.","In addition, it is also found that the IAE-LSTM-KL model demonstrates enhanced robustness to contaminated outliers in the dataset."],"url":"http://arxiv.org/abs/2404.19247v1","category":"cs.LG"}
{"created":"2024-04-30 02:48:20","title":"Transcrib3D: 3D Referring Expression Resolution through Large Language Models","abstract":"If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment. Understanding 3D referring expressions is challenging -- it requires the ability to both parse the 3D structure of the scene and correctly ground free-form language in the presence of distraction and clutter. We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs). Transcrib3D uses text as the unifying medium, which allows us to sidestep the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data. As a demonstration of its effectiveness, Transcrib3D achieves state-of-the-art results on 3D reference resolution benchmarks, with a great leap in performance from previous multi-modality baselines. To improve upon zero-shot performance and facilitate local deployment on edge computers and robots, we propose self-correction for fine-tuning that trains smaller models, resulting in performance close to that of large models. We show that our method enables a real robot to perform pick-and-place tasks given queries that contain challenging referring expressions. Project site is at https://ripl.github.io/Transcrib3D.","sentences":["If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment.","Understanding 3D referring expressions is challenging -- it requires the ability to both parse the 3D structure of the scene and correctly ground free-form language in the presence of distraction and clutter.","We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs).","Transcrib3D uses text as the unifying medium, which allows us to sidestep the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data.","As a demonstration of its effectiveness, Transcrib3D achieves state-of-the-art results on 3D reference resolution benchmarks, with a great leap in performance from previous multi-modality baselines.","To improve upon zero-shot performance and facilitate local deployment on edge computers and robots, we propose self-correction for fine-tuning that trains smaller models, resulting in performance close to that of large models.","We show that our method enables a real robot to perform pick-and-place tasks given queries that contain challenging referring expressions.","Project site is at https://ripl.github.io/Transcrib3D."],"url":"http://arxiv.org/abs/2404.19221v1","category":"cs.CV"}
{"created":"2024-04-30 02:44:41","title":"Regression for matrix-valued data via Kronecker products factorization","abstract":"We study the matrix-variate regression problem $Y_i = \\sum_{k} \\beta_{1k} X_i \\beta_{2k}^{\\top} + E_i$ for $i=1,2\\dots,n$ in the high dimensional regime wherein the response $Y_i$ are matrices whose dimensions $p_{1}\\times p_{2}$ outgrow both the sample size $n$ and the dimensions $q_{1}\\times q_{2}$ of the predictor variables $X_i$ i.e., $q_{1},q_{2} \\ll n \\ll p_{1},p_{2}$. We propose an estimation algorithm, termed KRO-PRO-FAC, for estimating the parameters $\\{\\beta_{1k}\\} \\subset \\Re^{p_1 \\times q_1}$ and $\\{\\beta_{2k}\\} \\subset \\Re^{p_2 \\times q_2}$ that utilizes the Kronecker product factorization and rearrangement operations from Van Loan and Pitsianis (1993). The KRO-PRO-FAC algorithm is computationally efficient as it does not require estimating the covariance between the entries of the $\\{Y_i\\}$. We establish perturbation bounds between $\\hat{\\beta}_{1k} -\\beta_{1k}$ and $\\hat{\\beta}_{2k} - \\beta_{2k}$ in spectral norm for the setting where either the rows of $E_i$ or the columns of $E_i$ are independent sub-Gaussian random vectors. Numerical studies on simulated and real data indicate that our procedure is competitive, in terms of both estimation error and predictive accuracy, compared to other existing methods.","sentences":["We study the matrix-variate regression problem $Y_i = \\sum_{k} \\beta_{1k} X_i \\beta_{2k}^{\\top} + E_i$ for $i=1,2\\dots,n$ in the high dimensional regime wherein the response $Y_i$ are matrices whose dimensions $p_{1}\\times p_{2}$ outgrow both the sample size $n$ and the dimensions $q_{1}\\times q_{2}$ of the predictor variables $X_i$ i.e., $q_{1},q_{2} \\ll n \\ll p_{1},p_{2}$.","We propose an estimation algorithm, termed KRO-PRO-FAC, for estimating the parameters $\\{\\beta_{1k}\\} \\subset \\Re^{p_1 \\times q_1}$ and $\\{\\beta_{2k}\\} \\subset \\Re^{p_2 \\times q_2}$ that utilizes the Kronecker product factorization and rearrangement operations from Van Loan and Pitsianis (1993).","The KRO-PRO-FAC algorithm is computationally efficient as it does not require estimating the covariance between the entries of the $\\{Y_i\\}$. We establish perturbation bounds between $\\hat{\\beta}_{1k} -\\beta_{1k}$ and $\\hat{\\beta}_{2k} - \\beta_{2k}$ in spectral norm for the setting where either the rows of $E_i$ or the columns of $E_i$ are independent sub-Gaussian random vectors.","Numerical studies on simulated and real data indicate that our procedure is competitive, in terms of both estimation error and predictive accuracy, compared to other existing methods."],"url":"http://arxiv.org/abs/2404.19220v1","category":"stat.ML"}
{"created":"2024-04-30 02:39:01","title":"Flight Trajectory Prediction Using an Enhanced CNN-LSTM Network","abstract":"Aiming at the problem of low accuracy of flight trajectory prediction caused by the high speed of fighters, the diversity of tactical maneuvers, and the transient nature of situational change in close range air combat, this paper proposes an enhanced CNN-LSTM network as a fighter flight trajectory prediction method. Firstly, we extract spatial features from fighter trajectory data using CNN, aggregate spatial features of multiple fighters using the social-pooling module to capture geographic information and positional relationships in the trajectories, and use the attention mechanism to capture mutated trajectory features in air combat; subsequently, we extract temporal features by using the memory nature of LSTM to capture long-term temporal dependence in the trajectories; and finally, we merge the temporal and spatial features to predict the flight trajectories of enemy fighters. Extensive simulation experiments verify that the proposed method improves the trajectory prediction accuracy compared to the original CNN-LSTM method, with the improvements of 32% and 34% in ADE and FDE indicators.","sentences":["Aiming at the problem of low accuracy of flight trajectory prediction caused by the high speed of fighters, the diversity of tactical maneuvers, and the transient nature of situational change in close range air combat, this paper proposes an enhanced CNN-LSTM network as a fighter flight trajectory prediction method.","Firstly, we extract spatial features from fighter trajectory data using CNN, aggregate spatial features of multiple fighters using the social-pooling module to capture geographic information and positional relationships in the trajectories, and use the attention mechanism to capture mutated trajectory features in air combat; subsequently, we extract temporal features by using the memory nature of LSTM to capture long-term temporal dependence in the trajectories; and finally, we merge the temporal and spatial features to predict the flight trajectories of enemy fighters.","Extensive simulation experiments verify that the proposed method improves the trajectory prediction accuracy compared to the original CNN-LSTM method, with the improvements of 32% and 34% in ADE and FDE indicators."],"url":"http://arxiv.org/abs/2404.19218v1","category":"cs.LG"}
{"created":"2024-04-30 00:36:26","title":"Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking","abstract":"A necessary capability for humanoid robots is the ability to stand and walk while rejecting natural disturbances. Recent progress has been made using sim-to-real reinforcement learning (RL) to train such locomotion controllers, with approaches differing mainly in their reward functions. However, prior works lack a clear method to systematically test new reward functions and compare controller performance through repeatable experiments. This limits our understanding of the trade-offs between approaches and hinders progress. To address this, we propose a low-cost, quantitative benchmarking method to evaluate and compare the real-world performance of standing and walking (SaW) controllers on metrics like command following, disturbance recovery, and energy efficiency. We also revisit reward function design and construct a minimally constraining reward function to train SaW controllers. We experimentally verify that our benchmarking framework can identify areas for improvement, which can be systematically addressed to enhance the policies. We also compare our new controller to state-of-the-art controllers on the Digit humanoid robot. The results provide clear quantitative trade-offs among the controllers and suggest directions for future improvements to the reward functions and expansion of the benchmarks.","sentences":["A necessary capability for humanoid robots is the ability to stand and walk while rejecting natural disturbances.","Recent progress has been made using sim-to-real reinforcement learning (RL) to train such locomotion controllers, with approaches differing mainly in their reward functions.","However, prior works lack a clear method to systematically test new reward functions and compare controller performance through repeatable experiments.","This limits our understanding of the trade-offs between approaches and hinders progress.","To address this, we propose a low-cost, quantitative benchmarking method to evaluate and compare the real-world performance of standing and walking (SaW) controllers on metrics like command following, disturbance recovery, and energy efficiency.","We also revisit reward function design and construct a minimally constraining reward function to train SaW controllers.","We experimentally verify that our benchmarking framework can identify areas for improvement, which can be systematically addressed to enhance the policies.","We also compare our new controller to state-of-the-art controllers on the Digit humanoid robot.","The results provide clear quantitative trade-offs among the controllers and suggest directions for future improvements to the reward functions and expansion of the benchmarks."],"url":"http://arxiv.org/abs/2404.19173v1","category":"cs.RO"}
{"created":"2024-04-29 22:31:21","title":"Integrating Present and Past in Unsupervised Continual Learning","abstract":"We formulate a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that are specific to the present and the past data, encompassing stability, plasticity, and cross-task consolidation. The framework reveals that many existing UCL approaches overlook cross-task consolidation and try to balance plasticity and stability in a shared embedding space. This results in worse performance due to a lack of within-task data diversity and reduced effectiveness in learning the current task. Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks, including two novel benchmarks proposed in this paper featuring semantically structured task sequences. Compared to standard benchmarks, these two structured benchmarks more closely resemble visual signals received by humans and animals when navigating real-world environments. Finally, we show some preliminary evidence that continual models can benefit from such realistic learning scenarios.","sentences":["We formulate a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that are specific to the present and the past data, encompassing stability, plasticity, and cross-task consolidation.","The framework reveals that many existing UCL approaches overlook cross-task consolidation and try to balance plasticity and stability in a shared embedding space.","This results in worse performance due to a lack of within-task data diversity and reduced effectiveness in learning the current task.","Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks, including two novel benchmarks proposed in this paper featuring semantically structured task sequences.","Compared to standard benchmarks, these two structured benchmarks more closely resemble visual signals received by humans and animals when navigating real-world environments.","Finally, we show some preliminary evidence that continual models can benefit from such realistic learning scenarios."],"url":"http://arxiv.org/abs/2404.19132v1","category":"cs.LG"}
{"created":"2024-04-29 21:38:39","title":"Characterising Payload Entropy in Packet Flows","abstract":"Accurate and timely detection of cyber threats is critical to keeping our online economy and data safe. A key technique in early detection is the classification of unusual patterns of network behaviour, often hidden as low-frequency events within complex time-series packet flows. One of the ways in which such anomalies can be detected is to analyse the information entropy of the payload within individual packets, since changes in entropy can often indicate suspicious activity - such as whether session encryption has been compromised, or whether a plaintext channel has been co-opted as a covert channel. To decide whether activity is anomalous we need to compare real-time entropy values with baseline values, and while the analysis of entropy in packet data is not particularly new, to the best of our knowledge there are no published baselines for payload entropy across common network services. We offer two contributions: 1) We analyse several large packet datasets to establish baseline payload information entropy values for common network services, 2) We describe an efficient method for engineering entropy metrics when performing flow recovery from live or offline packet data, which can be expressed within feature subsets for subsequent analysis and machine learning applications.","sentences":["Accurate and timely detection of cyber threats is critical to keeping our online economy and data safe.","A key technique in early detection is the classification of unusual patterns of network behaviour, often hidden as low-frequency events within complex time-series packet flows.","One of the ways in which such anomalies can be detected is to analyse the information entropy of the payload within individual packets, since changes in entropy can often indicate suspicious activity - such as whether session encryption has been compromised, or whether a plaintext channel has been co-opted as a covert channel.","To decide whether activity is anomalous we need to compare real-time entropy values with baseline values, and while the analysis of entropy in packet data is not particularly new, to the best of our knowledge there are no published baselines for payload entropy across common network services.","We offer two contributions: 1) We analyse several large packet datasets to establish baseline payload information entropy values for common network services, 2) We describe an efficient method for engineering entropy metrics when performing flow recovery from live or offline packet data, which can be expressed within feature subsets for subsequent analysis and machine learning applications."],"url":"http://arxiv.org/abs/2404.19121v1","category":"cs.CR"}
{"created":"2024-04-29 21:29:26","title":"Disentangling Exploration from Exploitation","abstract":"Starting from Robbins (1952), the literature on experimentation via multi-armed bandits has wed exploration and exploitation. Nonetheless, in many applications, agents' exploration and exploitation need not be intertwined: a policymaker may assess new policies different than the status quo; an investor may evaluate projects outside her portfolio. We characterize the optimal experimentation policy when exploration and exploitation are disentangled in the case of Poisson bandits, allowing for general news structures. The optimal policy features complete learning asymptotically, exhibits lots of persistence, but cannot be identified by an index a la Gittins. Disentanglement is particularly valuable for intermediate parameter values.","sentences":["Starting from Robbins (1952), the literature on experimentation via multi-armed bandits has wed exploration and exploitation.","Nonetheless, in many applications, agents' exploration and exploitation need not be intertwined: a policymaker may assess new policies different than the status quo; an investor may evaluate projects outside her portfolio.","We characterize the optimal experimentation policy when exploration and exploitation are disentangled in the case of Poisson bandits, allowing for general news structures.","The optimal policy features complete learning asymptotically, exhibits lots of persistence, but cannot be identified by an index a la Gittins.","Disentanglement is particularly valuable for intermediate parameter values."],"url":"http://arxiv.org/abs/2404.19116v1","category":"econ.TH"}
{"created":"2024-04-29 19:52:09","title":"Longitudinal Mammogram Risk Prediction","abstract":"Breast cancer is one of the leading causes of mortality among women worldwide. Early detection and risk assessment play a crucial role in improving survival rates. Therefore, annual or biennial mammograms are often recommended for screening in high-risk groups. Mammograms are typically interpreted by expert radiologists based on the Breast Imaging Reporting and Data System (BI-RADS), which provides a uniform way to describe findings and categorizes them to indicate the level of concern for breast cancer. Recently, machine learning (ML) and computational approaches have been developed to automate and improve the interpretation of mammograms. However, both BI-RADS and the ML-based methods focus on the analysis of data from the present and sometimes the most recent prior visit. While it is clear that temporal changes in image features of the longitudinal scans should carry value for quantifying breast cancer risk, no prior work has conducted a systematic study of this. In this paper, we extend a state-of-the-art ML model to ingest an arbitrary number of longitudinal mammograms and predict future breast cancer risk. On a large-scale dataset, we demonstrate that our model, LoMaR, achieves state-of-the-art performance when presented with only the present mammogram. Furthermore, we use LoMaR to characterize the predictive value of prior visits. Our results show that longer histories (e.g., up to four prior annual mammograms) can significantly boost the accuracy of predicting future breast cancer risk, particularly beyond the short-term. Our code and model weights are available at https://github.com/batuhankmkaraman/LoMaR.","sentences":["Breast cancer is one of the leading causes of mortality among women worldwide.","Early detection and risk assessment play a crucial role in improving survival rates.","Therefore, annual or biennial mammograms are often recommended for screening in high-risk groups.","Mammograms are typically interpreted by expert radiologists based on the Breast Imaging Reporting and Data System (BI-RADS), which provides a uniform way to describe findings and categorizes them to indicate the level of concern for breast cancer.","Recently, machine learning (ML) and computational approaches have been developed to automate and improve the interpretation of mammograms.","However, both BI-RADS and the ML-based methods focus on the analysis of data from the present and sometimes the most recent prior visit.","While it is clear that temporal changes in image features of the longitudinal scans should carry value for quantifying breast cancer risk, no prior work has conducted a systematic study of this.","In this paper, we extend a state-of-the-art ML model to ingest an arbitrary number of longitudinal mammograms and predict future breast cancer risk.","On a large-scale dataset, we demonstrate that our model, LoMaR, achieves state-of-the-art performance when presented with only the present mammogram.","Furthermore, we use LoMaR to characterize the predictive value of prior visits.","Our results show that longer histories (e.g., up to four prior annual mammograms) can significantly boost the accuracy of predicting future breast cancer risk, particularly beyond the short-term.","Our code and model weights are available at https://github.com/batuhankmkaraman/LoMaR."],"url":"http://arxiv.org/abs/2404.19083v1","category":"eess.IV"}
{"created":"2024-04-29 19:32:50","title":"Learning Sparse High-Dimensional Matrix-Valued Graphical Models From Dependent Data","abstract":"We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional, stationary matrix-variate Gaussian time series. All past work on high-dimensional matrix graphical models assumes that independent and identically distributed (i.i.d.) observations of the matrix-variate are available. Here we allow dependent observations. We consider a sparse-group lasso-based frequency-domain formulation of the problem with a Kronecker-decomposable power spectral density (PSD), and solve it via an alternating direction method of multipliers (ADMM) approach. The problem is bi-convex which is solved via flip-flop optimization. We provide sufficient conditions for local convergence in the Frobenius norm of the inverse PSD estimators to the true value. This result also yields a rate of convergence. We illustrate our approach using numerical examples utilizing both synthetic and real data.","sentences":["We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional, stationary matrix-variate Gaussian time series.","All past work on high-dimensional matrix graphical models assumes that independent and identically distributed (i.i.d.) observations of the matrix-variate are available.","Here we allow dependent observations.","We consider a sparse-group lasso-based frequency-domain formulation of the problem with a Kronecker-decomposable power spectral density (PSD), and solve it via an alternating direction method of multipliers (ADMM) approach.","The problem is bi-convex which is solved via flip-flop optimization.","We provide sufficient conditions for local convergence in the Frobenius norm of the inverse PSD estimators to the true value.","This result also yields a rate of convergence.","We illustrate our approach using numerical examples utilizing both synthetic and real data."],"url":"http://arxiv.org/abs/2404.19073v1","category":"stat.ML"}
{"created":"2024-04-29 18:28:36","title":"GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting","abstract":"We present GStalker, a 3D audio-driven talking face generation model with Gaussian Splatting for both fast training (40 minutes) and real-time rendering (125 FPS) with a 3$\\sim$5 minute video for training material, in comparison with previous 2D and 3D NeRF-based modeling frameworks which require hours of training and seconds of rendering per frame. Specifically, GSTalker learns an audio-driven Gaussian deformation field to translate and transform 3D Gaussians to synchronize with audio information, in which multi-resolution hashing grid-based tri-plane and temporal smooth module are incorporated to learn accurate deformation for fine-grained facial details. In addition, a pose-conditioned deformation field is designed to model the stabilized torso. To enable efficient optimization of the condition Gaussian deformation field, we initialize 3D Gaussians by learning a coarse static Gaussian representation. Extensive experiments in person-specific videos with audio tracks validate that GSTalker can generate high-fidelity and audio-lips synchronized results with fast training and real-time rendering speed.","sentences":["We present GStalker, a 3D audio-driven talking face generation model with Gaussian Splatting for both fast training (40 minutes) and real-time rendering (125 FPS) with a 3$\\sim$5 minute video for training material, in comparison with previous 2D and 3D NeRF-based modeling frameworks which require hours of training and seconds of rendering per frame.","Specifically, GSTalker learns an audio-driven Gaussian deformation field to translate and transform 3D Gaussians to synchronize with audio information, in which multi-resolution hashing grid-based tri-plane and temporal smooth module are incorporated to learn accurate deformation for fine-grained facial details.","In addition, a pose-conditioned deformation field is designed to model the stabilized torso.","To enable efficient optimization of the condition Gaussian deformation field, we initialize 3D Gaussians by learning a coarse static Gaussian representation.","Extensive experiments in person-specific videos with audio tracks validate that GSTalker can generate high-fidelity and audio-lips synchronized results with fast training and real-time rendering speed."],"url":"http://arxiv.org/abs/2404.19040v1","category":"cs.CV"}
{"created":"2024-04-30 11:45:49","title":"Global Phase Helps in Quantum Search: Yet Another Look at the Welded Tree Problem","abstract":"Up to now, relatively few exponential quantum speed-ups have been achieved. Out of them, the welded tree problem (Childs, Cleve, Deotto, Farhi, Gutmann, and Spielman'2003) is one of the unusual examples, as the exponential speed-up is attained by a quantum walk. In this paper, we give a very short proof of the optimal linear hitting time for this problem by a discrete-time quantum walk, which is based on a simple modification of the electric quantum walk framework. The same technique can be applied to other 1-dimensional hierarchical graphs, yielding results similar to (Balasubramanian, Li, and Harrow'2023).","sentences":["Up to now, relatively few exponential quantum speed-ups have been achieved.","Out of them, the welded tree problem (Childs, Cleve, Deotto, Farhi, Gutmann, and Spielman'2003) is one of the unusual examples, as the exponential speed-up is attained by a quantum walk.","In this paper, we give a very short proof of the optimal linear hitting time for this problem by a discrete-time quantum walk, which is based on a simple modification of the electric quantum walk framework.","The same technique can be applied to other 1-dimensional hierarchical graphs, yielding results similar to (Balasubramanian, Li, and Harrow'2023)."],"url":"http://arxiv.org/abs/2404.19476v1","category":"quant-ph"}
{"created":"2024-04-30 10:17:01","title":"From Quantum Mechanics to Quantum Software Engineering","abstract":"Victor Hugo's timeless observation, \"Nothing is more powerful than an idea whose time has come\", resonates today as Quantum Computing, once only a dream of a physicist, stands at the threshold of reality with the potential to revolutionise the world. To comprehend the surge of attention it commands today, one must delve into the motivations that birthed and nurtured Quantum Computing. While the past of Quantum Computing provides insights into the present, the future could unfold through the lens of Quantum Software Engineering. Quantum Software Engineering, guided by its principles and methodologies investigates the most effective ways to interact with Quantum Computers to unlock their true potential and usher in a new era of possibilities. To gain insight into the present landscape and anticipate the trajectory of Quantum Computing and Quantum Software Engineering, this paper embarks on a journey through their evolution and outlines potential directions for future research.","sentences":["Victor Hugo's timeless observation, \"Nothing is more powerful than an idea whose time has come\", resonates today as Quantum Computing, once only a dream of a physicist, stands at the threshold of reality with the potential to revolutionise the world.","To comprehend the surge of attention it commands today, one must delve into the motivations that birthed and nurtured Quantum Computing.","While the past of Quantum Computing provides insights into the present, the future could unfold through the lens of Quantum Software Engineering.","Quantum Software Engineering, guided by its principles and methodologies investigates the most effective ways to interact with Quantum Computers to unlock their true potential and usher in a new era of possibilities.","To gain insight into the present landscape and anticipate the trajectory of Quantum Computing and Quantum Software Engineering, this paper embarks on a journey through their evolution and outlines potential directions for future research."],"url":"http://arxiv.org/abs/2404.19428v1","category":"quant-ph"}
{"created":"2024-04-30 09:11:04","title":"SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction using Knowledge Graphs","abstract":"Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene including traffic participants, road topology, traffic signs as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. This paper describes a method SemanticFormer to predict multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. We extract high-level information in the form of semantic meta-paths from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. The proposed architecture comprises a hierarchical heterogeneous graph encoder, which can capture spatio-temporal and relational information across agents and between agents and road elements, and a predictor that fuses the different encodings and decodes trajectories with probabilities. Finally, a refinement module evaluates permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to the state-of-the-art methods.","sentences":["Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene including traffic participants, road topology, traffic signs as well as their semantic relations to each other.","Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently.","This paper describes a method SemanticFormer to predict multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach.","We extract high-level information in the form of semantic meta-paths from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories.","The proposed architecture comprises a hierarchical heterogeneous graph encoder, which can capture spatio-temporal and relational information across agents and between agents and road elements, and a predictor that fuses the different encodings and decodes trajectories with probabilities.","Finally, a refinement module evaluates permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories.","Evaluation of the nuScenes benchmark demonstrates improved performance compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.19379v1","category":"cs.CV"}
{"created":"2024-04-30 09:05:56","title":"Perspectives of a single-anode cylindrical chamber operating in ionization mode and high gas pressure","abstract":"As part of the R2D2 (Rare Decays with Radial Detector) R&D, the use of a gas detector with a spherical or cylindrical cathode, equipped with a single anode and operating at high pressure, was studied for the search of rare phenomena such as neutrinoless double-beta decay. The presented measurements were obtained with a cylindrical detector, covering gas pressures ranging from 1 to 10 bar in argon and 1 to 6 bar in xenon, using both a point-like source of $^{210}$Po (5.3 MeV $\\alpha$ ) and a diffuse source of $^{222}$Rn (5.5 MeV $\\alpha$). Analysis and interpretation of the data were developed using the anodic current waveform. Similar detection performances were achieved with both gases, and comparable energy resolutions were measured with both sources. As long as the purity of the gas was sufficient, no significant degradation of the measured energy was observed by increasing the pressure. At the highest operating pressure, an energy resolution better than 1.5% full-width at half-maximum (FWHM) was obtained for both gaseous media, although optimal noise conditions were not reached.","sentences":["As part of the R2D2 (Rare Decays with Radial Detector) R&D, the use of a gas detector with a spherical or cylindrical cathode, equipped with a single anode and operating at high pressure, was studied for the search of rare phenomena such as neutrinoless double-beta decay.","The presented measurements were obtained with a cylindrical detector, covering gas pressures ranging from 1 to 10 bar in argon and 1 to 6 bar in xenon, using both a point-like source of $^{210}$Po (5.3 MeV $\\alpha$ ) and a diffuse source of $^{222}$Rn (5.5 MeV $\\alpha$).","Analysis and interpretation of the data were developed using the anodic current waveform.","Similar detection performances were achieved with both gases, and comparable energy resolutions were measured with both sources.","As long as the purity of the gas was sufficient, no significant degradation of the measured energy was observed by increasing the pressure.","At the highest operating pressure, an energy resolution better than 1.5% full-width at half-maximum (FWHM) was obtained for both gaseous media, although optimal noise conditions were not reached."],"url":"http://arxiv.org/abs/2404.19374v1","category":"physics.ins-det"}
{"created":"2024-04-30 07:28:09","title":"A characterization of entangled two-qubit states via partial-transpose-moments","abstract":"Although quantum entanglement is an important resource, its characterization is quite challenging. The partial transposition is a common method to detect bipartite entanglement. In this paper, the authors study the partial-transpose(PT)-moments of two-qubit states,and completely describe the whole region, composed of the second and third PT-moments, for all two-qubit states. Furthermore, they determine the accurate region corresponding to all entangled two-qubit states. The states corresponding to those boundary points of the whole region, and to the border lines between separable and entangled states are analyzed. As an application, they characterize the entangled region of PT-moments for the two families of Werner states and Bell-diagonal states. The relations between entanglement and the pairs of PT-moments are revealed from these typical examples. They also numerically plot the whole region of possible PT-moments for all two-qubit X-states, and find that this region is almost the same as the whole region of PT-moments for all two-qubit states. Moreover, they extend their results to detect the entanglement of multiqubit states. By utilizing the PT-moment-based method to characterize the entanglement of the multiqubit states mixed by the GHZ and W states, they propose an operational way of verifying the genuine entanglement in such states.","sentences":["Although quantum entanglement is an important resource, its characterization is quite challenging.","The partial transposition is a common method to detect bipartite entanglement.","In this paper, the authors study the partial-transpose(PT)-moments of two-qubit states,and completely describe the whole region, composed of the second and third PT-moments, for all two-qubit states.","Furthermore, they determine the accurate region corresponding to all entangled two-qubit states.","The states corresponding to those boundary points of the whole region, and to the border lines between separable and entangled states are analyzed.","As an application, they characterize the entangled region of PT-moments for the two families of Werner states and Bell-diagonal states.","The relations between entanglement and the pairs of PT-moments are revealed from these typical examples.","They also numerically plot the whole region of possible PT-moments for all two-qubit X-states, and find that this region is almost the same as the whole region of PT-moments for all two-qubit states.","Moreover, they extend their results to detect the entanglement of multiqubit states.","By utilizing the PT-moment-based method to characterize the entanglement of the multiqubit states mixed by the GHZ and W states, they propose an operational way of verifying the genuine entanglement in such states."],"url":"http://arxiv.org/abs/2404.19308v1","category":"quant-ph"}
{"created":"2024-04-30 02:33:37","title":"Optimal quantum strategy for locating Unruh channels","abstract":"From the perspective of quantum information theory, the effect of Unruh radiation on a two-level accelerated detector can be modeled as a quantum channel. In this work, we employ the tools of channel-position finding to locate Unruh channels. The signal-idler and idler-free protocols are explored to determine the position of the target Unruh channel within a sequence of background channels. We derive the fidelity-based bounds for the ultimate error probability of each strategy and obtain the conditions where the signal-idler protocol is superior to the protocol involving idler-free states. It is found that the lower bound of the error probability for the signal-idler scheme exhibits clear advantages in all cases, while the idler-free scheme can only be implemented when the temperature of the two channels is very close and the number of initial states is insufficient. Interestingly, it is shown that the optimal detection protocol relies on the residual correlations shared between the emitted probe state and the retained idler modes.","sentences":["From the perspective of quantum information theory, the effect of Unruh radiation on a two-level accelerated detector can be modeled as a quantum channel.","In this work, we employ the tools of channel-position finding to locate Unruh channels.","The signal-idler and idler-free protocols are explored to determine the position of the target Unruh channel within a sequence of background channels.","We derive the fidelity-based bounds for the ultimate error probability of each strategy and obtain the conditions where the signal-idler protocol is superior to the protocol involving idler-free states.","It is found that the lower bound of the error probability for the signal-idler scheme exhibits clear advantages in all cases, while the idler-free scheme can only be implemented when the temperature of the two channels is very close and the number of initial states is insufficient.","Interestingly, it is shown that the optimal detection protocol relies on the residual correlations shared between the emitted probe state and the retained idler modes."],"url":"http://arxiv.org/abs/2404.19216v1","category":"gr-qc"}
{"created":"2024-04-29 23:55:14","title":"Optimal Bridge, Twin Bridges and Beyond: Inserting Edges into a Road Network to Minimize the Constrained Diameters","abstract":"Given a road network modelled as a planar straight-line graph $G=(V,E)$ with $|V|=n$, let $(u,v)\\in V\\times V$, the shortest path (distance) between $u,v$ is denoted as $\\delta_G(u,v)$. Let $\\delta(G)=\\max_{(u,v)}\\delta_G(u,v)$, for $(u,v)\\in V\\times V$, which is called the diameter of $G$. Given a disconnected road network modelled as two disjoint trees $T_1$ and $T_2$, this paper first aims at inserting one and two edges (bridges) between them to minimize the (constrained) diameter $\\delta(T_1\\cup T_2\\cup I_j)$ going through the inserted edges, where $I_j, j=1,2$, is the set of inserted edges with $|I_1|=1$ and $|I_2|=2$. The corresponding problems are called the {\\em optimal bridge} and {\\em twin bridges} problems. Since when more than one edge are inserted between two trees the resulting graph is becoming more complex, for the general network $G$ we consider the problem of inserting a minimum of $k$ edges such that the shortest distances between a set of $m$ pairs $P=\\{(u_i,v_i)\\mid u_i,v_i\\in V, i\\in [m]\\}$, $\\delta_G(u_i,v_i)$'s, are all decreased.   The main results of this paper are summarized as follows:   (1) We show that the optimal bridge problem can be solved in $O(n^2)$ time and that a variation of it has a near-quadratic lower bound unless SETH fails. The proof also implies that the famous 3-SUM problem does have a near-quadratic lower bound for large integers, e.g., each of the $n$ input integers has $\\Omega(\\log n)$ decimal digits. We then give a simple factor-2 $O(n\\log n)$ time approximation algorithm for the optimal bridge problem.   (2) We present an $O(n^4)$ time algorithm to solve the twin bridges problem, exploiting some new property not in the optimal bridge problem.   (3) For the general problem of inserting $k$ edges to reduce the (graph) distances between $m$ given pairs, we show that the problem is NP-complete.","sentences":["Given a road network modelled as a planar straight-line graph $G=(V,E)$ with $|V|=n$, let $(u,v)\\in V\\times V$, the shortest path (distance) between $u,v$ is denoted as $\\delta_G(u,v)$. Let $\\delta(G)=\\max_{(u,v)}\\delta_G(u,v)$, for $(u,v)\\in V\\times V$, which is called the diameter of $G$. Given a disconnected road network modelled as two disjoint trees $T_1$ and $T_2$, this paper first aims at inserting one and two edges (bridges) between them to minimize the (constrained) diameter $\\delta(T_1\\cup T_2\\cup I_j)$ going through the inserted edges, where $I_j, j=1,2$, is the set of inserted edges with $|I_1|=1$ and $|I_2|=2$. The corresponding problems are called the {\\em optimal bridge} and {\\em twin bridges} problems.","Since when more than one edge are inserted between two trees the resulting graph is becoming more complex, for the general network $G$ we consider the problem of inserting a minimum of $k$ edges such that the shortest distances between a set of $m$ pairs $P=\\{(u_i,v_i)\\mid u_i,v_i\\in V, i\\in [m]\\}$, $\\delta_G(u_i,v_i)$'s, are all decreased.   ","The main results of this paper are summarized as follows:   (1) We show that the optimal bridge problem can be solved in $O(n^2)$ time and that a variation of it has a near-quadratic lower bound unless SETH fails.","The proof also implies that the famous 3-SUM problem does have a near-quadratic lower bound for large integers, e.g., each of the $n$ input integers has $\\Omega(\\log n)$ decimal digits.","We then give a simple factor-2 $O(n\\log n)$ time approximation algorithm for the optimal bridge problem.   ","(2) We present an $O(n^4)$ time algorithm to solve the twin bridges problem, exploiting some new property not in the optimal bridge problem.   ","(3) For the general problem of inserting $k$ edges to reduce the (graph) distances between $m$ given pairs, we show that the problem is NP-complete."],"url":"http://arxiv.org/abs/2404.19164v1","category":"cs.CG"}
{"created":"2024-04-29 23:33:49","title":"Room temperature realization of artificial chiral magnets with reprogrammable magnon nonreciprocity at zero field","abstract":"Chiral magnets are materials which possess unique helical arrangements of magnetic moments, which give rise to nonreciprocal transport and fascinating physics phenomena. On the one hand, their exploration is guided by the prospects of unconventional signal processing, computation schemes and magnetic memory. On the other hand, progress in applications is hindered by the challenging materials synthesis, limited scalability and typically low critical temperature. Here, we report the creation and exploration of artificial chiral magnets (ACMs) at room temperature. By employing a mass production compatible deposition technology, we synthesize ACMs, which consist of helical Ni surfaces on central cylinders. Using optical microscopy, we reveal nonreciprocal magnon transport at GHz frequencies. It is controlled by programmable toroidal moments which result from the ACM's geometrical handedness and field-dependent spin chirality. We present materials-by-design rules which optimize the helically curved ferromagnets for 3D nonreciprocal transport at room temperature and zero magnetic field.","sentences":["Chiral magnets are materials which possess unique helical arrangements of magnetic moments, which give rise to nonreciprocal transport and fascinating physics phenomena.","On the one hand, their exploration is guided by the prospects of unconventional signal processing, computation schemes and magnetic memory.","On the other hand, progress in applications is hindered by the challenging materials synthesis, limited scalability and typically low critical temperature.","Here, we report the creation and exploration of artificial chiral magnets (ACMs) at room temperature.","By employing a mass production compatible deposition technology, we synthesize ACMs, which consist of helical Ni surfaces on central cylinders.","Using optical microscopy, we reveal nonreciprocal magnon transport at GHz frequencies.","It is controlled by programmable toroidal moments which result from the ACM's geometrical handedness and field-dependent spin chirality.","We present materials-by-design rules which optimize the helically curved ferromagnets for 3D nonreciprocal transport at room temperature and zero magnetic field."],"url":"http://arxiv.org/abs/2404.19153v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 21:59:07","title":"Accelerating Production LLMs with Combined Token/Embedding Speculators","abstract":"This technical report describes the design and training of novel speculative decoding draft models, for accelerating the inference speeds of large language models in a production environment. By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams, which the base model then accepts or rejects. This allows us to effectively predict multiple tokens per inference forward pass, accelerating wall-clock inference speeds of highly optimized base model implementations by a factor of 2-3x. We explore these initial results and describe next steps for further improvements.","sentences":["This technical report describes the design and training of novel speculative decoding draft models, for accelerating the inference speeds of large language models in a production environment.","By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams, which the base model then accepts or rejects.","This allows us to effectively predict multiple tokens per inference forward pass, accelerating wall-clock inference speeds of highly optimized base model implementations by a factor of 2-3x.","We explore these initial results and describe next steps for further improvements."],"url":"http://arxiv.org/abs/2404.19124v1","category":"cs.CL"}
{"created":"2024-04-29 20:56:04","title":"An Oracle with no $\\mathrm{UP}$-Complete Sets, but $\\mathrm{NP}=\\mathrm{PSPACE}$","abstract":"We construct an oracle relative to which $\\mathrm{NP} = \\mathrm{PSPACE}$, but $\\mathrm{UP}$ has no many-one complete sets. This combines the properties of an oracle by Hartmanis and Hemachandra [HH88] and one by Ogiwara and Hemachandra [OH93].   The oracle provides new separations of classical conjectures on optimal proof systems and complete sets in promise classes. This answers several questions by Pudl\\'ak [Pud17], e.g., the implications $\\mathsf{UP} \\Longrightarrow \\mathsf{CON}^{\\mathsf{N}}$ and $\\mathsf{SAT} \\Longrightarrow \\mathsf{TFNP}$ are false relative to our oracle.   Moreover, the oracle demonstrates that, in principle, it is possible that $\\mathrm{TFNP}$-complete problems exist, while at the same time $\\mathrm{SAT}$ has no p-optimal proof systems.","sentences":["We construct an oracle relative to which $\\mathrm{NP} = \\mathrm{PSPACE}$, but $\\mathrm{UP}$ has no many-one complete sets.","This combines the properties of an oracle by Hartmanis and Hemachandra","[HH88] and one by Ogiwara and Hemachandra","[OH93].   ","The oracle provides new separations of classical conjectures on optimal proof systems and complete sets in promise classes.","This answers several questions by Pudl\\'ak","[Pud17], e.g., the implications $\\mathsf{UP} \\Longrightarrow \\mathsf{CON}^{\\mathsf{N}}$ and $\\mathsf{SAT} \\Longrightarrow \\mathsf{TFNP}$ are false relative to our oracle.   ","Moreover, the oracle demonstrates that, in principle, it is possible that $\\mathrm{TFNP}$-complete problems exist, while at the same time $\\mathrm{SAT}$ has no p-optimal proof systems."],"url":"http://arxiv.org/abs/2404.19104v1","category":"cs.CC"}
{"created":"2024-04-29 20:46:16","title":"Modelling the Track of the GD-1 Stellar Stream Inside a Host with a Fermionic Dark Matter Core-Halo Distribution","abstract":"Traditional studies on stellar streams typically involve phenomenological $\\Lambda$CDM halos or ad hoc dark matter (DM) profiles with different degrees of triaxiality, which preclude to gain insights into the nature and mass of the DM particles. Recently, a Maximum Entropy Principle of halo formation has been applied to provide a DM halo model which incorporates the fermionic (quantum) nature of the particles, while leading to DM profiles which depend on the fermion mass. Such profiles develop a more general dense core - diluted halo morphology able to explain the Galactic rotation curve, while the degenerate fermion core can mimic the central massive black hole (BH). We attempt to model the GD-1 stellar stream using a spherical core-halo DM distribution for the host, which, at the same time, explains the dynamics of the S-cluster stars through its degenerate fermion-core with no central BH. We used two optimization algorithms in order to fit both the initial conditions of the stream orbit and the fermionic model. The stream observables are 5D phase-space data from the Gaia DR2 survey. We were able to find good fits for both the GD-1 stream and the S-stars for a family of fermionic core-halo profiles parameterized by the fermion mass. This work provides evidence that the fermionic profile is a reliable model for both the massive central object and the DM of the Galaxy. Remarkably, this model predicts a total MW mass of $2.3\\times 10^{11}M_{\\odot}$ which is in agreement with recent mass estimates obtained from Gaia DR3 rotation curves (Gaia RC). In summary, with one single fermionic model for the DM distribution of the MW, we obtain a good fit in three totally different distance scales of the Galaxy: $\\sim 10^{-6}$ kpc (central, S-stars), $\\sim14$ kpc (mid, GD-1) and $\\sim 30$ kpc (boundary, Gaia RC mass estimate).","sentences":["Traditional studies on stellar streams typically involve phenomenological $\\Lambda$CDM halos or ad hoc dark matter (DM) profiles with different degrees of triaxiality, which preclude to gain insights into the nature and mass of the DM particles.","Recently, a Maximum Entropy Principle of halo formation has been applied to provide a DM halo model which incorporates the fermionic (quantum) nature of the particles, while leading to DM profiles which depend on the fermion mass.","Such profiles develop a more general dense core - diluted halo morphology able to explain the Galactic rotation curve, while the degenerate fermion core can mimic the central massive black hole (BH).","We attempt to model the GD-1 stellar stream using a spherical core-halo DM distribution for the host, which, at the same time, explains the dynamics of the S-cluster stars through its degenerate fermion-core with no central BH.","We used two optimization algorithms in order to fit both the initial conditions of the stream orbit and the fermionic model.","The stream observables are 5D phase-space data from the Gaia DR2 survey.","We were able to find good fits for both the GD-1 stream and the S-stars for a family of fermionic core-halo profiles parameterized by the fermion mass.","This work provides evidence that the fermionic profile is a reliable model for both the massive central object and the DM of the Galaxy.","Remarkably, this model predicts a total MW mass of $2.3\\times 10^{11}M_{\\odot}$ which is in agreement with recent mass estimates obtained from Gaia DR3 rotation curves (Gaia RC).","In summary, with one single fermionic model for the DM distribution of the MW, we obtain a good fit in three totally different distance scales of the Galaxy: $\\sim 10^{-6}$ kpc (central, S-stars), $\\sim14$ kpc (mid, GD-1) and $\\sim 30$ kpc (boundary, Gaia RC mass estimate)."],"url":"http://arxiv.org/abs/2404.19102v1","category":"astro-ph.GA"}
{"created":"2024-04-29 20:11:40","title":"Transmit Power Optimization for Integrated Sensing and Backscatter Communication","abstract":"Ambient Internet of Things networks use low-cost, low-power backscatter tags in various industry applications. By exploiting those tags, we introduce the integrated sensing and backscatter communication (ISABC) system, featuring multiple backscatter tags, a user (reader), and a full-duplex base station (BS) that integrates sensing and (backscatter) communications. The BS undertakes dual roles of detecting backscatter tags and communicating with the user, leveraging the same temporal and frequency resources. The tag-reflected BS signals offer data to the user and enable the BS to sense the environment simultaneously. We derive both user and tag communication rates and the sensing rate of the BS. We jointly optimize the transmit/received beamformers and tag reflection coefficients to minimize the total BS power. To solve this problem, we employ the alternating optimization technique. We offer a closed-form solution for the received beamformers while utilizing semi-definite relaxation and slack-optimization for transmit beamformers and power reflection coefficients, respectively. For example, with ten transmit/reception antennas at the BS, ISABC delivers a 75% sum communication and sensing rates gain over a traditional backscatter while requiring a 3.4% increase in transmit power. Furthermore, ISABC with active tags only requires a 0.24% increase in transmit power over conventional integrated sensing and communication.","sentences":["Ambient Internet of Things networks use low-cost, low-power backscatter tags in various industry applications.","By exploiting those tags, we introduce the integrated sensing and backscatter communication (ISABC) system, featuring multiple backscatter tags, a user (reader), and a full-duplex base station (BS) that integrates sensing and (backscatter) communications.","The BS undertakes dual roles of detecting backscatter tags and communicating with the user, leveraging the same temporal and frequency resources.","The tag-reflected BS signals offer data to the user and enable the BS to sense the environment simultaneously.","We derive both user and tag communication rates and the sensing rate of the BS.","We jointly optimize the transmit/received beamformers and tag reflection coefficients to minimize the total BS power.","To solve this problem, we employ the alternating optimization technique.","We offer a closed-form solution for the received beamformers while utilizing semi-definite relaxation and slack-optimization for transmit beamformers and power reflection coefficients, respectively.","For example, with ten transmit/reception antennas at the BS, ISABC delivers a 75% sum communication and sensing rates gain over a traditional backscatter while requiring a 3.4% increase in transmit power.","Furthermore, ISABC with active tags only requires a 0.24% increase in transmit power over conventional integrated sensing and communication."],"url":"http://arxiv.org/abs/2404.19090v1","category":"cs.IT"}
{"created":"2024-04-29 19:46:59","title":"Billiard Partitions, Fibonacci Sequences, SIP Classes, and Quivers","abstract":"Starting from billiard partitions which arose recently in the description of periodic trajectories of ellipsoidal billiards in $d$-dimensional Euclidean space, we introduce a new type of separable integer partition classes, called type B. We study the numbers of basis partitions with $d$ parts and relate them to the Fibonacci sequence and its natural generalizations. Remarkably, the generating series of basis partitions can be related to the quiver generating series of symmetric quivers corresponding to the framed unknot via knots-quivers correspondence, and to the count of Schr\\\"oder paths.","sentences":["Starting from billiard partitions which arose recently in the description of periodic trajectories of ellipsoidal billiards in $d$-dimensional Euclidean space, we introduce a new type of separable integer partition classes, called type B.","We study the numbers of basis partitions with $d$ parts and relate them to the Fibonacci sequence and its natural generalizations.","Remarkably, the generating series of basis partitions can be related to the quiver generating series of symmetric quivers corresponding to the framed unknot via knots-quivers correspondence, and to the count of Schr\\\"oder paths."],"url":"http://arxiv.org/abs/2404.19078v1","category":"math.CO"}
{"created":"2024-04-29 19:28:57","title":"Influence of the downstream blade sweep on cross-flow turbine performance","abstract":"Cross-flow turbine blades encounter a relatively undisturbed inflow for the first half of each rotational cycle (\"upstream sweep\") and then pass through their own wake for the latter half (\"downstream sweep\"). While most research on cross-flow turbine optimization focuses on the power-generating upstream sweep, we use singled-bladed turbine experiments to show that the downstream sweep strongly affects time-averaged performance. We find that power generation from the upstream sweep continues to increase beyond the optimal tip-speed ratio. In contrast, the power consumption from the downstream sweep begins to increase approximately linearly beyond the optimal tip-speed ratio due in part to an increasingly unfavorable orientation of lift and drag relative to the rotation direction as well as high tangential blade velocities. Downstream power degradation increases faster than upstream power generation, indicating the downstream sweep strongly influences the optimal tip-speed ratio. In addition, particle image velocimetry data is obtained inside the turbine swept area at three tip-speed ratios. This illuminates the mechanisms underpinning the observed performance degradation in the downstream sweep and motivates an analytical model for a limited case with high induction. Performance results are shown to be consistent across 55 unique combinations of chord-to-radius ratio, preset pitch angle, and Reynolds number, underscoring the general relevance of the downstream sweep.","sentences":["Cross-flow turbine blades encounter a relatively undisturbed inflow for the first half of each rotational cycle (\"upstream sweep\") and then pass through their own wake for the latter half (\"downstream sweep\").","While most research on cross-flow turbine optimization focuses on the power-generating upstream sweep, we use singled-bladed turbine experiments to show that the downstream sweep strongly affects time-averaged performance.","We find that power generation from the upstream sweep continues to increase beyond the optimal tip-speed ratio.","In contrast, the power consumption from the downstream sweep begins to increase approximately linearly beyond the optimal tip-speed ratio due in part to an increasingly unfavorable orientation of lift and drag relative to the rotation direction as well as high tangential blade velocities.","Downstream power degradation increases faster than upstream power generation, indicating the downstream sweep strongly influences the optimal tip-speed ratio.","In addition, particle image velocimetry data is obtained inside the turbine swept area at three tip-speed ratios.","This illuminates the mechanisms underpinning the observed performance degradation in the downstream sweep and motivates an analytical model for a limited case with high induction.","Performance results are shown to be consistent across 55 unique combinations of chord-to-radius ratio, preset pitch angle, and Reynolds number, underscoring the general relevance of the downstream sweep."],"url":"http://arxiv.org/abs/2404.19072v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 18:11:53","title":"Better Optimization of Variational Quantum Eigensolvers by combining the Unitary Block Optimization Scheme with Classical Post-Processing","abstract":"Variational Quantum Eigensolvers (VQE) are a promising approach for finding the classically intractable ground state of a Hamiltonian. The Unitary Block Optimization Scheme (UBOS) is a state-of-the-art VQE method which works by sweeping over gates and finding optimal parameters for each gate in the environment of other gates. UBOS improves the convergence time to the ground state by an order of magnitude over Stochastic Gradient Descent (SGD). It nonetheless suffers in both rate of convergence and final converged energies in the face of highly noisy expectation values coming from shot noise. Here we develop two classical post-processing techniques which improve UBOS especially when measurements have large noise. Using Gaussian Process Regression (GPR) we generate artificial augmented data using original data from the quantum computer to reduce the overall error when solving for the improved parameters. Using Double Robust Optimization plus Rejection (DROPR), we prevent outlying data which are atypically noisy from resulting in a a particularly erroneous single optimization step thereby increasing robustness against noisy measurements. Combining these techniques further reduces the final relative error that UBOS reaches by a factor of three without adding additional quantum measurement or sampling overhead. This work further demonstrates that developing techniques which use classical resources to post-process quantum measurement results can significantly improve VQE algorithms.","sentences":["Variational Quantum Eigensolvers (VQE) are a promising approach for finding the classically intractable ground state of a Hamiltonian.","The Unitary Block Optimization Scheme (UBOS) is a state-of-the-art VQE method which works by sweeping over gates and finding optimal parameters for each gate in the environment of other gates.","UBOS improves the convergence time to the ground state by an order of magnitude over Stochastic Gradient Descent (SGD).","It nonetheless suffers in both rate of convergence and final converged energies in the face of highly noisy expectation values coming from shot noise.","Here we develop two classical post-processing techniques which improve UBOS especially when measurements have large noise.","Using Gaussian Process Regression (GPR) we generate artificial augmented data using original data from the quantum computer to reduce the overall error when solving for the improved parameters.","Using Double Robust Optimization plus Rejection (DROPR), we prevent outlying data which are atypically noisy from resulting in a a particularly erroneous single optimization step thereby increasing robustness against noisy measurements.","Combining these techniques further reduces the final relative error that UBOS reaches by a factor of three without adding additional quantum measurement or sampling overhead.","This work further demonstrates that developing techniques which use classical resources to post-process quantum measurement results can significantly improve VQE algorithms."],"url":"http://arxiv.org/abs/2404.19027v1","category":"quant-ph"}
{"created":"2024-04-29 18:04:23","title":"Optimal Parallel Algorithms for Dendrogram Computation and Single-Linkage Clustering","abstract":"Computing a Single-Linkage Dendrogram (SLD) is a key step in the classic single-linkage hierarchical clustering algorithm. Given an input edge-weighted tree $T$, the SLD of $T$ is a binary dendrogram that summarizes the $n-1$ clusterings obtained by contracting the edges of $T$ in order of weight. Existing algorithms for computing the SLD all require $\\Omega(n\\log n)$ work where $n = |T|$. Furthermore, to the best of our knowledge no prior work provides a parallel algorithm obtaining non-trivial speedup for this problem.   In this paper, we design faster parallel algorithms for computing SLDs both in theory and in practice based on new structural results about SLDs. In particular, we obtain a deterministic output-sensitive parallel algorithm based on parallel tree contraction that requires $O(n \\log h)$ work and $O(\\log^2 n \\log^2 h)$ depth, where $h$ is the height of the output SLD. We also give a deterministic bottom-up algorithm for the problem inspired by the nearest neighbor chain algorithm for hierarchical agglomerative clustering, and show that it achieves $O(n\\log h)$ work and $O(h \\log n)$ depth. Our results are based on a novel divide-and-conquer framework for building SLDs, inspired by divide-and-conquer algorithms for Cartesian trees. Our new algorithms can quickly compute the SLD on billion-scale trees, and obtain up to 150x speedup over the highly-efficient Union-Find algorithm typically used to compute SLDs in practice.","sentences":["Computing a Single-Linkage Dendrogram (SLD) is a key step in the classic single-linkage hierarchical clustering algorithm.","Given an input edge-weighted tree $T$, the SLD of $T$ is a binary dendrogram that summarizes the $n-1$ clusterings obtained by contracting the edges of $T$ in order of weight.","Existing algorithms for computing the SLD all require $\\Omega(n\\log n)$ work where $n = |T|$.","Furthermore, to the best of our knowledge no prior work provides a parallel algorithm obtaining non-trivial speedup for this problem.   ","In this paper, we design faster parallel algorithms for computing SLDs both in theory and in practice based on new structural results about SLDs.","In particular, we obtain a deterministic output-sensitive parallel algorithm based on parallel tree contraction that requires $O(n \\log h)$ work and $O(\\log^2 n \\log^2 h)$ depth, where $h$ is the height of the output SLD.","We also give a deterministic bottom-up algorithm for the problem inspired by the nearest neighbor chain algorithm for hierarchical agglomerative clustering, and show that it achieves $O(n\\log h)$ work and $O(h \\log n)$ depth.","Our results are based on a novel divide-and-conquer framework for building SLDs, inspired by divide-and-conquer algorithms for Cartesian trees.","Our new algorithms can quickly compute the SLD on billion-scale trees, and obtain up to 150x speedup over the highly-efficient Union-Find algorithm typically used to compute SLDs in practice."],"url":"http://arxiv.org/abs/2404.19019v1","category":"cs.DS"}
{"created":"2024-04-29 17:36:58","title":"Markovian Agents for Truthful Language Modeling","abstract":"Chain-of-Thought (CoT) reasoning could in principle enable a deeper understanding of a language model's (LM) internal reasoning. However, prior work suggests that some LMs answer questions similarly despite changes in their CoT, suggesting that those models are not truly using the CoT. We propose a training method to produce CoTs that are sufficient alone for predicting future text, independent of other context. This methodology gives a guarantee that if the LM can predict future tokens, then it must have used the CoT to understand its context. We formalize the idea that the truthfulness of a sender to a receiver LM is the degree to which the sender helps the receiver predict their future observations. Then we define a \"Markovian\" LM as one which predicts future text given only a CoT as context. We derive a \"Markovian training\" procedure by applying our definition of truthfulness to a Markovian LM and optimizing via policy gradient and Proximal Policy Optimization (PPO). We demonstrate the effectiveness of our training algorithm on long-context arithmetic problems, show that the model utilizes the CoT, and validate that the generated CoT is meaningful and usable by other models.","sentences":["Chain-of-Thought (CoT) reasoning could in principle enable a deeper understanding of a language model's (LM) internal reasoning.","However, prior work suggests that some LMs answer questions similarly despite changes in their CoT, suggesting that those models are not truly using the CoT.","We propose a training method to produce CoTs that are sufficient alone for predicting future text, independent of other context.","This methodology gives a guarantee that if the LM can predict future tokens, then it must have used the CoT to understand its context.","We formalize the idea that the truthfulness of a sender to a receiver LM is the degree to which the sender helps the receiver predict their future observations.","Then we define a \"Markovian\" LM as one which predicts future text given only a CoT as context.","We derive a \"Markovian training\" procedure by applying our definition of truthfulness to a Markovian LM and optimizing via policy gradient and Proximal Policy Optimization (PPO).","We demonstrate the effectiveness of our training algorithm on long-context arithmetic problems, show that the model utilizes the CoT, and validate that the generated CoT is meaningful and usable by other models."],"url":"http://arxiv.org/abs/2404.18988v1","category":"cs.CL"}
{"created":"2024-04-30 17:51:01","title":"The Width of an Electron-Capture Neutrino Wave Packet","abstract":"We expand on the methodology outlined in previous work that predicted the width of an antineutrino wave packet emerging from a beta-decaying nucleus, to the case of a neutrino from electron capture decay. Based on this result, we also respond to a recent Beryllium Electron capture in Superconducting Tunnel junctions Experiment (BeEST) paper which utilizes this previous work in forming their measurement of the neutrino wave packet width. According to our interpretation, the direct limit on the neutrino wave packet width from electron capture decay ($e^- + \\mathrm{^{7}Be}\\rightarrow\\mathrm{^{7}Li+\\nu_e}$) using the BeEST analysis should map to $\\sigma_{\\nu,x}>6.2\\,\\mathrm{pm}$ while our theoretical prediction is $\\sigma_{\\nu,x}\\sim2.7\\,\\mathrm{nm}$.","sentences":["We expand on the methodology outlined in previous work that predicted the width of an antineutrino wave packet emerging from a beta-decaying nucleus, to the case of a neutrino from electron capture decay.","Based on this result, we also respond to a recent Beryllium Electron capture in Superconducting Tunnel junctions Experiment (BeEST) paper which utilizes this previous work in forming their measurement of the neutrino wave packet width.","According to our interpretation, the direct limit on the neutrino wave packet width from electron capture decay ($e^- + \\mathrm{^{7}Be}\\rightarrow\\mathrm{^{7}Li+\\nu_e}$) using the BeEST analysis should map to $\\sigma_{\\nu,x}>6.2\\,\\mathrm{pm}$ while our theoretical prediction is $\\sigma_{\\nu,x}\\sim2.7\\,\\mathrm{nm}$."],"url":"http://arxiv.org/abs/2404.19746v1","category":"hep-ph"}
{"created":"2024-04-30 17:43:33","title":"Testing the accuracy of SED modeling techniques using the NIHAO-SKIRT-Catalog","abstract":"We use simulated galaxy observations from the NIHAO-SKIRT-Catalog to test the accuracy of Spectral Energy Distribution (SED) modeling techniques. SED modeling is an essential tool for inferring star-formation histories from nearby galaxy observations, but is fraught with difficulty due to our incomplete understanding of stellar populations, chemical enrichment processes, and the nonlinear, geometry-dependent effects of dust on our observations. The NIHAO-SKIRT-Catalog uses hydrodynamic simulations and radiative transfer to produce SEDs from the ultraviolet (UV) through the infrared (IR), accounting for the effects of dust. We use the commonly used Prospector software to perform inference on these SEDs, and compare the inferred stellar masses and star-formation rates (SFRs) to the known values in the simulation. We match the stellar population models to isolate the effects of differences in the star-formation history, the chemical evolution history, and the dust. We find that the combined effect of model mismatches for high mass ($> 10^{9.5} M_{\\odot}$) galaxies leads to inferred SFRs that are on average underestimated by a factor of 2 when fit to UV through IR photometry, and a factor of 3 when fit to UV through optical photometry. These biases lead to significant inaccuracies in the resulting sSFR-mass relations, with UV through optical fits showing particularly strong deviations from the true relation of the simulated galaxies. In the context of massive existing and upcoming photometric surveys, these results highlight that star-formation history inference from photometry remains imprecise and inaccurate, and that there is a pressing need for more realistic testing of existing techniques.","sentences":["We use simulated galaxy observations from the NIHAO-SKIRT-Catalog to test the accuracy of Spectral Energy Distribution (SED) modeling techniques.","SED modeling is an essential tool for inferring star-formation histories from nearby galaxy observations, but is fraught with difficulty due to our incomplete understanding of stellar populations, chemical enrichment processes, and the nonlinear, geometry-dependent effects of dust on our observations.","The NIHAO-SKIRT-Catalog uses hydrodynamic simulations and radiative transfer to produce SEDs from the ultraviolet (UV) through the infrared (IR), accounting for the effects of dust.","We use the commonly used Prospector software to perform inference on these SEDs, and compare the inferred stellar masses and star-formation rates (SFRs) to the known values in the simulation.","We match the stellar population models to isolate the effects of differences in the star-formation history, the chemical evolution history, and the dust.","We find that the combined effect of model mismatches for high mass ($> 10^{9.5} M_{\\odot}$) galaxies leads to inferred SFRs that are on average underestimated by a factor of 2 when fit to UV through IR photometry, and a factor of 3 when fit to UV through optical photometry.","These biases lead to significant inaccuracies in the resulting sSFR-mass relations, with UV through optical fits showing particularly strong deviations from the true relation of the simulated galaxies.","In the context of massive existing and upcoming photometric surveys, these results highlight that star-formation history inference from photometry remains imprecise and inaccurate, and that there is a pressing need for more realistic testing of existing techniques."],"url":"http://arxiv.org/abs/2404.19742v1","category":"astro-ph.GA"}
{"created":"2024-04-30 15:57:34","title":"Central Limit Theorem for tensor products of free variables","abstract":"We establish a central limit theorem for tensor product random variables $c_k:=a_k \\otimes a_k$, where $(a_k)_{k \\in \\mathbb{N}}$ is a free family of variables. We show that if the variables $a_k$ are centered, the limiting law is the semi-circle. Otherwise, the limiting law depends on the mean and variance of the variables $a_k$ and corresponds to a free interpolation between the semi-circle law and the classical convolution of two semi-circle laws.","sentences":["We establish a central limit theorem for tensor product random variables $c_k:=a_k \\otimes a_k$, where $(a_k)_{k \\in \\mathbb{N}}$ is a free family of variables.","We show that if the variables $a_k$ are centered, the limiting law is the semi-circle.","Otherwise, the limiting law depends on the mean and variance of the variables $a_k$ and corresponds to a free interpolation between the semi-circle law and the classical convolution of two semi-circle laws."],"url":"http://arxiv.org/abs/2404.19662v1","category":"math.PR"}
{"created":"2024-04-30 15:38:01","title":"Inverse Modeling of Bubble Size Dynamics for Interphase Mass Transfer and Gas Holdup in CO2 Bubble Column Reactors","abstract":"The use of microbial gas fermentation for transforming captured CO2 into sustainable fuels and chemicals has been identified as a promising decarbonization pathway. To accelerate the scale-up of gaseous CO2 fermentation reactors, computational models need to predict gas-to-liquid mass transfer which requires capturing the bubble size dynamics, i.e. bubble breakup and coalescence. In this work, an inverse modeling approach is used to calibrate the breakup and coalescence closure models, that are used in the Multiple-Size-Group (MUSIG) population balance modeling (PBM). The calibration aims at replicating experimental results obtained in a CO2-air-water-coflowing bubble column reactor. Bayesian inference is used to account for noise in the experimental dataset and bias in the simulation results. The estimated simulation bias also allows identifying the best-performing closure models irrespective of the model parameters used. The calibration results suggest that the breakage rate is underestimated by one order of magnitude in two different breakup modeling approaches.","sentences":["The use of microbial gas fermentation for transforming captured CO2 into sustainable fuels and chemicals has been identified as a promising decarbonization pathway.","To accelerate the scale-up of gaseous CO2 fermentation reactors, computational models need to predict gas-to-liquid mass transfer which requires capturing the bubble size dynamics, i.e. bubble breakup and coalescence.","In this work, an inverse modeling approach is used to calibrate the breakup and coalescence closure models, that are used in the Multiple-Size-Group (MUSIG) population balance modeling (PBM).","The calibration aims at replicating experimental results obtained in a CO2-air-water-coflowing bubble column reactor.","Bayesian inference is used to account for noise in the experimental dataset and bias in the simulation results.","The estimated simulation bias also allows identifying the best-performing closure models irrespective of the model parameters used.","The calibration results suggest that the breakage rate is underestimated by one order of magnitude in two different breakup modeling approaches."],"url":"http://arxiv.org/abs/2404.19636v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 15:23:36","title":"Recent Neutrino Parameters Impact on the Effective Majorana Neutrino Mass in 0$\u03bd\u03b2\u03b2$ Decay","abstract":"We investigate how recent updates to neutrino oscillation parameters and the sum of neutrino masses influence the sensitivity of neutrinoless double-beta (0$\\nu\\beta\\beta$) decay experiments. Incorporating the latest cosmological constraints on the sum of neutrino masses and laboratory measurements on oscillations, we determine the sum of neutrino masses for both the normal hierarchy (NH) and the inverted hierarchy (IH). Our analysis reveals a narrow range for the sum of neutrino masses, approximately 0.06 eV/c$^2$ for NH and 0.102 eV/c$^2$ for IH. Utilizing these constraints, we calculate the effective Majorana masses for both NH and IH scenarios, establishing the corresponding allowed regions. Importantly, we find that the minimum neutrino mass is non-zero, as constrained by the current oscillation parameters. Additionally, we estimate the half-life of 0$\\nu\\beta\\beta$ decay using these effective Majorana masses for both NH and IH. Our results suggest that upcoming ton-scale experiments will comprehensively explore the IH scenario, while 100-ton-scale experiments will effectively probe the parameter space for the NH scenario, provided the background index can achieve 1 event/kton-year in the region of interest.","sentences":["We investigate how recent updates to neutrino oscillation parameters and the sum of neutrino masses influence the sensitivity of neutrinoless double-beta (0$\\nu\\beta\\beta$) decay experiments.","Incorporating the latest cosmological constraints on the sum of neutrino masses and laboratory measurements on oscillations, we determine the sum of neutrino masses for both the normal hierarchy (NH) and the inverted hierarchy (IH).","Our analysis reveals a narrow range for the sum of neutrino masses, approximately 0.06 eV/c$^2$ for NH and 0.102 eV/c$^2$ for IH.","Utilizing these constraints, we calculate the effective Majorana masses for both NH and IH scenarios, establishing the corresponding allowed regions.","Importantly, we find that the minimum neutrino mass is non-zero, as constrained by the current oscillation parameters.","Additionally, we estimate the half-life of 0$\\nu\\beta\\beta$ decay using these effective Majorana masses for both NH and IH.","Our results suggest that upcoming ton-scale experiments will comprehensively explore the IH scenario, while 100-ton-scale experiments will effectively probe the parameter space for the NH scenario, provided the background index can achieve 1 event/kton-year in the region of interest."],"url":"http://arxiv.org/abs/2404.19624v1","category":"hep-ph"}
{"created":"2024-04-30 14:36:31","title":"Programmable activation of quantum emitters in high-purity silicon with focused carbon ion beams","abstract":"Carbon implantation at the nanoscale is highly desired for the engineering of defect-based qubits in a variety of materials, including silicon, diamond, SiC and hBN. However, the lack of focused carbon ion beams does not allow for the full disclosure of their potential for application in quantum technologies. Here, we develop and use a carbon source for focused ion beams for the simultaneous creation of two types of quantum emitters in silicon, the W and G centers. Furthermore, we apply a multi-step implantation protocol for the programmable activation of the G centers with sub-100- nm resolution. This approach provides a route for significant enhancement of the creation yield of single G centers in carbon-free silicon wafers. Our experimental demonstration is an important step towards nanoscale engineering of telecom quantum emitters in silicon of high crystalline quality and isotope purity.","sentences":["Carbon implantation at the nanoscale is highly desired for the engineering of defect-based qubits in a variety of materials, including silicon, diamond, SiC and hBN.","However, the lack of focused carbon ion beams does not allow for the full disclosure of their potential for application in quantum technologies.","Here, we develop and use a carbon source for focused ion beams for the simultaneous creation of two types of quantum emitters in silicon, the W and G centers.","Furthermore, we apply a multi-step implantation protocol for the programmable activation of the G centers with sub-100- nm resolution.","This approach provides a route for significant enhancement of the creation yield of single G centers in carbon-free silicon wafers.","Our experimental demonstration is an important step towards nanoscale engineering of telecom quantum emitters in silicon of high crystalline quality and isotope purity."],"url":"http://arxiv.org/abs/2404.19592v1","category":"quant-ph"}
{"created":"2024-04-30 14:25:38","title":"Single Atom Substituents in Copper Surfaces May Adsorb Multiple CO Molecules","abstract":"Copper is a good CO2 electroreduction catalyst as products beyond CO form, but efficiency and selectivity is low. Experiments have shown that admixture of other elements can help, and computational screening studies have pointed out various promising candidates based on the adsorption of a single CO molecule as a descriptor. Our calculations of CO adsorption on surfaces where a first row transition metal atom replaces a Cu atom show that multiple CO molecules, not just one, bind to the substitutional atom. For Fe, Co, and Ni atoms, a decrease in binding energy is found, but the reverse trend, namely increasing bond strength, is found for V, Cr, and Mn and the first three CO molecules. Magnetic moment, charge, and position of the substitutional atom are also strongly affected by the CO adsorption in most cases. Magnetic moment is stepwise reduced to zero, and the outward displacement of the substitutional atom increased.","sentences":["Copper is a good CO2 electroreduction catalyst as products beyond CO form, but efficiency and selectivity is low.","Experiments have shown that admixture of other elements can help, and computational screening studies have pointed out various promising candidates based on the adsorption of a single CO molecule as a descriptor.","Our calculations of CO adsorption on surfaces where a first row transition metal atom replaces a Cu atom show that multiple CO molecules, not just one, bind to the substitutional atom.","For Fe, Co, and Ni atoms, a decrease in binding energy is found, but the reverse trend, namely increasing bond strength, is found for V, Cr, and Mn and the first three CO molecules.","Magnetic moment, charge, and position of the substitutional atom are also strongly affected by the CO adsorption in most cases.","Magnetic moment is stepwise reduced to zero, and the outward displacement of the substitutional atom increased."],"url":"http://arxiv.org/abs/2404.19587v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 14:16:48","title":"Enduring two-dimensional perturbations with significant non-modal growth","abstract":"Laminar shear flows can display large non-modal perturbation growth, often through the lift-up mechansm, and can undergo subcritical transition to turbulence. The process is three-dimensional. Two-dimensional (2D) spanwise-independent perturbations are often considered less important as they typically undergo modest levels of transient growth and are short-lived. Strikingly, we show the existence of 2D non-modal perturbations that get amplified significantly and survive for long periods of time. Two-layer and three-layer viscosity stratified plane shear flows are taken to be the mean states. We show that while the two-layer flow is always modally stable, the three-layer flow supports exponential growing instabilities only when the middle layer is the least viscous. The non-modal stability analysis is performed only for the modally stable configurations of these flows. At later times, the non-modal perturbations feature strongly confined vortical structures near the interface in the two-layer flow. For the three-layer flow, similar observations are noted when all the three layers have different shear rates with the vortices prominently seen in the vicinity of the interface between the least viscous and middle layers. For the three-layer flow configuration with the outer layers having equal shear rates, the perturbation structure shows symmetry about the middle layer and evolves such that the Orr mechanism can repeatedly occur in a regenerative manner resulting in the perturbation energy evolving in a markedly non-monotonic fashion. When these same perturbations are introduced in a uniform plane shear flow, the extent of non-modal transient growth is shown to be significantly smaller.","sentences":["Laminar shear flows can display large non-modal perturbation growth, often through the lift-up mechansm, and can undergo subcritical transition to turbulence.","The process is three-dimensional.","Two-dimensional (2D) spanwise-independent perturbations are often considered less important as they typically undergo modest levels of transient growth and are short-lived.","Strikingly, we show the existence of 2D non-modal perturbations that get amplified significantly and survive for long periods of time.","Two-layer and three-layer viscosity stratified plane shear flows are taken to be the mean states.","We show that while the two-layer flow is always modally stable, the three-layer flow supports exponential growing instabilities only when the middle layer is the least viscous.","The non-modal stability analysis is performed only for the modally stable configurations of these flows.","At later times, the non-modal perturbations feature strongly confined vortical structures near the interface in the two-layer flow.","For the three-layer flow, similar observations are noted when all the three layers have different shear rates with the vortices prominently seen in the vicinity of the interface between the least viscous and middle layers.","For the three-layer flow configuration with the outer layers having equal shear rates, the perturbation structure shows symmetry about the middle layer and evolves such that the Orr mechanism can repeatedly occur in a regenerative manner resulting in the perturbation energy evolving in a markedly non-monotonic fashion.","When these same perturbations are introduced in a uniform plane shear flow, the extent of non-modal transient growth is shown to be significantly smaller."],"url":"http://arxiv.org/abs/2404.19580v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 13:52:57","title":"Towards a zero magnetic field environment for ultracold atoms experiments","abstract":"The minimization of the magnetic field plays a crucial role in ultracold gas research. For instance, the contact interaction dominates all the other energy scales in the zero magnetic field limit, giving rise to novel quantum phases of matter. However, lowering magnetic fields well below the mG level is often challenging in ultracold gas experiments. In this article, we apply Landau-Zener spectroscopy to characterize and reduce the magnetic field on an ultracold gas of sodium atoms to a few tens of {\\mu}G. The lowest magnetic field achieved here opens to observing novel phases of matter with ultracold spinor Bose gases.","sentences":["The minimization of the magnetic field plays a crucial role in ultracold gas research.","For instance, the contact interaction dominates all the other energy scales in the zero magnetic field limit, giving rise to novel quantum phases of matter.","However, lowering magnetic fields well below the mG level is often challenging in ultracold gas experiments.","In this article, we apply Landau-Zener spectroscopy to characterize and reduce the magnetic field on an ultracold gas of sodium atoms to a few tens of {\\mu}G. The lowest magnetic field achieved here opens to observing novel phases of matter with ultracold spinor Bose gases."],"url":"http://arxiv.org/abs/2404.19565v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 13:20:02","title":"Thermal conductivity reduction due to phonon geometrical scattering in nano-engineered epitaxial germanium","abstract":"Nano-engineering crystalline materials can be used to tailor their thermal properties. By adding new nanoscale phonon scattering centers and controlling their size, one can effectively decrease the phonon mean free path and hence the thermal conductivity of a fully crystalline material. In this letter, we use the 3$\\omega$ method in the temperature range of 100-300 K to experimentally report on the more than threefold reduction of the thermal conductivity of an epitaxially-grown crystalline germanium thin film with embedded polydispersed crystalline \\ch{Ge3Mn5} nano-inclusions with diameters ranging from 5 to 25~nm. A detailed analysis of the structure of the thin film coupled with Monte Carlo simulations of phonon transport highlight the role of the nano-inclusions volume fraction in the reduction of the phononic contribution to the thermal conductivity, in particular its temperature dependence, leading to a phonon mean free path that is set by geometrical constraints.","sentences":["Nano-engineering crystalline materials can be used to tailor their thermal properties.","By adding new nanoscale phonon scattering centers and controlling their size, one can effectively decrease the phonon mean free path and hence the thermal conductivity of a fully crystalline material.","In this letter, we use the 3$\\omega$ method in the temperature range of 100-300 K to experimentally report on the more than threefold reduction of the thermal conductivity of an epitaxially-grown crystalline germanium thin film with embedded polydispersed crystalline \\ch{Ge3Mn5} nano-inclusions with diameters ranging from 5 to 25~nm.","A detailed analysis of the structure of the thin film coupled with Monte Carlo simulations of phonon transport highlight the role of the nano-inclusions volume fraction in the reduction of the phononic contribution to the thermal conductivity, in particular its temperature dependence, leading to a phonon mean free path that is set by geometrical constraints."],"url":"http://arxiv.org/abs/2404.19550v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 13:14:58","title":"Extinction and AGN over host galaxy contrast effects on the optical spectroscopic classification of AGN","abstract":"The optical spectroscopic classification of active galactic nuclei (AGN) into type 1 and type 2 can be understood in the frame of the AGN unification models. However, it remains unclear which physical properties are driving the classification into intermediate sub-types (1.0,1.2,1.5,1.8,1.9). To shed light on this issue, we present an analysis of the effect of extinction and AGN and host galaxy luminosities on sub-type determination for a sample of 159 X-ray selected AGN with a complete and robust optical spectroscopic classification. The sample spans a rest-frame 2 - 10 keV X-ray luminosity range of $10^{42}-10^{46}$ erg s$^{-1}$ and redshifts between 0.05 and 0.75. From the fitting of their UV-to-mid-infrared spectral energy distributions, we extracted the observed AGN over total AGN+galaxy contrast, optical/UV line-of-sight extinction as well as host galaxy and AGN luminosities. The observed contrast exhibits a clear decline with sub-type, distinguishing two main groups: 1.0-5 and 1.8-9/2. This difference is partly driven by an increase in extinction following the same trend. Nevertheless, 50% of 1.9s and 2s lack sufficient extinction to explain the lack of detection of broad emission lines, unveiling the necessity of an additional effect. Our findings show that 1.8-9/2s preferentially live in host galaxies with higher luminosities while displaying similar intrinsic AGN luminosities to 1.0-5s. Consequently, the AGN to host galaxy luminosity ratio diminishes, hindering the detection of the emission of the broad emission lines, resulting in the 1.8-9/2 classification of those with insufficient extinction. Thus, the combination of increasing extinction and decreasing AGN/galaxy luminosity ratio, mainly driven by an increasing host galaxy luminosity, constitutes the main reasons behind the sub-type classification into 1.0-5 and 1.8-9/2.","sentences":["The optical spectroscopic classification of active galactic nuclei (AGN) into type 1 and type 2 can be understood in the frame of the AGN unification models.","However, it remains unclear which physical properties are driving the classification into intermediate sub-types (1.0,1.2,1.5,1.8,1.9).","To shed light on this issue, we present an analysis of the effect of extinction and AGN and host galaxy luminosities on sub-type determination for a sample of 159 X-ray selected AGN with a complete and robust optical spectroscopic classification.","The sample spans a rest-frame 2 - 10 keV X-ray luminosity range of $10^{42}-10^{46}$ erg s$^{-1}$ and redshifts between 0.05 and 0.75.","From the fitting of their UV-to-mid-infrared spectral energy distributions, we extracted the observed AGN over total AGN+galaxy contrast, optical/UV line-of-sight extinction as well as host galaxy and AGN luminosities.","The observed contrast exhibits a clear decline with sub-type, distinguishing two main groups: 1.0-5 and 1.8-9/2.","This difference is partly driven by an increase in extinction following the same trend.","Nevertheless, 50% of 1.9s and 2s lack sufficient extinction to explain the lack of detection of broad emission lines, unveiling the necessity of an additional effect.","Our findings show that 1.8-9/2s preferentially live in host galaxies with higher luminosities while displaying similar intrinsic AGN luminosities to 1.0-5s.","Consequently, the AGN to host galaxy luminosity ratio diminishes, hindering the detection of the emission of the broad emission lines, resulting in the 1.8-9/2 classification of those with insufficient extinction.","Thus, the combination of increasing extinction and decreasing AGN/galaxy luminosity ratio, mainly driven by an increasing host galaxy luminosity, constitutes the main reasons behind the sub-type classification into 1.0-5 and 1.8-9/2."],"url":"http://arxiv.org/abs/2404.19544v1","category":"astro-ph.GA"}
{"created":"2024-04-30 13:07:06","title":"Status Report on Global Pulsar-Timing-Array Efforts to Detect Gravitational Waves","abstract":"The stability of the spin of pulsars and the precision with which these spins can be determined, allows many unique tests of interest to physics and astrophysics. Perhaps the most challenging and revolutionary of these, is the detection of nanohertz gravitational waves. An increasing number of efforts to detect and study long-period gravitational waves by timing an array of pulsars have been ongoing for several decades and the field is moving ever closer to actual gravitational-wave science. In this review article, we summarise the state of this field by presenting the current sensitivity to gravitational waves and by reviewing recent progress along the multiple lines of research that are part of the continuous push towards greater sensitivity. We also briefly review some of the most recent efforts at astrophysical interpretation of the most recent GW estimates derived from pulsar timing.","sentences":["The stability of the spin of pulsars and the precision with which these spins can be determined, allows many unique tests of interest to physics and astrophysics.","Perhaps the most challenging and revolutionary of these, is the detection of nanohertz gravitational waves.","An increasing number of efforts to detect and study long-period gravitational waves by timing an array of pulsars have been ongoing for several decades and the field is moving ever closer to actual gravitational-wave science.","In this review article, we summarise the state of this field by presenting the current sensitivity to gravitational waves and by reviewing recent progress along the multiple lines of research that are part of the continuous push towards greater sensitivity.","We also briefly review some of the most recent efforts at astrophysical interpretation of the most recent GW estimates derived from pulsar timing."],"url":"http://arxiv.org/abs/2404.19529v1","category":"astro-ph.HE"}
{"created":"2024-04-30 11:38:05","title":"The recent star formation histories of nearby galaxies on resolved scales","abstract":"Star formation histories (SFHs) of galaxies are affected by a variety of factors, both external (field vs. cluster/group) and internal (presence of a bar and AGN, morphological type). In this work, we extend our previous study and apply the <SFR5>/<SFR200> metric to a sample of eleven nearby galaxies with MUSE observations. Based on a combination of H$\\alpha$ and UV photometry, <SFR5>/<SFR200> is sensitive to star formation timescales of ~5-200 Myr and therefore measures the present-day rate of change in the star formation rate, dSFR/dt. Within this limited galaxy sample, we do not observe systematic variations between the global value of <SFR5>/<SFR200> and the presence of an active galactic nucleus, stellar bar, nor with group or cluster membership. Within some of the individual galaxies, we however observe significant differences in <SFR5>/<SFR200> between the arm and interarm regions. In half of the galaxies, the recent SFH of both arm and interarm regions has been very similar. However, in the galaxies with higher bulge-to-total light ratios and earlier morphological type, the SFR is declining more rapidly in the interarm regions. This decline in SFR is not a result of low molecular gas surface density or a decrease in the star formation efficiency, implying that other factors are responsible for this SFR decrease.","sentences":["Star formation histories (SFHs) of galaxies are affected by a variety of factors, both external (field vs. cluster/group) and internal (presence of a bar and AGN, morphological type).","In this work, we extend our previous study and apply the <SFR5>/<SFR200> metric to a sample of eleven nearby galaxies with MUSE observations.","Based on a combination of H$\\alpha$ and UV photometry, <SFR5>/<SFR200> is sensitive to star formation timescales of ~5-200 Myr and therefore measures the present-day rate of change in the star formation rate, dSFR/dt.","Within this limited galaxy sample, we do not observe systematic variations between the global value of <SFR5>/<SFR200> and the presence of an active galactic nucleus, stellar bar, nor with group or cluster membership.","Within some of the individual galaxies, we however observe significant differences in <SFR5>/<SFR200> between the arm and interarm regions.","In half of the galaxies, the recent SFH of both arm and interarm regions has been very similar.","However, in the galaxies with higher bulge-to-total light ratios and earlier morphological type, the SFR is declining more rapidly in the interarm regions.","This decline in SFR is not a result of low molecular gas surface density or a decrease in the star formation efficiency, implying that other factors are responsible for this SFR decrease."],"url":"http://arxiv.org/abs/2404.19473v1","category":"astro-ph.GA"}
{"created":"2024-04-30 11:37:20","title":"AGN constraints on neutrino-dark matter scattering","abstract":"The IceCube collaboration has identified neutrinos of energy $\\sim 10-100$ TeV from the blazar TXS 0506+056 and the active galaxy NGC 1068, which must have traveled through a dense dark matter spike surrounding the supermassive black holes that power the galactic nuclei. We use this to set new constraints on dark matter-neutrino scattering, and interpret the results in terms of a dark photon that couples to baryon minus lepton number.","sentences":["The IceCube collaboration has identified neutrinos of energy $\\sim 10-100$ TeV from the blazar TXS 0506+056 and the active galaxy NGC 1068, which must have traveled through a dense dark matter spike surrounding the supermassive black holes that power the galactic nuclei.","We use this to set new constraints on dark matter-neutrino scattering, and interpret the results in terms of a dark photon that couples to baryon minus lepton number."],"url":"http://arxiv.org/abs/2404.19471v1","category":"astro-ph.HE"}
{"created":"2024-04-30 10:08:12","title":"Energy Cyber Attacks to Smart Healthcare Devices: A Testbed","abstract":"The Internet of Things (IoT) has garnered significant interest in both research and industry due to its profound impact on human life. The rapid expansion of IoT technology has ushered in smart healthcare, smart devices, smart cities, and smart grids. However, the security of IoT devices, particularly in healthcare, has become a major concern, with recent attacks revealing serious vulnerabilities. In IoT networks, where connected devices are susceptible to resource-constraint attacks, such as energy consumption attacks, security is paramount.   This paper explores the impact of Distributed Denial of Service (DDoS) and Fake Access Points (F-APs) attacks on WiFi-enabled smart healthcare devices. Specifically, it investigates how these attacks can disrupt service on victim devices and Access Points (APs), focusing on device connectivity and energy consumption during attacks. Key findings include identifying the attack rates of DDoS attacks that disrupt services and quantifying the energy consumption impact of Energy Consumption Distributed Denial of Service (EC-DDoS) and F-APs attacks on smart healthcare devices.   The study highlights communication protocols, attack rates, payload sizes, and port states of victim devices as critical factors influencing energy consumption. These insights provide a comprehensive understanding of IoT device vulnerabilities in smart healthcare environments and lay the groundwork for future defense strategies.","sentences":["The Internet of Things (IoT) has garnered significant interest in both research and industry due to its profound impact on human life.","The rapid expansion of IoT technology has ushered in smart healthcare, smart devices, smart cities, and smart grids.","However, the security of IoT devices, particularly in healthcare, has become a major concern, with recent attacks revealing serious vulnerabilities.","In IoT networks, where connected devices are susceptible to resource-constraint attacks, such as energy consumption attacks, security is paramount.   ","This paper explores the impact of Distributed Denial of Service (DDoS) and Fake Access Points (F-APs) attacks on WiFi-enabled smart healthcare devices.","Specifically, it investigates how these attacks can disrupt service on victim devices and Access Points (APs), focusing on device connectivity and energy consumption during attacks.","Key findings include identifying the attack rates of DDoS attacks that disrupt services and quantifying the energy consumption impact of Energy Consumption Distributed Denial of Service (EC-DDoS) and F-APs attacks on smart healthcare devices.   ","The study highlights communication protocols, attack rates, payload sizes, and port states of victim devices as critical factors influencing energy consumption.","These insights provide a comprehensive understanding of IoT device vulnerabilities in smart healthcare environments and lay the groundwork for future defense strategies."],"url":"http://arxiv.org/abs/2404.19418v1","category":"cs.CR"}
{"created":"2024-04-30 10:00:03","title":"Trace conservation laws in $T^2/Z_m$ orbifold gauge theories","abstract":"Gauge theory compactified on an orbifold is defined by gauge symmetry, matter contents, and boundary conditions. There are equivalence classes (ECs), each of which consists of physically equivalent boundary conditions. We propose the powerful necessary conditions, trace conservation laws (TCLs), which achieve a sufficient classification of ECs in U(N) and SU(N) gauge theories on $T^2/Z_m$ orbifolds $(m=2,3,4,6)$. The TCLs yield the equivalent relations between the diagonal boundary conditions without relying on an explicit form of gauge transformations. The TCLs also show the existence of off-diagonal ECs, which consist only of off-diagonal matrices, on $T^2/Z_4$ and $T^2/Z_6$. After the sufficient classification, the exact numbers of ECs are obtained.","sentences":["Gauge theory compactified on an orbifold is defined by gauge symmetry, matter contents, and boundary conditions.","There are equivalence classes (ECs), each of which consists of physically equivalent boundary conditions.","We propose the powerful necessary conditions, trace conservation laws (TCLs), which achieve a sufficient classification of ECs in U(N) and SU(N) gauge theories on $T^2/Z_m$ orbifolds $(m=2,3,4,6)$. The TCLs yield the equivalent relations between the diagonal boundary conditions without relying on an explicit form of gauge transformations.","The TCLs also show the existence of off-diagonal ECs, which consist only of off-diagonal matrices, on $T^2/Z_4$ and $T^2/Z_6$. After the sufficient classification, the exact numbers of ECs are obtained."],"url":"http://arxiv.org/abs/2404.19411v1","category":"hep-th"}
{"created":"2024-04-30 09:49:53","title":"Photoproduction of the \u03a3+ hyperon using linearly polarized photons with CLAS","abstract":"Background: Measurements of the polarization observables {\\Sigma}, P, T, Ox, Oz for the reaction {\\gamma}p {\\rightarrow) KS0 {\\Sigma}+ using a linearly polarized photon beam of energy 1.1 to 2.1 GeV are reported. Purpose: The measured data provide information on a channel that has not been studied extensively, but is required for a full coupled-channel analysis in the nucleon resonance region. Method: Observables have been simultaneously extracted using likelihood sampling with a Markov-Chain Monte- Carlo process. Results: Angular distributions in bins of photon energy E{\\gamma} are produced for each polarization observable. T, Ox and Oz are first time measurements of these observables in this reaction. The extraction of {\\Sigma} extends the energy range beyond a previous measurement. The measurement of P, the recoil polarization, is consistent with previous measurements. Conclusions: The measured data are shown to be significant enough to affect the estimation of the nucleon resonance parameters when fitted within a coupled-channels model.","sentences":["Background: Measurements of the polarization observables {\\Sigma}, P, T, Ox, Oz for the reaction {\\gamma}p {\\rightarrow) KS0 {\\Sigma}+ using a linearly polarized photon beam of energy 1.1 to 2.1 GeV are reported.","Purpose:","The measured data provide information on a channel that has not been studied extensively, but is required for a full coupled-channel analysis in the nucleon resonance region.","Method: Observables have been simultaneously extracted using likelihood sampling with a Markov-Chain Monte- Carlo process.","Results:","Angular distributions in bins of photon energy E{\\gamma} are produced for each polarization observable.","T, Ox and Oz are first time measurements of these observables in this reaction.","The extraction of {\\Sigma} extends the energy range beyond a previous measurement.","The measurement of P, the recoil polarization, is consistent with previous measurements.","Conclusions: The measured data are shown to be significant enough to affect the estimation of the nucleon resonance parameters when fitted within a coupled-channels model."],"url":"http://arxiv.org/abs/2404.19404v1","category":"nucl-ex"}
{"created":"2024-04-30 09:20:47","title":"Cerium Oxide-based Solid-State Thermal Transistors with Wide Switching Width of 9.5 W/mK","abstract":"Thermal transistors that electrically switch heat flow on and off have attracted attention as thermal management devices. Electrochemical reduction/oxidation switches the thermal conductivity (\\k{appa}) of active metal oxide layers. The \\k{appa}-switching width (difference between on-state and off-state \\k{appa}) of the previously proposed electrochemical thermal transistors is narrow, less than 5 W/mK. Here, we show solid-state electrochemical thermal transistors with a wide \\k{appa}-switching width of 9.5 W/mK. We used CeO2 thin film as the active layer directly deposited on a solid electrolyte YSZ substrate. A Pt thin film was deposited on the surface of the CeO2 thin film and the back surface of the YSZ substrate to create a solid-state electrochemical thermal transistor. When the CeO2 thin film was once reduced (off-state) and then oxidized (on-state), the \\k{appa} was approximately 2.5 W/mK in its most reduced state, and \\k{appa} increased with oxidation to 11.8 W/mK (on-state). This reduction (off-state)/oxidation (on-state) cycle was repeated five times and the average value of \\k{appa} was 2.5 W/mK after reduction (off-state) and 12 W/mK after oxidation (on-state). The \\k{appa}-switching width was 9.5 W/mK. The CeO2-based solid-state electrochemical thermal transistors are potential materials for thermal shutters and thermal displays.","sentences":["Thermal transistors that electrically switch heat flow on and off have attracted attention as thermal management devices.","Electrochemical reduction/oxidation switches the thermal conductivity (\\k{appa}) of active metal oxide layers.","The \\k{appa}-switching width (difference between on-state and off-state \\k{appa}) of the previously proposed electrochemical thermal transistors is narrow, less than 5 W/mK. Here, we show solid-state electrochemical thermal transistors with a wide \\k{appa}-switching width of 9.5 W/mK. We used CeO2 thin film as the active layer directly deposited on a solid electrolyte YSZ substrate.","A Pt thin film was deposited on the surface of the CeO2 thin film and the back surface of the YSZ substrate to create a solid-state electrochemical thermal transistor.","When the CeO2 thin film was once reduced (off-state) and then oxidized (on-state), the \\k{appa} was approximately 2.5 W/mK in its most reduced state, and \\k{appa} increased with oxidation to 11.8 W/mK (on-state).","This reduction (off-state)/oxidation (on-state) cycle was repeated five times and the average value of \\k{appa} was 2.5 W/mK after reduction (off-state) and 12 W/mK after oxidation (on-state).","The \\k{appa}-switching width was 9.5 W/mK. The CeO2-based solid-state electrochemical thermal transistors are potential materials for thermal shutters and thermal displays."],"url":"http://arxiv.org/abs/2404.19385v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 08:29:09","title":"Aluminum nuclear demagnetization refrigerator for powerful continuous cooling","abstract":"Many laboratories routinely cool samples to 10 mK, but relatively few can cool condensed matter below 1 mK. Easy access to the microkelvin range would propel fields such as quantum sensors and quantum materials. Such temperatures are achieved with adiabatic nuclear demagnetization. Existing nuclear demagnetization refrigerators (NDR) are \"single-shot\", and the recycling time is incompatible with proposed sub-mK experiments. Furthermore, a high cooling power is required to overcome the excess heat load of order nW on NDR pre-cooled by cryogen-free dilution refrigerators. We report the performance of an aluminum NDR designed for powerful cooling when part of a dual stage continuous NDR (CNDR). Its thermal resistance is minimized to maximize the cycling rate of the CNDR and consequently its cooling power. At the same time, its susceptibility to eddy current heating is minimized. A CNDR based on two of the aluminum NDR presented here would have a cooling power of approximately 40 nW at 560 $\\mu$K.","sentences":["Many laboratories routinely cool samples to 10 mK, but relatively few can cool condensed matter below 1 mK. Easy access to the microkelvin range would propel fields such as quantum sensors and quantum materials.","Such temperatures are achieved with adiabatic nuclear demagnetization.","Existing nuclear demagnetization refrigerators (NDR) are \"single-shot\", and the recycling time is incompatible with proposed sub-mK experiments.","Furthermore, a high cooling power is required to overcome the excess heat load of order nW on NDR pre-cooled by cryogen-free dilution refrigerators.","We report the performance of an aluminum NDR designed for powerful cooling when part of a dual stage continuous NDR (CNDR).","Its thermal resistance is minimized to maximize the cycling rate of the CNDR and consequently its cooling power.","At the same time, its susceptibility to eddy current heating is minimized.","A CNDR based on two of the aluminum NDR presented here would have a cooling power of approximately 40 nW at 560 $\\mu$K."],"url":"http://arxiv.org/abs/2404.19352v1","category":"physics.ins-det"}
{"created":"2024-04-30 08:06:19","title":"Small Instanton Effects on Composite Axion Mass","abstract":"This paper investigates the impact of small instanton effects on the axion mass in the composite accidental axion (CAA) models. These models are designed to address the axion quality problem, where QCD gauge symmetry is embedded as an unbroken diagonal subgroup of a product gauge group. These models contain small instantons not included in low-energy QCD, which could enhance the axion mass significantly. However, in the CAA models, our analysis reveals that these effects on the axion mass are non-vanishing but are negligible compared to the QCD effects. This highlights the important role of anomalous but non-spontaneously broken U(1) symmetries in restricting the impact of small instantons on the axion mass. Our study provides crucial insights into the dynamics within CAA models and suggests broader implications for understanding small instanton effects in other composite axion models.","sentences":["This paper investigates the impact of small instanton effects on the axion mass in the composite accidental axion (CAA) models.","These models are designed to address the axion quality problem, where QCD gauge symmetry is embedded as an unbroken diagonal subgroup of a product gauge group.","These models contain small instantons not included in low-energy QCD, which could enhance the axion mass significantly.","However, in the CAA models, our analysis reveals that these effects on the axion mass are non-vanishing but are negligible compared to the QCD effects.","This highlights the important role of anomalous but non-spontaneously broken U(1) symmetries in restricting the impact of small instantons on the axion mass.","Our study provides crucial insights into the dynamics within CAA models and suggests broader implications for understanding small instanton effects in other composite axion models."],"url":"http://arxiv.org/abs/2404.19342v1","category":"hep-ph"}
{"created":"2024-04-30 07:46:21","title":"Energy dependence of Quasi-periodic oscillations in accreting X-ray pulsars","abstract":"We present the results from an investigation of the energy dependence of Quasi-Periodic Oscillations (QPOs) exhibited by accreting X-ray pulsars using data from archival \\textit{XMM-Newton}, \\textit{NuSTAR}, \\textit{RXTE}, and \\textit{NICER} observations. In a search for the presence of QPOs in 99 \\textit{XMM-Newton} and \\textit{NuSTAR} observations, we detected QPOs in eleven observations of five sources, viz., 4U 1626--67 (48 mHz), IGR J19294+1816 (30 mHz), V 0332+53 (2, 18 and 40 mHz), Cen X--3 (30 mHz), and XTE J1858+034 (180 mHz). A positive correlation of the QPO rms amplitude with energy is exhibited by 4U 1626--67, IGR J19294+1816, Cen X--3 and XTE J1858+034, while no energy dependence is observed in V 0332+53. We also analysed the energy spectrum to decouple thermal (soft-excess) from non-thermal emission and determine if the soft-excess has different QPO properties. We found no evidence for different QPO characteristics of the soft excess. The \\textit{NuSTAR} observations of V 0332+53 during the Type-I outburst in 2016 show the presence of twin QPOs at 2.5 mHz and 18 mHz, while the \\textit{XMM-Newton} and \\textit{NuSTAR} observations during the Type-II outburst in 2015 show a QPO at 40 mHz. We review the observed QPO properties in the context of QPOs found in other types of accreting sources and the models usually used to explain the QPOs in accreting X-ray pulsars.","sentences":["We present the results from an investigation of the energy dependence of Quasi-Periodic Oscillations (QPOs) exhibited by accreting X-ray pulsars using data from archival \\textit{XMM-Newton}, \\textit{NuSTAR}, \\textit{RXTE}, and \\textit{NICER} observations.","In a search for the presence of QPOs in 99 \\textit{XMM-Newton} and \\textit{NuSTAR} observations, we detected QPOs in eleven observations of five sources, viz., 4U 1626--67 (48 mHz), IGR J19294+1816 (30 mHz), V 0332+53 (2, 18 and 40 mHz), Cen X--3 (30 mHz), and XTE J1858+034 (180 mHz).","A positive correlation of the QPO rms amplitude with energy is exhibited by 4U 1626--67, IGR J19294+1816, Cen X--3 and XTE J1858+034, while no energy dependence is observed in V 0332+53.","We also analysed the energy spectrum to decouple thermal (soft-excess) from non-thermal emission and determine if the soft-excess has different QPO properties.","We found no evidence for different QPO characteristics of the soft excess.","The \\textit{NuSTAR} observations of V 0332+53 during the Type-I outburst in 2016 show the presence of twin QPOs at 2.5 mHz and 18 mHz, while the \\textit{XMM-Newton} and \\textit{NuSTAR} observations during the Type-II outburst in 2015 show a QPO at 40 mHz.","We review the observed QPO properties in the context of QPOs found in other types of accreting sources and the models usually used to explain the QPOs in accreting X-ray pulsars."],"url":"http://arxiv.org/abs/2404.19323v1","category":"astro-ph.HE"}
{"created":"2024-04-30 07:33:02","title":"Alternative paths computation for congestion mitigation in segment-routing networks","abstract":"In backbone networks, it is fundamental to quickly protect traffic against any unexpected event, such as failures or congestions, which may impact Quality of Service (QoS). Standard solutions based on Segment Routing (SR), such as Topology-Independent Loop-Free Alternate (TI-LFA), are used in practice to handle failures, but no distributed solutions exist for distributed and tactical congestion mitigation. A promising approach leveraging SR has been recently proposed to quickly steer traffic away from congested links over alternative paths. As the pre-computation of alternative paths plays a paramount role to efficiently mitigating congestions, we investigate the associated path computation problem aiming at maximizing the amount of traffic that can be rerouted as well as the resilience against any 1-link failure. In particular, we focus on two variants of this problem. First, we maximize the residual flow after all possible failures. We show that the problem is NP-Hard, and we solve it via a Benders decomposition algorithm. Then, to provide a practical and scalable solution, we solve a relaxed variant problem, that maximizes, instead of flow, the number of surviving alternative paths after all possible failures. We provide a polynomial algorithm. Through numerical experiments, we compare the two variants and show that they allow to increase the amount of rerouted traffic and the resiliency of the network after any 1-link failure.","sentences":["In backbone networks, it is fundamental to quickly protect traffic against any unexpected event, such as failures or congestions, which may impact Quality of Service (QoS).","Standard solutions based on Segment Routing (SR), such as Topology-Independent Loop-Free Alternate (TI-LFA), are used in practice to handle failures, but no distributed solutions exist for distributed and tactical congestion mitigation.","A promising approach leveraging SR has been recently proposed to quickly steer traffic away from congested links over alternative paths.","As the pre-computation of alternative paths plays a paramount role to efficiently mitigating congestions, we investigate the associated path computation problem aiming at maximizing the amount of traffic that can be rerouted as well as the resilience against any 1-link failure.","In particular, we focus on two variants of this problem.","First, we maximize the residual flow after all possible failures.","We show that the problem is NP-Hard, and we solve it via a Benders decomposition algorithm.","Then, to provide a practical and scalable solution, we solve a relaxed variant problem, that maximizes, instead of flow, the number of surviving alternative paths after all possible failures.","We provide a polynomial algorithm.","Through numerical experiments, we compare the two variants and show that they allow to increase the amount of rerouted traffic and the resiliency of the network after any 1-link failure."],"url":"http://arxiv.org/abs/2404.19314v1","category":"cs.DM"}
{"created":"2024-04-30 07:32:27","title":"High-precision chemical quantum sensing in flowing monodisperse microdroplets","abstract":"We report on a novel flow-based method for high-precision chemical detection that integrates quantum sensing with droplet microfluidics. We deploy nanodiamond particles hosting fluorescent nitrogen vacancy defects as quantum sensors in flowing, monodisperse, picoliter-volume microdroplets containing analyte molecules. ND motion within these microcompartments facilitates close sensor-analyte interaction and mitigates particle heterogeneity. Microdroplet flow rates are rapid (upto 4cm/s) and with minimal drift. Pairing this controlled flow with microwave control of NV electronic spins, we introduce a new noise-suppressed mode of Optically Detected Magnetic Resonance that is sensitive to chemical analytes while resilient against experimental variations, achieving detection of analyte-induced signals at an unprecedented level of a few hundredths of a percent of the ND fluorescence. We demonstrate its application to detecting paramagnetic ions in droplets with simultaneously low limit-of-detection and low analyte volumes, in a manner significantly better than existing technologies. This is combined with exceptional measurement stability over >103s and across hundreds of thousands of droplets, while utilizing minimal sensor volumes and incurring low ND costs (<$0.70 for an hour of operation). Additionally, we demonstrate using these droplets as micro-confinement chambers by co-encapsulating ND quantum sensors with analytes, including single cells. This versatility suggests wide-ranging applications, like single-cell metabolomics and real-time intracellular measurements in bioreactors. Our work paves the way for portable, high-sensitivity, amplification-free, chemical assays with high throughput; introduces a new chemical imaging tool for probing chemical reactions in microenvironments; and establishes the foundation for developing movable, arrayed quantum sensors through droplet microfluidics.","sentences":["We report on a novel flow-based method for high-precision chemical detection that integrates quantum sensing with droplet microfluidics.","We deploy nanodiamond particles hosting fluorescent nitrogen vacancy defects as quantum sensors in flowing, monodisperse, picoliter-volume microdroplets containing analyte molecules.","ND motion within these microcompartments facilitates close sensor-analyte interaction and mitigates particle heterogeneity.","Microdroplet flow rates are rapid (upto 4cm/s) and with minimal drift.","Pairing this controlled flow with microwave control of NV electronic spins, we introduce a new noise-suppressed mode of Optically Detected Magnetic Resonance that is sensitive to chemical analytes while resilient against experimental variations, achieving detection of analyte-induced signals at an unprecedented level of a few hundredths of a percent of the ND fluorescence.","We demonstrate its application to detecting paramagnetic ions in droplets with simultaneously low limit-of-detection and low analyte volumes, in a manner significantly better than existing technologies.","This is combined with exceptional measurement stability over >103s and across hundreds of thousands of droplets, while utilizing minimal sensor volumes and incurring low ND costs (<$0.70 for an hour of operation).","Additionally, we demonstrate using these droplets as micro-confinement chambers by co-encapsulating ND quantum sensors with analytes, including single cells.","This versatility suggests wide-ranging applications, like single-cell metabolomics and real-time intracellular measurements in bioreactors.","Our work paves the way for portable, high-sensitivity, amplification-free, chemical assays with high throughput; introduces a new chemical imaging tool for probing chemical reactions in microenvironments; and establishes the foundation for developing movable, arrayed quantum sensors through droplet microfluidics."],"url":"http://arxiv.org/abs/2404.19313v1","category":"quant-ph"}
{"created":"2024-04-30 07:02:05","title":"Phase-Dependent Spectral Shape Changes in the Ultraluminous X-Ray Pulsar NGC 5907 ULX1","abstract":"Discovery of coherent pulsations from several ultraluminous X-ray pulsars (ULXPs) has provided direct evidence of super-critical accretion flow. However, geometrical structure of such accretion flow onto the central neutron star remains poorly understood. NGC 5907 ULX1 is one of the most luminous ULXPs with the luminosity exceeding $10^{41}~{\\rm erg~s^{-1}}$. Here we present a broadband X-ray study of this ULXP using the data from simultaneous observations with XMM-Newton and NuSTAR conducted in July 2014. The phase-resolved spectra are well reproduced by a model consisting of a multicolor disk blackbody emission with a temperature gradient of $p = 0.5~(T \\propto r^{-p})$ and a power law with an exponential cutoff. The disk component is phase-invariant, and has an innermost temperature of $\\sim 0.3~{\\rm keV}$. Its normalization suggests a relatively low inclination angle of the disk, in contrast to the previous claim in other literature. The power law component, attributed to the emission from the accretion flow inside the magnetosphere of the neutron star, indicates phase-dependent spectral shape changes; the spectrum is slightly harder in the pre-peak phase than in the post-peak phase. This implies that the magnetosphere has an asymmetric geometry around the magnetic axis, and that hotter regions close to the magnetic pole become visible before the pulse peak.","sentences":["Discovery of coherent pulsations from several ultraluminous X-ray pulsars (ULXPs) has provided direct evidence of super-critical accretion flow.","However, geometrical structure of such accretion flow onto the central neutron star remains poorly understood.","NGC 5907 ULX1 is one of the most luminous ULXPs with the luminosity exceeding $10^{41}~{\\rm erg~s^{-1}}$.","Here we present a broadband X-ray study of this ULXP using the data from simultaneous observations with XMM-Newton and NuSTAR conducted in July 2014.","The phase-resolved spectra are well reproduced by a model consisting of a multicolor disk blackbody emission with a temperature gradient of $p = 0.5~(T","\\propto r^{-p})$ and a power law with an exponential cutoff.","The disk component is phase-invariant, and has an innermost temperature of $\\sim 0.3~{\\rm keV}$. Its normalization suggests a relatively low inclination angle of the disk, in contrast to the previous claim in other literature.","The power law component, attributed to the emission from the accretion flow inside the magnetosphere of the neutron star, indicates phase-dependent spectral shape changes; the spectrum is slightly harder in the pre-peak phase than in the post-peak phase.","This implies that the magnetosphere has an asymmetric geometry around the magnetic axis, and that hotter regions close to the magnetic pole become visible before the pulse peak."],"url":"http://arxiv.org/abs/2404.19300v1","category":"astro-ph.HE"}
{"created":"2024-04-30 06:09:43","title":"Unveiling the effects of Cu doping on the H2 activation by CeO2 surface frustrated Lewis pairs","abstract":"Recently, the solid-state frustrated Lewis pairs (FLPs) on the surface of CeO2 have been demonstrated to effectively catalyze the selective hydrogenation of unsaturated substrates, hence, the relationship between their intrinsic properties and H2 activation at the atomic scale has attracted great attention. In this work, the effects of Cu doping on the intrinsic FLPs properties for different facets of CeO2 is investigated by using density functional theory calculations, including the geometric parameters between Lewis acid-base centers, and the reactivity of Lewis acid-base towards H2 activation. The study demonstrates that introducing O vacancies on different crystal facets of CeO2 creates FLPs with the ability to efficiently cleavage hydrogen molecules. After the substitution of Ce with Cu, the inadequate electron availability of Cu to bond with O contributes to a reduction in the formation energy of O vacancies. Importantly, Cu exert an influence not only on the intrinsic properties of FLPs but also on the formation of new Ce-O and Cu-O FLPs. Considering the H2 activation, the doping of Cu results in an enhancement for the thermodynamics by decreasing the reaction energies, while a hinderance for the kinetics by increasing the energy barriers. Overall, with these theoretical investigations, we propose certain hints for the future experimental studies concerning the synthesis of Cu doped CeO2 catalysts for the H2 activation and hydrogenation reactions.","sentences":["Recently, the solid-state frustrated Lewis pairs (FLPs) on the surface of CeO2 have been demonstrated to effectively catalyze the selective hydrogenation of unsaturated substrates, hence, the relationship between their intrinsic properties and H2 activation at the atomic scale has attracted great attention.","In this work, the effects of Cu doping on the intrinsic FLPs properties for different facets of CeO2 is investigated by using density functional theory calculations, including the geometric parameters between Lewis acid-base centers, and the reactivity of Lewis acid-base towards H2 activation.","The study demonstrates that introducing O vacancies on different crystal facets of CeO2 creates FLPs with the ability to efficiently cleavage hydrogen molecules.","After the substitution of Ce with Cu, the inadequate electron availability of Cu to bond with O contributes to a reduction in the formation energy of O vacancies.","Importantly, Cu exert an influence not only on the intrinsic properties of FLPs but also on the formation of new Ce-O and Cu-O FLPs.","Considering the H2 activation, the doping of Cu results in an enhancement for the thermodynamics by decreasing the reaction energies, while a hinderance for the kinetics by increasing the energy barriers.","Overall, with these theoretical investigations, we propose certain hints for the future experimental studies concerning the synthesis of Cu doped CeO2 catalysts for the H2 activation and hydrogenation reactions."],"url":"http://arxiv.org/abs/2404.19280v1","category":"physics.chem-ph"}
{"created":"2024-04-30 05:36:58","title":"Chandra Study of the Proper Motion of HST-1 in the Jet of M87","abstract":"The radio galaxy M87 is well known for its jet, which features a series of bright knots observable from radio to X-ray wavelengths. We analyze the X-ray image and flux variability of the knot HST-1 in the jet. Our analysis includes all 112 available \\textit{Chandra} ACIS-S observations from 2000-2021, with a total exposure time of $\\sim$887 ks. We use de-convolved images to study the brightness profile of the X-ray jet and measure the relative separation between the core and HST-1. From 2003-2005 (which coincides with a bright flare from HST-1), we find a correlation between the flux of HST-1 and its offset from the core. In subsequent data, we find a steady increase in this offset, which implies a bulk superluminal motion for HST-1 of 6.6$\\pm$0.9 c (2.0$\\pm$0.3 pc yr$^{-1}$), in keeping with prior results. We discuss models for the flux-offset correlation that feature either two or four emission regions separated by \\textbf{tens} of parsecs. We attribute these results to moving shocks in the jet, that allow us to measure the internal structure of the jet.","sentences":["The radio galaxy M87 is well known for its jet, which features a series of bright knots observable from radio to X-ray wavelengths.","We analyze the X-ray image and flux variability of the knot HST-1 in the jet.","Our analysis includes all 112 available \\textit{Chandra} ACIS-S observations from 2000-2021, with a total exposure time of $\\sim$887 ks.","We use de-convolved images to study the brightness profile of the X-ray jet and measure the relative separation between the core and HST-1.","From 2003-2005 (which coincides with a bright flare from HST-1), we find a correlation between the flux of HST-1 and its offset from the core.","In subsequent data, we find a steady increase in this offset, which implies a bulk superluminal motion for HST-1 of 6.6$\\pm$0.9 c (2.0$\\pm$0.3 pc yr$^{-1}$), in keeping with prior results.","We discuss models for the flux-offset correlation that feature either two or four emission regions separated by \\textbf{tens} of parsecs.","We attribute these results to moving shocks in the jet, that allow us to measure the internal structure of the jet."],"url":"http://arxiv.org/abs/2404.19272v1","category":"astro-ph.HE"}
{"created":"2024-04-30 02:53:14","title":"Variational approximations of possibilistic inferential models","abstract":"Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference. But despite IMs' theoretical/foundational advantages, efficient computation in applications is a major challenge. This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM's possibility contour, or at least its $\\alpha$-level set for a specified $\\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics.","sentences":["Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference.","But despite IMs' theoretical/foundational advantages, efficient computation in applications is a major challenge.","This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM's possibility contour, or at least its $\\alpha$-level set for a specified $\\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics."],"url":"http://arxiv.org/abs/2404.19224v1","category":"stat.CO"}
{"created":"2024-04-30 02:19:13","title":"Optical Spectroscopy of Type Ia Supernovae by the Carnegie Supernova Projects I and II","abstract":"We present the second and final release of optical spectroscopy of Type Ia Supernovae (SNe Ia) obtained during the first and second phases of the Carnegie Supernova Project (CSP-I and CSP-II). The newly released data consist of 148 spectra of 30 SNe Ia observed in the course of the CSP-I, and 234 spectra of 127 SNe Ia obtained during the CSP-II. We also present 216 optical spectra of 46 historical SNe Ia, including 53 spectra of 30 SNe Ia observed by the Cal\\'an/Tololo Supernova Survey. We combine these observations with previously published CSP data and publicly-available spectra to compile a large sample of measurements of spectroscopic parameters at maximum light, consisting of pseudo-equivalent widths and expansion velocities of selected features, for 232 CSP and historical SNe Ia (including more than 1000 spectra). Finally, we review some of the strongest correlations between spectroscopic and photometric properties of SNe Ia. Specifically, we define two samples: one consisting of SNe Ia discovered by targeted searches (most of them CSP-I objects) and the other composed of SNe Ia discovered by untargeted searches, which includes most of the CSP-II objects. The analysed correlations are similar for both samples. We find a larger incidence of SNe Ia belonging to the Cool (CL)and Broad Line (BL) Branch subtypes among the events discovered by targeted searches, Shallow Silicon (SS) SNe Ia are present with similar frequencies in both samples, while Core Normal (CN) SNe Ia are more frequent in untargeted searches.","sentences":["We present the second and final release of optical spectroscopy of Type Ia Supernovae (SNe Ia) obtained during the first and second phases of the Carnegie Supernova Project (CSP-I and CSP-II).","The newly released data consist of 148 spectra of 30 SNe Ia observed in the course of the CSP-I, and 234 spectra of 127 SNe Ia obtained during the CSP-II.","We also present 216 optical spectra of 46 historical SNe Ia, including 53 spectra of 30 SNe Ia observed by the Cal\\'an/Tololo Supernova Survey.","We combine these observations with previously published CSP data and publicly-available spectra to compile a large sample of measurements of spectroscopic parameters at maximum light, consisting of pseudo-equivalent widths and expansion velocities of selected features, for 232 CSP and historical SNe Ia (including more than 1000 spectra).","Finally, we review some of the strongest correlations between spectroscopic and photometric properties of SNe Ia. Specifically, we define two samples: one consisting of SNe Ia discovered by targeted searches (most of them CSP-I objects) and the other composed of SNe Ia discovered by untargeted searches, which includes most of the CSP-II objects.","The analysed correlations are similar for both samples.","We find a larger incidence of SNe Ia belonging to the Cool (CL)and Broad Line (BL) Branch subtypes among the events discovered by targeted searches, Shallow Silicon (SS) SNe","Ia are present with similar frequencies in both samples, while Core Normal (CN) SNe Ia are more frequent in untargeted searches."],"url":"http://arxiv.org/abs/2404.19208v1","category":"astro-ph.HE"}
{"created":"2024-04-30 01:48:16","title":"Evaluation of Thermal Performance of a Wick-free Vapor Chamber in Power Electronics Cooling","abstract":"Efficient thermal management in high-power electronics cooling can be achieved using phase-change heat transfer devices, such as vapor chambers. Traditional vapor chambers use wicks to transport condensate for efficient thermal exchange and to prevent \"dry-out\" of the evaporator. However, wicks in vapor chambers present significant design challenges arising out of large pressure drops across the wicking material, which slows down condensate transport rates and increases the chances for dry-out. Thicker wicks add to overall thermal resistance, while deterring the development of thinner devices by limiting the total thickness of the vapor chamber. Wickless vapor chambers eliminate the use of metal wicks entirely, by incorporating complementary wettability-patterned flat plates on both the evaporator and the condenser side. Such surface modifications enhance fluid transport on the evaporator side, while allowing the chambers to be virtually as thin as imaginable, thereby permitting design of thermally efficient thin electronic cooling devices. While wick-free vapor chambers have been studied and efficient design strategies have been suggested, we delve into real-life applications of wick-free vapor chambers in forced air cooling of high-power electronics. An experimental setup is developed wherein two Si-based MOSFETs of TO-247-3 packaging having high conduction resistance, are connected in parallel and switched at 100 kHz, to emulate high frequency power electronics operations. A rectangular copper wick-free vapor chamber spreads heat laterally over a surface 13 times larger than the heating area. This chamber is cooled externally by a fan that circulates air at room temperature. The present experimental setup extends our previous work on wick-free vapor chambers, while demonstrating the effectiveness of low-cost air cooling in vapor-chamber enhanced high-power electronics applications.","sentences":["Efficient thermal management in high-power electronics cooling can be achieved using phase-change heat transfer devices, such as vapor chambers.","Traditional vapor chambers use wicks to transport condensate for efficient thermal exchange and to prevent \"dry-out\" of the evaporator.","However, wicks in vapor chambers present significant design challenges arising out of large pressure drops across the wicking material, which slows down condensate transport rates and increases the chances for dry-out.","Thicker wicks add to overall thermal resistance, while deterring the development of thinner devices by limiting the total thickness of the vapor chamber.","Wickless vapor chambers eliminate the use of metal wicks entirely, by incorporating complementary wettability-patterned flat plates on both the evaporator and the condenser side.","Such surface modifications enhance fluid transport on the evaporator side, while allowing the chambers to be virtually as thin as imaginable, thereby permitting design of thermally efficient thin electronic cooling devices.","While wick-free vapor chambers have been studied and efficient design strategies have been suggested, we delve into real-life applications of wick-free vapor chambers in forced air cooling of high-power electronics.","An experimental setup is developed wherein two Si-based MOSFETs of TO-247-3 packaging having high conduction resistance, are connected in parallel and switched at 100 kHz, to emulate high frequency power electronics operations.","A rectangular copper wick-free vapor chamber spreads heat laterally over a surface 13 times larger than the heating area.","This chamber is cooled externally by a fan that circulates air at room temperature.","The present experimental setup extends our previous work on wick-free vapor chambers, while demonstrating the effectiveness of low-cost air cooling in vapor-chamber enhanced high-power electronics applications."],"url":"http://arxiv.org/abs/2404.19195v1","category":"eess.SY"}
{"created":"2024-04-30 01:37:23","title":"High-energy neutrino signals from supernova explosions: a new window into dark photon parameter space","abstract":"Dark photons, hypothetical feebly interacting massive vector bosons, appear in many extensions of the Standard Model. This study investigates their production and subsequent decay during supernova explosions. We demonstrate that the decay of dark photons, with masses ranging from 200 to 400 MeV, can lead to the emission of neutrinos with energies surpassing those emitted by supernovae. These neutrinos therefore serve as a distinct signal of new physics, allowing for the exploration of previously uncharted regions of the dark photon parameter space and complementing both accelerator-based searches and other astrophysical constraints. The signal is largely unaffected by the specifics of the supernova's temperature and density radial profiles outside the SN core, rendering the prediction both robust and model-independent. Our results indicate that searching for high-energy neutrinos accompanying supernova explosions provides a novel approach to probe physics beyond the Standard Model, including dark photons, heavy neutral leptons, and other feebly interacting particles with masses in the hundreds of MeV range.","sentences":["Dark photons, hypothetical feebly interacting massive vector bosons, appear in many extensions of the Standard Model.","This study investigates their production and subsequent decay during supernova explosions.","We demonstrate that the decay of dark photons, with masses ranging from 200 to 400 MeV, can lead to the emission of neutrinos with energies surpassing those emitted by supernovae.","These neutrinos therefore serve as a distinct signal of new physics, allowing for the exploration of previously uncharted regions of the dark photon parameter space and complementing both accelerator-based searches and other astrophysical constraints.","The signal is largely unaffected by the specifics of the supernova's temperature and density radial profiles outside the SN core, rendering the prediction both robust and model-independent.","Our results indicate that searching for high-energy neutrinos accompanying supernova explosions provides a novel approach to probe physics beyond the Standard Model, including dark photons, heavy neutral leptons, and other feebly interacting particles with masses in the hundreds of MeV range."],"url":"http://arxiv.org/abs/2404.19191v1","category":"hep-ph"}
{"created":"2024-04-30 01:17:18","title":"Low-energy Injection and Nonthermal Particle Acceleration in Relativistic Magnetic Turbulence","abstract":"Relativistic magnetic turbulence has been proposed as a process for producing nonthermal particles in high-energy astrophysics. The particle energization may be contributed by both magnetic reconnection and turbulent fluctuations, but their interplay is poorly understood. It has been suggested that during magnetic reconnection the parallel electric field dominates the particle acceleration up to the lower bound of the power-law particle spectrum, but recent studies show that electric fields perpendicular to the magnetic field can play an important, if not dominant role. In this study, we carry out fully kinetic particle-in-cell simulations of magnetically dominated decaying turbulence in a relativistic pair plasma. For a fixed magnetization parameter $\\sigma_0 = 20$, we find that the injection energy~$\\varepsilon_{\\rm inj}$ converges with increasing domain size to~$\\varepsilon_{\\rm inj} \\simeq 10 \\, m_ec^2$. In contrast, the power-law index, the cut-off energy, and the power-law extent increase steadily with domain size. We trace a large number of particles and evaluate the contributions of the work done by the parallel ($W_\\parallel$) and perpendicular ($W_\\perp$) electric fields during both the injection phase and the post-injection phase. We find that during the injection phase, the $W_\\perp$ contribution increases with domain size, suggesting that it may eventually dominate injection for a sufficiently large domain. In contrast, both components contribute equally during the post-injection phase, insensitive to the domain size. For high energy ($\\varepsilon \\gg \\varepsilon_{\\rm inj}$) particles, $W_\\perp$ dominates the subsequent energization. These findings may improve our understanding of nonthermal particles and their emissions in astrophysical plasmas.","sentences":["Relativistic magnetic turbulence has been proposed as a process for producing nonthermal particles in high-energy astrophysics.","The particle energization may be contributed by both magnetic reconnection and turbulent fluctuations, but their interplay is poorly understood.","It has been suggested that during magnetic reconnection the parallel electric field dominates the particle acceleration up to the lower bound of the power-law particle spectrum, but recent studies show that electric fields perpendicular to the magnetic field can play an important, if not dominant role.","In this study, we carry out fully kinetic particle-in-cell simulations of magnetically dominated decaying turbulence in a relativistic pair plasma.","For a fixed magnetization parameter $\\sigma_0 = 20$, we find that the injection energy~$\\varepsilon_{\\rm inj}$ converges with increasing domain size to~$\\varepsilon_{\\rm inj} \\simeq 10 \\, m_ec^2$. In contrast, the power-law index, the cut-off energy, and the power-law extent increase steadily with domain size.","We trace a large number of particles and evaluate the contributions of the work done by the parallel ($W_\\parallel$) and perpendicular ($W_\\perp$) electric fields during both the injection phase and the post-injection phase.","We find that during the injection phase, the $W_\\perp$ contribution increases with domain size, suggesting that it may eventually dominate injection for a sufficiently large domain.","In contrast, both components contribute equally during the post-injection phase, insensitive to the domain size.","For high energy ($\\varepsilon \\gg \\varepsilon_{\\rm inj}$) particles, $W_\\perp$ dominates the subsequent energization.","These findings may improve our understanding of nonthermal particles and their emissions in astrophysical plasmas."],"url":"http://arxiv.org/abs/2404.19181v1","category":"astro-ph.HE"}
{"created":"2024-04-30 00:34:34","title":"Striking the Right Balance of Encoding Electron Correlation in the Hamiltonian and the Wavefunction Ansatz","abstract":"Multi-configurational electronic structure theory delivers the most versatile approximations to many-electron wavefunctions, flexible enough to deal with all sorts of transformations, ranging from electronic excitations, to open-shell molecules and chemical reactions. Multi-configurational models are therefore essential to establish universally applicable, predictive ab initio methods for chemistry. Here, we present a discussion of explicit correlation approaches which address the nagging problem of dealing with static and dynamic electron correlation in multi-configurational active-space approaches. We review the latest developments and then point to their key obstacles. Our discussion is supported by new data obtained with tensor network methods. We argue in favor of simple electrons-only correlator expressions that may allow one to define transcorrelated models in which the correlator does not bear a dependence on molecular structure.","sentences":["Multi-configurational electronic structure theory delivers the most versatile approximations to many-electron wavefunctions, flexible enough to deal with all sorts of transformations, ranging from electronic excitations, to open-shell molecules and chemical reactions.","Multi-configurational models are therefore essential to establish universally applicable, predictive ab initio methods for chemistry.","Here, we present a discussion of explicit correlation approaches which address the nagging problem of dealing with static and dynamic electron correlation in multi-configurational active-space approaches.","We review the latest developments and then point to their key obstacles.","Our discussion is supported by new data obtained with tensor network methods.","We argue in favor of simple electrons-only correlator expressions that may allow one to define transcorrelated models in which the correlator does not bear a dependence on molecular structure."],"url":"http://arxiv.org/abs/2404.19172v1","category":"physics.chem-ph"}
{"created":"2024-04-30 00:20:52","title":"Correlations between X-rays, Visible Light and Drive-Beam Energy Loss Observed in Plasma Wakefield Acceleration Experiments at FACET-II","abstract":"This study documents several correlations observed during the first run of the plasma wakefield acceleration experiment E300 conducted at FACET-II, using a single drive electron bunch. The established correlations include those between the measured maximum energy loss of the drive electron beam and the integrated betatron x-ray signal, the calculated total beam energy deposited in the plasma and the integrated x-ray signal, among three visible light emission measuring cameras, and between the visible plasma light and x-ray signal. The integrated x-ray signal correlates almost linearly with both the maximum energy loss of the drive beam and the energy deposited into the plasma, demonstrating its usability as a measure of energy transfer from the drive beam to the plasma. Visible plasma light is found to be a useful indicator of the presence of wake at three locations that overall are two meters apart. Despite the complex dynamics and vastly different timescales, the x-ray radiation from the drive bunch and visible light emission from the plasma may prove to be effective non-invasive diagnostics for monitoring the energy transfer from the beam to the plasma in future high-repetition-rate experiments.","sentences":["This study documents several correlations observed during the first run of the plasma wakefield acceleration experiment E300 conducted at FACET-II, using a single drive electron bunch.","The established correlations include those between the measured maximum energy loss of the drive electron beam and the integrated betatron x-ray signal, the calculated total beam energy deposited in the plasma and the integrated x-ray signal, among three visible light emission measuring cameras, and between the visible plasma light and x-ray signal.","The integrated x-ray signal correlates almost linearly with both the maximum energy loss of the drive beam and the energy deposited into the plasma, demonstrating its usability as a measure of energy transfer from the drive beam to the plasma.","Visible plasma light is found to be a useful indicator of the presence of wake at three locations that overall are two meters apart.","Despite the complex dynamics and vastly different timescales, the x-ray radiation from the drive bunch and visible light emission from the plasma may prove to be effective non-invasive diagnostics for monitoring the energy transfer from the beam to the plasma in future high-repetition-rate experiments."],"url":"http://arxiv.org/abs/2404.19169v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 23:52:53","title":"Hidden Symmetries of Power-Law Inflation","abstract":"A scalar field with an exponential potential in FLRW universe admits the exact solution. We uncover the hidden symmetries behind the system by utilising the Eisenhart lift of field theories. We find that a conformal Killing vector field in the field space exists only for a particular combination of exponential functions which includes a single exponential potential. This implies the existence of additional conserved quantity and explains the integrability of the system.","sentences":["A scalar field with an exponential potential in FLRW universe admits the exact solution.","We uncover the hidden symmetries behind the system by utilising the Eisenhart lift of field theories.","We find that a conformal Killing vector field in the field space exists only for a particular combination of exponential functions which includes a single exponential potential.","This implies the existence of additional conserved quantity and explains the integrability of the system."],"url":"http://arxiv.org/abs/2404.19162v1","category":"gr-qc"}
{"created":"2024-04-29 23:50:02","title":"How to define the Unruh-DeWitt detector on manifolds","abstract":"The vacuum of an accelerating observer is defined on the local inertial frame, which is called a ``moving frame''. However, in the discussion of the Unruh-deWitt detector, many papers define the vacuum on a fixed frame. This paper discusses the Unruh-deWitt detector by defining the vacuum directly on the local inertial frame, showing that the problem of the Stokes phenomenon can be solved by using the exact WKB.","sentences":["The vacuum of an accelerating observer is defined on the local inertial frame, which is called a ``moving frame''.","However, in the discussion of the Unruh-deWitt detector, many papers define the vacuum on a fixed frame.","This paper discusses the Unruh-deWitt detector by defining the vacuum directly on the local inertial frame, showing that the problem of the Stokes phenomenon can be solved by using the exact WKB."],"url":"http://arxiv.org/abs/2404.19160v1","category":"hep-th"}
{"created":"2024-04-29 23:33:30","title":"Symmetry Strategy for Rapid Discovery of Abundant Fractional Quantum Ferroelectrics","abstract":"Traditional ferroelectrics are limited by Neumann's principle, which confines exploration of ferroelectrics within polar point groups. Our recent work [Nat. Commun. 15, 135, (2024)] proposes the concept of fractional quantum ferroelectricity (FQFE) that extend the playground of ferroelectricity to non-polar point groups. Here, we apply group theory and introduce an efficient symmetry strategy to identify FQFE candidates. Integrated with a high-throughput screening scheme, we go through 171,527 materials and identify 202 potential FQFE candidates, which are already experimentally synthesized. In addition, we point out that the essence of FQFE is fractional atomic displacements with respect to lattice vectors, which can actually result in both fractional (type-I) and integer (type-II) quantized polarization, respectively. Through performing first-principles calculations, we verify the symmetry-predicted switchable FQFE properties in bulk AlAgS2 and monolayer HgI2. Notably, AlAgS2 exhibits an ultra-low switching barrier of 23 meV/f.u. and interlocked in-plane/out-of-plane polarization, while HgI2 demonstrates large spontaneous polarization of 42 {\\mu}C/cm2. Our findings not only advance the understanding on FQFE, but also offer guidance for experimental exploration and design of novel ferroelectric materials.","sentences":["Traditional ferroelectrics are limited by Neumann's principle, which confines exploration of ferroelectrics within polar point groups.","Our recent work [Nat. Commun.","15, 135, (2024)] proposes the concept of fractional quantum ferroelectricity (FQFE) that extend the playground of ferroelectricity to non-polar point groups.","Here, we apply group theory and introduce an efficient symmetry strategy to identify FQFE candidates.","Integrated with a high-throughput screening scheme, we go through 171,527 materials and identify 202 potential FQFE candidates, which are already experimentally synthesized.","In addition, we point out that the essence of FQFE is fractional atomic displacements with respect to lattice vectors, which can actually result in both fractional (type-I) and integer (type-II) quantized polarization, respectively.","Through performing first-principles calculations, we verify the symmetry-predicted switchable FQFE properties in bulk AlAgS2 and monolayer HgI2.","Notably, AlAgS2 exhibits an ultra-low switching barrier of 23 meV/f.u. and interlocked in-plane/out-of-plane polarization, while HgI2 demonstrates large spontaneous polarization of 42 {\\mu}C/cm2.","Our findings not only advance the understanding on FQFE, but also offer guidance for experimental exploration and design of novel ferroelectric materials."],"url":"http://arxiv.org/abs/2404.19152v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 23:32:36","title":"Long term CCD photometry of the distant cluster NGC 2419: the CMD revisited","abstract":"Employing \\emph{VI} images of NGC 2419 acquired over 17 years, light curves for most of the known variables in the field of the cluster are produced. A cluster membership analysis for about 3100 stars in the cluster field with proper motions from $Gaia$-DR3, revealed the presence of member stars as far as 140 pc from the cluster center and enabled the construction of a cleaner CMD free of field stars. It was found that RRab and RRc stars share the inter-order region in the instability strip, which is unusual for OoII clusters. Theoretical considerations confirm that Pop II cepheids are descendants of extreme ZAHB blue tail stars with very thin envelopes of about 10\\% of the total mass. Member RR Lyrae stars were employed to calculate independent estimates of the mean cluster metallicity and distance; we found [Fe/H]$_{\\rm UV}= -1.90 \\pm 0.27$ and $D=86.3 \\pm 5.0$ kpc from the RRab and [Fe/H]$_{\\rm UV}= -1.88 \\pm 0.30$ and $D=83.1 \\pm 8.1$ kpc from the RRc light curves.","sentences":["Employing \\emph{VI} images of NGC 2419 acquired over 17 years, light curves for most of the known variables in the field of the cluster are produced.","A cluster membership analysis for about 3100 stars in the cluster field with proper motions from $Gaia$-DR3, revealed the presence of member stars as far as 140 pc from the cluster center and enabled the construction of a cleaner CMD free of field stars.","It was found that RRab and RRc stars share the inter-order region in the instability strip, which is unusual for OoII clusters.","Theoretical considerations confirm that Pop II cepheids are descendants of extreme ZAHB blue tail stars with very thin envelopes of about 10\\% of the total mass.","Member RR Lyrae stars were employed to calculate independent estimates of the mean cluster metallicity and distance; we found [Fe/H]$_{\\rm UV}= -1.90 \\pm 0.27$ and $D=86.3 \\pm 5.0$ kpc from the RRab and [Fe/H]$_{\\rm UV}= -1.88 \\pm 0.30$ and $D=83.1 \\pm 8.1$ kpc from the RRc light curves."],"url":"http://arxiv.org/abs/2404.19151v1","category":"astro-ph.SR"}
{"created":"2024-04-29 22:43:08","title":"Thermoelectric transport properties of the quasi-one-dimensional dimer-Mott insulator $\u03b2'$-(BEDT-TTF)$_2$ICl$_2$","abstract":"Low-dimensional materials, in which the electronic and transport properties are drastically modified in comparison to those of three-dimensional bulk materials, yield a key class of thermoelectric materials with high conversion efficiency. Among such materials, the organic compounds may serve peculiar properties owing to their unique molecular-based low-dimensional structures with highly anisotropic molecular orbitals. Here we present the thermoelectric transport properties of the quasi-one-dimensional dimer-Mott insulator $\\beta'$-(BEDT-TTF)$_2$ICl$_2$, where BEDT-TTF stands for bis(ethylenedithio)-tetrathiafulvalene. We find that the thermopower exhibits typical activation-type temperature variation expected for insulators but its absolute value is anomalously large compared to the expected value from the activation-type temperature dependence of the electrical resistivity. Successively, the Jonker-plot analysis, in which the thermopower is usually scaled by the logarithm of the resistivity, shows an unusual relation among such transport quantities. We discuss a role of the low dimensionality for the enhanced thermopower along with recent observations of such a large thermopower in several low-dimensional materials.","sentences":["Low-dimensional materials, in which the electronic and transport properties are drastically modified in comparison to those of three-dimensional bulk materials, yield a key class of thermoelectric materials with high conversion efficiency.","Among such materials, the organic compounds may serve peculiar properties owing to their unique molecular-based low-dimensional structures with highly anisotropic molecular orbitals.","Here we present the thermoelectric transport properties of the quasi-one-dimensional dimer-Mott insulator $\\beta'$-(BEDT-TTF)$_2$ICl$_2$, where BEDT-TTF stands for bis(ethylenedithio)-tetrathiafulvalene.","We find that the thermopower exhibits typical activation-type temperature variation expected for insulators but its absolute value is anomalously large compared to the expected value from the activation-type temperature dependence of the electrical resistivity.","Successively, the Jonker-plot analysis, in which the thermopower is usually scaled by the logarithm of the resistivity, shows an unusual relation among such transport quantities.","We discuss a role of the low dimensionality for the enhanced thermopower along with recent observations of such a large thermopower in several low-dimensional materials."],"url":"http://arxiv.org/abs/2404.19137v1","category":"cond-mat.str-el"}
