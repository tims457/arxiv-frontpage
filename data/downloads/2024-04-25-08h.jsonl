{"created":"2024-04-24 03:29:45","title":"Biologically-Informed Excitatory and Inhibitory Balance for Robust Spiking Neural Network Training","abstract":"Spiking neural networks drawing inspiration from biological constraints of the brain promise an energy-efficient paradigm for artificial intelligence. However, challenges exist in identifying guiding principles to train these networks in a robust fashion. In addition, training becomes an even more difficult problem when incorporating biological constraints of excitatory and inhibitory connections. In this work, we identify several key factors, such as low initial firing rates and diverse inhibitory spiking patterns, that determine the overall ability to train spiking networks with various ratios of excitatory to inhibitory neurons on AI-relevant datasets. The results indicate networks with the biologically realistic 80:20 excitatory:inhibitory balance can reliably train at low activity levels and in noisy environments. Additionally, the Van Rossum distance, a measure of spike train synchrony, provides insight into the importance of inhibitory neurons to increase network robustness to noise. This work supports further biologically-informed large-scale networks and energy efficient hardware implementations.","sentences":["Spiking neural networks drawing inspiration from biological constraints of the brain promise an energy-efficient paradigm for artificial intelligence.","However, challenges exist in identifying guiding principles to train these networks in a robust fashion.","In addition, training becomes an even more difficult problem when incorporating biological constraints of excitatory and inhibitory connections.","In this work, we identify several key factors, such as low initial firing rates and diverse inhibitory spiking patterns, that determine the overall ability to train spiking networks with various ratios of excitatory to inhibitory neurons on AI-relevant datasets.","The results indicate networks with the biologically realistic 80:20 excitatory:inhibitory balance can reliably train at low activity levels and in noisy environments.","Additionally, the Van Rossum distance, a measure of spike train synchrony, provides insight into the importance of inhibitory neurons to increase network robustness to noise.","This work supports further biologically-informed large-scale networks and energy efficient hardware implementations."],"url":"http://arxiv.org/abs/2404.15627v1","category":"cs.NE"}
{"created":"2024-04-24 03:25:53","title":"Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models","abstract":"The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOD Detection, dubbed as PGR-MOOD and hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules; ii) A creative graph generator to construct prototypical graphs that are in line with ID but away from OOD; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority.","sentences":["The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions.","Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model.","In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs.","Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection.","Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges.","First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure.","Second, the generative model involving iterative denoising steps is time-consuming especially when it runs on the enormous pool of drugs.","To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOD Detection, dubbed as PGR-MOOD and hinges on three innovations: i)","An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules; ii) A creative graph generator to construct prototypical graphs that are in line with ID but away from OOD; iii)","An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule.","Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority."],"url":"http://arxiv.org/abs/2404.15625v1","category":"cs.LG"}
{"created":"2024-04-24 03:24:49","title":"A new framework of high-order unfitted finite element methods using ALE maps for moving-domain problems","abstract":"As a sequel to our previous work [C. Ma, Q. Zhang and W. Zheng, SIAM J. Numer. Anal., 60 (2022)], [C. Ma and W. Zheng, J. Comput. Phys. 469 (2022)], this paper presents a generic framework of arbitrary Lagrangian-Eulerian unfitted finite element (ALE-UFE) methods for partial differential equations (PDEs) on time-varying domains. The ALE-UFE method has a great potential in developing high-order unfitted finite element methods. The usefulness of the method is demonstrated by a variety of moving-domain problems, including a linear problem with explicit velocity of the boundary (or interface), a PDE-domain coupled problem, and a problem whose domain has a topological change. Numerical experiments show that optimal convergence is achieved by both third- and fourth-order methods on domains with smooth boundaries, but is deteriorated to the second order when the domain has topological changes.","sentences":["As a sequel to our previous work [C. Ma, Q. Zhang and W. Zheng, SIAM J. Numer.","Anal., 60 (2022)], [C. Ma and W. Zheng, J. Comput.","Phys. 469 (2022)], this paper presents a generic framework of arbitrary Lagrangian-Eulerian unfitted finite element (ALE-UFE) methods for partial differential equations (PDEs) on time-varying domains.","The ALE-UFE method has a great potential in developing high-order unfitted finite element methods.","The usefulness of the method is demonstrated by a variety of moving-domain problems, including a linear problem with explicit velocity of the boundary (or interface), a PDE-domain coupled problem, and a problem whose domain has a topological change.","Numerical experiments show that optimal convergence is achieved by both third- and fourth-order methods on domains with smooth boundaries, but is deteriorated to the second order when the domain has topological changes."],"url":"http://arxiv.org/abs/2404.15624v1","category":"math.NA"}
{"created":"2024-04-24 03:23:13","title":"Characterizing the Age of Information with Multiple Coexisting Data Streams","abstract":"In this paper we analyze the distribution of the Age of Information (AoI) of a tagged data stream sharing a processor with a set of other data streams. We do so in the highly general setting in which the interarrival times pertaining to the tagged stream can have any distribution, and also the service times of both the tagged stream and the background stream are generally distributed. The packet arrival times of the background process are assumed to constitute a Poisson process, which is justified by the fact that it typically is a superposition of many relatively homogeneous streams. The first major contribution is that we derive an expression for the Laplace-Stieltjes transform of the AoI in the resulting GI+M/GI+GI/1 model. Second, we use stochastic ordering techniques to identify tight stochastic bounds on the AoI. In addition, when approximating the tagged stream's inter-generation times through a phase-type distribution (which can be done at any precision), we present a computational algorithm for the mean AoI. As illustrated through a sequence of numerical experiments, the analysis enables us to assess the impact of background traffic on the AoI of the tagged stream.","sentences":["In this paper we analyze the distribution of the Age of Information (AoI) of a tagged data stream sharing a processor with a set of other data streams.","We do so in the highly general setting in which the interarrival times pertaining to the tagged stream can have any distribution, and also the service times of both the tagged stream and the background stream are generally distributed.","The packet arrival times of the background process are assumed to constitute a Poisson process, which is justified by the fact that it typically is a superposition of many relatively homogeneous streams.","The first major contribution is that we derive an expression for the Laplace-Stieltjes transform of the AoI in the resulting GI+M/GI+GI/1 model.","Second, we use stochastic ordering techniques to identify tight stochastic bounds on the AoI. In addition, when approximating the tagged stream's inter-generation times through a phase-type distribution (which can be done at any precision), we present a computational algorithm for the mean AoI. As illustrated through a sequence of numerical experiments, the analysis enables us to assess the impact of background traffic on the AoI of the tagged stream."],"url":"http://arxiv.org/abs/2404.15623v1","category":"cs.NI"}
{"created":"2024-04-24 03:19:31","title":"Layer Ensemble Averaging for Improving Memristor-Based Artificial Neural Network Performance","abstract":"Artificial neural networks have advanced due to scaling dimensions, but conventional computing faces inefficiency due to the von Neumann bottleneck. In-memory computation architectures, like memristors, offer promise but face challenges due to hardware non-idealities. This work proposes and experimentally demonstrates layer ensemble averaging, a technique to map pre-trained neural network solutions from software to defective hardware crossbars of emerging memory devices and reliably attain near-software performance on inference. The approach is investigated using a custom 20,000-device hardware prototyping platform on a continual learning problem where a network must learn new tasks without catastrophically forgetting previously learned information. Results demonstrate that by trading off the number of devices required for layer mapping, layer ensemble averaging can reliably boost defective memristive network performance up to the software baseline. For the investigated problem, the average multi-task classification accuracy improves from 61 % to 72 % (< 1 % of software baseline) using the proposed approach.","sentences":["Artificial neural networks have advanced due to scaling dimensions, but conventional computing faces inefficiency due to the von Neumann bottleneck.","In-memory computation architectures, like memristors, offer promise but face challenges due to hardware non-idealities.","This work proposes and experimentally demonstrates layer ensemble averaging, a technique to map pre-trained neural network solutions from software to defective hardware crossbars of emerging memory devices and reliably attain near-software performance on inference.","The approach is investigated using a custom 20,000-device hardware prototyping platform on a continual learning problem where a network must learn new tasks without catastrophically forgetting previously learned information.","Results demonstrate that by trading off the number of devices required for layer mapping, layer ensemble averaging can reliably boost defective memristive network performance up to the software baseline.","For the investigated problem, the average multi-task classification accuracy improves from 61 % to 72 % (< 1 % of software baseline) using the proposed approach."],"url":"http://arxiv.org/abs/2404.15621v1","category":"cs.ET"}
{"created":"2024-04-24 03:11:12","title":"DPO: Differential reinforcement learning with application to optimal configuration search","abstract":"Reinforcement learning (RL) with continuous state and action spaces remains one of the most challenging problems within the field. Most current learning methods focus on integral identities such as value functions to derive an optimal strategy for the learning agent. In this paper, we instead study the dual form of the original RL formulation to propose the first differential RL framework that can handle settings with limited training samples and short-length episodes. Our approach introduces Differential Policy Optimization (DPO), a pointwise and stage-wise iteration method that optimizes policies encoded by local-movement operators. We prove a pointwise convergence estimate for DPO and provide a regret bound comparable with current theoretical works. Such pointwise estimate ensures that the learned policy matches the optimal path uniformly across different steps. We then apply DPO to a class of practical RL problems which search for optimal configurations with Lagrangian rewards. DPO is easy to implement, scalable, and shows competitive results on benchmarking experiments against several popular RL methods.","sentences":["Reinforcement learning (RL) with continuous state and action spaces remains one of the most challenging problems within the field.","Most current learning methods focus on integral identities such as value functions to derive an optimal strategy for the learning agent.","In this paper, we instead study the dual form of the original RL formulation to propose the first differential RL framework that can handle settings with limited training samples and short-length episodes.","Our approach introduces Differential Policy Optimization (DPO), a pointwise and stage-wise iteration method that optimizes policies encoded by local-movement operators.","We prove a pointwise convergence estimate for DPO and provide a regret bound comparable with current theoretical works.","Such pointwise estimate ensures that the learned policy matches the optimal path uniformly across different steps.","We then apply DPO to a class of practical RL problems which search for optimal configurations with Lagrangian rewards.","DPO is easy to implement, scalable, and shows competitive results on benchmarking experiments against several popular RL methods."],"url":"http://arxiv.org/abs/2404.15617v1","category":"cs.LG"}
{"created":"2024-04-24 03:11:10","title":"A Bi-directional Quantum Search Algorithm","abstract":"Grover's search algorithms, including various partial Grover searches, experience scaling problems as the number of iterations rises with increased qubits, making implementation more computationally expensive. This paper combines Partial Grover's search algorithm and Bi-directional Search to create a fast Grover's quantum search algorithm, referred to as Bi-Directional Grover Search (BDGS). We incorporated a bi-directional search tactic with a partial Grover search, starting from an initial state and a single marked state in parallel. We have shown in this article that our novel approach requires $\\frac{\\pi}{4\\sqrt{2}}\\sqrt{N}(1-\\sqrt{\\frac{1}{b^{r/2k}}})$ iterations over regular Grover Search and Partial Grover Search (PGS), which takes $\\frac{\\pi}{4}\\sqrt{N}\\sqrt{1-\\frac{1}{b}}$ (here, $N=2^r$ elements, $b$ is the branching factor of partial search, and $k= \\lceil\\log_2b \\rceil$). The proposed BDGS algorithm is benchmarked against the state-of-the-art Depth-First Grover's Search (DFGS) and generic Grover's Search (GS) implementations for $2$ to $20$ qubits and provides promising results. The Qiskit Python implementation of the proposed BDGS algorithm is available on Github (https://github.com/hafeezzwiz21/DFGS-BDGS).","sentences":["Grover's search algorithms, including various partial Grover searches, experience scaling problems as the number of iterations rises with increased qubits, making implementation more computationally expensive.","This paper combines Partial Grover's search algorithm and Bi-directional Search to create a fast Grover's quantum search algorithm, referred to as Bi-Directional Grover Search (BDGS).","We incorporated a bi-directional search tactic with a partial Grover search, starting from an initial state and a single marked state in parallel.","We have shown in this article that our novel approach requires $\\frac{\\pi}{4\\sqrt{2}}\\sqrt{N}(1-\\sqrt{\\frac{1}{b^{r/2k}}})$ iterations over regular Grover Search and Partial Grover Search (PGS), which takes $\\frac{\\pi}{4}\\sqrt{N}\\sqrt{1-\\frac{1}{b}}$ (here, $N=2^r$ elements, $b$ is the branching factor of partial search, and $k= \\lceil\\log_2b \\rceil$).","The proposed BDGS algorithm is benchmarked against the state-of-the-art Depth-First Grover's Search (DFGS) and generic Grover's Search (GS) implementations for $2$ to $20$ qubits and provides promising results.","The Qiskit Python implementation of the proposed BDGS algorithm is available on Github (https://github.com/hafeezzwiz21/DFGS-BDGS)."],"url":"http://arxiv.org/abs/2404.15616v1","category":"quant-ph"}
{"created":"2024-04-24 02:42:24","title":"Hybrid LLM/Rule-based Approaches to Business Insights Generation from Structured Data","abstract":"In the field of business data analysis, the ability to extract actionable insights from vast and varied datasets is essential for informed decision-making and maintaining a competitive edge. Traditional rule-based systems, while reliable, often fall short when faced with the complexity and dynamism of modern business data. Conversely, Artificial Intelligence (AI) models, particularly Large Language Models (LLMs), offer significant potential in pattern recognition and predictive analytics but can lack the precision necessary for specific business applications. This paper explores the efficacy of hybrid approaches that integrate the robustness of rule-based systems with the adaptive power of LLMs in generating actionable business insights.","sentences":["In the field of business data analysis, the ability to extract actionable insights from vast and varied datasets is essential for informed decision-making and maintaining a competitive edge.","Traditional rule-based systems, while reliable, often fall short when faced with the complexity and dynamism of modern business data.","Conversely, Artificial Intelligence (AI) models, particularly Large Language Models (LLMs), offer significant potential in pattern recognition and predictive analytics but can lack the precision necessary for specific business applications.","This paper explores the efficacy of hybrid approaches that integrate the robustness of rule-based systems with the adaptive power of LLMs in generating actionable business insights."],"url":"http://arxiv.org/abs/2404.15604v1","category":"cs.CL"}
{"created":"2024-04-24 02:40:58","title":"Deepfakes and Higher Education: A Research Agenda and Scoping Review of Synthetic Media","abstract":"The availability of software which can produce convincing yet synthetic media poses both threats and benefits to tertiary education globally. While other forms of synthetic media exist, this study focuses on deepfakes, which are advanced Generative AI (GenAI) fakes of real people. This conceptual paper assesses the current literature on deepfakes across multiple disciplines by conducting an initial scoping review of 182 peer-reviewed publications.   The review reveals three major trends: detection methods, malicious applications, and potential benefits, although no specific studies on deepfakes in the tertiary educational context were found. Following a discussion of these trends, this study applies the findings to postulate the major risks and potential mitigation strategies of deepfake technologies in higher education, as well as potential beneficial uses to aid the teaching and learning of both deepfakes and synthetic media. This culminates in the proposal of a research agenda to build a comprehensive, cross-cultural approach to investigate deepfakes in higher education.","sentences":["The availability of software which can produce convincing yet synthetic media poses both threats and benefits to tertiary education globally.","While other forms of synthetic media exist, this study focuses on deepfakes, which are advanced Generative AI (GenAI) fakes of real people.","This conceptual paper assesses the current literature on deepfakes across multiple disciplines by conducting an initial scoping review of 182 peer-reviewed publications.   ","The review reveals three major trends: detection methods, malicious applications, and potential benefits, although no specific studies on deepfakes in the tertiary educational context were found.","Following a discussion of these trends, this study applies the findings to postulate the major risks and potential mitigation strategies of deepfake technologies in higher education, as well as potential beneficial uses to aid the teaching and learning of both deepfakes and synthetic media.","This culminates in the proposal of a research agenda to build a comprehensive, cross-cultural approach to investigate deepfakes in higher education."],"url":"http://arxiv.org/abs/2404.15601v1","category":"cs.CY"}
{"created":"2024-04-24 02:23:12","title":"Human-in-the-loop Learning for Dynamic Congestion Games","abstract":"Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Waze). Yet such platforms simply cater to selfish users' myopic interests to recommend the shortest path, and do not encourage enough users to travel and learn other paths for future others. Prior studies focus on one-shot congestion games without considering users' information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a human-in-the-loop manner. Our analysis shows that the myopic routing policy leads to severe under-exploration of stochastic paths. This results in a price of anarchy (PoA) greater than $2$, as compared to the socially optimal policy in minimizing the long-term social cost. Besides, the myopic policy fails to ensure the correct learning convergence about users' traffic hazard beliefs. To address this, we focus on informational (non-monetary) mechanisms as they are easier to implement than pricing. We first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even (\\text{PoA}=\\infty). Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group. Our CHAR successfully ensures PoA less than (\\frac{5}{4}), which cannot be further reduced by any other informational (non-monetary) mechanism. Besides the parallel network, we further extend our analysis and CHAR to more general linear path graphs with multiple intermediate nodes, and we prove that the PoA results remain unchanged. Additionally, we carry out experiments with real-world datasets to further extend our routing graphs and verify the close-to-optimal performance of our CHAR.","sentences":["Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Waze).","Yet such platforms simply cater to selfish users' myopic interests to recommend the shortest path, and do not encourage enough users to travel and learn other paths for future others.","Prior studies focus on one-shot congestion games without considering users' information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a human-in-the-loop manner.","Our analysis shows that the myopic routing policy leads to severe under-exploration of stochastic paths.","This results in a price of anarchy (PoA) greater than $2$, as compared to the socially optimal policy in minimizing the long-term social cost.","Besides, the myopic policy fails to ensure the correct learning convergence about users' traffic hazard beliefs.","To address this, we focus on informational (non-monetary) mechanisms as they are easier to implement than pricing.","We first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even (\\text{PoA}=\\infty).","Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group.","Our CHAR successfully ensures PoA less than (\\frac{5}{4}), which cannot be further reduced by any other informational (non-monetary) mechanism.","Besides the parallel network, we further extend our analysis and CHAR to more general linear path graphs with multiple intermediate nodes, and we prove that the PoA results remain unchanged.","Additionally, we carry out experiments with real-world datasets to further extend our routing graphs and verify the close-to-optimal performance of our CHAR."],"url":"http://arxiv.org/abs/2404.15599v1","category":"cs.GT"}
{"created":"2024-04-24 02:22:50","title":"Federated Learning with Only Positive Labels by Exploring Label Correlations","abstract":"Federated learning aims to collaboratively learn a model by using the data from multiple users under privacy constraints. In this paper, we study the multi-label classification problem under the federated learning setting, where trivial solution and extremely poor performance may be obtained, especially when only positive data w.r.t. a single class label are provided for each client. This issue can be addressed by adding a specially designed regularizer on the server-side. Although effective sometimes, the label correlations are simply ignored and thus sub-optimal performance may be obtained. Besides, it is expensive and unsafe to exchange user's private embeddings between server and clients frequently, especially when training model in the contrastive way. To remedy these drawbacks, we propose a novel and generic method termed Federated Averaging by exploring Label Correlations (FedALC). Specifically, FedALC estimates the label correlations in the class embedding learning for different label pairs and utilizes it to improve the model training. To further improve the safety and also reduce the communication overhead, we propose a variant to learn fixed class embedding for each client, so that the server and clients only need to exchange class embeddings once. Extensive experiments on multiple popular datasets demonstrate that our FedALC can significantly outperform existing counterparts.","sentences":["Federated learning aims to collaboratively learn a model by using the data from multiple users under privacy constraints.","In this paper, we study the multi-label classification problem under the federated learning setting, where trivial solution and extremely poor performance may be obtained, especially when only positive data w.r.t.","a single class label are provided for each client.","This issue can be addressed by adding a specially designed regularizer on the server-side.","Although effective sometimes, the label correlations are simply ignored and thus sub-optimal performance may be obtained.","Besides, it is expensive and unsafe to exchange user's private embeddings between server and clients frequently, especially when training model in the contrastive way.","To remedy these drawbacks, we propose a novel and generic method termed Federated Averaging by exploring Label Correlations (FedALC).","Specifically, FedALC estimates the label correlations in the class embedding learning for different label pairs and utilizes it to improve the model training.","To further improve the safety and also reduce the communication overhead, we propose a variant to learn fixed class embedding for each client, so that the server and clients only need to exchange class embeddings once.","Extensive experiments on multiple popular datasets demonstrate that our FedALC can significantly outperform existing counterparts."],"url":"http://arxiv.org/abs/2404.15598v1","category":"cs.LG"}
{"created":"2024-04-24 02:20:50","title":"GRSN: Gated Recurrent Spiking Neurons for POMDPs and MARL","abstract":"Spiking neural networks (SNNs) are widely applied in various fields due to their energy-efficient and fast-inference capabilities. Applying SNNs to reinforcement learning (RL) can significantly reduce the computational resource requirements for agents and improve the algorithm's performance under resource-constrained conditions. However, in current spiking reinforcement learning (SRL) algorithms, the simulation results of multiple time steps can only correspond to a single-step decision in RL. This is quite different from the real temporal dynamics in the brain and also fails to fully exploit the capacity of SNNs to process temporal data. In order to address this temporal mismatch issue and further take advantage of the inherent temporal dynamics of spiking neurons, we propose a novel temporal alignment paradigm (TAP) that leverages the single-step update of spiking neurons to accumulate historical state information in RL and introduces gated units to enhance the memory capacity of spiking neurons. Experimental results show that our method can solve partially observable Markov decision processes (POMDPs) and multi-agent cooperation problems with similar performance as recurrent neural networks (RNNs) but with about 50% power consumption.","sentences":["Spiking neural networks (SNNs) are widely applied in various fields due to their energy-efficient and fast-inference capabilities.","Applying SNNs to reinforcement learning (RL) can significantly reduce the computational resource requirements for agents and improve the algorithm's performance under resource-constrained conditions.","However, in current spiking reinforcement learning (SRL) algorithms, the simulation results of multiple time steps can only correspond to a single-step decision in RL.","This is quite different from the real temporal dynamics in the brain and also fails to fully exploit the capacity of SNNs to process temporal data.","In order to address this temporal mismatch issue and further take advantage of the inherent temporal dynamics of spiking neurons, we propose a novel temporal alignment paradigm (TAP) that leverages the single-step update of spiking neurons to accumulate historical state information in RL and introduces gated units to enhance the memory capacity of spiking neurons.","Experimental results show that our method can solve partially observable Markov decision processes (POMDPs) and multi-agent cooperation problems with similar performance as recurrent neural networks (RNNs) but with about 50% power consumption."],"url":"http://arxiv.org/abs/2404.15597v1","category":"cs.NE"}
{"created":"2024-04-24 02:16:11","title":"VulEval: Towards Repository-Level Evaluation of Software Vulnerability Detection","abstract":"Deep Learning (DL)-based methods have proven to be effective for software vulnerability detection, with a potential for substantial productivity enhancements for detecting vulnerabilities. Current methods mainly focus on detecting single functions (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice. For example, developers routinely engage with program analysis to detect vulnerabilities that span multiple functions within repositories. In addition, the widely-used benchmark datasets generally contain only intra-procedural vulnerabilities, leaving the assessment of inter-procedural vulnerability detection capabilities unexplored.   To mitigate the issues, we propose a repository-level evaluation system, named \\textbf{VulEval}, aiming at evaluating the detection performance of inter- and intra-procedural vulnerabilities simultaneously. Specifically, VulEval consists of three interconnected evaluation tasks: \\textbf{(1) Function-Level Vulnerability Detection}, aiming at detecting intra-procedural vulnerability given a code snippet; \\textbf{(2) Vulnerability-Related Dependency Prediction}, aiming at retrieving the most relevant dependencies from call graphs for providing developers with explanations about the vulnerabilities; and \\textbf{(3) Repository-Level Vulnerability Detection}, aiming at detecting inter-procedural vulnerabilities by combining with the dependencies identified in the second task. VulEval also consists of a large-scale dataset, with a total of 4,196 CVE entries, 232,239 functions, and corresponding 4,699 repository-level source code in C/C++ programming languages. Our analysis highlights the current progress and future directions for software vulnerability detection.","sentences":["Deep Learning (DL)-based methods have proven to be effective for software vulnerability detection, with a potential for substantial productivity enhancements for detecting vulnerabilities.","Current methods mainly focus on detecting single functions (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice.","For example, developers routinely engage with program analysis to detect vulnerabilities that span multiple functions within repositories.","In addition, the widely-used benchmark datasets generally contain only intra-procedural vulnerabilities, leaving the assessment of inter-procedural vulnerability detection capabilities unexplored.   ","To mitigate the issues, we propose a repository-level evaluation system, named \\textbf{VulEval}, aiming at evaluating the detection performance of inter- and intra-procedural vulnerabilities simultaneously.","Specifically, VulEval consists of three interconnected evaluation tasks: \\textbf{(1) Function-Level Vulnerability Detection}, aiming at detecting intra-procedural vulnerability given a code snippet; \\textbf{(2) Vulnerability-Related Dependency Prediction}, aiming at retrieving the most relevant dependencies from call graphs for providing developers with explanations about the vulnerabilities; and \\textbf{(3)","Repository-Level Vulnerability Detection}, aiming at detecting inter-procedural vulnerabilities by combining with the dependencies identified in the second task.","VulEval also consists of a large-scale dataset, with a total of 4,196 CVE entries, 232,239 functions, and corresponding 4,699 repository-level source code in C/C++ programming languages.","Our analysis highlights the current progress and future directions for software vulnerability detection."],"url":"http://arxiv.org/abs/2404.15596v1","category":"cs.SE"}
{"created":"2024-04-24 02:16:00","title":"Variational Deep Survival Machines: Survival Regression with Censored Outcomes","abstract":"Survival regression aims to predict the time when an event of interest will take place, typically a death or a failure. A fully parametric method [18] is proposed to estimate the survival function as a mixture of individual parametric distributions in the presence of censoring. In this paper, We present a novel method to predict the survival time by better clustering the survival data and combine primitive distributions. We propose two variants of variational auto-encoder (VAE), discrete and continuous, to generate the latent variables for clustering input covariates. The model is trained end to end by jointly optimizing the VAE loss and regression loss. Thorough experiments on dataset SUPPORT and FLCHAIN show that our method can effectively improve the clustering result and reach competitive scores with previous methods. We demonstrate the superior result of our model prediction in the long-term. Our code is available at https://github.com/qinzzz/auton-survival-785.","sentences":["Survival regression aims to predict the time when an event of interest will take place, typically a death or a failure.","A fully parametric method [18] is proposed to estimate the survival function as a mixture of individual parametric distributions in the presence of censoring.","In this paper, We present a novel method to predict the survival time by better clustering the survival data and combine primitive distributions.","We propose two variants of variational auto-encoder (VAE), discrete and continuous, to generate the latent variables for clustering input covariates.","The model is trained end to end by jointly optimizing the VAE loss and regression loss.","Thorough experiments on dataset SUPPORT and FLCHAIN show that our method can effectively improve the clustering result and reach competitive scores with previous methods.","We demonstrate the superior result of our model prediction in the long-term.","Our code is available at https://github.com/qinzzz/auton-survival-785."],"url":"http://arxiv.org/abs/2404.15595v1","category":"cs.LG"}
{"created":"2024-04-24 01:54:40","title":"ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction","abstract":"Existing datasets for attribute value extraction (AVE) predominantly focus on explicit attribute values while neglecting the implicit ones, lack product images, are often not publicly available, and lack an in-depth human inspection across diverse domains. To address these limitations, we present ImplicitAVE, the first, publicly available multimodal dataset for implicit attribute value extraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated and expanded to include implicit AVE and multimodality, resulting in a refined dataset of 68k training and 1.6k testing data across five domains. We also explore the application of multimodal large language models (MLLMs) to implicit AVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE dataset. Six recent MLLMs with eleven variants are evaluated across diverse settings, revealing that implicit value extraction remains a challenging task for MLLMs. The contributions of this work include the development and release of ImplicitAVE, and the exploration and benchmarking of various MLLMs for implicit AVE, providing valuable insights and potential future research directions. Dataset and code are available at https://github.com/HenryPengZou/ImplicitAVE","sentences":["Existing datasets for attribute value extraction (AVE) predominantly focus on explicit attribute values while neglecting the implicit ones, lack product images, are often not publicly available, and lack an in-depth human inspection across diverse domains.","To address these limitations, we present ImplicitAVE, the first, publicly available multimodal dataset for implicit attribute value extraction.","ImplicitAVE, sourced from the MAVE dataset, is carefully curated and expanded to include implicit AVE and multimodality, resulting in a refined dataset of 68k training and 1.6k testing data across five domains.","We also explore the application of multimodal large language models (MLLMs) to implicit AVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE dataset.","Six recent MLLMs with eleven variants are evaluated across diverse settings, revealing that implicit value extraction remains a challenging task for MLLMs.","The contributions of this work include the development and release of ImplicitAVE, and the exploration and benchmarking of various MLLMs for implicit AVE, providing valuable insights and potential future research directions.","Dataset and code are available at https://github.com/HenryPengZou/ImplicitAVE"],"url":"http://arxiv.org/abs/2404.15592v1","category":"cs.CV"}
{"created":"2024-04-24 01:44:09","title":"Minimal Evidence Group Identification for Claim Verification","abstract":"Claim verification in real-world settings (e.g. against a large collection of candidate evidences retrieved from the web) typically requires identifying and aggregating a complete set of evidence pieces that collectively provide full support to the claim. The problem becomes particularly challenging when there exists distinct sets of evidence that could be used to verify the claim from different perspectives. In this paper, we formally define and study the problem of identifying such minimal evidence groups (MEGs) for claim verification. We show that MEG identification can be reduced from Set Cover problem, based on entailment inference of whether a given evidence group provides full/partial support to a claim. Our proposed approach achieves 18.4% and 34.8% absolute improvements on the WiCE and SciFact datasets over LLM prompting. Finally, we demonstrate the benefits of MEGs in downstream applications such as claim generation.","sentences":["Claim verification in real-world settings (e.g. against a large collection of candidate evidences retrieved from the web) typically requires identifying and aggregating a complete set of evidence pieces that collectively provide full support to the claim.","The problem becomes particularly challenging when there exists distinct sets of evidence that could be used to verify the claim from different perspectives.","In this paper, we formally define and study the problem of identifying such minimal evidence groups (MEGs) for claim verification.","We show that MEG identification can be reduced from Set Cover problem, based on entailment inference of whether a given evidence group provides full/partial support to a claim.","Our proposed approach achieves 18.4% and 34.8% absolute improvements on the WiCE and SciFact datasets over LLM prompting.","Finally, we demonstrate the benefits of MEGs in downstream applications such as claim generation."],"url":"http://arxiv.org/abs/2404.15588v1","category":"cs.CL"}
{"created":"2024-04-24 01:43:07","title":"Security Analysis of WiFi-based Sensing Systems: Threats from Perturbation Attacks","abstract":"Deep learning technologies are pivotal in enhancing the performance of WiFi-based wireless sensing systems. However, they are inherently vulnerable to adversarial perturbation attacks, and regrettably, there is lacking serious attention to this security issue within the WiFi sensing community. In this paper, we elaborate such an attack, called WiIntruder, distinguishing itself with universality, robustness, and stealthiness, which serves as a catalyst to assess the security of existing WiFi-based sensing systems. This attack encompasses the following salient features: (1) Maximizing transferability by differentiating user-state-specific feature spaces across sensing models, leading to a universally effective perturbation attack applicable to common applications; (2) Addressing perturbation signal distortion caused by device synchronization and wireless propagation when critical parameters are optimized through a heuristic particle swarm-driven perturbation generation algorithm; and (3) Enhancing attack pattern diversity and stealthiness through random switching of perturbation surrogates generated by a generative adversarial network. Extensive experimental results confirm the practical threats of perturbation attacks to common WiFi-based services, including user authentication and respiratory monitoring.","sentences":["Deep learning technologies are pivotal in enhancing the performance of WiFi-based wireless sensing systems.","However, they are inherently vulnerable to adversarial perturbation attacks, and regrettably, there is lacking serious attention to this security issue within the WiFi sensing community.","In this paper, we elaborate such an attack, called WiIntruder, distinguishing itself with universality, robustness, and stealthiness, which serves as a catalyst to assess the security of existing WiFi-based sensing systems.","This attack encompasses the following salient features: (1) Maximizing transferability by differentiating user-state-specific feature spaces across sensing models, leading to a universally effective perturbation attack applicable to common applications; (2) Addressing perturbation signal distortion caused by device synchronization and wireless propagation when critical parameters are optimized through a heuristic particle swarm-driven perturbation generation algorithm; and (3) Enhancing attack pattern diversity and stealthiness through random switching of perturbation surrogates generated by a generative adversarial network.","Extensive experimental results confirm the practical threats of perturbation attacks to common WiFi-based services, including user authentication and respiratory monitoring."],"url":"http://arxiv.org/abs/2404.15587v1","category":"cs.CR"}
{"created":"2024-04-24 01:35:27","title":"Multi-Agent Reinforcement Learning for Energy Networks: Computational Challenges, Progress and Open Problems","abstract":"The rapidly changing architecture and functionality of electrical networks and the increasing penetration of renewable and distributed energy resources have resulted in various technological and managerial challenges. These have rendered traditional centralized energy-market paradigms insufficient due to their inability to support the dynamic and evolving nature of the network. This survey explores how multi-agent reinforcement learning (MARL) can support the decentralization and decarbonization of energy networks and mitigate the 12 associated challenges. This is achieved by specifying key computational challenges in managing energy networks, reviewing recent research progress on addressing them, and highlighting open challenges that may be addressed using MARL.","sentences":["The rapidly changing architecture and functionality of electrical networks and the increasing penetration of renewable and distributed energy resources have resulted in various technological and managerial challenges.","These have rendered traditional centralized energy-market paradigms insufficient due to their inability to support the dynamic and evolving nature of the network.","This survey explores how multi-agent reinforcement learning (MARL) can support the decentralization and decarbonization of energy networks and mitigate the 12 associated challenges.","This is achieved by specifying key computational challenges in managing energy networks, reviewing recent research progress on addressing them, and highlighting open challenges that may be addressed using MARL."],"url":"http://arxiv.org/abs/2404.15583v1","category":"cs.AI"}
{"created":"2024-04-24 01:31:23","title":"Armored Core of PKI: Remove Signing Keys for CA via Physically Unclonable Function","abstract":"The protection of CA's signing keys is one of the most crucial security concerns in PKI. However, these keys can still be exposed today by human errors or various carefully designed attacks. Traditional protections like TEE and HSM fail to eliminate this risk since they can be bypassed by skilled attackers. This dilemma motivates us to consider removing CA' signing keys and propose Armored Core, a PKI security extension applying the physically trusted binding provided by Physically Unclonable Function (PUF) for CA.   CAs in Armored Core issue PUF-based X509v3 TLS certificates, where they use PUF instead of signing algorithms to generate endorsements for domain public keys. The new transparency logging mechanism, built upon CT, will record the PUF calling behaviors of CA, ensuring the monitoring of PUF usage. We provide a formal cryptographic proof of Armored Core's main functions. We also implement it on the real-world PKI codebase. The results show that the incorporation of Armored Core into original systems do not cause any extra overhead, but instead improves computing efficiency by >4.9% and saves >20% of certificate storage.","sentences":["The protection of CA's signing keys is one of the most crucial security concerns in PKI.","However, these keys can still be exposed today by human errors or various carefully designed attacks.","Traditional protections like TEE and HSM fail to eliminate this risk since they can be bypassed by skilled attackers.","This dilemma motivates us to consider removing CA' signing keys and propose Armored Core, a PKI security extension applying the physically trusted binding provided by Physically Unclonable Function (PUF) for CA.   CAs in Armored Core issue PUF-based X509v3 TLS certificates, where they use PUF instead of signing algorithms to generate endorsements for domain public keys.","The new transparency logging mechanism, built upon CT, will record the PUF calling behaviors of CA, ensuring the monitoring of PUF usage.","We provide a formal cryptographic proof of Armored Core's main functions.","We also implement it on the real-world PKI codebase.","The results show that the incorporation of Armored Core into original systems do not cause any extra overhead, but instead improves computing efficiency by >4.9% and saves >20% of certificate storage."],"url":"http://arxiv.org/abs/2404.15582v1","category":"cs.CR"}
{"created":"2024-04-24 00:56:22","title":"Can Foundational Large Language Models Assist with Conducting Pharmaceuticals Manufacturing Investigations?","abstract":"General purpose Large Language Models (LLM) such as the Generative Pretrained Transformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted much attention in recent years. There is strong evidence that these models can perform remarkably well in various natural language processing tasks. However, how to leverage them to approach domain-specific use cases and drive value remains an open question. In this work, we focus on a specific use case, pharmaceutical manufacturing investigations, and propose that leveraging historical records of manufacturing incidents and deviations in an organization can be beneficial for addressing and closing new cases, or de-risking new manufacturing campaigns. Using a small but diverse dataset of real manufacturing deviations selected from different product lines, we evaluate and quantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2) in performing tasks related to the above goal. In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined. While our results point to the high accuracy of GPT-4 and Claude-2 in the information extraction task, we discuss cases of complex interplay between the apparent reasoning and hallucination behavior of LLMs as a risk factor. Furthermore, we show that semantic search on vector embedding of deviation descriptions can be used to identify similar records, such as those with a similar type of defect, with a high level of accuracy. We discuss further improvements to enhance the accuracy of similar record identification.","sentences":["General purpose Large Language Models (LLM) such as the Generative Pretrained Transformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted much attention in recent years.","There is strong evidence that these models can perform remarkably well in various natural language processing tasks.","However, how to leverage them to approach domain-specific use cases and drive value remains an open question.","In this work, we focus on a specific use case, pharmaceutical manufacturing investigations, and propose that leveraging historical records of manufacturing incidents and deviations in an organization can be beneficial for addressing and closing new cases, or de-risking new manufacturing campaigns.","Using a small but diverse dataset of real manufacturing deviations selected from different product lines, we evaluate and quantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2) in performing tasks related to the above goal.","In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined.","While our results point to the high accuracy of GPT-4 and Claude-2 in the information extraction task, we discuss cases of complex interplay between the apparent reasoning and hallucination behavior of LLMs as a risk factor.","Furthermore, we show that semantic search on vector embedding of deviation descriptions can be used to identify similar records, such as those with a similar type of defect, with a high level of accuracy.","We discuss further improvements to enhance the accuracy of similar record identification."],"url":"http://arxiv.org/abs/2404.15578v1","category":"cs.CL"}
{"created":"2024-04-24 00:35:40","title":"Designing AI-Enabled Games to Support Social-Emotional Learning for Children with Autism Spectrum Disorders","abstract":"Children with autism spectrum disorder (ASD) experience challenges in grasping social-emotional cues, which can result in difficulties in recognizing emotions and understanding and responding to social interactions. Social-emotional intervention is an effective method to improve emotional understanding and facial expression recognition among individuals with ASD. Existing work emphasizes the importance of personalizing interventions to meet individual needs and motivate engagement for optimal outcomes in daily settings. We design a social-emotional game for ASD children, which generates personalized stories by leveraging the current advancement of artificial intelligence. Via a co-design process with five domain experts, this work offers several design insights into developing future AI-enabled gamified systems for families with autistic children. We also propose a fine-tuned AI model and a dataset of social stories for different basic emotions.","sentences":["Children with autism spectrum disorder (ASD) experience challenges in grasping social-emotional cues, which can result in difficulties in recognizing emotions and understanding and responding to social interactions.","Social-emotional intervention is an effective method to improve emotional understanding and facial expression recognition among individuals with ASD.","Existing work emphasizes the importance of personalizing interventions to meet individual needs and motivate engagement for optimal outcomes in daily settings.","We design a social-emotional game for ASD children, which generates personalized stories by leveraging the current advancement of artificial intelligence.","Via a co-design process with five domain experts, this work offers several design insights into developing future AI-enabled gamified systems for families with autistic children.","We also propose a fine-tuned AI model and a dataset of social stories for different basic emotions."],"url":"http://arxiv.org/abs/2404.15576v1","category":"cs.HC"}
{"created":"2024-04-24 00:24:03","title":"Retrieval Head Mechanistically Explains Long-Context Factuality","abstract":"Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.","sentences":["Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context.","This paper aims to address this question.","Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads.","We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval.","(3) intrinsic: retrieval heads already exist in models pretrained with short context.","When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval.","(4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed.","The rest of the retrieval heads are activated in different contexts.","(5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability.","We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context.","Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads.","These observations collectively explain which internal part of the model seeks information from the input tokens.","We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache."],"url":"http://arxiv.org/abs/2404.15574v1","category":"cs.CL"}
{"created":"2024-04-23 23:57:56","title":"Self-gravitating solutions in Yang-Mills-Chern-Simons theory coupled to 3D massive gravity","abstract":"We study self-gravitating solutions of 3-dimensional massive gravity coupled to Yang-Mills-Chern-Simons gauge theory. Among these, there is a family of asymptotically Warped-Anti de Sitter black holes that come to generalize previous solutions found in the literature and studied in the context of WAdS$_3$/CFT$_2$. We also present self-gravitating solutions to the 3-dimensional Einstein-Yang-Mills theory, as well as other self-gravitating solutions in the presence of higher-curvature terms.","sentences":["We study self-gravitating solutions of 3-dimensional massive gravity coupled to Yang-Mills-Chern-Simons gauge theory.","Among these, there is a family of asymptotically Warped-Anti de Sitter black holes that come to generalize previous solutions found in the literature and studied in the context of WAdS$_3$/CFT$_2$.","We also present self-gravitating solutions to the 3-dimensional Einstein-Yang-Mills theory, as well as other self-gravitating solutions in the presence of higher-curvature terms."],"url":"http://arxiv.org/abs/2404.15569v1","category":"hep-th"}
{"created":"2024-04-23 23:45:56","title":"Dynamics of a small quantum system open to a bath with thermostat","abstract":"We investigate dynamics of a small quantum system open to a bath with thermostat. We introduce another bath, called super bath, weakly coupled with the bath to provide it with thermostat, which has either the Lindblad or Redfield type. We treat the interaction between the system and bath via a rigorous perturbation theory. Due to the thermostat, the bath behaves dissipative and stochastic, for which the usual Born-Markov assumption is not needed. We consider a specific example of a harmonic oscillator system, and a photonic bath in a large container, and a super bath of the Caldeira-Legget oscillators distributed on the inner surface of the container. We use the $P$-representation for the total harmonic system. We derive the reduced time-evolution equation for the system by explicitly finding the correlation between the system and bath beyond the product state, that was not obtainable in the previous theory for the system and bath isolated from environment, and marginalizing bath degrees of freedom. Remarkably, the associated dynamic equation for the system density matrix is of the same form as the Redfield master equation with different coefficients depending on thermostat used. We find steady state does not depend on thermostat, but time-dependent state does, that agrees with common expectation. We expect to apply our theory to general systems. Unlike the usual quantum master equations, our reduced dynamics allows investigation for time-dependent protocols and non-equilibrium quantum stochastic dynamics will be investigated in future.","sentences":["We investigate dynamics of a small quantum system open to a bath with thermostat.","We introduce another bath, called super bath, weakly coupled with the bath to provide it with thermostat, which has either the Lindblad or Redfield type.","We treat the interaction between the system and bath via a rigorous perturbation theory.","Due to the thermostat, the bath behaves dissipative and stochastic, for which the usual Born-Markov assumption is not needed.","We consider a specific example of a harmonic oscillator system, and a photonic bath in a large container, and a super bath of the Caldeira-Legget oscillators distributed on the inner surface of the container.","We use the $P$-representation for the total harmonic system.","We derive the reduced time-evolution equation for the system by explicitly finding the correlation between the system and bath beyond the product state, that was not obtainable in the previous theory for the system and bath isolated from environment, and marginalizing bath degrees of freedom.","Remarkably, the associated dynamic equation for the system density matrix is of the same form as the Redfield master equation with different coefficients depending on thermostat used.","We find steady state does not depend on thermostat, but time-dependent state does, that agrees with common expectation.","We expect to apply our theory to general systems.","Unlike the usual quantum master equations, our reduced dynamics allows investigation for time-dependent protocols and non-equilibrium quantum stochastic dynamics will be investigated in future."],"url":"http://arxiv.org/abs/2404.15568v1","category":"quant-ph"}
{"created":"2024-04-23 23:36:01","title":"Cohomology of BiHom-Associative Trialgebras","abstract":"The paper concerns the cohomology of (multiplicative) BiHom-associative trialgebras. We first detail the correspondence between central extensions and second cohomology. This is followed by a general cohomology theory that unifies those of BiHom-associative algebras and associative trialgebras. Finally, we introduce one-parameter formal deformations and classify generalized $\\alpha\\beta$-derivations of 3-dimensional BiHom-associative trialgebras.","sentences":["The paper concerns the cohomology of (multiplicative) BiHom-associative trialgebras.","We first detail the correspondence between central extensions and second cohomology.","This is followed by a general cohomology theory that unifies those of BiHom-associative algebras and associative trialgebras.","Finally, we introduce one-parameter formal deformations and classify generalized $\\alpha\\beta$-derivations of 3-dimensional BiHom-associative trialgebras."],"url":"http://arxiv.org/abs/2404.15567v1","category":"math.RA"}
{"created":"2024-04-23 23:26:02","title":"Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency","abstract":"This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction. We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. We propose two propositions for these two objectives and prove the necessity of evaluating them. We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model. Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.","sentences":["This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations.","We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction.","We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations.","We propose two propositions for these two objectives and prove the necessity of evaluating them.","We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model.","Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude."],"url":"http://arxiv.org/abs/2404.15564v1","category":"cs.CV"}
{"created":"2024-04-23 23:24:39","title":"Highly Squeezed States in Ring Resonators: Beyond the Undepleted Pump Approximation","abstract":"We present a multimode theory of squeezed state generation in resonant systems valid for arbitrary pump power and including pump depletion. The Hamiltonian is written in terms of asymptotic-in and -out fields from scattering theory, capable of describing a general interaction. As an example we consider the lossy generation of a highly squeezed state by an effective second-order interaction in a silicon nitride ring resonator point-coupled to a waveguide. We calculate the photon number, Schmidt number, and the second-order correlation function of the generated state in the waveguide. The treatment we present provides a path forward to study the deterministic generation of non-Gaussian states in resonant systems.","sentences":["We present a multimode theory of squeezed state generation in resonant systems valid for arbitrary pump power and including pump depletion.","The Hamiltonian is written in terms of asymptotic-in and -out fields from scattering theory, capable of describing a general interaction.","As an example we consider the lossy generation of a highly squeezed state by an effective second-order interaction in a silicon nitride ring resonator point-coupled to a waveguide.","We calculate the photon number, Schmidt number, and the second-order correlation function of the generated state in the waveguide.","The treatment we present provides a path forward to study the deterministic generation of non-Gaussian states in resonant systems."],"url":"http://arxiv.org/abs/2404.15563v1","category":"quant-ph"}
{"created":"2024-04-23 23:22:36","title":"Polyconvex neural network models of thermoelasticity","abstract":"Machine-learning function representations such as neural networks have proven to be excellent constructs for constitutive modeling due to their flexibility to represent highly nonlinear data and their ability to incorporate constitutive constraints, which also allows them to generalize well to unseen data. In this work, we extend a polyconvex hyperelastic neural network framework to thermo-hyperelasticity by specifying the thermodynamic and material theoretic requirements for an expansion of the Helmholtz free energy expressed in terms of deformation invariants and temperature. Different formulations which a priori ensure polyconvexity with respect to deformation and concavity with respect to temperature are proposed and discussed. The physics-augmented neural networks are furthermore calibrated with a recently proposed sparsification algorithm that not only aims to fit the training data but also penalizes the number of active parameters, which prevents overfitting in the low data regime and promotes generalization. The performance of the proposed framework is demonstrated on synthetic data, which illustrate the expected thermomechanical phenomena, and existing temperature-dependent uniaxial tension and tension-torsion experimental datasets.","sentences":["Machine-learning function representations such as neural networks have proven to be excellent constructs for constitutive modeling due to their flexibility to represent highly nonlinear data and their ability to incorporate constitutive constraints, which also allows them to generalize well to unseen data.","In this work, we extend a polyconvex hyperelastic neural network framework to thermo-hyperelasticity by specifying the thermodynamic and material theoretic requirements for an expansion of the Helmholtz free energy expressed in terms of deformation invariants and temperature.","Different formulations which a priori ensure polyconvexity with respect to deformation and concavity with respect to temperature are proposed and discussed.","The physics-augmented neural networks are furthermore calibrated with a recently proposed sparsification algorithm that not only aims to fit the training data but also penalizes the number of active parameters, which prevents overfitting in the low data regime and promotes generalization.","The performance of the proposed framework is demonstrated on synthetic data, which illustrate the expected thermomechanical phenomena, and existing temperature-dependent uniaxial tension and tension-torsion experimental datasets."],"url":"http://arxiv.org/abs/2404.15562v1","category":"cond-mat.soft"}
{"created":"2024-04-23 23:21:54","title":"Casimir energy on the sphere and 6D CFT trace anomaly","abstract":"We elucidate the dependence of the Casimir energy on the trace anomaly coefficients for a six-dimensional CFT on $R\\times S^5$. This extends the universal dependence on the central charge in 2D and the relation by Cappelli and Coste in 4D, unveiling the role of the trivial total derivatives in the anomaly that renders the Casimir energy scheme dependent. We obtain   $$E_o=-\\frac{15}{8}\\,a_6 -\\frac{5}{12}\\,\\left(g_5+\\frac{1}{4}\\,g_7+\\frac{1}{2}\\,g_8-10\\, g_9+g_{10}\\right),$$ with $a_6$ being the type A central charge and the $g$'s, the coefficients of five out of six terms that form a basis for trivial total derivatives. The derivation is based on the Polyakov formulas (conformal primitive) resulting from the integration of the trace anomaly.   Alternatively, on a 6D conformally flat background the above basis is redundant and one can simplify further to get, in terms of the Schouten scalar $J$ and the Schouten tensor $V$, Branson's basis for trivial total derivatives $\\nabla^2\\nabla^2 J$, $\\nabla^2J^2$ and $\\nabla^2|V|^2+2\\nabla\\cdot(V\\cdot\\nabla\\,J)$ with coefficients $\\gamma_1, \\gamma_2$ and $\\gamma_3$, respectively, \\begin{equation} \\nonumber   E_o=-\\frac{15}{8}a_6-\\frac{1}{24}\\left(\\gamma_1-\\gamma_2 -\\frac{1}{8}\\gamma_3\\right)~. \\end{equation}","sentences":["We elucidate the dependence of the Casimir energy on the trace anomaly coefficients for a six-dimensional CFT on $R\\times S^5$.","This extends the universal dependence on the central charge in 2D and the relation by Cappelli and Coste in 4D, unveiling the role of the trivial total derivatives in the anomaly that renders the Casimir energy scheme dependent.","We obtain   $$E_o=-\\frac{15}{8}\\,a_6 -\\frac{5}{12}\\,\\left(g_5+\\frac{1}{4}\\,g_7+\\frac{1}{2}\\,g_8-10\\, g_9+g_{10}\\right),$$ with $a_6$ being the type A central charge and the $g$'s, the coefficients of five out of six terms that form a basis for trivial total derivatives.","The derivation is based on the Polyakov formulas (conformal primitive) resulting from the integration of the trace anomaly.   ","Alternatively, on a 6D conformally flat background the above basis is redundant and one can simplify further to get, in terms of the Schouten scalar $J$ and the Schouten tensor $V$, Branson's basis for trivial total derivatives $\\nabla^2\\nabla^2 J$, $\\nabla^2J^2$ and $\\nabla^2|V|^2+2\\nabla\\cdot(V\\cdot\\nabla\\,J)$ with coefficients $\\gamma_1, \\gamma_2$ and $\\gamma_3$, respectively, \\begin{equation} \\nonumber   E_o=-\\frac{15}{8}a_6-\\frac{1}{24}\\left(\\gamma_1-\\gamma_2 -\\frac{1}{8}\\gamma_3\\right)~. \\end{equation}"],"url":"http://arxiv.org/abs/2404.15561v1","category":"hep-th"}
{"created":"2024-04-23 23:15:05","title":"Low-Bandwidth Matrix Multiplication: Faster Algorithms and More General Forms of Sparsity","abstract":"In prior work, Gupta et al. (SPAA 2022) presented a distributed algorithm for multiplying sparse $n \\times n$ matrices, using $n$ computers. They assumed that the input matrices are uniformly sparse -- there are at most $d$ non-zeros in each row and column -- and the task is to compute a uniformly sparse part of the product matrix. Initially each computer knows one row of each input matrix, and eventually each computer needs to know one row of the product matrix. In each communication round each computer can send and receive one $O(\\log n)$-bit message. Their algorithm solves this task in $O(d^{1.907})$ rounds, while the trivial bound is $O(d^2)$. We improve on the prior work in two dimensions: First, we show that we can solve the same task faster, in only $O(d^{1.832})$ rounds. Second, we explore what happens when matrices are not uniformly sparse. We consider the following alternative notions of sparsity: row-sparse matrices (at most $d$ non-zeros per row), column-sparse matrices, matrices with bounded degeneracy (we can recursively delete a row or column with at most $d$ non-zeros), average-sparse matrices (at most $dn$ non-zeros in total), and general matrices. We show that we can still compute $X = AB$ in $O(d^{1.832})$ rounds even if one of the three matrices ($A$, $B$, or $X$) is average-sparse instead of uniformly sparse. We present algorithms that handle a much broader range of sparsity in $O(d^2 + \\log n)$ rounds, and present conditional hardness results that put limits on further improvements and generalizations.","sentences":["In prior work, Gupta et al. (SPAA 2022) presented a distributed algorithm for multiplying sparse $n \\times n$ matrices, using $n$ computers.","They assumed that the input matrices are uniformly sparse -- there are at most $d$ non-zeros in each row and column -- and the task is to compute a uniformly sparse part of the product matrix.","Initially each computer knows one row of each input matrix, and eventually each computer needs to know one row of the product matrix.","In each communication round each computer can send and receive one $O(\\log n)$-bit message.","Their algorithm solves this task in $O(d^{1.907})$ rounds, while the trivial bound is $O(d^2)$. We improve on the prior work in two dimensions:","First, we show that we can solve the same task faster, in only $O(d^{1.832})$ rounds.","Second, we explore what happens when matrices are not uniformly sparse.","We consider the following alternative notions of sparsity: row-sparse matrices (at most $d$ non-zeros per row), column-sparse matrices, matrices with bounded degeneracy (we can recursively delete a row or column with at most $d$ non-zeros), average-sparse matrices (at most $dn$ non-zeros in total), and general matrices.","We show that we can still compute $X = AB$ in $O(d^{1.832})$ rounds even if one of the three matrices ($A$, $B$, or $X$) is average-sparse instead of uniformly sparse.","We present algorithms that handle a much broader range of sparsity in $O(d^2 + \\log n)$ rounds, and present conditional hardness results that put limits on further improvements and generalizations."],"url":"http://arxiv.org/abs/2404.15559v1","category":"cs.DC"}
{"created":"2024-04-23 22:54:51","title":"Cross-Temporal Spectrogram Autoencoder (CTSAE): Unsupervised Dimensionality Reduction for Clustering Gravitational Wave Glitches","abstract":"The advancement of The Laser Interferometer Gravitational-Wave Observatory (LIGO) has significantly enhanced the feasibility and reliability of gravitational wave detection. However, LIGO's high sensitivity makes it susceptible to transient noises known as glitches, which necessitate effective differentiation from real gravitational wave signals. Traditional approaches predominantly employ fully supervised or semi-supervised algorithms for the task of glitch classification and clustering. In the future task of identifying and classifying glitches across main and auxiliary channels, it is impractical to build a dataset with manually labeled ground-truth. In addition, the patterns of glitches can vary with time, generating new glitches without manual labels. In response to this challenge, we introduce the Cross-Temporal Spectrogram Autoencoder (CTSAE), a pioneering unsupervised method for the dimensionality reduction and clustering of gravitational wave glitches. CTSAE integrates a novel four-branch autoencoder with a hybrid of Convolutional Neural Networks (CNN) and Vision Transformers (ViT). To further extract features across multi-branches, we introduce a novel multi-branch fusion method using the CLS (Class) token. Our model, trained and evaluated on the GravitySpy O3 dataset on the main channel, demonstrates superior performance in clustering tasks when compared to state-of-the-art semi-supervised learning methods. To the best of our knowledge, CTSAE represents the first unsupervised approach tailored specifically for clustering LIGO data, marking a significant step forward in the field of gravitational wave research. The code of this paper is available at https://github.com/Zod-L/CTSAE","sentences":["The advancement of The Laser Interferometer Gravitational-Wave Observatory (LIGO) has significantly enhanced the feasibility and reliability of gravitational wave detection.","However, LIGO's high sensitivity makes it susceptible to transient noises known as glitches, which necessitate effective differentiation from real gravitational wave signals.","Traditional approaches predominantly employ fully supervised or semi-supervised algorithms for the task of glitch classification and clustering.","In the future task of identifying and classifying glitches across main and auxiliary channels, it is impractical to build a dataset with manually labeled ground-truth.","In addition, the patterns of glitches can vary with time, generating new glitches without manual labels.","In response to this challenge, we introduce the Cross-Temporal Spectrogram Autoencoder (CTSAE), a pioneering unsupervised method for the dimensionality reduction and clustering of gravitational wave glitches.","CTSAE integrates a novel four-branch autoencoder with a hybrid of Convolutional Neural Networks (CNN) and Vision Transformers (ViT).","To further extract features across multi-branches, we introduce a novel multi-branch fusion method using the CLS (Class) token.","Our model, trained and evaluated on the GravitySpy O3 dataset on the main channel, demonstrates superior performance in clustering tasks when compared to state-of-the-art semi-supervised learning methods.","To the best of our knowledge, CTSAE represents the first unsupervised approach tailored specifically for clustering LIGO data, marking a significant step forward in the field of gravitational wave research.","The code of this paper is available at https://github.com/Zod-L/CTSAE"],"url":"http://arxiv.org/abs/2404.15552v1","category":"cs.CV"}
{"created":"2024-04-23 22:38:01","title":"Fractional maximal operators on weighted variable Lebesgue spaces over the spaces of homogeneous type","abstract":"Let $(X,d,\\mu)$ is a space of homogeneous type, we establish a new class of fractional-type variable weights $A_{p(\\cdot), q(\\cdot)}(X)$. Then, we get the new weighted strong-type and weak-type characterizations for fractional maximal operators $M_\\eta$ on weighted variable Lebesgue spaces over $(X,d,\\mu)$. This study generalizes the results by Cruz-Uribe-Fiorenza-Neugebauer (2012), Bernardis-Dalmasso-Pradolini (2014), Cruz-Uribe-Shukla (2018), and Cruz-Uribe-Cummings (2022).","sentences":["Let $(X,d,\\mu)$ is a space of homogeneous type, we establish a new class of fractional-type variable weights $A_{p(\\cdot), q(\\cdot)}(X)$. Then, we get the new weighted strong-type and weak-type characterizations for fractional maximal operators $M_\\eta$ on weighted variable Lebesgue spaces over $(X,d,\\mu)$.","This study generalizes the results by Cruz-Uribe-Fiorenza-Neugebauer (2012), Bernardis-Dalmasso-Pradolini (2014), Cruz-Uribe-Shukla (2018), and Cruz-Uribe-Cummings (2022)."],"url":"http://arxiv.org/abs/2404.15550v1","category":"math.CA"}
{"created":"2024-04-23 22:33:19","title":"PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models","abstract":"Clinical trial matching is the task of identifying trials for which patients may be potentially eligible. Typically, this task is labor-intensive and requires detailed verification of patient electronic health records (EHRs) against the stringent inclusion and exclusion criteria of clinical trials. This process is manual, time-intensive, and challenging to scale up, resulting in many patients missing out on potential therapeutic options. Recent advancements in Large Language Models (LLMs) have made automating patient-trial matching possible, as shown in multiple concurrent research studies. However, the current approaches are confined to constrained, often synthetic datasets that do not adequately mirror the complexities encountered in real-world medical data. In this study, we present the first, end-to-end large-scale empirical evaluation of clinical trial matching using real-world EHRs. Our study showcases the capability of LLMs to accurately match patients with appropriate clinical trials. We perform experiments with proprietary LLMs, including GPT-4 and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show that OncoLLM, despite its significantly smaller size, not only outperforms GPT-3.5 but also matches the performance of qualified medical doctors. All experiments were carried out on real-world EHRs that include clinical notes and available clinical trials from a single cancer center in the United States.","sentences":["Clinical trial matching is the task of identifying trials for which patients may be potentially eligible.","Typically, this task is labor-intensive and requires detailed verification of patient electronic health records (EHRs) against the stringent inclusion and exclusion criteria of clinical trials.","This process is manual, time-intensive, and challenging to scale up, resulting in many patients missing out on potential therapeutic options.","Recent advancements in Large Language Models (LLMs) have made automating patient-trial matching possible, as shown in multiple concurrent research studies.","However, the current approaches are confined to constrained, often synthetic datasets that do not adequately mirror the complexities encountered in real-world medical data.","In this study, we present the first, end-to-end large-scale empirical evaluation of clinical trial matching using real-world EHRs.","Our study showcases the capability of LLMs to accurately match patients with appropriate clinical trials.","We perform experiments with proprietary LLMs, including GPT-4 and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show that OncoLLM, despite its significantly smaller size, not only outperforms GPT-3.5 but also matches the performance of qualified medical doctors.","All experiments were carried out on real-world EHRs that include clinical notes and available clinical trials from a single cancer center in the United States."],"url":"http://arxiv.org/abs/2404.15549v1","category":"cs.CL"}
{"created":"2024-04-23 22:33:05","title":"Quantum metrology of rotations with mixed spin states","abstract":"The efficiency of a quantum metrology protocol can be considerably reduced by the interaction of a quantum system with its environment, resulting in a loss of purity and, consequently, a mixed state for the probing system. In this paper we examine the potential of mixed spin-$j$ states to achieve sensitivity comparable, and even equal, to that of pure states in the measurement of infinitesimal rotations about arbitrary axes. We introduce the concept of mixed optimal quantum rotosensors based on a maximization of the Fisher quantum information and show that it is related to the notion of anticoherence of spin states and its generalization to subspaces. We present several examples of anticoherent subspaces and their associated mixed optimal quantum rotosensors. We also show that the latter maximize negativity for specific bipartitions, reaching the same maximum value as pure states. These results elucidate the interplay between quantum metrology of rotations, anticoherence and entanglement in the framework of mixed spin states.","sentences":["The efficiency of a quantum metrology protocol can be considerably reduced by the interaction of a quantum system with its environment, resulting in a loss of purity and, consequently, a mixed state for the probing system.","In this paper we examine the potential of mixed spin-$j$ states to achieve sensitivity comparable, and even equal, to that of pure states in the measurement of infinitesimal rotations about arbitrary axes.","We introduce the concept of mixed optimal quantum rotosensors based on a maximization of the Fisher quantum information and show that it is related to the notion of anticoherence of spin states and its generalization to subspaces.","We present several examples of anticoherent subspaces and their associated mixed optimal quantum rotosensors.","We also show that the latter maximize negativity for specific bipartitions, reaching the same maximum value as pure states.","These results elucidate the interplay between quantum metrology of rotations, anticoherence and entanglement in the framework of mixed spin states."],"url":"http://arxiv.org/abs/2404.15548v1","category":"quant-ph"}
{"created":"2024-04-23 22:15:48","title":"Magnetic ordering and dynamics in monolayers and bilayers of chromium trihalides: atomistic simulations approach","abstract":"We analyze magnetic properties of monolayers and bilayers of chromium trihalides, CrI$_3$, in two different stacking configurations: AA and rhombohedral ones. Our main focus is on the corresponding Curie temperatures, hysteresis curves, equilibrium spin structures, and spin wave excitations. To obtain all these magnetic characteristic, we employ the atomistic spin dynamics and Monte Carlo simulation techniques. The model Hamiltonian includes isotropic exchange coupling, magnetic anisotropy, and Dzyaloshinskii-Moriya interaction. Though the latter is relatively weak in CrI$_3$, we consider a more general case assuming also an enhancement of Dzyaloshinskii-Moriya interaction in the corresponding Janus structures and by external electric fields. An important issue of the analysis is the correlation between hysteresis curves and spin configurations in the system, as well as formation of the skyrmion textures.","sentences":["We analyze magnetic properties of monolayers and bilayers of chromium trihalides, CrI$_3$, in two different stacking configurations: AA and rhombohedral ones.","Our main focus is on the corresponding Curie temperatures, hysteresis curves, equilibrium spin structures, and spin wave excitations.","To obtain all these magnetic characteristic, we employ the atomistic spin dynamics and Monte Carlo simulation techniques.","The model Hamiltonian includes isotropic exchange coupling, magnetic anisotropy, and Dzyaloshinskii-Moriya interaction.","Though the latter is relatively weak in CrI$_3$, we consider a more general case assuming also an enhancement of Dzyaloshinskii-Moriya interaction in the corresponding Janus structures and by external electric fields.","An important issue of the analysis is the correlation between hysteresis curves and spin configurations in the system, as well as formation of the skyrmion textures."],"url":"http://arxiv.org/abs/2404.15543v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 22:12:43","title":"Twisted MoSe2 Homobilayer Behaving as a Heterobilayer","abstract":"Heterostructures (HSs) formed by the transition-metal dichalcogenides (TMDCs) materials have shown great promise in next-generation opto/electronic and photonic applications. An artificially twisted HS, allows us to manipulate the optical, and electronic properties. With this work, we introduce the understanding of the complex energy transfer (ET) process governed by the dipolar interaction in a twisted molybdenum diselenide (MoSe2) homobilayer without any charge-blocking interlayer. We fabricated an unconventional homobilayer (i.e., HS) with a large twist angle by combining the chemical vapor deposition (CVD) and mechanical exfoliation (Exf.) techniques to fully exploit the lattice parameters mismatch and indirect/direct (CVD/Exf.) bandgap nature. This effectively weaken the charge transfer (CT) process and allows the ET process to take over the carrier recombination channels. We utilize a series of optical and electron spectroscopy techniques complementing by the density functional theory calculations, to describe a massive photoluminescence enhancement from the HS area due to an efficient ET process. Our results show that the electronically decoupled MoSe2 homobilayer is coupled by the ET process, mimicking a 'true' heterobilayer nature.","sentences":["Heterostructures (HSs) formed by the transition-metal dichalcogenides (TMDCs) materials have shown great promise in next-generation opto/electronic and photonic applications.","An artificially twisted HS, allows us to manipulate the optical, and electronic properties.","With this work, we introduce the understanding of the complex energy transfer (ET) process governed by the dipolar interaction in a twisted molybdenum diselenide (MoSe2) homobilayer without any charge-blocking interlayer.","We fabricated an unconventional homobilayer (i.e., HS) with a large twist angle by combining the chemical vapor deposition (CVD) and mechanical exfoliation (Exf.)","techniques to fully exploit the lattice parameters mismatch and indirect/direct (CVD/Exf.)","bandgap nature.","This effectively weaken the charge transfer (CT) process and allows the ET process to take over the carrier recombination channels.","We utilize a series of optical and electron spectroscopy techniques complementing by the density functional theory calculations, to describe a massive photoluminescence enhancement from the HS area due to an efficient ET process.","Our results show that the electronically decoupled MoSe2 homobilayer is coupled by the ET process, mimicking a 'true' heterobilayer nature."],"url":"http://arxiv.org/abs/2404.15542v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 22:05:27","title":"A Unified Framework for Total Variation Regularized Optimization in Fluid Dynamics and Related Physical Systems","abstract":"An optimization framework is presented for minimizing the energy functional developed around a generalized equation governing physical systems such as fluid dynamics, particle transport, phase transition, and other related systems. The convexity of the energy functional is investigated to derive the necessary conditions for a smooth and global optimum solution. Furthermore, the Total Variation (TV) regularization term is introduced to gain insights into the solution space and convergence analysis of convection-dominated problems.   We demonstrate the practical application of our method by applying it to some selected examples such as the Boltzmann, Navier-Stokes, and Maxwell equations.","sentences":["An optimization framework is presented for minimizing the energy functional developed around a generalized equation governing physical systems such as fluid dynamics, particle transport, phase transition, and other related systems.","The convexity of the energy functional is investigated to derive the necessary conditions for a smooth and global optimum solution.","Furthermore, the Total Variation (TV) regularization term is introduced to gain insights into the solution space and convergence analysis of convection-dominated problems.   ","We demonstrate the practical application of our method by applying it to some selected examples such as the Boltzmann, Navier-Stokes, and Maxwell equations."],"url":"http://arxiv.org/abs/2404.15540v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 21:57:14","title":"DreamCraft: Text-Guided Generation of Functional 3D Environments in Minecraft","abstract":"Procedural Content Generation (PCG) algorithms enable the automatic generation of complex and diverse artifacts. However, they don't provide high-level control over the generated content and typically require domain expertise. In contrast, text-to-3D methods allow users to specify desired characteristics in natural language, offering a high amount of flexibility and expressivity. But unlike PCG, such approaches cannot guarantee functionality, which is crucial for certain applications like game design. In this paper, we present a method for generating functional 3D artifacts from free-form text prompts in the open-world game Minecraft. Our method, DreamCraft, trains quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when viewed in-game, match given text descriptions. We find that DreamCraft produces more aligned in-game artifacts than a baseline that post-processes the output of an unconstrained NeRF. Thanks to the quantized representation of the environment, functional constraints can be integrated using specialized loss terms. We show how this can be leveraged to generate 3D structures that match a target distribution or obey certain adjacency rules over the block types. DreamCraft inherits a high degree of expressivity and controllability from the NeRF, while still being able to incorporate functional constraints through domain-specific objectives.","sentences":["Procedural Content Generation (PCG) algorithms enable the automatic generation of complex and diverse artifacts.","However, they don't provide high-level control over the generated content and typically require domain expertise.","In contrast, text-to-3D methods allow users to specify desired characteristics in natural language, offering a high amount of flexibility and expressivity.","But unlike PCG, such approaches cannot guarantee functionality, which is crucial for certain applications like game design.","In this paper, we present a method for generating functional 3D artifacts from free-form text prompts in the open-world game Minecraft.","Our method, DreamCraft, trains quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when viewed in-game, match given text descriptions.","We find that DreamCraft produces more aligned in-game artifacts than a baseline that post-processes the output of an unconstrained NeRF.","Thanks to the quantized representation of the environment, functional constraints can be integrated using specialized loss terms.","We show how this can be leveraged to generate 3D structures that match a target distribution or obey certain adjacency rules over the block types.","DreamCraft inherits a high degree of expressivity and controllability from the NeRF, while still being able to incorporate functional constraints through domain-specific objectives."],"url":"http://arxiv.org/abs/2404.15538v1","category":"cs.GR"}
{"created":"2024-04-23 21:54:26","title":"Constraining dark energy equations of state in $F(R,T)$ gravity","abstract":"In this paper, we examine the acceleration of the Universe's expansion in $F(R,T)$ gravity, where $R$ denotes the Ricci scalar and $T$ the trace of energy-momentum tensor. Indeed, the unknown nature of the source controlling this acceleration in general relativity leads scientists to investigate its properties by means of some alternative theories to general relativity. Our study is restricted to the particular case where $F(R,T)=R+2\\kappa^2 \\lambda T$ , with $\\lambda$ being a constant. We use a Bayesian analysis of current observational datasets, including the type Ia supernovae constitution compilation and $H(z)$ measurements, to constrain free parameters of the model. To parametrize dark energy, we consider two well known equations of state. We find the best fit values for each model by running a Markov chain Monte Carlo technic. The best fit parameters are used to compare both models to $\\Lambda$CDM by means of the Akaike information criterion and the Bayesian information criterion. We show that the Universe underwent recently a transition from a deceleration to an acceleration for both models. Furthermore, the data shows a phantom nature of the equation of state for both models.","sentences":["In this paper, we examine the acceleration of the Universe's expansion in $F(R,T)$ gravity, where $R$ denotes the Ricci scalar and $T$ the trace of energy-momentum tensor.","Indeed, the unknown nature of the source controlling this acceleration in general relativity leads scientists to investigate its properties by means of some alternative theories to general relativity.","Our study is restricted to the particular case where $F(R,T)=R+2\\kappa^2","\\lambda T$ , with $\\lambda$ being a constant.","We use a Bayesian analysis of current observational datasets, including the type Ia supernovae constitution compilation and $H(z)$ measurements, to constrain free parameters of the model.","To parametrize dark energy, we consider two well known equations of state.","We find the best fit values for each model by running a Markov chain Monte Carlo technic.","The best fit parameters are used to compare both models to $\\Lambda$CDM by means of the Akaike information criterion and the Bayesian information criterion.","We show that the Universe underwent recently a transition from a deceleration to an acceleration for both models.","Furthermore, the data shows a phantom nature of the equation of state for both models."],"url":"http://arxiv.org/abs/2404.15537v1","category":"gr-qc"}
{"created":"2024-04-23 21:42:27","title":"Constraints on the reheating phase after Higgs inflation in the hybrid metric-Palatini approach","abstract":"In this paper, we study the post-inflationary era called reheating stage. For this purpose, we consider a model in which the inflaton is non-minimally coupled to the curvature within the hybrid metric-Palatini approach. Furthermore, to investigate the consistency of our results with the observational data, we relate reheating parameters to those of inflation model. By taking into consideration the Higgs potential $V(\\phi)=\\lambda/4 \\phi^4$; we derive the necessary quantities needed to obtain the reheating duration and the reheating temperature. Moreover, we plot reheating e-folds and temperature as a function of the spectral index, respectively. We consider three cases depending on the coupling constant $\\xi$. In addition, we use some specific values of the effective equation of state $\\omega$, which is presumed to remain relatively constant within the range of $-\\frac{1}{3} \\leq \\omega < \\frac{1}{3}$. We find that for $\\xi=10^{-4.1}$ our results are in agreement with the recent Planck data as the reheating instant is corresponding to the central value of the spectral index and to a maximum temperature required by the scale of baryogenesis models.","sentences":["In this paper, we study the post-inflationary era called reheating stage.","For this purpose, we consider a model in which the inflaton is non-minimally coupled to the curvature within the hybrid metric-Palatini approach.","Furthermore, to investigate the consistency of our results with the observational data, we relate reheating parameters to those of inflation model.","By taking into consideration the Higgs potential $V(\\phi)=\\lambda/4 \\phi^4$; we derive the necessary quantities needed to obtain the reheating duration and the reheating temperature.","Moreover, we plot reheating e-folds and temperature as a function of the spectral index, respectively.","We consider three cases depending on the coupling constant $\\xi$. In addition, we use some specific values of the effective equation of state $\\omega$, which is presumed to remain relatively constant within the range of $-\\frac{1}{3} \\leq \\omega < \\frac{1}{3}$.","We find that for $\\xi=10^{-4.1}$ our results are in agreement with the recent Planck data as the reheating instant is corresponding to the central value of the spectral index and to a maximum temperature required by the scale of baryogenesis models."],"url":"http://arxiv.org/abs/2404.15535v1","category":"gr-qc"}
{"created":"2024-04-23 21:40:11","title":"Designing, simulating, and performing the 100-AV field test for the CIRCLES consortium: Methodology and Implementation of the Largest mobile traffic control experiment to date","abstract":"Previous controlled experiments on single-lane ring roads have shown that a single partially autonomous vehicle (AV) can effectively mitigate traffic waves. This naturally prompts the question of how these findings can be generalized to field operational, high-density traffic conditions. To address this question, the Congestion Impacts Reduction via CAV-in-the-loop Lagrangian Energy Smoothing (CIRCLES) Consortium conducted MegaVanderTest (MVT), a live traffic control experiment involving 100 vehicles near Nashville, TN, USA. This article is a tutorial for developing analytical and simulation-based tools essential for designing and executing a live traffic control experiment like the MVT. It presents an overview of the proposed roadmap and various procedures used in designing, monitoring, and conducting the MVT, which is the largest mobile traffic control experiment at the time. The design process is aimed at evaluating the impact of the CIRCLES AVs on surrounding traffic. The article discusses the agent-based traffic simulation framework created for this evaluation. A novel methodological framework is introduced to calibrate this microsimulation, aiming to accurately capture traffic dynamics and assess the impact of adding 100 vehicles to existing traffic. The calibration model's effectiveness is verified using data from a six-mile section of Nashville's I-24 highway. The results indicate that the proposed model establishes an effective feedback loop between the optimizer and the simulator, thereby calibrating flow and speed with different spatiotemporal characteristics to minimize the error between simulated and real-world data. Finally, We simulate AVs in multiple scenarios to assess their effect on traffic congestion. This evaluation validates the AV routes, thereby contributing to the execution of a safe and successful live traffic control experiment via AVs.","sentences":["Previous controlled experiments on single-lane ring roads have shown that a single partially autonomous vehicle (AV) can effectively mitigate traffic waves.","This naturally prompts the question of how these findings can be generalized to field operational, high-density traffic conditions.","To address this question, the Congestion Impacts Reduction via CAV-in-the-loop","Lagrangian Energy Smoothing (CIRCLES) Consortium conducted MegaVanderTest (MVT), a live traffic control experiment involving 100 vehicles near Nashville, TN, USA.","This article is a tutorial for developing analytical and simulation-based tools essential for designing and executing a live traffic control experiment like the MVT.","It presents an overview of the proposed roadmap and various procedures used in designing, monitoring, and conducting the MVT, which is the largest mobile traffic control experiment at the time.","The design process is aimed at evaluating the impact of the CIRCLES AVs on surrounding traffic.","The article discusses the agent-based traffic simulation framework created for this evaluation.","A novel methodological framework is introduced to calibrate this microsimulation, aiming to accurately capture traffic dynamics and assess the impact of adding 100 vehicles to existing traffic.","The calibration model's effectiveness is verified using data from a six-mile section of Nashville's I-24 highway.","The results indicate that the proposed model establishes an effective feedback loop between the optimizer and the simulator, thereby calibrating flow and speed with different spatiotemporal characteristics to minimize the error between simulated and real-world data.","Finally, We simulate AVs in multiple scenarios to assess their effect on traffic congestion.","This evaluation validates the AV routes, thereby contributing to the execution of a safe and successful live traffic control experiment via AVs."],"url":"http://arxiv.org/abs/2404.15533v1","category":"eess.SY"}
{"created":"2024-04-23 21:37:22","title":"BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis","abstract":"This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI's potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.","sentences":["This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System.","This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time.","It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers.","The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes.","It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging.","These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints.","The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios.","This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts.","Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals.","BattelAgent illustrates AI's potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society."],"url":"http://arxiv.org/abs/2404.15532v1","category":"cs.HC"}
{"created":"2024-04-23 21:18:36","title":"3+1 non-linear evolution of Ricci-coupled scalar-Gauss-Bonnet gravity","abstract":"Scalar-Gauss-Bonnet (sGB) gravity with an additional coupling between the scalar field and the Ricci scalar exhibits very interesting properties related to black hole stability, evasion of binary pulsar constraints, and general relativity as a late-time cosmology attractor. Furthermore, it was demonstrated that a spherically symmetric collapse is well-posed for a wide range of parameters. In the present paper we examine further the well-posedness through $3+1$ evolution of static and rotating black holes. We show that the evolution is indeed hyperbolic if the weak coupling condition is not severely violated. The loss of hyperbolicity is caused by the gravitational sector of the physical modes, thus it is not an artifact of the gauge choice. We further seek to compare the Ricci-coupled sGB theory against the standard sGB gravity with additional terms in the Gauss-Bonnet coupling. We find strong similarities in terms of well-posedness, but we also point out important differences in the stationary solutions. As a byproduct, we show strong indications that stationary near-extremal scalarized black holes exist within the Ricci-coupled sGB theory, where the scalar field is sourced by the spacetime curvature rather than the black hole spin.","sentences":["Scalar-Gauss-Bonnet (sGB) gravity with an additional coupling between the scalar field and the Ricci scalar exhibits very interesting properties related to black hole stability, evasion of binary pulsar constraints, and general relativity as a late-time cosmology attractor.","Furthermore, it was demonstrated that a spherically symmetric collapse is well-posed for a wide range of parameters.","In the present paper we examine further the well-posedness through $3+1$ evolution of static and rotating black holes.","We show that the evolution is indeed hyperbolic if the weak coupling condition is not severely violated.","The loss of hyperbolicity is caused by the gravitational sector of the physical modes, thus it is not an artifact of the gauge choice.","We further seek to compare the Ricci-coupled sGB theory against the standard sGB gravity with additional terms in the Gauss-Bonnet coupling.","We find strong similarities in terms of well-posedness, but we also point out important differences in the stationary solutions.","As a byproduct, we show strong indications that stationary near-extremal scalarized black holes exist within the Ricci-coupled sGB theory, where the scalar field is sourced by the spacetime curvature rather than the black hole spin."],"url":"http://arxiv.org/abs/2404.15526v1","category":"gr-qc"}
{"created":"2024-04-23 21:08:49","title":"Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models","abstract":"Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really \"reason\" over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.","sentences":["Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks.","But, can they really \"reason\" over the natural language?","This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied.","However, the crucial skill pertaining to 'logical reasoning' has remained underexplored.","Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic.","Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics.","To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule.","We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting.","Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations.","Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion.","We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs.","Data and code are available at https://github.com/Mihir3009/LogicBench."],"url":"http://arxiv.org/abs/2404.15522v1","category":"cs.CL"}
{"created":"2024-04-23 21:05:47","title":"Are early-type galaxies quenched by present-day environment? A study of dwarfs in the Fornax Cluster","abstract":"Galaxies undergo processes throughout their lifetimes that ultimately lead to the expulsion of the gas and the cessation of the star-forming activity. This phenomenon commonly known as quenching, can be caused by environmental processes. For this we use the results of Romero-G\\'omez et al. (2024), who analyzed galaxies from the SAMI-Fornax and ATLAS$^{3D}$ survey. Using t$_{90}$ as an approximation for the quenching time and comparing it with the infall time derived from phase-space models, we determine the probability of the quenching being produced by the local environment of galaxies. Our results reveal a relation between galaxy mass and quenching probability. Down to M$_{\\star}$ $\\sim$10$^{10}$ M$_{\\odot}$, galaxies exhibit almost zero probability of quenching, suggesting their independence from environmental effects. As we move into the mass regime of dwarf galaxies, the probability increases with decreasing mass, highlighting their sensitivity to environmental quenching. For the dwarfs, 10$^{7}$ - 10$^{9}$ M$_{\\odot}$, 36$\\pm$9% of our observational data are consistent with this hypothesis, challenging the idea that the present-day cluster, Fornax, is the primary driver of quenching in the low mass galaxies. We compare these results with cosmological simulations, selecting galaxies under similar conditions to our observational sample. The simulated sample shows lower quenching probabilities as we move down in mass, only 5$\\pm$1% of galaxies meet the quenching criteria. This discrepancy between observations and simulations underlines that modelling quenching is still in its infancy. In general, the number of observed galaxies quenched by their environment is lower than expected, which suggests that pre-processing plays a larger role in galaxy evolution. Ultimately, our results highlight the need for higher-quality simulations and refinement of galaxy formation and evolution models.","sentences":["Galaxies undergo processes throughout their lifetimes that ultimately lead to the expulsion of the gas and the cessation of the star-forming activity.","This phenomenon commonly known as quenching, can be caused by environmental processes.","For this we use the results of Romero-G\\'omez et al. (2024), who analyzed galaxies from the SAMI-Fornax and ATLAS$^{3D}$ survey.","Using t$_{90}$ as an approximation for the quenching time and comparing it with the infall time derived from phase-space models, we determine the probability of the quenching being produced by the local environment of galaxies.","Our results reveal a relation between galaxy mass and quenching probability.","Down to M$_{\\star}$ $\\sim$10$^{10}$ M$_{\\odot}$, galaxies exhibit almost zero probability of quenching, suggesting their independence from environmental effects.","As we move into the mass regime of dwarf galaxies, the probability increases with decreasing mass, highlighting their sensitivity to environmental quenching.","For the dwarfs, 10$^{7}$ - 10$^{9}$ M$_{\\odot}$, 36$\\pm$9% of our observational data are consistent with this hypothesis, challenging the idea that the present-day cluster, Fornax, is the primary driver of quenching in the low mass galaxies.","We compare these results with cosmological simulations, selecting galaxies under similar conditions to our observational sample.","The simulated sample shows lower quenching probabilities as we move down in mass, only 5$\\pm$1% of galaxies meet the quenching criteria.","This discrepancy between observations and simulations underlines that modelling quenching is still in its infancy.","In general, the number of observed galaxies quenched by their environment is lower than expected, which suggests that pre-processing plays a larger role in galaxy evolution.","Ultimately, our results highlight the need for higher-quality simulations and refinement of galaxy formation and evolution models."],"url":"http://arxiv.org/abs/2404.15519v1","category":"astro-ph.GA"}
{"created":"2024-04-23 21:02:58","title":"An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models","abstract":"In traditional statistical learning, data points are usually assumed to be independently and identically distributed (i.i.d.) following an unknown probability distribution. This paper presents a contrasting viewpoint, perceiving data points as interconnected and employing a Markov reward process (MRP) for data modeling. We reformulate the typical supervised learning as an on-policy policy evaluation problem within reinforcement learning (RL), introducing a generalized temporal difference (TD) learning algorithm as a resolution. Theoretically, our analysis draws connections between the solutions of linear TD learning and ordinary least squares (OLS). We also show that under specific conditions, particularly when noises are correlated, the TD's solution proves to be a more effective estimator than OLS. Furthermore, we establish the convergence of our generalized TD algorithms under linear function approximation. Empirical studies verify our theoretical results, examine the vital design of our TD algorithm and show practical utility across various datasets, encompassing tasks such as regression and image classification with deep learning.","sentences":["In traditional statistical learning, data points are usually assumed to be independently and identically distributed (i.i.d.)","following an unknown probability distribution.","This paper presents a contrasting viewpoint, perceiving data points as interconnected and employing a Markov reward process (MRP) for data modeling.","We reformulate the typical supervised learning as an on-policy policy evaluation problem within reinforcement learning (RL), introducing a generalized temporal difference (TD) learning algorithm as a resolution.","Theoretically, our analysis draws connections between the solutions of linear TD learning and ordinary least squares (OLS).","We also show that under specific conditions, particularly when noises are correlated, the TD's solution proves to be a more effective estimator than OLS.","Furthermore, we establish the convergence of our generalized TD algorithms under linear function approximation.","Empirical studies verify our theoretical results, examine the vital design of our TD algorithm and show practical utility across various datasets, encompassing tasks such as regression and image classification with deep learning."],"url":"http://arxiv.org/abs/2404.15518v1","category":"cs.LG"}
{"created":"2024-04-23 21:00:22","title":"Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) is a task that retrieves images similar to a query, based on a provided textual modification. Current techniques rely on supervised learning for CIR models using labeled triplets of the reference image, text, target image. These specific triplets are not as commonly available as simple image-text pairs, limiting the widespread use of CIR and its scalability. On the other hand, zero-shot CIR can be relatively easily trained with image-caption pairs without considering the image-to-image relation, but this approach tends to yield lower accuracy. We propose a new semi-supervised CIR approach where we search for a reference and its related target images in auxiliary data and learn our large language model-based Visual Delta Generator (VDG) to generate text describing the visual difference (i.e., visual delta) between the two. VDG, equipped with fluent language knowledge and being model agnostic, can generate pseudo triplets to boost the performance of CIR models. Our approach significantly improves the existing supervised learning approaches and achieves state-of-the-art results on the CIR benchmarks.","sentences":["Composed Image Retrieval (CIR) is a task that retrieves images similar to a query, based on a provided textual modification.","Current techniques rely on supervised learning for CIR models using labeled triplets of the reference image, text, target image.","These specific triplets are not as commonly available as simple image-text pairs, limiting the widespread use of CIR and its scalability.","On the other hand, zero-shot CIR can be relatively easily trained with image-caption pairs without considering the image-to-image relation, but this approach tends to yield lower accuracy.","We propose a new semi-supervised CIR approach where we search for a reference and its related target images in auxiliary data and learn our large language model-based Visual Delta Generator (VDG) to generate text describing the visual difference (i.e., visual delta) between the two.","VDG, equipped with fluent language knowledge and being model agnostic, can generate pseudo triplets to boost the performance of CIR models.","Our approach significantly improves the existing supervised learning approaches and achieves state-of-the-art results on the CIR benchmarks."],"url":"http://arxiv.org/abs/2404.15516v1","category":"cs.CV"}
{"created":"2024-04-23 20:59:03","title":"ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic Executors in Large Language Models","abstract":"Theory of Mind (ToM) refers to the ability of individuals to attribute mental states to others. While Large Language Models (LLMs) have shown some promise with ToM ability, they still struggle with complex ToM reasoning. Our approach leverages an external symbolic executor, specifically the SMCDEL model checker, and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach, an LLM is first fine-tuned through pairs of natural language and symbolic formulation representation of ToM problems and is then instructed to generate the symbolic formulation with a one-shot in-context example. The generated symbolic formulation is then executed by the SMCDEL model checker to perform transparent and verifiable ToM reasoning and give the final result. We demonstrate that our approach, ToM-LM, shows a significant improvement over all the constructed baselines. Our study proposes a novel view about externalizing a particular component of ToM reasoning, mainly reasoning about beliefs, and suggests generalizing it to other aspects of ToM reasoning.","sentences":["Theory of Mind (ToM) refers to the ability of individuals to attribute mental states to others.","While Large Language Models (LLMs) have shown some promise with ToM ability, they still struggle with complex ToM reasoning.","Our approach leverages an external symbolic executor, specifically the SMCDEL model checker, and fine-tuning to improve the ToM reasoning ability of LLMs.","In our approach, an LLM is first fine-tuned through pairs of natural language and symbolic formulation representation of ToM problems and is then instructed to generate the symbolic formulation with a one-shot in-context example.","The generated symbolic formulation is then executed by the SMCDEL model checker to perform transparent and verifiable ToM reasoning and give the final result.","We demonstrate that our approach, ToM-LM, shows a significant improvement over all the constructed baselines.","Our study proposes a novel view about externalizing a particular component of ToM reasoning, mainly reasoning about beliefs, and suggests generalizing it to other aspects of ToM reasoning."],"url":"http://arxiv.org/abs/2404.15515v1","category":"cs.CL"}
{"created":"2024-04-23 20:37:26","title":"FedGreen: Carbon-aware Federated Learning with Model Size Adaptation","abstract":"Federated learning (FL) provides a promising collaborative framework to build a model from distributed clients, and this work investigates the carbon emission of the FL process. Cloud and edge servers hosting FL clients may exhibit diverse carbon footprints influenced by their geographical locations with varying power sources, offering opportunities to reduce carbon emissions by training local models with adaptive computations and communications. In this paper, we propose FedGreen, a carbon-aware FL approach to efficiently train models by adopting adaptive model sizes shared with clients based on their carbon profiles and locations using ordered dropout as a model compression technique. We theoretically analyze the trade-offs between the produced carbon emissions and the convergence accuracy, considering the carbon intensity discrepancy across countries to choose the parameters optimally. Empirical studies show that FedGreen can substantially reduce the carbon footprints of FL compared to the state-of-the-art while maintaining competitive model accuracy.","sentences":["Federated learning (FL) provides a promising collaborative framework to build a model from distributed clients, and this work investigates the carbon emission of the FL process.","Cloud and edge servers hosting FL clients may exhibit diverse carbon footprints influenced by their geographical locations with varying power sources, offering opportunities to reduce carbon emissions by training local models with adaptive computations and communications.","In this paper, we propose FedGreen, a carbon-aware FL approach to efficiently train models by adopting adaptive model sizes shared with clients based on their carbon profiles and locations using ordered dropout as a model compression technique.","We theoretically analyze the trade-offs between the produced carbon emissions and the convergence accuracy, considering the carbon intensity discrepancy across countries to choose the parameters optimally.","Empirical studies show that FedGreen can substantially reduce the carbon footprints of FL compared to the state-of-the-art while maintaining competitive model accuracy."],"url":"http://arxiv.org/abs/2404.15503v1","category":"cs.LG"}
{"created":"2024-04-23 20:26:07","title":"Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information","abstract":"This paper presents Killkan, the first dataset for automatic speech recognition (ASR) in the Kichwa language, an indigenous language of Ecuador. Kichwa is an extremely low-resource endangered language, and there have been no resources before Killkan for Kichwa to be incorporated in applications of natural language processing. The dataset contains approximately 4 hours of audio with transcription, translation into Spanish, and morphosyntactic annotation in the format of Universal Dependencies. The audio data was retrieved from a publicly available radio program in Kichwa. This paper also provides corpus-linguistic analyses of the dataset with a special focus on the agglutinative morphology of Kichwa and frequent code-switching with Spanish. The experiments show that the dataset makes it possible to develop the first ASR system for Kichwa with reliable quality despite its small dataset size. This dataset, the ASR model, and the code used to develop them will be publicly available. Thus, our study positively showcases resource building and its applications for low-resource languages and their community.","sentences":["This paper presents Killkan, the first dataset for automatic speech recognition (ASR) in the Kichwa language, an indigenous language of Ecuador.","Kichwa is an extremely low-resource endangered language, and there have been no resources before Killkan for Kichwa to be incorporated in applications of natural language processing.","The dataset contains approximately 4 hours of audio with transcription, translation into Spanish, and morphosyntactic annotation in the format of Universal Dependencies.","The audio data was retrieved from a publicly available radio program in Kichwa.","This paper also provides corpus-linguistic analyses of the dataset with a special focus on the agglutinative morphology of Kichwa and frequent code-switching with Spanish.","The experiments show that the dataset makes it possible to develop the first ASR system for Kichwa with reliable quality despite its small dataset size.","This dataset, the ASR model, and the code used to develop them will be publicly available.","Thus, our study positively showcases resource building and its applications for low-resource languages and their community."],"url":"http://arxiv.org/abs/2404.15501v1","category":"cs.CL"}
{"created":"2024-04-23 20:23:37","title":"GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots","abstract":"Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions. However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios. In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms. We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent's proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions. By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images. By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts.","sentences":["Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions.","However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios.","In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms.","We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent's proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions.","By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images.","By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts."],"url":"http://arxiv.org/abs/2404.15500v1","category":"cs.AI"}
{"created":"2024-04-23 20:22:10","title":"Timing resolution performance of Timepix4 bump-bonded assemblies","abstract":"The timing performance of the Timepix4 application-specific integrated circuit (ASIC) bump-bonded to a $100\\;\\mu\\textrm{m}$ thick n-on-p silicon sensor is presented. A picosecond pulsed infrared laser was used to generate electron-hole pairs in the silicon bulk in a repeatable fashion, controlling the amount, position and time of the stimulated charge signal. The timing resolution for a single pixel has been measured to $107\\;\\textrm{ps}$ r.m.s. for laser-stimulated signals in the silicon sensor bulk. Considering multi-pixel clusters, the measured timing resolution reached $33\\;\\textrm{ps}$ r.m.s. exploiting oversampling of the timing information over several pixels.","sentences":["The timing performance of the Timepix4 application-specific integrated circuit (ASIC) bump-bonded to a $100\\;\\mu\\textrm{m}$ thick n-on-p silicon sensor is presented.","A picosecond pulsed infrared laser was used to generate electron-hole pairs in the silicon bulk in a repeatable fashion, controlling the amount, position and time of the stimulated charge signal.","The timing resolution for a single pixel has been measured to $107\\;\\textrm{ps}$ r.m.s.","for laser-stimulated signals in the silicon sensor bulk.","Considering multi-pixel clusters, the measured timing resolution reached $33\\;\\textrm{ps}$ r.m.s.","exploiting oversampling of the timing information over several pixels."],"url":"http://arxiv.org/abs/2404.15499v1","category":"physics.ins-det"}
{"created":"2024-04-23 20:19:17","title":"The Algebras for Automatic Relations","abstract":"We introduce \"synchronous algebras\", an algebraic structure tailored to recognize automatic relations (a.k.a. synchronous relations, or regular relations). They are the equivalent of monoids for regular languages, however they conceptually differ in two points: first, they are typed and second, they are equipped with a dependency relation expressing constraints between elements of different types.   We first show that the three pillars of algebraic language theory hold for synchronous algebras: (a) any relation admits a syntactic synchronous algebra recognizing it, and moreover, the relation is synchronous if, and only if, its minimal algebra is finite; (b) classes of synchronous relations with desirable closure properties (called \"pseudovarieties\") correspond to pseudovarieties of synchronous algebras; and (c) pseudovarieties of synchronous algebras are exactly the classes of synchronous algebras defined by a generalization of profinite equations called \"profinite dependencies\".   Building on these results, we show how algebraic characterizations of pseudovarieties of regular languages can be lifted to the pseudovarieties of synchronous relations that they induce. A typical (and running) example of such a pseudovariety is the class of \"group relations\", defined as the relations recognized by finite-state synchronous permutation automata.","sentences":["We introduce \"synchronous algebras\", an algebraic structure tailored to recognize automatic relations (a.k.a. synchronous relations, or regular relations).","They are the equivalent of monoids for regular languages, however they conceptually differ in two points: first, they are typed and second, they are equipped with a dependency relation expressing constraints between elements of different types.   ","We first show that the three pillars of algebraic language theory hold for synchronous algebras: (a) any relation admits a syntactic synchronous algebra recognizing it, and moreover, the relation is synchronous if, and only if, its minimal algebra is finite; (b) classes of synchronous relations with desirable closure properties (called \"pseudovarieties\") correspond to pseudovarieties of synchronous algebras; and (c) pseudovarieties of synchronous algebras are exactly the classes of synchronous algebras defined by a generalization of profinite equations called \"profinite dependencies\".   ","Building on these results, we show how algebraic characterizations of pseudovarieties of regular languages can be lifted to the pseudovarieties of synchronous relations that they induce.","A typical (and running) example of such a pseudovariety is the class of \"group relations\", defined as the relations recognized by finite-state synchronous permutation automata."],"url":"http://arxiv.org/abs/2404.15496v1","category":"cs.FL"}
{"created":"2024-04-23 20:06:56","title":"Multi-scale Intervention Planning based on Generative Design","abstract":"The scarcity of green spaces, in urban environments, consists a critical challenge. There are multiple adverse effects, impacting the health and well-being of the citizens. Small scale interventions, e.g. pocket parks, is a viable solution, but comes with multiple constraints, involving the design and implementation over a specific area. In this study, we harness the capabilities of generative AI for multi-scale intervention planning, focusing on nature based solutions. By leveraging image-to-image and image inpainting algorithms, we propose a methodology to address the green space deficit in urban areas. Focusing on two alleys in Thessaloniki, where greenery is lacking, we demonstrate the efficacy of our approach in visualizing NBS interventions. Our findings underscore the transformative potential of emerging technologies in shaping the future of urban intervention planning processes.","sentences":["The scarcity of green spaces, in urban environments, consists a critical challenge.","There are multiple adverse effects, impacting the health and well-being of the citizens.","Small scale interventions, e.g. pocket parks, is a viable solution, but comes with multiple constraints, involving the design and implementation over a specific area.","In this study, we harness the capabilities of generative AI for multi-scale intervention planning, focusing on nature based solutions.","By leveraging image-to-image and image inpainting algorithms, we propose a methodology to address the green space deficit in urban areas.","Focusing on two alleys in Thessaloniki, where greenery is lacking, we demonstrate the efficacy of our approach in visualizing NBS interventions.","Our findings underscore the transformative potential of emerging technologies in shaping the future of urban intervention planning processes."],"url":"http://arxiv.org/abs/2404.15492v1","category":"cs.AI"}
{"created":"2024-04-23 20:04:23","title":"Evaluating Advanced Nuclear Fission Technologies for Future Decarbonized Power Grids","abstract":"In the coming decades, the United States aims to undergo an energy transition away from fossil fuels and toward a fully decarbonized power grid. There are many pathways that the US could pursue toward this objective, each of which relies on different types of generating technologies to provide clean and reliable electricity. One potential contributor to these pathways is advanced nuclear fission, which encompasses various innovative nuclear reactor designs. However, little is known about how cost-competitive these reactors would be compared to other technologies, or about which aspects of their designs offer the most value to a decarbonized power grid. We employ an electricity system optimization model and a case study of a decarbonized U.S. Eastern Interconnection circa 2050 to generate initial indicators of future economic value for advanced reactors and the sensitivity of future value to various design parameters, the availability of competing technologies, and the underlying policy environment. These results can inform long-term cost targets and guide near-term innovation priorities, investments, and reactor design decisions. We find that advanced reactors should cost \\$5.1-\\$6.6/W to gain an initial market share (assuming 30 year asset life and 3.5-6.5% real WACC), while those that include thermal storage in their designs can cost up to \\$5.5-\\$7.0/W (not including cost of storage). Since the marginal value of advanced fission reactors declines as market penetration increases, break-even costs fall around 19% at 100 GW of cumulative capacity and around 40% at 300 GW. Additionally, policies that provide investment tax credits for nuclear energy create the most favorable environment for advanced nuclear fission. Stakeholders and investors should consider these findings when deciding which technologies to consider for decarbonizing the US power grid.","sentences":["In the coming decades, the United States aims to undergo an energy transition away from fossil fuels and toward a fully decarbonized power grid.","There are many pathways that the US could pursue toward this objective, each of which relies on different types of generating technologies to provide clean and reliable electricity.","One potential contributor to these pathways is advanced nuclear fission, which encompasses various innovative nuclear reactor designs.","However, little is known about how cost-competitive these reactors would be compared to other technologies, or about which aspects of their designs offer the most value to a decarbonized power grid.","We employ an electricity system optimization model and a case study of a decarbonized U.S. Eastern Interconnection circa 2050 to generate initial indicators of future economic value for advanced reactors and the sensitivity of future value to various design parameters, the availability of competing technologies, and the underlying policy environment.","These results can inform long-term cost targets and guide near-term innovation priorities, investments, and reactor design decisions.","We find that advanced reactors should cost \\$5.1-\\$6.6/W to gain an initial market share (assuming 30 year asset life and 3.5-6.5% real WACC), while those that include thermal storage in their designs can cost up to \\$5.5-\\$7.0/W (not including cost of storage).","Since the marginal value of advanced fission reactors declines as market penetration increases, break-even costs fall around 19% at 100 GW of cumulative capacity and around 40% at 300 GW.","Additionally, policies that provide investment tax credits for nuclear energy create the most favorable environment for advanced nuclear fission.","Stakeholders and investors should consider these findings when deciding which technologies to consider for decarbonizing the US power grid."],"url":"http://arxiv.org/abs/2404.15491v1","category":"physics.soc-ph"}
{"created":"2024-04-23 20:03:45","title":"Fock space of local fields of the discrete GFF and its scaling limit bosonic CFT","abstract":"To connect conformal field theories (CFT) to probabilistic lattice models, recent works [HKV22, Ada23] have introduced a novel definition of local fields of the lattice models. Local fields in this picture are probabilistically concrete: they are built from random variables in the model. The key insight is that discrete complex analysis ideas allow to equip the space of local fields with the main structure of a CFT: a representation of the Virasoro algebra.   In this article, for the first time, we fully analyze the structure of the space of local fields of a lattice model as a representation, and use this to establish a one-to-one correspondence between the local fields of a lattice model and those of a conformal field theory. The CFT we consider is probabilistically realized in terms of the gradient of the Gaussian Free Field (GFF). Its space of local fields is just a bosonic Fock space for two chiral symmetry algebras. The corresponding lattice model is the discrete Gaussian Free Field. Our first main result is that the space of local fields of polynomials in the gradient of the discrete GFF is isomorphic to the Fock space. Notably, local fields in this setup make sense with both Dirichlet and Neumann boundary conditions. Our second main result is that with the appropriate renormalization, correlation functions of local fields of the discrete GFF converge in the scaling limit to the correlation functions of the CFT. The renormalization needed is, conceptually correctly, according to the eigenvalue of the Virasoro generator $L_0 + \\bar{L}_0$ on the local field.","sentences":["To connect conformal field theories (CFT) to probabilistic lattice models, recent works [HKV22, Ada23] have introduced a novel definition of local fields of the lattice models.","Local fields in this picture are probabilistically concrete: they are built from random variables in the model.","The key insight is that discrete complex analysis ideas allow to equip the space of local fields with the main structure of a CFT: a representation of the Virasoro algebra.   ","In this article, for the first time, we fully analyze the structure of the space of local fields of a lattice model as a representation, and use this to establish a one-to-one correspondence between the local fields of a lattice model and those of a conformal field theory.","The CFT we consider is probabilistically realized in terms of the gradient of the Gaussian Free Field (GFF).","Its space of local fields is just a bosonic Fock space for two chiral symmetry algebras.","The corresponding lattice model is the discrete Gaussian Free Field.","Our first main result is that the space of local fields of polynomials in the gradient of the discrete GFF is isomorphic to the Fock space.","Notably, local fields in this setup make sense with both Dirichlet and Neumann boundary conditions.","Our second main result is that with the appropriate renormalization, correlation functions of local fields of the discrete GFF converge in the scaling limit to the correlation functions of the CFT.","The renormalization needed is, conceptually correctly, according to the eigenvalue of the Virasoro generator $L_0 + \\bar{L}_0$ on the local field."],"url":"http://arxiv.org/abs/2404.15490v1","category":"math-ph"}
{"created":"2024-04-23 20:00:37","title":"IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection & Correction Task On the Shoulders of Medical Agents","abstract":"In natural language processing applied to the clinical domain, utilizing large language models has emerged as a promising avenue for error detection and correction on clinical notes, a knowledge-intensive task for which annotated data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a suite of four LLM-based medical agents. The MedReAct agent initiates the process by observing, analyzing, and taking action, generating trajectories to guide the search to target a potential error in the clinical notes. Subsequently, the MedEval agent employs five evaluators to assess the targeted error and the proposed correction. In cases where MedReAct's actions prove insufficient, the MedReFlex agent intervenes, engaging in reflective analysis and proposing alternative strategies. Finally, the MedFinalParser agent formats the final output, preserving the original style while ensuring the integrity of the error correction process. One core component of our method is our RAG pipeline based on our ClinicalCorp corpora. Among other well-known sources containing clinical guidelines and information, we preprocess and release the open-source MedWiki dataset for clinical RAG application. Our results demonstrate the central role of our RAG approach with ClinicalCorp leveraged through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the MEDIQA-CORR 2024 final leaderboard.","sentences":["In natural language processing applied to the clinical domain, utilizing large language models has emerged as a promising avenue for error detection and correction on clinical notes, a knowledge-intensive task for which annotated data is scarce.","This paper presents MedReAct'N'MedReFlex, which leverages a suite of four LLM-based medical agents.","The MedReAct agent initiates the process by observing, analyzing, and taking action, generating trajectories to guide the search to target a potential error in the clinical notes.","Subsequently, the MedEval agent employs five evaluators to assess the targeted error and the proposed correction.","In cases where MedReAct's actions prove insufficient, the MedReFlex agent intervenes, engaging in reflective analysis and proposing alternative strategies.","Finally, the MedFinalParser agent formats the final output, preserving the original style while ensuring the integrity of the error correction process.","One core component of our method is our RAG pipeline based on our ClinicalCorp corpora.","Among other well-known sources containing clinical guidelines and information, we preprocess and release the open-source MedWiki dataset for clinical RAG application.","Our results demonstrate the central role of our RAG approach with ClinicalCorp leveraged through the MedReAct'N'MedReFlex framework.","It achieved the ninth rank on the MEDIQA-CORR 2024 final leaderboard."],"url":"http://arxiv.org/abs/2404.15488v1","category":"cs.CL"}
{"created":"2024-04-23 19:55:18","title":"Large Language Models Spot Phishing Emails with Surprising Accuracy: A Comparative Analysis of Performance","abstract":"Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world. By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security. These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources. By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information. Building on this pervasive issue within modern technology, this paper aims to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of \"419 Scam\" emails. The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria. The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails.","sentences":["Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world.","By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security.","These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources.","By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information.","Building on this pervasive issue within modern technology, this paper aims to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of \"419 Scam\" emails.","The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria.","The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails."],"url":"http://arxiv.org/abs/2404.15485v1","category":"cs.CL"}
{"created":"2024-04-23 19:46:20","title":"Transverse Quantum Superfluids","abstract":"Even when ideal solids are insulating, their states with crystallographic defects may have superfluid properties. It became clear recently that edge dislocations in $^4$He featuring a combination of microscopic quantum roughness and superfluidity of their cores may represent a new paradigmatic class of quasi-one-dimensional superfluids. The new state of matter, termed transverse quantum fluid (TQF), is found in a variety of physical setups. The key ingredient defining the class of TQF systems is infinite compressibility, which is responsible for all other unusual properties such as the quadratic spectrum (or even the absence) of normal modes, irrelevance of the Landau criterion, off-diagonal long-range order at $T = 0$, and the exponential dependence of the phase slip probability on the inverse flow velocity. From a conceptual point of view, the TQF state is a striking demonstration of the conditional character of many dogmas associated with superfluidity, including the necessity of elementary excitations, in general, and the ones obeying Landau criterion in particular.","sentences":["Even when ideal solids are insulating, their states with crystallographic defects may have superfluid properties.","It became clear recently that edge dislocations in $^4$He featuring a combination of microscopic quantum roughness and superfluidity of their cores may represent a new paradigmatic class of quasi-one-dimensional superfluids.","The new state of matter, termed transverse quantum fluid (TQF), is found in a variety of physical setups.","The key ingredient defining the class of TQF systems is infinite compressibility, which is responsible for all other unusual properties such as the quadratic spectrum (or even the absence) of normal modes, irrelevance of the Landau criterion, off-diagonal long-range order at $T = 0$, and the exponential dependence of the phase slip probability on the inverse flow velocity.","From a conceptual point of view, the TQF state is a striking demonstration of the conditional character of many dogmas associated with superfluidity, including the necessity of elementary excitations, in general, and the ones obeying Landau criterion in particular."],"url":"http://arxiv.org/abs/2404.15480v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-23 19:29:03","title":"Chaotic almost minimal actions","abstract":"Motivated by Furstenberg's Theorem on sets in the circle invariant under multiplication by a non-lacunary semigroup, we define a general class of dynamical systems possessing similar topological dynamical properties. We call such systems chaotic almost minimal, reflecting that these systems are chaotic, but in some sense are close to minimal. We study properties of the acting group needed to admit such an action, and show the existence of a chaotic almost minimal $\\mathbb{Z}$-action. We show there exists chaotic almost minimal $\\mathbb{Z}^{d}$-actions which support multiple distinct nonatomic ergodic probability measures.","sentences":["Motivated by Furstenberg's Theorem on sets in the circle invariant under multiplication by a non-lacunary semigroup, we define a general class of dynamical systems possessing similar topological dynamical properties.","We call such systems chaotic almost minimal, reflecting that these systems are chaotic, but in some sense are close to minimal.","We study properties of the acting group needed to admit such an action, and show the existence of a chaotic almost minimal $\\mathbb{Z}$-action.","We show there exists chaotic almost minimal $\\mathbb{Z}^{d}$-actions which support multiple distinct nonatomic ergodic probability measures."],"url":"http://arxiv.org/abs/2404.15476v1","category":"math.DS"}
{"created":"2024-04-23 19:19:18","title":"NMBEnet: Efficient Near-field mmWave Beam Training for Multiuser OFDM Systems Using Sub-6 GHz Pilots","abstract":"Combining millimetre-wave (mmWave) communications with an extremely large-scale antenna array (ELAA) presents a promising avenue for meeting the spectral efficiency demands of the future sixth generation (6G) mobile communications. However, beam training for mmWave ELAA systems is challenged by excessive pilot overheads as well as insufficient accuracy, as the huge near-field codebook has to be accounted for. In this paper, inspired by the similarity between far-field sub-6 GHz channels and near-field mmWave channels, we propose to leverage sub-6 GHz uplink pilot signals to directly estimate the optimal near-field mmWave codeword, which aims to reduce pilot overhead and bypass the channel estimation. Moreover, we adopt deep learning to perform this dual mapping function, i.e., sub-6 GHz to mmWave, far-field to near-field, and a novel neural network structure called NMBEnet is designed to enhance the precision of beam training. Specifically, when considering the orthogonal frequency division multiplexing (OFDM) communication scenarios with high user density, correlations arise both between signals from different users and between signals from different subcarriers. Accordingly, the convolutional neural network (CNN) module and graph neural network (GNN) module included in the proposed NMBEnet can leverage these two correlations to further enhance the precision of beam training.","sentences":["Combining millimetre-wave (mmWave) communications with an extremely large-scale antenna array (ELAA) presents a promising avenue for meeting the spectral efficiency demands of the future sixth generation (6G) mobile communications.","However, beam training for mmWave ELAA systems is challenged by excessive pilot overheads as well as insufficient accuracy, as the huge near-field codebook has to be accounted for.","In this paper, inspired by the similarity between far-field sub-6 GHz channels and near-field mmWave channels, we propose to leverage sub-6 GHz uplink pilot signals to directly estimate the optimal near-field mmWave codeword, which aims to reduce pilot overhead and bypass the channel estimation.","Moreover, we adopt deep learning to perform this dual mapping function, i.e., sub-6 GHz to mmWave, far-field to near-field, and a novel neural network structure called NMBEnet is designed to enhance the precision of beam training.","Specifically, when considering the orthogonal frequency division multiplexing (OFDM) communication scenarios with high user density, correlations arise both between signals from different users and between signals from different subcarriers.","Accordingly, the convolutional neural network (CNN) module and graph neural network (GNN) module included in the proposed NMBEnet can leverage these two correlations to further enhance the precision of beam training."],"url":"http://arxiv.org/abs/2404.15469v1","category":"cs.IT"}
{"created":"2024-04-23 19:19:01","title":"Synthetic stellar spectra to study multiple populations in globular clusters: an extended grid and the effects on the integrated light","abstract":"Most Galactic Globular Clusters (GCs) harbour multiple populations of stars (MPs), composed of at least two generations: the first characterized by a \"standard\" $\\alpha$-enhanced metal mixture, as observed in field halo stars of the Milky Way, and the second displaying anti-correlated CN--ONa chemical abundance pattern in combination with an enhanced helium fraction. Adequate collections of stellar spectra are needed to characterize the effect of such stellar abundance changes on the integrated light of GCs. We present a grid of synthetic stellar spectra covering the atmospheric parameters relevant to old stellar populations at four subsolar metallicities and two abundance patterns, representative of first- and second-generations of stars in GCs. Integrated spectra of populations were computed using our stellar grid and empirical stellar populations, namely, colour-magnitude diagrams from literature for Galactic GCs. The spectra range from 290 to 1000nm, where we measured the effect on several spectrophotometric indices due to the surface abundance variations attributed to MPs. We find non-negligible effects of the MPs on spectroscopic indices sensitive to C, N, Ca, or Na, and on Balmer indices; we also describe how MPs modify specific regions in the near-UV and near-IR that can be measured with narrow or medium photometric passbands. The effects vary with metallicity. A number of these changes remain detectable even when accounting for the stochastic fluctuations due to the finite nature of the stellar population cluster.","sentences":["Most Galactic Globular Clusters (GCs) harbour multiple populations of stars (MPs), composed of at least two generations: the first characterized by a \"standard\" $\\alpha$-enhanced metal mixture, as observed in field halo stars of the Milky Way, and the second displaying anti-correlated CN--ONa chemical abundance pattern in combination with an enhanced helium fraction.","Adequate collections of stellar spectra are needed to characterize the effect of such stellar abundance changes on the integrated light of GCs.","We present a grid of synthetic stellar spectra covering the atmospheric parameters relevant to old stellar populations at four subsolar metallicities and two abundance patterns, representative of first- and second-generations of stars in GCs.","Integrated spectra of populations were computed using our stellar grid and empirical stellar populations, namely, colour-magnitude diagrams from literature for Galactic GCs.","The spectra range from 290 to 1000nm, where we measured the effect on several spectrophotometric indices due to the surface abundance variations attributed to MPs.","We find non-negligible effects of the MPs on spectroscopic indices sensitive to C, N, Ca, or Na, and on Balmer indices; we also describe how MPs modify specific regions in the near-UV and near-IR that can be measured with narrow or medium photometric passbands.","The effects vary with metallicity.","A number of these changes remain detectable even when accounting for the stochastic fluctuations due to the finite nature of the stellar population cluster."],"url":"http://arxiv.org/abs/2404.15468v1","category":"astro-ph.SR"}
{"created":"2024-04-23 19:15:47","title":"A Review on Message Complexity of the Algorithms for Clock Synchronization in Distributed Systems","abstract":"In this work, we present an extensive analysis of clock synchronization algorithms, with a specific focus on message complexity. We begin by introducing fundamental concepts in clock synchronization, such as the Byzantine generals problem and specific concepts like clock accuracy, precision, skew, offset, timestamping, and clock drift estimation. Describing the concept of logical clocks, their implementation in distributed systems is discussed, highlighting their significance and various approaches. The paper then examines four prominent clock synchronization algorithms: Lamport's Algorithm, Ricart-Agrawala Algorithm, Vector Clocks Algorithm, and Christian's Algorithm. Special attention is given to the analysis of message complexity, providing insights into the efficiency of each algorithm. Finally, we compare the message complexities of the discussed algorithms.","sentences":["In this work, we present an extensive analysis of clock synchronization algorithms, with a specific focus on message complexity.","We begin by introducing fundamental concepts in clock synchronization, such as the Byzantine generals problem and specific concepts like clock accuracy, precision, skew, offset, timestamping, and clock drift estimation.","Describing the concept of logical clocks, their implementation in distributed systems is discussed, highlighting their significance and various approaches.","The paper then examines four prominent clock synchronization algorithms: Lamport's Algorithm, Ricart-Agrawala Algorithm, Vector Clocks Algorithm, and Christian's Algorithm.","Special attention is given to the analysis of message complexity, providing insights into the efficiency of each algorithm.","Finally, we compare the message complexities of the discussed algorithms."],"url":"http://arxiv.org/abs/2404.15467v1","category":"cs.DC"}
{"created":"2024-04-23 19:15:43","title":"Private Optimal Inventory Policy Learning for Feature-based Newsvendor with Unknown Demand","abstract":"The data-driven newsvendor problem with features has recently emerged as a significant area of research, driven by the proliferation of data across various sectors such as retail, supply chains, e-commerce, and healthcare. Given the sensitive nature of customer or organizational data often used in feature-based analysis, it is crucial to ensure individual privacy to uphold trust and confidence. Despite its importance, privacy preservation in the context of inventory planning remains unexplored. A key challenge is the nonsmoothness of the newsvendor loss function, which sets it apart from existing work on privacy-preserving algorithms in other settings. This paper introduces a novel approach to estimate a privacy-preserving optimal inventory policy within the f-differential privacy framework, an extension of the classical $(\\epsilon, \\delta)$-differential privacy with several appealing properties. We develop a clipped noisy gradient descent algorithm based on convolution smoothing for optimal inventory estimation to simultaneously address three main challenges: (1) unknown demand distribution and nonsmooth loss function; (2) provable privacy guarantees for individual-level data; and (3) desirable statistical precision. We derive finite-sample high-probability bounds for optimal policy parameter estimation and regret analysis. By leveraging the structure of the newsvendor problem, we attain a faster excess population risk bound compared to that obtained from an indiscriminate application of existing results for general nonsmooth convex loss. Our bound aligns with that for strongly convex and smooth loss function. Our numerical experiments demonstrate that the proposed new method can achieve desirable privacy protection with a marginal increase in cost.","sentences":["The data-driven newsvendor problem with features has recently emerged as a significant area of research, driven by the proliferation of data across various sectors such as retail, supply chains, e-commerce, and healthcare.","Given the sensitive nature of customer or organizational data often used in feature-based analysis, it is crucial to ensure individual privacy to uphold trust and confidence.","Despite its importance, privacy preservation in the context of inventory planning remains unexplored.","A key challenge is the nonsmoothness of the newsvendor loss function, which sets it apart from existing work on privacy-preserving algorithms in other settings.","This paper introduces a novel approach to estimate a privacy-preserving optimal inventory policy within the f-differential privacy framework, an extension of the classical $(\\epsilon, \\delta)$-differential privacy with several appealing properties.","We develop a clipped noisy gradient descent algorithm based on convolution smoothing for optimal inventory estimation to simultaneously address three main challenges: (1) unknown demand distribution and nonsmooth loss function; (2) provable privacy guarantees for individual-level data; and (3) desirable statistical precision.","We derive finite-sample high-probability bounds for optimal policy parameter estimation and regret analysis.","By leveraging the structure of the newsvendor problem, we attain a faster excess population risk bound compared to that obtained from an indiscriminate application of existing results for general nonsmooth convex loss.","Our bound aligns with that for strongly convex and smooth loss function.","Our numerical experiments demonstrate that the proposed new method can achieve desirable privacy protection with a marginal increase in cost."],"url":"http://arxiv.org/abs/2404.15466v1","category":"stat.ML"}
{"created":"2024-04-23 19:13:03","title":"Wall Shear Stress Generated by a Bernoulli Pad: Hot-Film Anemometry and Model Testing","abstract":"The wall shear stress generated by a Bernoulli pad over a workpiece is of interest for the particular application of non-contact biofouling mitigation from ship hulls. The shear stress distribution has been determined numerically in the literature; it is directly measured experimentally for the first time in this paper. A constant temperature anemometer is used with a hot-film sensor and water as the working fluid; the sensor is calibrated using fully developed channel flow. Experiments with a Bernoulli pad accurately capture the magnitude of the maximum shear stress and its location resulting from constriction of the flow and flow separation due to sudden change in direction from axial to radial. Several numerical models are tested against the experimental measurements.","sentences":["The wall shear stress generated by a Bernoulli pad over a workpiece is of interest for the particular application of non-contact biofouling mitigation from ship hulls.","The shear stress distribution has been determined numerically in the literature; it is directly measured experimentally for the first time in this paper.","A constant temperature anemometer is used with a hot-film sensor and water as the working fluid; the sensor is calibrated using fully developed channel flow.","Experiments with a Bernoulli pad accurately capture the magnitude of the maximum shear stress and its location resulting from constriction of the flow and flow separation due to sudden change in direction from axial to radial.","Several numerical models are tested against the experimental measurements."],"url":"http://arxiv.org/abs/2404.15463v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 19:10:06","title":"Property $\\mathrm{(NL)}$ in Coexeter groups","abstract":"A group has Property $\\mathrm{(NL)}$ if it does not admit a loxodromic element in any hyperbolic action. In other words, a group with this property is inaccessible for study from the perspective of hyperbolic actions. This property was introduced by Balasubramanya, Fournier-Facio and Genevois, who initiated the study of this property. We expand on this research by studying Property $\\mathrm{(NL)}$ in Coxeter groups, a class of groups that are defined by an underlying graph. One of our main results show that a right-angled Coxeter group (RACG) has Property $\\mathrm{(NL)}$ if and only if its defining graph is complete. We then move beyond the right-angled case to show that if a defining graph is disconnected, its corresponding Coxeter group does not have Property $\\mathrm{(NL)}$. Lastly, we classify which triangle groups (Coxeter groups with three generators) have Property $\\mathrm{(NL)}$.","sentences":["A group has Property $\\mathrm{(NL)}$ if it does not admit a loxodromic element in any hyperbolic action.","In other words, a group with this property is inaccessible for study from the perspective of hyperbolic actions.","This property was introduced by Balasubramanya, Fournier-Facio and Genevois, who initiated the study of this property.","We expand on this research by studying Property $\\mathrm{(NL)}$ in Coxeter groups, a class of groups that are defined by an underlying graph.","One of our main results show that a right-angled Coxeter group (RACG) has Property $\\mathrm{(NL)}$ if and only if its defining graph is complete.","We then move beyond the right-angled case to show that if a defining graph is disconnected, its corresponding Coxeter group does not have Property $\\mathrm{(NL)}$. Lastly, we classify which triangle groups (Coxeter groups with three generators) have Property $\\mathrm{(NL)}$."],"url":"http://arxiv.org/abs/2404.15459v1","category":"math.GR"}
{"created":"2024-04-23 19:00:45","title":"Hidden in Plain Sight: Exploring the Intersections of Mental Health, Eating Disorders, and Content Moderation on TikTok","abstract":"Social media platforms actively moderate content glorifying harmful behaviors like eating disorders, which include anorexia and bulimia. However, users have adapted to evade moderation by using coded hashtags. Our study investigates the prevalence of moderation evaders on the popular social media platform TikTok and contrasts their use and emotional valence with mainstream hashtags. We notice that moderation evaders and mainstream hashtags appear together, indicating that vulnerable users might inadvertently encounter harmful content even when searching for mainstream terms. Additionally, through an analysis of emotional expressions in video descriptions and comments, we find that mainstream hashtags generally promote positive engagement, while moderation evaders evoke a wider range of emotions, including heightened negativity. These findings provide valuable insights for content creators, platform moderation efforts, and interventions aimed at cultivating a supportive online environment for discussions on mental health and eating disorders.","sentences":["Social media platforms actively moderate content glorifying harmful behaviors like eating disorders, which include anorexia and bulimia.","However, users have adapted to evade moderation by using coded hashtags.","Our study investigates the prevalence of moderation evaders on the popular social media platform TikTok and contrasts their use and emotional valence with mainstream hashtags.","We notice that moderation evaders and mainstream hashtags appear together, indicating that vulnerable users might inadvertently encounter harmful content even when searching for mainstream terms.","Additionally, through an analysis of emotional expressions in video descriptions and comments, we find that mainstream hashtags generally promote positive engagement, while moderation evaders evoke a wider range of emotions, including heightened negativity.","These findings provide valuable insights for content creators, platform moderation efforts, and interventions aimed at cultivating a supportive online environment for discussions on mental health and eating disorders."],"url":"http://arxiv.org/abs/2404.15457v1","category":"cs.SI"}
{"created":"2024-04-23 18:57:24","title":"Inertial Torsion Noise in Matter-Wave Interferometers for Gravity Experiments","abstract":"Matter-wave interferometry is susceptible to non-inertial noise sources, which can induce dephasing and a resulting loss of interferometric visibility. Here, we focus on inertial torsion noise (ITN), which arises from the rotational motion of the experimental apparatus suspended by a thin wire and subject to random external torques. We provide analytical expressions for the ITN noise starting from Langevin equations describing the experimental box in a thermal environment which can then be used together with the transfer function to obtain the dephasing factor. We verify the theoretical modelling and the validity of the approximations using Monte Carlo simulations obtaining good agreement between theory and numerics. As an application we estimate the size of the effects for the next-generation of interferometery experiments with femtogram particles, which could be used as the building block for entanglement-based tests of the quantum nature of gravity. We find that the ambient gas is a weak source of ITN, posing mild restrictions on the ambient pressure and temperature, and conclude with a discussion about the general ITN constrains by assuming a Langevin equation parameterized by three phenomenological parameters.","sentences":["Matter-wave interferometry is susceptible to non-inertial noise sources, which can induce dephasing and a resulting loss of interferometric visibility.","Here, we focus on inertial torsion noise (ITN), which arises from the rotational motion of the experimental apparatus suspended by a thin wire and subject to random external torques.","We provide analytical expressions for the ITN noise starting from Langevin equations describing the experimental box in a thermal environment which can then be used together with the transfer function to obtain the dephasing factor.","We verify the theoretical modelling and the validity of the approximations using Monte Carlo simulations obtaining good agreement between theory and numerics.","As an application we estimate the size of the effects for the next-generation of interferometery experiments with femtogram particles, which could be used as the building block for entanglement-based tests of the quantum nature of gravity.","We find that the ambient gas is a weak source of ITN, posing mild restrictions on the ambient pressure and temperature, and conclude with a discussion about the general ITN constrains by assuming a Langevin equation parameterized by three phenomenological parameters."],"url":"http://arxiv.org/abs/2404.15455v1","category":"quant-ph"}
{"created":"2024-04-23 18:46:07","title":"CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and Detection","abstract":"Feature pyramids have been widely adopted in convolutional neural networks (CNNs) and transformers for tasks like medical image segmentation and object detection. However, the currently existing models generally focus on the Encoder-side Transformer to extract features, from which decoder improvement can bring further potential with well-designed architecture. We propose CFPFormer, a novel decoder block that integrates feature pyramids and transformers. Specifically, by leveraging patch embedding, cross-layer feature concatenation, and Gaussian attention mechanisms, CFPFormer enhances feature extraction capabilities while promoting generalization across diverse tasks. Benefiting from Transformer structure and U-shaped Connections, our introduced model gains the ability to capture long-range dependencies and effectively up-sample feature maps. Our model achieves superior performance in detecting small objects compared to existing methods. We evaluate CFPFormer on medical image segmentation datasets and object detection benchmarks (VOC 2007, VOC2012, MS-COCO), demonstrating its effectiveness and versatility. On the ACDC Post-2017-MICCAI-Challenge online test set, our model reaches exceptionally impressive accuracy, and performed well compared with the original decoder setting in Synapse multi-organ segmentation dataset.","sentences":["Feature pyramids have been widely adopted in convolutional neural networks (CNNs) and transformers for tasks like medical image segmentation and object detection.","However, the currently existing models generally focus on the Encoder-side Transformer to extract features, from which decoder improvement can bring further potential with well-designed architecture.","We propose CFPFormer, a novel decoder block that integrates feature pyramids and transformers.","Specifically, by leveraging patch embedding, cross-layer feature concatenation, and Gaussian attention mechanisms, CFPFormer enhances feature extraction capabilities while promoting generalization across diverse tasks.","Benefiting from Transformer structure and U-shaped Connections, our introduced model gains the ability to capture long-range dependencies and effectively up-sample feature maps.","Our model achieves superior performance in detecting small objects compared to existing methods.","We evaluate CFPFormer on medical image segmentation datasets and object detection benchmarks (VOC 2007, VOC2012, MS-COCO), demonstrating its effectiveness and versatility.","On the ACDC Post-2017-MICCAI-Challenge online test set, our model reaches exceptionally impressive accuracy, and performed well compared with the original decoder setting in Synapse multi-organ segmentation dataset."],"url":"http://arxiv.org/abs/2404.15451v1","category":"cs.CV"}
{"created":"2024-04-23 18:43:29","title":"A possible origin of the $\u03b1$-vacuum as the initial state of the Universe","abstract":"We investigate the cosmological observables using the Euclidean path integral approach. Specifically, we study both the no-boundary compact instantons scenario and the Euclidean wormholes scenario that can induce the creation of two universes from nothing. It is known that perturbations associated with the no-boundary scenario can only be consistent with the Bunch-Davies vacuum. Here we demonstrate that the Euclidean wormholes can allow for a de Sitter invariant vacuum, the so-called $\\alpha$-vacuum state, where the Bunch-Davies vacuum is a special case. This therefore provides the $\\alpha$-vacuum a geometrical origin. As an aside, we discuss a subtle phase issue when considering the power spectrum related to $\\alpha$-vacuum in the closed universe framework.","sentences":["We investigate the cosmological observables using the Euclidean path integral approach.","Specifically, we study both the no-boundary compact instantons scenario and the Euclidean wormholes scenario that can induce the creation of two universes from nothing.","It is known that perturbations associated with the no-boundary scenario can only be consistent with the Bunch-Davies vacuum.","Here we demonstrate that the Euclidean wormholes can allow for a de Sitter invariant vacuum, the so-called $\\alpha$-vacuum state, where the Bunch-Davies vacuum is a special case.","This therefore provides the $\\alpha$-vacuum a geometrical origin.","As an aside, we discuss a subtle phase issue when considering the power spectrum related to $\\alpha$-vacuum in the closed universe framework."],"url":"http://arxiv.org/abs/2404.15450v1","category":"gr-qc"}
{"created":"2024-04-23 18:41:56","title":"ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning","abstract":"The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \\textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \\textbf{Project Page: \\url{https://idaligner.github.io/}}","sentences":["The rapid development of diffusion models has triggered diverse applications.","Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising.","While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously.","To address these issues, we present \\textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance.","To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation.","Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals.","Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains.","Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach.","\\textbf{Project","Page: \\url{https://idaligner.github.io/}}"],"url":"http://arxiv.org/abs/2404.15449v1","category":"cs.CV"}
{"created":"2024-04-23 18:39:57","title":"GLoD: Composing Global Contexts and Local Details in Image Generation","abstract":"Diffusion models have demonstrated their capability to synthesize high-quality and diverse images from textual prompts. However, simultaneous control over both global contexts (e.g., object layouts and interactions) and local details (e.g., colors and emotions) still remains a significant challenge. The models often fail to understand complex descriptions involving multiple objects and reflect specified visual attributes to wrong targets or ignore them. This paper presents Global-Local Diffusion (\\textit{GLoD}), a novel framework which allows simultaneous control over the global contexts and the local details in text-to-image generation without requiring training or fine-tuning. It assigns multiple global and local prompts to corresponding layers and composes their noises to guide a denoising process using pre-trained diffusion models. Our framework enables complex global-local compositions, conditioning objects in the global prompt with the local prompts while preserving other unspecified identities. Our quantitative and qualitative evaluations demonstrate that GLoD effectively generates complex images that adhere to both user-provided object interactions and object details.","sentences":["Diffusion models have demonstrated their capability to synthesize high-quality and diverse images from textual prompts.","However, simultaneous control over both global contexts (e.g., object layouts and interactions) and local details (e.g., colors and emotions) still remains a significant challenge.","The models often fail to understand complex descriptions involving multiple objects and reflect specified visual attributes to wrong targets or ignore them.","This paper presents Global-Local Diffusion (\\textit{GLoD}), a novel framework which allows simultaneous control over the global contexts and the local details in text-to-image generation without requiring training or fine-tuning.","It assigns multiple global and local prompts to corresponding layers and composes their noises to guide a denoising process using pre-trained diffusion models.","Our framework enables complex global-local compositions, conditioning objects in the global prompt with the local prompts while preserving other unspecified identities.","Our quantitative and qualitative evaluations demonstrate that GLoD effectively generates complex images that adhere to both user-provided object interactions and object details."],"url":"http://arxiv.org/abs/2404.15447v1","category":"cs.CV"}
{"created":"2024-04-23 18:39:50","title":"OffRAMPS: An FPGA-based Intermediary for Analysis and Modification of Additive Manufacturing Control Systems","abstract":"Cybersecurity threats in Additive Manufacturing (AM) are an increasing concern as AM adoption continues to grow. AM is now being used for parts in the aerospace, transportation, and medical domains. Threat vectors which allow for part compromise are particularly concerning, as any failure in these domains would have life-threatening consequences. A major challenge to investigation of AM part-compromises comes from the difficulty in evaluating and benchmarking both identified threat vectors as well as methods for detecting adversarial actions. In this work, we introduce a generalized platform for systematic analysis of attacks against and defenses for 3D printers. Our \"OFFRAMPS\" platform is based on the open-source 3D printer control board \"RAMPS.\" OFFRAMPS allows analysis, recording, and modification of all control signals and I/O for a 3D printer. We show the efficacy of OFFRAMPS by presenting a series of case studies based on several Trojans, including ones identified in the literature, and show that OFFRAMPS can both emulate and detect these attacks, i.e., it can both change and detect arbitrary changes to the g-code print commands.","sentences":["Cybersecurity threats in Additive Manufacturing (AM) are an increasing concern as AM adoption continues to grow.","AM is now being used for parts in the aerospace, transportation, and medical domains.","Threat vectors which allow for part compromise are particularly concerning, as any failure in these domains would have life-threatening consequences.","A major challenge to investigation of AM part-compromises comes from the difficulty in evaluating and benchmarking both identified threat vectors as well as methods for detecting adversarial actions.","In this work, we introduce a generalized platform for systematic analysis of attacks against and defenses for 3D printers.","Our \"OFFRAMPS\" platform is based on the open-source 3D printer control board \"RAMPS.\"","OFFRAMPS allows analysis, recording, and modification of all control signals and I/O for a 3D printer.","We show the efficacy of OFFRAMPS by presenting a series of case studies based on several Trojans, including ones identified in the literature, and show that OFFRAMPS can both emulate and detect these attacks, i.e., it can both change and detect arbitrary changes to the g-code print commands."],"url":"http://arxiv.org/abs/2404.15446v1","category":"cs.CR"}
{"created":"2024-04-23 18:34:51","title":"The Gravity Collective: A Comprehensive Analysis of the Electromagnetic Search for the Binary Neutron Star Merger GW190425","abstract":"We present an ultraviolet-to-infrared search for the electromagnetic (EM) counterpart to GW190425, the second-ever binary neutron star (BNS) merger discovered by the LIGO-Virgo-KAGRA Collaboration (LVK). GW190425 was more distant and had a larger localization area than GW170817, therefore we use a new tool teglon to redistribute the GW190425 localization probability in the context of galaxy catalogs within the final localization volume. We derive a 90th percentile area of 6,688 deg$^{2}$, a $\\sim$1.5$\\times$ improvement relative to the LIGO/Virgo map, and show how teglon provides an order of magnitude boost to the search efficiency of small ($\\leq$1 deg$^{2}$) field-of-view instruments. We combine our data with all publicly reported imaging data, covering 9,078.59 deg$^2$ of unique area and 48.13% of the LIGO/Virgo-assigned localization probability, to calculate the most comprehensive kilonova, short gamma-ray burst (sGRB) afterglow, and model-independent constraints on the EM emission from a hypothetical counterpart to GW190425 to date under the assumption that no counterpart was found in these data. If the counterpart were similar to AT 2017gfo, there was a 28.4% chance that it would have been detected in the combined dataset. We are relatively insensitive to an on-axis sGRB, and rule out a generic transient with a similar peak luminosity and decline rate as AT 2017gfo to 30% confidence. Finally, across our new imaging and all publicly-reported data, we find 28 candidate optical counterparts that we cannot rule out as being associated with GW190425, finding that 4 such counterparts discovered within the localization volume and within 5 days of merger exhibit luminosities consistent with a kilonova.","sentences":["We present an ultraviolet-to-infrared search for the electromagnetic (EM) counterpart to GW190425, the second-ever binary neutron star (BNS) merger discovered by the LIGO-Virgo-KAGRA Collaboration (LVK).","GW190425 was more distant and had a larger localization area than GW170817, therefore we use a new tool teglon to redistribute the GW190425 localization probability in the context of galaxy catalogs within the final localization volume.","We derive a 90th percentile area of 6,688 deg$^{2}$, a $\\sim$1.5$\\times$ improvement relative to the LIGO/Virgo map, and show how teglon provides an order of magnitude boost to the search efficiency of small ($\\leq$1 deg$^{2}$) field-of-view instruments.","We combine our data with all publicly reported imaging data, covering 9,078.59 deg$^2$ of unique area and 48.13% of the LIGO/Virgo-assigned localization probability, to calculate the most comprehensive kilonova, short gamma-ray burst (sGRB) afterglow, and model-independent constraints on the EM emission from a hypothetical counterpart to GW190425 to date under the assumption that no counterpart was found in these data.","If the counterpart were similar to AT 2017gfo, there was a 28.4% chance that it would have been detected in the combined dataset.","We are relatively insensitive to an on-axis sGRB, and rule out a generic transient with a similar peak luminosity and decline rate as AT 2017gfo to 30% confidence.","Finally, across our new imaging and all publicly-reported data, we find 28 candidate optical counterparts that we cannot rule out as being associated with GW190425, finding that 4 such counterparts discovered within the localization volume and within 5 days of merger exhibit luminosities consistent with a kilonova."],"url":"http://arxiv.org/abs/2404.15441v1","category":"astro-ph.HE"}
{"created":"2024-04-23 18:31:03","title":"Exploring Convergence in Relation using Association Rules Mining: A Case Study in Collaborative Knowledge Production","abstract":"This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes. Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists. By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights. Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production. To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced. This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors. Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data. This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age. Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age.","sentences":["This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes.","Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists.","By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights.","Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production.","To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced.","This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors.","Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data.","This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age.","Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age."],"url":"http://arxiv.org/abs/2404.15440v1","category":"cs.HC"}
{"created":"2024-04-23 18:27:17","title":"A magnetic oriented approach to the systematic coupling of field and circuit equations","abstract":"A novel strategy is proposed for the coupling of field and circuit equations when modeling power devices in the low-frequency regime. The resulting systems of differential-algebraic equations have a particular geometric structure which explicitly encodes the energy storage, dissipation, and transfer mechanisms. This implies a power balance on the continuous level which can be preserved under appropriate discretization in space and time. The models and main results are presented in detail for linear constitutive models, but the extension to nonlinear elements and more general coupling mechanisms is possible. The theoretical findings are demonstrated by numerical results.","sentences":["A novel strategy is proposed for the coupling of field and circuit equations when modeling power devices in the low-frequency regime.","The resulting systems of differential-algebraic equations have a particular geometric structure which explicitly encodes the energy storage, dissipation, and transfer mechanisms.","This implies a power balance on the continuous level which can be preserved under appropriate discretization in space and time.","The models and main results are presented in detail for linear constitutive models, but the extension to nonlinear elements and more general coupling mechanisms is possible.","The theoretical findings are demonstrated by numerical results."],"url":"http://arxiv.org/abs/2404.15438v1","category":"math.NA"}
{"created":"2024-04-23 18:26:32","title":"Accretion disks properties around regular black hole solutions obtained from non-linear electrodynamics","abstract":"We investigate a family of spherically symmetric, static, charged regular black hole solutions derived within the framework of Einstein-nonlinear electrodynamics. Our study focuses on examining the characteristics of accretion disks in the spacetimes described by the Dymnikova and Fan-Wang solutions. We explore circular geodesics of test particles and calculate various properties, including the radius of the innermost stable circular orbit, radiant energy, temperature, and conversion efficiency of accretion mass into radiation. We employ the Novikov-Thorne-Page thin accretion disk model as a background. By comparing our findings with those obtained in the Schwarzschild black hole case, we reveal significant modifications in the overall spectral properties. Specifically, we observe an increase in the energy emitted from the disk surface, resulting in higher temperatures for the accretion disks under certain values of the free parameters. Consequently, we note an enhanced efficiency of mass conversion into radiation compared to the Schwarzschild spacetime.","sentences":["We investigate a family of spherically symmetric, static, charged regular black hole solutions derived within the framework of Einstein-nonlinear electrodynamics.","Our study focuses on examining the characteristics of accretion disks in the spacetimes described by the Dymnikova and Fan-Wang solutions.","We explore circular geodesics of test particles and calculate various properties, including the radius of the innermost stable circular orbit, radiant energy, temperature, and conversion efficiency of accretion mass into radiation.","We employ the Novikov-Thorne-Page thin accretion disk model as a background.","By comparing our findings with those obtained in the Schwarzschild black hole case, we reveal significant modifications in the overall spectral properties.","Specifically, we observe an increase in the energy emitted from the disk surface, resulting in higher temperatures for the accretion disks under certain values of the free parameters.","Consequently, we note an enhanced efficiency of mass conversion into radiation compared to the Schwarzschild spacetime."],"url":"http://arxiv.org/abs/2404.15437v1","category":"gr-qc"}
{"created":"2024-04-23 18:26:11","title":"Iterative Cluster Harvesting for Wafer Map Defect Patterns","abstract":"Unsupervised clustering of wafer map defect patterns is challenging because the appearance of certain defect patterns varies significantly. This includes changing shape, location, density, and rotation of the defect area on the wafer. We present a harvesting approach, which can cluster even challenging defect patterns of wafer maps well. Our approach makes use of a well-known, three-step procedure: feature extraction, dimension reduction, and clustering. The novelty in our approach lies in repeating dimensionality reduction and clustering iteratively while filtering out one cluster per iteration according to its silhouette score. This method leads to an improvement of clustering performance in general and is especially useful for difficult defect patterns. The low computational effort allows for a quick assessment of large datasets and can be used to support manual labeling efforts. We benchmark against related approaches from the literature and show improved results on a real-world industrial dataset.","sentences":["Unsupervised clustering of wafer map defect patterns is challenging because the appearance of certain defect patterns varies significantly.","This includes changing shape, location, density, and rotation of the defect area on the wafer.","We present a harvesting approach, which can cluster even challenging defect patterns of wafer maps well.","Our approach makes use of a well-known, three-step procedure: feature extraction, dimension reduction, and clustering.","The novelty in our approach lies in repeating dimensionality reduction and clustering iteratively while filtering out one cluster per iteration according to its silhouette score.","This method leads to an improvement of clustering performance in general and is especially useful for difficult defect patterns.","The low computational effort allows for a quick assessment of large datasets and can be used to support manual labeling efforts.","We benchmark against related approaches from the literature and show improved results on a real-world industrial dataset."],"url":"http://arxiv.org/abs/2404.15436v1","category":"cs.CV"}
{"created":"2024-04-23 18:19:54","title":"Exoplanet Geology: What can we learn from current and future observations?","abstract":"Nearly 30 years after the discovery of the first exoplanet around a main sequence star, thousands of planets have now been confirmed. These discoveries have completely revolutionized our understanding of planetary systems, revealing types of planets that do not exist in our solar system but are common in extrasolar systems, and a wide range of system architectures. Our solar system is clearly not the default for planetary systems. The community is now moving beyond basic characterization of exoplanets (mass, radius, and orbits) towards a deeper characterization of their atmospheres and even surfaces. With improved observational capabilities there is potential to now probe the geology of rocky exoplanets; this raises the possibility of an analogous revolution in our understanding of rocky planet evolution. However, characterizing the geology or geological processes occurring on rocky exoplanets is a major challenge, even with next generation telescopes. This chapter reviews what we may be able to accomplish with these efforts in the near-term and long-term. In the near-term, the James Webb Space Telescope (JWST) is revealing which rocky planets lose versus retain their atmospheres. This chapter discusses the implications of such discoveries, including how even planets with no or minimal atmospheres can still provide constraints on surface geology and long-term geological evolution. Longer-term possibilities are then reviewed, including whether the hypothesis of climate stabilization by the carbonate-silicate cycle can be tested by next generation telescopes. New modeling strategies sweeping through ranges of possibly evolutionary scenarios will be needed to use the current and future observations to constrain rocky exoplanet geology and evolution.","sentences":["Nearly 30 years after the discovery of the first exoplanet around a main sequence star, thousands of planets have now been confirmed.","These discoveries have completely revolutionized our understanding of planetary systems, revealing types of planets that do not exist in our solar system but are common in extrasolar systems, and a wide range of system architectures.","Our solar system is clearly not the default for planetary systems.","The community is now moving beyond basic characterization of exoplanets (mass, radius, and orbits) towards a deeper characterization of their atmospheres and even surfaces.","With improved observational capabilities there is potential to now probe the geology of rocky exoplanets; this raises the possibility of an analogous revolution in our understanding of rocky planet evolution.","However, characterizing the geology or geological processes occurring on rocky exoplanets is a major challenge, even with next generation telescopes.","This chapter reviews what we may be able to accomplish with these efforts in the near-term and long-term.","In the near-term, the James Webb Space Telescope (JWST) is revealing which rocky planets lose versus retain their atmospheres.","This chapter discusses the implications of such discoveries, including how even planets with no or minimal atmospheres can still provide constraints on surface geology and long-term geological evolution.","Longer-term possibilities are then reviewed, including whether the hypothesis of climate stabilization by the carbonate-silicate cycle can be tested by next generation telescopes.","New modeling strategies sweeping through ranges of possibly evolutionary scenarios will be needed to use the current and future observations to constrain rocky exoplanet geology and evolution."],"url":"http://arxiv.org/abs/2404.15433v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:19:23","title":"Exoplanet Magnetic Fields","abstract":"Planetary magnetic fields are important indicators of planetary processes and evolution, from a planet's outer core to its surface (if it possesses one) to its atmosphere and near-space environment. Magnetic fields are most directly measured in situ, and determining whether distant planetary objects possess magnetic fields can be challenging. At present we have no unambiguous measurements of magnetic fields on exoplanets. Nevertheless, it would be surprising if at least some exoplanets did not generate a magnetic field, like many planetary bodies in the solar system. This chapter provides an overview of the current understanding of exoplanetary magnetic fields and their consequences. In the next section we review the current understanding of planetary dynamo generation as it applies to solar system objects and discuss the implications for exoplanetary magnetic field generation. Following this, we describe seven methods for determining the existence and strength of an exoplanetary magnetic field and discuss the near-term prospects for each method. We close by highlighting four main consequences of exoplanetary magnetic fields for a planet and its evolution.","sentences":["Planetary magnetic fields are important indicators of planetary processes and evolution, from a planet's outer core to its surface (if it possesses one) to its atmosphere and near-space environment.","Magnetic fields are most directly measured in situ, and determining whether distant planetary objects possess magnetic fields can be challenging.","At present we have no unambiguous measurements of magnetic fields on exoplanets.","Nevertheless, it would be surprising if at least some exoplanets did not generate a magnetic field, like many planetary bodies in the solar system.","This chapter provides an overview of the current understanding of exoplanetary magnetic fields and their consequences.","In the next section we review the current understanding of planetary dynamo generation as it applies to solar system objects and discuss the implications for exoplanetary magnetic field generation.","Following this, we describe seven methods for determining the existence and strength of an exoplanetary magnetic field and discuss the near-term prospects for each method.","We close by highlighting four main consequences of exoplanetary magnetic fields for a planet and its evolution."],"url":"http://arxiv.org/abs/2404.15429v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:18:48","title":"Meteorites and Planet Formation","abstract":"Meteorites are a remarkable resource. They capture the imagination of people worldwide with their spectacular entry through Earth's atmosphere as fireballs, and their exotic character of being pieces of other worlds. Scientifically, they are critical to interpreting the early stages of formation of the Solar System, as well as the geological evolution of asteroids, the Moon, and Mars, and they are vital to understanding planetary formation processes. With the burgeoning exploration of extrasolar planetary systems, knowledge of the fundamental process of planetary growth from protoplanetary disks has taken on a new significance. Meteorites provide essential and detailed insight into the formation of planetary systems, although we must bear in mind that they only represent one reference point (our own Solar System) in what is clearly a wide spectrum of possible chemical and physical parameters governing the diverse realm of extrasolar planets. This chapter summarises the nature of our meteorite collections, and the ways in which meteorites contribute to our understanding of the formation and evolution of our own Solar System, with broader implications for planetary systems in general.","sentences":["Meteorites are a remarkable resource.","They capture the imagination of people worldwide with their spectacular entry through Earth's atmosphere as fireballs, and their exotic character of being pieces of other worlds.","Scientifically, they are critical to interpreting the early stages of formation of the Solar System, as well as the geological evolution of asteroids, the Moon, and Mars, and they are vital to understanding planetary formation processes.","With the burgeoning exploration of extrasolar planetary systems, knowledge of the fundamental process of planetary growth from protoplanetary disks has taken on a new significance.","Meteorites provide essential and detailed insight into the formation of planetary systems, although we must bear in mind that they only represent one reference point (our own Solar System) in what is clearly a wide spectrum of possible chemical and physical parameters governing the diverse realm of extrasolar planets.","This chapter summarises the nature of our meteorite collections, and the ways in which meteorites contribute to our understanding of the formation and evolution of our own Solar System, with broader implications for planetary systems in general."],"url":"http://arxiv.org/abs/2404.15424v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:10:42","title":"XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference","abstract":"In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.","sentences":["In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information.","Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable.","However, caching transformer states can easily require almost as much space as the model parameters.","When the right context isn't known in advance, caching ICL can be challenging.","This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt.","More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers.","We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude."],"url":"http://arxiv.org/abs/2404.15420v1","category":"cs.CL"}
{"created":"2024-04-23 18:10:18","title":"Using Deep Learning to Identify Initial Error Sensitivity of ENSO Forecasts","abstract":"We introduce a hybrid method that integrates deep learning with model-analog forecasting, a straightforward yet effective approach that generates forecasts from similar initial climate states in a repository of model simulations. This hybrid framework employs a convolutional neural network to estimate state-dependent weights to identify analog states. The advantage of our method lies in its physical interpretability, offering insights into initial-error-sensitive regions through estimated weights and the ability to trace the physically-based temporal evolution of the system through analog forecasting. We evaluate our approach using the Community Earth System Model Version 2 Large Ensemble to forecast the El Ni\\~no-Southern Oscillation (ENSO) on a seasonal-to-annual time scale. Results show a 10% improvement in forecasting sea surface temperature anomalies over the equatorial Pacific at 9-12 months leads compared to the traditional model-analog technique. Furthermore, our hybrid model demonstrates improvements in boreal winter and spring initialization when evaluated against a reanalysis dataset. Our deep learning-based approach reveals state-dependent sensitivity linked to various seasonally varying physical processes, including the Pacific Meridional Modes, equatorial recharge oscillator, and stochastic wind forcing. Notably, disparities emerge in the sensitivity associated with El Ni\\~no and La Ni\\~na events. We find that sea surface temperature over the tropical Pacific plays a more crucial role in El Ni\\~no forecasting, while zonal wind stress over the same region exhibits greater significance in La Ni\\~na prediction. This approach has broad implications for forecasting diverse climate phenomena, including regional temperature and precipitation, which are challenging for the traditional model-analog forecasting method.","sentences":["We introduce a hybrid method that integrates deep learning with model-analog forecasting, a straightforward yet effective approach that generates forecasts from similar initial climate states in a repository of model simulations.","This hybrid framework employs a convolutional neural network to estimate state-dependent weights to identify analog states.","The advantage of our method lies in its physical interpretability, offering insights into initial-error-sensitive regions through estimated weights and the ability to trace the physically-based temporal evolution of the system through analog forecasting.","We evaluate our approach using the Community Earth System Model Version 2 Large Ensemble to forecast the El Ni\\~no-Southern Oscillation (ENSO) on a seasonal-to-annual time scale.","Results show a 10% improvement in forecasting sea surface temperature anomalies over the equatorial Pacific at 9-12 months leads compared to the traditional model-analog technique.","Furthermore, our hybrid model demonstrates improvements in boreal winter and spring initialization when evaluated against a reanalysis dataset.","Our deep learning-based approach reveals state-dependent sensitivity linked to various seasonally varying physical processes, including the Pacific Meridional Modes, equatorial recharge oscillator, and stochastic wind forcing.","Notably, disparities emerge in the sensitivity associated with El Ni\\~no and La Ni\\~na events.","We find that sea surface temperature over the tropical Pacific plays a more crucial role in El Ni\\~no forecasting, while zonal wind stress over the same region exhibits greater significance in La Ni\\~na prediction.","This approach has broad implications for forecasting diverse climate phenomena, including regional temperature and precipitation, which are challenging for the traditional model-analog forecasting method."],"url":"http://arxiv.org/abs/2404.15419v1","category":"physics.ao-ph"}
{"created":"2024-04-23 18:09:53","title":"The Power of Resets in Online Reinforcement Learning","abstract":"Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with {local simulator access} (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:   - We show that MDPs with low coverability Xie et al. 2023 -- a general structural condition that subsumes Block MDPs and Low-Rank MDPs -- can be learned in a sample-efficient fashion with only $Q^{\\star}$-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.   - As a consequence, we show that the notorious Exogenous Block MDP problem Efroni et al. 2022 is tractable under local simulator access.   The results above are achieved through a computationally inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.","sentences":["Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation.","We explore the power of simulators through online reinforcement learning with {local simulator access} (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training.","We use local simulator access to unlock new statistical guarantees that were previously out of reach:   - We show that MDPs with low coverability Xie et al. 2023 -- a general structural condition that subsumes Block MDPs and Low-Rank MDPs -- can be learned in a sample-efficient fashion with only $Q^{\\star}$-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.   ","- As a consequence, we show that the notorious Exogenous Block MDP problem Efroni et al. 2022 is tractable under local simulator access.   ","The results above are achieved through a computationally inefficient algorithm.","We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability.","RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation."],"url":"http://arxiv.org/abs/2404.15417v1","category":"cs.LG"}
{"created":"2024-04-23 18:09:53","title":"Machine Learning Techniques with Fairness for Prediction of Completion of Drug and Alcohol Rehabilitation","abstract":"The aim of this study is to look at predicting whether a person will complete a drug and alcohol rehabilitation program and the number of times a person attends. The study is based on demographic data obtained from Substance Abuse and Mental Health Services Administration (SAMHSA) from both admissions and discharge data from drug and alcohol rehabilitation centers in Oklahoma. Demographic data is highly categorical which led to binary encoding being used and various fairness measures being utilized to mitigate bias of nine demographic variables. Kernel methods such as linear, polynomial, sigmoid, and radial basis functions were compared using support vector machines at various parameter ranges to find the optimal values. These were then compared to methods such as decision trees, random forests, and neural networks. Synthetic Minority Oversampling Technique Nominal (SMOTEN) for categorical data was used to balance the data with imputation for missing data. The nine bias variables were then intersectionalized to mitigate bias and the dual and triple interactions were integrated to use the probabilities to look at worst case ratio fairness mitigation. Disparate Impact, Statistical Parity difference, Conditional Statistical Parity Ratio, Demographic Parity, Demographic Parity Ratio, Equalized Odds, Equalized Odds Ratio, Equal Opportunity, and Equalized Opportunity Ratio were all explored at both the binary and multiclass scenarios.","sentences":["The aim of this study is to look at predicting whether a person will complete a drug and alcohol rehabilitation program and the number of times a person attends.","The study is based on demographic data obtained from Substance Abuse and Mental Health Services Administration (SAMHSA) from both admissions and discharge data from drug and alcohol rehabilitation centers in Oklahoma.","Demographic data is highly categorical which led to binary encoding being used and various fairness measures being utilized to mitigate bias of nine demographic variables.","Kernel methods such as linear, polynomial, sigmoid, and radial basis functions were compared using support vector machines at various parameter ranges to find the optimal values.","These were then compared to methods such as decision trees, random forests, and neural networks.","Synthetic Minority Oversampling Technique Nominal (SMOTEN) for categorical data was used to balance the data with imputation for missing data.","The nine bias variables were then intersectionalized to mitigate bias and the dual and triple interactions were integrated to use the probabilities to look at worst case ratio fairness mitigation.","Disparate Impact, Statistical Parity difference, Conditional Statistical Parity Ratio, Demographic Parity, Demographic Parity Ratio, Equalized Odds, Equalized Odds Ratio, Equal Opportunity, and Equalized Opportunity Ratio were all explored at both the binary and multiclass scenarios."],"url":"http://arxiv.org/abs/2404.15418v1","category":"cs.LG"}
{"created":"2024-04-23 18:09:23","title":"The Open Effective Field Theory of Inflation","abstract":"In our quest to understand the generation of cosmological perturbations, we face two serious obstacles: we do not have direct information about the environment experienced by primordial perturbations during inflation, and our observables are practically limited to correlators of massless fields, heavier fields and derivatives decaying exponentially in the number of e-foldings. The flexible and general framework of open systems has been developed precisely to face similar challenges. Building on previous work, we develop a Schwinger-Keldysh path integral description for an open effective field theory of inflation, describing the possibly dissipative and non-unitary evolution of the Goldstone boson of time translations interacting with an unspecified environment, under the key assumption of locality in space and time. Working in the decoupling limit, we study the linear and interacting theory in de Sitter and derive predictions for the power spectrum and bispectrum that depend on a finite number of effective couplings organised in a derivative expansion. The smoking gun of interactions with the environment is an enhanced but finite bispectrum close to the folded kinematical limit. We demonstrate the generality of our approach by matching our open effective theory to an explicit model. Our construction provides a standard model to simultaneously study phenomenological predictions as well as quantum information aspects of the inflationary dynamics.","sentences":["In our quest to understand the generation of cosmological perturbations, we face two serious obstacles: we do not have direct information about the environment experienced by primordial perturbations during inflation, and our observables are practically limited to correlators of massless fields, heavier fields and derivatives decaying exponentially in the number of e-foldings.","The flexible and general framework of open systems has been developed precisely to face similar challenges.","Building on previous work, we develop a Schwinger-Keldysh path integral description for an open effective field theory of inflation, describing the possibly dissipative and non-unitary evolution of the Goldstone boson of time translations interacting with an unspecified environment, under the key assumption of locality in space and time.","Working in the decoupling limit, we study the linear and interacting theory in de Sitter and derive predictions for the power spectrum and bispectrum that depend on a finite number of effective couplings organised in a derivative expansion.","The smoking gun of interactions with the environment is an enhanced but finite bispectrum close to the folded kinematical limit.","We demonstrate the generality of our approach by matching our open effective theory to an explicit model.","Our construction provides a standard model to simultaneously study phenomenological predictions as well as quantum information aspects of the inflationary dynamics."],"url":"http://arxiv.org/abs/2404.15416v1","category":"hep-th"}
{"created":"2024-04-23 18:01:30","title":"Planning the path with Reinforcement Learning: Optimal Robot Motion Planning in RoboCup Small Size League Environments","abstract":"This work investigates the potential of Reinforcement Learning (RL) to tackle robot motion planning challenges in the dynamic RoboCup Small Size League (SSL). Using a heuristic control approach, we evaluate RL's effectiveness in obstacle-free and single-obstacle path-planning environments. Ablation studies reveal significant performance improvements. Our method achieved a 60% time gain in obstacle-free environments compared to baseline algorithms. Additionally, our findings demonstrated dynamic obstacle avoidance capabilities, adeptly navigating around moving blocks. These findings highlight the potential of RL to enhance robot motion planning in the challenging and unpredictable SSL environment.","sentences":["This work investigates the potential of Reinforcement Learning (RL) to tackle robot motion planning challenges in the dynamic RoboCup Small Size League (SSL).","Using a heuristic control approach, we evaluate RL's effectiveness in obstacle-free and single-obstacle path-planning environments.","Ablation studies reveal significant performance improvements.","Our method achieved a 60% time gain in obstacle-free environments compared to baseline algorithms.","Additionally, our findings demonstrated dynamic obstacle avoidance capabilities, adeptly navigating around moving blocks.","These findings highlight the potential of RL to enhance robot motion planning in the challenging and unpredictable SSL environment."],"url":"http://arxiv.org/abs/2404.15410v1","category":"cs.RO"}
{"created":"2024-04-23 18:00:22","title":"Towards efficient Effective One Body models for generic, non-planar orbits","abstract":"Complete waveform models able to account for arbitrary non-planar orbits represent a holy grail in current gravitational-wave astronomy. Here, we take a step towards this direction and present a simple yet efficient prescription to obtain the evolution of the spin vectors and of the orbital angular momentum along non-circularized orbits, that can be applied to any eccentric aligned-spins waveform model. The scheme employed is motivated by insights gained from the post-Newtonian (PN) regime. We investigate the phenomenology of the Euler angles characterizing the time-dependent rotation that connects the co-precessing frame to the inertial one, gauging the importance of non-circular terms in the evolution of the spins of a precessing binary. We demonstrate that such terms are largely negligible, irrespectively of the details of the orbit. Such insights are confirmed by studying the radiation-frame of a few eccentric, precessing numerical relativity (NR) simulations. Our investigations confirm that the usual \"twisting\" technique employed for quasi-spherical systems can be safely applied to non-circularized binaries. By then augmenting a state-of-the-art Effective-One-Body (EOB) model for non-circular planar orbits with the prescription discussed, we obtain an inspiral-merger-ringdown (IMR) model for eccentric, precessing binary black holes (BBHs). We validate the model in the quasi-spherical limit via mismatches and present one phasing comparison against a precessing, eccentric simulation from the RIT catalog.","sentences":["Complete waveform models able to account for arbitrary non-planar orbits represent a holy grail in current gravitational-wave astronomy.","Here, we take a step towards this direction and present a simple yet efficient prescription to obtain the evolution of the spin vectors and of the orbital angular momentum along non-circularized orbits, that can be applied to any eccentric aligned-spins waveform model.","The scheme employed is motivated by insights gained from the post-Newtonian (PN) regime.","We investigate the phenomenology of the Euler angles characterizing the time-dependent rotation that connects the co-precessing frame to the inertial one, gauging the importance of non-circular terms in the evolution of the spins of a precessing binary.","We demonstrate that such terms are largely negligible, irrespectively of the details of the orbit.","Such insights are confirmed by studying the radiation-frame of a few eccentric, precessing numerical relativity (NR) simulations.","Our investigations confirm that the usual \"twisting\" technique employed for quasi-spherical systems can be safely applied to non-circularized binaries.","By then augmenting a state-of-the-art Effective-One-Body (EOB) model for non-circular planar orbits with the prescription discussed, we obtain an inspiral-merger-ringdown (IMR) model for eccentric, precessing binary black holes (BBHs).","We validate the model in the quasi-spherical limit via mismatches and present one phasing comparison against a precessing, eccentric simulation from the RIT catalog."],"url":"http://arxiv.org/abs/2404.15408v1","category":"gr-qc"}
{"created":"2024-04-23 18:00:09","title":"Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs","abstract":"Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.","sentences":["Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality.","As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge.","Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline.","Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues.","We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach."],"url":"http://arxiv.org/abs/2404.15406v1","category":"cs.CV"}
{"created":"2024-04-23 18:00:01","title":"Proof of a Universal Speed Limit on Fast Scrambling in Quantum Systems","abstract":"We prove that the time required for sustained information scrambling in any Hamiltonian quantum system is universally at least logarithmic in the entanglement entropy of scrambled states. This addresses two foundational problems in nonequilibrium quantum dynamics. (1) It sets the earliest possible time for the applicability of equilibrium statistical mechanics in a quantum system coupled to a bath at a finite temperature. (2) It proves a version of the fast scrambling conjecture, originally motivated in models associated with black holes, as a fundamental property of quantum mechanics itself. Our result builds on a refinement of the energy-time uncertainty principle in terms of the infinite temperature spectral form factor in quantum chaos. We generalize this formulation to arbitrary initial states of the bath, including finite temperature states, by mapping Hamiltonian dynamics with any initial state to nonunitary dynamics at infinite temperature. A regularized spectral form factor emerges naturally from this procedure, whose decay is universally constrained by analyticity in complex time. This establishes an exact speed limit on information scrambling by the most general quantum mechanical Hamiltonian, without any restrictions on locality or the nature of interactions.","sentences":["We prove that the time required for sustained information scrambling in any Hamiltonian quantum system is universally at least logarithmic in the entanglement entropy of scrambled states.","This addresses two foundational problems in nonequilibrium quantum dynamics.","(1) It sets the earliest possible time for the applicability of equilibrium statistical mechanics in a quantum system coupled to a bath at a finite temperature.","(2) It proves a version of the fast scrambling conjecture, originally motivated in models associated with black holes, as a fundamental property of quantum mechanics itself.","Our result builds on a refinement of the energy-time uncertainty principle in terms of the infinite temperature spectral form factor in quantum chaos.","We generalize this formulation to arbitrary initial states of the bath, including finite temperature states, by mapping Hamiltonian dynamics with any initial state to nonunitary dynamics at infinite temperature.","A regularized spectral form factor emerges naturally from this procedure, whose decay is universally constrained by analyticity in complex time.","This establishes an exact speed limit on information scrambling by the most general quantum mechanical Hamiltonian, without any restrictions on locality or the nature of interactions."],"url":"http://arxiv.org/abs/2404.15403v1","category":"quant-ph"}
{"created":"2024-04-23 18:00:00","title":"Planet Hunters NGTS: New Planet Candidates from a Citizen Science Search of the Next Generation Transit Survey Public Data","abstract":"We present the results from the first two years of the Planet Hunters NGTS citizen science project, which searches for transiting planet candidates in data from the Next Generation Transit Survey (NGTS) by enlisting the help of members of the general public. Over 8,000 registered volunteers reviewed 138,198 light curves from the NGTS Public Data Releases 1 and 2. We utilize a user weighting scheme to combine the classifications of multiple users to identify the most promising planet candidates not initially discovered by the NGTS team. We highlight the five most interesting planet candidates detected through this search, which are all candidate short-period giant planets. This includes the TIC-165227846 system that, if confirmed, would be the lowest-mass star to host a close-in giant planet. We assess the detection efficiency of the project by determining the number of confirmed planets from the NASA Exoplanet Archive and TESS Objects of Interest (TOIs) successfully recovered by this search and find that 74% of confirmed planets and 63% of TOIs detected by NGTS are recovered by the Planet Hunters NGTS project. The identification of new planet candidates shows that the citizen science approach can provide a complementary method to the detection of exoplanets with ground-based surveys such as NGTS.","sentences":["We present the results from the first two years of the Planet Hunters NGTS citizen science project, which searches for transiting planet candidates in data from the Next Generation Transit Survey (NGTS) by enlisting the help of members of the general public.","Over 8,000 registered volunteers reviewed 138,198 light curves from the NGTS Public Data Releases 1 and 2.","We utilize a user weighting scheme to combine the classifications of multiple users to identify the most promising planet candidates not initially discovered by the NGTS team.","We highlight the five most interesting planet candidates detected through this search, which are all candidate short-period giant planets.","This includes the TIC-165227846 system that, if confirmed, would be the lowest-mass star to host a close-in giant planet.","We assess the detection efficiency of the project by determining the number of confirmed planets from the NASA Exoplanet Archive and TESS Objects of Interest (TOIs) successfully recovered by this search and find that 74% of confirmed planets and 63% of TOIs detected by NGTS are recovered by the Planet Hunters NGTS project.","The identification of new planet candidates shows that the citizen science approach can provide a complementary method to the detection of exoplanets with ground-based surveys such as NGTS."],"url":"http://arxiv.org/abs/2404.15395v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:00:00","title":"Nonthermal Signatures of Radiative Supernova Remnants","abstract":"The end of supernova remnant (SNR) evolution is characterized by a so-called \"radiative\" stage, in which efficient cooling of the hot bubble inside the forward shock slows expansion, leading to eventual shock breakup. Understanding SNR evolution at this stage is vital for predicting feedback in galaxies, since SNRs are expected to deposit their energy and momentum into the interstellar medium at the ends of their lives. A key prediction of SNR evolutionary models is the formation at the onset of the radiative stage of a cold, dense shell behind the forward shock. However, searches for these shells via their neutral hydrogen emission have had limited success. We instead introduce an independent observational signal of shell formation arising from the interaction between nonthermal particles accelerated by the SNR forward shock (cosmic rays) and the dense shell. Using a semi-analytic model of particle acceleration based on state-of-the-art simulations coupled with a high-resolution hydrodynamic model of SNR evolution, we predict the nonthermal emission that arises from this interaction. We demonstrate that the onset of the radiative stage leads to nonthermal signatures from radio to $\\gamma$-rays, including radio and $\\gamma$-ray brightening by nearly two orders of magnitude. Such a signature may be detectable with current instruments, and will be resolvable with the next generation of gamma-ray telescopes (namely, the Cherenkov Telescope Array).","sentences":["The end of supernova remnant (SNR) evolution is characterized by a so-called \"radiative\" stage, in which efficient cooling of the hot bubble inside the forward shock slows expansion, leading to eventual shock breakup.","Understanding SNR evolution at this stage is vital for predicting feedback in galaxies, since SNRs are expected to deposit their energy and momentum into the interstellar medium at the ends of their lives.","A key prediction of SNR evolutionary models is the formation at the onset of the radiative stage of a cold, dense shell behind the forward shock.","However, searches for these shells via their neutral hydrogen emission have had limited success.","We instead introduce an independent observational signal of shell formation arising from the interaction between nonthermal particles accelerated by the SNR forward shock (cosmic rays) and the dense shell.","Using a semi-analytic model of particle acceleration based on state-of-the-art simulations coupled with a high-resolution hydrodynamic model of SNR evolution, we predict the nonthermal emission that arises from this interaction.","We demonstrate that the onset of the radiative stage leads to nonthermal signatures from radio to $\\gamma$-rays, including radio and $\\gamma$-ray brightening by nearly two orders of magnitude.","Such a signature may be detectable with current instruments, and will be resolvable with the next generation of gamma-ray telescopes (namely, the Cherenkov Telescope Array)."],"url":"http://arxiv.org/abs/2404.15396v1","category":"astro-ph.HE"}
{"created":"2024-04-23 18:00:00","title":"Nanoscale sensing of spatial correlations in nonequilibrium current noise","abstract":"Nitrogen-vacancy centers are spatially resolved probes of current noise. So far, current noise sensing with NV centers has primarily been used as a way to probe equilibrium transport coefficients. We develop a framework for computing the spatiotemporal correlations of nonequilibrium current noise in the Boltzmann regime, and apply it to two-dimensional metals in current-biased steady states. We argue that the spatial structure of the noise reveals the nonequilibrium nature of the electron distribution function, and more generally reveals the nature and lifetimes of the excitations responsible for transport. We estimate the visibility of these signatures in near-term experiments.","sentences":["Nitrogen-vacancy centers are spatially resolved probes of current noise.","So far, current noise sensing with NV centers has primarily been used as a way to probe equilibrium transport coefficients.","We develop a framework for computing the spatiotemporal correlations of nonequilibrium current noise in the Boltzmann regime, and apply it to two-dimensional metals in current-biased steady states.","We argue that the spatial structure of the noise reveals the nonequilibrium nature of the electron distribution function, and more generally reveals the nature and lifetimes of the excitations responsible for transport.","We estimate the visibility of these signatures in near-term experiments."],"url":"http://arxiv.org/abs/2404.15398v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 17:11:07","title":"On Generating Cancelable Biometric Template using Reverse of Boolean XOR","abstract":"Cancelable Biometric is repetitive distortion embedded in original Biometric image for keeping it secure from unauthorized access. In this paper, we have generated Cancelable Biometric templates with Reverse Boolean XOR technique. Three different methods have been proposed for generation of Cancelable Biometric templates based on Visual Secret Sharing scheme. In each method, one Secret image and n-1 Cover images are used as: (M1) One original Biometric image (Secret) with n- 1 randomly chosen Gray Cover images (M2) One original Secret image with n-1 Cover images, which are Randomly Permuted version of the original Secret image (M3) One Secret image with n-1 Cover images, both Secret image and Cover images are Randomly Permuted version of original Biometric image. Experiment works have performed on publicly available ORL Face database and IIT Delhi Iris database. The performance of the proposed methods is compared in terms of Co-relation Coefficient (Cr), Mean Square Error (MSE), Mean Absolute Error (MAE), Structural Similarity (SSIM), Peak Signal to Noise Ratio (PSNR), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). It is found that among the three proposed method, M3 generates good quality Cancelable templates and gives best performance in terms of quality. M3 is also better in quantitative terms on ORL dataset while M2 and M3 are comparable on IIT Delhi Iris dataset.","sentences":["Cancelable Biometric is repetitive distortion embedded in original Biometric image for keeping it secure from unauthorized access.","In this paper, we have generated Cancelable Biometric templates with Reverse Boolean XOR technique.","Three different methods have been proposed for generation of Cancelable Biometric templates based on Visual Secret Sharing scheme.","In each method, one Secret image and n-1 Cover images are used as: (M1)","One original Biometric image (Secret) with n- 1 randomly chosen Gray Cover images (M2) One original Secret image with n-1","Cover images, which are Randomly Permuted version of the original Secret image (M3)","One Secret image with n-1","Cover images, both Secret image and Cover images are Randomly Permuted version of original Biometric image.","Experiment works have performed on publicly available ORL Face database and IIT Delhi Iris database.","The performance of the proposed methods is compared in terms of Co-relation Coefficient (Cr), Mean Square Error (MSE), Mean Absolute Error (MAE), Structural Similarity (SSIM), Peak Signal to Noise Ratio (PSNR), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI).","It is found that among the three proposed method, M3 generates good quality Cancelable templates and gives best performance in terms of quality.","M3 is also better in quantitative terms on ORL dataset while M2 and M3 are comparable on IIT Delhi Iris dataset."],"url":"http://arxiv.org/abs/2404.15394v1","category":"eess.IV"}
{"created":"2024-04-23 16:55:45","title":"Na\u00efve Bayes and Random Forest for Crop Yield Prediction","abstract":"This study analyzes crop yield prediction in India from 1997 to 2020, focusing on various crops and key environmental factors. It aims to predict agricultural yields by utilizing advanced machine learning techniques like Linear Regression, Decision Tree, KNN, Na\\\"ive Bayes, K-Mean Clustering, and Random Forest. The models, particularly Na\\\"ive Bayes and Random Forest, demonstrate high effectiveness, as shown through data visualizations. The research concludes that integrating these analytical methods significantly enhances the accuracy and reliability of crop yield predictions, offering vital contributions to agricultural data science.","sentences":["This study analyzes crop yield prediction in India from 1997 to 2020, focusing on various crops and key environmental factors.","It aims to predict agricultural yields by utilizing advanced machine learning techniques like Linear Regression, Decision Tree, KNN, Na\\\"ive Bayes, K-Mean Clustering, and Random Forest.","The models, particularly Na\\\"ive Bayes and Random Forest, demonstrate high effectiveness, as shown through data visualizations.","The research concludes that integrating these analytical methods significantly enhances the accuracy and reliability of crop yield predictions, offering vital contributions to agricultural data science."],"url":"http://arxiv.org/abs/2404.15392v1","category":"cs.LG"}
{"created":"2024-04-23 16:47:23","title":"Adaptive Mechanism Design using Multi-Agent Revealed Preferences","abstract":"This paper constructs an algorithmic framework for adaptively achieving the mechanism design objective, finding a mechanism inducing socially optimal Nash equilibria, without knowledge of the utility functions of the agents. We consider a probing scheme where the designer can iteratively enact mechanisms and observe Nash equilibria responses. We first derive necessary and sufficient conditions, taking the form of linear program feasibility, for the existence of utility functions under which the empirical Nash equilibria responses are socially optimal. Then, we utilize this to construct a loss function with respect to the mechanism, and show that its global minimization occurs at mechanisms under which Nash equilibria system responses are also socially optimal. We develop a simulated annealing-based gradient algorithm, and prove that it converges in probability to this set of global minima, thus achieving adaptive mechanism design.","sentences":["This paper constructs an algorithmic framework for adaptively achieving the mechanism design objective, finding a mechanism inducing socially optimal Nash equilibria, without knowledge of the utility functions of the agents.","We consider a probing scheme where the designer can iteratively enact mechanisms and observe Nash equilibria responses.","We first derive necessary and sufficient conditions, taking the form of linear program feasibility, for the existence of utility functions under which the empirical Nash equilibria responses are socially optimal.","Then, we utilize this to construct a loss function with respect to the mechanism, and show that its global minimization occurs at mechanisms under which Nash equilibria system responses are also socially optimal.","We develop a simulated annealing-based gradient algorithm, and prove that it converges in probability to this set of global minima, thus achieving adaptive mechanism design."],"url":"http://arxiv.org/abs/2404.15391v1","category":"cs.GT"}
{"created":"2024-04-23 16:26:29","title":"Uncertainty in latent representations of variational autoencoders optimized for visual tasks","abstract":"Deep learning methods are increasingly becoming instrumental as modeling tools in computational neuroscience, employing optimality principles to build bridges between neural responses and perception or behavior. Developing models that adequately represent uncertainty is however challenging for deep learning methods, which often suffer from calibration problems. This constitutes a difficulty in particular when modeling cortical circuits in terms of Bayesian inference, beyond single point estimates such as the posterior mean or the maximum a posteriori. In this work we systematically studied uncertainty representations in latent representations of variational auto-encoders (VAEs), both in a perceptual task from natural images and in two other canonical tasks of computer vision, finding a poor alignment between uncertainty and informativeness or ambiguities in the images. We next showed how a novel approach which we call explaining-away variational auto-encoders (EA-VAEs), fixes these issues, producing meaningful reports of uncertainty in a variety of scenarios, including interpolation, image corruption, and even out-of-distribution detection. We show EA-VAEs may prove useful both as models of perception in computational neuroscience and as inference tools in computer vision.","sentences":["Deep learning methods are increasingly becoming instrumental as modeling tools in computational neuroscience, employing optimality principles to build bridges between neural responses and perception or behavior.","Developing models that adequately represent uncertainty is however challenging for deep learning methods, which often suffer from calibration problems.","This constitutes a difficulty in particular when modeling cortical circuits in terms of Bayesian inference, beyond single point estimates such as the posterior mean or the maximum a posteriori.","In this work we systematically studied uncertainty representations in latent representations of variational auto-encoders (VAEs), both in a perceptual task from natural images and in two other canonical tasks of computer vision, finding a poor alignment between uncertainty and informativeness or ambiguities in the images.","We next showed how a novel approach which we call explaining-away variational auto-encoders (EA-VAEs), fixes these issues, producing meaningful reports of uncertainty in a variety of scenarios, including interpolation, image corruption, and even out-of-distribution detection.","We show EA-VAEs may prove useful both as models of perception in computational neuroscience and as inference tools in computer vision."],"url":"http://arxiv.org/abs/2404.15390v1","category":"cs.LG"}
{"created":"2024-04-23 15:05:35","title":"Tunable Entanglement in Cavity-Magnon Optomechanics","abstract":"Cavity optomechanics, providing an inherently nonlinear interaction between photons and phonons, have shown enomerous potential in generating macroscopic quantum entanglement. Here we propose to realize diverse bipartite and tripartite entanglement in cavity-magnon optomechanics. By introducing magnons to standard cavity optomechanics, not only tunable optomechanical entanglement and magnon-magnon entanglement can be achieved, but also flexible tripartite entanglement including magnon-photon-phonon entanglement, magnon-magnon-photon and -phonon entanglement can be generated. Moreover, optimal bipartite and tripartite entanglement can be achieved by tuning parameters. We further show that all entanglement can be enhanced via engineering the magnon-photon coupling,and is proven to be robust against the bath temperature within the survival temperature. Besides, we find that the optomechanical entanglement can be protected or restored by bad magnons with large decay rate, while other entanglement is severely reduced. The results indicate that our proposal provides a novel avenue to explore and control tunable macroscopic quantum effects in hybrid cavity-magnon optomechanics.","sentences":["Cavity optomechanics, providing an inherently nonlinear interaction between photons and phonons, have shown enomerous potential in generating macroscopic quantum entanglement.","Here we propose to realize diverse bipartite and tripartite entanglement in cavity-magnon optomechanics.","By introducing magnons to standard cavity optomechanics, not only tunable optomechanical entanglement and magnon-magnon entanglement can be achieved, but also flexible tripartite entanglement including magnon-photon-phonon entanglement, magnon-magnon-photon and -phonon entanglement can be generated.","Moreover, optimal bipartite and tripartite entanglement can be achieved by tuning parameters.","We further show that all entanglement can be enhanced via engineering the magnon-photon coupling,and is proven to be robust against the bath temperature within the survival temperature.","Besides, we find that the optomechanical entanglement can be protected or restored by bad magnons with large decay rate, while other entanglement is severely reduced.","The results indicate that our proposal provides a novel avenue to explore and control tunable macroscopic quantum effects in hybrid cavity-magnon optomechanics."],"url":"http://arxiv.org/abs/2404.15111v2","category":"quant-ph"}
{"created":"2024-04-23 14:19:36","title":"ML-based identification of the interface regions for coupling local and nonlocal models","abstract":"Local-nonlocal coupling approaches combine the computational efficiency of local models and the accuracy of nonlocal models. However, the coupling process is challenging, requiring expertise to identify the interface between local and nonlocal regions. This study introduces a machine learning-based approach to automatically detect the regions in which the local and nonlocal models should be used in a coupling approach. This identification process uses the loading functions and provides as output the selected model at the grid points. Training is based on datasets of loading functions for which reference coupling configurations are computed using accurate coupled solutions, where accuracy is measured in terms of the relative error between the solution to the coupling approach and the solution to the nonlocal model. We study two approaches that differ from one another in terms of the data structure. The first approach, referred to as the full-domain input data approach, inputs the full load vector and outputs a full label vector. In this case, the classification process is carried out globally. The second approach consists of a window-based approach, where loads are preprocessed and partitioned into windows and the problem is formulated as a node-wise classification approach in which the central point of each window is treated individually. The classification problems are solved via deep learning algorithms based on convolutional neural networks. The performance of these approaches is studied on one-dimensional numerical examples using F1-scores and accuracy metrics. In particular, it is shown that the windowing approach provides promising results, achieving an accuracy of 0.96 and an F1-score of 0.97. These results underscore the potential of the approach to automate coupling processes, leading to more accurate and computationally efficient solutions for material science applications.","sentences":["Local-nonlocal coupling approaches combine the computational efficiency of local models and the accuracy of nonlocal models.","However, the coupling process is challenging, requiring expertise to identify the interface between local and nonlocal regions.","This study introduces a machine learning-based approach to automatically detect the regions in which the local and nonlocal models should be used in a coupling approach.","This identification process uses the loading functions and provides as output the selected model at the grid points.","Training is based on datasets of loading functions for which reference coupling configurations are computed using accurate coupled solutions, where accuracy is measured in terms of the relative error between the solution to the coupling approach and the solution to the nonlocal model.","We study two approaches that differ from one another in terms of the data structure.","The first approach, referred to as the full-domain input data approach, inputs the full load vector and outputs a full label vector.","In this case, the classification process is carried out globally.","The second approach consists of a window-based approach, where loads are preprocessed and partitioned into windows and the problem is formulated as a node-wise classification approach in which the central point of each window is treated individually.","The classification problems are solved via deep learning algorithms based on convolutional neural networks.","The performance of these approaches is studied on one-dimensional numerical examples using F1-scores and accuracy metrics.","In particular, it is shown that the windowing approach provides promising results, achieving an accuracy of 0.96 and an F1-score of 0.97.","These results underscore the potential of the approach to automate coupling processes, leading to more accurate and computationally efficient solutions for material science applications."],"url":"http://arxiv.org/abs/2404.15388v1","category":"cs.LG"}
{"created":"2024-04-23 14:19:13","title":"BotDGT: Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers","abstract":"Detecting social bots has evolved into a pivotal yet intricate task, aimed at combating the dissemination of misinformation and preserving the authenticity of online interactions. While earlier graph-based approaches, which leverage topological structure of social networks, yielded notable outcomes, they overlooked the inherent dynamicity of social networks -- In reality, they largely depicted the social network as a static graph and solely relied on its most recent state. Due to the absence of dynamicity modeling, such approaches are vulnerable to evasion, particularly when advanced social bots interact with other users to camouflage identities and escape detection. To tackle these challenges, we propose BotDGT, a novel framework that not only considers the topological structure, but also effectively incorporates dynamic nature of social network. Specifically, we characterize a social network as a dynamic graph. A structural module is employed to acquire topological information from each historical snapshot. Additionally, a temporal module is proposed to integrate historical context and model the evolving behavior patterns exhibited by social bots and legitimate users. Experimental results demonstrate the superiority of BotDGT against the leading methods that neglected the dynamic nature of social networks in terms of accuracy, recall, and F1-score.","sentences":["Detecting social bots has evolved into a pivotal yet intricate task, aimed at combating the dissemination of misinformation and preserving the authenticity of online interactions.","While earlier graph-based approaches, which leverage topological structure of social networks, yielded notable outcomes, they overlooked the inherent dynamicity of social networks -- In reality, they largely depicted the social network as a static graph and solely relied on its most recent state.","Due to the absence of dynamicity modeling, such approaches are vulnerable to evasion, particularly when advanced social bots interact with other users to camouflage identities and escape detection.","To tackle these challenges, we propose BotDGT, a novel framework that not only considers the topological structure, but also effectively incorporates dynamic nature of social network.","Specifically, we characterize a social network as a dynamic graph.","A structural module is employed to acquire topological information from each historical snapshot.","Additionally, a temporal module is proposed to integrate historical context and model the evolving behavior patterns exhibited by social bots and legitimate users.","Experimental results demonstrate the superiority of BotDGT against the leading methods that neglected the dynamic nature of social networks in terms of accuracy, recall, and F1-score."],"url":"http://arxiv.org/abs/2404.15070v2","category":"cs.SI"}
{"created":"2024-04-23 14:13:31","title":"Machine Learning Applied to the Detection of Mycotoxin in Food: A Review","abstract":"Mycotoxins, toxic secondary metabolites produced by certain fungi, pose significant threats to global food safety and public health. These compounds can contaminate a variety of crops, leading to economic losses and health risks to both humans and animals. Traditional lab analysis methods for mycotoxin detection can be time-consuming and may not always be suitable for large-scale screenings. However, in recent years, machine learning (ML) methods have gained popularity for use in the detection of mycotoxins and in the food safety industry in general, due to their accurate and timely predictions. We provide a systematic review on some of the recent ML applications for detecting/predicting the presence of mycotoxin on a variety of food ingredients, highlighting their advantages, challenges, and potential for future advancements. We address the need for reproducibility and transparency in ML research through open access to data and code. An observation from our findings is the frequent lack of detailed reporting on hyperparameters in many studies as well as a lack of open source code, which raises concerns about the reproducibility and optimisation of the ML models used. The findings reveal that while the majority of studies predominantly utilised neural networks for mycotoxin detection, there was a notable diversity in the types of neural network architectures employed, with convolutional neural networks being the most popular.","sentences":["Mycotoxins, toxic secondary metabolites produced by certain fungi, pose significant threats to global food safety and public health.","These compounds can contaminate a variety of crops, leading to economic losses and health risks to both humans and animals.","Traditional lab analysis methods for mycotoxin detection can be time-consuming and may not always be suitable for large-scale screenings.","However, in recent years, machine learning (ML) methods have gained popularity for use in the detection of mycotoxins and in the food safety industry in general, due to their accurate and timely predictions.","We provide a systematic review on some of the recent ML applications for detecting/predicting the presence of mycotoxin on a variety of food ingredients, highlighting their advantages, challenges, and potential for future advancements.","We address the need for reproducibility and transparency in ML research through open access to data and code.","An observation from our findings is the frequent lack of detailed reporting on hyperparameters in many studies as well as a lack of open source code, which raises concerns about the reproducibility and optimisation of the ML models used.","The findings reveal that while the majority of studies predominantly utilised neural networks for mycotoxin detection, there was a notable diversity in the types of neural network architectures employed, with convolutional neural networks being the most popular."],"url":"http://arxiv.org/abs/2404.15387v1","category":"q-bio.QM"}
{"created":"2024-04-24 02:59:56","title":"Dynamic fault detection and diagnosis for alkaline water electrolyzer with variational Bayesian Sparse principal component analysis","abstract":"Electrolytic hydrogen production serves as not only a vital source of green hydrogen but also a key strategy for addressing renewable energy consumption challenges. For the safe production of hydrogen through alkaline water electrolyzer (AWE), dependable process monitoring technology is essential. However, random noise can easily contaminate the AWE process data collected in industrial settings, presenting new challenges for monitoring methods. In this study, we develop the variational Bayesian sparse principal component analysis (VBSPCA) method for process monitoring. VBSPCA methods based on Gaussian prior and Laplace prior are derived to obtain the sparsity of the projection matrix, which corresponds to $\\ell_2$ regularization and $\\ell_1$ regularization, respectively. The correlation of dynamic latent variables is then analyzed by sparse autoregression and fault variables are diagnosed by fault reconstruction. The effectiveness of the method is verified by an industrial hydrogen production process, and the test results demonstrated that both Gaussian prior and Laplace prior based VBSPCA can effectively detect and diagnose critical faults in AWEs.","sentences":["Electrolytic hydrogen production serves as not only a vital source of green hydrogen but also a key strategy for addressing renewable energy consumption challenges.","For the safe production of hydrogen through alkaline water electrolyzer (AWE), dependable process monitoring technology is essential.","However, random noise can easily contaminate the AWE process data collected in industrial settings, presenting new challenges for monitoring methods.","In this study, we develop the variational Bayesian sparse principal component analysis (VBSPCA) method for process monitoring.","VBSPCA methods based on Gaussian prior and Laplace prior are derived to obtain the sparsity of the projection matrix, which corresponds to $\\ell_2$ regularization and $\\ell_1$ regularization, respectively.","The correlation of dynamic latent variables is then analyzed by sparse autoregression and fault variables are diagnosed by fault reconstruction.","The effectiveness of the method is verified by an industrial hydrogen production process, and the test results demonstrated that both Gaussian prior and Laplace prior based VBSPCA can effectively detect and diagnose critical faults in AWEs."],"url":"http://arxiv.org/abs/2404.15609v1","category":"eess.SY"}
{"created":"2024-04-23 21:40:25","title":"The Ability of Virtual Reality Technologies to Improve Comprehension of Speech Therapy Device Training","abstract":"This study evaluates the usage of virtual reality (VR) technologies as a teaching tool in oral placement therapy, a subset of speech therapy. The researcher distributed instructional videos using traditional lecture and modified three-dimensional video to prompt responses. Data was gathered with a two-part Google Form: In \"Section 1: Knowledge Test\" participants were asked to determine how well they received the information displayed to them. In \"Section 2: Opinion Test\" participants were asked diagnostic and subjective questions via Likert scale ranging from 1 (\"Strongly Disagree\") to 5 (\"Strongly Agree\") to determine how well they enjoyed viewing the information displayed to them. Averages for Section 1 were 92.00% for the control group (viewing 2D, unmodified video) and 77.88% for the experimental group (viewing 3D, VR video). Almost all participants answered at least 60% of the questions correctly. Averages for 2D and 3D participants were 4.53/5 and 3.82/5, respectively for \"positive\" prompts. Exactly 50% of participants experiencing VR video preferred the method to a traditional lecture. This study determines that virtual reality is viable as a learning tool, but knowledge obtained is not necessarily as high as using traditional lecture. Further experimentation is required to determine how well oral placement therapists respond to physically interacting with a model instead of only viewing it. Copies of the Google Form used to collect responses, all raw data, and a flowchart outlining each step used to construct the 3D video can be found in the Appendix.","sentences":["This study evaluates the usage of virtual reality (VR) technologies as a teaching tool in oral placement therapy, a subset of speech therapy.","The researcher distributed instructional videos using traditional lecture and modified three-dimensional video to prompt responses.","Data was gathered with a two-part Google Form: In \"Section 1: Knowledge Test\" participants were asked to determine how well they received the information displayed to them.","In \"Section 2: Opinion Test\" participants were asked diagnostic and subjective questions via Likert scale ranging from 1 (\"Strongly Disagree\") to 5 (\"Strongly Agree\") to determine how well they enjoyed viewing the information displayed to them.","Averages for Section 1 were 92.00% for the control group (viewing 2D, unmodified video) and 77.88% for the experimental group (viewing 3D, VR video).","Almost all participants answered at least 60% of the questions correctly.","Averages for 2D and 3D participants were 4.53/5 and 3.82/5, respectively for \"positive\" prompts.","Exactly 50% of participants experiencing VR video preferred the method to a traditional lecture.","This study determines that virtual reality is viable as a learning tool, but knowledge obtained is not necessarily as high as using traditional lecture.","Further experimentation is required to determine how well oral placement therapists respond to physically interacting with a model instead of only viewing it.","Copies of the Google Form used to collect responses, all raw data, and a flowchart outlining each step used to construct the 3D video can be found in the Appendix."],"url":"http://arxiv.org/abs/2404.15534v1","category":"cs.HC"}
{"created":"2024-04-23 20:47:20","title":"Joint Soil and Above-Ground Biomass Characterization Using Radars","abstract":"Soil moisture sensing through biomass or vegetation canopy has challenged researchers, even those who use SAR sensors with penetration capabilities. This is mainly due to the imposed extra time and phase offsets on Radio Frequency (RF) signals as they travel through the canopy. These offsets depend on the vegetation canopy moisture and height, both of which are typically unknown in agricultural and forest fields. In this paper, we leverage the mobility of an unmanned aerial system (UAS) to collect spatially-diverse radar measurements, enabling the joint estimation of soil moisture, above-ground biomass moisture, and biomass height, all without assuming any calibration steps. We leverage the changes in time-of-flight (ToF) and angle-of-arrival (AoA) measurements of reflected radar signals as the UAS flies above a reflector buried under the soil. We demonstrate the effectiveness of our algorithm by simulating its performance under realistic measurement noises as well as conducting lab experiments with different types of above-ground biomass. Our simulation results conclude that our algorithm is capable of estimating volumetric soil moisture to less than 1% median absolute error (MAE), vegetation height to 11.1cm MAE, and vegetation relative permittivity to 0.32 MAE. Our experimental results demonstrate the effectiveness of the proposed method in practical scenarios for varying biomass moistures and heights.","sentences":["Soil moisture sensing through biomass or vegetation canopy has challenged researchers, even those who use SAR sensors with penetration capabilities.","This is mainly due to the imposed extra time and phase offsets on Radio Frequency (RF) signals as they travel through the canopy.","These offsets depend on the vegetation canopy moisture and height, both of which are typically unknown in agricultural and forest fields.","In this paper, we leverage the mobility of an unmanned aerial system (UAS) to collect spatially-diverse radar measurements, enabling the joint estimation of soil moisture, above-ground biomass moisture, and biomass height, all without assuming any calibration steps.","We leverage the changes in time-of-flight (ToF) and angle-of-arrival (AoA) measurements of reflected radar signals as the UAS flies above a reflector buried under the soil.","We demonstrate the effectiveness of our algorithm by simulating its performance under realistic measurement noises as well as conducting lab experiments with different types of above-ground biomass.","Our simulation results conclude that our algorithm is capable of estimating volumetric soil moisture to less than 1% median absolute error (MAE), vegetation height to 11.1cm MAE, and vegetation relative permittivity to 0.32 MAE.","Our experimental results demonstrate the effectiveness of the proposed method in practical scenarios for varying biomass moistures and heights."],"url":"http://arxiv.org/abs/2404.15508v1","category":"eess.SP"}
{"created":"2024-04-23 20:15:12","title":"Correlations versus noise in the NFT market","abstract":"The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. To deepen the understanding of the dynamics of this market, in the current study, based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform, the degree of correlation in this market is examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized.","sentences":["The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market.","To deepen the understanding of the dynamics of this market, in the current study, based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform, the degree of correlation in this market is examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix.","It appears that correlation strength is lower here than that observed in previously studied markets.","Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain.","The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations.","Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized."],"url":"http://arxiv.org/abs/2404.15495v1","category":"q-fin.ST"}
{"created":"2024-04-23 19:21:08","title":"Understanding Robot Minds: Leveraging Machine Teaching for Transparent Human-Robot Collaboration Across Diverse Groups","abstract":"In this work, we aim to improve transparency and efficacy in human-robot collaboration by developing machine teaching algorithms suitable for groups with varied learning capabilities. While previous approaches focused on tailored approaches for teaching individuals, our method teaches teams with various compositions of diverse learners using team belief representations to address personalization challenges within groups. We investigate various group teaching strategies, such as focusing on individual beliefs or the group's collective beliefs, and assess their impact on learning robot policies for different team compositions. Our findings reveal that team belief strategies yield less variation in learning duration and better accommodate diverse teams compared to individual belief strategies, suggesting their suitability in mixed-proficiency settings with limited resources. Conversely, individual belief strategies provide a more uniform knowledge level, particularly effective for homogeneously inexperienced groups. Our study indicates that the teaching strategy's efficacy is significantly influenced by team composition and learner proficiency, highlighting the importance of real-time assessment of learner proficiency and adapting teaching approaches based on learner proficiency for optimal teaching outcomes.","sentences":["In this work, we aim to improve transparency and efficacy in human-robot collaboration by developing machine teaching algorithms suitable for groups with varied learning capabilities.","While previous approaches focused on tailored approaches for teaching individuals, our method teaches teams with various compositions of diverse learners using team belief representations to address personalization challenges within groups.","We investigate various group teaching strategies, such as focusing on individual beliefs or the group's collective beliefs, and assess their impact on learning robot policies for different team compositions.","Our findings reveal that team belief strategies yield less variation in learning duration and better accommodate diverse teams compared to individual belief strategies, suggesting their suitability in mixed-proficiency settings with limited resources.","Conversely, individual belief strategies provide a more uniform knowledge level, particularly effective for homogeneously inexperienced groups.","Our study indicates that the teaching strategy's efficacy is significantly influenced by team composition and learner proficiency, highlighting the importance of real-time assessment of learner proficiency and adapting teaching approaches based on learner proficiency for optimal teaching outcomes."],"url":"http://arxiv.org/abs/2404.15472v1","category":"cs.RO"}
{"created":"2024-04-23 18:25:05","title":"Introduction to Eye Tracking: A Hands-On Tutorial for Students and Practitioners","abstract":"Eye-tracking technology is widely used in various application areas such as psychology, neuroscience, marketing, and human-computer interaction, as it is a valuable tool for understanding how people process information and interact with their environment. This tutorial provides a comprehensive introduction to eye tracking, from the basics of eye anatomy and physiology to the principles and applications of different eye-tracking systems. The guide is designed to provide a hands-on learning experience for everyone interested in working with eye-tracking technology. Therefore, we include practical case studies to teach students and professionals how to effectively set up and operate an eye-tracking system. The tutorial covers a variety of eye-tracking systems, calibration techniques, data collection, and analysis methods, including fixations, saccades, pupil diameter, and visual scan path analysis. In addition, we emphasize the importance of considering ethical aspects when conducting eye-tracking research and experiments, especially informed consent and participant privacy. We aim to give the reader a solid understanding of basic eye-tracking principles and the practical skills needed to conduct their experiments. Python-based code snippets and illustrative examples are included in the tutorials and can be downloaded at: https://gitlab.lrz.de/hctl/Eye-Tracking-Tutorial.","sentences":["Eye-tracking technology is widely used in various application areas such as psychology, neuroscience, marketing, and human-computer interaction, as it is a valuable tool for understanding how people process information and interact with their environment.","This tutorial provides a comprehensive introduction to eye tracking, from the basics of eye anatomy and physiology to the principles and applications of different eye-tracking systems.","The guide is designed to provide a hands-on learning experience for everyone interested in working with eye-tracking technology.","Therefore, we include practical case studies to teach students and professionals how to effectively set up and operate an eye-tracking system.","The tutorial covers a variety of eye-tracking systems, calibration techniques, data collection, and analysis methods, including fixations, saccades, pupil diameter, and visual scan path analysis.","In addition, we emphasize the importance of considering ethical aspects when conducting eye-tracking research and experiments, especially informed consent and participant privacy.","We aim to give the reader a solid understanding of basic eye-tracking principles and the practical skills needed to conduct their experiments.","Python-based code snippets and illustrative examples are included in the tutorials and can be downloaded at: https://gitlab.lrz.de/hctl/Eye-Tracking-Tutorial."],"url":"http://arxiv.org/abs/2404.15435v1","category":"cs.HC"}
{"created":"2024-04-23 11:58:40","title":"Large-Scale Multipurpose Benchmark Datasets For Assessing Data-Driven Deep Learning Approaches For Water Distribution Networks","abstract":"Currently, the number of common benchmark datasets that researchers can use straight away for assessing data-driven deep learning approaches is very limited. Most studies provide data as configuration files. It is still up to each practitioner to follow a particular data generation method and run computationally intensive simulations to obtain usable data for model training and evaluation. In this work, we provide a collection of datasets that includes several small and medium size publicly available Water Distribution Networks (WDNs), including Anytown, Modena, Balerma, C-Town, D-Town, L-Town, Ky1, Ky6, Ky8, and Ky13. In total 1,394,400 hours of WDNs data operating under normal conditions is made available to the community.","sentences":["Currently, the number of common benchmark datasets that researchers can use straight away for assessing data-driven deep learning approaches is very limited.","Most studies provide data as configuration files.","It is still up to each practitioner to follow a particular data generation method and run computationally intensive simulations to obtain usable data for model training and evaluation.","In this work, we provide a collection of datasets that includes several small and medium size publicly available Water Distribution Networks (WDNs), including Anytown, Modena, Balerma, C-Town, D-Town, L-Town, Ky1, Ky6, Ky8, and Ky13.","In total 1,394,400 hours of WDNs data operating under normal conditions is made available to the community."],"url":"http://arxiv.org/abs/2404.15386v1","category":"cs.LG"}
{"created":"2024-04-23 10:59:44","title":"Sum of Group Error Differences: A Critical Examination of Bias Evaluation in Biometric Verification and a Dual-Metric Measure","abstract":"Biometric Verification (BV) systems often exhibit accuracy disparities across different demographic groups, leading to biases in BV applications. Assessing and quantifying these biases is essential for ensuring the fairness of BV systems. However, existing bias evaluation metrics in BV have limitations, such as focusing exclusively on match or non-match error rates, overlooking bias on demographic groups with performance levels falling between the best and worst performance levels, and neglecting the magnitude of the bias present.   This paper presents an in-depth analysis of the limitations of current bias evaluation metrics in BV and, through experimental analysis, demonstrates their contextual suitability, merits, and limitations. Additionally, it introduces a novel general-purpose bias evaluation measure for BV, the ``Sum of Group Error Differences (SEDG)''. Our experimental results on controlled synthetic datasets demonstrate the effectiveness of demographic bias quantification when using existing metrics and our own proposed measure. We discuss the applicability of the bias evaluation metrics in a set of simulated demographic bias scenarios and provide scenario-based metric recommendations. Our code is publicly available under \\url{https://github.com/alaaobeid/SEDG}.","sentences":["Biometric Verification (BV) systems often exhibit accuracy disparities across different demographic groups, leading to biases in BV applications.","Assessing and quantifying these biases is essential for ensuring the fairness of BV systems.","However, existing bias evaluation metrics in BV have limitations, such as focusing exclusively on match or non-match error rates, overlooking bias on demographic groups with performance levels falling between the best and worst performance levels, and neglecting the magnitude of the bias present.   ","This paper presents an in-depth analysis of the limitations of current bias evaluation metrics in BV and, through experimental analysis, demonstrates their contextual suitability, merits, and limitations.","Additionally, it introduces a novel general-purpose bias evaluation measure for BV, the ``Sum of Group Error Differences (SEDG)''.","Our experimental results on controlled synthetic datasets demonstrate the effectiveness of demographic bias quantification when using existing metrics and our own proposed measure.","We discuss the applicability of the bias evaluation metrics in a set of simulated demographic bias scenarios and provide scenario-based metric recommendations.","Our code is publicly available under \\url{https://github.com/alaaobeid/SEDG}."],"url":"http://arxiv.org/abs/2404.15385v1","category":"cs.CV"}
{"created":"2024-04-23 10:50:38","title":"FL-TAC: Enhanced Fine-Tuning in Federated Learning via Low-Rank, Task-Specific Adapter Clustering","abstract":"Although large-scale pre-trained models hold great potential for adapting to downstream tasks through fine-tuning, the performance of such fine-tuned models is often limited by the difficulty of collecting sufficient high-quality, task-specific data. Federated Learning (FL) offers a promising solution by enabling fine-tuning across large-scale clients with a variety of task data, but it is bottlenecked by significant communication overhead due to the pre-trained models' extensive size. This paper addresses the high communication cost for fine-tuning large pre-trained models within FL frameworks through low-rank fine-tuning. Specifically, we train a low-rank adapter for each individual task on the client side, followed by server-side clustering for similar group of adapters to achieve task-specific aggregation. Extensive experiments on various language and vision tasks, such as GLUE and CIFAR-10/100, reveal the evolution of task-specific adapters throughout the FL training process and verify the effectiveness of the proposed low-rank task-specific adapter clustering (TAC) method.","sentences":["Although large-scale pre-trained models hold great potential for adapting to downstream tasks through fine-tuning, the performance of such fine-tuned models is often limited by the difficulty of collecting sufficient high-quality, task-specific data.","Federated Learning (FL) offers a promising solution by enabling fine-tuning across large-scale clients with a variety of task data, but it is bottlenecked by significant communication overhead due to the pre-trained models' extensive size.","This paper addresses the high communication cost for fine-tuning large pre-trained models within FL frameworks through low-rank fine-tuning.","Specifically, we train a low-rank adapter for each individual task on the client side, followed by server-side clustering for similar group of adapters to achieve task-specific aggregation.","Extensive experiments on various language and vision tasks, such as GLUE and CIFAR-10/100, reveal the evolution of task-specific adapters throughout the FL training process and verify the effectiveness of the proposed low-rank task-specific adapter clustering (TAC) method."],"url":"http://arxiv.org/abs/2404.15384v1","category":"cs.LG"}
{"created":"2024-04-23 10:20:17","title":"WANDR: Intention-guided Human Motion Generation","abstract":"Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de.","sentences":["Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications.","Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness.","A primary obstacle is the scarcity of training data that combines locomotion with goal reaching.","To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location.","To solve this, we introduce novel intention features that drive rich goal-oriented movement.","Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path.","Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not.","WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets.","We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations.","Our models and code are available for research purposes at wandr.is.tue.mpg.de."],"url":"http://arxiv.org/abs/2404.15383v1","category":"cs.CV"}
{"created":"2024-04-23 10:15:10","title":"Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion Detection","abstract":"In recent years, there has been a growing interest in using Machine Learning (ML), especially Deep Learning (DL) to solve Network Intrusion Detection (NID) problems. However, the feature distribution shift problem remains a difficulty, because the change in features' distributions over time negatively impacts the model's performance. As one promising solution, model pretraining has emerged as a novel training paradigm, which brings robustness against feature distribution shift and has proven to be successful in Computer Vision (CV) and Natural Language Processing (NLP). To verify whether this paradigm is beneficial for NID problem, we propose SwapCon, a ML model in the context of NID, which compresses shift-invariant feature information during the pretraining stage and refines during the finetuning stage. We exemplify the evidence of feature distribution shift using the Kyoto2006+ dataset. We demonstrate how pretraining a model with the proper size can increase robustness against feature distribution shifts by over 8%. Moreover, we show how an adequate numerical embedding strategy also enhances the performance of pretrained models. Further experiments show that the proposed SwapCon model also outperforms eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor (KNN) based models by a large margin.","sentences":["In recent years, there has been a growing interest in using Machine Learning (ML), especially Deep Learning (DL) to solve Network Intrusion Detection (NID) problems.","However, the feature distribution shift problem remains a difficulty, because the change in features' distributions over time negatively impacts the model's performance.","As one promising solution, model pretraining has emerged as a novel training paradigm, which brings robustness against feature distribution shift and has proven to be successful in Computer Vision (CV) and Natural Language Processing (NLP).","To verify whether this paradigm is beneficial for NID problem, we propose SwapCon, a ML model in the context of NID, which compresses shift-invariant feature information during the pretraining stage and refines during the finetuning stage.","We exemplify the evidence of feature distribution shift using the Kyoto2006+ dataset.","We demonstrate how pretraining a model with the proper size can increase robustness against feature distribution shifts by over 8%.","Moreover, we show how an adequate numerical embedding strategy also enhances the performance of pretrained models.","Further experiments show that the proposed SwapCon model also outperforms eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor (KNN) based models by a large margin."],"url":"http://arxiv.org/abs/2404.15382v1","category":"cs.LG"}
{"created":"2024-04-23 09:44:58","title":"Advances and Open Challenges in Federated Learning with Foundation Models","abstract":"The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI), offering enhanced capabilities while addressing concerns of privacy, data decentralization, and computational efficiency. This paper provides a comprehensive survey of the emerging field of Federated Foundation Models (FedFM), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of foundation models. A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization. Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed. Moreover, the paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL, highlighting the potential of quantum computing to revolutionize the training, inference, optimization and data encryption processes. This survey underscores the importance of further research to propel innovation in FedFM, emphasizing the need for developing trustworthy solutions. It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field.","sentences":["The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI), offering enhanced capabilities while addressing concerns of privacy, data decentralization, and computational efficiency.","This paper provides a comprehensive survey of the emerging field of Federated Foundation Models (FedFM), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of foundation models.","A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization.","Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed.","Moreover, the paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL, highlighting the potential of quantum computing to revolutionize the training, inference, optimization and data encryption processes.","This survey underscores the importance of further research to propel innovation in FedFM, emphasizing the need for developing trustworthy solutions.","It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field."],"url":"http://arxiv.org/abs/2404.15381v1","category":"cs.LG"}
{"created":"2024-04-23 09:42:45","title":"ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model","abstract":"Generating trajectory data is among promising solutions to addressing privacy concerns, collection costs, and proprietary restrictions usually associated with human mobility analyses. However, existing trajectory generation methods are still in their infancy due to the inherent diversity and unpredictability of human activities, grappling with issues such as fidelity, flexibility, and generalizability. To overcome these obstacles, we propose ControlTraj, a Controllable Trajectory generation framework with the topology-constrained diffusion model. Distinct from prior approaches, ControlTraj utilizes a diffusion model to generate high-fidelity trajectories while integrating the structural constraints of road network topology to guide the geographical outcomes. Specifically, we develop a novel road segment autoencoder to extract fine-grained road segment embedding. The encoded features, along with trip attributes, are subsequently merged into the proposed geographic denoising UNet architecture, named GeoUNet, to synthesize geographic trajectories from white noise. Through experimentation across three real-world data settings, ControlTraj demonstrates its ability to produce human-directed, high-fidelity trajectory generation with adaptability to unexplored geographical contexts.","sentences":["Generating trajectory data is among promising solutions to addressing privacy concerns, collection costs, and proprietary restrictions usually associated with human mobility analyses.","However, existing trajectory generation methods are still in their infancy due to the inherent diversity and unpredictability of human activities, grappling with issues such as fidelity, flexibility, and generalizability.","To overcome these obstacles, we propose ControlTraj, a Controllable Trajectory generation framework with the topology-constrained diffusion model.","Distinct from prior approaches, ControlTraj utilizes a diffusion model to generate high-fidelity trajectories while integrating the structural constraints of road network topology to guide the geographical outcomes.","Specifically, we develop a novel road segment autoencoder to extract fine-grained road segment embedding.","The encoded features, along with trip attributes, are subsequently merged into the proposed geographic denoising UNet architecture, named GeoUNet, to synthesize geographic trajectories from white noise.","Through experimentation across three real-world data settings, ControlTraj demonstrates its ability to produce human-directed, high-fidelity trajectory generation with adaptability to unexplored geographical contexts."],"url":"http://arxiv.org/abs/2404.15380v1","category":"cs.LG"}
{"created":"2024-04-23 07:16:13","title":"Clustering of timed sequences -- Application to the analysis of care pathways","abstract":"Improving the future of healthcare starts by better understanding the current actual practices in hospitals. This motivates the objective of discovering typical care pathways from patient data. Revealing homogeneous groups of care pathways can be achieved through clustering. The difficulty in clustering care pathways, represented by sequences of timestamped events, lies in defining a semantically appropriate metric and clustering algorithms.   In this article, we adapt two methods developed for time series to time sequences: the drop-DTW metric and the DBA approach for the construction of averaged time sequences. These methods are then applied in clustering algorithms to propose original and sound clustering algorithms for timed sequences.   This approach is experimented with and evaluated on synthetic and real use cases.","sentences":["Improving the future of healthcare starts by better understanding the current actual practices in hospitals.","This motivates the objective of discovering typical care pathways from patient data.","Revealing homogeneous groups of care pathways can be achieved through clustering.","The difficulty in clustering care pathways, represented by sequences of timestamped events, lies in defining a semantically appropriate metric and clustering algorithms.   ","In this article, we adapt two methods developed for time series to time sequences: the drop-DTW metric and the DBA approach for the construction of averaged time sequences.","These methods are then applied in clustering algorithms to propose original and sound clustering algorithms for timed sequences.   ","This approach is experimented with and evaluated on synthetic and real use cases."],"url":"http://arxiv.org/abs/2404.15379v1","category":"cs.LG"}
{"created":"2024-04-23 03:04:22","title":"Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions","abstract":"Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison.","sentences":["Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability.","However, the SW and the GSW are only defined between distributions supported on a homogeneous domain.","This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains.","Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set.","To address the issue, we propose two new slicing operators i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT).","In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments.","By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions.","We then discuss the topological, statistical, and computational properties of H2SW.","Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison."],"url":"http://arxiv.org/abs/2404.15378v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:36","title":"CrossScore: Towards Multi-View Image Evaluation and Scoring","abstract":"We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references.","sentences":["We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore.","Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references.","By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable.","Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references."],"url":"http://arxiv.org/abs/2404.14409v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:29","title":"SpaceByte: Towards Deleting Tokenization from Large Language Modeling","abstract":"Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.","sentences":["Tokenization is widely used in large language models because it significantly improves performance.","However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity.","To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling.","SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers.","We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries.","Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures."],"url":"http://arxiv.org/abs/2404.14408v1","category":"cs.CL"}
{"created":"2024-04-22 17:55:56","title":"PARAMANU-GANITA: Language Model with Mathematical Capabilities","abstract":"In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics. The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus. We evaluate our model on both perplexity metric and GSM8k mathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively. Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively. The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters. Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation. Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end. In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model.","sentences":["In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics.","The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus.","We evaluate our model on both perplexity metric and GSM8k mathematical benchmark.","Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively.","Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1","33B by 3.8% points and Vicuna 13B by 11.8% points respectively.","The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters.","Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent.","Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation.","Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end.","In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model."],"url":"http://arxiv.org/abs/2404.14395v1","category":"cs.CL"}
{"created":"2024-04-22 17:55:11","title":"A Multimodal Automated Interpretability Agent","abstract":"This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.","sentences":["This paper describes MAIA, a Multimodal Automated Interpretability Agent.","MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery.","It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior.","These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results.","Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior.","We evaluate applications of MAIA to computer vision models.","We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images.","Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters.","We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified."],"url":"http://arxiv.org/abs/2404.14394v1","category":"cs.AI"}
{"created":"2024-04-22 17:43:23","title":"A Survey on Self-Evolution of Large Language Models","abstract":"Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs.","sentences":["Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications.","However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.","To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing.","This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence.","In this work, we present a comprehensive survey of self-evolution approaches in LLMs.","We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation.","Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module.","Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs."],"url":"http://arxiv.org/abs/2404.14387v1","category":"cs.CL"}
{"created":"2024-04-22 17:28:52","title":"The Life and Legacy of Bui Tuong Phong","abstract":"We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year. We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts. Crucially, we present evidence that his name has been cited incorrectly over the last five decades. His family name appears to be B\\`ui, not Phong. By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future.","sentences":["We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year.","We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts.","Crucially, we present evidence that his name has been cited incorrectly over the last five decades.","His family name appears to be B\\`ui, not Phong.","By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future."],"url":"http://arxiv.org/abs/2404.14376v1","category":"cs.GR"}
{"created":"2024-04-22 17:24:08","title":"On the incidence rate of RR Lyrae stars with non-radial modes","abstract":"Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars. These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61. The incidence rate of this phenomenon changes from population to population. It is also strongly dependent on the quality of the analyzed data. Current models explaining these additional signals involve non-radial modes of degrees 8 and 9. Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities. We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models. We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population. To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS. The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity. We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations. Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes.","sentences":["Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars.","These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61.","The incidence rate of this phenomenon changes from population to population.","It is also strongly dependent on the quality of the analyzed data.","Current models explaining these additional signals involve non-radial modes of degrees 8 and 9.","Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities.","We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models.","We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population.","To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS.","The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity.","We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations.","Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes."],"url":"http://arxiv.org/abs/2404.14373v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:22:31","title":"Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph","abstract":"Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs. Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph.","sentences":["Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs).","However, it can fall short in specific scenarios where simple customized methods excel.","In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data.","Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up.","Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text.","As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction.","Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly.","We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.","Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph."],"url":"http://arxiv.org/abs/2404.14372v1","category":"cs.CL"}
{"created":"2024-04-22 17:21:24","title":"Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation","abstract":"The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely.","sentences":["The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes.","This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files.","In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams.","We used 3 different prompts for each input, and we manually evaluated the results.","We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code.","On average, the model was able to generate source code for 88% of the elements shown in the diagrams.","Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files.","However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams.","In summary, further investigations are necessary to exploit the model's potential completely."],"url":"http://arxiv.org/abs/2404.14370v1","category":"cs.SE"}
{"created":"2024-04-22 17:20:38","title":"Graphic Design with Large Multimodal Model","abstract":"In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design. One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements. It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload. In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements. To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models. Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element. We develop new evaluation metrics for HLG. Graphist outperforms prior arts and establishes a strong baseline for this field. Project homepage: https://github.com/graphic-design-ai/graphist","sentences":["In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design.","One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements.","It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload.","In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements.","To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models.","Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element.","We develop new evaluation metrics for HLG.","Graphist outperforms prior arts and establishes a strong baseline for this field.","Project homepage: https://github.com/graphic-design-ai/graphist"],"url":"http://arxiv.org/abs/2404.14368v1","category":"cs.CV"}
{"created":"2024-04-22 17:20:18","title":"Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data","abstract":"Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.","sentences":["Learning from preference labels plays a crucial role in fine-tuning large language models.","There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning.","Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient.","This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why?","In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems.","Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives.","We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions.","Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively.","Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement."],"url":"http://arxiv.org/abs/2404.14367v2","category":"cs.LG"}
{"created":"2024-04-22 17:07:25","title":"Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the Calculator Improves Numeracy in Language Models","abstract":"Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Our code and data are available at https://github.com/calc-cmu/pre-calc.","sentences":["Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models.","While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders.","In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively.","We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.","Our code and data are available at https://github.com/calc-cmu/pre-calc."],"url":"http://arxiv.org/abs/2404.14355v1","category":"cs.CL"}
{"created":"2024-04-22 17:02:33","title":"Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer","abstract":"We address the task of estimating camera parameters from a set of images depicting a scene. Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud. We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction. This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching. We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images. Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images. Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis. Project page: https://nianticlabs.github.io/acezero/","sentences":["We address the task of estimating camera parameters from a set of images depicting a scene.","Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud.","We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction.","This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching.","We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images.","Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images.","Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis.","Project page: https://nianticlabs.github.io/acezero/"],"url":"http://arxiv.org/abs/2404.14351v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:57","title":"Automatic Discovery of Visual Circuits","abstract":"To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor. We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept. We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity. We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks.","sentences":["To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor.","We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept.","We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity.","We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks."],"url":"http://arxiv.org/abs/2404.14349v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:31","title":"Machine Learning in Viscoelastic Fluids via Energy-Based Kernel Embedding","abstract":"The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles. In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models. To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy. Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning. Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations. Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers. Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions. Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems.","sentences":["The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles.","In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models.","To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy.","Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning.","Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations.","Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers.","Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions.","Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems."],"url":"http://arxiv.org/abs/2404.14347v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 16:47:10","title":"Full Event Particle-Level Unfolding with Variable-Length Latent Variational Diffusion","abstract":"The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions. One approach, unfolding, statistically adjusts the experimental data for detector effects. Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions. However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data. A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces. The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider.","sentences":["The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions.","One approach, unfolding, statistically adjusts the experimental data for detector effects.","Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions.","However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data.","A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces.","The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider."],"url":"http://arxiv.org/abs/2404.14332v1","category":"hep-ex"}
{"created":"2024-04-22 16:38:38","title":"Adapting to time: why nature evolved a diverse set of neurons","abstract":"Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information. In addition, it is known empirically that spike timing is a significant factor in neural computations. However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases. In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns. In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure. Indeed, when adaptation was restricted to weights, networks were unable to solve most problems. We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design.","sentences":["Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information.","In addition, it is known empirically that spike timing is a significant factor in neural computations.","However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases.","In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns.","In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure.","Indeed, when adaptation was restricted to weights, networks were unable to solve most problems.","We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design."],"url":"http://arxiv.org/abs/2404.14325v1","category":"cs.NE"}
{"created":"2024-04-22 16:02:48","title":"Explaining Arguments' Strength: Unveiling the Role of Attacks and Supports (Technical Report)","abstract":"Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention. Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments. These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength. In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength. We show that RAEs satisfy several desirable properties. We also propose a probabilistic algorithm to approximate RAEs efficiently. Finally, we show the application value of RAEs in fraud detection and large language models case studies.","sentences":["Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention.","Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments.","These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength.","In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength.","We show that RAEs satisfy several desirable properties.","We also propose a probabilistic algorithm to approximate RAEs efficiently.","Finally, we show the application value of RAEs in fraud detection and large language models case studies."],"url":"http://arxiv.org/abs/2404.14304v1","category":"cs.AI"}
{"created":"2024-04-22 16:00:46","title":"Marking: Visual Grading with Highlighting Errors and Annotating Missing Bits","abstract":"In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights. Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers. We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task. We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing. The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively. We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers. Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset. We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study. Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future. The code and dataset can be found at https://github.com/luffycodes/marking.","sentences":["In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights.","Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers.","We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task.","We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing.","The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively.","We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers.","Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset.","We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study.","Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future.","The code and dataset can be found at https://github.com/luffycodes/marking."],"url":"http://arxiv.org/abs/2404.14301v1","category":"cs.CL"}
{"created":"2024-04-22 15:54:53","title":"Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach","abstract":"Recent years have witnessed significant progress in developing deep learning-based models for automated code completion. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.","sentences":["Recent years have witnessed significant progress in developing deep learning-based models for automated code completion.","Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement.","In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model?","To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion.","In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior.","The acquired posteriors from these shadow models are subsequently employed to train a membership classifier.","Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model.","We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder).","Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively.","Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement.","Finally, we also try to explain the findings from the perspective of model memorization."],"url":"http://arxiv.org/abs/2404.14296v1","category":"cs.SE"}
{"created":"2024-04-22 15:53:08","title":"A Survey on Efficient Inference for Large Language Models","abstract":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","sentences":["Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks.","However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios.","Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference.","This paper presents a comprehensive survey of the existing literature on efficient LLM inference.","We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach.","Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization.","Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights.","Last but not least, we provide some knowledge summary and discuss future research directions."],"url":"http://arxiv.org/abs/2404.14294v1","category":"cs.CL"}
{"created":"2024-04-22 15:42:50","title":"Methodological Reconstruction of Historical Landslide Tsunamis Using Bayesian Inference","abstract":"Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards. Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region. Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis. This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing. After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts. However, the most viable landslides may push the boundaries of geological plausibility. Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event.","sentences":["Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards.","Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region.","Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   ","Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis.","This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing.","After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts.","However, the most viable landslides may push the boundaries of geological plausibility.","Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event."],"url":"http://arxiv.org/abs/2404.14288v1","category":"physics.geo-ph"}
{"created":"2024-04-22 15:35:33","title":"LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots","abstract":"Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.","sentences":["Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities.","However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences.","We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics.","Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations.","The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller.","Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner.","In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences.","We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences.","Project page: https://donggehan.github.io/projectllmpersonalize/."],"url":"http://arxiv.org/abs/2404.14285v1","category":"cs.RO"}
{"created":"2024-04-22 15:29:19","title":"RESFM: Robust Equivariant Multiview Structure from Motion","abstract":"Multiview Structure from Motion is a fundamental and challenging computer vision problem. A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections. This work however made the unrealistic assumption that the point tracks given as input are clean of outliers. Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step. Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers.","sentences":["Multiview Structure from Motion is a fundamental and challenging computer vision problem.","A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections.","This work however made the unrealistic assumption that the point tracks given as input are clean of outliers.","Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step.","Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers."],"url":"http://arxiv.org/abs/2404.14280v1","category":"cs.CV"}
{"created":"2024-04-22 15:26:31","title":"Dipolar order controls dielectric response of glass-forming liquids","abstract":"The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations. A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum. Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids. While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory.","sentences":["The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations.","A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum.","Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids.","While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory."],"url":"http://arxiv.org/abs/2404.14277v1","category":"cond-mat.soft"}
{"created":"2024-04-22 14:57:17","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images","abstract":"Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future.","sentences":["Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media.","One notable consequence is the use of AI-generated images for fake profiles on social media.","While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking.","In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter.","We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline.","Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform.","We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior.","The results also reveal several motives, including spamming and political amplification campaigns.","Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future."],"url":"http://arxiv.org/abs/2404.14244v1","category":"cs.CR"}
{"created":"2024-04-24 03:17:14","title":"Neuromorphic Shack-Hartmann wave normal sensing","abstract":"The Shack-Hartmann wavefront sensor is widely employed in adaptive optics systems to measure optical aberrations. However, simultaneously achieving high sensitivity and large dynamic range is still challenging, limiting the performance of diagnosing fast-changing turbulence. To overcome this limitation, we propose neuromorphic Shack-Hartmann wave normal sensing (NeuroSH). NeuroSH is a unifying framework that harnesses the computational neuromorphic imaging paradigm to extract the high-dimensional wave normal from temporal diversity measurements. Both numerical analysis and experimental verification demonstrate the feasibility of NeuroSH. To the best of our knowledge, the proposed NeuroSH is the first scheme to surpass the optical dynamic range limitation under challenging dynamic scenarios, thereby advancing ultra-fast turbulence mitigation technology for cutting-edge imagers.","sentences":["The Shack-Hartmann wavefront sensor is widely employed in adaptive optics systems to measure optical aberrations.","However, simultaneously achieving high sensitivity and large dynamic range is still challenging, limiting the performance of diagnosing fast-changing turbulence.","To overcome this limitation, we propose neuromorphic Shack-Hartmann wave normal sensing (NeuroSH).","NeuroSH is a unifying framework that harnesses the computational neuromorphic imaging paradigm to extract the high-dimensional wave normal from temporal diversity measurements.","Both numerical analysis and experimental verification demonstrate the feasibility of NeuroSH.","To the best of our knowledge, the proposed NeuroSH is the first scheme to surpass the optical dynamic range limitation under challenging dynamic scenarios, thereby advancing ultra-fast turbulence mitigation technology for cutting-edge imagers."],"url":"http://arxiv.org/abs/2404.15619v1","category":"physics.optics"}
{"created":"2024-04-24 03:08:05","title":"Unitary Basis Transformations in Mixed Quantum-Classical Dynamics","abstract":"A common approach to minimizing the cost of quantum computations is by transforming a quantum system into a basis that can be optimally truncated. Here, we derive classical equations of motion subjected to similar unitary transformations, and propose their integration into mixed quantum-classical dynamics, enabling this class of methods to be applied within arbitrary bases for both the quantum and classical coordinates. To this end, canonical positions and momenta are combined into a set of complex-valued classical coordinates amenable to unitary transformations. We demonstrate the potential of the resulting approach by means of surface hopping calculations of an electronic carrier scattering onto a single impurity in the presence of phonons. Appropriate basis transformations, capturing both the localization of the impurity and the delocalization of higher-energy excitations, are shown to faithfully capture the dynamics within a fraction of the classical and quantum basis sets.","sentences":["A common approach to minimizing the cost of quantum computations is by transforming a quantum system into a basis that can be optimally truncated.","Here, we derive classical equations of motion subjected to similar unitary transformations, and propose their integration into mixed quantum-classical dynamics, enabling this class of methods to be applied within arbitrary bases for both the quantum and classical coordinates.","To this end, canonical positions and momenta are combined into a set of complex-valued classical coordinates amenable to unitary transformations.","We demonstrate the potential of the resulting approach by means of surface hopping calculations of an electronic carrier scattering onto a single impurity in the presence of phonons.","Appropriate basis transformations, capturing both the localization of the impurity and the delocalization of higher-energy excitations, are shown to faithfully capture the dynamics within a fraction of the classical and quantum basis sets."],"url":"http://arxiv.org/abs/2404.15614v1","category":"quant-ph"}
{"created":"2024-04-24 03:07:43","title":"A nearly-$4\\log n$ depth lower bound for formulas with restriction on top","abstract":"One of the major open problems in complexity theory is to demonstrate an explicit function which requires super logarithmic depth, a.k.a, the $\\mathbf{P}$ versus $\\mathbf{NC^1}$ problem. The current best depth lower bound is $(3-o(1))\\cdot \\log n$, and it is widely open how to prove a super-$3\\log n$ depth lower bound. Recently Mihajlin and Sofronova (CCC'22) show if considering formulas with restriction on top, we can break the $3\\log n$ barrier. Formally, they prove there exist two functions $f:\\{0,1\\}^n \\rightarrow \\{0,1\\},g:\\{0,1\\}^n \\rightarrow \\{0,1\\}^n$, such that for any constant $0<\\alpha<0.4$ and constant $0<\\epsilon<\\alpha/2$, their XOR composition $f(g(x)\\oplus y)$ is not computable by an AND of $2^{(\\alpha-\\epsilon)n}$ formulas of size at most $2^{(1-\\alpha/2-\\epsilon)n}$. This implies a modified version of Andreev function is not computable by any circuit of depth $(3.2-\\epsilon)\\log n$ with the restriction that top $0.4-\\epsilon$ layers only consist of AND gates for any small constant $\\epsilon>0$. They ask whether the parameter $\\alpha$ can be push up to nearly $1$ thus implying a nearly-$3.5\\log n$ depth lower bound.   In this paper, we provide a stronger answer to their question. We show there exist two functions $f:\\{0,1\\}^n \\rightarrow \\{0,1\\},g:\\{0,1\\}^n \\rightarrow \\{0,1\\}^n$, such that for any constant $0<\\alpha<2-o(1)$, their XOR composition $f(g(x)\\oplus y)$ is not computable by an AND of $2^{\\alpha n}$ formulas of size at most $2^{(1-\\alpha/2-o(1))n}$. This implies a $(4-o(1))\\log n$ depth lower bound with the restriction that top $2-o(1)$ layers only consist of AND gates. We prove it by observing that one crucial component in Mihajlin and Sofronova's work, called the well-mixed set of functions, can be significantly simplified thus improved. Then with this observation and a more careful analysis, we obtain these nearly tight results.","sentences":["One of the major open problems in complexity theory is to demonstrate an explicit function which requires super logarithmic depth, a.k.a, the $\\mathbf{P}$ versus $\\mathbf{NC^1}$ problem.","The current best depth lower bound is $(3-o(1))\\cdot \\log n$, and it is widely open how to prove a super-$3\\log n$ depth lower bound.","Recently Mihajlin and Sofronova (CCC'22) show if considering formulas with restriction on top, we can break the $3\\log n$ barrier.","Formally, they prove there exist two functions $f:\\{0,1\\}^n \\rightarrow \\{0,1\\},g:\\{0,1\\}^n \\rightarrow \\{0,1\\}^n$, such that for any constant $0<\\alpha<0.4$ and constant $0<\\epsilon<\\alpha/2$, their XOR composition $f(g(x)\\oplus y)$ is not computable by an AND of $2^{(\\alpha-\\epsilon)n}$ formulas of size at most $2^{(1-\\alpha/2-\\epsilon)n}$. This implies a modified version of Andreev function is not computable by any circuit of depth $(3.2-\\epsilon)\\log n$ with the restriction that top $0.4-\\epsilon$ layers only consist of AND gates for any small constant $\\epsilon>0$. They ask whether the parameter $\\alpha$ can be push up to nearly $1$ thus implying a nearly-$3.5\\log n$ depth lower bound.   ","In this paper, we provide a stronger answer to their question.","We show there exist two functions $f:\\{0,1\\}^n \\rightarrow \\{0,1\\},g:\\{0,1\\}^n \\rightarrow \\{0,1\\}^n$, such that for any constant $0<\\alpha<2-o(1)$, their XOR composition $f(g(x)\\oplus y)$ is not computable by an AND of $2^{\\alpha n}$ formulas of size at most $2^{(1-\\alpha/2-o(1))n}$.","This implies a $(4-o(1))\\log n$ depth lower bound with the restriction that top $2-o(1)$ layers only consist of AND gates.","We prove it by observing that one crucial component in Mihajlin and Sofronova's work, called the well-mixed set of functions, can be significantly simplified thus improved.","Then with this observation and a more careful analysis, we obtain these nearly tight results."],"url":"http://arxiv.org/abs/2404.15613v1","category":"cs.CC"}
{"created":"2024-04-24 03:02:21","title":"PoisonedFL: Model Poisoning Attacks to Federated Learning via Multi-Round Consistency","abstract":"Model poisoning attacks are critical security threats to Federated Learning (FL). Existing model poisoning attacks suffer from two key limitations: 1) they achieve suboptimal effectiveness when defenses are deployed, and/or 2) they require knowledge of the model updates or local training data on genuine clients. In this work, we make a key observation that their suboptimal effectiveness arises from only leveraging model-update consistency among malicious clients within individual training rounds, making the attack effect self-cancel across training rounds. In light of this observation, we propose PoisonedFL, which enforces multi-round consistency among the malicious clients' model updates while not requiring any knowledge about the genuine clients. Our empirical evaluation on five benchmark datasets shows that PoisonedFL breaks eight state-of-the-art defenses and outperforms seven existing model poisoning attacks. Moreover, we also explore new defenses that are tailored to PoisonedFL, but our results show that we can still adapt PoisonedFL to break them. Our study shows that FL systems are considerably less robust than previously thought, underlining the urgency for the development of new defense mechanisms.","sentences":["Model poisoning attacks are critical security threats to Federated Learning (FL).","Existing model poisoning attacks suffer from two key limitations: 1) they achieve suboptimal effectiveness when defenses are deployed, and/or 2) they require knowledge of the model updates or local training data on genuine clients.","In this work, we make a key observation that their suboptimal effectiveness arises from only leveraging model-update consistency among malicious clients within individual training rounds, making the attack effect self-cancel across training rounds.","In light of this observation, we propose PoisonedFL, which enforces multi-round consistency among the malicious clients' model updates while not requiring any knowledge about the genuine clients.","Our empirical evaluation on five benchmark datasets shows that PoisonedFL breaks eight state-of-the-art defenses and outperforms seven existing model poisoning attacks.","Moreover, we also explore new defenses that are tailored to PoisonedFL, but our results show that we can still adapt PoisonedFL to break them.","Our study shows that FL systems are considerably less robust than previously thought, underlining the urgency for the development of new defense mechanisms."],"url":"http://arxiv.org/abs/2404.15611v1","category":"cs.CR"}
{"created":"2024-04-24 02:51:13","title":"Understanding and Improving CNNs with Complex Structure Tensor: A Biometrics Study","abstract":"Our study provides evidence that CNNs struggle to effectively extract orientation features. We show that the use of Complex Structure Tensor, which contains compact orientation features with certainties, as input to CNNs consistently improves identification accuracy compared to using grayscale inputs alone. Experiments also demonstrated that our inputs, which were provided by mini complex conv-nets, combined with reduced CNN sizes, outperformed full-fledged, prevailing CNN architectures. This suggests that the upfront use of orientation features in CNNs, a strategy seen in mammalian vision, not only mitigates their limitations but also enhances their explainability and relevance to thin-clients. Experiments were done on publicly available data sets comprising periocular images for biometric identification and verification (Close and Open World) using 6 State of the Art CNN architectures. We reduced SOA Equal Error Rate (EER) on the PolyU dataset by 5-26% depending on data and scenario.","sentences":["Our study provides evidence that CNNs struggle to effectively extract orientation features.","We show that the use of Complex Structure Tensor, which contains compact orientation features with certainties, as input to CNNs consistently improves identification accuracy compared to using grayscale inputs alone.","Experiments also demonstrated that our inputs, which were provided by mini complex conv-nets, combined with reduced CNN sizes, outperformed full-fledged, prevailing CNN architectures.","This suggests that the upfront use of orientation features in CNNs, a strategy seen in mammalian vision, not only mitigates their limitations but also enhances their explainability and relevance to thin-clients.","Experiments were done on publicly available data sets comprising periocular images for biometric identification and verification (Close and Open World) using 6 State of the Art CNN architectures.","We reduced SOA Equal Error Rate (EER) on the PolyU dataset by 5-26% depending on data and scenario."],"url":"http://arxiv.org/abs/2404.15608v1","category":"cs.CV"}
{"created":"2024-04-24 02:40:43","title":"Toric wedge induction and toric lifting property for piecewise linear spheres with a few vertices","abstract":"Let $K$ be an $(n-1)$-dimensional piecewise linear sphere on $[m]$, where $m\\leq n+4$. There are a canonical action of $m$-dimensional torus $T^m$ on the moment-angle complex $\\mathcal{Z}_K$, and a canonical action of $\\mathbb{Z}_2^m$ on the real moment-angle complex $\\mathbb{R}\\mathcal{Z}_K$, where $\\mathbb{Z}_2$ is the additive group with two elements. We prove that any subgroup of $\\mathbb{Z}_2^m$ acting freely on $\\mathbb{R}\\mathcal{Z}_K$ is induced by a subtorus of $T^m$ acting freely on $\\mathcal{Z}_K$. The proof primarily utilizes a suitably modified method of toric wedge induction and the combinatorial structure of a specific binary matroid of rank $4$.","sentences":["Let $K$ be an $(n-1)$-dimensional piecewise linear sphere on $[m]$, where $m\\leq n+4$. There are a canonical action of $m$-dimensional torus $T^m$ on the moment-angle complex $\\mathcal{Z}_K$, and a canonical action of $\\mathbb{Z}_2^m$ on the real moment-angle complex $\\mathbb{R}\\mathcal{Z}_K$, where $\\mathbb{Z}_2$ is the additive group with two elements.","We prove that any subgroup of $\\mathbb{Z}_2^m$ acting freely on $\\mathbb{R}\\mathcal{Z}_K$ is induced by a subtorus of $T^m$ acting freely on $\\mathcal{Z}_K$. The proof primarily utilizes a suitably modified method of toric wedge induction and the combinatorial structure of a specific binary matroid of rank $4$."],"url":"http://arxiv.org/abs/2404.15600v1","category":"math.AT"}
{"created":"2024-04-24 01:45:11","title":"The impact of complexity in the built environment on vehicular routing behavior: Insights from an empirical study of taxi mobility in Beijing, China","abstract":"The modeling of disaggregated vehicular mobility and its associations with the ambient urban built environment is essential for developing operative transport intervention and urban optimization plans. However, established vehicular route choice models failed to fully consider the bounded behavioral rationality and the complex characteristics of the urban built environment affecting drivers' route choice preference. Therefore, the spatio-temporal characteristics of vehicular mobility patterns were not fully explained, which limited the granular implementation of relevant transport interventions. To address this limitation, we proposed a vehicular route choice model that mimics the anchoring effect and the exposure preference while driving. The proposed model enables us to quantitatively examine the impact of the built environment on vehicular routing behavior, which has been largely neglected in previous studies. Results show that the proposed model performs 12% better than the conventional vehicular route choice model based on the shortest path principle. Our empirical analysis of taxi drivers' routing behavior patterns in Beijing, China uncovers that drivers are inclined to choose routes with shorter time duration and with less loss at traversal intersections. Counterintuitively, we also found that drivers heavily rely on circuitous ring roads and expressways to deliver passengers, which are unexpectedly longer than the shortest paths. Moreover, characteristics of the urban built environment including road eccentricity, centrality, average road length, land use diversity, sky visibility, and building coverage can affect drivers' route choice behaviors, accounting for about 5% of the increase in the proposed model's performance. We also refine the above explorations according to the modeling results of trips that differ in departure time, travel distance, and occupation status.","sentences":["The modeling of disaggregated vehicular mobility and its associations with the ambient urban built environment is essential for developing operative transport intervention and urban optimization plans.","However, established vehicular route choice models failed to fully consider the bounded behavioral rationality and the complex characteristics of the urban built environment affecting drivers' route choice preference.","Therefore, the spatio-temporal characteristics of vehicular mobility patterns were not fully explained, which limited the granular implementation of relevant transport interventions.","To address this limitation, we proposed a vehicular route choice model that mimics the anchoring effect and the exposure preference while driving.","The proposed model enables us to quantitatively examine the impact of the built environment on vehicular routing behavior, which has been largely neglected in previous studies.","Results show that the proposed model performs 12% better than the conventional vehicular route choice model based on the shortest path principle.","Our empirical analysis of taxi drivers' routing behavior patterns in Beijing, China uncovers that drivers are inclined to choose routes with shorter time duration and with less loss at traversal intersections.","Counterintuitively, we also found that drivers heavily rely on circuitous ring roads and expressways to deliver passengers, which are unexpectedly longer than the shortest paths.","Moreover, characteristics of the urban built environment including road eccentricity, centrality, average road length, land use diversity, sky visibility, and building coverage can affect drivers' route choice behaviors, accounting for about 5% of the increase in the proposed model's performance.","We also refine the above explorations according to the modeling results of trips that differ in departure time, travel distance, and occupation status."],"url":"http://arxiv.org/abs/2404.15589v1","category":"stat.AP"}
{"created":"2024-04-24 01:36:21","title":"Research on OPF control of three-phase four-wire low-voltage distribution network considering uncertainty","abstract":"As power systems become more complex and uncertain, low-voltage distribution networks face numerous challenges, including three-phase imbalances caused by asymmetrical loads and distributed energy resources. We propose a robust stochastic optimization (RSO) based optimal power flow (OPF) control method for three-phase, four-wire low-voltage distribution networks that consider uncertainty to address these issues. Using historical data and deep learning classification methods, the proposed method simulates optimal system behaviour without requiring communication infrastructure. The simulation results verify that the proposed method effectively controls the voltage and current amplitude while minimizing the operational cost and three-phase imbalance within acceptable limits. The proposed method shows promise for managing uncertainties and optimizing performance in low-voltage distribution networks.","sentences":["As power systems become more complex and uncertain, low-voltage distribution networks face numerous challenges, including three-phase imbalances caused by asymmetrical loads and distributed energy resources.","We propose a robust stochastic optimization (RSO) based optimal power flow (OPF) control method for three-phase, four-wire low-voltage distribution networks that consider uncertainty to address these issues.","Using historical data and deep learning classification methods, the proposed method simulates optimal system behaviour without requiring communication infrastructure.","The simulation results verify that the proposed method effectively controls the voltage and current amplitude while minimizing the operational cost and three-phase imbalance within acceptable limits.","The proposed method shows promise for managing uncertainties and optimizing performance in low-voltage distribution networks."],"url":"http://arxiv.org/abs/2404.15584v1","category":"eess.SY"}
{"created":"2024-04-24 01:07:49","title":"Photonic variational quantum eigensolver using entanglement measurements","abstract":"Variational quantum eigensolver (VQE), which combines quantum systems with classical computational power, has been arisen as a promising candidate for near-term quantum computing applications. However, the experimental resources such as the number of measurements to implement VQE rapidly increases as the Hamiltonian problem size grows. Applying entanglement measurements to reduce the number of measurement setups has been proposed to address this issue, however, entanglement measurements themselves can introduce additional resource demands. Here, we apply entanglement measurements to the photonic VQE utilizing polarization and path degrees of freedom of a single-photon. In our photonic VQE, entanglement measurements can be deterministically implemented using linear optics, so it takes full advantage of introducing entanglement measurements without additional experimental demands. Moreover, we show that such a setup can mitigate errors in measurement apparatus for a certain Hamiltonian.","sentences":["Variational quantum eigensolver (VQE), which combines quantum systems with classical computational power, has been arisen as a promising candidate for near-term quantum computing applications.","However, the experimental resources such as the number of measurements to implement VQE rapidly increases as the Hamiltonian problem size grows.","Applying entanglement measurements to reduce the number of measurement setups has been proposed to address this issue, however, entanglement measurements themselves can introduce additional resource demands.","Here, we apply entanglement measurements to the photonic VQE utilizing polarization and path degrees of freedom of a single-photon.","In our photonic VQE, entanglement measurements can be deterministically implemented using linear optics, so it takes full advantage of introducing entanglement measurements without additional experimental demands.","Moreover, we show that such a setup can mitigate errors in measurement apparatus for a certain Hamiltonian."],"url":"http://arxiv.org/abs/2404.15579v1","category":"quant-ph"}
{"created":"2024-04-24 00:43:08","title":"Granular jamming gripper with integrated suction","abstract":"Granular grippers can manipulate a wide variety of objects, but need to be pressed on the object to conform to it. If the object is placed on unstable ground, e.g., on sand or water, this step might cause the object to sink or move away from the gripper, hindering proper operation. We introduce a granular gripper with an integrated suction cup, where suction and jamming are controlled independently. We demonstrate the system's robust and enhanced gripping capabilities by comparing its grasping performance with a typical granular gripper design. We show that the proposed device can grip objects that are challenging for typical granular grippers, including those placed on unstable ground, as the suction cup stabilizes the object, allowing the gripper to conform.","sentences":["Granular grippers can manipulate a wide variety of objects, but need to be pressed on the object to conform to it.","If the object is placed on unstable ground, e.g., on sand or water, this step might cause the object to sink or move away from the gripper, hindering proper operation.","We introduce a granular gripper with an integrated suction cup, where suction and jamming are controlled independently.","We demonstrate the system's robust and enhanced gripping capabilities by comparing its grasping performance with a typical granular gripper design.","We show that the proposed device can grip objects that are challenging for typical granular grippers, including those placed on unstable ground, as the suction cup stabilizes the object, allowing the gripper to conform."],"url":"http://arxiv.org/abs/2404.15577v1","category":"cond-mat.soft"}
{"created":"2024-04-24 00:26:40","title":"Jitter Characterization of the HyTI Satellite","abstract":"The Hyperspectral Thermal Imager (HyTI) is a technology demonstration mission that will obtain high spatial, spectral, and temporal resolution long-wave infrared images of Earth's surface from a 6U cubesat. HyTI science requires that the pointing accuracy of the optical axis shall not exceed 2.89 arcsec over the 0.5 ms integration time due to microvibration effects (known as jitter). Two sources of vibration are a cryocooler that is added to maintain the detector at 68 K and three orthogonally placed reaction wheels that are a part of the attitude control system. Both of these parts will introduce vibrations that are propagated through to the satellite structure while imaging. Typical methods of characterizing and measuring jitter involve complex finite element methods and specialized equipment and setups. In this paper, we describe a novel method of characterizing jitter for small satellite systems that is low-cost and minimally modifies the subject's mass distribution. The metrology instrument is comprised of a laser source, a small mirror mounted via a 3D printed clamp to a jig, and a lateral effect position-sensing detector. The position-sensing detector samples 1000 Hz and can measure displacements as little as 0.15 arcsec at distances of one meter. This paper provides an experimental procedure that incrementally analyzes vibratory sources to establish causal relationships between sources and the vibratory modes they create. We demonstrate the capabilities of this metrology system and testing procedure on HyTI in the Hawaii Space Flight Lab's clean room. Results include power spectral density plots that show fundamental and higher-order vibratory modal frequencies. Results from metrology show that jitter from reaction wheels meets HyTI system requirements within 3$\\sigma$.","sentences":["The Hyperspectral Thermal Imager (HyTI) is a technology demonstration mission that will obtain high spatial, spectral, and temporal resolution long-wave infrared images of Earth's surface from a 6U cubesat.","HyTI science requires that the pointing accuracy of the optical axis shall not exceed 2.89 arcsec over the 0.5 ms integration time due to microvibration effects (known as jitter).","Two sources of vibration are a cryocooler that is added to maintain the detector at 68 K and three orthogonally placed reaction wheels that are a part of the attitude control system.","Both of these parts will introduce vibrations that are propagated through to the satellite structure while imaging.","Typical methods of characterizing and measuring jitter involve complex finite element methods and specialized equipment and setups.","In this paper, we describe a novel method of characterizing jitter for small satellite systems that is low-cost and minimally modifies the subject's mass distribution.","The metrology instrument is comprised of a laser source, a small mirror mounted via a 3D printed clamp to a jig, and a lateral effect position-sensing detector.","The position-sensing detector samples 1000 Hz and can measure displacements as little as 0.15 arcsec at distances of one meter.","This paper provides an experimental procedure that incrementally analyzes vibratory sources to establish causal relationships between sources and the vibratory modes they create.","We demonstrate the capabilities of this metrology system and testing procedure on HyTI in the Hawaii Space Flight Lab's clean room.","Results include power spectral density plots that show fundamental and higher-order vibratory modal frequencies.","Results from metrology show that jitter from reaction wheels meets HyTI system requirements within 3$\\sigma$."],"url":"http://arxiv.org/abs/2404.15575v1","category":"astro-ph.IM"}
{"created":"2024-04-24 00:17:15","title":"Mapping Incidence and Prevalence Peak Data for SIR Forecasting Applications","abstract":"Infectious disease modeling and forecasting have played a key role in helping assess and respond to epidemics and pandemics. Recent work has leveraged data on disease peak infection and peak hospital incidence to fit compartmental models for the purpose of forecasting and describing the dynamics of a disease outbreak. Incorporating these data can greatly stabilize a compartmental model fit on early observations, where slight perturbations in the data may lead to model fits that project wildly unrealistic peak infection. We introduce a new method for incorporating historic data on the value and time of peak incidence of hospitalization into the fit for a Susceptible-Infectious-Recovered (SIR) model by formulating the relationship between an SIR model's starting parameters and peak incidence as a system of two equations that can be solved computationally. This approach is assessed for practicality in terms of accuracy and speed of computation via simulation. To exhibit the modeling potential, we update the Dirichlet-Beta State Space modeling framework to use hospital incidence data, as this framework was previously formulated to incorporate only data on total infections.","sentences":["Infectious disease modeling and forecasting have played a key role in helping assess and respond to epidemics and pandemics.","Recent work has leveraged data on disease peak infection and peak hospital incidence to fit compartmental models for the purpose of forecasting and describing the dynamics of a disease outbreak.","Incorporating these data can greatly stabilize a compartmental model fit on early observations, where slight perturbations in the data may lead to model fits that project wildly unrealistic peak infection.","We introduce a new method for incorporating historic data on the value and time of peak incidence of hospitalization into the fit for a Susceptible-Infectious-Recovered (SIR) model by formulating the relationship between an SIR model's starting parameters and peak incidence as a system of two equations that can be solved computationally.","This approach is assessed for practicality in terms of accuracy and speed of computation via simulation.","To exhibit the modeling potential, we update the Dirichlet-Beta State Space modeling framework to use hospital incidence data, as this framework was previously formulated to incorporate only data on total infections."],"url":"http://arxiv.org/abs/2404.15572v1","category":"stat.ME"}
{"created":"2024-04-24 00:14:18","title":"A note on the generalised Hessian of the least squares associated with systems of linear inequalities","abstract":"The goal of this note is to point out an erroneous formula for the generalised Hessian of the least squares associated with a system of linear inequalities, that was given in the paper \"A finite Newton method for classification\" by O.L. Mangasarian (Optim. Methods Softw. 17: 913--929, 2002) and reproduced multiple times in other publications. We also provide sufficient contiditions for the validity of Mangasarian's formula and show that Slater's condition guarantees that some particular elements from the set defined by Mangasarian belong to the generalised Hessian of the corresponding function.","sentences":["The goal of this note is to point out an erroneous formula for the generalised Hessian of the least squares associated with a system of linear inequalities, that was given in the paper \"A finite Newton method for classification\" by O.L. Mangasarian (Optim.","Methods Softw.","17: 913--929, 2002) and reproduced multiple times in other publications.","We also provide sufficient contiditions for the validity of Mangasarian's formula and show that Slater's condition guarantees that some particular elements from the set defined by Mangasarian belong to the generalised Hessian of the corresponding function."],"url":"http://arxiv.org/abs/2404.15571v1","category":"math.OC"}
{"created":"2024-04-24 00:00:08","title":"Air-taxi trajectory optimization with aerodynamic and motor models","abstract":"Many air-taxi concepts are capable of vertical takeoff and landing, enabling them to fly to and from urban locations. An important capability for these air taxis is the transition between hover and forward flight. We propose a robust methodology for computing optimal takeoff and transition trajectories using surrogate models trained on data from physics-based models. The use of surrogate models reduces the computational complexity and improves the robustness of the trajectory optimization algorithm. We demonstrate the versatility and robustness of the proposed methodology by applying it to 12 trajectory optimization problems that involve air-taxi takeoff and outbound transition. These trajectories are representative of real air-taxi operations, with a variety of constraints derived, in part, from proposed mission requirements.","sentences":["Many air-taxi concepts are capable of vertical takeoff and landing, enabling them to fly to and from urban locations.","An important capability for these air taxis is the transition between hover and forward flight.","We propose a robust methodology for computing optimal takeoff and transition trajectories using surrogate models trained on data from physics-based models.","The use of surrogate models reduces the computational complexity and improves the robustness of the trajectory optimization algorithm.","We demonstrate the versatility and robustness of the proposed methodology by applying it to 12 trajectory optimization problems that involve air-taxi takeoff and outbound transition.","These trajectories are representative of real air-taxi operations, with a variety of constraints derived, in part, from proposed mission requirements."],"url":"http://arxiv.org/abs/2404.15570v1","category":"math.OC"}
{"created":"2024-04-23 23:06:00","title":"Planet formation -- observational constraints, physical processes, and compositional patterns","abstract":"The goal of planet formation as a field of study is not only to provide the understanding of how planets come into existence. It is also an interdisciplinary bridge which links astronomy to geology and mineralogy. Recent observations of young stars accompanied by their protoplanetary disks (Manara et al. 2022) provide direct insights into the conditions at which planets are forming. These astronomical observations can be taken as initial conditions for the models of planet formation. In this chapter, we first give an brief overview of key observational constraints for planet formation theory derived from both the solar system and from the exoplanet population. We then review physical mechanisms governing planetary system formation and discuss how they can be put together to form global planet formation models. Finally, we discuss how the compositional links from protoplanetary disks to planetary atmospheres put novel constraints on planet formation theory. In particular, we are currently gaining insights into the composition of the inner, planet-forming region within the disks thanks to observations from the James Webb Space Telescope (Grant et al. 2023; Perotti et al. 2023). The task for planet formation modelling is then to link these observational properties and compositional content to the physical properties, and also the elementary inventory of meteorites, the Moon, Earth, and the other planets. If successful, this global approach can provide useful constraints for geological studies.","sentences":["The goal of planet formation as a field of study is not only to provide the understanding of how planets come into existence.","It is also an interdisciplinary bridge which links astronomy to geology and mineralogy.","Recent observations of young stars accompanied by their protoplanetary disks (Manara et al. 2022) provide direct insights into the conditions at which planets are forming.","These astronomical observations can be taken as initial conditions for the models of planet formation.","In this chapter, we first give an brief overview of key observational constraints for planet formation theory derived from both the solar system and from the exoplanet population.","We then review physical mechanisms governing planetary system formation and discuss how they can be put together to form global planet formation models.","Finally, we discuss how the compositional links from protoplanetary disks to planetary atmospheres put novel constraints on planet formation theory.","In particular, we are currently gaining insights into the composition of the inner, planet-forming region within the disks thanks to observations from the James Webb Space Telescope (Grant et al. 2023;","Perotti et al. 2023).","The task for planet formation modelling is then to link these observational properties and compositional content to the physical properties, and also the elementary inventory of meteorites, the Moon, Earth, and the other planets.","If successful, this global approach can provide useful constraints for geological studies."],"url":"http://arxiv.org/abs/2404.15555v1","category":"astro-ph.EP"}
{"created":"2024-04-23 22:25:39","title":"Modular Forms in Combinatorial Optimization","abstract":"Combinatorial optimization problems, such as the Asymmetric Traveling Salesman Problem (ATSP), find applications across various domains including logistics, genome sequencing, and robotics. Despite their extensive applications, there have not been significant advancements in deriving optimal solutions for these problems. The lack of theoretical understanding owing to the complex structure of these problems has hindered the development of sophisticated algorithms. This paper proposes an unconventional approach by translating the ATSP into the complex domain, revealing an intrinsic modular nature of the problem. Furthermore, we have exploited modularity conditions to gain deeper insights into both unconstrained and constrained optimal solutions. The theoretical framework laid out in this paper can lead to important results at the intersection of combinatorial optimization and number theory.","sentences":["Combinatorial optimization problems, such as the Asymmetric Traveling Salesman Problem (ATSP), find applications across various domains including logistics, genome sequencing, and robotics.","Despite their extensive applications, there have not been significant advancements in deriving optimal solutions for these problems.","The lack of theoretical understanding owing to the complex structure of these problems has hindered the development of sophisticated algorithms.","This paper proposes an unconventional approach by translating the ATSP into the complex domain, revealing an intrinsic modular nature of the problem.","Furthermore, we have exploited modularity conditions to gain deeper insights into both unconstrained and constrained optimal solutions.","The theoretical framework laid out in this paper can lead to important results at the intersection of combinatorial optimization and number theory."],"url":"http://arxiv.org/abs/2404.15546v1","category":"math.CO"}
{"created":"2024-04-23 22:19:51","title":"Springs and a stopwatch: neural units with time-dependent multifunctionality","abstract":"Several branches of computing use a system's physical dynamics to do computation. We show that the dynamics of an underdamped harmonic oscillator can perform multifunctional computation, solving distinct problems at distinct times within a single dynamical trajectory. Oscillator computing usually focuses on the oscillator's phase as the information-carrying component. Here we focus on the time-resolved amplitude of an oscillator whose inputs influence its frequency, which has a natural parallel as the activity of a time-dependent neural unit. Because the activity of the unit at fixed time is a nonmonotonic function of the input, the unit can solve nonlinearly-separable problems such as XOR. Because the activity of the unit at fixed input is a nonmonotonic function of time, the unit is multifunctional in a temporal sense, able to carry out distinct nonlinear computations at distinct times within the same dynamical trajectory. Time-resolved computing of this nature can be done in or out of equilibrium, with the natural time evolution of the system giving us multiple computations for the price of one.","sentences":["Several branches of computing use a system's physical dynamics to do computation.","We show that the dynamics of an underdamped harmonic oscillator can perform multifunctional computation, solving distinct problems at distinct times within a single dynamical trajectory.","Oscillator computing usually focuses on the oscillator's phase as the information-carrying component.","Here we focus on the time-resolved amplitude of an oscillator whose inputs influence its frequency, which has a natural parallel as the activity of a time-dependent neural unit.","Because the activity of the unit at fixed time is a nonmonotonic function of the input, the unit can solve nonlinearly-separable problems such as XOR.","Because the activity of the unit at fixed input is a nonmonotonic function of time, the unit is multifunctional in a temporal sense, able to carry out distinct nonlinear computations at distinct times within the same dynamical trajectory.","Time-resolved computing of this nature can be done in or out of equilibrium, with the natural time evolution of the system giving us multiple computations for the price of one."],"url":"http://arxiv.org/abs/2404.15545v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-23 21:58:18","title":"Distinguishing noisy crystal symmetries in coarse-grained computer simulations: New procedures for noise reduction and lattice reconstruction","abstract":"We suggest new modification (we call it a noise reduction procedure) for Steinhardt parameters which are often used for detecting crystalline structures in computer simulation of solids and soft matter systems. We have also developed a new methodology how to reconstruct \"ideal\" lattice structure in the whole simulation box that would be most close to a real noisy crystalline symmetry, when it is defined locally and then averaged over the whole box. For this second procedure, which we call lattice reconstruction procedure, we have developed an algorithm for finding the lattice vectors from the values of Steinhardt parameters obtained after the noise reduction procedure. We apply noise to the classical crystalline structures (sc, bcc, fcc, hcp), and use both procedures to detect the crystalline structures in these classical but noisy systems. We demonstrate advantages of our procedures in comparison with existing methods and discuss their applicability limits.","sentences":["We suggest new modification (we call it a noise reduction procedure) for Steinhardt parameters which are often used for detecting crystalline structures in computer simulation of solids and soft matter systems.","We have also developed a new methodology how to reconstruct \"ideal\" lattice structure in the whole simulation box that would be most close to a real noisy crystalline symmetry, when it is defined locally and then averaged over the whole box.","For this second procedure, which we call lattice reconstruction procedure, we have developed an algorithm for finding the lattice vectors from the values of Steinhardt parameters obtained after the noise reduction procedure.","We apply noise to the classical crystalline structures (sc, bcc, fcc, hcp), and use both procedures to detect the crystalline structures in these classical but noisy systems.","We demonstrate advantages of our procedures in comparison with existing methods and discuss their applicability limits."],"url":"http://arxiv.org/abs/2404.15539v1","category":"physics.comp-ph"}
{"created":"2024-04-23 21:25:07","title":"Co-existing/Cooperating Multicell Massive MIMO and Cell-Free Massive MIMO Deployments: Heuristic Designs and Performance Analysis","abstract":"Cell-free massive MIMO (CF-mMIMO) represent a deeply investigated evolution from the conventional multicell co-located massive MIMO (MC-mMIMO) network deployments. Anticipating a gradual integration of CF-mMIMO systems alongside pre-existing MC-mMIMO network elements, this paper considers a scenario where both deployments coexist, in order to serve a large number of users using a shared set of frequencies. The investigation explores the impact of this coexistence on the network's downlink performance, considering various degrees of mutual cooperation, precoder selection, and power control strategies. Moreover, to take into account the effect of the proposed cooperation scenarios on the fronthaul links, this paper also provides a fronthaul-aware heuristic association algorithm between users and network elements, which permits fulfilling the fronthaul requirement on each link. The research is finally completed by extensive simulations, shedding light on the performance outcomes associated with the diverse cooperation levels and several solutions delineated in the paper.","sentences":["Cell-free massive MIMO (CF-mMIMO) represent a deeply investigated evolution from the conventional multicell co-located massive MIMO (MC-mMIMO) network deployments.","Anticipating a gradual integration of CF-mMIMO systems alongside pre-existing MC-mMIMO network elements, this paper considers a scenario where both deployments coexist, in order to serve a large number of users using a shared set of frequencies.","The investigation explores the impact of this coexistence on the network's downlink performance, considering various degrees of mutual cooperation, precoder selection, and power control strategies.","Moreover, to take into account the effect of the proposed cooperation scenarios on the fronthaul links, this paper also provides a fronthaul-aware heuristic association algorithm between users and network elements, which permits fulfilling the fronthaul requirement on each link.","The research is finally completed by extensive simulations, shedding light on the performance outcomes associated with the diverse cooperation levels and several solutions delineated in the paper."],"url":"http://arxiv.org/abs/2404.15530v1","category":"cs.IT"}
{"created":"2024-04-23 21:22:49","title":"Path integral approach to bosonisation and nonlinearities in exciton-polariton systems","abstract":"Large exciton-polariton optical nonlinearities present a key mechanism for photonics-based communication, ultimately in the quantum regime. Enhanced nonlinear response from various materials hosting excitons and allowing for their strong coupling with light is therefore the topic of intense studies, both in theoretical and experimental domains. Reports on the scattering rates arising due to various system's nonlinearities, such as the exciton-exciton Coulomb interaction and the Pauli blocking that leads to the saturation of the exciton oscillator strength, however, are contradictory. In this work, we develop a formalism allowing to track the exciton nonlinearities appearing in the regime of strong coupling with photons, that includes finite temperatures, mixing of the exciton excited states, and the dark exciton contributions to saturation self-consistently. The equilibrium path integration approach employed here to address the polariton composite nature, leads to a transparent hierarchy of various contributions to nonlinearity. At the same time, by taking the simplest limit of zero temperature and so-called \"rigid\" excitons, through our framework we retrieve the expressions derived in conventional approaches for exciton interaction constants. In particular, our theory allows to clearly show that such interaction constants cannot be used as fitting parameters tunable in a wide range of values, as they are strictly defined by the material properties, and that other explanations are due for large optical nonlinearities recently reported.","sentences":["Large exciton-polariton optical nonlinearities present a key mechanism for photonics-based communication, ultimately in the quantum regime.","Enhanced nonlinear response from various materials hosting excitons and allowing for their strong coupling with light is therefore the topic of intense studies, both in theoretical and experimental domains.","Reports on the scattering rates arising due to various system's nonlinearities, such as the exciton-exciton Coulomb interaction and the Pauli blocking that leads to the saturation of the exciton oscillator strength, however, are contradictory.","In this work, we develop a formalism allowing to track the exciton nonlinearities appearing in the regime of strong coupling with photons, that includes finite temperatures, mixing of the exciton excited states, and the dark exciton contributions to saturation self-consistently.","The equilibrium path integration approach employed here to address the polariton composite nature, leads to a transparent hierarchy of various contributions to nonlinearity.","At the same time, by taking the simplest limit of zero temperature and so-called \"rigid\" excitons, through our framework we retrieve the expressions derived in conventional approaches for exciton interaction constants.","In particular, our theory allows to clearly show that such interaction constants cannot be used as fitting parameters tunable in a wide range of values, as they are strictly defined by the material properties, and that other explanations are due for large optical nonlinearities recently reported."],"url":"http://arxiv.org/abs/2404.15529v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 21:13:17","title":"A Rapid Adapting and Continual Learning Spiking Neural Network Path Planning Algorithm for Mobile Robots","abstract":"Mapping traversal costs in an environment and planning paths based on this map are important for autonomous navigation. We present a neurobotic navigation system that utilizes a Spiking Neural Network Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment. We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner, allows for adaptive planning by selectively considering any combination of costs. The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain. Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope. In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the Spiking Wavefront Planner. On simulated paths, the Spiking Wavefront Planner plans significantly shorter and lower cost paths than A* and RRT*. The spiking wavefront planner is compatible with neuromorphic hardware and could be used for applications requiring low size, weight, and power.","sentences":["Mapping traversal costs in an environment and planning paths based on this map are important for autonomous navigation.","We present a neurobotic navigation system that utilizes a Spiking Neural Network Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment.","We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner, allows for adaptive planning by selectively considering any combination of costs.","The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain.","Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope.","In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the Spiking Wavefront Planner.","On simulated paths, the Spiking Wavefront Planner plans significantly shorter and lower cost paths than A* and RRT*.","The spiking wavefront planner is compatible with neuromorphic hardware and could be used for applications requiring low size, weight, and power."],"url":"http://arxiv.org/abs/2404.15524v1","category":"cs.RO"}
{"created":"2024-04-23 21:02:03","title":"A Rotational/Roto-translational Constraint Method for Condensed Matter","abstract":"In condensed matter physics, particularly in perovskite materials, the rotational motion of molecules and ions is associated with important issues such as ion conduction mechanism. Constrained Molecular Dynamics (MD) simulations offer a means to separate translational, vibrational, and rotational motions, enabling the independent study of their effects. In this study, we introduce a rotational and roto-translational constraint algorithm based on the Velocity Verlet integrator, which has been implemented into a homebrew version of the CP2K package. The MD results show that our program can selectively constrain the molecules and ions in the system and support long-time MD runs. The algorithm can help the future study of important rotation related dynamic problems in condensed matter systems.","sentences":["In condensed matter physics, particularly in perovskite materials, the rotational motion of molecules and ions is associated with important issues such as ion conduction mechanism.","Constrained Molecular Dynamics (MD) simulations offer a means to separate translational, vibrational, and rotational motions, enabling the independent study of their effects.","In this study, we introduce a rotational and roto-translational constraint algorithm based on the Velocity Verlet integrator, which has been implemented into a homebrew version of the CP2K package.","The MD results show that our program can selectively constrain the molecules and ions in the system and support long-time MD runs.","The algorithm can help the future study of important rotation related dynamic problems in condensed matter systems."],"url":"http://arxiv.org/abs/2404.15517v1","category":"physics.chem-ph"}
{"created":"2024-04-23 20:55:14","title":"Grain boundary segregation prediction with a dual-solute model","abstract":"Solute segregation along grain boundaries (GBs) profoundly affects their thermodynamic and kinetic behavior in polycrystalline materials. Recently, it has become a promising strategy for alloy design, mitigating grain growth by reducing excess GB energy and strengthening the GB network in nanocrystalline metals. In this context, the spectrum approach has emerged as a powerful tool to predict GB segregation. However, previous GB segregation predictions using this method relied heavily on single-solute segregation spectra, neglecting the crucial role of solute-solute interactions, which are often incorporated through a fitting parameter. In this work, we developed a dual-solute model whose segregation energy spectrum intrinsically considers the solute-solute interactions. Further improvement was made by describing the volume fraction of GBs as a varying parameter that scales with the total solute concentration and temperature. The refined dual-solute model was attempted to predict the GB segregation at finite temperatures in several binary systems. It shows significant improvement over the single-solute model and can accurately predict the hybrid Molecular Dynamics/Monte Carlo data within a broad temperature range with varying solute concentrations before forming secondary phases. This dual-solute model provides an effective way to statistically predict GB segregation with considerable accuracy in nanocrystalline metals.","sentences":["Solute segregation along grain boundaries (GBs) profoundly affects their thermodynamic and kinetic behavior in polycrystalline materials.","Recently, it has become a promising strategy for alloy design, mitigating grain growth by reducing excess GB energy and strengthening the GB network in nanocrystalline metals.","In this context, the spectrum approach has emerged as a powerful tool to predict GB segregation.","However, previous GB segregation predictions using this method relied heavily on single-solute segregation spectra, neglecting the crucial role of solute-solute interactions, which are often incorporated through a fitting parameter.","In this work, we developed a dual-solute model whose segregation energy spectrum intrinsically considers the solute-solute interactions.","Further improvement was made by describing the volume fraction of GBs as a varying parameter that scales with the total solute concentration and temperature.","The refined dual-solute model was attempted to predict the GB segregation at finite temperatures in several binary systems.","It shows significant improvement over the single-solute model and can accurately predict the hybrid Molecular Dynamics/Monte Carlo data within a broad temperature range with varying solute concentrations before forming secondary phases.","This dual-solute model provides an effective way to statistically predict GB segregation with considerable accuracy in nanocrystalline metals."],"url":"http://arxiv.org/abs/2404.15513v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 20:52:12","title":"Deep Hankel matrices with random elements","abstract":"Willems' fundamental lemma enables a trajectory-based characterization of linear systems through data-based Hankel matrices. However, in the presence of measurement noise, we ask: Is this noisy Hankel-based model expressive enough to re-identify itself? In other words, we study the output prediction accuracy from recursively applying the same persistently exciting input sequence to the model. We find an asymptotic connection to this self-consistency question in terms of the amount of data. More importantly, we also connect this question to the depth (number of rows) of the Hankel model, showing the simple act of reconfiguring a finite dataset significantly improves accuracy. We apply these insights to find a parsimonious depth for LQR problems over the trajectory space.","sentences":["Willems' fundamental lemma enables a trajectory-based characterization of linear systems through data-based Hankel matrices.","However, in the presence of measurement noise, we ask: Is this noisy Hankel-based model expressive enough to re-identify itself?","In other words, we study the output prediction accuracy from recursively applying the same persistently exciting input sequence to the model.","We find an asymptotic connection to this self-consistency question in terms of the amount of data.","More importantly, we also connect this question to the depth (number of rows) of the Hankel model, showing the simple act of reconfiguring a finite dataset significantly improves accuracy.","We apply these insights to find a parsimonious depth for LQR problems over the trajectory space."],"url":"http://arxiv.org/abs/2404.15512v1","category":"eess.SY"}
{"created":"2024-04-23 20:49:05","title":"SMI-5: Five Dimensions of Social Media Interaction for Platform (De)Centralization","abstract":"Web 3.0 focuses on the decentralization of the internet and creating a system of interconnected and independent computers for improved privacy and security. We extend the idea of the decentralization of the web to the social media space: whereby we ask: in the context of the social media space, what does \"decentralization\" mean? Does decentralization of social media affect user interactions? We put forth the notion that decentralization in the social media does not solely take place on the physical network level, but can be compartmentalized across the entire social media stack. This paper puts forth SMI-5: the five dimensions of social media interaction for describing the (de)centralization of social platforms. We then illustrate a case study that the user interactions differ based on the slices of the SMI layer analyzed, highlighting the importance of understanding the (de)centralization of social media platforms from an a more encompassing perspective rather than only the physical network.","sentences":["Web 3.0 focuses on the decentralization of the internet and creating a system of interconnected and independent computers for improved privacy and security.","We extend the idea of the decentralization of the web to the social media space: whereby we ask: in the context of the social media space, what does \"decentralization\" mean?","Does decentralization of social media affect user interactions?","We put forth the notion that decentralization in the social media does not solely take place on the physical network level, but can be compartmentalized across the entire social media stack.","This paper puts forth SMI-5: the five dimensions of social media interaction for describing the (de)centralization of social platforms.","We then illustrate a case study that the user interactions differ based on the slices of the SMI layer analyzed, highlighting the importance of understanding the (de)centralization of social media platforms from an a more encompassing perspective rather than only the physical network."],"url":"http://arxiv.org/abs/2404.15509v1","category":"cs.SI"}
{"created":"2024-04-23 20:41:05","title":"A Population Analysis of 20 Exoplanets Observed from the Optical of the Near-infrared Wavelengths with HST: Evidence for Widespread Stellar Contamination","abstract":"We present a population study of 20 exoplanets, ranging from Neptune-like to inflated hot-Jupiter planets, observed during transit with the STIS and WFC3 instruments aboard the Hubble Space Telescope. To obtain spectral information from the near-UV to the near-infrared, we reanalysed sixteen WFC3 and over fifty STIS archival data sets with our dedicated HST pipeline. We also include twenty-four WFC3 data sets previously reduced with the same software. Across our target sample we observe significant divergence among multiple observations conducted with the same STIS grating at various epochs, whilst we do not detect variations in the WFC3 data sets. These results are suggestive of stellar contamination, which we have investigated further using known Bayesian tools and other tailored metrics, facilitating a more objective assessment of stellar activity intensity within each system. Our findings reveal that stellar activity contaminates up to half of the studied exoplanet atmospheres, albeit at varying extents. Accounting for stellar activity can significantly alter planetary atmospheric parameters like molecular abundances (up to 6 orders of magnitude) and temperature (up to 145 %), contrasting with the results of analyses that neglect activity. Our results emphasise the importance of considering the effects of stellar contamination in exoplanet transit studies; this issue is particularly true for data sets obtained with facilities that do not cover the optical and/or UV spectral range where the activity is expected to be more impactful but also more easily detectable. Our results also provide a catalogue of potentially active stars for further investigation and monitoring.","sentences":["We present a population study of 20 exoplanets, ranging from Neptune-like to inflated hot-Jupiter planets, observed during transit with the STIS and WFC3 instruments aboard the Hubble Space Telescope.","To obtain spectral information from the near-UV to the near-infrared, we reanalysed sixteen WFC3 and over fifty STIS archival data sets with our dedicated HST pipeline.","We also include twenty-four WFC3 data sets previously reduced with the same software.","Across our target sample we observe significant divergence among multiple observations conducted with the same STIS grating at various epochs, whilst we do not detect variations in the WFC3 data sets.","These results are suggestive of stellar contamination, which we have investigated further using known Bayesian tools and other tailored metrics, facilitating a more objective assessment of stellar activity intensity within each system.","Our findings reveal that stellar activity contaminates up to half of the studied exoplanet atmospheres, albeit at varying extents.","Accounting for stellar activity can significantly alter planetary atmospheric parameters like molecular abundances (up to 6 orders of magnitude) and temperature (up to 145 %), contrasting with the results of analyses that neglect activity.","Our results emphasise the importance of considering the effects of stellar contamination in exoplanet transit studies; this issue is particularly true for data sets obtained with facilities that do not cover the optical and/or UV spectral range where the activity is expected to be more impactful but also more easily detectable.","Our results also provide a catalogue of potentially active stars for further investigation and monitoring."],"url":"http://arxiv.org/abs/2404.15505v1","category":"astro-ph.EP"}
{"created":"2024-04-23 20:40:36","title":"Simultaneous Chandra and HST observations of the quiescent neutron-star low-mass X-ray binaries in 47 Tucanae","abstract":"We present simultaneous Chandra X-ray Observatory and Hubble Space Telescope observations of three certain (X5, X7, W37) and two likely (X4, W17) quiescent neutron-star low-mass X-ray binaries (qLMXBs) in the globular cluster 47 Tuc. We study these systems in the X-ray, optical and near-ultraviolet (NUV) using the simultaneous data and additional non-contemporaneous HST data. We have discovered a blue and variable NUV counterpart to W17. We have not securely identified the eclipsing qLMXB W37 in the optical or NUV. Deeper high-resolution imaging is needed to further investigate the faint NUV excess near the centre of the W37 error circle. We suggest that a previously identified optical astrometric match to X7 is likely the true counterpart. The Halpha emission and the location of the counterpart in the colour-magnitude diagram, indicate that the secondary is probably a non-degenerate, H-rich star. This is consistent with previous results from fitting X7's X-ray spectrum. In X4, the simultaneous X-ray and optical behaviour supports the earlier suggestion that the X-ray variability is driven by changes in accretion rate. The X-ray eclipses in X5 coincide with minima in the optical/NUV light curves. Comparison of the 47 Tuc qLMXBs with the cataclysmic variables (CVs) in the cluster confirms that overall the qLMXBs have larger X-ray-to-optical flux ratios. Based on their optical/NUV colors, we conclude that the accretion disks in the qLMXBs are less prominent than in CVs. This makes the ratio of X-ray flux to excess blue optical flux a powerful discriminator between CVs and qLMXBs.","sentences":["We present simultaneous Chandra X-ray Observatory and Hubble Space Telescope observations of three certain (X5, X7, W37) and two likely (X4, W17) quiescent neutron-star low-mass X-ray binaries (qLMXBs) in the globular cluster 47 Tuc.","We study these systems in the X-ray, optical and near-ultraviolet (NUV) using the simultaneous data and additional non-contemporaneous HST data.","We have discovered a blue and variable NUV counterpart to W17.","We have not securely identified the eclipsing qLMXB W37 in the optical or NUV.","Deeper high-resolution imaging is needed to further investigate the faint NUV excess near the centre of the W37 error circle.","We suggest that a previously identified optical astrometric match to X7 is likely the true counterpart.","The Halpha emission and the location of the counterpart in the colour-magnitude diagram, indicate that the secondary is probably a non-degenerate, H-rich star.","This is consistent with previous results from fitting X7's X-ray spectrum.","In X4, the simultaneous X-ray and optical behaviour supports the earlier suggestion that the X-ray variability is driven by changes in accretion rate.","The X-ray eclipses in X5 coincide with minima in the optical/NUV light curves.","Comparison of the 47 Tuc qLMXBs with the cataclysmic variables (CVs) in the cluster confirms that overall the qLMXBs have larger X-ray-to-optical flux ratios.","Based on their optical/NUV colors, we conclude that the accretion disks in the qLMXBs are less prominent than in CVs.","This makes the ratio of X-ray flux to excess blue optical flux a powerful discriminator between CVs and qLMXBs."],"url":"http://arxiv.org/abs/2404.15504v1","category":"astro-ph.HE"}
{"created":"2024-04-23 20:31:11","title":"A New Parameterization for Finding Solutions for Microlensing Exoplanet Light Curves","abstract":"The gravitational microlensing method of discovering exoplanets and multi-star systems can produce degenerate solutions, some of which require in-depth analysis to uncover. We propose a new parameter space that can be used to sample potential solutions more efficiently and is more robust at finding all degenerate solutions. We identified two new parameters, k and h, that can be sampled in place of the mass ratios and separations of the systems under analysis to identify degenerate solutions. The parameter k is related to the size of the central caustic, $\\Delta\\xi_c$, while h is related to the distance of a point along the k contour from log(s)=0, where s is the projected planet-host separation. In this work, we present the characteristics of these parameters and the tests we conducted to prove their efficacy.","sentences":["The gravitational microlensing method of discovering exoplanets and multi-star systems can produce degenerate solutions, some of which require in-depth analysis to uncover.","We propose a new parameter space that can be used to sample potential solutions more efficiently and is more robust at finding all degenerate solutions.","We identified two new parameters, k and h, that can be sampled in place of the mass ratios and separations of the systems under analysis to identify degenerate solutions.","The parameter k is related to the size of the central caustic, $\\Delta\\xi_c$, while h is related to the distance of a point along the k contour from log(s)=0, where s is the projected planet-host separation.","In this work, we present the characteristics of these parameters and the tests we conducted to prove their efficacy."],"url":"http://arxiv.org/abs/2404.15502v1","category":"astro-ph.EP"}
{"created":"2024-04-23 20:20:27","title":"Drop-Connect as a Fault-Tolerance Approach for RRAM-based Deep Neural Network Accelerators","abstract":"Resistive random-access memory (RRAM) is widely recognized as a promising emerging hardware platform for deep neural networks (DNNs). Yet, due to manufacturing limitations, current RRAM devices are highly susceptible to hardware defects, which poses a significant challenge to their practical applicability. In this paper, we present a machine learning technique that enables the deployment of defect-prone RRAM accelerators for DNN applications, without necessitating modifying the hardware, retraining of the neural network, or implementing additional detection circuitry/logic. The key idea involves incorporating a drop-connect inspired approach during the training phase of a DNN, where random subsets of weights are selected to emulate fault effects (e.g., set to zero to mimic stuck-at-1 faults), thereby equipping the DNN with the ability to learn and adapt to RRAM defects with the corresponding fault rates. Our results demonstrate the viability of the drop-connect approach, coupled with various algorithm and system-level design and trade-off considerations. We show that, even in the presence of high defect rates (e.g., up to 30%), the degradation of DNN accuracy can be as low as less than 1% compared to that of the fault-free version, while incurring minimal system-level runtime/energy costs.","sentences":["Resistive random-access memory (RRAM) is widely recognized as a promising emerging hardware platform for deep neural networks (DNNs).","Yet, due to manufacturing limitations, current RRAM devices are highly susceptible to hardware defects, which poses a significant challenge to their practical applicability.","In this paper, we present a machine learning technique that enables the deployment of defect-prone RRAM accelerators for DNN applications, without necessitating modifying the hardware, retraining of the neural network, or implementing additional detection circuitry/logic.","The key idea involves incorporating a drop-connect inspired approach during the training phase of a DNN, where random subsets of weights are selected to emulate fault effects (e.g., set to zero to mimic stuck-at-1 faults), thereby equipping the DNN with the ability to learn and adapt to RRAM defects with the corresponding fault rates.","Our results demonstrate the viability of the drop-connect approach, coupled with various algorithm and system-level design and trade-off considerations.","We show that, even in the presence of high defect rates (e.g., up to 30%), the degradation of DNN accuracy can be as low as less than 1% compared to that of the fault-free version, while incurring minimal system-level runtime/energy costs."],"url":"http://arxiv.org/abs/2404.15498v1","category":"cs.ET"}
{"created":"2024-04-23 20:20:00","title":"Deep-learning Optical Flow Outperforms PIV in Obtaining Velocity Fields from Active Nematics","abstract":"Deep learning-based optical flow (DLOF) extracts features in adjacent video frames with deep convolutional neural networks. It uses those features to estimate the inter-frame motions of objects at the pixel level. In this article, we evaluate the ability of optical flow to quantify the spontaneous flows of MT-based active nematics under different labeling conditions. We compare DLOF against the commonly used technique, particle imaging velocimetry (PIV). We obtain flow velocity ground truths either by performing semi-automated particle tracking on samples with sparsely labeled filaments, or from passive tracer beads. We find that DLOF produces significantly more accurate velocity fields than PIV for densely labeled samples. We show that the breakdown of PIV arises because the algorithm cannot reliably distinguish contrast variations at high densities, particularly in directions parallel to the nematic director. DLOF overcomes this limitation. For sparsely labeled samples, DLOF and PIV produce results with similar accuracy, but DLOF gives higher-resolution fields. Our work establishes DLOF as a versatile tool for measuring fluid flows in a broad class of active, soft, and biophysical systems.","sentences":["Deep learning-based optical flow (DLOF) extracts features in adjacent video frames with deep convolutional neural networks.","It uses those features to estimate the inter-frame motions of objects at the pixel level.","In this article, we evaluate the ability of optical flow to quantify the spontaneous flows of MT-based active nematics under different labeling conditions.","We compare DLOF against the commonly used technique, particle imaging velocimetry (PIV).","We obtain flow velocity ground truths either by performing semi-automated particle tracking on samples with sparsely labeled filaments, or from passive tracer beads.","We find that DLOF produces significantly more accurate velocity fields than PIV for densely labeled samples.","We show that the breakdown of PIV arises because the algorithm cannot reliably distinguish contrast variations at high densities, particularly in directions parallel to the nematic director.","DLOF overcomes this limitation.","For sparsely labeled samples, DLOF and PIV produce results with similar accuracy, but DLOF gives higher-resolution fields.","Our work establishes DLOF as a versatile tool for measuring fluid flows in a broad class of active, soft, and biophysical systems."],"url":"http://arxiv.org/abs/2404.15497v1","category":"cond-mat.soft"}
{"created":"2024-04-23 19:59:03","title":"Minimum Consistent Subset in Trees and Interval Graphs","abstract":"In the Minimum Consistent Subset (MCS) problem, we are presented with a connected simple undirected graph $G=(V,E)$, consisting of a vertex set $V$ of size $n$ and an edge set $E$. Each vertex in $V$ is assigned a color from the set $\\{1,2,\\ldots, c\\}$. The objective is to determine a subset $V' \\subseteq V$ with minimum possible cardinality, such that for every vertex $v \\in V$, at least one of its nearest neighbors in $V'$ (measured in terms of the hop distance) shares the same color as $v$. The decision problem, indicating whether there exists a subset $V'$ of cardinality at most $l$ for some positive integer $l$, is known to be NP-complete even for planar graphs.   In this paper, we establish that the MCS problem for trees, when the number of colors $c$ is considered an input parameter, is NP-complete. We propose a fixed-parameter tractable (FPT) algorithm for MCS on trees running in $O(2^{6c}n^6)$ time, significantly improving the currently best-known algorithm whose running time is $O(2^{4c}n^{2c+3})$.   In an effort to comprehensively understand the computational complexity of the MCS problem across different graph classes, we extend our investigation to interval graphs. We show that it remains NP-complete for interval graphs, thus enriching graph classes where MCS remains intractable.","sentences":["In the Minimum Consistent Subset (MCS) problem, we are presented with a connected simple undirected graph $G=(V,E)$, consisting of a vertex set $V$ of size $n$ and an edge set $E$. Each vertex in $V$ is assigned a color from the set $\\{1,2,\\ldots, c\\}$. The objective is to determine a subset $V' \\subseteq V$ with minimum possible cardinality, such that for every vertex $v \\in V$, at least one of its nearest neighbors in $V'$ (measured in terms of the hop distance) shares the same color as $v$. The decision problem, indicating whether there exists a subset $V'$ of cardinality at most $l$ for some positive integer $l$, is known to be NP-complete even for planar graphs.   ","In this paper, we establish that the MCS problem for trees, when the number of colors $c$ is considered an input parameter, is NP-complete.","We propose a fixed-parameter tractable (FPT) algorithm for MCS on trees running in $O(2^{6c}n^6)$ time, significantly improving the currently best-known algorithm whose running time is $O(2^{4c}n^{2c+3})$.   In an effort to comprehensively understand the computational complexity of the MCS problem across different graph classes, we extend our investigation to interval graphs.","We show that it remains NP-complete for interval graphs, thus enriching graph classes where MCS remains intractable."],"url":"http://arxiv.org/abs/2404.15487v1","category":"cs.CG"}
{"created":"2024-04-23 19:49:50","title":"Strategy Complexity of B\u00fcchi Objectives in Concurrent Stochastic Games","abstract":"We study 2-player concurrent stochastic B\\\"uchi games on countable graphs. Two players, Max and Min, seek respectively to maximize and minimize the probability of visiting a set of target states infinitely often. We show that there always exist $\\varepsilon$-optimal Max strategies that use just a step counter plus 1 bit of public memory. This upper bound holds for all countable graphs, but it is a new result even for the special case of finite graphs. The upper bound is tight in the sense that Max strategies that use just a step counter, or just finite memory, are not sufficient even on finite game graphs.   The upper bound is a consequence of a slightly stronger new result: $\\varepsilon$-optimal Max strategies for the combined B\\\"uchi and Transience objective require just 1 bit of public memory (but cannot be memoryless). Our proof techniques also yield a closely related result, that $\\varepsilon$-optimal Max strategies for the Transience objective alone (which is only meaningful in infinite graphs) can be memoryless.","sentences":["We study 2-player concurrent stochastic B\\\"uchi games on countable graphs.","Two players, Max and Min, seek respectively to maximize and minimize the probability of visiting a set of target states infinitely often.","We show that there always exist $\\varepsilon$-optimal Max strategies that use just a step counter plus 1 bit of public memory.","This upper bound holds for all countable graphs, but it is a new result even for the special case of finite graphs.","The upper bound is tight in the sense that Max strategies that use just a step counter, or just finite memory, are not sufficient even on finite game graphs.   ","The upper bound is a consequence of a slightly stronger new result: $\\varepsilon$-optimal Max strategies for the combined B\\\"uchi and Transience objective require just 1 bit of public memory (but cannot be memoryless).","Our proof techniques also yield a closely related result, that $\\varepsilon$-optimal Max strategies for the Transience objective alone (which is only meaningful in infinite graphs) can be memoryless."],"url":"http://arxiv.org/abs/2404.15483v1","category":"cs.GT"}
{"created":"2024-04-23 19:31:12","title":"Entanglement in Quantum Dots: Insights from Dynamic Susceptibility and Quantum Fisher Information","abstract":"This study investigates the entanglement properties of quantum dots (QDs) under a universal Hamiltonian where the Coulomb interaction between particles (electrons or holes) decouples into a charging energy and an exchange coupling term. While this formalism typically decouples the charge and spin components, the confinement-induced energy splitting can induce unexpected entanglement in the system. By analyzing the dynamic susceptibility and quantum Fisher information (QFI), we uncover intriguing behaviors influenced by exchange constants, temperature variations, and confinement effects. In Ising QDs, far below the Stoner instability point where the QD is in a disordered paramagnetic phase, temperature reductions unexpectedly lead to decreased entanglement, challenging conventional expectations. Conversely, anisotropic Heisenberg models exhibit enhanced entanglement near isotropic points. Our findings highlight the intricate interplay between exchange interactions and entanglement in QDs, laying the groundwork for future studies on topological entanglement and the influence of entanglement on material properties. Overall, this work contributes to advancing our understanding of entanglement in QDs and its potential applications in quantum technologies.","sentences":["This study investigates the entanglement properties of quantum dots (QDs) under a universal Hamiltonian where the Coulomb interaction between particles (electrons or holes) decouples into a charging energy and an exchange coupling term.","While this formalism typically decouples the charge and spin components, the confinement-induced energy splitting can induce unexpected entanglement in the system.","By analyzing the dynamic susceptibility and quantum Fisher information (QFI), we uncover intriguing behaviors influenced by exchange constants, temperature variations, and confinement effects.","In Ising QDs, far below the Stoner instability point where the QD is in a disordered paramagnetic phase, temperature reductions unexpectedly lead to decreased entanglement, challenging conventional expectations.","Conversely, anisotropic Heisenberg models exhibit enhanced entanglement near isotropic points.","Our findings highlight the intricate interplay between exchange interactions and entanglement in QDs, laying the groundwork for future studies on topological entanglement and the influence of entanglement on material properties.","Overall, this work contributes to advancing our understanding of entanglement in QDs and its potential applications in quantum technologies."],"url":"http://arxiv.org/abs/2404.15477v1","category":"quant-ph"}
{"created":"2024-04-23 19:20:41","title":"Training all-mechanical neural networks for task learning through in situ backpropagation","abstract":"Recent advances unveiled physical neural networks as promising machine learning platforms, offering faster and more energy-efficient information processing. Compared with extensively-studied optical neural networks, the development of mechanical neural networks (MNNs) remains nascent and faces significant challenges, including heavy computational demands and learning with approximate gradients. Here, we introduce the mechanical analogue of in situ backpropagation to enable highly efficient training of MNNs. We demonstrate that the exact gradient can be obtained locally in MNNs, enabling learning through their immediate vicinity. With the gradient information, we showcase the successful training of MNNs for behavior learning and machine learning tasks, achieving high accuracy in regression and classification. Furthermore, we present the retrainability of MNNs involving task-switching and damage, demonstrating the resilience. Our findings, which integrate the theory for training MNNs and experimental and numerical validations, pave the way for mechanical machine learning hardware and autonomous self-learning material systems.","sentences":["Recent advances unveiled physical neural networks as promising machine learning platforms, offering faster and more energy-efficient information processing.","Compared with extensively-studied optical neural networks, the development of mechanical neural networks (MNNs) remains nascent and faces significant challenges, including heavy computational demands and learning with approximate gradients.","Here, we introduce the mechanical analogue of in situ backpropagation to enable highly efficient training of MNNs.","We demonstrate that the exact gradient can be obtained locally in MNNs, enabling learning through their immediate vicinity.","With the gradient information, we showcase the successful training of MNNs for behavior learning and machine learning tasks, achieving high accuracy in regression and classification.","Furthermore, we present the retrainability of MNNs involving task-switching and damage, demonstrating the resilience.","Our findings, which integrate the theory for training MNNs and experimental and numerical validations, pave the way for mechanical machine learning hardware and autonomous self-learning material systems."],"url":"http://arxiv.org/abs/2404.15471v1","category":"cs.LG"}
{"created":"2024-04-23 19:20:17","title":"Supersymmetric Analysis of Spinning Cosmic String Spacetime Within External fields with Aharonov-Bohm interaction","abstract":"The supersymmetric analysis of spinning cosmic string spacetime, involving an electron in magnetic fields, has been conducted. We examined the Dirac system within extended special functions known as exceptional orthogonal polynomials. Corresponding Dirac system is transformed to a relativistic system with a nonlinear isotonic oscillator. Furthermore, new potential models that extend the radial oscillator by adding rational terms are expressed in terms of the exceptional orthogonal Laguerre $X_{m}$ polynomial. The necessary analyses of the potential, energy levels, and probability density graphs are introduced for various cosmic string topological defects and Aharonov-Bohm interaction parameters.","sentences":["The supersymmetric analysis of spinning cosmic string spacetime, involving an electron in magnetic fields, has been conducted.","We examined the Dirac system within extended special functions known as exceptional orthogonal polynomials.","Corresponding Dirac system is transformed to a relativistic system with a nonlinear isotonic oscillator.","Furthermore, new potential models that extend the radial oscillator by adding rational terms are expressed in terms of the exceptional orthogonal Laguerre $X_{m}$ polynomial.","The necessary analyses of the potential, energy levels, and probability density graphs are introduced for various cosmic string topological defects and Aharonov-Bohm interaction parameters."],"url":"http://arxiv.org/abs/2404.15470v1","category":"math-ph"}
{"created":"2024-04-23 19:12:34","title":"Environmental permittivity-asymmetric BIC metasurfaces with electrical reconfigurability","abstract":"In the rapidly evolving field of nanophotonics, achieving precise spectral and temporal light manipulation at the nanoscale remains a critical challenge. While photonic bound states in the continuum (BICs) have emerged as a powerful means of controlling light, their common reliance on geometrical symmetry breaking for obtaining tailored resonances makes them highly susceptible to fabrication imperfections and fundamentally limits their maximum resonance quality factor. Here, we introduce the concept of environmental symmetry breaking by embedding identical resonators into a surrounding medium with carefully placed regions of contrasting refractive indexes, activating permittivity-driven quasi-BIC resonances without any alterations of the underlying resonator geometry and unlocking an additional degree of freedom for light manipulation through actively tuning the surrounding refractive index contrast. We demonstrate this concept by integrating polyaniline (PANI), an electro-optically active polymer, to achieve electrically reconfigurable qBICs. This integration not only demonstrates rapid switching speeds, and exceptional durability but also significantly boosts the system's optical response to environmental perturbations. Our strategy significantly expands the capabilities of resonant light manipulation through permittivity modulation, opening avenues for on-chip optical devices, advanced sensing, and beyond.","sentences":["In the rapidly evolving field of nanophotonics, achieving precise spectral and temporal light manipulation at the nanoscale remains a critical challenge.","While photonic bound states in the continuum (BICs) have emerged as a powerful means of controlling light, their common reliance on geometrical symmetry breaking for obtaining tailored resonances makes them highly susceptible to fabrication imperfections and fundamentally limits their maximum resonance quality factor.","Here, we introduce the concept of environmental symmetry breaking by embedding identical resonators into a surrounding medium with carefully placed regions of contrasting refractive indexes, activating permittivity-driven quasi-BIC resonances without any alterations of the underlying resonator geometry and unlocking an additional degree of freedom for light manipulation through actively tuning the surrounding refractive index contrast.","We demonstrate this concept by integrating polyaniline (PANI), an electro-optically active polymer, to achieve electrically reconfigurable qBICs.","This integration not only demonstrates rapid switching speeds, and exceptional durability but also significantly boosts the system's optical response to environmental perturbations.","Our strategy significantly expands the capabilities of resonant light manipulation through permittivity modulation, opening avenues for on-chip optical devices, advanced sensing, and beyond."],"url":"http://arxiv.org/abs/2404.15462v1","category":"physics.optics"}
{"created":"2024-04-23 19:11:53","title":"Meta-stable states in the Ising model with Glauber-Kawasaki competing dynamics","abstract":"Meta-stable states are identified in the Ising model with competition between the Glauber and Kawasaki dynamics. The model of interaction between magnetic moments was implemented on a network where the degree distribution follows a power-law of the form, $P(k)\\sim k^{-\\alpha}$. The evolution towards the stationary state occurred through the competition between two dynamics, driving the system out of equilibrium. In this competition, with probability $q$, the system was simulated in contact with a heat bath at temperature $T$ by the Glauber dynamics, while with probability $1-q$, the system experienced an external energy influx governed by the Kawasaki dynamics. The phase diagrams of $T$ versus $q$ were obtained, which are dependent on the initial state of the system, and exhibit first- and second-order phase transitions. In all diagrams, for intermediate values of $T$, the phenomenon of self-organization between the ordered phases was observed. In the regions of second-order phase transitions, we have verified the universality class of the system through the critical exponents of the order parameter $\\beta$, susceptibility $\\gamma$, and correlation length $\\nu$. Furthermore, in the regions of first-order phase transitions, we have demonstrated the instability due to transitions between the ordered phases through hysteresis-like curves of the order parameter, in addition to the existence of absorbing states. We also estimated the value of the tricritical points when the discontinuity in the order parameter in the phase transitions was no longer observed.","sentences":["Meta-stable states are identified in the Ising model with competition between the Glauber and Kawasaki dynamics.","The model of interaction between magnetic moments was implemented on a network where the degree distribution follows a power-law of the form,","$P(k)\\sim k^{-\\alpha}$.","The evolution towards the stationary state occurred through the competition between two dynamics, driving the system out of equilibrium.","In this competition, with probability $q$, the system was simulated in contact with a heat bath at temperature $T$ by the Glauber dynamics, while with probability $1-q$, the system experienced an external energy influx governed by the Kawasaki dynamics.","The phase diagrams of $T$ versus $q$ were obtained, which are dependent on the initial state of the system, and exhibit first- and second-order phase transitions.","In all diagrams, for intermediate values of $T$, the phenomenon of self-organization between the ordered phases was observed.","In the regions of second-order phase transitions, we have verified the universality class of the system through the critical exponents of the order parameter $\\beta$, susceptibility $\\gamma$, and correlation length $\\nu$. Furthermore, in the regions of first-order phase transitions, we have demonstrated the instability due to transitions between the ordered phases through hysteresis-like curves of the order parameter, in addition to the existence of absorbing states.","We also estimated the value of the tricritical points when the discontinuity in the order parameter in the phase transitions was no longer observed."],"url":"http://arxiv.org/abs/2404.15460v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-23 19:05:42","title":"Can Large Language Models Learn the Physics of Metamaterials? An Empirical Study with ChatGPT","abstract":"Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans. We present a LLM fine-tuned on up to 40,000 data that can predict electromagnetic spectra over a range of frequencies given a text prompt that only specifies the metasurface geometry. Results are compared to conventional machine learning approaches including feed-forward neural networks, random forest, linear regression, and K-nearest neighbor (KNN). Remarkably, the fine-tuned LLM (FT-LLM) achieves a lower error across all dataset sizes explored compared to all machine learning approaches including a deep neural network. We also demonstrate the LLM's ability to solve inverse problems by providing the geometry necessary to achieve a desired spectrum. LLMs possess some advantages over humans that may give them benefits for research, including the ability to process enormous amounts of data, find hidden patterns in data, and operate in higher-dimensional spaces. We propose that fine-tuning LLMs on large datasets specific to a field allows them to grasp the nuances of that domain, making them valuable tools for research and analysis.","sentences":["Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans.","We present a LLM fine-tuned on up to 40,000 data that can predict electromagnetic spectra over a range of frequencies given a text prompt that only specifies the metasurface geometry.","Results are compared to conventional machine learning approaches including feed-forward neural networks, random forest, linear regression, and K-nearest neighbor (KNN).","Remarkably, the fine-tuned LLM (FT-LLM) achieves a lower error across all dataset sizes explored compared to all machine learning approaches including a deep neural network.","We also demonstrate the LLM's ability to solve inverse problems by providing the geometry necessary to achieve a desired spectrum.","LLMs possess some advantages over humans that may give them benefits for research, including the ability to process enormous amounts of data, find hidden patterns in data, and operate in higher-dimensional spaces.","We propose that fine-tuning LLMs on large datasets specific to a field allows them to grasp the nuances of that domain, making them valuable tools for research and analysis."],"url":"http://arxiv.org/abs/2404.15458v1","category":"physics.optics"}
{"created":"2024-04-23 18:37:37","title":"Deep multi-prototype capsule networks","abstract":"Capsule networks are a type of neural network that identify image parts and form the instantiation parameters of a whole hierarchically. The goal behind the network is to perform an inverse computer graphics task, and the network parameters are the mapping weights that transform parts into a whole. The trainability of capsule networks in complex data with high intra-class or intra-part variation is challenging. This paper presents a multi-prototype architecture for guiding capsule networks to represent the variations in the image parts. To this end, instead of considering a single capsule for each class and part, the proposed method employs several capsules (co-group capsules), capturing multiple prototypes of an object. In the final layer, co-group capsules compete, and their soft output is considered the target for a competitive cross-entropy loss. Moreover, in the middle layers, the most active capsules map to the next layer with a shared weight among the co-groups. Consequently, due to the reduction in parameters, implicit weight-sharing makes it possible to have more deep capsule network layers. The experimental results on MNIST, SVHN, C-Cube, CEDAR, MCYT, and UTSig datasets reveal that the proposed model outperforms others regarding image classification accuracy.","sentences":["Capsule networks are a type of neural network that identify image parts and form the instantiation parameters of a whole hierarchically.","The goal behind the network is to perform an inverse computer graphics task, and the network parameters are the mapping weights that transform parts into a whole.","The trainability of capsule networks in complex data with high intra-class or intra-part variation is challenging.","This paper presents a multi-prototype architecture for guiding capsule networks to represent the variations in the image parts.","To this end, instead of considering a single capsule for each class and part, the proposed method employs several capsules (co-group capsules), capturing multiple prototypes of an object.","In the final layer, co-group capsules compete, and their soft output is considered the target for a competitive cross-entropy loss.","Moreover, in the middle layers, the most active capsules map to the next layer with a shared weight among the co-groups.","Consequently, due to the reduction in parameters, implicit weight-sharing makes it possible to have more deep capsule network layers.","The experimental results on MNIST, SVHN, C-Cube, CEDAR, MCYT, and UTSig datasets reveal that the proposed model outperforms others regarding image classification accuracy."],"url":"http://arxiv.org/abs/2404.15445v1","category":"cs.CV"}
{"created":"2024-04-23 18:36:01","title":"The Frobenius equivalence and Beck-Chevalley condition for Algebraic Weak Factorisation Systems","abstract":"If a locally cartesian closed category carries a weak factorisation system, then the left maps are stable under pullback along right maps if and only if the right maps are closed under pushforward along right maps. We refer to this statement as the Frobenius equivalence and in this paper we state and prove an analogical statement for algebraic weak factorisation systems. These algebraic weak factorisation systems are an explicit variant of the more traditional weak factorisation systems in that the factorisation and the lifts are part of the structure of an algebraic weak factorisation system and are not merely required to exist. Our work has been motivated by the categorical semantics of type theory, where the Frobenius equivalence provides a useful tool for constructing dependent function types. We illustrate our ideas using split fibrations of groupoids, which are the backbone of the groupoid model of Hofmann and Streicher.","sentences":["If a locally cartesian closed category carries a weak factorisation system, then the left maps are stable under pullback along right maps if and only if the right maps are closed under pushforward along right maps.","We refer to this statement as the Frobenius equivalence and in this paper we state and prove an analogical statement for algebraic weak factorisation systems.","These algebraic weak factorisation systems are an explicit variant of the more traditional weak factorisation systems in that the factorisation and the lifts are part of the structure of an algebraic weak factorisation system and are not merely required to exist.","Our work has been motivated by the categorical semantics of type theory, where the Frobenius equivalence provides a useful tool for constructing dependent function types.","We illustrate our ideas using split fibrations of groupoids, which are the backbone of the groupoid model of Hofmann and Streicher."],"url":"http://arxiv.org/abs/2404.15443v1","category":"math.CT"}
{"created":"2024-04-23 18:19:48","title":"The early Earth as an analogue for exoplanetary biogeochemistry","abstract":"Planet Earth has evolved from an entirely anoxic planet with possibly a different tectonic regime to the oxygenated world with horizontal plate tectonics that we know today. For most of this time, Earth has been inhabited by a purely microbial biosphere albeit with seemingly increasing complexity over time. A rich record of this geobiological evolution over most of Earth's history provides insights into the remote detectability of microbial life under a variety of planetary conditions. We leverage Earth's geobiological record with the aim of a) illustrating the current state of knowledge and key knowledge gaps about the early Earth as a reference point in exoplanet science research; b) compiling biotic and abiotic mechanisms that controlled the evolution of the atmosphere over time; and c) reviewing current constraints on the detectability of Earth's early biosphere with state-of-the-art telescope technology. We highlight that life may have originated on a planet with a different tectonic regime and strong hydrothermal activity, and under these conditions, biogenic CH$_4$ gas was perhaps the most detectable atmospheric biosignature. Oxygenic photosynthesis, which is responsible for essentially all O$_2$ gas in the modern atmosphere, appears to have emerged concurrently with the establishment of modern plate tectonics and the continental crust, but O$_2$ accumulation to modern levels only occurred late in Earth's history, perhaps tied to the rise of land plants. Nutrient limitation in anoxic oceans, promoted by hydrothermal Fe = fluxes, may have limited biological productivity and O$_2$ production. N$_2$O is an alternative biosignature that was perhaps significant on the redox-stratified Proterozoic Earth. We conclude that the detectability of atmospheric biosignatures on Earth was not only dependent on biological evolution but also strongly controlled by the evolving tectonic context.","sentences":["Planet Earth has evolved from an entirely anoxic planet with possibly a different tectonic regime to the oxygenated world with horizontal plate tectonics that we know today.","For most of this time, Earth has been inhabited by a purely microbial biosphere albeit with seemingly increasing complexity over time.","A rich record of this geobiological evolution over most of Earth's history provides insights into the remote detectability of microbial life under a variety of planetary conditions.","We leverage Earth's geobiological record with the aim of a) illustrating the current state of knowledge and key knowledge gaps about the early Earth as a reference point in exoplanet science research; b) compiling biotic and abiotic mechanisms that controlled the evolution of the atmosphere over time; and c) reviewing current constraints on the detectability of Earth's early biosphere with state-of-the-art telescope technology.","We highlight that life may have originated on a planet with a different tectonic regime and strong hydrothermal activity, and under these conditions, biogenic CH$_4$ gas was perhaps the most detectable atmospheric biosignature.","Oxygenic photosynthesis, which is responsible for essentially all O$_2$ gas in the modern atmosphere, appears to have emerged concurrently with the establishment of modern plate tectonics and the continental crust, but O$_2$ accumulation to modern levels only occurred late in Earth's history, perhaps tied to the rise of land plants.","Nutrient limitation in anoxic oceans, promoted by hydrothermal Fe = fluxes, may have limited biological productivity and O$_2$ production.","N$_2$O is an alternative biosignature that was perhaps significant on the redox-stratified Proterozoic Earth.","We conclude that the detectability of atmospheric biosignatures on Earth was not only dependent on biological evolution but also strongly controlled by the evolving tectonic context."],"url":"http://arxiv.org/abs/2404.15432v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:19:37","title":"Transiting Exoplanet Atmospheres in the Era of JWST","abstract":"The field of exoplanet atmospheric characterization has recently made considerable advances with the advent of high-resolution spectroscopy from large ground-based telescopes and the commissioning of the James Webb Space Telescope (JWST). We have entered an era in which atmospheric compositions, aerosol properties, thermal structures, mass loss, and three-dimensional effects can be reliably constrained. While the challenges of remote sensing techniques imply that individual exoplanet atmospheres will likely never be characterized to the degree of detail that is possible for solar system bodies, exoplanets present an exciting opportunity to characterize a diverse array of worlds with properties that are not represented in our solar system. This review article summarizes the current state of exoplanet atmospheric studies for transiting planets. We focus on how observational results inform our understanding of exoplanet properties and ultimately address broad questions about planetary formation, evolution, and diversity. This review is meant to provide an overview of the exoplanet atmospheres field for planetary- and geo-scientists without astronomy backgrounds, and exoplanet specialists, alike. We give special attention to the first year of JWST data and recent results in high-resolution spectroscopy that have not been summarized by previous review articles.","sentences":["The field of exoplanet atmospheric characterization has recently made considerable advances with the advent of high-resolution spectroscopy from large ground-based telescopes and the commissioning of the James Webb Space Telescope (JWST).","We have entered an era in which atmospheric compositions, aerosol properties, thermal structures, mass loss, and three-dimensional effects can be reliably constrained.","While the challenges of remote sensing techniques imply that individual exoplanet atmospheres will likely never be characterized to the degree of detail that is possible for solar system bodies, exoplanets present an exciting opportunity to characterize a diverse array of worlds with properties that are not represented in our solar system.","This review article summarizes the current state of exoplanet atmospheric studies for transiting planets.","We focus on how observational results inform our understanding of exoplanet properties and ultimately address broad questions about planetary formation, evolution, and diversity.","This review is meant to provide an overview of the exoplanet atmospheres field for planetary- and geo-scientists without astronomy backgrounds, and exoplanet specialists, alike.","We give special attention to the first year of JWST data and recent results in high-resolution spectroscopy that have not been summarized by previous review articles."],"url":"http://arxiv.org/abs/2404.15430v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:19:16","title":"Some Tectonic Concepts Relevant to the Study of Rocky Exoplanets","abstract":"We'll examine plate tectonics on Earth -- its features and forces -- and examine some concepts that may allow astronomers to ask useful questions regarding numeric models that putatively predict tectonic activity. But exo-planetologists should be aware that geologists are still attempting to understand: why does Earth operates as it does, and so much differently than its neighbors? Has it always operated this way and have other planets of the inner Solar System ever mimicked Earth's behavior in their past? These problems are unsolved, though some interesting speculative notions have emerged. Studies by Foley et al. et al. (2012) and Weller and Lenardic (2018), for example, attempt to distill the essential planetary properties that may influence if not dictate possible tectonic states, while Yin et al. (2016) propose a model of planetary tectonic surface features that appears remarkably precise. These studies yield some compelling expedients for analyses of planetary objects both within and outside our Solar System.","sentences":["We'll examine plate tectonics on Earth -- its features and forces -- and examine some concepts that may allow astronomers to ask useful questions regarding numeric models that putatively predict tectonic activity.","But exo-planetologists should be aware that geologists are still attempting to understand: why does Earth operates as it does, and so much differently than its neighbors?","Has it always operated this way and have other planets of the inner Solar System ever mimicked Earth's behavior in their past?","These problems are unsolved, though some interesting speculative notions have emerged.","Studies by Foley et al. et al.","(2012) and Weller and Lenardic (2018), for example, attempt to distill the essential planetary properties that may influence if not dictate possible tectonic states, while Yin et al.","(2016) propose a model of planetary tectonic surface features that appears remarkably precise.","These studies yield some compelling expedients for analyses of planetary objects both within and outside our Solar System."],"url":"http://arxiv.org/abs/2404.15428v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:18:56","title":"The chemistry of extra-solar materials from white dwarf planetary systems","abstract":"White dwarf planetary systems provide a unique way to measure the bulk composition of exoplanetary material. Extrasolar asteroids/comets/moons which have survived the evolution of their host star can end up in the atmosphere of the white dwarf. Asteroids and boulders appear to be the most common pollutants, where we use the term \"asteroids\" to refer to the parent body that is polluting the atmosphere. The presence of the planetary material is detected via absorption lines of heavy elements. White dwarfs with these absorption features are called \"polluted\" white dwarfs. Polluted white dwarfs were expected to be rare objects because white dwarfs have high surface gravities, therefore, these heavy elements will settle out of the white dwarf's atmospheres in a short amount of time (Paquette et al. 1986). However, high-resolution spectroscopic surveys found that 25-50% of white dwarfs are polluted (Zuckerman et al. 2003, 2010; Koester et al. 2014). The mechanism responsible for making a polluted white dwarf must be common and efficient. There is strong theoretical and observational evidence that white dwarfs are accreting from planetary material. There are different mechanisms that can deliver exoplanetary material into the Roche lobe of the white dwarf. Debris disks, transits from disintegrating bodies, and intact planets have all been detected around white dwarfs (e.g., Jura et al. 2007; Vanderburg et al. 2015, 2020). This chapter will describe how the chemical autopsies are conducted, and what is learnt about exoplanetary material from polluted white dwarfs.","sentences":["White dwarf planetary systems provide a unique way to measure the bulk composition of exoplanetary material.","Extrasolar asteroids/comets/moons which have survived the evolution of their host star can end up in the atmosphere of the white dwarf.","Asteroids and boulders appear to be the most common pollutants, where we use the term \"asteroids\" to refer to the parent body that is polluting the atmosphere.","The presence of the planetary material is detected via absorption lines of heavy elements.","White dwarfs with these absorption features are called \"polluted\" white dwarfs.","Polluted white dwarfs were expected to be rare objects because white dwarfs have high surface gravities, therefore, these heavy elements will settle out of the white dwarf's atmospheres in a short amount of time (Paquette et al. 1986).","However, high-resolution spectroscopic surveys found that 25-50% of white dwarfs are polluted (Zuckerman et al. 2003, 2010; Koester et al. 2014).","The mechanism responsible for making a polluted white dwarf must be common and efficient.","There is strong theoretical and observational evidence that white dwarfs are accreting from planetary material.","There are different mechanisms that can deliver exoplanetary material into the Roche lobe of the white dwarf.","Debris disks, transits from disintegrating bodies, and intact planets have all been detected around white dwarfs (e.g., Jura et al. 2007; Vanderburg et al. 2015, 2020).","This chapter will describe how the chemical autopsies are conducted, and what is learnt about exoplanetary material from polluted white dwarfs."],"url":"http://arxiv.org/abs/2404.15425v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:18:37","title":"Chemistry in Protoplanetary Disks","abstract":"Planets are formed inside disks around young stars. The gas, dust, and ice in these natal disks are the building materials of planets, and therefore their compositions fundamentally shape the final chemical compositions of planets. In this review, we summarize current observations of molecular lines in protoplanetary disks, from near-infrared to millimeter wavelengths. We discuss the basic types of chemical reactions in disks and the current development of chemical modeling. In particular, we highlight the progress made in understanding snowline locations, abundances of main carriers of carbon, oxygen, and nitrogen, and complex organic molecules in disks. Finally, we discuss efforts to trace planet formation history by combining the understanding of disk chemistry and planet formation processes.","sentences":["Planets are formed inside disks around young stars.","The gas, dust, and ice in these natal disks are the building materials of planets, and therefore their compositions fundamentally shape the final chemical compositions of planets.","In this review, we summarize current observations of molecular lines in protoplanetary disks, from near-infrared to millimeter wavelengths.","We discuss the basic types of chemical reactions in disks and the current development of chemical modeling.","In particular, we highlight the progress made in understanding snowline locations, abundances of main carriers of carbon, oxygen, and nitrogen, and complex organic molecules in disks.","Finally, we discuss efforts to trace planet formation history by combining the understanding of disk chemistry and planet formation processes."],"url":"http://arxiv.org/abs/2404.15423v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:11:38","title":"Lov\u00e1sz Theorems for Modal Languages","abstract":"A famous result due to Lov\\'asz states that two finite relational structures $M$ and $N$ are isomorphic if, and only if, for all finite relational structures $T$, the number of homomorphisms from $T$ to $M$ is equal to the number of homomorphisms from $T$ to $N$. Since first-order logic (FOL) can describe finite structures up to isomorphism, this can be interpreted as a characterization of FOL-equivalence via homomorphism-count indistinguishability with respect to the class of finite structures. We identify classes of labeled transition systems (LTSs) such that homomorphism-count indistinguishability with respect to these classes, where counting is done within an appropriate semiring structure, captures equivalence with respect to positive-existential modal logic, graded modal logic, and hybrid logic, as well as the extensions of these logics with either backward or global modalities. A novelty of our positive results is that they apply not only to finite structures, as with previous Lov\\'asz-style theorems, but also to well-behaved infinite structures. We also show that equivalence with respect to positive modal logic and equivalence with respect to the basic modal language are not captured by homomorphism-count indistinguishability with respect to any class of LTSs, regardless of which semiring is used for counting.","sentences":["A famous result due to Lov\\'asz states that two finite relational structures $M$ and $N$ are isomorphic if, and only if, for all finite relational structures $T$, the number of homomorphisms from $T$ to $M$ is equal to the number of homomorphisms from $T$ to $N$. Since first-order logic (FOL) can describe finite structures up to isomorphism, this can be interpreted as a characterization of FOL-equivalence via homomorphism-count indistinguishability with respect to the class of finite structures.","We identify classes of labeled transition systems (LTSs) such that homomorphism-count indistinguishability with respect to these classes, where counting is done within an appropriate semiring structure, captures equivalence with respect to positive-existential modal logic, graded modal logic, and hybrid logic, as well as the extensions of these logics with either backward or global modalities.","A novelty of our positive results is that they apply not only to finite structures, as with previous Lov\\'asz-style theorems, but also to well-behaved infinite structures.","We also show that equivalence with respect to positive modal logic and equivalence with respect to the basic modal language are not captured by homomorphism-count indistinguishability with respect to any class of LTSs, regardless of which semiring is used for counting."],"url":"http://arxiv.org/abs/2404.15421v1","category":"cs.LO"}
{"created":"2024-04-23 18:00:17","title":"Quantum Walks on Simplicial Complexes and Harmonic Homology: Application to Topological Data Analysis with Superpolynomial Speedups","abstract":"Incorporating higher-order interactions in information processing enables us to build more accurate models, gain deeper insights into complex systems, and address real-world challenges more effectively. However, existing methods, such as random walks on oriented simplices and homology, which capture these interactions, are not known to be efficient. This work investigates whether quantum walks on simplicial complexes exhibit quantum advantages. We introduce a novel quantum walk that encodes the combinatorial Laplacian, a key mathematical object whose spectral properties reflect the topology of the underlying simplicial complex. Furthermore, we construct a unitary encoding that projects onto the kernel of the Laplacian, representing the space of harmonic cycles in the complex's homology. Combined with the efficient construction of quantum walk unitaries for clique complexes that we present, this paves the way for utilizing quantum walks to explore higher-order interactions within topological structures. Our results achieve superpolynomial quantum speedup with quantum walks without relying on quantum oracles for large datasets.   Crucially, the walk operates on a state space encompassing both positively and negatively oriented simplices, effectively doubling its size compared to unoriented approaches. Through coherent interference of these paired simplices, we are able to successfully encode the combinatorial Laplacian, which would otherwise be impossible. This observation constitutes our major technical contribution. We also extend the framework by constructing variant quantum walks. These variants enable us to: (1) estimate the normalized persistent Betti numbers, capturing topological information throughout a deformation process, and (2) verify a specific QMA$_1$-hard problem, showcasing potential applications in computational complexity theory.","sentences":["Incorporating higher-order interactions in information processing enables us to build more accurate models, gain deeper insights into complex systems, and address real-world challenges more effectively.","However, existing methods, such as random walks on oriented simplices and homology, which capture these interactions, are not known to be efficient.","This work investigates whether quantum walks on simplicial complexes exhibit quantum advantages.","We introduce a novel quantum walk that encodes the combinatorial Laplacian, a key mathematical object whose spectral properties reflect the topology of the underlying simplicial complex.","Furthermore, we construct a unitary encoding that projects onto the kernel of the Laplacian, representing the space of harmonic cycles in the complex's homology.","Combined with the efficient construction of quantum walk unitaries for clique complexes that we present, this paves the way for utilizing quantum walks to explore higher-order interactions within topological structures.","Our results achieve superpolynomial quantum speedup with quantum walks without relying on quantum oracles for large datasets.   ","Crucially, the walk operates on a state space encompassing both positively and negatively oriented simplices, effectively doubling its size compared to unoriented approaches.","Through coherent interference of these paired simplices, we are able to successfully encode the combinatorial Laplacian, which would otherwise be impossible.","This observation constitutes our major technical contribution.","We also extend the framework by constructing variant quantum walks.","These variants enable us to: (1) estimate the normalized persistent Betti numbers, capturing topological information throughout a deformation process, and (2) verify a specific QMA$_1$-hard problem, showcasing potential applications in computational complexity theory."],"url":"http://arxiv.org/abs/2404.15407v1","category":"quant-ph"}
{"created":"2024-04-23 14:58:44","title":"Detecting unresolved lensed SNe Ia in LSST using blended light curves","abstract":"Strong-gravitationally lensed supernovae (LSNe) are promising probes for providing absolute distance measurements using gravitational lens time delays. Spatially unresolved LSNe offer an opportunity to enhance the sample size for precision cosmology. We predict that there will be approximately $3$ times more unresolved than resolved LSNe Ia in the Legacy Survey of Space and Time (LSST) by the Rubin Observatory. In this article, we explore the feasibility of detecting unresolved LSNe Ia from the shape of the observed blended light curves using deep learning techniques, and we find that $\\sim 30\\%$ can be detected with a simple 1D CNN using well-sampled $rizy$-band light curves (with a false-positive rate of $\\sim 3\\%$). Even when the light curve is well-observed in only a single band among $r$, $i$, and $z$, detection is still possible with false-positive rates ranging from $\\sim 4-7\\%$, depending on the band. Furthermore, we demonstrate that these unresolved cases can be detected at an early stage using light curves up to $\\sim20$ days from the first observation, with well-controlled false-positive rates, providing ample opportunities for triggering follow-up observations. Additionally, we demonstrate the feasibility of time-delay estimations using solely LSST-like data of unresolved light curves, particularly for doubles, when excluding systems with low time delay and magnification ratio. However, the abundance of such systems among those unresolved in LSST poses a significant challenge. This approach holds potential utility for upcoming wide-field surveys, and overall results could significantly improve with enhanced cadence and depth in the future surveys.","sentences":["Strong-gravitationally lensed supernovae (LSNe) are promising probes for providing absolute distance measurements using gravitational lens time delays.","Spatially unresolved LSNe offer an opportunity to enhance the sample size for precision cosmology.","We predict that there will be approximately $3$ times more unresolved than resolved LSNe Ia in the Legacy Survey of Space and Time (LSST) by the Rubin Observatory.","In this article, we explore the feasibility of detecting unresolved LSNe Ia from the shape of the observed blended light curves using deep learning techniques, and we find that $\\sim 30\\%$ can be detected with a simple 1D CNN using well-sampled $rizy$-band light curves (with a false-positive rate of $\\sim 3\\%$).","Even when the light curve is well-observed in only a single band among $r$, $i$, and $z$, detection is still possible with false-positive rates ranging from $\\sim 4-7\\%$, depending on the band.","Furthermore, we demonstrate that these unresolved cases can be detected at an early stage using light curves up to $\\sim20$ days from the first observation, with well-controlled false-positive rates, providing ample opportunities for triggering follow-up observations.","Additionally, we demonstrate the feasibility of time-delay estimations using solely LSST-like data of unresolved light curves, particularly for doubles, when excluding systems with low time delay and magnification ratio.","However, the abundance of such systems among those unresolved in LSST poses a significant challenge.","This approach holds potential utility for upcoming wide-field surveys, and overall results could significantly improve with enhanced cadence and depth in the future surveys."],"url":"http://arxiv.org/abs/2404.15389v1","category":"astro-ph.IM"}
{"created":"2024-04-24 03:16:48","title":"Neural Operator induced Gaussian Process framework for probabilistic solution of parametric partial differential equations","abstract":"The study of neural operators has paved the way for the development of efficient approaches for solving partial differential equations (PDEs) compared with traditional methods. However, most of the existing neural operators lack the capability to provide uncertainty measures for their predictions, a crucial aspect, especially in data-driven scenarios with limited available data. In this work, we propose a novel Neural Operator-induced Gaussian Process (NOGaP), which exploits the probabilistic characteristics of Gaussian Processes (GPs) while leveraging the learning prowess of operator learning. The proposed framework leads to improved prediction accuracy and offers a quantifiable measure of uncertainty. The proposed framework is extensively evaluated through experiments on various PDE examples, including Burger's equation, Darcy flow, non-homogeneous Poisson, and wave-advection equations. Furthermore, a comparative study with state-of-the-art operator learning algorithms is presented to highlight the advantages of NOGaP. The results demonstrate superior accuracy and expected uncertainty characteristics, suggesting the promising potential of the proposed framework.","sentences":["The study of neural operators has paved the way for the development of efficient approaches for solving partial differential equations (PDEs) compared with traditional methods.","However, most of the existing neural operators lack the capability to provide uncertainty measures for their predictions, a crucial aspect, especially in data-driven scenarios with limited available data.","In this work, we propose a novel Neural Operator-induced Gaussian Process (NOGaP), which exploits the probabilistic characteristics of Gaussian Processes (GPs) while leveraging the learning prowess of operator learning.","The proposed framework leads to improved prediction accuracy and offers a quantifiable measure of uncertainty.","The proposed framework is extensively evaluated through experiments on various PDE examples, including Burger's equation, Darcy flow, non-homogeneous Poisson, and wave-advection equations.","Furthermore, a comparative study with state-of-the-art operator learning algorithms is presented to highlight the advantages of NOGaP.","The results demonstrate superior accuracy and expected uncertainty characteristics, suggesting the promising potential of the proposed framework."],"url":"http://arxiv.org/abs/2404.15618v1","category":"stat.ML"}
{"created":"2024-04-24 02:45:30","title":"Multilevel Particle Filters for Partially Observed McKean-Vlasov Stochastic Differential Equations","abstract":"In this paper we consider the filtering problem associated to partially observed McKean-Vlasov stochastic differential equations (SDEs). The model consists of data that are observed at regular and discrete times and the objective is to compute the conditional expectation of (functionals) of the solutions of the SDE at the current time. This problem, even the ordinary SDE case is challenging and requires numerical approximations. Based upon the ideas in [3, 12] we develop a new particle filter (PF) and multilevel particle filter (MLPF) to approximate the afore-mentioned expectations. We prove under assumptions that, for $\\epsilon>0$, to obtain a mean square error of $\\mathcal{O}(\\epsilon^2)$ the PF has a cost per-observation time of $\\mathcal{O}(\\epsilon^{-5})$ and the MLPF costs $\\mathcal{O}(\\epsilon^{-4})$ (best case) or $\\mathcal{O}(\\epsilon^{-4}\\log(\\epsilon)^2)$ (worst case). Our theoretical results are supported by numerical experiments.","sentences":["In this paper we consider the filtering problem associated to partially observed McKean-Vlasov stochastic differential equations (SDEs).","The model consists of data that are observed at regular and discrete times and the objective is to compute the conditional expectation of (functionals) of the solutions of the SDE at the current time.","This problem, even the ordinary SDE case is challenging and requires numerical approximations.","Based upon the ideas in [3, 12] we develop a new particle filter (PF) and multilevel particle filter (MLPF) to approximate the afore-mentioned expectations.","We prove under assumptions that, for $\\epsilon>0$, to obtain a mean square error of $\\mathcal{O}(\\epsilon^2)$ the PF has a cost per-observation time of $\\mathcal{O}(\\epsilon^{-5})$ and the MLPF costs $\\mathcal{O}(\\epsilon^{-4})$ (best case) or $\\mathcal{O}(\\epsilon^{-4}\\log(\\epsilon)^2)$ (worst case).","Our theoretical results are supported by numerical experiments."],"url":"http://arxiv.org/abs/2404.15606v1","category":"math.NA"}
{"created":"2024-04-24 02:45:04","title":"Magnetic flux-induced topological superconductivity in magnetic atomic rings","abstract":"There have been numerous studies on topological superconductivity in magnetic atomic chains deposited on s-wave superconductors. Most of these investigations have focused on spin-orbit interactions or helical spin orders. In this paper, we propose a new model for achieving one-dimensional topological superconductivity in a magnetic atomic ring. This model utilizes a magnetic field and an antiferromagnetic/ferromagnetic order, under the condition that the magnetic field is perpendicular to the moments of the magnetic order. On a quasi-one-dimensional substrate surface, where the half-filled ring favors an antiferromagnetic configuration, we demonstrate that either the magnetic field itself or a Rashba spin-orbit coupling guarantees the perpendicularity. On a two-dimensional surface, where the ring favors ferromagnetic orders, the perpendicularity is achieved by introducing a minor Rashba spin-orbit coupling.","sentences":["There have been numerous studies on topological superconductivity in magnetic atomic chains deposited on s-wave superconductors.","Most of these investigations have focused on spin-orbit interactions or helical spin orders.","In this paper, we propose a new model for achieving one-dimensional topological superconductivity in a magnetic atomic ring.","This model utilizes a magnetic field and an antiferromagnetic/ferromagnetic order, under the condition that the magnetic field is perpendicular to the moments of the magnetic order.","On a quasi-one-dimensional substrate surface, where the half-filled ring favors an antiferromagnetic configuration, we demonstrate that either the magnetic field itself or a Rashba spin-orbit coupling guarantees the perpendicularity.","On a two-dimensional surface, where the ring favors ferromagnetic orders, the perpendicularity is achieved by introducing a minor Rashba spin-orbit coupling."],"url":"http://arxiv.org/abs/2404.15605v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-24 01:59:02","title":"A Survey of Deep Long-Tail Classification Advancements","abstract":"Many data distributions in the real world are hardly uniform. Instead, skewed and long-tailed distributions of various kinds are commonly observed. This poses an interesting problem for machine learning, where most algorithms assume or work well with uniformly distributed data. The problem is further exacerbated by current state-of-the-art deep learning models requiring large volumes of training data. As such, learning from imbalanced data remains a challenging research problem and a problem that must be solved as we move towards more real-world applications of deep learning. In the context of class imbalance, state-of-the-art (SOTA) accuracies on standard benchmark datasets for classification typically fall less than 75%, even for less challenging datasets such as CIFAR100. Nonetheless, there has been progress in this niche area of deep learning. To this end, in this survey, we provide a taxonomy of various methods proposed for addressing the problem of long-tail classification, focusing on works that happened in the last few years under a single mathematical framework. We also discuss standard performance metrics, convergence studies, feature distribution and classifier analysis. We also provide a quantitative comparison of the performance of different SOTA methods and conclude the survey by discussing the remaining challenges and future research direction.","sentences":["Many data distributions in the real world are hardly uniform.","Instead, skewed and long-tailed distributions of various kinds are commonly observed.","This poses an interesting problem for machine learning, where most algorithms assume or work well with uniformly distributed data.","The problem is further exacerbated by current state-of-the-art deep learning models requiring large volumes of training data.","As such, learning from imbalanced data remains a challenging research problem and a problem that must be solved as we move towards more real-world applications of deep learning.","In the context of class imbalance, state-of-the-art (SOTA) accuracies on standard benchmark datasets for classification typically fall less than 75%, even for less challenging datasets such as CIFAR100.","Nonetheless, there has been progress in this niche area of deep learning.","To this end, in this survey, we provide a taxonomy of various methods proposed for addressing the problem of long-tail classification, focusing on works that happened in the last few years under a single mathematical framework.","We also discuss standard performance metrics, convergence studies, feature distribution and classifier analysis.","We also provide a quantitative comparison of the performance of different SOTA methods and conclude the survey by discussing the remaining challenges and future research direction."],"url":"http://arxiv.org/abs/2404.15593v1","category":"cs.LG"}
{"created":"2024-04-24 01:39:47","title":"Multiple testing with anytime-valid Monte-Carlo p-values","abstract":"In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested. Due to their high power, and finite sample guarantees on type-1 error under weak assumptions, Monte-Carlo permutation tests are often considered as gold standard for these settings. However, the enormous computational effort required for (thousands of) permutation tests is a major burden. Recently, Fischer and Ramdas (2024) constructed a permutation test for a single hypothesis in which the permutations are drawn sequentially one-by-one and the testing process can be stopped at any point without inflating the type I error. They showed that the number of permutations can be substantially reduced (under null and alternative) while the power remains similar. We show how their approach can be modified to make it suitable for a broad class of multiple testing procedures. In particular, we discuss its use with the Benjamini-Hochberg procedure and illustrate the application on a large dataset.","sentences":["In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested.","Due to their high power, and finite sample guarantees on type-1 error under weak assumptions, Monte-Carlo permutation tests are often considered as gold standard for these settings.","However, the enormous computational effort required for (thousands of) permutation tests is a major burden.","Recently, Fischer and Ramdas (2024) constructed a permutation test for a single hypothesis in which the permutations are drawn sequentially one-by-one and the testing process can be stopped at any point without inflating the type I error.","They showed that the number of permutations can be substantially reduced (under null and alternative) while the power remains similar.","We show how their approach can be modified to make it suitable for a broad class of multiple testing procedures.","In particular, we discuss its use with the Benjamini-Hochberg procedure and illustrate the application on a large dataset."],"url":"http://arxiv.org/abs/2404.15586v1","category":"stat.ME"}
{"created":"2024-04-24 01:26:51","title":"Decentralized Exchangeable Stochastic Dynamic Teams in Continuous-time, their Mean-Field Limits and Optimality of Symmetric Policies","abstract":"We study a class of stochastic exchangeable teams comprising a finite number of decision makers (DMs) as well as their mean-field limits involving infinite numbers of DMs. In the finite population regime, we study exchangeable teams under the centralized information structure. For the infinite population setting, we study exchangeable teams under the decentralized mean-field information sharing. The paper makes the following main contributions: i) For finite population exchangeable teams, we establish the existence of a randomized optimal policy that is exchangeable (permutation invariant) and Markovian; ii) As our main result in the paper, we show that a sequence of exchangeable optimal policies for finite population settings converges to a conditionally symmetric (identical), independent, and decentralized randomized policy for the infinite population problem, which is globally optimal for the infinite population problem. This result establishes the existence of a symmetric, independent, decentralized optimal randomized policy for the infinite population problem. Additionally, this proves the optimality of the limiting measure-valued MDP for the representative DM; iii) Finally, we show that symmetric, independent, decentralized optimal randomized policies are approximately optimal for the corresponding finite-population team with a large number of DMs under the centralized information structure. Our paper thus establishes the relation between the controlled McKean-Vlasov dynamics and the optimal infinite population decentralized stochastic control problem (without an apriori restriction of symmetry in policies of individual agents), for the first time, to our knowledge.","sentences":["We study a class of stochastic exchangeable teams comprising a finite number of decision makers (DMs) as well as their mean-field limits involving infinite numbers of DMs.","In the finite population regime, we study exchangeable teams under the centralized information structure.","For the infinite population setting, we study exchangeable teams under the decentralized mean-field information sharing.","The paper makes the following main contributions: i) For finite population exchangeable teams, we establish the existence of a randomized optimal policy that is exchangeable (permutation invariant) and Markovian; ii)","As our main result in the paper, we show that a sequence of exchangeable optimal policies for finite population settings converges to a conditionally symmetric (identical), independent, and decentralized randomized policy for the infinite population problem, which is globally optimal for the infinite population problem.","This result establishes the existence of a symmetric, independent, decentralized optimal randomized policy for the infinite population problem.","Additionally, this proves the optimality of the limiting measure-valued MDP for the representative DM; iii)","Finally, we show that symmetric, independent, decentralized optimal randomized policies are approximately optimal for the corresponding finite-population team with a large number of DMs under the centralized information structure.","Our paper thus establishes the relation between the controlled McKean-Vlasov dynamics and the optimal infinite population decentralized stochastic control problem (without an apriori restriction of symmetry in policies of individual agents), for the first time, to our knowledge."],"url":"http://arxiv.org/abs/2404.15581v1","category":"math.OC"}
{"created":"2024-04-23 23:27:29","title":"CASPR: Automated Evaluation Metric for Contrastive Summarization","abstract":"Summarizing comparative opinions about entities (e.g., hotels, phones) from a set of source reviews, often referred to as contrastive summarization, can considerably aid users in decision making. However, reliably measuring the contrastiveness of the output summaries without relying on human evaluations remains an open problem. Prior work has proposed token-overlap based metrics, Distinctiveness Score, to measure contrast which does not take into account the sensitivity to meaning-preserving lexical variations. In this work, we propose an automated evaluation metric CASPR to better measure contrast between a pair of summaries. Our metric is based on a simple and light-weight method that leverages natural language inference (NLI) task to measure contrast by segmenting reviews into single-claim sentences and carefully aggregating NLI scores between them to come up with a summary-level score. We compare CASPR with Distinctiveness Score and a simple yet powerful baseline based on BERTScore. Our results on a prior dataset CoCoTRIP demonstrate that CASPR can more reliably capture the contrastiveness of the summary pairs compared to the baselines.","sentences":["Summarizing comparative opinions about entities (e.g., hotels, phones) from a set of source reviews, often referred to as contrastive summarization, can considerably aid users in decision making.","However, reliably measuring the contrastiveness of the output summaries without relying on human evaluations remains an open problem.","Prior work has proposed token-overlap based metrics, Distinctiveness Score, to measure contrast which does not take into account the sensitivity to meaning-preserving lexical variations.","In this work, we propose an automated evaluation metric CASPR to better measure contrast between a pair of summaries.","Our metric is based on a simple and light-weight method that leverages natural language inference (NLI) task to measure contrast by segmenting reviews into single-claim sentences and carefully aggregating NLI scores between them to come up with a summary-level score.","We compare CASPR with Distinctiveness Score and a simple yet powerful baseline based on BERTScore.","Our results on a prior dataset CoCoTRIP demonstrate that CASPR can more reliably capture the contrastiveness of the summary pairs compared to the baselines."],"url":"http://arxiv.org/abs/2404.15565v1","category":"cs.CL"}
{"created":"2024-04-23 23:11:42","title":"Safe POMDP Online Planning among Dynamic Agents via Adaptive Conformal Prediction","abstract":"Online planning for partially observable Markov decision processes (POMDPs) provides efficient techniques for robot decision-making under uncertainty. However, existing methods fall short of preventing safety violations in dynamic environments. This work presents a novel safe POMDP online planning approach that offers probabilistic safety guarantees amidst environments populated by multiple dynamic agents. Our approach utilizes data-driven trajectory prediction models of dynamic agents and applies Adaptive Conformal Prediction (ACP) for assessing the uncertainties in these predictions. Leveraging the obtained ACP-based trajectory predictions, our approach constructs safety shields on-the-fly to prevent unsafe actions within POMDP online planning. Through experimental evaluation in various dynamic environments using real-world pedestrian trajectory data, the proposed approach has been shown to effectively maintain probabilistic safety guarantees while accommodating up to hundreds of dynamic agents.","sentences":["Online planning for partially observable Markov decision processes (POMDPs) provides efficient techniques for robot decision-making under uncertainty.","However, existing methods fall short of preventing safety violations in dynamic environments.","This work presents a novel safe POMDP online planning approach that offers probabilistic safety guarantees amidst environments populated by multiple dynamic agents.","Our approach utilizes data-driven trajectory prediction models of dynamic agents and applies Adaptive Conformal Prediction (ACP) for assessing the uncertainties in these predictions.","Leveraging the obtained ACP-based trajectory predictions, our approach constructs safety shields on-the-fly to prevent unsafe actions within POMDP online planning.","Through experimental evaluation in various dynamic environments using real-world pedestrian trajectory data, the proposed approach has been shown to effectively maintain probabilistic safety guarantees while accommodating up to hundreds of dynamic agents."],"url":"http://arxiv.org/abs/2404.15557v1","category":"cs.RO"}
{"created":"2024-04-23 22:31:50","title":"Fractional quantum Hall effect of partons and the nature of the 8/17 state in the zeroth Landau level of bilayer graphene","abstract":"We consider the fractional quantum Hall effect (FQHE) at the filling factor $8/17$, where signatures of incompressibility have been observed in the zeroth Landau level of bilayer graphene. We propose an Abelian state described by the \"$\\overline{(8/3)}\\bar{2}1^{3}$\" parton wave function, where a parton itself forms an FQHE state. This state is topologically distinct from the $8/17$ Levin-Halperin state, a daughter state of the Moore-Read state. We carry out extensive numerical exact diagonalization of the Coulomb interaction at 8/17 in the zeroth Landau level of bilayer graphene but find that our results cannot conclusively determine the topological order of the underlying ground state. We work out the low-energy effective theory of the $\\overline{(8/3)}\\bar{2}1^{3}$ edge and make predictions for experimentally measurable properties of the state which can tell it apart from the 8/17 Levin-Halperin state.","sentences":["We consider the fractional quantum Hall effect (FQHE) at the filling factor $8/17$, where signatures of incompressibility have been observed in the zeroth Landau level of bilayer graphene.","We propose an Abelian state described by the \"$\\overline{(8/3)}\\bar{2}1^{3}$\" parton wave function, where a parton itself forms an FQHE state.","This state is topologically distinct from the $8/17$ Levin-Halperin state, a daughter state of the Moore-Read state.","We carry out extensive numerical exact diagonalization of the Coulomb interaction at 8/17 in the zeroth Landau level of bilayer graphene but find that our results cannot conclusively determine the topological order of the underlying ground state.","We work out the low-energy effective theory of the $\\overline{(8/3)}\\bar{2}1^{3}$ edge and make predictions for experimentally measurable properties of the state which can tell it apart from the 8/17 Levin-Halperin state."],"url":"http://arxiv.org/abs/2404.15547v1","category":"cond-mat.str-el"}
{"created":"2024-04-23 21:29:09","title":"Maximal Procurement under a Budget","abstract":"We study the problem of a principal who wants to influence an agent's observable action, subject to an ex-post budget. The agent has a private type determining their cost function. This paper endogenizes the value of the resource driving incentives, which holds no inherent value but is restricted by finite availability. We characterize the optimal mechanism, showing the emergence of a pooling region where the budget constraint binds for low-cost types. We then introduce a linear value for the transferable resource; as the principal's value increases, the mechanism demands more from agents with binding budget constraint but less from others.","sentences":["We study the problem of a principal who wants to influence an agent's observable action, subject to an ex-post budget.","The agent has a private type determining their cost function.","This paper endogenizes the value of the resource driving incentives, which holds no inherent value but is restricted by finite availability.","We characterize the optimal mechanism, showing the emergence of a pooling region where the budget constraint binds for low-cost types.","We then introduce a linear value for the transferable resource; as the principal's value increases, the mechanism demands more from agents with binding budget constraint but less from others."],"url":"http://arxiv.org/abs/2404.15531v1","category":"econ.TH"}
{"created":"2024-04-23 21:11:30","title":"Understanding Hyperbolic Metric Learning through Hard Negative Sampling","abstract":"In recent years, there has been a growing trend of incorporating hyperbolic geometry methods into computer vision. While these methods have achieved state-of-the-art performance on various metric learning tasks using hyperbolic distance measurements, the underlying theoretical analysis supporting this superior performance remains under-exploited. In this study, we investigate the effects of integrating hyperbolic space into metric learning, particularly when training with contrastive loss. We identify a need for a comprehensive comparison between Euclidean and hyperbolic spaces regarding the temperature effect in the contrastive loss within the existing literature. To address this gap, we conduct an extensive investigation to benchmark the results of Vision Transformers (ViTs) using a hybrid objective function that combines loss from Euclidean and hyperbolic spaces. Additionally, we provide a theoretical analysis of the observed performance improvement. We also reveal that hyperbolic metric learning is highly related to hard negative sampling, providing insights for future work. This work will provide valuable data points and experience in understanding hyperbolic image embeddings. To shed more light on problem-solving and encourage further investigation into our approach, our code is available online (https://github.com/YunYunY/HypMix).","sentences":["In recent years, there has been a growing trend of incorporating hyperbolic geometry methods into computer vision.","While these methods have achieved state-of-the-art performance on various metric learning tasks using hyperbolic distance measurements, the underlying theoretical analysis supporting this superior performance remains under-exploited.","In this study, we investigate the effects of integrating hyperbolic space into metric learning, particularly when training with contrastive loss.","We identify a need for a comprehensive comparison between Euclidean and hyperbolic spaces regarding the temperature effect in the contrastive loss within the existing literature.","To address this gap, we conduct an extensive investigation to benchmark the results of Vision Transformers (ViTs) using a hybrid objective function that combines loss from Euclidean and hyperbolic spaces.","Additionally, we provide a theoretical analysis of the observed performance improvement.","We also reveal that hyperbolic metric learning is highly related to hard negative sampling, providing insights for future work.","This work will provide valuable data points and experience in understanding hyperbolic image embeddings.","To shed more light on problem-solving and encourage further investigation into our approach, our code is available online (https://github.com/YunYunY/HypMix)."],"url":"http://arxiv.org/abs/2404.15523v1","category":"cs.CV"}
{"created":"2024-04-23 20:09:25","title":"Temperature dependent spin-phonon coupling of boron-vacancy centers in hexagonal boron nitride","abstract":"The negatively charged boron-vacancy center ($\\mathrm{V}_{\\mathrm{B}}^-$) in hexagonal boron nitride (hBN) has recently emerged as a highly promising quantum sensor. Compared to the nitrogen-vacancy (NV) center in diamond, the change with temperature of the spin transition energy of $\\mathrm{V}_{\\mathrm{B}}^-$ is more than an order of magnitude larger, making it a potential nanoscale thermometer with superior sensitivity. However, the underlying mechanism of the observed large temperature dependence remains an open question. In this work, using isotopically purified $\\mathrm{h}{}^{10}\\mathrm{B}{}^{15}\\mathrm{N}$, we systematically characterize the zero-field splitting, hyperfine interaction, and spin relaxation time of $\\mathrm{V}_{\\mathrm{B}}^-$ from 10 to 350$~$K. We carry out first-principle calculations of the $\\mathrm{V}_{\\mathrm{B}}^-$ spin-phonon interaction and show that a second-order effect from finite-temperature phonon excitations is responsible for the observed changes in experiments. By fitting our experimental results to a physically motivated model, we extract the dominant phonon mode which agrees well with our simulations. Finally, we investigate the dynamic nuclear spin polarization process at cryogenic temperatures. Our results provide key insights in $\\mathrm{V}_{\\mathrm{B}}^-$ centers and their utilization as nanoscale thermometers and phonon sensors.","sentences":["The negatively charged boron-vacancy center ($\\mathrm{V}_{\\mathrm{B}}^-$) in hexagonal boron nitride (hBN) has recently emerged as a highly promising quantum sensor.","Compared to the nitrogen-vacancy (NV) center in diamond, the change with temperature of the spin transition energy of $\\mathrm{V}_{\\mathrm{B}}^-$ is more than an order of magnitude larger, making it a potential nanoscale thermometer with superior sensitivity.","However, the underlying mechanism of the observed large temperature dependence remains an open question.","In this work, using isotopically purified $\\mathrm{h}{}^{10}\\mathrm{B}{}^{15}\\mathrm{N}$, we systematically characterize the zero-field splitting, hyperfine interaction, and spin relaxation time of $\\mathrm{V}_{\\mathrm{B}}^-$ from 10 to 350$~$K. We carry out first-principle calculations of the $\\mathrm{V}_{\\mathrm{B}}^-$ spin-phonon interaction and show that a second-order effect from finite-temperature phonon excitations is responsible for the observed changes in experiments.","By fitting our experimental results to a physically motivated model, we extract the dominant phonon mode which agrees well with our simulations.","Finally, we investigate the dynamic nuclear spin polarization process at cryogenic temperatures.","Our results provide key insights in $\\mathrm{V}_{\\mathrm{B}}^-$ centers and their utilization as nanoscale thermometers and phonon sensors."],"url":"http://arxiv.org/abs/2404.15493v1","category":"quant-ph"}
{"created":"2024-04-23 20:03:12","title":"Multiblock MEV opportunities & protections in dynamic AMMs","abstract":"Maximal Extractable Value (MEV) in Constant Function Market Making is fairly well understood. Does having dynamic weights, as found in liquidity boostrap pools (LBPs), Temporal-function market makers (TFMMs), and Replicating market makers (RMMs), introduce new attack vectors? In this paper we explore how inter-block weight changes can be analogous to trades, and can potentially lead to a multi-block MEV attack. New inter-block protections required to guard against this new attack vector are analysed. We also carry our a raft of numerical simulations, more than 450 million potential attack scenarios, showing both successful attacks and successful defense.","sentences":["Maximal Extractable Value (MEV) in Constant Function Market Making is fairly well understood.","Does having dynamic weights, as found in liquidity boostrap pools (LBPs), Temporal-function market makers (TFMMs), and Replicating market makers (RMMs), introduce new attack vectors?","In this paper we explore how inter-block weight changes can be analogous to trades, and can potentially lead to a multi-block MEV attack.","New inter-block protections required to guard against this new attack vector are analysed.","We also carry our a raft of numerical simulations, more than 450 million potential attack scenarios, showing both successful attacks and successful defense."],"url":"http://arxiv.org/abs/2404.15489v1","category":"q-fin.TR"}
{"created":"2024-04-23 19:53:36","title":"Uncertainty, Imprecise Probabilities and Interval Capacity Measures on a Product Space","abstract":"In Basili and Pratelli (2024), a novel and coherent concept of interval probability measures has been introduced, providing a method for representing imprecise probabilities and uncertainty. Within the framework of set algebra, we introduced the concepts of weak complementation and interval probability measures associated with a family of random variables, which effectively capture the inherent uncertainty in any event. This paper conducts a comprehensive analysis of these concepts within a specific probability space. Additionally, we elaborate on an updating rule for events, integrating essential concepts of statistical independence, dependence, and stochastic dominance.","sentences":["In Basili and Pratelli (2024), a novel and coherent concept of interval probability measures has been introduced, providing a method for representing imprecise probabilities and uncertainty.","Within the framework of set algebra, we introduced the concepts of weak complementation and interval probability measures associated with a family of random variables, which effectively capture the inherent uncertainty in any event.","This paper conducts a comprehensive analysis of these concepts within a specific probability space.","Additionally, we elaborate on an updating rule for events, integrating essential concepts of statistical independence, dependence, and stochastic dominance."],"url":"http://arxiv.org/abs/2404.15484v1","category":"math.ST"}
{"created":"2024-04-23 19:34:52","title":"Algorithmic Market Making in Spot Precious Metals","abstract":"The primary challenge of market making in spot precious metals is navigating the liquidity that is mainly provided by futures contracts. The Exchange for Physical (EFP) spread, which is the price difference between futures and spot, plays a pivotal role and exhibits multiple modes of relaxation corresponding to the diverse trading horizons of market participants. In this paper, we introduce a novel framework utilizing a nested Ornstein-Uhlenbeck process to model the EFP spread. We demonstrate the suitability of the framework for maximizing the expected P\\&L of a market maker while minimizing inventory risk across both spot and futures. Using a computationally efficient technique to approximate the solution of the Hamilton-Jacobi-Bellman equation associated with the corresponding stochastic optimal control problem, our methodology facilitates strategy optimization on demand in near real-time, paving the way for advanced algorithmic market making that capitalizes on the co-integration properties intrinsic to the precious metals sector.","sentences":["The primary challenge of market making in spot precious metals is navigating the liquidity that is mainly provided by futures contracts.","The Exchange for Physical (EFP) spread, which is the price difference between futures and spot, plays a pivotal role and exhibits multiple modes of relaxation corresponding to the diverse trading horizons of market participants.","In this paper, we introduce a novel framework utilizing a nested Ornstein-Uhlenbeck process to model the EFP spread.","We demonstrate the suitability of the framework for maximizing the expected P\\&L of a market maker while minimizing inventory risk across both spot and futures.","Using a computationally efficient technique to approximate the solution of the Hamilton-Jacobi-Bellman equation associated with the corresponding stochastic optimal control problem, our methodology facilitates strategy optimization on demand in near real-time, paving the way for advanced algorithmic market making that capitalizes on the co-integration properties intrinsic to the precious metals sector."],"url":"http://arxiv.org/abs/2404.15478v1","category":"q-fin.TR"}
{"created":"2024-04-23 19:12:28","title":"Modal Semantics for Reasoning with Probability and Uncertainty","abstract":"This paper belongs to the field of probabilistic modal logic, focusing on a comparative analysis of two distinct semantics: one rooted in Kripke semantics and the other in neighbourhood semantics. The primary distinction lies in the following: The latter allows us to adequately express belief functions (lower probabilities) over propositions, whereas the former does not. Thus, neighbourhood semantics is more expressive. The main part of the work is a section in which we study the modal equivalence between probabilistic Kripke models and a subclass of belief neighbourhood models, namely additive ones. We study how to obtain modally equivalent structures.","sentences":["This paper belongs to the field of probabilistic modal logic, focusing on a comparative analysis of two distinct semantics: one rooted in Kripke semantics and the other in neighbourhood semantics.","The primary distinction lies in the following: The latter allows us to adequately express belief functions (lower probabilities) over propositions, whereas the former does not.","Thus, neighbourhood semantics is more expressive.","The main part of the work is a section in which we study the modal equivalence between probabilistic Kripke models and a subclass of belief neighbourhood models, namely additive ones.","We study how to obtain modally equivalent structures."],"url":"http://arxiv.org/abs/2404.15461v1","category":"math.LO"}
{"created":"2024-04-23 18:55:30","title":"Prediction from compression for models with infinite memory, with applications to hidden Markov and renewal processes","abstract":"Consider the problem of predicting the next symbol given a sample path of length n, whose joint distribution belongs to a distribution class that may have long-term memory. The goal is to compete with the conditional predictor that knows the true model. For both hidden Markov models (HMMs) and renewal processes, we determine the optimal prediction risk in Kullback- Leibler divergence up to universal constant factors. Extending existing results in finite-order Markov models [HJW23] and drawing ideas from universal compression, the proposed estimator has a prediction risk bounded by redundancy of the distribution class and a memory term that accounts for the long-range dependency of the model. Notably, for HMMs with bounded state and observation spaces, a polynomial-time estimator based on dynamic programming is shown to achieve the optimal prediction risk {\\Theta}(log n/n); prior to this work, the only known result of this type is O(1/log n) obtained using Markov approximation [Sha+18]. Matching minimax lower bounds are obtained by making connections to redundancy and mutual information via a reduction argument.","sentences":["Consider the problem of predicting the next symbol given a sample path of length n, whose joint distribution belongs to a distribution class that may have long-term memory.","The goal is to compete with the conditional predictor that knows the true model.","For both hidden Markov models (HMMs) and renewal processes, we determine the optimal prediction risk in Kullback- Leibler divergence up to universal constant factors.","Extending existing results in finite-order Markov models [HJW23] and drawing ideas from universal compression, the proposed estimator has a prediction risk bounded by redundancy of the distribution class and a memory term that accounts for the long-range dependency of the model.","Notably, for HMMs with bounded state and observation spaces, a polynomial-time estimator based on dynamic programming is shown to achieve the optimal prediction risk {\\Theta}(log n/n); prior to this work, the only known result of this type is O(1/log n) obtained using Markov approximation","[Sha+18].","Matching minimax lower bounds are obtained by making connections to redundancy and mutual information via a reduction argument."],"url":"http://arxiv.org/abs/2404.15454v1","category":"math.ST"}
{"created":"2024-04-23 18:37:36","title":"Renting Servers for Multi-Parameter Jobs in the Cloud","abstract":"We study the Renting Servers in the Cloud problem (RSiC) in multiple dimensions. In this problem, a sequence of multi-parameter jobs must be scheduled on servers that can be rented on-demand. Each job has an arrival time, a finishing time, and a multi-dimensional size vector that specifies its resource demands. Each server has a multi-dimensional capacity and jobs can be scheduled on a server as long as in each dimension the sum of sizes of jobs does not exceed the capacity of the server in that dimension. The goal is to minimize the total rental time of servers needed to process the job sequence. AF algorithms do not rent new servers to accommodate a job unless they have to. We introduce a sub-family of AF algorithms called monotone AF algorithms. We show this family have a tight competitive ratio of $Theta(d mu)$, where $d$ is the dimension of the problem and $mu$ is the ratio between the maximum and minimum duration of jobs in the input sequence. We also show that upper bounds for the RSiC problem obey the direct-sum property with respect to dimension $d$, that is we show how to transform $1$-dimensional algorithms for RSiC to work in the $d$-dimensional setting with competitive ratio scaling by a factor of $d$. As a corollary, we obtain an $O(d\\sqrt{log mu})$ upper bound for $d$-dimensional clairvoyant RSiC. We also establish a lower bound of $\\widetilde{Omega}(d mu)$ for both deterministic and randomized algorithms for $d$-dimensional non-clairvoyant RSiC, under the assumption that $mu \\le log d - 2$. Lastly, we propose a natural greedy algorithm called Greedy. Greedy, is a clairvoyant algorithm belongs to the monotone AF family, achieves a competitive ratio of $Theta(d mu)$. Our experimental results indicate that Greedy performs better or matches all other existing algorithms, for almost all the settings of arrival rates and values of mu and $d$ that we implemented.","sentences":["We study the Renting Servers in the Cloud problem (RSiC) in multiple dimensions.","In this problem, a sequence of multi-parameter jobs must be scheduled on servers that can be rented on-demand.","Each job has an arrival time, a finishing time, and a multi-dimensional size vector that specifies its resource demands.","Each server has a multi-dimensional capacity and jobs can be scheduled on a server as long as in each dimension the sum of sizes of jobs does not exceed the capacity of the server in that dimension.","The goal is to minimize the total rental time of servers needed to process the job sequence.","AF algorithms do not rent new servers to accommodate a job unless they have to.","We introduce a sub-family of AF algorithms called monotone AF algorithms.","We show this family have a tight competitive ratio of $Theta(d mu)$, where $d$ is the dimension of the problem and $mu$ is the ratio between the maximum and minimum duration of jobs in the input sequence.","We also show that upper bounds for the RSiC problem obey the direct-sum property with respect to dimension $d$, that is we show how to transform $1$-dimensional algorithms for RSiC to work in the $d$-dimensional setting with competitive ratio scaling by a factor of $d$. As a corollary, we obtain an $O(d\\sqrt{log mu})$ upper bound for $d$-dimensional clairvoyant RSiC.","We also establish a lower bound of $\\widetilde{Omega}(d mu)$ for both deterministic and randomized algorithms for $d$-dimensional non-clairvoyant RSiC, under the assumption that $mu \\le log d - 2$.","Lastly, we propose a natural greedy algorithm called Greedy.","Greedy, is a clairvoyant algorithm belongs to the monotone AF family, achieves a competitive ratio of $Theta(d mu)$. Our experimental results indicate that Greedy performs better or matches all other existing algorithms, for almost all the settings of arrival rates and values of mu and $d$ that we implemented."],"url":"http://arxiv.org/abs/2404.15444v1","category":"cs.DS"}
{"created":"2024-04-23 18:21:07","title":"Fractal uncertainty principle for random Cantor sets","abstract":"We continue our investigation of the fractal uncertainty principle (FUP) for random fractal sets. In the prequel (arXiv:2107.08276), we considered the Cantor sets in the discrete setting with alphabets randomly chosen from a base of digits so the dimension d is in (0,2/3). We proved that, with overwhelming probability, the FUP with an exponent >=1/2-3d/4- holds for these discrete Cantor sets with random alphabets.   In this sequel, we construct random Cantor sets with dimension d in (0,2/3) in R via a different random procedure from the one in the prequel. We prove that, with overwhelming probability, the FUP with an exponent >=1/2-3d/4- holds. The proof follows from establishing a Fourier decay estimate of the corresponding random Cantor measures, which is in turn based on a concentration of measure phenomenon in an appropriate probability space for the random Cantor sets.","sentences":["We continue our investigation of the fractal uncertainty principle (FUP) for random fractal sets.","In the prequel (arXiv:2107.08276), we considered the Cantor sets in the discrete setting with alphabets randomly chosen from a base of digits so the dimension d is in (0,2/3).","We proved that, with overwhelming probability, the FUP with an exponent >=1/2-3d/4- holds for these discrete Cantor sets with random alphabets.   ","In this sequel, we construct random Cantor sets with dimension d in (0,2/3) in R via a different random procedure from the one in the prequel.","We prove that, with overwhelming probability, the FUP with an exponent >=1/2-3d/4- holds.","The proof follows from establishing a Fourier decay estimate of the corresponding random Cantor measures, which is in turn based on a concentration of measure phenomenon in an appropriate probability space for the random Cantor sets."],"url":"http://arxiv.org/abs/2404.15434v1","category":"math.CA"}
{"created":"2024-04-23 18:19:11","title":"From stars to diverse mantles, melts, crusts and atmospheres of rocky exoplanets","abstract":"This review is focused on describing the logic by which we make predictions of exoplanetary compositions and mineralogies, and how these processes could lead to compositional diversity among rocky exoplanets. We use these predictions to determine the sensitivity of present-day and future observations to detecting compositional differences between rocky exoplanets and the four terrestrial planets. First, we review data on stellar abundances and infer how changes in composition may manifest themselves in the expected bulk compositions of rocky exoplanets (section 2). Converting this information in mass-radius relationships requires calculation of the stable mineral assemblages at a given temperature-pressure-composition (T-P-X), an exercise we describe in section 3. Should the planet be hot enough to engender partial melting of the mantle, then these liquids are likely to rise to the surface and erupt to form planetary crusts; the possible compositional and mineralogical variability of which we examine in section 4. Finally, the expected spectroscopic responses of such crusts are examined in section 5.","sentences":["This review is focused on describing the logic by which we make predictions of exoplanetary compositions and mineralogies, and how these processes could lead to compositional diversity among rocky exoplanets.","We use these predictions to determine the sensitivity of present-day and future observations to detecting compositional differences between rocky exoplanets and the four terrestrial planets.","First, we review data on stellar abundances and infer how changes in composition may manifest themselves in the expected bulk compositions of rocky exoplanets (section 2).","Converting this information in mass-radius relationships requires calculation of the stable mineral assemblages at a given temperature-pressure-composition (T-P-X), an exercise we describe in section 3.","Should the planet be hot enough to engender partial melting of the mantle, then these liquids are likely to rise to the surface and erupt to form planetary crusts; the possible compositional and mineralogical variability of which we examine in section 4.","Finally, the expected spectroscopic responses of such crusts are examined in section 5."],"url":"http://arxiv.org/abs/2404.15427v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:04:12","title":"A Spatially Resolved [CII] Survey of 31 $z\\sim7$ Massive Galaxies Hosting Luminous Quasars","abstract":"The [CII] 158 $\\mu$m emission line and the underlying far-infrared (FIR) dust continuum are important tracers for studying star formation and kinematic properties of early galaxies. We present a survey of the [CII] emission lines and FIR continua of 31 luminous quasars at $z>6.5$ using the Atacama Large Millimeter Array (ALMA) and the NOrthern Extended Millimeter Array (NOEMA) at sub-arcsec resolution. This survey more than doubles the number of quasars with [CII] and FIR observations at these redshifts and enables statistical studies of quasar host galaxies deep into the epoch of reionization. We detect [CII] emission in 27 quasar hosts with a luminosity range of $L_{\\rm [CII]}=(0.3-5.5)\\times10^9~L_\\odot$ and detect the FIR continuum of 28 quasar hosts with a luminosity range of $L_{\\rm FIR}=(0.5-13.0)\\times10^{12}~L_\\odot$. Both $L_{\\rm [CII]}$ and $L_{\\rm FIR}$ are correlated ($\\rho\\simeq0.4$) with the quasar bolometric luminosity, albeit with substantial scatter. The quasar hosts detected by ALMA are clearly resolved with a median diameter of $\\sim$5 kpc. About 40% of the quasar host galaxies show a velocity gradient in [CII] emission, while the rest show either dispersion-dominated or disturbed kinematics. Basic estimates of the dynamical masses of the rotation-dominated host galaxies yield $M_{\\rm dyn}=(0.1-7.5)\\times10^{11}~M_\\odot$. Considering our findings alongside those of literature studies, we found that the ratio between $M_{\\rm BH}$ and $M_{\\rm dyn}$ is about ten times higher than that of local $M_{\\rm BH}-M_{\\rm dyn}$ relation on average but with substantial scatter (the ratio difference ranging from $\\sim$0.6 to 60) and large uncertainties.","sentences":["The [CII] 158 $\\mu$m emission line and the underlying far-infrared (FIR) dust continuum are important tracers for studying star formation and kinematic properties of early galaxies.","We present a survey of the [CII] emission lines and FIR continua of 31 luminous quasars at $z>6.5$ using the Atacama Large Millimeter Array (ALMA) and the NOrthern Extended Millimeter Array (NOEMA) at sub-arcsec resolution.","This survey more than doubles the number of quasars with [CII] and FIR observations at these redshifts and enables statistical studies of quasar host galaxies deep into the epoch of reionization.","We detect [CII] emission in 27 quasar hosts with a luminosity range of $L_{\\rm [CII]}=(0.3-5.5)\\times10^9~L_\\odot$ and detect the FIR continuum of 28 quasar hosts with a luminosity range of $L_{\\rm FIR}=(0.5-13.0)\\times10^{12}~L_\\odot$.","Both $L_{\\rm [CII]}$ and $L_{\\rm FIR}$ are correlated ($\\rho\\simeq0.4$) with the quasar bolometric luminosity, albeit with substantial scatter.","The quasar hosts detected by ALMA are clearly resolved with a median diameter of $\\sim$5 kpc.","About 40% of the quasar host galaxies show a velocity gradient in [CII] emission, while the rest show either dispersion-dominated or disturbed kinematics.","Basic estimates of the dynamical masses of the rotation-dominated host galaxies yield $M_{\\rm dyn}=(0.1-7.5)\\times10^{11}~M_\\odot$. Considering our findings alongside those of literature studies, we found that the ratio between $M_{\\rm BH}$ and $M_{\\rm dyn}$ is about ten times higher than that of local $M_{\\rm BH}-M_{\\rm dyn}$ relation on average but with substantial scatter (the ratio difference ranging from $\\sim$0.6 to 60) and large uncertainties."],"url":"http://arxiv.org/abs/2404.15413v1","category":"astro-ph.GA"}
{"created":"2024-04-23 18:01:51","title":"Stellar atmospheric parameters of $\\sim$ 11,000 RR Lyrae stars from LAMOST Spectra","abstract":"Accurate determination of the stellar atmospheric parameters of RR Lyrae stars (RRLs) requires short individual exposures of the spectra to mitigate pulsation effects. We present improved template matching methods to determine the stellar atmospheric parameters of RRLs from single-epoch spectra of LAMOST (Large Sky Area Multi-Object Fiber Spectroscopic Telescope, also known as the Guoshoujing telescope). We determine the radial velocities and stellar atmospheric parameters (effective temperature: $T_\\mathrm{eff}$, surface gravity: $\\log{g}$, and metallicity: [M/H]) of 10,486 and 1,027 RRLs from 42,729 low-resolution spectra (LRS) and 7,064 medium-resolution spectra (MRS) of LAMOST, respectively. Our results are in good agreement with the parameters of other databases, where the external uncertainties of $T_\\mathrm{eff}$, $\\log{g}$, and [M/H] for LRS/MRS are estimated to be 314/274 K, 0.42/0.29 dex, and 0.39/0.31 dex, respectively. We conclude with the variation characteristics of the radial velocities ($RV$) and stellar atmospheric parameters for RRLs during the pulsation phase. There is a significant difference of $28\\pm21$ km/s between the peak-to-peak amplitude ($A_\\mathrm{ptp}$) of $RV$ from H$\\alpha$ line ($RV_\\mathrm{H\\alpha}$) and from metal lines ($RV_\\mathrm{metal}$) for RRab, whereas it is only $4\\pm17$ km/s for RRc. The $A_\\mathrm{ptp}$ of $T_\\mathrm{eff}$ is $930\\pm456$ and $409\\pm375$ K for RRab and RRc, respectively. The $\\log{g}$ of RRab show mild variation of approximately $0.23\\pm0.42$ dex near the phase of $\\varphi = 0.9$, while that of RRc almost remains constant. The [M/H] of RRab and RRc show a minor variation of about $0.25\\pm0.50$ and $0.28\\pm0.55$ dex, respectively, near the phase of $\\varphi = 0.9$.","sentences":["Accurate determination of the stellar atmospheric parameters of RR Lyrae stars (RRLs) requires short individual exposures of the spectra to mitigate pulsation effects.","We present improved template matching methods to determine the stellar atmospheric parameters of RRLs from single-epoch spectra of LAMOST (Large Sky Area Multi-Object Fiber Spectroscopic Telescope, also known as the Guoshoujing telescope).","We determine the radial velocities and stellar atmospheric parameters (effective temperature: $T_\\mathrm{eff}$, surface gravity: $\\log{g}$, and metallicity:","[M/H]) of 10,486 and 1,027 RRLs from 42,729 low-resolution spectra (LRS) and 7,064 medium-resolution spectra (MRS) of LAMOST, respectively.","Our results are in good agreement with the parameters of other databases, where the external uncertainties of $T_\\mathrm{eff}$, $\\log{g}$, and [M/H] for LRS/MRS are estimated to be 314/274 K, 0.42/0.29 dex, and 0.39/0.31 dex, respectively.","We conclude with the variation characteristics of the radial velocities ($RV$) and stellar atmospheric parameters for RRLs during the pulsation phase.","There is a significant difference of $28\\pm21$ km/s between the peak-to-peak amplitude ($A_\\mathrm{ptp}$) of $RV$ from H$\\alpha$ line ($RV_\\mathrm{H\\alpha}$) and from metal lines ($RV_\\mathrm{metal}$) for RRab, whereas it is only $4\\pm17$ km/s for RRc.","The $A_\\mathrm{ptp}$ of $T_\\mathrm{eff}$ is $930\\pm456$ and $409\\pm375$ K for RRab and RRc, respectively.","The $\\log{g}$ of RRab show mild variation of approximately $0.23\\pm0.42$ dex near the phase of $\\varphi = 0.9$, while that of RRc almost remains constant.","The [M/H] of RRab and RRc show a minor variation of about $0.25\\pm0.50$ and $0.28\\pm0.55$ dex, respectively, near the phase of $\\varphi = 0.9$."],"url":"http://arxiv.org/abs/2404.15411v1","category":"astro-ph.SR"}
{"created":"2024-04-23 18:00:01","title":"KiDS-SBI: Simulation-Based Inference Analysis of KiDS-1000 Cosmic Shear","abstract":"We present a simulation-based inference (SBI) cosmological analysis of cosmic shear two-point statistics from the fourth weak gravitational lensing data release of the ESO Kilo-Degree Survey (KiDS-1000). KiDS-SBI efficiently performs non-Limber projection of the matter power spectrum via Levin's method, and constructs log-normal random matter fields on the curved sky for arbitrary cosmologies, including effective prescriptions for intrinsic alignments and baryonic feedback. The forward model samples realistic galaxy positions and shapes based on the observational characteristics, incorporating shear measurement and redshift calibration uncertainties, as well as angular anisotropies due to variations in depth and point-spread function. To enable direct comparison with standard inference, we limit our analysis to pseudo-angular power spectra. The SBI is based on sequential neural likelihood estimation to infer the posterior distribution of spatially-flat $\\Lambda$CDM cosmological parameters from 18,000 realisations. We infer a mean marginal of the growth of structure parameter $S_{8} \\equiv \\sigma_8 (\\Omega_\\mathrm{m} / 0.3)^{0.5} = 0.731\\pm 0.033$ ($68 \\%$). We present a measure of goodness-of-fit for SBI and determine that the forward model fits the data well with a probability-to-exceed of $0.42$. For fixed cosmology, the learnt likelihood is approximately Gaussian, while constraints widen compared to a Gaussian likelihood analysis due to cosmology dependence in the covariance. Neglecting variable depth and anisotropies in the point spread function in the model can cause $S_{8}$ to be overestimated by ${\\sim}5\\%$. Our results are in agreement with previous analysis of KiDS-1000 and reinforce a $2.9 \\sigma$ tension with constraints from cosmic microwave background measurements. This work highlights the importance of forward-modelling systematic effects in upcoming galaxy surveys.","sentences":["We present a simulation-based inference (SBI) cosmological analysis of cosmic shear two-point statistics from the fourth weak gravitational lensing data release of the ESO Kilo-Degree Survey (KiDS-1000).","KiDS-SBI efficiently performs non-Limber projection of the matter power spectrum via Levin's method, and constructs log-normal random matter fields on the curved sky for arbitrary cosmologies, including effective prescriptions for intrinsic alignments and baryonic feedback.","The forward model samples realistic galaxy positions and shapes based on the observational characteristics, incorporating shear measurement and redshift calibration uncertainties, as well as angular anisotropies due to variations in depth and point-spread function.","To enable direct comparison with standard inference, we limit our analysis to pseudo-angular power spectra.","The SBI is based on sequential neural likelihood estimation to infer the posterior distribution of spatially-flat $\\Lambda$CDM cosmological parameters from 18,000 realisations.","We infer a mean marginal of the growth of structure parameter $S_{8} \\equiv \\sigma_8 (\\Omega_\\mathrm{m} / 0.3)^{0.5} = 0.731\\pm 0.033$ ($68 \\%$).","We present a measure of goodness-of-fit for SBI and determine that the forward model fits the data well with a probability-to-exceed of $0.42$. For fixed cosmology, the learnt likelihood is approximately Gaussian, while constraints widen compared to a Gaussian likelihood analysis due to cosmology dependence in the covariance.","Neglecting variable depth and anisotropies in the point spread function in the model can cause $S_{8}$ to be overestimated by ${\\sim}5\\%$. Our results are in agreement with previous analysis of KiDS-1000 and reinforce a $2.9 \\sigma$ tension with constraints from cosmic microwave background measurements.","This work highlights the importance of forward-modelling systematic effects in upcoming galaxy surveys."],"url":"http://arxiv.org/abs/2404.15402v1","category":"astro-ph.CO"}
{"created":"2024-04-24 03:18:49","title":"A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution","abstract":"Deep learning-based methods have achieved significant successes on solving the blind super-resolution (BSR) problem. However, most of them request supervised pre-training on labelled datasets. This paper proposes an unsupervised kernel estimation model, named dynamic kernel prior (DKP), to realize an unsupervised and pre-training-free learning-based algorithm for solving the BSR problem. DKP can adaptively learn dynamic kernel priors to realize real-time kernel estimation, and thereby enables superior HR image restoration performances. This is achieved by a Markov chain Monte Carlo sampling process on random kernel distributions. The learned kernel prior is then assigned to optimize a blur kernel estimation network, which entails a network-based Langevin dynamic optimization strategy. These two techniques ensure the accuracy of the kernel estimation. DKP can be easily used to replace the kernel estimation models in the existing methods, such as Double-DIP and FKP-DIP, or be added to the off-the-shelf image restoration model, such as diffusion model. In this paper, we incorporate our DKP model with DIP and diffusion model, referring to DIP-DKP and Diff-DKP, for validations. Extensive simulations on Gaussian and motion kernel scenarios demonstrate that the proposed DKP model can significantly improve the kernel estimation with comparable runtime and memory usage, leading to state-of-the-art BSR results. The code is available at https://github.com/XYLGroup/DKP.","sentences":["Deep learning-based methods have achieved significant successes on solving the blind super-resolution (BSR) problem.","However, most of them request supervised pre-training on labelled datasets.","This paper proposes an unsupervised kernel estimation model, named dynamic kernel prior (DKP), to realize an unsupervised and pre-training-free learning-based algorithm for solving the BSR problem.","DKP can adaptively learn dynamic kernel priors to realize real-time kernel estimation, and thereby enables superior HR image restoration performances.","This is achieved by a Markov chain Monte Carlo sampling process on random kernel distributions.","The learned kernel prior is then assigned to optimize a blur kernel estimation network, which entails a network-based Langevin dynamic optimization strategy.","These two techniques ensure the accuracy of the kernel estimation.","DKP can be easily used to replace the kernel estimation models in the existing methods, such as Double-DIP and FKP-DIP, or be added to the off-the-shelf image restoration model, such as diffusion model.","In this paper, we incorporate our DKP model with DIP and diffusion model, referring to DIP-DKP and Diff-DKP, for validations.","Extensive simulations on Gaussian and motion kernel scenarios demonstrate that the proposed DKP model can significantly improve the kernel estimation with comparable runtime and memory usage, leading to state-of-the-art BSR results.","The code is available at https://github.com/XYLGroup/DKP."],"url":"http://arxiv.org/abs/2404.15620v1","category":"eess.IV"}
{"created":"2024-04-24 03:08:25","title":"MDDD: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition","abstract":"Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces represents a significant area within the field of affective computing. In the present study, we propose a novel non-deep transfer learning method, termed as Manifold-based Domain adaptation with Dynamic Distribution (MDDD). The proposed MDDD includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data undergoes a transformation onto an optimal Grassmann manifold space, enabling dynamic alignment of the source and target domains. This process prioritizes both marginal and conditional distributions according to their significance, ensuring enhanced adaptation efficiency across various types of data. In the classifier learning, the principle of structural risk minimization is integrated to develop robust classification models. This is complemented by dynamic distribution alignment, which refines the classifier iteratively. Additionally, the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process, which leverages the diversity of the classifiers to enhance the overall prediction accuracy. The experimental results indicate that MDDD outperforms traditional non-deep learning methods, achieving an average improvement of 3.54%, and is comparable to deep learning methods. This suggests that MDDD could be a promising method for enhancing the utility and applicability of aBCIs in real-world scenarios.","sentences":["Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces represents a significant area within the field of affective computing.","In the present study, we propose a novel non-deep transfer learning method, termed as Manifold-based Domain adaptation with Dynamic Distribution (MDDD).","The proposed MDDD includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning.","The data undergoes a transformation onto an optimal Grassmann manifold space, enabling dynamic alignment of the source and target domains.","This process prioritizes both marginal and conditional distributions according to their significance, ensuring enhanced adaptation efficiency across various types of data.","In the classifier learning, the principle of structural risk minimization is integrated to develop robust classification models.","This is complemented by dynamic distribution alignment, which refines the classifier iteratively.","Additionally, the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process, which leverages the diversity of the classifiers to enhance the overall prediction accuracy.","The experimental results indicate that MDDD outperforms traditional non-deep learning methods, achieving an average improvement of 3.54%, and is comparable to deep learning methods.","This suggests that MDDD could be a promising method for enhancing the utility and applicability of aBCIs in real-world scenarios."],"url":"http://arxiv.org/abs/2404.15615v1","category":"cs.HC"}
{"created":"2024-04-24 01:50:36","title":"Domain Adaptation for Learned Image Compression with Supervised Adapters","abstract":"In Learned Image Compression (LIC), a model is trained at encoding and decoding images sampled from a source domain, often outperforming traditional codecs on natural images; yet its performance may be far from optimal on images sampled from different domains. In this work, we tackle the problem of adapting a pre-trained model to multiple target domains by plugging into the decoder an adapter module for each of them, including the source one. Each adapter improves the decoder performance on a specific domain, without the model forgetting about the images seen at training time. A gate network computes the weights to optimally blend the contributions from the adapters when the bitstream is decoded. We experimentally validate our method over two state-of-the-art pre-trained models, observing improved rate-distortion efficiency on the target domains without penalties on the source domain. Furthermore, the gate's ability to find similarities with the learned target domains enables better encoding efficiency also for images outside them.","sentences":["In Learned Image Compression (LIC), a model is trained at encoding and decoding images sampled from a source domain, often outperforming traditional codecs on natural images; yet its performance may be far from optimal on images sampled from different domains.","In this work, we tackle the problem of adapting a pre-trained model to multiple target domains by plugging into the decoder an adapter module for each of them, including the source one.","Each adapter improves the decoder performance on a specific domain, without the model forgetting about the images seen at training time.","A gate network computes the weights to optimally blend the contributions from the adapters when the bitstream is decoded.","We experimentally validate our method over two state-of-the-art pre-trained models, observing improved rate-distortion efficiency on the target domains without penalties on the source domain.","Furthermore, the gate's ability to find similarities with the learned target domains enables better encoding efficiency also for images outside them."],"url":"http://arxiv.org/abs/2404.15591v1","category":"cs.CV"}
{"created":"2024-04-23 23:13:14","title":"The extended Lipkin model: proposal for implementation in a quantum platform and machine learning analysis of its phase diagram","abstract":"We investigate the Extended Lipkin Model (ELM), whose phase diagram mirrors that of the Interacting Boson Approximation model (IBA). Unlike the standard Lipkin model, the ELM (as the IBA) features both first- and second-order quantum shape phase transitions depending on the model parameters. Our goal is to implement the ELM on a quantum platform, leveraging Machine Learning techniques to identify its quantum phase transitions and critical lines. To achieve this, we offer: i) ground state energy calculations using a variational quantum eigensolver; ii) a detailed formulation for ELM dynamics within quantum computing, facilitating experimental exploration of the IBA phase diagram; and iii) a phase diagram determination using various Machine Learning methods. We successfully replicate the ELM ground-state energy using the Adaptive Derivative-Assembled Pseudo-Trotter ansatz Variational Quantum Eigensolver (ADAPT-VQE) algorithm across the entire phase space. Our framework ensures ELM implementation on quantum platforms with controlled errors. Lastly, our ML predictions yield a meaningful phase diagram for the model.   Keywords: Quantum Platforms Nuclear Models ADAPT-VQE Quantum Shape Phase Transitions Interacting Boson Approximation Extended Lipkin Model Machine Learning","sentences":["We investigate the Extended Lipkin Model (ELM), whose phase diagram mirrors that of the Interacting Boson Approximation model (IBA).","Unlike the standard Lipkin model, the ELM (as the IBA) features both first- and second-order quantum shape phase transitions depending on the model parameters.","Our goal is to implement the ELM on a quantum platform, leveraging Machine Learning techniques to identify its quantum phase transitions and critical lines.","To achieve this, we offer: i) ground state energy calculations using a variational quantum eigensolver; ii) a detailed formulation for ELM dynamics within quantum computing, facilitating experimental exploration of the IBA phase diagram; and iii) a phase diagram determination using various Machine Learning methods.","We successfully replicate the ELM ground-state energy using the Adaptive Derivative-Assembled Pseudo-Trotter ansatz Variational Quantum Eigensolver (ADAPT-VQE) algorithm across the entire phase space.","Our framework ensures ELM implementation on quantum platforms with controlled errors.","Lastly, our ML predictions yield a meaningful phase diagram for the model.   ","Keywords: Quantum Platforms Nuclear Models ADAPT-VQE Quantum Shape Phase Transitions Interacting Boson Approximation Extended Lipkin Model Machine Learning"],"url":"http://arxiv.org/abs/2404.15558v1","category":"quant-ph"}
{"created":"2024-04-22 17:05:29","title":"A Genetic Algorithm For Convex Hull Optimisation","abstract":"Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures. A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species. This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures.","sentences":["Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures.","A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species.","This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures."],"url":"http://arxiv.org/abs/2404.14354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 16:58:37","title":"Heterogeneous Face Recognition Using Domain Invariant Units","abstract":"Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra. However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data. In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap. The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework. This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods.","sentences":["Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra.","However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data.","In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap.","The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework.","This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data.","We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.14343v1","category":"cs.CV"}
{"created":"2024-04-22 16:56:43","title":"Zero-shot Cross-lingual Stance Detection via Adversarial Language Adaptation","abstract":"Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian. We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub.","sentences":["Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines.","Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model.","This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages.","Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy.","Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian.","We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model.","Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model.","We have made our source code accessible on GitHub."],"url":"http://arxiv.org/abs/2404.14339v1","category":"cs.CL"}
{"created":"2024-04-22 16:41:14","title":"Comparison of Empirical Models of Ionospheric Heating to Global Simulations","abstract":"Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating. This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively. Several empirical models estimate Joule heating based on ionospheric currents using the AE index. In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models. We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating. We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations.","sentences":["Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating.","This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively.","Several empirical models estimate Joule heating based on ionospheric currents using the AE index.","In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models.","We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating.","We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations."],"url":"http://arxiv.org/abs/2404.14330v1","category":"astro-ph.EP"}
{"created":"2024-04-22 16:40:11","title":"X-Ray: A Sequential 3D Representation for Generation","abstract":"In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects. Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces. This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos. Such a structure ensures the 3D representation is composed solely of critical surface information. Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models. The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field.","sentences":["In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects.","Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces.","This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos.","Such a structure ensures the 3D representation is composed solely of critical surface information.","Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models.","The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field."],"url":"http://arxiv.org/abs/2404.14329v1","category":"cs.CV"}
{"created":"2024-04-22 16:11:12","title":"Cryogenic sapphire optical reference cavity with crystalline coatings at $\\mathrm{ 1 \\times 10^{-16}}$ fractional instability","abstract":"The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length. These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties. Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented. Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail. The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s.","sentences":["The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length.","These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties.","Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented.","Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail.","The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s."],"url":"http://arxiv.org/abs/2404.14310v1","category":"physics.optics"}
{"created":"2024-04-22 15:26:59","title":"Mass-radius relationships and contraction of condensed planets by cooling or despinning","abstract":"Condensed planets contract or expand as their temperature changes. With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components. However, changes in density affect pressure and gravity and, consequently, the planet's compressibility. A planet's radius is also linked to its rate of rotation. Here again, changes in pressure, gravity and compressibility are coupled. In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model. We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies. Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate. We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility). This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter. While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius. Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets.","sentences":["Condensed planets contract or expand as their temperature changes.","With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components.","However, changes in density affect pressure and gravity and, consequently, the planet's compressibility.","A planet's radius is also linked to its rate of rotation.","Here again, changes in pressure, gravity and compressibility are coupled.","In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model.","We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies.","Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate.","We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility).","This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter.","While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius.","Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets."],"url":"http://arxiv.org/abs/2404.14278v1","category":"astro-ph.EP"}
{"created":"2024-04-22 15:23:30","title":"Maximally informative feature selection using Information Imbalance: Application to COVID-19 severity prediction","abstract":"Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others. We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes. We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients. We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021. Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease. The optimal number of features, which is determined automatically, turns out to be between 10 and 15. These features can be measured at admission. The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation. Clinical insights deriving from this study are also discussed.","sentences":["Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others.","We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes.","We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients.","We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021.","Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease.","The optimal number of features, which is determined automatically, turns out to be between 10 and 15.","These features can be measured at admission.","The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation.","Clinical insights deriving from this study are also discussed."],"url":"http://arxiv.org/abs/2404.14275v1","category":"stat.ME"}
{"created":"2024-04-22 15:00:51","title":"From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition","abstract":"Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios. However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch. In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap. We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems. The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap. Our method enables end-to-end training using a small set of paired samples. We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available","sentences":["Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios.","However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch.","In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap.","We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems.","The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap.","Our method enables end-to-end training using a small set of paired samples.","We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods.","The source code and protocols for reproducing the findings will be made publicly available"],"url":"http://arxiv.org/abs/2404.14247v1","category":"cs.CV"}
{"created":"2024-04-22 14:53:27","title":"UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain Adaptation","abstract":"Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications. However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains. To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval. UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity. It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance. Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains. Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap.","sentences":["Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications.","However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains.","To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval.","UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity.","It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance.","Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains.","Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap."],"url":"http://arxiv.org/abs/2404.14241v1","category":"cs.CV"}
{"created":"2024-04-22 14:45:30","title":"Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting and Cognitive Load on User Attention and Saliency Prediction","abstract":"Visual highlighting can guide user attention in complex interfaces. However, its effectiveness under limited attentional capacities is underexplored. This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour. Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing. The presence of these factors significantly alters what people attend to and thus what is salient. Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads. Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking.","sentences":["Visual highlighting can guide user attention in complex interfaces.","However, its effectiveness under limited attentional capacities is underexplored.","This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour.","Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing.","The presence of these factors significantly alters what people attend to and thus what is salient.","Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads.","Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking."],"url":"http://arxiv.org/abs/2404.14232v1","category":"cs.HC"}
{"created":"2024-04-22 14:32:46","title":"Robust electrothermal switching of optical phase change materials through computer-aided adaptive pulse optimization","abstract":"Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states. This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage. Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states. Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging. In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events. To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices. Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films.","sentences":["Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states.","This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage.","Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states.","Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging.","In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events.","To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices.","Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films."],"url":"http://arxiv.org/abs/2404.14220v1","category":"physics.optics"}
{"created":"2024-04-22 14:31:55","title":"General protocols for the efficient distillation of indistinguishable photons","abstract":"Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing. We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance. We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols. We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry.","sentences":["Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing.","We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance.","We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols.","We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry."],"url":"http://arxiv.org/abs/2404.14217v1","category":"quant-ph"}
{"created":"2024-04-22 14:11:54","title":"Rotting Infinitely Many-armed Bandits beyond the Worst-case Rotting: An Adaptive Approach","abstract":"In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithms using synthetic datasets.","sentences":["In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged.","We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario.","To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards.","Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios.","Lastly, we demonstrate the performance of our algorithms using synthetic datasets."],"url":"http://arxiv.org/abs/2404.14202v1","category":"cs.LG"}
{"created":"2024-04-22 14:06:27","title":"Universal formal asymptotics for localized oscillation of a discrete mass-spring-damper system of time-varying properties, embedded into a one-dimensional medium described by the telegraph equation with variable coefficients","abstract":"We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system. This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation. In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed. Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible. In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position. This general statement can describe a number of mechanical systems of various nature. We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system. In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters. In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters. Finally, we have justified the universal asymptotics by numerical calculations for some particular cases.","sentences":["We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system.","This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation.","In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed.","Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible.","In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position.","This general statement can describe a number of mechanical systems of various nature.","We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system.","In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters.","In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters.","Finally, we have justified the universal asymptotics by numerical calculations for some particular cases."],"url":"http://arxiv.org/abs/2404.14196v1","category":"physics.class-ph"}
{"created":"2024-04-22 13:49:42","title":"Face2Face: Label-driven Facial Retouching Restoration","abstract":"With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs. This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media. By altering facial images, users can easily create deceptive images, leading to the dissemination of false information. This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud. To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching. To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN). Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees. Then FaceR restores the retouched image based on the predicted retouching label. Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models. Extensive experiments demonstrate the effectiveness of our framework and each module.","sentences":["With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs.","This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media.","By altering facial images, users can easily create deceptive images, leading to the dissemination of false information.","This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud.","To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching.","To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN).","Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees.","Then FaceR restores the retouched image based on the predicted retouching label.","Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models.","Extensive experiments demonstrate the effectiveness of our framework and each module."],"url":"http://arxiv.org/abs/2404.14177v1","category":"cs.CV"}
{"created":"2024-04-22 13:41:03","title":"The effects of turbulence modeling on dynamic stall","abstract":"A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented. The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models. After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks. Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex. In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted. Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one. Finally, a comparison of the proposed approaches with other numerical simulations is given.","sentences":["A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented.","The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models.","After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks.","Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex.","In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted.","Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one.","Finally, a comparison of the proposed approaches with other numerical simulations is given."],"url":"http://arxiv.org/abs/2404.14172v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 13:36:25","title":"Optimal frequency for undulatory motion in granular media","abstract":"Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour. This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand. Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed. Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate. We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium. Potential implications in the design of artificial swimmers are discussed.","sentences":["Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour.","This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand.","Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed.","Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate.","We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium.","Potential implications in the design of artificial swimmers are discussed."],"url":"http://arxiv.org/abs/2404.14168v1","category":"cond-mat.soft"}
{"created":"2024-04-22 13:34:50","title":"A multi-robot system for the detection of explosive devices","abstract":"In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role. However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system. In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms. This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines. This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation. The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles. This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator. Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection.","sentences":["In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role.","However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system.","In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms.","This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines.","This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation.","The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles.","This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator.","Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection."],"url":"http://arxiv.org/abs/2404.14167v1","category":"cs.RO"}
{"created":"2024-04-22 13:20:01","title":"Multidimensional Interpolants","abstract":"In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases. In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework. Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations. Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path. Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization. When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations. The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier.","sentences":["In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases.","In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework.","Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations.","Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path.","Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization.","When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations.","The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier."],"url":"http://arxiv.org/abs/2404.14161v1","category":"cs.LG"}
{"created":"2024-04-22 12:55:21","title":"Travelling waves in an ensemble of excitable oscillators: the interplay of memristive coupling and noise","abstract":"Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model. First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling. In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction. In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves.","sentences":["Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model.","First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling.","In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction.","In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves."],"url":"http://arxiv.org/abs/2404.14147v1","category":"nlin.AO"}
{"created":"2024-04-22 12:05:34","title":"Anomalous dispersion via dissipative coupling in a quantum well exciton-polariton microcavity","abstract":"According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian. This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel. This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion. However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system. Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion. We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample. The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model. Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems.","sentences":["According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian.","This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel.","This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion.","However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system.","Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion.","We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample.","The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model.","Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems."],"url":"http://arxiv.org/abs/2404.14116v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 11:06:06","title":"Evidence and concerns about a latent, embryonic phase tectonic evolution and the existence of the young subsurface ocean on Mimas","abstract":"New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean. Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean. Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation. In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs. Simple tectonic features are identified as young compared to complex structures. The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns. In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean. However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago.","sentences":["New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean.","Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean.","Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation.","In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs.","Simple tectonic features are identified as young compared to complex structures.","The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns.","In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean.","However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago."],"url":"http://arxiv.org/abs/2404.14084v1","category":"astro-ph.EP"}
{"created":"2024-04-22 10:52:52","title":"Dynamical scaling and Planckian dissipation due to heavy-fermion quantum criticality","abstract":"We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT). In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling. We propose a scaling Ansatz that describes this behavior. It also implies Planckian dissipation for the longest-lived excitations. The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling. This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point. Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities. Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$.","sentences":["We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT).","In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling.","We propose a scaling Ansatz that describes this behavior.","It also implies Planckian dissipation for the longest-lived excitations.","The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling.","This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point.","Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities.","Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$."],"url":"http://arxiv.org/abs/2404.14079v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 10:19:16","title":"GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text Recognition System","abstract":"The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition. Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem. Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition. However, this method has several drawbacks. These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation. Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies. Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder. The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models. The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line. During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step. In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network. Our results at line and page levels also favour our new GatedLexiconNet. This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets.","sentences":["The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition.","Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem.","Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition.","However, this method has several drawbacks.","These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation.","Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies.","Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder.","The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models.","The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line.","During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step.","In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network.","Our results at line and page levels also favour our new GatedLexiconNet.","This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets."],"url":"http://arxiv.org/abs/2404.14062v1","category":"cs.CV"}
{"created":"2024-04-22 09:57:53","title":"HashPoint: Accelerated Point Searching and Sampling for Neural Rendering","abstract":"In this paper, we address the problem of efficient point searching and sampling for volume neural rendering. Within this realm, two typical approaches are employed: rasterization and ray tracing. The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity. In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time. We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering. Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches. Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray. Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets. The code will be available at https://jiahao-ma.github.io/hashpoint/.","sentences":["In this paper, we address the problem of efficient point searching and sampling for volume neural rendering.","Within this realm, two typical approaches are employed: rasterization and ray tracing.","The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity.","In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time.","We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering.","Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches.","Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray.","Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets.","The code will be available at https://jiahao-ma.github.io/hashpoint/."],"url":"http://arxiv.org/abs/2404.14044v1","category":"cs.CV"}
{"created":"2024-04-22 09:40:07","title":"Exploring neural oscillations during speech perception via surrogate gradient spiking neural networks","abstract":"Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs. Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance. Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology.","sentences":["Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales.","We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network.","Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs.","Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance.","Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology."],"url":"http://arxiv.org/abs/2404.14024v1","category":"cs.CL"}
{"created":"2024-04-22 09:35:48","title":"Physics-Informed Neural Networks and Beyond: Enforcing Physical Constraints in Quantum Dissipative Dynamics","abstract":"Neural networks (NNs) accelerate simulations of quantum dissipative dynamics. Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches. We show that this may lead to implausible results measured by violation of the trace conservation. To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend. Beyond that, we introduce an approach enforcing the perfect trace conservation by design.","sentences":["Neural networks (NNs) accelerate simulations of quantum dissipative dynamics.","Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches.","We show that this may lead to implausible results measured by violation of the trace conservation.","To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend.","Beyond that, we introduce an approach enforcing the perfect trace conservation by design."],"url":"http://arxiv.org/abs/2404.14021v1","category":"physics.chem-ph"}
{"created":"2024-04-22 09:32:38","title":"Hybrid Ensemble-Based Travel Mode Prediction","abstract":"Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips. This is also a major step towards fostering sustainable transportation. As behaviour may evolve over time, we also face the question of detecting concept drift in the data. This necessitates using appropriate methods to address potential concept drift. In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models. To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data. It relies on the combination of drift detectors with batch learning and stream mining models. We compare it against batch and incremental learners, including methods relying on active drift detection. Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data. The method has a higher rank than batch and stream learners.","sentences":["Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips.","This is also a major step towards fostering sustainable transportation.","As behaviour may evolve over time, we also face the question of detecting concept drift in the data.","This necessitates using appropriate methods to address potential concept drift.","In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models.","To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data.","It relies on the combination of drift detectors with batch learning and stream mining models.","We compare it against batch and incremental learners, including methods relying on active drift detection.","Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data.","The method has a higher rank than batch and stream learners."],"url":"http://arxiv.org/abs/2404.14017v1","category":"cs.LG"}
{"created":"2024-04-22 09:09:09","title":"Temporal genomics help in deciphering neutral and adaptive patterns in the contemporary evolution of kelp populations","abstract":"The impact of climate change on populations will be contingent upon their contemporary adaptive evolution. In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing. These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations. Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients. This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered. Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time. This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time. In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity. The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments. Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change.","sentences":["The impact of climate change on populations will be contingent upon their contemporary adaptive evolution.","In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing.","These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations.","Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients.","This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered.","Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time.","This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time.","In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity.","The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments.","Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change."],"url":"http://arxiv.org/abs/2404.14003v1","category":"q-bio.PE"}
{"created":"2024-04-22 08:57:46","title":"QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models -- Extended Version","abstract":"We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes. It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers. To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits. The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy. This one-time calibration works for deployments in static environments. However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions. The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices. The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive. We propose QCore to enable continual calibration on the edge. First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths. We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data. Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation. An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods.","sentences":["We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes.","It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers.","To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits.","The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy.","This one-time calibration works for deployments in static environments.","However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions.","The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices.","The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive.","We propose QCore to enable continual calibration on the edge.","First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths.","We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data.","Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation.","An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods."],"url":"http://arxiv.org/abs/2404.13990v1","category":"cs.LG"}
{"created":"2024-04-22 08:44:10","title":"Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network","abstract":"Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape. As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape. However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body. In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow. Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph. AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map. The design could better improve the consistency between body parts. (2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain. Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD. With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting. Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics.","sentences":["Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape.","As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape.","However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body.","In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow.","Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph.","AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map.","The design could better improve the consistency between body parts.","(2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain.","Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD.","With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting.","Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics."],"url":"http://arxiv.org/abs/2404.13983v1","category":"cs.CV"}
{"created":"2024-04-22 08:28:41","title":"Non-Uniform Exposure Imaging via Neuromorphic Shutter Control","abstract":"By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments. However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control. To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure. Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes. To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns. Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches.","sentences":["By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments.","However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control.","To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure.","Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes.","To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns.","Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.13972v1","category":"cs.CV"}
{"created":"2024-04-24 03:22:49","title":"FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture Search","abstract":"Neural Architecture Search (NAS) has emerged as a key tool in identifying optimal configurations of deep neural networks tailored to specific tasks. However, training and assessing numerous architectures introduces considerable computational overhead. One method to mitigating this is through performance predictors, which offer a means to estimate the potential of an architecture without exhaustive training. Given that neural architectures fundamentally resemble Directed Acyclic Graphs (DAGs), Graph Neural Networks (GNNs) become an apparent choice for such predictive tasks. Nevertheless, the scarcity of training data can impact the precision of GNN-based predictors. To address this, we introduce a novel GNN predictor for NAS. This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views. Additionally, we incorporate a customized training loss within the GNN predictor to ensure efficient utilization of both types of representations. We subsequently assessed our method through experiments on benchmark datasets including NAS-Bench-101, NAS-Bench-201, and the DARTS search space, with a training dataset ranging from 50 to 400 samples. Benchmarked against leading GNN predictors, the experimental results showcase a significant improvement in prediction accuracy, with a 3%--16% increase in Kendall-tau correlation. Source codes are available at https://github.com/EMI-Group/fr-nas.","sentences":["Neural Architecture Search (NAS) has emerged as a key tool in identifying optimal configurations of deep neural networks tailored to specific tasks.","However, training and assessing numerous architectures introduces considerable computational overhead.","One method to mitigating this is through performance predictors, which offer a means to estimate the potential of an architecture without exhaustive training.","Given that neural architectures fundamentally resemble Directed Acyclic Graphs (DAGs), Graph Neural Networks (GNNs) become an apparent choice for such predictive tasks.","Nevertheless, the scarcity of training data can impact the precision of GNN-based predictors.","To address this, we introduce a novel GNN predictor for NAS.","This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views.","Additionally, we incorporate a customized training loss within the GNN predictor to ensure efficient utilization of both types of representations.","We subsequently assessed our method through experiments on benchmark datasets including NAS-Bench-101, NAS-Bench-201, and the DARTS search space, with a training dataset ranging from 50 to 400 samples.","Benchmarked against leading GNN predictors, the experimental results showcase a significant improvement in prediction accuracy, with a 3%--16% increase in Kendall-tau correlation.","Source codes are available at https://github.com/EMI-Group/fr-nas."],"url":"http://arxiv.org/abs/2404.15622v1","category":"cs.LG"}
{"created":"2024-04-24 03:06:01","title":"DyGCL: Dynamic Graph Contrastive Learning For Event Prediction","abstract":"Predicting events such as political protests, flu epidemics, and criminal activities is crucial to proactively taking necessary measures and implementing required responses to address emerging challenges. Capturing contextual information from textual data for event forecasting poses significant challenges due to the intricate structure of the documents and the evolving nature of events. Recently, dynamic Graph Neural Networks (GNNs) have been introduced to capture the dynamic patterns of input text graphs. However, these models only utilize node-level representation, causing the loss of the global information from graph-level representation. On the other hand, both node-level and graph-level representations are essential for effective event prediction as node-level representation gives insight into the local structure, and the graph-level representation provides an understanding of the global structure of the temporal graph. To address these challenges, in this paper, we propose a Dynamic Graph Contrastive Learning (DyGCL) method for event prediction. Our model DyGCL employs a local view encoder to learn the evolving node representations, which effectively captures the local dynamic structure of input graphs. Additionally, it harnesses a global view encoder to perceive the hierarchical dynamic graph representation of the input graphs. Then we update the graph representations from both encoders using contrastive learning. In the final stage, DyGCL combines both representations using an attention mechanism and optimizes its capability to predict future events. Our extensive experiment demonstrates that our proposed method outperforms the baseline methods for event prediction on six real-world datasets.","sentences":["Predicting events such as political protests, flu epidemics, and criminal activities is crucial to proactively taking necessary measures and implementing required responses to address emerging challenges.","Capturing contextual information from textual data for event forecasting poses significant challenges due to the intricate structure of the documents and the evolving nature of events.","Recently, dynamic Graph Neural Networks (GNNs) have been introduced to capture the dynamic patterns of input text graphs.","However, these models only utilize node-level representation, causing the loss of the global information from graph-level representation.","On the other hand, both node-level and graph-level representations are essential for effective event prediction as node-level representation gives insight into the local structure, and the graph-level representation provides an understanding of the global structure of the temporal graph.","To address these challenges, in this paper, we propose a Dynamic Graph Contrastive Learning (DyGCL) method for event prediction.","Our model DyGCL employs a local view encoder to learn the evolving node representations, which effectively captures the local dynamic structure of input graphs.","Additionally, it harnesses a global view encoder to perceive the hierarchical dynamic graph representation of the input graphs.","Then we update the graph representations from both encoders using contrastive learning.","In the final stage, DyGCL combines both representations using an attention mechanism and optimizes its capability to predict future events.","Our extensive experiment demonstrates that our proposed method outperforms the baseline methods for event prediction on six real-world datasets."],"url":"http://arxiv.org/abs/2404.15612v1","category":"cs.SI"}
{"created":"2024-04-24 02:08:25","title":"Curvature, diameter and signs of graphs","abstract":"We prove a Li-Yau type eigenvalue-diameter estimate for signed graphs. That is, the nonzero eigenvalues of the Laplacian of a non-negatively curved signed graph are lower bounded by $1/D^2$ up to a constant, where $D$ stands for the diameter. This leads to several interesting applications, including a volume estimate for non-negatively curved signed graphs in terms of frustration index and diameter, and a two-sided Li-Yau estimate for triangle-free graphs. Our proof is built upon a combination of Chung-Lin-Yau type gradient estimate and a new trick involving strong nodal domain walks of signed graphs. We further discuss extensions of part of our results to nonlinear Laplacians on signed graphs.","sentences":["We prove a Li-Yau type eigenvalue-diameter estimate for signed graphs.","That is, the nonzero eigenvalues of the Laplacian of a non-negatively curved signed graph are lower bounded by $1/D^2$ up to a constant, where $D$ stands for the diameter.","This leads to several interesting applications, including a volume estimate for non-negatively curved signed graphs in terms of frustration index and diameter, and a two-sided Li-Yau estimate for triangle-free graphs.","Our proof is built upon a combination of Chung-Lin-Yau type gradient estimate and a new trick involving strong nodal domain walks of signed graphs.","We further discuss extensions of part of our results to nonlinear Laplacians on signed graphs."],"url":"http://arxiv.org/abs/2404.15594v1","category":"math.CO"}
{"created":"2024-04-24 01:37:20","title":"Brain Storm Optimization Based Swarm Learning for Diabetic Retinopathy Image Classification","abstract":"The application of deep learning techniques to medical problems has garnered widespread research interest in recent years, such as applying convolutional neural networks to medical image classification tasks. However, data in the medical field is often highly private, preventing different hospitals from sharing data to train an accurate model. Federated learning, as a privacy-preserving machine learning architecture, has shown promising performance in balancing data privacy and model utility by keeping private data on the client's side and using a central server to coordinate a set of clients for model training through aggregating their uploaded model parameters. Yet, this architecture heavily relies on a trusted third-party server, which is challenging to achieve in real life. Swarm learning, as a specialized decentralized federated learning architecture that does not require a central server, utilizes blockchain technology to enable direct parameter exchanges between clients. However, the mining of blocks requires significant computational resources, limiting its scalability. To address this issue, this paper integrates the brain storm optimization algorithm into the swarm learning framework, named BSO-SL. This approach clusters similar clients into different groups based on their model distributions. Additionally, leveraging the architecture of BSO, clients are given the probability to engage in collaborative learning both within their cluster and with clients outside their cluster, preventing the model from converging to local optima. The proposed method has been validated on a real-world diabetic retinopathy image classification dataset, and the experimental results demonstrate the effectiveness of the proposed approach.","sentences":["The application of deep learning techniques to medical problems has garnered widespread research interest in recent years, such as applying convolutional neural networks to medical image classification tasks.","However, data in the medical field is often highly private, preventing different hospitals from sharing data to train an accurate model.","Federated learning, as a privacy-preserving machine learning architecture, has shown promising performance in balancing data privacy and model utility by keeping private data on the client's side and using a central server to coordinate a set of clients for model training through aggregating their uploaded model parameters.","Yet, this architecture heavily relies on a trusted third-party server, which is challenging to achieve in real life.","Swarm learning, as a specialized decentralized federated learning architecture that does not require a central server, utilizes blockchain technology to enable direct parameter exchanges between clients.","However, the mining of blocks requires significant computational resources, limiting its scalability.","To address this issue, this paper integrates the brain storm optimization algorithm into the swarm learning framework, named BSO-SL.","This approach clusters similar clients into different groups based on their model distributions.","Additionally, leveraging the architecture of BSO, clients are given the probability to engage in collaborative learning both within their cluster and with clients outside their cluster, preventing the model from converging to local optima.","The proposed method has been validated on a real-world diabetic retinopathy image classification dataset, and the experimental results demonstrate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2404.15585v1","category":"cs.LG"}
{"created":"2024-04-23 22:50:02","title":"$\u03b3_5$ subtleties in distinct regularizations: the Bumblebee model example","abstract":"We examine the subtleties of regularization schemes in four-dimensional space ($4S$), related in particular to the introduction of the $\\gamma_5$ matrix. To illustrate we use a \"Bumblebee\" model featuring dynamically induced Lorentz symmetry violation. The analysis centers on how different regularization methods affect the solutions to the gap equation in this model. We highlight the resolution of ambiguities associated with the $\\gamma_5$ matrix in ultraviolet divergent integrals by employing an enhanced Implicit Regularization (IREG) method. This method extends IREG to a quasi-four-dimensional space, $Q4S = 4S \\oplus X$, drawing parallels with the consistent approach of Dimensional Reduction (DRED). Comparative analysis is conducted against results from the 't Hooft-Veltman regularization scheme, conventional IREG in strict $4S$, and sharp momentum cutoff techniques. Our results illustrate a scheme to compute $\\gamma_5$ interactions in physical dimension of divergent amplitudes, confirming the approach in [1].","sentences":["We examine the subtleties of regularization schemes in four-dimensional space ($4S$), related in particular to the introduction of the $\\gamma_5$ matrix.","To illustrate we use a \"Bumblebee\" model featuring dynamically induced Lorentz symmetry violation.","The analysis centers on how different regularization methods affect the solutions to the gap equation in this model.","We highlight the resolution of ambiguities associated with the $\\gamma_5$ matrix in ultraviolet divergent integrals by employing an enhanced Implicit Regularization (IREG) method.","This method extends IREG to a quasi-four-dimensional space, $Q4S = 4S \\oplus X$, drawing parallels with the consistent approach of Dimensional Reduction (DRED).","Comparative analysis is conducted against results from the 't Hooft-Veltman regularization scheme, conventional IREG in strict $4S$, and sharp momentum cutoff techniques.","Our results illustrate a scheme to compute $\\gamma_5$ interactions in physical dimension of divergent amplitudes, confirming the approach in [1]."],"url":"http://arxiv.org/abs/2404.15551v1","category":"hep-ph"}
{"created":"2024-04-23 22:10:27","title":"Boundary determination and local rigidity of analytic metrics in the Lorentzian scattering rigidity problem","abstract":"We study the scattering rigidity problem in Lorentzian geometry: recovery of a Lorentzian metric from the scattering relation known on a lateral timelike boundary. We show that one can recover the jet of the metric up to a gauge transformation near a lightlike strictly convex point. Assuming that the metric is real analytic, we show that one can recover the metric up to a gauge transformation as well near such a point.","sentences":["We study the scattering rigidity problem in Lorentzian geometry: recovery of a Lorentzian metric from the scattering relation known on a lateral timelike boundary.","We show that one can recover the jet of the metric up to a gauge transformation near a lightlike strictly convex point.","Assuming that the metric is real analytic, we show that one can recover the metric up to a gauge transformation as well near such a point."],"url":"http://arxiv.org/abs/2404.15541v1","category":"math.DG"}
{"created":"2024-04-23 20:56:44","title":"Numerical study of transitions in lid-driven flows in semicircular cavities","abstract":"In this article, three-dimensional (3D) lid-driven flows in semicircular cavities are studied. The numerical solution of the Navier-Stokes equations modeling incompressible viscous fluid flow in cavities is obtained via a methodology combining a first-order accurate operator-splitting scheme, a fictitious domain formulation, and finite element space approximations. The critical Reynolds numbers (Re_{cr}) for having oscillatory flow (a Hopf bifurcation) are obtained. The associated oscillating motion in a semicircular cavity with length equal to width has been studied in detail. Based on the averaged velocity field in one period of oscillating motion, the flow difference (called oscillation mode) between the velocity field and averaged one at several time instances in such period shows almost the same flow pattern for the Reynolds numbers close to Re_{cr}. This oscillation mode in a semicircular cavity shows a close similarity to the one obtained in a shallow cavity, but with some difference in a shallow cavity which is triggered by the presence of two vertical side walls and downstream wall.","sentences":["In this article, three-dimensional (3D) lid-driven flows in semicircular cavities are studied.","The numerical solution of the Navier-Stokes equations modeling incompressible viscous fluid flow in cavities is obtained via a methodology combining a first-order accurate operator-splitting scheme, a fictitious domain formulation, and finite element space approximations.","The critical Reynolds numbers (Re_{cr}) for having oscillatory flow (a Hopf bifurcation) are obtained.","The associated oscillating motion in a semicircular cavity with length equal to width has been studied in detail.","Based on the averaged velocity field in one period of oscillating motion, the flow difference (called oscillation mode) between the velocity field and averaged one at several time instances in such period shows almost the same flow pattern for the Reynolds numbers close to Re_{cr}.","This oscillation mode in a semicircular cavity shows a close similarity to the one obtained in a shallow cavity, but with some difference in a shallow cavity which is triggered by the presence of two vertical side walls and downstream wall."],"url":"http://arxiv.org/abs/2404.15514v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 20:51:09","title":"NeuraChip: Accelerating GNN Computations with a Hash-based Decoupled Spatial Accelerator","abstract":"Graph Neural Networks (GNNs) are emerging as a formidable tool for processing non-euclidean data across various domains, ranging from social network analysis to bioinformatics. Despite their effectiveness, their adoption has not been pervasive because of scalability challenges associated with large-scale graph datasets, particularly when leveraging message passing.   To tackle these challenges, we introduce NeuraChip, a novel GNN spatial accelerator based on Gustavson's algorithm. NeuraChip decouples the multiplication and addition computations in sparse matrix multiplication. This separation allows for independent exploitation of their unique data dependencies, facilitating efficient resource allocation. We introduce a rolling eviction strategy to mitigate data idling in on-chip memory as well as address the prevalent issue of memory bloat in sparse graph computations. Furthermore, the compute resource load balancing is achieved through a dynamic reseeding hash-based mapping, ensuring uniform utilization of computing resources agnostic of sparsity patterns. Finally, we present NeuraSim, an open-source, cycle-accurate, multi-threaded, modular simulator for comprehensive performance analysis.   Overall, NeuraChip presents a significant improvement, yielding an average speedup of 22.1x over Intel's MKL, 17.1x over NVIDIA's cuSPARSE, 16.7x over AMD's hipSPARSE, and 1.5x over prior state-of-the-art SpGEMM accelerator and 1.3x over GNN accelerator. The source code for our open-sourced simulator and performance visualizer is publicly accessible on GitHub https://neurachip.us","sentences":["Graph Neural Networks (GNNs) are emerging as a formidable tool for processing non-euclidean data across various domains, ranging from social network analysis to bioinformatics.","Despite their effectiveness, their adoption has not been pervasive because of scalability challenges associated with large-scale graph datasets, particularly when leveraging message passing.   ","To tackle these challenges, we introduce NeuraChip, a novel GNN spatial accelerator based on Gustavson's algorithm.","NeuraChip decouples the multiplication and addition computations in sparse matrix multiplication.","This separation allows for independent exploitation of their unique data dependencies, facilitating efficient resource allocation.","We introduce a rolling eviction strategy to mitigate data idling in on-chip memory as well as address the prevalent issue of memory bloat in sparse graph computations.","Furthermore, the compute resource load balancing is achieved through a dynamic reseeding hash-based mapping, ensuring uniform utilization of computing resources agnostic of sparsity patterns.","Finally, we present NeuraSim, an open-source, cycle-accurate, multi-threaded, modular simulator for comprehensive performance analysis.   ","Overall, NeuraChip presents a significant improvement, yielding an average speedup of 22.1x over Intel's MKL, 17.1x over NVIDIA's cuSPARSE, 16.7x over AMD's hipSPARSE, and 1.5x over prior state-of-the-art SpGEMM accelerator and 1.3x over GNN accelerator.","The source code for our open-sourced simulator and performance visualizer is publicly accessible on GitHub https://neurachip.us"],"url":"http://arxiv.org/abs/2404.15510v1","category":"cs.AR"}
{"created":"2024-04-23 19:55:31","title":"Symmetry results for a nonlocal eigenvalue problem","abstract":"In this paper, we study the optimal constant in the nonlocal Poincar\\'e-Wirtinger inequality in $(a,b)\\subset\\mathbb R$: \\begin{equation*} \\lambda_\\alpha(p,q,r){\\left(\\int_{a}^{b}|u|^{q}dx\\right)^\\frac pq}\\le{\\int_{a}^{b}|u'|^{p}dx+\\alpha\\left|\\int_{a}^{b}|u|^{r-2}u\\, dx\\right|^{\\frac p{r-1}}}, \\end{equation*} where $\\alpha\\in\\mathbb R$, $p,q,r >1$ such that $\\frac 45 p\\le q\\le p$ and $\\frac q2 +1\\le r \\le q+\\frac q p$. This problem can be casted as a nonlocal minimum problem, whose Euler-Lagrange associated equation contains an integral term of the unknown function over the whole interval of definition. Furthermore, the problem can be also seen as an eigenvalue problem.   We show that there exists a critical value $\\alpha_C=\\alpha_C (p,q,r)$ such that the minimizers are even with constant sign when $\\alpha\\le\\alpha_{C}$ and are odd when $\\alpha\\geq \\alpha_{C}$.","sentences":["In this paper, we study the optimal constant in the nonlocal Poincar\\'e-Wirtinger inequality in $(a,b)\\subset\\mathbb R$: \\begin{equation*} \\lambda_\\alpha(p,q,r){\\left(\\int_{a}^{b}|u|^{q}dx\\right)^\\frac pq}\\le{\\int_{a}^{b}|u'|^{p}dx+\\alpha\\left|\\int_{a}^{b}|u|^{r-2}u\\, dx\\right|^{\\frac p{r-1}}}, \\end{equation*} where $\\alpha\\in\\mathbb R$, $p,q,r >1$ such that $\\frac 45 p\\le q\\le p$ and $\\frac q2 +1\\le r \\le q+\\frac","q p$.","This problem can be casted as a nonlocal minimum problem, whose Euler-Lagrange associated equation contains an integral term of the unknown function over the whole interval of definition.","Furthermore, the problem can be also seen as an eigenvalue problem.   ","We show that there exists a critical value $\\alpha_C=\\alpha_C (p,q,r)$ such that the minimizers are even with constant sign when $\\alpha\\le\\alpha_{C}$ and are odd when $\\alpha\\geq \\alpha_{C}$."],"url":"http://arxiv.org/abs/2404.15486v1","category":"math.AP"}
{"created":"2024-04-23 18:52:56","title":"Reducing polynomial degree by one for inner-stage operators affects neither stability nor accuracy of the Runge--Kutta discontinuous Galerkin method","abstract":"The Runge--Kutta (RK) discontinuous Galerkin (DG) method is a mainstream numerical algorithm for solving hyperbolic equations. In this paper, we use the linear advection equation in one and two dimensions as a model problem to prove the following results: For an arbitrarily high-order RKDG scheme in Butcher form, as long as we use the $P^k$ approximation in the final stage, even if we drop the $k$th-order polynomial modes and use the $P^{k-1}$ approximation for the DG operators at all inner RK stages, the resulting numerical method still maintains the same type of stability and convergence rate as those of the original RKDG method. Numerical examples are provided to validate the analysis. The numerical method analyzed in this paper is a special case of the Class A RKDG method with stage-dependent polynomial spaces proposed in arXiv:2402.15150. Our analysis provides theoretical justifications for employing cost-effective and low-order spatial discretization at specific RK stages for developing more efficient DG schemes without affecting stability and accuracy of the original method.","sentences":["The Runge--Kutta (RK) discontinuous Galerkin (DG) method is a mainstream numerical algorithm for solving hyperbolic equations.","In this paper, we use the linear advection equation in one and two dimensions as a model problem to prove the following results: For an arbitrarily high-order RKDG scheme in Butcher form, as long as we use the $P^k$ approximation in the final stage, even if we drop the $k$th-order polynomial modes and use the $P^{k-1}$ approximation for the DG operators at all inner RK stages, the resulting numerical method still maintains the same type of stability and convergence rate as those of the original RKDG method.","Numerical examples are provided to validate the analysis.","The numerical method analyzed in this paper is a special case of the Class A RKDG method with stage-dependent polynomial spaces proposed in arXiv:2402.15150.","Our analysis provides theoretical justifications for employing cost-effective and low-order spatial discretization at specific RK stages for developing more efficient DG schemes without affecting stability and accuracy of the original method."],"url":"http://arxiv.org/abs/2404.15453v1","category":"math.NA"}
{"created":"2024-04-23 18:19:03","title":"Exoplanet Mineralogy","abstract":"This chapter begins with some basic concepts regarding the structure and mineralogy of rocky planets, how to read and construct ternary diagrams, and why partial melting occurs when plate tectonics is operative. Partial melting is a key concept in that it governs crust and core formation, which in turn control mineralogy. These sections are for astronomers, or geologists new to the study of igneous petrology. From there, computational approaches for estimating planetary mineral assemblages will be introduced. These quantitative methods are simple, consonant with the level of information currently available on exoplanet compositions, and while largely intended for mineralogists, should be accessible to non-specialists as well. Such methods are followed by a study of error when plotting mineral abundances in ternary diagrams, for mineralogists and petrologists who construct such diagrams. The chapter concludes with caveats, and the ways in which exoplanets might surprise us.","sentences":["This chapter begins with some basic concepts regarding the structure and mineralogy of rocky planets, how to read and construct ternary diagrams, and why partial melting occurs when plate tectonics is operative.","Partial melting is a key concept in that it governs crust and core formation, which in turn control mineralogy.","These sections are for astronomers, or geologists new to the study of igneous petrology.","From there, computational approaches for estimating planetary mineral assemblages will be introduced.","These quantitative methods are simple, consonant with the level of information currently available on exoplanet compositions, and while largely intended for mineralogists, should be accessible to non-specialists as well.","Such methods are followed by a study of error when plotting mineral abundances in ternary diagrams, for mineralogists and petrologists who construct such diagrams.","The chapter concludes with caveats, and the ways in which exoplanets might surprise us."],"url":"http://arxiv.org/abs/2404.15426v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:18:30","title":"Host Stars and How Their Compositions Influence Exoplanets","abstract":"It has become a common practice within the exoplanet field to say that \"to know the star is to know the planet.\" The properties of the host star have a strong, direct influence on the interior and surface conditions of the orbiting planet and oftentimes measurements of planetary properties are made relative to the star's properties. Not only are observational measurements of the star necessary to determine even the most basic aspects of the planet (such as mass and radius), but the stellar environment influences how the planet evolves. Therefore, in this chapter, we begin by discussing the basics of stars, providing an overview of stellar formation, structure, photon and particle emissions, and evolution. Next, we go over the possible ways to determine the age of a star. We then outline how different kinds of stars are distributed within the Milky Way galaxy. Afterwards, we explain how to measure the composition of stars and the underlying math inherent to those observations, including caveats that are important when using the data for research applications. Finally, we explain the underlying physics and observations that enable stellar composition to be used as a proxy for planetary composition. In addition, given that this chapter focuses more on astronomy/astrophysics and uses a variety of important terms that may not be familiar to all readers, we have defined many terms either within the text or as a footnote for better interdisciplinary comprehension.","sentences":["It has become a common practice within the exoplanet field to say that \"to know the star is to know the planet.\"","The properties of the host star have a strong, direct influence on the interior and surface conditions of the orbiting planet and oftentimes measurements of planetary properties are made relative to the star's properties.","Not only are observational measurements of the star necessary to determine even the most basic aspects of the planet (such as mass and radius), but the stellar environment influences how the planet evolves.","Therefore, in this chapter, we begin by discussing the basics of stars, providing an overview of stellar formation, structure, photon and particle emissions, and evolution.","Next, we go over the possible ways to determine the age of a star.","We then outline how different kinds of stars are distributed within the Milky Way galaxy.","Afterwards, we explain how to measure the composition of stars and the underlying math inherent to those observations, including caveats that are important when using the data for research applications.","Finally, we explain the underlying physics and observations that enable stellar composition to be used as a proxy for planetary composition.","In addition, given that this chapter focuses more on astronomy/astrophysics and uses a variety of important terms that may not be familiar to all readers, we have defined many terms either within the text or as a footnote for better interdisciplinary comprehension."],"url":"http://arxiv.org/abs/2404.15422v1","category":"astro-ph.EP"}
{"created":"2024-04-23 18:00:38","title":"Insufficient Statistics Perturbation: Stable Estimators for Private Least Squares","abstract":"We present a sample- and time-efficient differentially private algorithm for ordinary least squares, with error that depends linearly on the dimension and is independent of the condition number of $X^\\top X$, where $X$ is the design matrix. All prior private algorithms for this task require either $d^{3/2}$ examples, error growing polynomially with the condition number, or exponential time. Our near-optimal accuracy guarantee holds for any dataset with bounded statistical leverage and bounded residuals. Technically, we build on the approach of Brown et al. (2023) for private mean estimation, adding scaled noise to a carefully designed stable nonprivate estimator of the empirical regression vector.","sentences":["We present a sample- and time-efficient differentially private algorithm for ordinary least squares, with error that depends linearly on the dimension and is independent of the condition number of $X^\\top X$, where $X$ is the design matrix.","All prior private algorithms for this task require either $d^{3/2}$ examples, error growing polynomially with the condition number, or exponential time.","Our near-optimal accuracy guarantee holds for any dataset with bounded statistical leverage and bounded residuals.","Technically, we build on the approach of Brown et al. (2023) for private mean estimation, adding scaled noise to a carefully designed stable nonprivate estimator of the empirical regression vector."],"url":"http://arxiv.org/abs/2404.15409v1","category":"cs.LG"}
{"created":"2024-04-23 18:00:03","title":"Photometry of Saturated Stars with Machine Learning","abstract":"We develop a deep neural network (DNN) to obtain photometry of saturated stars in the All-Sky Automated Survey for Supernovae (ASAS-SN). The DNN can obtain unbiased photometry for stars from g=4 to 14 mag with a dispersion (15%-85% 1sigma range around median) of 0.12 mag for saturated (g<11.5 mag) stars. More importantly, the light curve of a non-variable saturated star has a median dispersion of only 0.037 mag. The DNN light curves are, in many cases, spectacularly better than provided by the standard ASAS-SN pipelines. While the network was trained on g band data from only one of ASAS-SN's 20 cameras, initial experiments suggest that it can be used for any camera and the older ASAS-SN V band data as well. The dominant problems seem to be associated with correctable issues in the ASAS-SN data reduction pipeline for saturated stars more than the DNN itself. The method is publicly available as a light curve option on ASAS-SN Sky Patrol v1.0.","sentences":["We develop a deep neural network (DNN) to obtain photometry of saturated stars in the All-Sky Automated Survey for Supernovae (ASAS-SN).","The DNN can obtain unbiased photometry for stars from g=4 to 14 mag with a dispersion (15%-85% 1sigma range around median) of 0.12 mag for saturated (g<11.5 mag) stars.","More importantly, the light curve of a non-variable saturated star has a median dispersion of only 0.037 mag.","The DNN light curves are, in many cases, spectacularly better than provided by the standard ASAS-SN pipelines.","While the network was trained on g band data from only one of ASAS-SN's 20 cameras, initial experiments suggest that it can be used for any camera and the older ASAS-SN V band data as well.","The dominant problems seem to be associated with correctable issues in the ASAS-SN data reduction pipeline for saturated stars more than the DNN itself.","The method is publicly available as a light curve option on ASAS-SN Sky Patrol v1.0."],"url":"http://arxiv.org/abs/2404.15405v1","category":"astro-ph.SR"}
{"created":"2024-04-23 01:35:07","title":"Fourier Series Guided Design of Quantum Convolutional Neural Networks for Enhanced Time Series Forecasting","abstract":"In this study, we apply 1D quantum convolution to address the task of time series forecasting. By encoding multiple points into the quantum circuit to predict subsequent data, each point becomes a feature, transforming the problem into a multidimensional one. Building on theoretical foundations from prior research, which demonstrated that Variational Quantum Circuits (VQCs) can be expressed as multidimensional Fourier series, we explore the capabilities of different architectures and ansatz. This analysis considers the concepts of circuit expressibility and the presence of barren plateaus. Analyzing the problem within the framework of the Fourier series enabled the design of an architecture that incorporates data reuploading, resulting in enhanced performance. Rather than a strict requirement for the number of free parameters to exceed the degrees of freedom of the Fourier series, our findings suggest that even a limited number of parameters can produce Fourier functions of higher degrees. This highlights the remarkable expressive power of quantum circuits. This observation is also significant in reducing training times. The ansatz with greater expressibility and number of non-zero Fourier coefficients consistently delivers favorable results across different scenarios, with performance metrics improving as the number of qubits increases.","sentences":["In this study, we apply 1D quantum convolution to address the task of time series forecasting.","By encoding multiple points into the quantum circuit to predict subsequent data, each point becomes a feature, transforming the problem into a multidimensional one.","Building on theoretical foundations from prior research, which demonstrated that Variational Quantum Circuits (VQCs) can be expressed as multidimensional Fourier series, we explore the capabilities of different architectures and ansatz.","This analysis considers the concepts of circuit expressibility and the presence of barren plateaus.","Analyzing the problem within the framework of the Fourier series enabled the design of an architecture that incorporates data reuploading, resulting in enhanced performance.","Rather than a strict requirement for the number of free parameters to exceed the degrees of freedom of the Fourier series, our findings suggest that even a limited number of parameters can produce Fourier functions of higher degrees.","This highlights the remarkable expressive power of quantum circuits.","This observation is also significant in reducing training times.","The ansatz with greater expressibility and number of non-zero Fourier coefficients consistently delivers favorable results across different scenarios, with performance metrics improving as the number of qubits increases."],"url":"http://arxiv.org/abs/2404.15377v1","category":"quant-ph"}
{"created":"2024-04-22 17:59:24","title":"A covariant formulation for cosmological radiative transfer of the 21-cm line","abstract":"The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation. In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion. These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium. We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation. The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer. Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data.","sentences":["The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation.","In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion.","These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium.","We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation.","The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer.","Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data."],"url":"http://arxiv.org/abs/2404.14407v1","category":"astro-ph.CO"}
{"created":"2024-04-24 03:27:02","title":"An Electromagnetism-Inspired Method for Estimating In-Grasp Torque from Visuotactile Sensors","abstract":"Tactile sensing has become a popular sensing modality for robot manipulators, due to the promise of providing robots with the ability to measure the rich contact information that gets transmitted through its sense of touch. Among the diverse range of information accessible from tactile sensors, torques transmitted from the grasped object to the fingers through extrinsic environmental contact may be particularly important for tasks such as object insertion. However, tactile torque estimation has received relatively little attention when compared to other sensing modalities, such as force, texture, or slip identification. In this work, we introduce the notion of the Tactile Dipole Moment, which we use to estimate tilt torques from gel-based visuotactile sensors. This method does not rely on deep learning, sensor-specific mechanical, or optical modeling, and instead takes inspiration from electromechanics to analyze the vector field produced from 2D marker displacements. Despite the simplicity of our technique, we demonstrate its ability to provide accurate torque readings over two different tactile sensors and three object geometries, and highlight its practicality for the task of USB stick insertion with a compliant robot arm. These results suggest that simple analytical calculations based on dipole moments can sufficiently extract physical quantities from visuotactile sensors.","sentences":["Tactile sensing has become a popular sensing modality for robot manipulators, due to the promise of providing robots with the ability to measure the rich contact information that gets transmitted through its sense of touch.","Among the diverse range of information accessible from tactile sensors, torques transmitted from the grasped object to the fingers through extrinsic environmental contact may be particularly important for tasks such as object insertion.","However, tactile torque estimation has received relatively little attention when compared to other sensing modalities, such as force, texture, or slip identification.","In this work, we introduce the notion of the Tactile Dipole Moment, which we use to estimate tilt torques from gel-based visuotactile sensors.","This method does not rely on deep learning, sensor-specific mechanical, or optical modeling, and instead takes inspiration from electromechanics to analyze the vector field produced from 2D marker displacements.","Despite the simplicity of our technique, we demonstrate its ability to provide accurate torque readings over two different tactile sensors and three object geometries, and highlight its practicality for the task of USB stick insertion with a compliant robot arm.","These results suggest that simple analytical calculations based on dipole moments can sufficiently extract physical quantities from visuotactile sensors."],"url":"http://arxiv.org/abs/2404.15626v1","category":"cs.RO"}
{"created":"2024-04-24 01:14:33","title":"MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image Analysis","abstract":"The Vision Transformer (ViT) has demonstrated remarkable performance in Self-Supervised Learning (SSL) for 3D medical image analysis. Mask AutoEncoder (MAE) for feature pre-training can further unleash the potential of ViT on various medical vision tasks. However, due to large spatial sizes with much higher dimensions of 3D medical images, the lack of hierarchical design for MAE may hinder the performance of downstream tasks. In this paper, we propose a novel \\textit{Mask in Mask (MiM)} pre-training framework for 3D medical images, which aims to advance MAE by learning discriminative representation from hierarchical visual tokens across varying scales. We introduce multiple levels of granularity for masked inputs from the volume, which are then reconstructed simultaneously ranging at both fine and coarse levels. Additionally, a cross-level alignment mechanism is applied to adjacent level volumes to enforce anatomical similarity hierarchically. Furthermore, we adopt a hybrid backbone to enhance the hierarchical representation learning efficiently during the pre-training. MiM was pre-trained on a large scale of available 3D volumetric images, \\textit{i.e.,} Computed Tomography (CT) images containing various body parts. Extensive experiments on thirteen public datasets demonstrate the superiority of MiM over other SSL methods in organ/lesion/tumor segmentation and disease classification. We further scale up the MiM to large pre-training datasets with more than 10k volumes, showing that large-scale pre-training can further enhance the performance of downstream tasks. The improvement also concluded that the research community should pay more attention to the scale of the pre-training dataset towards the healthcare foundation model for 3D medical images.","sentences":["The Vision Transformer (ViT) has demonstrated remarkable performance in Self-Supervised Learning (SSL) for 3D medical image analysis.","Mask AutoEncoder (MAE) for feature pre-training can further unleash the potential of ViT on various medical vision tasks.","However, due to large spatial sizes with much higher dimensions of 3D medical images, the lack of hierarchical design for MAE may hinder the performance of downstream tasks.","In this paper, we propose a novel \\textit{Mask in Mask (MiM)} pre-training framework for 3D medical images, which aims to advance MAE by learning discriminative representation from hierarchical visual tokens across varying scales.","We introduce multiple levels of granularity for masked inputs from the volume, which are then reconstructed simultaneously ranging at both fine and coarse levels.","Additionally, a cross-level alignment mechanism is applied to adjacent level volumes to enforce anatomical similarity hierarchically.","Furthermore, we adopt a hybrid backbone to enhance the hierarchical representation learning efficiently during the pre-training.","MiM was pre-trained on a large scale of available 3D volumetric images, \\textit{i.e.,} Computed Tomography (CT) images containing various body parts.","Extensive experiments on thirteen public datasets demonstrate the superiority of MiM over other SSL methods in organ/lesion/tumor segmentation and disease classification.","We further scale up the MiM to large pre-training datasets with more than 10k volumes, showing that large-scale pre-training can further enhance the performance of downstream tasks.","The improvement also concluded that the research community should pay more attention to the scale of the pre-training dataset towards the healthcare foundation model for 3D medical images."],"url":"http://arxiv.org/abs/2404.15580v1","category":"cs.CV"}
{"created":"2024-04-23 21:18:49","title":"NGC1856: Using machine learning techniques to uncover detailed stellar abundances from MUSE data","abstract":"We present the first application of the novel approach based on data-driven machine learning methods applied to \\textit{Multi-Unit Spectroscopic Explorer} (MUSE) field data to derive stellar abundances of star clusters. MUSE has been used to target more than 10,000 fields, and it is unique in its ability to study dense stellar fields such as stellar clusters providing spectra for each individual star. We use MUSE data of the extragalactic young stellar cluster NGC 1856, located in the Large Magellanic Cloud (LMC). We present the individual stellar [Fe/H] abundance of 327 cluster members in addition to [Mg/Fe], [Si/Fe], [Ti/Fe], [C/Fe], [Ni/Fe], and [Cr/Fe] abundances of subsample sets. Our results match the LMC abundances obtained in the literature for [Mg/Fe], [Ti/Fe], [Ni/Fe], and [Cr/Fe]. This study is the first to derive [Si/Fe] and [C/Fe] abundances for this cluster. The revolutionary combination of integral-field spectroscopy and data-driven modeling will allow us to understand the chemical enrichment of star clusters and their host galaxies in greater detail expanding our understanding of galaxy evolution.","sentences":["We present the first application of the novel approach based on data-driven machine learning methods applied to \\textit{Multi-Unit Spectroscopic Explorer} (MUSE) field data to derive stellar abundances of star clusters.","MUSE has been used to target more than 10,000 fields, and it is unique in its ability to study dense stellar fields such as stellar clusters providing spectra for each individual star.","We use MUSE data of the extragalactic young stellar cluster NGC 1856, located in the Large Magellanic Cloud (LMC).","We present the individual stellar [Fe/H] abundance of 327 cluster members in addition to [Mg/Fe], [Si/Fe], [Ti/Fe], [C/Fe], [Ni/Fe], and [Cr/Fe] abundances of subsample sets.","Our results match the LMC abundances obtained in the literature for [Mg/Fe], [Ti/Fe], [Ni/Fe], and [Cr/Fe].","This study is the first to derive [Si/Fe] and [C/Fe] abundances for this cluster.","The revolutionary combination of integral-field spectroscopy and data-driven modeling will allow us to understand the chemical enrichment of star clusters and their host galaxies in greater detail expanding our understanding of galaxy evolution."],"url":"http://arxiv.org/abs/2404.15527v1","category":"astro-ph.GA"}
{"created":"2024-04-23 19:15:21","title":"Thermal boundary conductance of sharp metal-diamond interfaces predicted by machine learning molecular dynamics","abstract":"Thermal transport across sharp metal-diamond interfaces plays a critical role in the thermal management of future diamond-based ultrawide bandgap semiconductor devices. However, experimental thermal boundary conductance (TBC) values are mostly nonexistent, and current theoretical models are inaccurate in predicting the TBCs since accurate interatomic potentials of metal-diamond heterostructures are unavailable. In this letter, we show the prediction of TBCs of several practically promising sharp metal-diamond interfaces using nonequilibrium molecular dynamics (NEMD) simulations by developing accurate machine learning interatomic potentials (MLIPs). The predicted TBCs of Al, Mo, Zr, and Au-diamond interfaces are approximately 316, 88, 52, and 55 MW/m2K, respectively, after quantum corrections. The corresponding thermal boundary resistances (TBRs) are equivalent to 0.75-{\\mu}m thick of Al, 1.38-{\\mu}m Mo, 0.30-{\\mu}m Zr and 5.28-{\\mu}m Au, respectively. These low TBC values need to be considered in future diamond-based semiconductor designs. We also find that, the conventional simple models such as the acoustic mismatch model (AMM) and diffuse mismatch model (DMM), even including the full band phonon dispersion from first principles, largely mispredict the TBC because they do not include inelastic transmission as well as interfacial structural and bonding information. The quantum correction of TBC matches well with the quantum correction of phonon specific heat of metals, instead of diamond. Additionally, we reveal that the Debye temperature ratio is a better indicator of TBC than the elastic modulus ratio.","sentences":["Thermal transport across sharp metal-diamond interfaces plays a critical role in the thermal management of future diamond-based ultrawide bandgap semiconductor devices.","However, experimental thermal boundary conductance (TBC) values are mostly nonexistent, and current theoretical models are inaccurate in predicting the TBCs since accurate interatomic potentials of metal-diamond heterostructures are unavailable.","In this letter, we show the prediction of TBCs of several practically promising sharp metal-diamond interfaces using nonequilibrium molecular dynamics (NEMD) simulations by developing accurate machine learning interatomic potentials (MLIPs).","The predicted TBCs of Al, Mo, Zr, and Au-diamond interfaces are approximately 316, 88, 52, and 55 MW/m2K, respectively, after quantum corrections.","The corresponding thermal boundary resistances (TBRs) are equivalent to 0.75-{\\mu}m thick of Al, 1.38-{\\mu}m Mo, 0.30-{\\mu}m Zr and 5.28-{\\mu}m Au, respectively.","These low TBC values need to be considered in future diamond-based semiconductor designs.","We also find that, the conventional simple models such as the acoustic mismatch model (AMM) and diffuse mismatch model (DMM), even including the full band phonon dispersion from first principles, largely mispredict the TBC because they do not include inelastic transmission as well as interfacial structural and bonding information.","The quantum correction of TBC matches well with the quantum correction of phonon specific heat of metals, instead of diamond.","Additionally, we reveal that the Debye temperature ratio is a better indicator of TBC than the elastic modulus ratio."],"url":"http://arxiv.org/abs/2404.15465v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 02:41:10","title":"Decentralized Multi-Agent Trajectory Planning in Dynamic Environments with Spatiotemporal Occupancy Grid Maps","abstract":"This paper proposes a decentralized trajectory planning framework for the collision avoidance problem of multiple micro aerial vehicles (MAVs) in environments with static and dynamic obstacles. The framework utilizes spatiotemporal occupancy grid maps (SOGM), which forecast the occupancy status of neighboring space in the near future, as the environment representation. Based on this representation, we extend the kinodynamic A* and the corridor-constrained trajectory optimization algorithms to efficiently tackle static and dynamic obstacles with arbitrary shapes. Collision avoidance between communicating robots is integrated by sharing planned trajectories and projecting them onto the SOGM. The simulation results show that our method achieves competitive performance against state-of-the-art methods in dynamic environments with different numbers and shapes of obstacles. Finally, the proposed method is validated in real experiments.","sentences":["This paper proposes a decentralized trajectory planning framework for the collision avoidance problem of multiple micro aerial vehicles (MAVs) in environments with static and dynamic obstacles.","The framework utilizes spatiotemporal occupancy grid maps (SOGM), which forecast the occupancy status of neighboring space in the near future, as the environment representation.","Based on this representation, we extend the kinodynamic A* and the corridor-constrained trajectory optimization algorithms to efficiently tackle static and dynamic obstacles with arbitrary shapes.","Collision avoidance between communicating robots is integrated by sharing planned trajectories and projecting them onto the SOGM.","The simulation results show that our method achieves competitive performance against state-of-the-art methods in dynamic environments with different numbers and shapes of obstacles.","Finally, the proposed method is validated in real experiments."],"url":"http://arxiv.org/abs/2404.15602v1","category":"cs.RO"}
{"created":"2024-04-23 21:48:45","title":"Optimal sizing of 1D vibrating columns accounting for axial compression and self-weight","abstract":"We investigate the effect of axial compression on the optimal design of columns, for the maximization of the fundamental vibration frequency. The compression may be due to a force at the columns' tip or to a load distributed along its axis, which may act either independently or simultaneously. We discuss the influence of these contributions on the optimality conditions, and show how the optimal beam design, and the corresponding frequency gain drastically change with the level of compression. We also discuss the indirect effect of frequency optimization on the critical load factors for the tip ($\\lambda_{P}$) and distributed ($\\lambda_{Q}$) loads. Finally, we provide some quantitative results for the optimal design problem parametrized by the triple ($\\lambda_{P}$, $\\lambda_{Q}$, $\\Omega^{2}$) of buckling and dynamic eigenvalues.","sentences":["We investigate the effect of axial compression on the optimal design of columns, for the maximization of the fundamental vibration frequency.","The compression may be due to a force at the columns' tip or to a load distributed along its axis, which may act either independently or simultaneously.","We discuss the influence of these contributions on the optimality conditions, and show how the optimal beam design, and the corresponding frequency gain drastically change with the level of compression.","We also discuss the indirect effect of frequency optimization on the critical load factors for the tip ($\\lambda_{P}$) and distributed ($\\lambda_{Q}$) loads.","Finally, we provide some quantitative results for the optimal design problem parametrized by the triple ($\\lambda_{P}$, $\\lambda_{Q}$, $\\Omega^{2}$) of buckling and dynamic eigenvalues."],"url":"http://arxiv.org/abs/2404.15536v1","category":"physics.class-ph"}
{"created":"2024-04-22 20:21:28","title":"On planar (110) channeling of 855 MeV electrons in a boron-doped diamond undulator","abstract":"A 4-period diamond undulator with a thickness of 20 $\\mu$m was produced with the method of Chemical Vapour Deposition (CVC), applying boron doping, on a straight diamond crystal with an effective thickness of 165.5 $\\mu$m. A planar (110) channeling experiment, performed with the high quality 855 MeV electron beam of the Mainz Microtron MAMI accelerator facility, failed to observe the expected undulator peak. Simulation calculations which are based on the continuum potential picture revealed unexpected results for radiation spectra at the chosen observation direction. They suggest, in addition, that at an optimized observation angle, for which the undulator peak is the strongest, the channeling radiation from the rather thick backing crystal can be significantly suppressed. A byproduct of this case study was the experimental observation that beam deflection can be achieved even with a flat 50 $\\mu$m thick diamond crystal.","sentences":["A 4-period diamond undulator with a thickness of 20 $\\mu$m was produced with the method of Chemical Vapour Deposition (CVC), applying boron doping, on a straight diamond crystal with an effective thickness of 165.5 $\\mu$m.","A planar (110) channeling experiment, performed with the high quality 855 MeV electron beam of the Mainz Microtron MAMI accelerator facility, failed to observe the expected undulator peak.","Simulation calculations which are based on the continuum potential picture revealed unexpected results for radiation spectra at the chosen observation direction.","They suggest, in addition, that at an optimized observation angle, for which the undulator peak is the strongest, the channeling radiation from the rather thick backing crystal can be significantly suppressed.","A byproduct of this case study was the experimental observation that beam deflection can be achieved even with a flat 50 $\\mu$m thick diamond crystal."],"url":"http://arxiv.org/abs/2404.15376v1","category":"physics.acc-ph"}
{"created":"2024-04-22 17:59:50","title":"Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses","abstract":"In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.","sentences":["In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input.","As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together.","In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world.","To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances.","We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations.","After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human.","Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches."],"url":"http://arxiv.org/abs/2404.14410v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:07","title":"Learning H-Infinity Locomotion Control","abstract":"Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances. However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities. In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint. In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration. To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces. Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances. We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot. The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice. On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains. All code, checkpoints, and real-world deployment guidance will be made public.","sentences":["Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances.","However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities.","In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint.","In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration.","To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces.","Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances.","We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot.","The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice.","On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains.","All code, checkpoints, and real-world deployment guidance will be made public."],"url":"http://arxiv.org/abs/2404.14405v1","category":"cs.RO"}
{"created":"2024-04-22 17:58:36","title":"GeoDiffuser: Geometry-Based Image Editing with Diffusion Models","abstract":"The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information.","sentences":["The success of image generative models has enabled us to build methods that can edit images based on text or other user input.","However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits.","We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method.","Our key insight is to view image editing operations as geometric transformations.","We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations.","Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows.","It also inpaints disoccluded parts of the image where the object was originally located.","Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing.","GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal.","We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods.","Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information."],"url":"http://arxiv.org/abs/2404.14403v1","category":"cs.CV"}
{"created":"2024-04-22 17:58:29","title":"On the convergence rates of discrete solutions to the Wave Kinetic Equation","abstract":"In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE). This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation. Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times. Moreover, there is convergence to a single Dirac mass at long times. The first goal of this paper is to give quantitative rates for the speed of said convergence. In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions.","sentences":["In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE).","This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation.","Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times.","Moreover, there is convergence to a single Dirac mass at long times.","The first goal of this paper is to give quantitative rates for the speed of said convergence.","In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions."],"url":"http://arxiv.org/abs/2404.14400v1","category":"math.AP"}
{"created":"2024-04-22 17:58:13","title":"MLQAOA: Graph Learning Accelerated Hybrid Quantum-Classical Multilevel QAOA","abstract":"Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches. We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms. The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA. We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility: Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}","sentences":["Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches.","We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms.","The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA.","We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility:","Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}"],"url":"http://arxiv.org/abs/2404.14399v1","category":"quant-ph"}
{"created":"2024-04-22 17:50:27","title":"Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction","abstract":"Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data. This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems. Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated. In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge. We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension. Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines.","sentences":["Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data.","This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems.","Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated.","In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge.","We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension.","Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines."],"url":"http://arxiv.org/abs/2404.14389v1","category":"cs.NI"}
{"created":"2024-04-22 17:46:29","title":"STROOBnet Optimization via GPU-Accelerated Proximal Recurrence Strategies","abstract":"Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors. This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring. Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances. Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach. It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage.","sentences":["Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors.","This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring.","Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances.","Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach.","It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage."],"url":"http://arxiv.org/abs/2404.14388v1","category":"cs.LG"}
{"created":"2024-04-22 17:37:17","title":"A New Optimization Model for Multiple-Control Toffoli Quantum Circuit Design","abstract":"As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research. This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing. While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently. This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used. Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks. Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees.","sentences":["As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research.","This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing.","While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently.","This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used.","Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks.","Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees."],"url":"http://arxiv.org/abs/2404.14384v1","category":"math.OC"}
{"created":"2024-04-22 17:37:04","title":"Observational characterisation of large-scale transport and horizontal turbulent diffusivity in the quiet Sun","abstract":"The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection. This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields. Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle. Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc. We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface. These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion. We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed. This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations. Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems.","sentences":["The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection.","This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields.","Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle.","Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc.","We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface.","These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion.","We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed.","This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations.","Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems."],"url":"http://arxiv.org/abs/2404.14383v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:36:34","title":"A unified theory of tunneling times promoted by Ramsey clocks","abstract":"What time does a clock tell after quantum tunneling? Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time. Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories. Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities. Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling.","sentences":["What time does a clock tell after quantum tunneling?","Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time.","Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories.","Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities.","Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling."],"url":"http://arxiv.org/abs/2404.14382v1","category":"quant-ph"}
{"created":"2024-04-22 17:21:39","title":"Analysing the interaction of expansion decisions by end customers and grid development in the context of a municipal energy system","abstract":"In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector. Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators. These measures thus change both the load and the generation of the future energy supply concept. Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary. The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions. The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation. To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion. As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated.","sentences":["In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector.","Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators.","These measures thus change both the load and the generation of the future energy supply concept.","Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary.","The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions.","The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation.","To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion.","As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated."],"url":"http://arxiv.org/abs/2404.14371v1","category":"eess.SY"}
{"created":"2024-04-22 17:12:58","title":"A General Continuous-Time Formulation of Stochastic ADMM and Its Variants","abstract":"Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems. In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis. The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM. Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise. Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2.","sentences":["Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems.","In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis.","The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM.","Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise.","Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2."],"url":"http://arxiv.org/abs/2404.14358v1","category":"math.OC"}
{"created":"2024-04-22 17:12:06","title":"A Stochastic Geo-spatiotemporal Bipartite Network to Optimize GCOOS Sensor Placement Strategies","abstract":"This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness. The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes. The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements. This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes. In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico. This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes. The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model.","sentences":["This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness.","The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes.","The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements.","This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes.","In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico.","This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes.","The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model."],"url":"http://arxiv.org/abs/2404.14357v1","category":"cs.MA"}
{"created":"2024-04-22 17:00:48","title":"Operando Analysis of Adsorption-Limited Hydrogen Oxidation Reaction at Palladium Surfaces","abstract":"Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions. This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates. Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive. This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst. Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy. The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution. Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced. Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors. These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies. The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions.","sentences":["Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions.","This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates.","Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive.","This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst.","Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy.","The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution.","Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced.","Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors.","These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies.","The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions."],"url":"http://arxiv.org/abs/2404.14348v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 16:56:39","title":"Managing Expectations and Imbalanced Training Data in Reactive Force Field Development: an Application to Water Adsorption on Alumina","abstract":"ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems. When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce. ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others. To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set. One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set. This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure. The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters. Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed. The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina. This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training. We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model.","sentences":["ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems.","When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce.","ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others.","To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set.","One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set.","This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure.","The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters.","Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed.","The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina.","This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training.","We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model."],"url":"http://arxiv.org/abs/2404.14338v1","category":"physics.chem-ph"}
{"created":"2024-04-22 16:30:03","title":"Multi-Agent Hybrid SAC for Joint SS-DSA in CRNs","abstract":"Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs). In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network. However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate. In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power. Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing. In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme. Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin. We also explore the impact of wireless variations such as coherence time on the efficacy of the system.","sentences":["Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs).","In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network.","However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate.","In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power.","Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing.","In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme.","Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin.","We also explore the impact of wireless variations such as coherence time on the efficacy of the system."],"url":"http://arxiv.org/abs/2404.14319v1","category":"eess.SY"}
{"created":"2024-04-22 16:29:26","title":"Meta-GGAs vs. Hybrid Functionals for Point Defects: The Best of Both Worlds Applied to Layered MnO$_2$, NiO$_2$ and KCoO$_2$","abstract":"Defects in a material can significantly tune its properties and enhance its utility. Hybrid functionals like HSE06 are often used to describe solids with defects. However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study. The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides. We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state. The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0. K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion.","sentences":["Defects in a material can significantly tune its properties and enhance its utility.","Hybrid functionals like HSE06 are often used to describe solids with defects.","However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study.","The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides.","We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state.","The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0.","K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion."],"url":"http://arxiv.org/abs/2404.14317v1","category":"physics.comp-ph"}
{"created":"2024-04-22 16:16:06","title":"Structure-preserving neural networks for the regularzied entropy-based closure of the Boltzmann moment system","abstract":"The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations. In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time. We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures. The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure. We conduct a numerical analysis of this approximation and investigate optimal parameter choices. Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy. The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}.","sentences":["The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations.","In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time.","We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures.","The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure.","We conduct a numerical analysis of this approximation and investigate optimal parameter choices.","Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy.","The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}."],"url":"http://arxiv.org/abs/2404.14312v1","category":"math.NA"}
{"created":"2024-04-22 16:10:38","title":"Towards Better Adversarial Purification via Adversarial Denoising Diffusion Training","abstract":"Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks. However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support. We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness. To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness. Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations. To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution. Empirical results show that ADDT improves the robustness of DBP models. Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations.","sentences":["Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks.","However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support.","We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness.","To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness.","Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations.","To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT).","This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution.","Empirical results show that ADDT improves the robustness of DBP models.","Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations."],"url":"http://arxiv.org/abs/2404.14309v1","category":"cs.CV"}
{"created":"2024-04-22 16:08:52","title":"One Trillion True Random Bits Generated with a Field Programmable Gate Array Actuated Magnetic Tunnel Junction","abstract":"Large quantities of random numbers are crucial in a wide range of applications. We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses. However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates. Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs (SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation. In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ. The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography.","sentences":["Large quantities of random numbers are crucial in a wide range of applications.","We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses.","However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates.","Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs","(SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation.","In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ.","The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography."],"url":"http://arxiv.org/abs/2404.14307v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 16:00:24","title":"Linear Search for an Escaping Target with Unknown Speed","abstract":"We consider linear search for an escaping target whose speed and initial position are unknown to the searcher. A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line. An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction. We consider two cases, depending on whether $d$ is known or unknown. The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings. This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown. Our results solve an open problem proposed in [Coleman et al., Proc. OPODIS 2022].","sentences":["We consider linear search for an escaping target whose speed and initial position are unknown to the searcher.","A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line.","An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction.","We consider two cases, depending on whether $d$ is known or unknown.","The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings.","This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown.","Our results solve an open problem proposed in [Coleman et al., Proc.","OPODIS 2022]."],"url":"http://arxiv.org/abs/2404.14300v2","category":"cs.DM"}
{"created":"2024-04-22 15:44:33","title":"Comparison of h-BN and graphene layers as grain boundary materials for granular FePt-$\\text{L}1_0$ thin films","abstract":"Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media. While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN. The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C. The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe. However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries. Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media.","sentences":["Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media.","While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN.","The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C.","The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe.","However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries.","Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media."],"url":"http://arxiv.org/abs/2404.14290v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 15:44:08","title":"Laser-synthesized TiN nanoparticles as novel efficient sorbent for environmental water cleaning","abstract":"Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems. Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection. In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect. TiN nanoparticles were synthesized by laser ablation method in liquid. Water, acetone and acetonitrile are used as solvent. Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x). TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles. Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states. The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g.","sentences":["Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems.","Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.","TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection.","In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect.","TiN nanoparticles were synthesized by laser ablation method in liquid.","Water, acetone and acetonitrile are used as solvent.","Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x).","TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles.","Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states.","The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g."],"url":"http://arxiv.org/abs/2404.14289v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 23:27:43","title":"On the existence of thermodynamically stable asymptotically flat black holes","abstract":"We use the quasilocal formalism of Brown and York, supplemented with counterterms, to investigate the thermodynamics of asymptotically flat black holes. We consider two families of exact regular black hole solutions, which are thermodynamically stable. The first one consists of four-dimensional static charged hairy black holes in extended supergravity. The second family consists of five-dimensional static charged black holes in Gauss-Bonnet (GB) gravity. Despite the fact that their characteristics are completely different, we found a striking similarity between their thermodynamic behaviour.","sentences":["We use the quasilocal formalism of Brown and York, supplemented with counterterms, to investigate the thermodynamics of asymptotically flat black holes.","We consider two families of exact regular black hole solutions, which are thermodynamically stable.","The first one consists of four-dimensional static charged hairy black holes in extended supergravity.","The second family consists of five-dimensional static charged black holes in Gauss-Bonnet (GB) gravity.","Despite the fact that their characteristics are completely different, we found a striking similarity between their thermodynamic behaviour."],"url":"http://arxiv.org/abs/2404.15566v1","category":"hep-th"}
{"created":"2024-04-23 21:08:42","title":"A Ramsey Neutron-Beam Experiment to Search for Ultralight Axion Dark Matter at the ESS","abstract":"High-intensity neutron beams, such as those available at the European Spallation Source (ESS), provide new opportunities for fundamental discoveries. Here we discuss a novel Ramsey neutron-beam experiment to search for ultralight axion dark matter through its coupling to neutron spins, which would cause the neutron spins to rotate about the velocity of the neutrons relative to the dark matter halo. We estimate that experiments at the HIBEAM beamline at the ESS can improve the sensitivity to the axion-neutron coupling compared to the current best laboratory limits by up to $2-3$ orders of magnitude over the axion mass range $10^{-22} \\, \\textrm{eV} - 10^{-16}$\\,eV.","sentences":["High-intensity neutron beams, such as those available at the European Spallation Source (ESS), provide new opportunities for fundamental discoveries.","Here we discuss a novel Ramsey neutron-beam experiment to search for ultralight axion dark matter through its coupling to neutron spins, which would cause the neutron spins to rotate about the velocity of the neutrons relative to the dark matter halo.","We estimate that experiments at the HIBEAM beamline at the ESS can improve the sensitivity to the axion-neutron coupling compared to the current best laboratory limits by up to $2-3$ orders of magnitude over the axion mass range $10^{-22} \\, \\textrm{eV} - 10^{-16}$\\,eV."],"url":"http://arxiv.org/abs/2404.15521v1","category":"hep-ph"}
{"created":"2024-04-23 19:48:57","title":"Self-similarity and recurrence in stability spectra of near-extreme Stokes waves","abstract":"We consider steady surface waves in an infinitely deep two--dimensional ideal fluid with potential flow, focusing on high-amplitude waves near the steepest wave with a 120 degree corner at the crest. The stability of these solutions with respect to coperiodic and subharmonic perturbations is studied, using new matrix-free numerical methods. We provide evidence for a plethora of conjectures on the nature of the instabilities as the steepest wave is approached, especially with regards to the self-similar recurrence of the stability spectrum near the origin of the spectral plane.","sentences":["We consider steady surface waves in an infinitely deep two--dimensional ideal fluid with potential flow, focusing on high-amplitude waves near the steepest wave with a 120 degree corner at the crest.","The stability of these solutions with respect to coperiodic and subharmonic perturbations is studied, using new matrix-free numerical methods.","We provide evidence for a plethora of conjectures on the nature of the instabilities as the steepest wave is approached, especially with regards to the self-similar recurrence of the stability spectrum near the origin of the spectral plane."],"url":"http://arxiv.org/abs/2404.15481v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 19:42:32","title":"Right-angled Artin subgroups and free products in one-relator groups","abstract":"We investigate criteria ensuring that a one-relator group $G$ contains a right-angled Artin subgroup $A(\\Gamma)$, corresponding to a finite graph $\\Gamma$. In particular, we prove that if the positive submonoid $T(\\Gamma)$, of $A(\\Gamma)$, embeds into $G$ then so does all of $A(\\Gamma)$, unless $\\Gamma$ is totally disconnected. As by-products of our methods we obtain characterisations of one-relator groups that have property $P_{nai}$ and that are $C^*$-simple.","sentences":["We investigate criteria ensuring that a one-relator group $G$ contains a right-angled Artin subgroup $A(\\Gamma)$, corresponding to a finite graph $\\Gamma$.","In particular, we prove that if the positive submonoid $T(\\Gamma)$, of $A(\\Gamma)$, embeds into $G$ then so does all of $A(\\Gamma)$, unless $\\Gamma$ is totally disconnected.","As by-products of our methods we obtain characterisations of one-relator groups that have property $P_{nai}$ and that are $C^*$-simple."],"url":"http://arxiv.org/abs/2404.15479v1","category":"math.GR"}
{"created":"2024-04-23 18:58:20","title":"Gauge Equivalence through Gauge Fixing","abstract":"Phenomena in gauge theory are often described in the physics literature via a specific choice of gauge. In foundational and philosophical discussions this is often criticized as introducing gauge dependence, and contrasted against (often aspirational) \"gauge-invariant\" descriptions of the physics. I argue, largely in the context of scalar electrodynamics, that this is misguided, and that descriptions of a physical process within a specific gauge are in fact gauge-invariant descriptions. However, most of them are non-local descriptions of that physics, and I suggest that this ought to be the real objection to such descriptions. I explore the unitary gauge as the exception to this nonlocality and consider its strengths and limitations, as well as (more briefly) its extension beyond scalar electrodynamics.","sentences":["Phenomena in gauge theory are often described in the physics literature via a specific choice of gauge.","In foundational and philosophical discussions this is often criticized as introducing gauge dependence, and contrasted against (often aspirational) \"gauge-invariant\" descriptions of the physics.","I argue, largely in the context of scalar electrodynamics, that this is misguided, and that descriptions of a physical process within a specific gauge are in fact gauge-invariant descriptions.","However, most of them are non-local descriptions of that physics, and I suggest that this ought to be the real objection to such descriptions.","I explore the unitary gauge as the exception to this nonlocality and consider its strengths and limitations, as well as (more briefly) its extension beyond scalar electrodynamics."],"url":"http://arxiv.org/abs/2404.15456v1","category":"physics.hist-ph"}
{"created":"2024-04-23 18:51:51","title":"Helical trilayer graphene in magnetic field: Chern mosaic and higher Chern number ideal flat bands","abstract":"Helical trilayer graphene (hTG) exhibits a supermoir\\'e pattern with large domains centered around stacking points ABA and BAB, where two well-separated low-energy bands appear with different total Chern numbers at each valley, forming a Chern mosaic pattern. In the chiral limit, the low-energy bands become exactly flat at zero energy for magic-angle twists. Here we investigate these zero-energy flat bands and their topological properties in the presence of a perpendicular magnetic field. We show that hTG retains the precise flatness of the zero-energy bands, even at finite magnetic fields. We find topological phase transitions at fields corresponding to unit and half magnetic flux leading to an emergence of higher Chern number flat bands. Consequently the Chern mosaic gets modified for finite magnetic fields. We further find the analytical forms of zero-energy wave functions and identify a set of hidden wave functions, which gives crucial insights into both the topological transitions and enhancement of Chern numbers across them. We also find topological transitions away from the chiral limit with finite corrugations and at different magic angles.","sentences":["Helical trilayer graphene (hTG) exhibits a supermoir\\'e pattern with large domains centered around stacking points ABA and BAB, where two well-separated low-energy bands appear with different total Chern numbers at each valley, forming a Chern mosaic pattern.","In the chiral limit, the low-energy bands become exactly flat at zero energy for magic-angle twists.","Here we investigate these zero-energy flat bands and their topological properties in the presence of a perpendicular magnetic field.","We show that hTG retains the precise flatness of the zero-energy bands, even at finite magnetic fields.","We find topological phase transitions at fields corresponding to unit and half magnetic flux leading to an emergence of higher Chern number flat bands.","Consequently the Chern mosaic gets modified for finite magnetic fields.","We further find the analytical forms of zero-energy wave functions and identify a set of hidden wave functions, which gives crucial insights into both the topological transitions and enhancement of Chern numbers across them.","We also find topological transitions away from the chiral limit with finite corrugations and at different magic angles."],"url":"http://arxiv.org/abs/2404.15452v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 18:35:21","title":"Electromagnetic Form Factors for Nucleons in Short-Range Correlations and the EMC effect","abstract":"The relationship between medium modifications of nucleon electromagnetic form factors and nucleon structure functions is examined using a model motivated by Light-Front Holographic QCD (LFHQCD). These modifications are closely connected with the influence of short-ranged correlations. The size of the modifications to nucleon form factors is shown to be about the same as the modifications to the structure functions. Thus, small limits on form factors modifications do not rule out an explanation of the EMC effect motivated by the influence of short range correlations, as claimed by a recent paper.","sentences":["The relationship between medium modifications of nucleon electromagnetic form factors and nucleon structure functions is examined using a model motivated by Light-Front Holographic QCD (LFHQCD).","These modifications are closely connected with the influence of short-ranged correlations.","The size of the modifications to nucleon form factors is shown to be about the same as the modifications to the structure functions.","Thus, small limits on form factors modifications do not rule out an explanation of the EMC effect motivated by the influence of short range correlations, as claimed by a recent paper."],"url":"http://arxiv.org/abs/2404.15442v1","category":"nucl-th"}
{"created":"2024-04-23 18:28:45","title":"Thermal boundary conductance and thermal conductivity strongly depend on nearby environment","abstract":"At the nanoscale, the thermal boundary conductance (TBC) and thermal conductivity are not intrinsic properties of interfaces or materials but depend on the nearby environment. In this study, we demonstrate how the TBC of an interface is affected by the existence of a second interface, as well as how the thermal conductivity of a material is affected by the nearby materials. Using Si and Ge modeled by classical molecular dynamics simulations, the following phenomena are discovered. (1) The existence of a nearby interface can significantly change the TBC of the original interface. For example, by adding an interface after Si/Ge, the TBC can be increased from 400 to 700 MW/m2K. This is because the nearby interface serves as a filter of phonon modes, which selectively allows particular modes to pass through and affect the TBC of the original interfaces. This impact will disappear at the diffusive limit when the distance between interfaces is much longer than the phonon mean free path so that phonon modes recover equilibrium statistics before arriving at the second interface. (2) The thermal conductivity of a material can be significantly changed by the existence of neighboring materials. For example, a standalone 30-nm-thick Si's thermal conductivity can be increased from 50 to 280 W/mK, a more than 4-fold increase, beating the bulk thermal conductivity of Si, after being sandwiched between two Ge slabs. This is because the Ge slabs on the two sides serve as filters that only allow low-frequency phonons to transport heat in Si, which carry more heat than optical phonons. This work opens a new area of successive interface thermal transport and is expected to be important for nanoscale thermal characterization and thermal management.","sentences":["At the nanoscale, the thermal boundary conductance (TBC) and thermal conductivity are not intrinsic properties of interfaces or materials but depend on the nearby environment.","In this study, we demonstrate how the TBC of an interface is affected by the existence of a second interface, as well as how the thermal conductivity of a material is affected by the nearby materials.","Using Si and Ge modeled by classical molecular dynamics simulations, the following phenomena are discovered.","(1) The existence of a nearby interface can significantly change the TBC of the original interface.","For example, by adding an interface after Si/Ge, the TBC can be increased from 400 to 700 MW/m2K. This is because the nearby interface serves as a filter of phonon modes, which selectively allows particular modes to pass through and affect the TBC of the original interfaces.","This impact will disappear at the diffusive limit when the distance between interfaces is much longer than the phonon mean free path so that phonon modes recover equilibrium statistics before arriving at the second interface.","(2) The thermal conductivity of a material can be significantly changed by the existence of neighboring materials.","For example, a standalone 30-nm-thick Si's thermal conductivity can be increased from 50 to 280 W/mK, a more than 4-fold increase, beating the bulk thermal conductivity of Si, after being sandwiched between two Ge slabs.","This is because the Ge slabs on the two sides serve as filters that only allow low-frequency phonons to transport heat in Si, which carry more heat than optical phonons.","This work opens a new area of successive interface thermal transport and is expected to be important for nanoscale thermal characterization and thermal management."],"url":"http://arxiv.org/abs/2404.15439v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 18:07:03","title":"The Type-I Seesaw family","abstract":"We provide a comprehensive analysis of the Type-I Seesaw family of neutrino mass models, including the conventional type-I seesaw and its low-scale variants, namely the linear and inverse seesaws. We establish that all these models essentially correspond to a particular form of the type-I seesaw in the context of explicit lepton number violation. We then focus into the more interesting scenario of spontaneous lepton number violation, systematically categorizing all inequivalent minimal models. Furthermore, we identify and flesh out specific models that feature a rich majoron phenomenology and discuss some scenarios which, despite having heavy mediators and being invisible in processes such as $\\mu \\to e \\gamma$, predict sizable rates for decays including the majoron in the final state.","sentences":["We provide a comprehensive analysis of the Type-I Seesaw family of neutrino mass models, including the conventional type-I seesaw and its low-scale variants, namely the linear and inverse seesaws.","We establish that all these models essentially correspond to a particular form of the type-I seesaw in the context of explicit lepton number violation.","We then focus into the more interesting scenario of spontaneous lepton number violation, systematically categorizing all inequivalent minimal models.","Furthermore, we identify and flesh out specific models that feature a rich majoron phenomenology and discuss some scenarios which, despite having heavy mediators and being invisible in processes such as $\\mu \\to e \\gamma$, predict sizable rates for decays including the majoron in the final state."],"url":"http://arxiv.org/abs/2404.15415v1","category":"hep-ph"}
{"created":"2024-04-23 18:06:38","title":"Axions in the Dark Dimension","abstract":"The dark dimension scenario, which is motivated from Swampland principles and predicts a single micron scale extra dimension, suggests a consistent framework for the dark sector of the universe. We consider the implications of this scenario for the QCD axion. We find that in the scenario in which the axion is localized on the standard model brane (which we will argue is natural), a combination of theoretical (being bounded by the 5D Planck mass) and observational constraints forces it to have decay constant in a narrow range $f \\sim 10^9 - 10^{10}$ GeV. This corresponds to a mass for the QCD axion of $m_a \\sim (1 - 10)$ meV. The axion mass surprisingly coincides with the mass scale for the dark energy, the dark matter tower, and the neutrinos. In this scenario axions are not expected to form a large fraction of the dark matter but nevertheless this range of axion parameters is accessible to observations in near future experiments.","sentences":["The dark dimension scenario, which is motivated from Swampland principles and predicts a single micron scale extra dimension, suggests a consistent framework for the dark sector of the universe.","We consider the implications of this scenario for the QCD axion.","We find that in the scenario in which the axion is localized on the standard model brane (which we will argue is natural), a combination of theoretical (being bounded by the 5D Planck mass) and observational constraints forces it to have decay constant in a narrow range $f \\sim 10^9 - 10^{10}$ GeV.","This corresponds to a mass for the QCD axion of $m_a \\sim (1 - 10)$ meV.","The axion mass surprisingly coincides with the mass scale for the dark energy, the dark matter tower, and the neutrinos.","In this scenario axions are not expected to form a large fraction of the dark matter but nevertheless this range of axion parameters is accessible to observations in near future experiments."],"url":"http://arxiv.org/abs/2404.15414v1","category":"hep-th"}
{"created":"2024-04-23 18:00:01","title":"On de Sitter vacua in O(d,d) invariant cosmology","abstract":"We perform a thorough analysis of de Sitter vacua in O(d,d) invariant cosmologies. Starting with a homogeneous and isotropic framework we examine conditions for the existence of such vacua, non-perturbative in \\alpha' in both the string frame and the Einstein frame. We elucidate the nature of the instability in the string frame vacuum. For the Einstein frame, we demonstrate that the de Sitter vacuum cannot be eternal. We then extend our analysis to include Bianchi I universes where the O(d,d) symmetry includes scale factor exchange as well as scale factor duality. We show how the theory can be extended to the anisotropic case so that it admits de Sitter vacua, noting the crucial role played by the O(d,d) symmetry in satisfying any additional constraints.","sentences":["We perform a thorough analysis of de Sitter vacua in O(d,d) invariant cosmologies.","Starting with a homogeneous and isotropic framework we examine conditions for the existence of such vacua, non-perturbative in \\alpha' in both the string frame and the Einstein frame.","We elucidate the nature of the instability in the string frame vacuum.","For the Einstein frame, we demonstrate that the de Sitter vacuum cannot be eternal.","We then extend our analysis to include Bianchi I universes where the O(d,d) symmetry includes scale factor exchange as well as scale factor duality.","We show how the theory can be extended to the anisotropic case so that it admits de Sitter vacua, noting the crucial role played by the O(d,d) symmetry in satisfying any additional constraints."],"url":"http://arxiv.org/abs/2404.15401v1","category":"hep-th"}
{"created":"2024-04-23 18:00:00","title":"The quantum adiabatic algorithm suppresses the proliferation of errors","abstract":"The propagation of errors severely compromises the reliability of quantum computations. The quantum adiabatic algorithm is a physically motivated method to prepare ground states of classical and quantum Hamiltonians. Here, we analyze the proliferation of a single error event in the adiabatic algorithm. We give numerical evidence using tensor network methods that the intrinsic properties of adiabatic processes effectively constrain the amplification of errors during the evolution for geometrically local Hamiltonians. Our findings indicate that low energy states could remain attainable even in the presence of a single error event, which contrasts with results for error propagation in typical quantum circuits.","sentences":["The propagation of errors severely compromises the reliability of quantum computations.","The quantum adiabatic algorithm is a physically motivated method to prepare ground states of classical and quantum Hamiltonians.","Here, we analyze the proliferation of a single error event in the adiabatic algorithm.","We give numerical evidence using tensor network methods that the intrinsic properties of adiabatic processes effectively constrain the amplification of errors during the evolution for geometrically local Hamiltonians.","Our findings indicate that low energy states could remain attainable even in the presence of a single error event, which contrasts with results for error propagation in typical quantum circuits."],"url":"http://arxiv.org/abs/2404.15397v1","category":"quant-ph"}
{"created":"2024-04-23 18:00:00","title":"Holographic scattering and non-minimal RT surfaces","abstract":"In the AdS/CFT correspondence, the causal structure of the bulk AdS spacetime is tied to entanglement in the dual CFT. This relationship is captured by the connected wedge theorem, which states that a bulk scattering process implies the existence of $O(1/G_N)$ entanglement between associated boundary subregions. In this paper, we study the connected wedge theorem in two asymptotically AdS$_{2+1}$ spacetimes: the conical defect and BTZ black hole geometries. In these settings, we find that bulk scattering processes require not just large entanglement, but also additional restrictions related to candidate RT surfaces which are non-minimal. We argue these extra relationships imply a certain CFT entanglement structure involving internal degrees of freedom. Because bulk scattering relies on sub-AdS scale physics, this supports the idea that sub-AdS scale locality emerges from internal degrees of freedom. While the new restriction that we identify on non-minimal surfaces is stronger than the initial statement of the connected wedge theorem, we find that it is necessary but still not sufficient to imply bulk scattering in mixed states.","sentences":["In the AdS/CFT correspondence, the causal structure of the bulk AdS spacetime is tied to entanglement in the dual CFT.","This relationship is captured by the connected wedge theorem, which states that a bulk scattering process implies the existence of $O(1/G_N)$ entanglement between associated boundary subregions.","In this paper, we study the connected wedge theorem in two asymptotically AdS$_{2+1}$ spacetimes: the conical defect and BTZ black hole geometries.","In these settings, we find that bulk scattering processes require not just large entanglement, but also additional restrictions related to candidate RT surfaces which are non-minimal.","We argue these extra relationships imply a certain CFT entanglement structure involving internal degrees of freedom.","Because bulk scattering relies on sub-AdS scale physics, this supports the idea that sub-AdS scale locality emerges from internal degrees of freedom.","While the new restriction that we identify on non-minimal surfaces is stronger than the initial statement of the connected wedge theorem, we find that it is necessary but still not sufficient to imply bulk scattering in mixed states."],"url":"http://arxiv.org/abs/2404.15400v1","category":"hep-th"}
