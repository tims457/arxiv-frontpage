<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=IBM Plex Mono' rel='stylesheet'>
</head>
<style>
  .mono-font {
    font-family: 'IBM Plex Mono';
  }
</style>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold mono-font"><a class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800" href=https://sullivantm.com>Tim's</a> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-02-26.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is generated by scraping new papers on Arxiv and using an embedding model to find papers matching topics I'm interested in. Currently, the false positive rate is fairly high. The repo is <a class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm" href=https://github.com/tims457/arxiv-frontpage/>here.</a> Forked and customized from<a class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm" href=https://koaning.github.io/arxiv-frontpage/> this project </a></p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Artificial General Intelligence</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>One of the important questions to be answered is how to conduct credit assignment in a multi-agent system.There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms.Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate.In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory.We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov Shapley value respectively.We represent a global reward game as a Markov convex game under the grand coalition.As a result, Markov Shapley value can be reasonably used as a credit assignment scheme in the global reward game.Markov Shapley value possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment.Based on Markov Shapley value, we propose three multi-agent reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO.Furthermore, we extend Markov convex game to partial observability to deal with the partially observable problems, named as partially observable Markov convex game.In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15324v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper addresses task planning problems for language-instructed robot teams. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects.<span class='px-1 mx-1 bg-yellow-200'>Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.826</span></span>However, these approaches lack mission performance and safety guarantees.To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates.This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models.CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise.We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates while minimizing the overall number of help requests.We demonstrate the performance of our approach on multi-robot home service applications.We also show through comparative experiments, that our method outperforms recent centralized and decentralized multi-robot LLM-based planners in terms of in terms of its ability to design correct plans.The advantage of our algorithm over baselines becomes more pronounced with increasing mission complexity and robot team size.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15368v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reputational Algorithm Aversion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called "algorithm aversion".This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability.I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal.Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it.However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill.<span class='px-1 mx-1 bg-yellow-200'>The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15418v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments.Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions.This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments.<span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene.To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities.<span class='px-1 mx-1 bg-yellow-200'>The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>We apply our system across various real-world settings in a zero-shot manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before.Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15487v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Autonomous agents powered by large language models (LLMs) have garnered significant research attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.846</span></span>However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories.In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges.\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios.It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training.Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training.Additionally, we present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15506v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Complex Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Probing critical phenomena in open quantum systems using atom arrays
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>At continuous phase transitions, quantum many-body systems exhibit scale-invariance and complex, emergent universal behavior. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span>Most strikingly, at a quantum critical point, correlations decay as a power law, with exponents determined by a set of universal scaling dimensions.Experimentally probing such power-law correlations is extremely challenging, owing to the complex interplay between decoherence, the vanishing energy gap, and boundary effects.Here, we employ a Rydberg quantum simulator to adiabatically prepare critical ground states of both a one-dimensional ring and a two-dimensional square lattice.By accounting for and tuning the openness of our quantum system, which is well-captured by the introduction of a single phenomenological length scale, we are able to directly observe power-law correlations and extract the corresponding scaling dimensions.Moreover, in two dimensions, we observe a decoupling between phase transitions in the bulk and on the boundary, allowing us to identify two distinct boundary universality classes.Our work demonstrates that direct adiabatic preparation of critical states in quantum simulators can complement recent approaches to studying quantum criticality using the Kibble-Zurek mechanism or digital quantum circuits.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15376v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Decision Making Under Uncertainty</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Information-Theoretic Safe Bayesian Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.821</span></span>A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability.Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case.Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter.In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate.The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion.Our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters.We theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision.Empirical evaluations demonstrate improved data-efficiency and scalability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15347v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Sample Average Approximation Method for Solving Two-Stage Stochastic Programs with Endogenous Uncertainty
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Real-world decision-making problems involve Type 1 decision-dependent uncertainty, where the probability distribution of the stochastic process depends on the model decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.881</span></span>However, few studies focus on two-stage stochastic programs with this type of endogenous uncertainty, and those that do lack general methodologies.We thus propose herein a general method for solving a class of these programs based on the transformation of random variables, a technique widely employed in probability and statistics.The proposed method is tailored to large-scale problems with discrete or continuous endogenous random variables.The random variable transformation allows the use of the sample average approximation (SAA) method, which provides optimality convergence guarantees under certain conditions.We show that, for some classical distributions, the proposed method reduces to solving mixed-integer linear or convex programs.Finally, we validate this method by applying it to a network design and facility-protection problem, considering distinct decision-dependent distributions for the random variables.Whereas most distributions result in a nonlinear nonconvex deterministic equivalent program, the proposed method solves mixed-integer linear programs in all cases.In addition, it produces attractive performance estimators for the SAA method in a reasonable computational time and outperforms the case in which the endogenous distribution defines a mixed-integer deterministic equivalent.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15486v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Reinforcement Learning</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span>It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations.For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward.However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will.This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario.In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation.This requires the introduction an original learning framework that copes with the intrinsic difficulty of the setting, for which the data coverage is not under control.Then, we propose two computationally and statistically efficient algorithms, IRLO and PIRLO, for addressing the problem.In particular, the latter adopts a specific form of pessimism to enforce the novel desirable property of inclusion monotonicity of the delivered feasible set.With this work, we aim to provide a panorama of the challenges of the offline IRL problem and how they can be fruitfully addressed.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15392v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.878</span></span>However, formulating realistic policies for robots demands responses from humans to an extensive array of queries.In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting.To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans.To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM.In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications.Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape.We provide video examples of the trained policies at https://sites.google.com/view/rl-predilect</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15420v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span>In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously.<span class='px-1 mx-1 bg-yellow-200'>The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.828</span></span>The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space.This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints.Through our theoretical analysis of SORL, we propose a condition for SORL's converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff.The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods.The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15197v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.898</span></span>One of the important questions to be answered is how to conduct credit assignment in a multi-agent system.There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms.Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate.In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory.We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov Shapley value respectively.We represent a global reward game as a Markov convex game under the grand coalition.As a result, Markov Shapley value can be reasonably used as a credit assignment scheme in the global reward game.Markov Shapley value possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment.Based on Markov Shapley value, we propose three multi-agent reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO.Furthermore, we extend Markov convex game to partial observability to deal with the partially observable problems, named as partially observable Markov convex game.In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15324v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel.We provide the first study on online DRMDPs with function approximation for off-dynamics RL.We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation.By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation.We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics RL with function approximation, and establish a polynomial suboptimality bound that is independent of the state and action space sizes.Our work makes the first step towards a deeper understanding of the provable efficiency of online DRMDPs with linear function approximation.Finally, we substantiate the performance and robustness of DR-LSVI-UCB through different numerical experiments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15399v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Trajectory Optimization</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Optimisation-based alignment of wideband integrated superconducting spectrometers for sub-mm astronomy
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Integrated superconducting spectrometers (ISSs) for wideband sub-mm astronomy utilise quasi-optical systems for coupling radiation from the telescope to the instrument.Misalignment in these systems is detrimental to the system performance.The common method of using an optical laser to align the quasi-optical components requires accurate alignment of the laser to the sub-mm beam coming from the instrument, which is not always guaranteed to a sufficient accuracy.We develop an alignment strategy for wideband ISSs directly utilising the sub-mm beam of the wideband ISS.The strategy should be applicable in both telescope and laboratory environments.Moreover, the strategy should deliver similar quality of the alignment across the spectral range of the wideband ISS.We measure misalignment in a quasi-optical system operating at sub-mm wavelengths using a novel phase and amplitude measurement scheme, capable of simultaneously measuring the complex beam patterns of a direct-detecting ISS across a harmonic range of frequencies.The direct detection nature of the MKID detectors in our device-under-test, DESHIMA 2.0, necessitates the use of this measurement scheme.<span class='px-1 mx-1 bg-yellow-200'>Using geometrical optics, the measured misalignment, a mechanical hexapod, and an optimisation algorithm, we follow a numerical approach to optimise the positioning of corrective optics with respect to a given cost function. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span>Laboratory measurements of the complex beam patterns are taken across a harmonic range between 205 and 391 GHz and simulated through a model of the ASTE telescope in order to assess the performance of the optimisation at the ASTE telescope.Laboratory measurements show that the optimised optical setup corrects for tilts and offsets of the sub-mm beam.Moreover, we find that the simulated telescope aperture efficiency is increased across the frequency range of the ISS after the optimisation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15381v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Information-Theoretic Safe Bayesian Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span>A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability.Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case.Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter.In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate.The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion.Our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters.We theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision.Empirical evaluations demonstrate improved data-efficiency and scalability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.15347v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
</script>
</body>
</html>